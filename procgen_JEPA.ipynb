{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "8471953a-c1b6-47df-eee6-c123530b02b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "e3bd4887-9934-4c5e-b56b-931c499e00d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model=256, drop=0.5):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Dropout(p=drop),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Bos81kQf1dwh"
      },
      "outputs": [],
      "source": [
        "# @title transfer_sd store_sd load_sd\n",
        "\n",
        "def transfer_sd(tgt_sd, src_sd): #\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            # print(wht_name, tgt_wht.shape, src_wht.shape)\n",
        "            if tgt_wht.shape==src_wht.shape:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "                continue\n",
        "            if tgt_wht.shape[0] != src_wht.shape[0]: continue # output dim diff\n",
        "            if len(tgt_wht.shape)==2: tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "    return tgt_sd\n",
        "\n",
        "def store_sd(all_sd, new_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in new_sd.keys():\n",
        "            if not wht_name in all_sd.keys():\n",
        "                # print(wht_name, new_sd[wht_name].shape)\n",
        "                all_sd[wht_name] = (new_sd[wht_name],)\n",
        "                continue\n",
        "            all_tpl, new_wht = all_sd[wht_name], new_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                print(wht_name, all_wht.shape, new_wht.shape)\n",
        "                if all_wht.shape==new_wht.shape:\n",
        "                    all_wht = new_wht\n",
        "                    break\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: continue # diff output shape\n",
        "                if len(all_wht.shape)==2: all_wht[:, :new_wht.shape[1]] = new_wht[:, :all_wht.shape[1]]\n",
        "                break\n",
        "            if len(all_wht.shape)>=2 and len(all_wht.shape)>=2:\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: all_tpl = all_tpl + (new_wht,) # wht not in all_wht\n",
        "    return all_sd\n",
        "\n",
        "def load_sd(tgt_sd, all_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in all_sd.keys(): continue\n",
        "            tgt_wht, all_tpl = tgt_sd[wht_name], all_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                # try: print(wht_name, tgt_wht.shape, all_wht.shape)\n",
        "                # except: print(wht_name, tgt_wht, all_wht)\n",
        "                if tgt_wht.shape==all_wht.shape:\n",
        "                    tgt_wht.copy_(all_wht)\n",
        "                    break\n",
        "                if tgt_wht.shape[0] != all_wht.shape[0]: continue # output dim diff\n",
        "                if len(tgt_wht.shape)==2: tgt_wht[:, :all_wht.shape[1]].copy_(all_wht[:, :tgt_wht.shape[1]])\n",
        "                break\n",
        "    return tgt_sd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# modelsd = torch.load('agent.pkl', map_location=device).values()\n",
        "# tgt_sd = transfer_sd(agent.state_dict(), modelsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = {}\n",
        "# all_sd = store_sd(all_sd, agent1.state_dict())\n",
        "# print(all_sd.keys())\n",
        "# checkpoint = {'model': all_sd}\n",
        "# torch.save(checkpoint, 'all_sd.pkl')\n",
        "\n",
        "# agent3 = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "# agent3.tcost = tcost3\n",
        "# tgt_sd = load_sd(agent3.state_dict(), all_sd)\n",
        "# agent3.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "# for x,y in zip(agent1.state_dict().values(), agent3.state_dict().values()):\n",
        "#     print((x==y).all())\n",
        "\n",
        "# print(agent1.jepa.enc.cnn[1].num_batches_tracked)\n",
        "# jepa.enc.cnn.0.weight\n",
        "# print(agent1.jepa.enc.cnn[0].weight.shape)\n",
        "# print(agent1.jepa.enc.cnn[0].weight[0][0])\n",
        "# print(agent3.jepa.enc.cnn[0].weight[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "SFVbGqMDqcDR"
      },
      "outputs": [],
      "source": [
        "# @title rename_sd\n",
        "def rename_sd(agent_sd):\n",
        "    sd_={}\n",
        "    convert={}\n",
        "    na_=''\n",
        "    for wht_name, wht in agent_sd.items():\n",
        "        o=wht_name.split('.')\n",
        "        # print(\"####\", wht_name)\n",
        "        name=wht_name\n",
        "        for i in range(len(o)):\n",
        "            c = o[i]\n",
        "            if c.isnumeric():\n",
        "                na, me = '.'.join(o[:i]), '.'.join(o[i+1:])\n",
        "                c=int(c)\n",
        "                if na!=na_: # param name diff\n",
        "                    j=0 # reset num\n",
        "                    c_=c # track wht_name num\n",
        "                    na_=na # track param name\n",
        "                elif c_<c: # same param name, diff num\n",
        "                    j+=1\n",
        "                    c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "        # print(name)\n",
        "        sd_[name] = wht\n",
        "        convert[name] = wht_name\n",
        "    return sd_, convert\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# # modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, _ = rename_sd(modelsd)\n",
        "\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "riBHnAAkkzrd"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim me\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# def transfer_optim(tgt_sd, src_sd, tgt_optim, src_optim): #\n",
        "def transfer_optim(tgt_sd, src_sd, tgt_optim_sd, src_optim_sd): #\n",
        "    non_lst = ['running_mean', 'running_var', 'num_batches_tracked', 'num_batches_tracked', 'loss_fn']\n",
        "    tgt_lst, src_lst = [], []\n",
        "    for i, (k,v) in enumerate(tgt_sd.items()):\n",
        "        # print(i, k, v.shape, any(s in k for s in non_lst))\n",
        "        if not any(s in k for s in non_lst): tgt_lst.append(k)\n",
        "    for i, (k,v) in enumerate(src_sd.items()):\n",
        "        if not any(s in k for s in non_lst): src_lst.append(k)\n",
        "\n",
        "    # tgt_optim_st, src_optim_st = tgt_optim.state_dict()['state'], src_optim.state_dict()['state']\n",
        "    tgt_optim_st, src_optim_st = tgt_optim_sd['state'], src_optim_sd['state']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, wht_name in enumerate(tgt_lst):\n",
        "            if not wht_name in src_lst: continue\n",
        "            tgt_wht, src_wht = tgt_optim_st[tgt_lst.index(wht_name)], src_optim_st[src_lst.index(wht_name)]\n",
        "            # print(wht_name, tgt_wht, src_wht)\n",
        "            tgt_shp, src_shp = tgt_wht['exp_avg'].shape, src_wht['exp_avg'].shape\n",
        "            if tgt_shp==src_shp:\n",
        "                tgt_wht = src_wht\n",
        "                continue\n",
        "            if tgt_shp[0] != src_shp[0]: continue # output dim diff\n",
        "            if len(tgt_shp)==2:\n",
        "                tgt_wht['step'] = src_wht['step']\n",
        "                tgt_wht['exp_avg'][:, :src_shp[1]] = src_wht['exp_avg'][:, :tgt_shp[1]]\n",
        "                tgt_wht['exp_avg_sq'][:, :src_shp[1]] = src_wht['exp_avg_sq'][:, :tgt_shp[1]]\n",
        "    # return tgt_optim.state_dict()\n",
        "    return tgt_optim_sd\n",
        "\n",
        "# model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "# model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "# source_optimizer = optim.AdamW(model_src.parameters())\n",
        "# target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "# dummy_input = torch.randn(3, 10)\n",
        "# dummy_target = torch.randn(3, 5)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "# output = model_src(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# source_optimizer.step()\n",
        "\n",
        "# dummy_input = torch.randn(3, 20)\n",
        "# output = model_tgt(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# target_optimizer.step()\n",
        "\n",
        "\n",
        "# print(source_optimizer.state_dict())\n",
        "# print(target_optimizer.state_dict())\n",
        "\n",
        "# optimsd = transfer_optim(model_tgt.state_dict(), model_src.state_dict(), target_optimizer, source_optimizer)\n",
        "# target_optimizer.load_state_dict(optimsd)\n",
        "# print(target_optimizer.state_dict())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AfjFbveH64Io"
      },
      "outputs": [],
      "source": [
        "# @title TCost\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TCost(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=256): # in_dim=(1+self.jepa.pred.num_layers)*d_model\n",
        "        super().__init__()\n",
        "        self.tc = torch.tensor([-1., 0.], device=device).unsqueeze(-1) # unsqueeze(0).T\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_dim, 2, bias=False), nn.Softmax(dim=-1),\n",
        "            # nn.Linear(in_dim, d_model), nn.ReLU(),\n",
        "            # nn.Dropout(p=0.5),\n",
        "            # nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, 2), nn.Softmax(),\n",
        "            )\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def update_loss_weight(self, train_data):\n",
        "        a = len(buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tcost(x)@self.tc\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        out = self.tcost(x)\n",
        "        y = torch.where(y < -0.5, 0, 1)\n",
        "        return self.loss_fn(out, y)\n",
        "\n",
        "\n",
        "# tcost=TCost(1024)\n",
        "# x=torch.rand(256,1024)\n",
        "# import time\n",
        "# start = time.time()\n",
        "# out=tcost(x)\n",
        "# # out=F.gumbel_softmax(out)\n",
        "# print(time.time()-start)\n",
        "# # nn.AdaptiveLogSoftmaxWithLoss(in_features=2, n_classes=2, cutoffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "a1f3f925-301c-4be0-ef0a-0760a8cc27ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-5d08c402e0cb>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v, drop=0.2):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=drop)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)# + self.z_coeff * torch.norm(z)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "# torch.norm(z, dim=-1)\n",
        "# -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "# in RL, distribution of action, if certainty is high, entropy is low\n",
        "\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "9bb23ab0-a075-4837-f157-a0b7cb409814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-ff80b479d892>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=8, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=50. # 50 # 10 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 20 # 50 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 0.001 # 1 # ν cov Covariance\n",
        "        self.closs_coeff=100. # 100 # 100\n",
        "        self.zloss_coeff=10. # 20 # 1\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        self.lx, self.lz = torch.empty((0,dim_a),device=device), torch.empty((0,dim_z),device=device) # [T,dim_az]\n",
        "        self.sx = self.jepa.enc(torch.zeros((1, 3,64,64)))\n",
        "        self.la = torch.empty(0,device=device)\n",
        "\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        if len(self.la)>1 or laction!=None:\n",
        "            self.update_h0(lstate, laction)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                self.sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(self.sx, T=8, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.la, self.lx, self.lz = lact, lx, lz\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx - torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    self.la = torch.cat([torch.tensor(laction, device=device), self.la[len(laction):]], dim=-1)\n",
        "                la = self.emb(self.la[:seq_len])\n",
        "\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5) # torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        optim_z = torch.optim.SGD([lz], lr=1e1) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e0 ; 3e-2 1e-1\n",
        "        lsx, la = lsx.detach(), la.detach() # [T, d_model], [T, dim_a]\n",
        "        # print(\"update_h0 lz\", lz.data)\n",
        "        self.jepa.pred.train()\n",
        "        for i in range(1): # 1?\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0.detach()) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                loss = F.mse_loss(out_, out.squeeze(0))\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            # print(\"update_h0 loss, lz\",i,loss.item(), lz.data)\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1].unsqueeze(0)\n",
        "        # print(\"update_h0\", self.lx.data)\n",
        "        # print(self.la.shape, self.lx.shape, self.lz.shape, self.la[seq_len:].shape, self.lx[seq_len:].shape, self.lz[seq_len:].shape)\n",
        "        self.la, self.lx, self.lz = self.la[seq_len:], self.lx[seq_len:], self.lz[seq_len:] # [T, dim_a], [T, dim_z]\n",
        "        return h0\n",
        "\n",
        "    def argm_s(self, sx, x, h0): # batch argm z for search\n",
        "        T, _ = x.shape\n",
        "        batch = 64 # 16\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        optim_z = torch.optim.SGD([z], lr=1e4, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        with torch.no_grad():\n",
        "            z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_z]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        x = x.detach().repeat(batch,1,1) # [batch, seq_len, dim_a]\n",
        "        # print(\"argm\", z[0].squeeze())\n",
        "        for i in range(2): # 5\n",
        "            loss, lh0 = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"argm z loss\", z[0].squeeze().data, loss[0].squeeze().data)\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return z[idx]\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        x = nn.Parameter(torch.empty((T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        optim_x = torch.optim.SGD([x], lr=1e3) # 1e-1,1e-0,1e4 ; 1e2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        with torch.no_grad(): x[:self.lx.shape[0]] = self.lx[:T] # [seq_len, dim_az]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        # print(\"search x\",x.squeeze().data)\n",
        "        for i in range(2): # 5\n",
        "            dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data # [T, dim_a]\n",
        "            z = self.argm_s(sx, x_,h0) # [T, dim_z]\n",
        "            loss, lh0 = self.rnn_pred(sx, x_.unsqueeze(0), z.unsqueeze(0), h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i, \"search x loss\", x.squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [T]\n",
        "        return lact, lh0, x.data, z # [T], [T, num_layers, batch, d_model], [T, dim_a], [T, dim_z]\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lsx = torch.empty((batch, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1) # [batch, 1, d_model+dim_a/z]\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        syh0 = torch.cat([lsx, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,T,d_model], [T,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "        tcost = -self.tcost(syh0).unflatten(0, (batch, seq_len)).squeeze(-1)\n",
        "        c = (tcost + icost)*gamma**torch.arange(seq_len, device=device)\n",
        "        # if len(c.shape) == 1: print(\"rnn_pred c\", [f'{cc.item():g}' for cc in c.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        if len(tcost.shape) == 1: print(\"rnn_pred tcost\", [f'{cc.item():g}' for cc in tcost.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        return c.sum(), lh0\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd):\n",
        "        self.tcost.eval()\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "            lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "                for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                    syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                    out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                    lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                    lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "                repr_loss = F.mse_loss(lsy, lsy_)\n",
        "                syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "                # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "                cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        return lz.detach()\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        for batch, (state, action, reward) in enumerate(dataloader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            sy_ = self.jepa.enc(torch.zeros((batch_size, 3,64,64), device=device)).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "            # sx=sy_\n",
        "            state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "            for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "                lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "                lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    lsy = self.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "                    la = self.emb(act) # [batch_size, bptt, dim_a]\n",
        "                    lz = self.argm(lsy, sy_, h0, la, rwd) # [batch_size, bptt, d_model],\n",
        "                    with torch.no_grad(): lz.mul_(torch.rand_like(lz)).mul_((torch.rand_like(lz)>0.5).bool()) # dropout without scailing\n",
        "\n",
        "                    for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                        syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                        out_, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                        sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                        lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                        lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "                    repr_loss = F.mse_loss(lsy, lsy_) # [batch_size, bptt, d_model]\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "\n",
        "                    # syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                    syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                    clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    pred = self.tcost(syh0).squeeze(-1).unflatten(0, rwd.shape) # [batch_size, bptt]\n",
        "                    print(\"pred\",pred[0])\n",
        "                    print(\"rwd\",rwd[0])\n",
        "                    mask = torch.where(abs(rwd- pred)>0.5,1,0).bool()\n",
        "                    # # print(\"rwd, pred, clossl\", rwd[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(st[0].cpu(), nrow=10))\n",
        "                    # # try: imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "                loss = jloss + closs\n",
        "\n",
        "                # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                # norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "                # z_norm = torch.norm(z)\n",
        "                # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                # print(\"clossl, wrong\", clossl.item(), mask.sum())\n",
        "                print(\"repr, std, cov, clossl, wrong\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), mask.sum().item())\n",
        "                # print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                scaler.scale(loss).backward()\n",
        "                # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "                optim.zero_grad()\n",
        "                sy_, h0 = sy_.detach(), h0.detach()\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.999)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "# !pip show torch triton\n",
        "# # !pip install --upgrade torch\n",
        "# !pip install --upgrade triton\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "# print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "# print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "# print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(agent.tcost._parameters['weight'].shape)\n"
      ],
      "metadata": {
        "id": "FwgWasBjZ04u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bedcfc8-cb62-4a2d-fb49-e20e8bf478aa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1605, 0.5321, 0.4531],\n",
            "        [0.0124, 0.9639, 0.6233]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(agent.jepa.enc.parameters().values()[0].requires_grad)\n",
        "# for name, param in agent.tcost.named_parameters():\n",
        "# # # for name, param in agent.named_parameters():\n",
        "# #     # print(name, param.requires_grad)\n",
        "#     print(name, param)\n",
        "\n",
        "for name, param in agent.tcost.named_parameters(): print(param.data)\n",
        "\n",
        "# print(agent.tcost.1.weight.data)\n",
        "\n",
        "# print(agent.tcost.named_parameters()['tcost.1.weight'])\n",
        "\n",
        "# print(vars(agent.jepa.exp.named_parameters()['exp.1.weight']))"
      ],
      "metadata": {
        "id": "49RERFWFMgA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEH1P802JkHU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dim_a, dim_z = 3, 8\n",
        "# batch, T = 4,6\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_a),device=device))\n",
        "# torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "# dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# x = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "# z = nn.Parameter(torch.zeros((batch, T, dim_z),device=device))\n",
        "# torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# state = torch.zeros((1, 3,64,64))\n",
        "# # state = torch.rand((1, 3,64,64), device=device)\n",
        "# sx = agent.jepa.enc(state)\n",
        "\n",
        "act = agent([state], k=4)\n",
        "# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\n",
        "# loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "# print(loss,c)\n",
        "# print(lact, lh0, lx, lz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "ed643f6d-de11-4936-b923-df6d95210842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH\n",
            "From (redirected): https://drive.google.com/uc?id=1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH&confirm=t&uuid=9bebb1db-7115-4ae9-9f06-d12c0b907643\n",
            "To: /content/agentoptim.pkl\n",
            "100% 28.1M/28.1M [00:00<00:00, 51.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ\n",
            "From (redirected): https://drive.google.com/uc?id=1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ&confirm=t&uuid=8ea041fe-9ca8-41f6-be9e-dc44587ee113\n",
            "To: /content/buffergo.pkl\n",
            "100% 1.80G/1.80G [00:28<00:00, 62.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1 gru3 tcost1\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2 gru3 tcost1\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4 gru1 tcost1 drop\n",
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 1UDgNtFsWGAhvqR9lwA0QbMLhUtmip4ne -O agentoptim.pkl # M1 agentoptimgru3tcost1\n",
        "# !gdown 1-0oc6yucS5JXLHX1zqbYe3NTVMuhP_5r -O agentoptim.pkl # A2 agentoptim25251c25z3\n",
        "# !gdown 1U1CuCU1FugkrzPXsvTPpIX-wzWz6szl2 -O agentoptim.pkl # T4 agentoptimargm\n",
        "# !gdown 1CWZAtiEwSnglClJbq2LJTYlKhPN10gfo -O agentoptim.pkl # S3 agentoptimargm\n",
        "# !gdown 1XAbr6l1pCmcUCKR6kYlQ_dSDsOBqRg_j -O agentoptim.pkl # B2 argm2search2\n",
        "# !gdown 1UkQuf-IC2LYErSapkF6rZM1dv3svGI5P -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1-4sNf6mINCiD5YsBdQvCrlyqzzfS64si -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1MV9Qj_53Vu6wpe7nOFn47M5vDj7F7-gv -O agentoptim.pkl # S3 agentoptimargm2\n",
        "# !gdown 1--1Vl3337zugQng-j1qbptFY8EvhZA-T -O agentoptim.pkl # T4 agentoptimargm3 online\n",
        "# !gdown 1XHFBVPSH4T4FpUOBKN8X20xDQLNmL7go -O agentoptim.pkl # M1 agentoptimargm4\n",
        "!gdown 1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH -O agentoptim.pkl # B2 agentoptimargm4\n",
        "\n",
        "# !gdown 1sCW9uvcdCJkCH5HQDdISLws5rMvmkmFR -O all_sd.pkl # M1 all_sd\n",
        "\n",
        "import pickle\n",
        "# !gdown 1j9hOq8_752duPB0PMYUJqabNvYoGLysX -O buffer512down.pkl # S\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "# with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB -O buffergo.pkl # S3\n",
        "# !gdown 1egXy0t_kn0M0oL6sbwixoVr7bqMfcB8j -O buffergo.pkl # T4\n",
        "!gdown 1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ -O buffergo.pkl # B2\n",
        "with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "c98cbd95-f856-4278-802f-860eff20955b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "# with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptimargm4.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# modelsd = transfer_sd(agent.state_dict(), modelsd)\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "# # optimsd = transfer_optim(agent.state_dict(), modelsd, optim.state_dict(), optimsd)\n",
        "optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = torch.load(folder+'all_sd.pkl', map_location=device)\n",
        "# # all_sd = torch.load('all_sd.pkl', map_location=device)\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in all_sd.items())\n",
        "# allsd = {}\n",
        "# for (k, v) in all_sd.items():\n",
        "#     try: allsd[convert[k]] = v\n",
        "#     except Exception as e: print('dict err', e)\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# tgt_sd = load_sd(agent.state_dict(), allsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# for i, (k,v) in enumerate(modelsd.items()):\n",
        "# for i, (k,v) in enumerate(agent.state_dict().items()):\n",
        "#     print(i,k,v.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# checkpoint = {'model': agentsd, 'optimizer': optim.state_dict(),}\n",
        "checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "# torch.save(checkpoint, folder+'agentoptimargm4.pkl')\n",
        "# torch.save(checkpoint, 'agentoptim.pkl')\n",
        "\n",
        "# all_sd = {}\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# all_sd = store_sd(all_sd, agentsd)\n",
        "# # torch.save(all_sd, 'all_sd.pkl')\n",
        "# torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 100):] for episode in cleaned]\n",
        "        random.shuffle(cleaned)\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "\n",
        "    # def pop_unif(self, buffer_, n=3):\n",
        "    #     buffer_.pop(random.randrange(len(buffer_)))\n",
        "    #     return buffer_\n",
        "\n",
        "# while len(train_data.data)>10000:\n",
        "#     buffer.pop(random.randrange(len(buffer)))\n",
        "#     train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True)\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # # [3,T,batch]\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "1e3fpbtNOiz1",
        "outputId": "41543b98-7b10-40f0-abba-288e05ca4205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.,  0., -1., -1., -1.,  0.,  0., -1., -1.,  0.])\n",
            "tensor([ 0., -1., -1., -1., -1., -1., -1.,  0.,  0., -1.])\n",
            "tensor([-1.,  0., -1., -1.,  0., -1.,  0., -1., -1.,  0.])\n",
            "tensor([ 0., -1., -1., -1., -1., -1.,  0., -1., -1., -1.])\n",
            "tensor([-4.5080e-02, -6.2152e-03, -9.7132e-01, -9.9999e-01, -2.1281e-02,\n",
            "        -1.1073e-03, -2.1539e-05, -9.9987e-01, -9.2031e-01, -7.7323e-02])\n",
            "tensor([-5.8012e-06, -5.8340e-01, -1.0000e+00, -9.8076e-03, -9.6899e-01,\n",
            "        -9.9598e-01, -2.4301e-01, -6.9262e-07, -2.2051e-02, -9.0329e-02])\n",
            "tensor([-9.9955e-01, -1.4793e-03, -9.9994e-01, -7.5381e-02, -9.9995e-01,\n",
            "        -6.1456e-04, -1.7773e-04, -9.9959e-01, -9.9923e-01, -2.0209e-04])\n",
            "tensor([-1.1746e-02, -2.5695e-02, -1.0000e+00, -2.3721e-03, -7.4694e-01,\n",
            "        -9.4715e-01, -4.3037e-07, -9.9999e-01, -2.7835e-02, -1.0000e+00])\n",
            "tensor(0.6742, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor(0.2562)\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1])\n",
            "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0])\n",
            "tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0])\n",
            "reward, pred tensor([-1., -1., -1., -1., -1., -1.,  0., -1., -1., -1., -1.]) tensor([-4.5080e-02, -2.1281e-02, -9.8076e-03, -2.4301e-01, -9.0329e-02,\n",
            "        -7.5381e-02, -9.9995e-01, -6.1456e-04, -2.5695e-02, -2.3721e-03,\n",
            "        -2.7835e-02])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "ctrain_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    ctrain_data=list(zip(state,reward))\n",
        "    ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# images, labels = images.to(device), labels.to(device)\n",
        "batch=40\n",
        "images, labels = images[:batch], labels[:batch]\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range(len(labels)//10):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# # try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch, agent.d_model), device=device)\n",
        "    # h0 = torch.empty((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device)\n",
        "    # torch.nn.init.xavier_normal_(h0)\n",
        "    sy = agent.jepa.enc(images.to(device)) # [batch_size, d_model]\n",
        "    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "    pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "\n",
        "    # print(pred)\n",
        "    for x in range(len(pred)//10):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(agent.tcost.loss(syh0, labels.to(device)).squeeze(-1))\n",
        "print(F.mse_loss(labels, pred))\n",
        "\n",
        "# torch.where(abs(labels- pred)>0.5,1,0)\n",
        "for x in range(len(pred)//10):\n",
        "    print(torch.where(abs(labels- pred)>0.5,1,0)[10*x:10*x+10])\n",
        "\n",
        "mask = torch.where(abs(labels- pred)>0.5,1,0).bool()\n",
        "print(\"reward, pred\", labels[mask].data, pred[mask].data)\n",
        "try: imshow(torchvision.utils.make_grid(images[mask], nrow=10))\n",
        "except ZeroDivisionError: pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viimAIpYSJq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70264497-acd0-4475-e627-1be37d7ec960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5161, 0.4839],\n",
            "        [0.4553, 0.5447],\n",
            "        [0.6737, 0.3263],\n",
            "        [0.4455, 0.5545],\n",
            "        [0.2712, 0.7288],\n",
            "        [0.6653, 0.3347],\n",
            "        [0.5739, 0.4261],\n",
            "        [0.3988, 0.6012],\n",
            "        [0.5869, 0.4131],\n",
            "        [0.5607, 0.4393]])\n",
            "tensor(0.5933)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "labels = torch.tensor([0])\n",
        "# pred = torch.tensor([[.999,.001]])\n",
        "pred = torch.tensor([[1.,0.]])\n",
        "pred = torch.tensor([[5.,-5.]])\n",
        "# pred = torch.tensor([[.5,.5]])\n",
        "# pred = torch.tensor([[1/a, 1/(1-a)]])\n",
        "# pred = torch.tensor([[1/(1-a), 1/a]])\n",
        "# print(F.mse_loss(labels, pred))\n",
        "\n",
        "pred = torch.rand(10,2)\n",
        "pred = nn.Softmax(dim=-1)(pred)\n",
        "print(pred)\n",
        "labels = torch.where(torch.rand(10)>0.5,1,0)\n",
        "\n",
        "\n",
        "a=10\n",
        "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)]))\n",
        "print(loss_fn(pred, labels))\n",
        "\n",
        "# print((pred@torch.log(pred).T).sum())\n",
        "# print(pred,torch.log(pred).T)\n",
        "\n",
        "# 0.6931\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# optim = torch.optim.SGD(agent.parameters(), 1e-1, momentum=0.9, dampening=0, weight_decay=0)\n",
        "# print(optim.param_groups[0][\"lr\"])\n",
        "# print(optim)\n",
        "optim.param_groups[0][\"lr\"] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OksdjCeJYpYh",
        "outputId": "d7a9aabc-e82b-46de-860c-6786fbabfe7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-ff80b479d892>:252: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-42-ff80b479d892>:215: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03094419464468956 0.0011606216430664062 0.6808014512062073 0.35034745931625366 tensor(66, device='cuda:0')\n",
            "pred tensor([-9.0599e-06, -1.9646e-04, -4.4098e-03, -2.4915e-04, -3.0220e-05,\n",
            "        -3.9935e-05, -1.6093e-04, -5.6267e-04, -6.7749e-02, -9.4727e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.9670e-06, -2.9135e-04,\n",
            "        -6.5756e-04, -2.6727e-04, -6.0463e-04, -1.0424e-03, -2.6822e-06,\n",
            "        -2.5630e-06, -3.2306e-05, -3.1710e-05, -3.9935e-06, -9.2089e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02924066036939621 0.0013208389282226562 0.6793785095214844 0.34211206436157227 tensor(50, device='cuda:0')\n",
            "pred tensor([-4.6196e-03, -3.7336e-04, -7.1955e-04, -1.6041e-03, -6.2037e-04,\n",
            "        -1.1593e-04, -2.2888e-05, -9.1124e-04, -4.5258e-02, -2.4014e-03,\n",
            "        -4.4155e-04, -2.9869e-03, -9.3269e-04, -1.1545e-04, -2.5606e-04,\n",
            "        -5.1260e-05, -2.0921e-05, -9.7215e-05, -2.9802e-05, -1.6689e-05,\n",
            "        -2.1935e-05, -7.4463e-03, -2.4242e-03, -3.7372e-05, -1.0431e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032025985419750214 0.0004925727844238281 0.7044843435287476 0.3335520923137665 tensor(57, device='cuda:0')\n",
            "pred tensor([-3.3855e-04, -2.0564e-05, -1.1414e-04, -5.2452e-06, -3.2377e-04,\n",
            "        -1.0431e-04, -5.5432e-06, -6.1393e-06, -1.1104e-04, -8.8215e-06,\n",
            "        -2.7418e-06, -9.1791e-06, -1.2219e-05, -2.8431e-05, -4.1962e-05,\n",
            "        -9.1260e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -2.1782e-03,\n",
            "        -3.0828e-04, -4.6110e-04, -3.5977e-04, -1.7262e-04, -2.3782e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029607262462377548 0.0003528594970703125 0.7091867923736572 0.337412565946579 tensor(64, device='cuda:0')\n",
            "pred tensor([-1.3769e-04, -5.3465e-05, -1.6153e-05, -1.3709e-05, -4.9067e-04,\n",
            "        -8.9943e-05, -7.4959e-04, -1.4257e-04, -2.3842e-07, -8.3447e-07,\n",
            "        -3.7575e-03, -5.2588e-01, -9.9365e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -3.2878e-04, -9.1124e-04, -1.2894e-03, -7.0572e-04, -3.3212e-04,\n",
            "        -6.8963e-05, -8.0585e-05, -5.5194e-05, -3.6478e-05, -2.7776e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029825204983353615 0.0003819465637207031 0.7086137533187866 0.3449750244617462 tensor(52, device='cuda:0')\n",
            "pred tensor([-2.2876e-04, -3.9458e-05, -4.3564e-03, -3.1638e-04, -9.2804e-05,\n",
            "        -6.2048e-05, -6.3300e-05, -8.2493e-04, -3.0696e-05, -1.9722e-03,\n",
            "        -1.8179e-05, -1.3447e-04, -4.5657e-04, -1.6153e-05, -1.1235e-04,\n",
            "        -2.0111e-04, -1.3661e-04, -4.9651e-05, -4.2295e-04, -1.1158e-03,\n",
            "        -1.8177e-03, -3.1137e-04, -7.2813e-04, -1.0902e-02, -2.0742e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028820984065532684 0.00014162063598632812 0.740606427192688 0.3408146798610687 tensor(68, device='cuda:0')\n",
            "pred tensor([-9.4833e-03, -5.1439e-05, -1.7130e-04, -2.8193e-05, -1.0598e-04,\n",
            "        -5.1856e-05, -9.6083e-05, -1.4710e-04, -3.3927e-04, -2.8253e-04,\n",
            "        -6.1393e-06, -1.0967e-05, -1.5962e-04, -2.0027e-04, -1.6284e-04,\n",
            "        -1.7548e-03, -2.3422e-03, -3.2961e-05, -9.8038e-03, -1.0862e-03,\n",
            "        -1.2338e-04, -2.0993e-04, -4.9889e-05, -1.5473e-04, -8.4162e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030026806518435478 0.00029659271240234375 0.6894150376319885 0.3624778091907501 tensor(45, device='cuda:0')\n",
            "pred tensor([-6.1333e-05, -8.3447e-04, -7.1955e-04, -3.4070e-04, -5.6601e-04,\n",
            "        -1.7679e-04, -9.3079e-04, -2.2659e-03, -1.1225e-03, -3.6693e-04,\n",
            "        -2.9736e-01, -7.0648e-03, -7.2449e-02, -5.8411e-02, -3.6564e-03,\n",
            "        -3.6640e-03, -1.0109e-04, -1.2617e-03, -4.4098e-03, -1.6797e-04,\n",
            "        -3.8548e-03, -2.3139e-04, -2.0266e-04, -2.2781e-04, -3.6693e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02883344143629074 0.0014324188232421875 0.6643908023834229 0.3409091830253601 tensor(56, device='cuda:0')\n",
            "pred tensor([-3.7766e-04, -6.1333e-05, -1.0890e-04, -7.9691e-05, -1.5593e-04,\n",
            "        -1.1504e-05, -1.5020e-05, -6.1417e-04, -8.5926e-04, -2.8014e-06,\n",
            "        -7.7486e-06, -4.3631e-05, -1.4019e-03, -3.7441e-03, -1.9491e-04,\n",
            "        -2.9981e-05, -8.1539e-05, -2.5749e-05, -1.4591e-04, -7.3969e-05,\n",
            "        -7.2241e-05, -7.1526e-06, -2.3842e-07, -3.9935e-06, -3.0220e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030463628470897675 0.001445770263671875 0.7133703827857971 0.35937562584877014 tensor(64, device='cuda:0')\n",
            "pred tensor([-2.2423e-04, -2.1076e-04, -6.8140e-04, -3.0422e-04, -3.2067e-05,\n",
            "        -4.7112e-04, -7.0333e-06, -2.6226e-06, -1.3399e-04, -3.8218e-04,\n",
            "        -2.4140e-05, -1.9867e-02, -1.0228e-04, -1.4043e-04, -4.0531e-05,\n",
            "        -1.6415e-04, -4.5657e-04, -3.0398e-06, -3.2640e-04, -3.3975e-06,\n",
            "        -2.7156e-04, -1.0000e+00, -1.9348e-01, -1.0000e+00, -8.5235e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03332885354757309 0.0005698204040527344 0.7361682057380676 0.33730846643447876 tensor(80, device='cuda:0')\n",
            "pred tensor([-1.1593e-04, -6.3181e-06, -3.1352e-05, -1.0550e-05, -8.4043e-06,\n",
            "        -8.6486e-05, -6.4552e-05, -9.8765e-05, -2.3508e-04, -1.3113e-06,\n",
            "        -1.9610e-05, -9.9182e-05, -8.5473e-05, -1.3185e-04, -2.8729e-05,\n",
            "        -1.6344e-04, -5.0068e-06, -1.3113e-05, -1.1730e-04, -7.5698e-06,\n",
            "        -4.3511e-06, -3.3927e-04, -2.3544e-05, -1.4007e-05, -3.9279e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03392943739891052 0.0006308555603027344 0.723933219909668 0.34203508496284485 tensor(81, device='cuda:0')\n",
            "pred tensor([-2.1315e-04, -2.9016e-04, -8.6427e-06, -3.9506e-04, -5.1439e-05,\n",
            "        -4.7684e-07, -2.1827e-04, -2.7418e-06, -2.1231e-04, -3.3069e-04,\n",
            "        -5.3596e-04, -3.7909e-05, -3.5763e-06, -1.6665e-04, -2.6047e-05,\n",
            "        -3.6354e-03, -1.6570e-05, -1.1997e-03, -3.1710e-05, -1.3292e-04,\n",
            "        -1.0931e-04, -2.1398e-04, -4.1485e-05, -3.4392e-05, -7.9334e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03791482001543045 0.00023317337036132812 0.7438192367553711 0.3417430520057678 tensor(98, device='cuda:0')\n",
            "59\n",
            "pred tensor([-8.2552e-05, -5.8055e-05, -1.0691e-03, -2.2471e-05, -6.5517e-04,\n",
            "        -3.6068e-03, -2.9898e-04, -5.4741e-04, -1.2481e-04, -1.0431e-01,\n",
            "        -2.3270e-03, -4.0932e-03, -4.0802e-02, -4.0894e-01, -2.3694e-01,\n",
            "        -1.7176e-03, -2.4147e-03, -1.4365e-04, -9.3985e-04, -2.1706e-03,\n",
            "        -2.4109e-03, -1.3800e-03, -1.6153e-04, -5.3520e-03, -9.8975e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03698691353201866 0.0005030632019042969 0.7079929113388062 0.34727349877357483 tensor(97, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -1.0000e+00, -5.1737e-04, -3.2377e-04,\n",
            "        -4.7016e-04, -9.8572e-03, -1.1104e-04, -3.4760e-02, -8.8120e-04,\n",
            "        -1.7464e-04, -5.6534e-03, -5.5075e-04, -4.1890e-04, -3.6144e-03,\n",
            "        -6.0043e-03, -1.8372e-02, -1.4896e-03, -1.9417e-03, -2.0618e-03,\n",
            "        -8.0469e-01, -1.0000e+00, -1.7853e-03, -4.9067e-04, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0403427891433239 0.0007190704345703125 0.7112514972686768 0.3458927273750305 tensor(114, device='cuda:0')\n",
            "pred tensor([-2.0337e-04, -1.6224e-04, -2.3186e-05, -8.0943e-05, -8.5068e-04,\n",
            "        -4.0932e-03, -1.9264e-04, -5.2035e-05, -8.0585e-05, -1.0848e-05,\n",
            "        -1.3030e-04, -8.1658e-06, -1.8668e-04, -1.3590e-05, -7.4100e-04,\n",
            "        -1.3399e-04, -2.0266e-04, -2.9297e-03, -3.9756e-05, -1.4191e-03,\n",
            "        -9.9756e-01, -9.8535e-01, -1.0000e+00, -1.8579e-01, -5.6000e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.034370146691799164 0.0006184577941894531 0.7068727016448975 0.3466205596923828 tensor(97, device='cuda:0')\n",
            "pred tensor([-7.1220e-03, -1.1139e-03, -3.1710e-05, -2.6226e-06, -9.7752e-06,\n",
            "        -2.4974e-05, -2.5392e-05, -4.3392e-04, -1.1559e-03, -3.7122e-04,\n",
            "        -1.1325e-06, -2.2781e-04, -5.1439e-05, -5.7638e-05, -3.7789e-05,\n",
            "        -1.9789e-04, -7.8738e-05, -1.2577e-05, -5.9319e-04, -1.8954e-05,\n",
            "        -3.1137e-04, -3.0880e-03, -8.3447e-07, -8.2850e-05, -3.7136e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03712812066078186 0.000835418701171875 0.7351144552230835 0.3627614974975586 tensor(118, device='cuda:0')\n",
            "pred tensor([-6.1768e-01, -1.0000e+00, -1.0000e+00, -3.1948e-05, -2.3842e-07,\n",
            "        -7.0930e-06, -2.0266e-06, -2.1458e-06, -2.7418e-05, -7.2002e-05,\n",
            "        -5.0843e-05, -1.8606e-03, -1.0848e-04, -3.7193e-05, -1.2846e-03,\n",
            "        -7.9691e-05, -2.6703e-05, -3.7789e-05, -1.3709e-05, -1.8299e-05,\n",
            "        -2.9945e-04, -9.3579e-06, -4.4346e-05, -1.6665e-04, -8.3506e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.033091697841882706 0.0002446174621582031 0.7747712731361389 0.33545681834220886 tensor(72, device='cuda:0')\n",
            "pred tensor([-3.6764e-04, -5.2273e-05, -8.0943e-05, -8.9407e-07, -4.5776e-05,\n",
            "        -1.7941e-05, -2.2650e-05, -1.9407e-04, -3.1710e-05, -2.1219e-05,\n",
            "        -3.1757e-04, -1.4305e-05, -5.1856e-06, -1.1325e-06, -3.0458e-05,\n",
            "        -3.6359e-06, -1.1545e-04, -2.4438e-06, -7.7486e-07, -2.0206e-05,\n",
            "        -1.4544e-05, -2.0266e-06, -1.5676e-05, -3.7551e-06, -2.2602e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03217566758394241 0.0004868507385253906 0.7301279902458191 0.33952492475509644 tensor(78, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -9.7156e-06, -1.6510e-05, -5.0068e-05, -1.4365e-04,\n",
            "        -6.7353e-06, -6.2656e-04, -1.8179e-05, -2.4676e-05, -5.7220e-06,\n",
            "        -3.6359e-06, -1.4961e-05, -2.9087e-05, -9.9658e-01, -1.0000e+00,\n",
            "        -1.4582e-03, -7.8278e-03, -4.0932e-03, -6.1111e-03, -2.6131e-04,\n",
            "        -1.9252e-05, -2.1534e-03, -2.1994e-04, -1.9119e-02, -3.4189e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03720247372984886 0.00016927719116210938 0.7854328155517578 0.3479125499725342 tensor(93, device='cuda:0')\n",
            "pred tensor([-0.0026, -0.0020, -0.0003, -0.0017, -0.0040, -0.0007, -0.0069, -0.0039,\n",
            "        -0.0017, -0.0066, -0.0065, -0.0014, -0.0009, -0.0865, -0.0008, -0.0003,\n",
            "        -0.0003, -0.0003, -0.0040, -0.0002, -0.0001, -0.0004, -0.0012, -0.0002,\n",
            "        -0.0002], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03306343033909798 0.00104522705078125 0.7003024220466614 0.3497734069824219 tensor(75, device='cuda:0')\n",
            "pred tensor([-3.5934e-03, -9.8389e-01, -1.4636e-01, -9.1457e-04, -1.9491e-04,\n",
            "        -2.9325e-05, -7.5698e-06, -2.7275e-04, -2.4438e-06, -2.9469e-04,\n",
            "        -5.4836e-06, -1.4305e-06, -8.6129e-05, -3.8087e-05, -1.8635e-03,\n",
            "        -8.3008e-03, -9.0742e-04, -5.2834e-04, -5.1260e-06, -1.1504e-05,\n",
            "        -1.0133e-06, -3.6776e-05, -9.6858e-05, -1.8668e-04, -1.3094e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03077498823404312 0.0008234977722167969 0.7125474214553833 0.33192428946495056 tensor(60, device='cuda:0')\n",
            "pred tensor([-3.1590e-06, -7.8678e-06, -7.1526e-06, -6.6340e-05, -2.2650e-05,\n",
            "        -7.1168e-05, -4.1723e-06, -1.7881e-07, -1.7285e-05, -1.6689e-06,\n",
            "        -2.6306e-02, -1.0000e+00, -9.9951e-01, -1.0000e+00, -2.8610e-05,\n",
            "        -1.7703e-05, -2.9683e-05, -2.9802e-06, -3.0398e-06, -1.4842e-05,\n",
            "        -4.0233e-05, -2.9206e-06, -4.0710e-05, -5.7220e-06, -4.6492e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03529180958867073 0.00043773651123046875 0.7307519912719727 0.33328866958618164 tensor(61, device='cuda:0')\n",
            "pred tensor([-1.6797e-04, -2.5094e-05, -2.5215e-03, -9.8096e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -7.4267e-05, -5.6267e-05, -4.1723e-07,\n",
            "        -1.2457e-05, -4.9472e-05, -3.0065e-04, -2.4819e-04, -6.0797e-06,\n",
            "        -1.0133e-06, -3.1471e-05, -4.6492e-05, -2.3305e-05, -6.1393e-06,\n",
            "        -1.1902e-03, -6.3181e-06, -8.2850e-05, -3.7136e-03, -6.1989e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03594982624053955 0.0009541511535644531 0.6874473094940186 0.3622336685657501 tensor(72, device='cuda:0')\n",
            "pred tensor([-1.0550e-05, -2.9564e-05, -3.2961e-05, -1.3399e-04, -2.0504e-03,\n",
            "        -2.0444e-05, -3.4809e-05, -6.9201e-05, -1.5783e-04, -2.0266e-04,\n",
            "        -1.6987e-05, -6.2048e-05, -1.2338e-04, -5.3465e-05, -1.2684e-04,\n",
            "        -6.1095e-05, -2.0111e-04, -1.5378e-05, -3.0458e-05, -5.4741e-04,\n",
            "        -4.5240e-05, -5.2035e-05, -2.3723e-05, -2.2340e-04, -5.8532e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031645480543375015 0.0008473396301269531 0.7127159833908081 0.34295427799224854 tensor(77, device='cuda:0')\n",
            "pred tensor([-1.9133e-05, -4.5300e-06, -2.5988e-05, -3.3283e-04, -9.8765e-05,\n",
            "        -3.4523e-04, -6.8521e-04, -2.7537e-05, -4.9472e-06, -1.3471e-05,\n",
            "        -1.8477e-05, -1.2338e-04, -1.9336e-04, -7.4863e-05, -1.9133e-05,\n",
            "        -1.3769e-04, -7.4244e-04, -3.4153e-05, -2.2113e-05, -7.4565e-05,\n",
            "        -2.0504e-03, -6.5231e-04, -3.1877e-04, -1.1915e-04, -8.5831e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02649725414812565 0.001216888427734375 0.7539127469062805 0.3502153158187866 tensor(52, device='cuda:0')\n",
            "pred tensor([-1.4484e-04, -2.3663e-05, -1.1593e-04, -2.1911e-04, -1.9407e-04,\n",
            "        -1.4091e-04, -5.2691e-05, -2.3484e-05, -3.2306e-05, -9.6619e-02,\n",
            "        -5.0116e-04, -3.7727e-03, -5.3048e-06, -5.2273e-05, -1.3561e-03,\n",
            "        -8.6844e-05, -2.4068e-04, -6.6805e-04, -1.9038e-04, -4.3058e-04,\n",
            "        -4.2498e-05, -8.0943e-05, -2.0828e-03, -1.2195e-04, -2.0337e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027878694236278534 0.0005087852478027344 0.7753236293792725 0.3519482910633087 tensor(71, device='cuda:0')\n",
            "pred tensor([-3.4881e-04, -1.2255e-03, -1.0281e-03, -4.9934e-03, -1.7536e-04,\n",
            "        -6.0368e-04, -1.1104e-04, -1.5903e-04, -4.9639e-04, -2.8348e-04,\n",
            "        -2.0714e-03, -1.6260e-03, -4.3654e-04, -9.7275e-03, -1.7204e-03,\n",
            "        -3.0899e-04, -1.2529e-04, -3.3569e-03, -7.2327e-03, -9.8779e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -2.5034e-06, -3.2043e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02557986043393612 0.0013275146484375 0.7601686120033264 0.35030946135520935 tensor(48, device='cuda:0')\n",
            "pred tensor([-4.1485e-04, -2.4815e-03, -4.1413e-04, -6.6280e-04, -3.6359e-05,\n",
            "        -5.6624e-06, -9.9540e-06, -1.1325e-04, -1.3185e-04, -8.1100e-03,\n",
            "        -2.1148e-04, -7.0143e-04, -2.2876e-04, -2.9206e-06, -1.5497e-06,\n",
            "        -3.9279e-05, -2.3842e-07, -2.7120e-05, -9.4175e-06, -1.1802e-05,\n",
            "        -6.2305e-01, -9.9268e-01, -1.0000e+00, -1.2517e-06, -6.7329e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030790168792009354 0.00027370452880859375 0.7905948162078857 0.339300274848938 tensor(72, device='cuda:0')\n",
            "pred tensor([-1.6422e-03, -5.8842e-04, -8.7595e-04, -5.7268e-04, -3.6478e-05,\n",
            "        -5.6877e-03, -1.8143e-02, -4.0703e-03, -3.3203e-02, -4.0841e-04,\n",
            "        -1.9789e-04, -2.0889e-02, -8.5402e-04, -1.6975e-03, -1.6870e-03,\n",
            "        -1.2032e-02, -4.6349e-03, -2.0182e-04, -3.5143e-04, -1.2140e-03,\n",
            "        -9.2554e-04, -8.7128e-03, -2.4338e-03, -4.0841e-04, -1.7824e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0265816580504179 0.000461578369140625 0.7848636507987976 0.34202268719673157 tensor(45, device='cuda:0')\n",
            "pred tensor([-6.1321e-04, -1.3199e-02, -2.7313e-03, -2.5902e-03, -6.0501e-03,\n",
            "        -6.8963e-05, -1.0147e-03, -1.3666e-03, -7.6294e-04, -1.6193e-03,\n",
            "        -2.3327e-03, -1.5015e-02, -2.4724e-04, -2.3193e-03, -1.5249e-03,\n",
            "        -1.6136e-03, -1.0204e-03, -5.2643e-04, -6.7711e-04, -5.3978e-04,\n",
            "        -8.2970e-04, -9.5010e-05, -2.6703e-05, -8.9188e-03, -1.2941e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026314249262213707 0.001926422119140625 0.6776626110076904 0.3381699025630951 tensor(57, device='cuda:0')\n",
            "pred tensor([-1.3332e-03, -4.1246e-04, -3.4153e-05, -1.0473e-04, -1.4544e-04,\n",
            "        -1.9312e-05, -6.7949e-06, -4.5562e-04, -1.9908e-05, -1.0192e-04,\n",
            "        -3.6359e-05, -5.3644e-06, -1.3947e-05, -4.5598e-05, -1.1921e-07,\n",
            "        -1.8477e-06, -7.9334e-05,  0.0000e+00, -6.3721e-01, -4.0674e-04,\n",
            "        -9.9951e-01, -1.0000e+00, -8.3804e-05, -2.6846e-04, -2.0752e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02887185662984848 0.0011348724365234375 0.7072645425796509 0.35226675868034363 tensor(71, device='cuda:0')\n",
            "pred tensor([-4.7569e-03, -3.5906e-04, -1.4484e-04, -1.9722e-03, -4.8876e-04,\n",
            "        -1.1772e-04, -5.8270e-04, -2.0337e-04, -2.1636e-05, -6.2656e-04,\n",
            "        -7.1239e-04, -2.1636e-05, -3.6895e-05, -7.8440e-05, -2.9707e-04,\n",
            "        -2.4140e-05, -1.6584e-03, -1.2481e-04, -3.0184e-04, -6.8235e-04,\n",
            "        -1.7881e-06, -5.5432e-05, -6.7353e-06, -1.7130e-04, -1.0691e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027379387989640236 0.0014810562133789062 0.7183222770690918 0.353122353553772 tensor(77, device='cuda:0')\n",
            "pred tensor([-1.9717e-04, -1.2636e-04, -3.3539e-02, -1.0000e+00, -1.0000e+00,\n",
            "        -2.7704e-04, -2.1496e-03, -2.1286e-02, -5.5885e-03, -4.4584e-04,\n",
            "        -1.7333e-04, -2.7061e-04, -1.7607e-04, -9.8133e-04, -4.0531e-04,\n",
            "        -3.4523e-04, -1.8930e-03, -8.5402e-04, -4.5133e-04, -6.7978e-03,\n",
            "        -2.2644e-01, -7.8516e-01, -1.6260e-01, -1.7737e-01, -2.8348e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030698908492922783 0.00011682510375976562 0.8220943808555603 0.3425878584384918 tensor(92, device='cuda:0')\n",
            "pred tensor([-4.8304e-04, -3.8395e-03, -1.2627e-02, -3.3081e-05, -1.7464e-05,\n",
            "        -4.3333e-05, -3.2425e-05, -4.4346e-05, -2.5821e-04, -4.4464e-02,\n",
            "        -8.4782e-04, -2.7657e-05, -4.0233e-05, -1.1063e-04, -4.6825e-04,\n",
            "        -3.1686e-04, -1.9968e-05, -4.3297e-04, -1.4702e-02, -3.8981e-04,\n",
            "        -3.2687e-04, -1.7333e-04, -1.3661e-04, -8.3506e-05, -2.7061e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027660025283694267 0.0001964569091796875 0.75920569896698 0.33531302213668823 tensor(63, device='cuda:0')\n",
            "pred tensor([-1.8966e-04, -8.4114e-04, -1.1950e-03, -9.6741e-03, -2.9182e-03,\n",
            "        -5.8670e-03, -1.1635e-04, -1.4484e-04, -2.6011e-04, -1.9491e-04,\n",
            "        -1.4668e-03, -8.8871e-05, -6.0618e-05, -4.6272e-03, -4.1318e-04,\n",
            "        -1.1146e-04, -6.1333e-05, -1.2646e-03, -7.0953e-04, -8.9407e-06,\n",
            "        -4.2152e-03, -6.8626e-03, -4.5471e-03, -3.9363e-04, -4.6372e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029569333419203758 0.00018405914306640625 0.7387186884880066 0.33894404768943787 tensor(77, device='cuda:0')\n",
            "pred tensor([-5.7817e-05, -2.7418e-05, -2.7370e-04, -1.1053e-03, -4.9353e-04,\n",
            "        -7.0572e-05, -7.7248e-05, -3.0853e-02, -1.5011e-03, -2.1911e-04,\n",
            "        -7.6008e-04, -3.5429e-04, -6.6817e-05, -5.0068e-05, -1.3876e-04,\n",
            "        -8.9493e-03, -3.7694e-04, -4.2152e-04, -2.2163e-03, -7.5996e-05,\n",
            "        -5.2869e-05, -8.1241e-05, -8.6784e-04, -3.5000e-04, -2.3127e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02893076278269291 0.00022125244140625 0.7188401222229004 0.33373335003852844 tensor(52, device='cuda:0')\n",
            "pred tensor([-5.0664e-06, -3.9935e-06, -1.4424e-05, -1.1921e-07,  0.0000e+00,\n",
            "        -1.1921e-07, -1.7881e-07, -5.9605e-08, -7.1526e-07, -2.9206e-05,\n",
            "        -2.0182e-04, -8.8745e-02, -9.3555e-01, -1.8884e-01, -8.9407e-07,\n",
            "        -8.4782e-04, -6.0201e-06, -5.0843e-05, -2.1231e-04, -2.1994e-04,\n",
            "        -4.1779e-02, -1.2147e-04, -6.7592e-05, -9.8765e-05, -1.0723e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02888636849820614 0.0005412101745605469 0.7234504222869873 0.3595607280731201 tensor(67, device='cuda:0')\n",
            "pred tensor([-2.5225e-04, -4.4155e-04, -1.2169e-02, -9.9512e-01, -9.9902e-01,\n",
            "        -9.2041e-01, -5.8289e-02, -1.8096e-04, -1.5056e-04, -5.4121e-05,\n",
            "        -6.1810e-05, -4.7374e-04, -7.3910e-06, -1.6510e-05, -8.4180e-01,\n",
            "        -6.4514e-02, -1.1034e-03, -4.3755e-03, -1.1734e-02, -3.6478e-05,\n",
            "        -3.6120e-04, -3.9612e-02, -6.0141e-05, -1.7810e-04, -9.0218e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029785970225930214 0.00104522705078125 0.7175278663635254 0.36772099137306213 tensor(96, device='cuda:0')\n",
            "pred tensor([-1.1456e-04, -1.8001e-05, -6.4552e-05, -1.6868e-04, -1.8525e-04,\n",
            "        -2.6464e-05, -3.5167e-06, -2.8014e-06, -6.5136e-04, -1.5175e-04,\n",
            "        -2.2471e-05, -1.7881e-06, -5.5850e-05, -1.5140e-05, -2.7955e-05,\n",
            "        -1.3185e-04, -4.6468e-04, -8.1897e-05, -1.0192e-04, -1.7524e-05,\n",
            "        -4.4703e-05, -8.1062e-06, -6.7890e-05, -2.6536e-04, -6.1572e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02716355212032795 0.0014019012451171875 0.7078387141227722 0.3363742232322693 tensor(61, device='cuda:0')\n",
            "pred tensor([-1.0147e-03, -7.5161e-05, -2.5010e-04, -5.7268e-04, -1.6224e-04,\n",
            "        -1.7464e-04, -1.2934e-04, -7.5531e-04, -3.3283e-04, -2.7585e-04,\n",
            "        -3.3617e-05, -1.5974e-05, -5.0664e-06, -4.1306e-05, -4.1246e-04,\n",
            "        -4.6849e-05, -2.8193e-05, -8.2855e-03, -2.4438e-06, -3.2306e-05,\n",
            "        -1.1803e-02, -4.3988e-05, -8.5831e-06, -1.5488e-03, -2.5034e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02986086718738079 0.0015430450439453125 0.7069790363311768 0.3403739333152771 tensor(88, device='cuda:0')\n",
            "pred tensor([-2.6569e-03, -1.1063e-04, -5.6934e-04, -5.6362e-04, -5.9128e-03,\n",
            "        -4.5753e-04, -5.7364e-04, -2.1782e-03, -2.5654e-03, -9.1457e-04,\n",
            "        -6.2523e-03, -1.0073e-04, -1.6356e-03, -1.5869e-02, -4.8876e-05,\n",
            "        -4.0550e-03, -1.3428e-03, -6.2895e-04, -6.0940e-04, -9.1629e-03,\n",
            "        -3.5305e-03, -1.2573e-01, -9.9951e-01, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03033408150076866 8.58306884765625e-05 0.7722005844116211 0.34428438544273376 tensor(95, device='cuda:0')\n",
            "pred tensor([-2.1534e-03, -5.0888e-03, -1.4544e-05, -1.6224e-04, -6.8808e-04,\n",
            "        -1.2195e-04, -9.8348e-06, -7.2002e-05, -1.9717e-04, -1.3590e-05,\n",
            "        -1.8454e-04, -1.5459e-03, -4.5240e-05, -3.5477e-04, -4.9889e-05,\n",
            "        -1.9014e-05, -9.1457e-04, -1.3794e-01, -3.6359e-05, -1.1005e-03,\n",
            "        -7.0667e-04, -1.6093e-06, -6.9737e-06, -5.0664e-05, -1.9336e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029658354818820953 8.106231689453125e-06 0.817969024181366 0.34823715686798096 tensor(103, device='cuda:0')\n",
            "pred tensor([-1.0353e-04, -4.7684e-06, -2.7120e-05, -6.6042e-05, -1.9913e-03,\n",
            "        -4.3058e-04, -1.7607e-04, -1.8034e-03, -4.5419e-05, -2.4529e-03,\n",
            "        -7.6950e-05, -8.8013e-02, -1.0000e+00, -1.0000e+00, -8.8215e-05,\n",
            "        -3.4571e-06, -2.4629e-04, -4.1580e-04, -4.6492e-05, -1.5676e-05,\n",
            "        -3.4928e-05, -1.9670e-06, -7.7486e-07, -9.3079e-04, -1.1406e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028923777863383293 3.2901763916015625e-05 0.7807586193084717 0.3403116762638092 tensor(77, device='cuda:0')\n",
            "pred tensor([-9.3408e-01, -3.5571e-01, -2.5415e-04, -9.3079e-04, -4.7851e-04,\n",
            "        -1.9133e-05, -2.7418e-05, -1.2052e-04, -1.6689e-06, -1.6689e-06,\n",
            "        -2.4140e-05, -9.1732e-05, -3.0994e-06, -6.9141e-06, -5.9605e-07,\n",
            "        -4.1008e-05, -1.2517e-06, -7.7546e-05, -1.7822e-05, -5.9605e-08,\n",
            "        -1.4305e-06, -9.9540e-06, -1.7881e-07, -1.2517e-06, -5.8532e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031007803976535797 6.0558319091796875e-05 0.7704507112503052 0.3443310856819153 tensor(104, device='cuda:0')\n",
            "pred tensor([-4.1504e-03, -1.4770e-04, -2.5513e-02, -7.4530e-04, -7.9036e-05,\n",
            "        -4.2305e-03, -4.7646e-03, -8.6927e-04, -4.5824e-04, -1.6797e-04,\n",
            "        -2.6822e-06, -5.2214e-04, -1.3800e-03, -5.5420e-02, -2.3035e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.2770e-03, -4.6539e-03,\n",
            "        -2.5073e-01, -1.6724e-02, -4.2496e-03, -3.7012e-01, -4.4441e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02955501899123192 0.0009465217590332031 0.6928998231887817 0.34682995080947876 tensor(88, device='cuda:0')\n",
            "pred tensor([-0.1956, -0.1019, -0.0136, -0.0403, -0.0104, -0.1575, -0.0090, -0.0025,\n",
            "        -0.0083, -0.0133, -0.0065, -0.0070, -0.0014, -0.0586, -0.3147, -0.9810,\n",
            "        -0.3772, -0.2104, -0.7905, -0.8071, -0.6777, -0.9927, -0.9814, -0.9419,\n",
            "        -1.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029062718152999878 0.0014629364013671875 0.6614218950271606 0.3400856852531433 tensor(81, device='cuda:0')\n",
            "pred tensor([-4.2391e-04, -2.5916e-04, -4.0474e-03, -5.1212e-04, -3.3081e-05,\n",
            "        -6.4015e-05, -1.9407e-04, -6.1572e-05, -6.3539e-05, -2.6321e-04,\n",
            "        -8.2016e-04, -2.8610e-06, -5.9605e-07, -8.8310e-04, -6.5384e-03,\n",
            "        -9.5508e-01, -1.0000e+00, -2.2602e-04, -6.8128e-05, -3.9291e-04,\n",
            "        -2.8193e-05, -8.5831e-05, -8.5831e-06, -2.4724e-04, -2.4147e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029151909053325653 0.001903533935546875 0.6648827791213989 0.3393501937389374 tensor(86, device='cuda:0')\n",
            "pred tensor([-1.8349e-03, -6.8665e-04, -2.1660e-04, -1.5364e-03, -3.3716e-01,\n",
            "        -6.5136e-04, -2.6321e-03, -9.5444e-03, -2.9469e-04, -9.3162e-05,\n",
            "        -6.5660e-04, -7.6294e-05, -4.3130e-04, -1.3661e-04, -1.2999e-03,\n",
            "        -3.9291e-04, -4.8676e-02, -5.2547e-04, -1.2052e-04, -4.3564e-03,\n",
            "        -6.7825e-03, -5.5170e-04, -4.5955e-05, -7.8678e-06, -1.1730e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028583532199263573 0.0011386871337890625 0.6901559233665466 0.35441353917121887 tensor(74, device='cuda:0')\n",
            "pred tensor([-1.5414e-04, -2.3139e-04, -1.0848e-04, -8.9586e-05, -2.6584e-05,\n",
            "        -1.8311e-04, -1.4007e-05, -5.4240e-06, -1.3769e-05, -2.5725e-04,\n",
            "        -8.3113e-04, -1.2146e-02, -3.7003e-03, -1.6451e-05, -8.7857e-05,\n",
            "        -9.9540e-06, -7.4506e-06, -3.4261e-04, -5.2869e-05, -2.2054e-06,\n",
            "        -1.4305e-06, -1.4544e-04, -2.5787e-02, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030404912307858467 0.00027751922607421875 0.7137203812599182 0.3453178107738495 tensor(90, device='cuda:0')\n",
            "pred tensor([-4.1723e-07, -1.8477e-06, -1.0133e-06, -2.9802e-07, -9.2983e-06,\n",
            "        -4.2319e-06, -1.3173e-05, -2.9802e-07, -2.3842e-07, -1.7881e-06,\n",
            "        -3.3379e-06, -1.4937e-04, -7.3671e-05, -3.9935e-06, -8.9407e-07,\n",
            "        -6.7353e-06, -1.1921e-07, -5.4836e-06, -5.2273e-05, -1.3292e-05,\n",
            "        -1.1325e-05, -5.9605e-08, -1.7881e-07, -2.3842e-07, -4.2915e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031614359468221664 0.00011301040649414062 0.7485031485557556 0.3632853031158447 tensor(108, device='cuda:0')\n",
            "60\n",
            "pred tensor([-1.0133e-06, -7.1704e-05, -1.4544e-05, -7.9274e-06, -3.7336e-04,\n",
            "        -2.0337e-04, -7.1704e-05, -2.1875e-05, -1.0192e-05, -1.7285e-06,\n",
            "        -6.3300e-05, -1.0000e+00, -1.0000e+00, -5.7399e-05, -1.1683e-05,\n",
            "        -4.7684e-06, -1.8883e-04, -2.2256e-04, -8.9228e-05, -4.2796e-04,\n",
            "        -1.5175e-04, -4.3511e-06, -1.3471e-05, -2.0111e-04, -2.7490e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03321214020252228 9.632110595703125e-05 0.7395691871643066 0.33740234375 tensor(83, device='cuda:0')\n",
            "pred tensor([-7.0286e-04, -3.0518e-03, -1.7443e-03, -1.6034e-04, -4.6134e-05,\n",
            "        -8.7595e-04, -1.0806e-04, -4.3130e-04, -2.2078e-04, -7.5459e-05,\n",
            "        -1.6606e-04, -5.8365e-04, -6.8512e-03, -5.7068e-03, -2.7409e-03,\n",
            "        -1.8349e-03, -6.1569e-03, -1.1593e-04, -1.0147e-03, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -3.1662e-03, -7.0953e-04, -3.3569e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02924410253763199 0.00016164779663085938 0.7546788454055786 0.3529295027256012 tensor(81, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -3.9506e-04, -8.0585e-05, -1.1772e-04,\n",
            "        -4.2975e-05, -1.0848e-05, -4.1008e-05, -4.7112e-04, -3.9124e-04,\n",
            "        -2.6727e-04, -2.1279e-05, -7.0000e-04, -9.6512e-03, -9.8705e-04,\n",
            "        -7.6592e-05, -4.1485e-05, -6.5565e-07, -6.3777e-06, -2.8729e-05,\n",
            "        -4.7386e-05, -4.5395e-04, -1.2398e-03, -1.7929e-03, -6.6943e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029702186584472656 0.0018663406372070312 0.6675722599029541 0.35285961627960205 tensor(96, device='cuda:0')\n",
            "pred tensor([-8.4162e-05, -1.3661e-04, -2.7156e-04, -3.0339e-05, -1.8740e-04,\n",
            "        -4.1318e-04, -3.9744e-04, -4.7088e-06, -2.2054e-06, -2.3842e-06,\n",
            "        -4.7684e-07, -6.9499e-05, -1.0133e-06, -1.3447e-04, -7.6294e-05,\n",
            "        -8.7857e-05, -2.1040e-05, -4.5466e-04, -3.4027e-03, -9.9854e-01,\n",
            "        -9.9121e-01, -2.0027e-04, -8.2850e-06, -7.2837e-05, -4.8339e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029602045193314552 0.0005011558532714844 0.7168547511100769 0.3486413359642029 tensor(81, device='cuda:0')\n",
            "pred tensor([-3.0688e-01, -1.0000e+00, -1.0000e+00, -8.0185e-03, -3.1257e-04,\n",
            "        -8.9169e-04, -2.0542e-03, -7.7400e-03, -7.7009e-04, -1.0777e-03,\n",
            "        -7.0286e-04, -5.0735e-04, -1.2922e-03, -4.0829e-05, -5.2691e-05,\n",
            "        -8.3148e-05, -3.0184e-04, -1.0815e-03, -4.3640e-03, -8.9169e-04,\n",
            "        -2.8133e-04, -3.9983e-04, -6.4135e-04, -2.7523e-03, -5.7936e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030476553365588188 0.0007905960083007812 0.6835982799530029 0.3406979739665985 tensor(81, device='cuda:0')\n",
            "pred tensor([-9.4775e-01, -3.7231e-02, -2.9707e-04, -1.3456e-03, -7.7200e-04,\n",
            "        -2.0752e-03, -8.8453e-04, -8.5115e-05, -1.1683e-05, -5.0068e-05,\n",
            "        -1.6069e-03, -5.9357e-03, -4.2295e-04, -4.7207e-05, -1.2100e-05,\n",
            "        -1.7285e-05, -7.9870e-06, -4.2915e-06, -1.8179e-05, -5.0068e-06,\n",
            "        -3.2187e-06, -2.4002e-02, -1.6451e-03, -1.0948e-03, -1.0681e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02956361696124077 0.0008249282836914062 0.6634901762008667 0.33935171365737915 tensor(77, device='cuda:0')\n",
            "pred tensor([-5.8174e-04, -8.9943e-05, -3.0823e-03, -3.2158e-03, -2.4567e-02,\n",
            "        -2.2221e-03, -1.8250e-02, -1.6434e-02, -5.5542e-03, -1.2722e-03,\n",
            "        -7.9036e-05, -4.1890e-04, -1.5974e-03, -2.0542e-03, -6.5804e-05,\n",
            "        -3.3665e-04, -2.0027e-04, -6.1333e-05, -4.7469e-04, -1.2887e-04,\n",
            "        -3.9935e-05, -1.7703e-05, -2.9016e-04, -4.9651e-05, -1.8525e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03429960086941719 0.00030422210693359375 0.7361781597137451 0.33538803458213806 tensor(80, device='cuda:0')\n",
            "pred tensor([-1.3030e-04, -1.8177e-03, -9.2480e-01, -1.1902e-03, -7.8535e-04,\n",
            "        -1.5843e-04, -8.0943e-05, -1.7810e-04, -2.1040e-05, -9.3460e-04,\n",
            "        -2.6166e-05, -7.3373e-05, -2.7103e-03, -4.3297e-04, -5.1117e-04,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.4198e-04, -5.8055e-05, -4.8161e-05, -1.6451e-05, -1.1802e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029399055987596512 7.486343383789062e-05 0.7390692830085754 0.3543802797794342 tensor(59, device='cuda:0')\n",
            "pred tensor([-1.4651e-04, -3.3319e-05, -2.9826e-04, -7.3528e-04, -7.8869e-04,\n",
            "        -1.2481e-04, -1.0777e-03, -2.2650e-06, -1.8597e-04, -6.3181e-06,\n",
            "        -3.5167e-06, -1.0133e-05, -6.6042e-05, -2.0504e-04, -2.5570e-05,\n",
            "        -5.6922e-05, -2.6011e-04, -2.0385e-05, -4.3511e-06, -1.3947e-05,\n",
            "        -3.5214e-04, -3.2306e-05, -4.7183e-04, -1.6665e-04, -1.2481e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028986552730202675 0.0010862350463867188 0.6696330308914185 0.33404290676116943 tensor(63, device='cuda:0')\n",
            "pred tensor([-4.9114e-05, -7.6294e-06, -6.5565e-05, -2.7800e-04, -1.6987e-05,\n",
            "        -8.7142e-05, -1.7653e-03, -1.0681e-04, -3.2306e-05, -4.7743e-05,\n",
            "        -4.0531e-05, -1.8239e-04, -1.7202e-04, -5.0843e-05, -7.8738e-05,\n",
            "        -1.1367e-04, -2.7370e-04, -1.9722e-03, -2.1482e-04, -3.9458e-05,\n",
            "        -9.5010e-05, -3.8330e-01, -1.0000e+00, -1.0000e+00, -5.8746e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02659687027335167 0.0025959014892578125 0.642633318901062 0.3314616084098816 tensor(53, device='cuda:0')\n",
            "pred tensor([-6.1095e-05, -4.8590e-04, -1.9562e-04, -3.1257e-04, -2.2423e-04,\n",
            "        -5.1403e-04, -1.0633e-03, -1.6034e-04, -5.8532e-05, -2.1148e-04,\n",
            "        -2.6464e-05, -2.5415e-04, -1.4484e-04, -1.6034e-05, -1.5914e-05,\n",
            "        -9.0599e-06, -1.7583e-05, -7.2718e-06, -1.1444e-03, -1.5662e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -2.2054e-05, -1.9038e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02924344874918461 0.00023031234741210938 0.7275855541229248 0.3316914439201355 tensor(58, device='cuda:0')\n",
            "pred tensor([-1.3030e-04, -1.1504e-04, -3.4928e-04, -3.2187e-05, -1.3943e-03,\n",
            "        -7.5758e-05, -2.6822e-05, -8.9586e-05, -1.1146e-04, -4.0710e-05,\n",
            "        -1.0014e-05, -1.4150e-04, -1.2684e-04, -3.5858e-02, -9.7942e-04,\n",
            "        -2.9826e-04, -7.1239e-04, -3.3140e-04, -3.7074e-05, -6.1095e-05,\n",
            "        -9.9540e-05, -1.7333e-04, -4.2381e-03, -8.3923e-04, -4.3726e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025332221761345863 0.00019168853759765625 0.7724875807762146 0.3347211480140686 tensor(52, device='cuda:0')\n",
            "pred tensor([-5.5933e-04, -5.4550e-04, -4.9472e-05, -6.0201e-06, -1.4091e-04,\n",
            "        -4.1306e-05, -3.9279e-05, -1.3173e-05, -9.1267e-04, -2.8198e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -8.7976e-04, -6.4254e-05, -1.6034e-04,\n",
            "        -7.4565e-05, -9.3877e-05, -5.6267e-05, -2.2256e-04, -7.3528e-04,\n",
            "        -2.8563e-04, -5.3072e-04, -2.7156e-04, -6.3300e-05, -2.1040e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02701744996011257 0.0001163482666015625 0.7380639910697937 0.3301198482513428 tensor(44, device='cuda:0')\n",
            "pred tensor([-1.3151e-03, -8.8513e-05, -1.7853e-03, -2.1332e-02, -3.0575e-03,\n",
            "        -8.3923e-04, -1.2665e-03, -2.3878e-04, -5.6877e-03, -1.0312e-05,\n",
            "        -1.2994e-05, -5.1260e-06, -7.2539e-05, -4.9293e-05, -1.2338e-04,\n",
            "        -7.8440e-05, -5.6362e-04, -3.8981e-05, -3.9458e-05, -8.6844e-05,\n",
            "        -4.2295e-04, -3.0458e-05, -2.0564e-05, -5.5432e-05, -1.8282e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025150105357170105 6.961822509765625e-05 0.7234715223312378 0.3686154782772064 tensor(45, device='cuda:0')\n",
            "pred tensor([-2.2217e-02, -2.0844e-02, -9.8242e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -6.5565e-06, -8.0109e-04, -1.1146e-04, -1.7810e-04, -5.8293e-05,\n",
            "        -3.1877e-04, -2.7490e-04, -1.2481e-04, -1.4365e-04, -1.1265e-05,\n",
            "        -1.4889e-04, -8.4460e-05, -5.2452e-05, -5.3883e-05, -8.1241e-05,\n",
            "        -8.6844e-05, -3.3021e-04, -4.7755e-04, -1.3888e-05, -1.4997e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026467215269804 0.0005240440368652344 0.7126141786575317 0.3640933036804199 tensor(47, device='cuda:0')\n",
            "pred tensor([-1.2459e-02, -1.4162e-03, -5.1212e-04, -6.7186e-04, -1.1683e-05,\n",
            "        -6.9499e-05, -4.0054e-05, -6.3300e-05, -2.2113e-05, -8.9693e-04,\n",
            "        -4.6849e-05, -2.1148e-04, -2.1756e-05, -2.3127e-05, -5.4836e-06,\n",
            "        -5.1439e-05, -6.3777e-05, -9.6500e-05, -7.1526e-06, -5.8594e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.5855e-05, -5.8413e-06, -4.6015e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026279639452695847 0.0052337646484375 0.6416889429092407 0.334611713886261 tensor(58, device='cuda:0')\n",
            "pred tensor([-6.2037e-04, -3.1877e-04, -7.2837e-05, -1.6212e-05, -3.2187e-06,\n",
            "        -2.3544e-05, -6.0380e-05, -6.4373e-06, -4.3154e-05, -4.0233e-05,\n",
            "        -6.5136e-04, -6.2275e-04, -7.0667e-04, -8.2636e-04, -4.5061e-05,\n",
            "        -9.9540e-05, -1.3089e-04, -3.6144e-03, -4.9067e-04, -1.2741e-03,\n",
            "        -1.0269e-02, -3.9062e-02, -7.3291e-01, -7.9041e-02, -3.5524e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029095754027366638 0.00032806396484375 0.7800483703613281 0.3323093056678772 tensor(70, device='cuda:0')\n",
            "pred tensor([-1.3828e-05, -4.2152e-04, -7.4530e-04, -4.1187e-05, -4.8208e-04,\n",
            "        -7.6294e-05, -3.2449e-04, -2.4796e-05, -5.0068e-06, -3.7611e-05,\n",
            "        -1.0431e-04, -4.8208e-04, -3.5346e-05, -4.1962e-05, -7.2813e-04,\n",
            "        -1.2243e-04, -1.2887e-04, -1.7202e-04, -4.4680e-04, -1.1854e-03,\n",
            "        -2.1148e-04, -3.0339e-05, -2.1458e-06, -2.1660e-04, -1.4582e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02824210375547409 0.0001239776611328125 0.7802116870880127 0.3522258400917053 tensor(76, device='cuda:0')\n",
            "pred tensor([-5.2452e-05, -9.4175e-06, -1.2755e-05, -1.5295e-04, -5.0664e-05,\n",
            "        -6.3777e-05, -7.9274e-06, -9.7215e-05, -1.2016e-03, -4.7028e-05,\n",
            "        -3.1590e-06, -3.0398e-06, -1.5962e-04, -1.0967e-03, -5.5850e-05,\n",
            "        -5.3465e-05, -7.3338e-04, -5.5599e-04, -1.3876e-04, -1.0556e-04,\n",
            "        -5.3465e-05, -5.2035e-05, -2.4395e-03, -9.3877e-05, -2.5821e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029688404873013496 0.0003337860107421875 0.7117072939872742 0.34713128209114075 tensor(74, device='cuda:0')\n",
            "pred tensor([-1.2100e-04, -5.4121e-05, -3.3200e-05, -1.6999e-04, -7.7248e-05,\n",
            "        -5.2605e-03, -2.3878e-04, -1.4105e-03, -1.7607e-04, -6.0618e-05,\n",
            "        -8.0943e-05, -5.8055e-05, -1.0192e-05, -7.2384e-04, -8.9359e-04,\n",
            "        -2.0182e-04, -7.1168e-05, -7.8440e-05, -3.5286e-04, -3.3593e-04,\n",
            "        -5.0831e-04, -4.5815e-03, -4.7922e-05, -1.7576e-03, -1.5251e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02746587246656418 0.000579833984375 0.7133650779724121 0.3519037663936615 tensor(71, device='cuda:0')\n",
            "pred tensor([-2.0659e-04, -6.6662e-04, -5.6171e-04, -2.4915e-04, -3.1352e-05,\n",
            "        -2.8610e-06, -8.2031e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -2.6264e-03, -1.6093e-04, -1.0550e-05, -2.8729e-05,\n",
            "        -6.8128e-05, -3.4595e-04, -4.3631e-05, -6.1083e-04, -2.8172e-03,\n",
            "        -7.3373e-05, -6.1083e-04, -1.7464e-04, -1.0151e-04, -1.8139e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030378898605704308 0.00011682510375976562 0.7415932416915894 0.3353666365146637 tensor(79, device='cuda:0')\n",
            "pred tensor([-1.2350e-03, -1.4651e-04, -3.3808e-04, -1.3983e-04, -1.0948e-03,\n",
            "        -4.2498e-05, -2.4724e-04, -7.0333e-05, -3.5477e-04, -4.3058e-04,\n",
            "        -1.1104e-04, -5.1928e-04, -1.7929e-03, -7.2174e-03, -3.2715e-02,\n",
            "        -5.7220e-06, -2.0111e-04, -1.9872e-04, -2.0103e-03, -1.2817e-03,\n",
            "        -1.1162e-02, -3.3617e-05, -1.8096e-04, -1.0187e-01, -8.8672e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026692142710089684 0.001079559326171875 0.6756244897842407 0.3343583047389984 tensor(63, device='cuda:0')\n",
            "pred tensor([-1.4257e-04, -2.0027e-04, -1.0312e-04, -8.9359e-04, -9.6924e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -4.7946e-04, -2.8563e-04, -7.6151e-04,\n",
            "        -5.4836e-04, -5.4777e-05, -1.0633e-03, -1.0556e-04, -1.0300e-03,\n",
            "        -6.7186e-04, -6.1417e-04, -2.1148e-04, -1.1969e-03, -4.3654e-04,\n",
            "        -8.6486e-05, -9.1791e-06, -3.2783e-06, -3.1829e-05, -7.0143e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028425689786672592 0.00079345703125 0.6935124397277832 0.3340344727039337 tensor(61, device='cuda:0')\n",
            "pred tensor([-7.0143e-04, -8.4257e-04, -1.6534e-04, -7.6294e-06, -6.1083e-04,\n",
            "        -2.3317e-04, -1.8597e-04, -2.8920e-04, -3.2544e-05, -1.5533e-04,\n",
            "        -5.7793e-04, -9.5367e-05, -3.7694e-04, -1.2684e-04, -7.2241e-05,\n",
            "        -4.0352e-05, -8.2195e-05, -3.9434e-04, -2.9325e-05, -7.0333e-05,\n",
            "        -1.1456e-04, -1.9133e-05, -8.2195e-05, -7.2241e-05, -1.7333e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02700667828321457 0.0012226104736328125 0.6630523204803467 0.3372340500354767 tensor(73, device='cuda:0')\n",
            "pred tensor([-3.3021e-04, -9.9072e-01, -9.9854e-01, -7.3047e-01, -1.0000e+00,\n",
            "        -9.9658e-01, -1.0000e+00, -1.0000e+00, -1.7958e-03, -1.3409e-03,\n",
            "        -1.2732e-04, -1.3185e-04, -4.6849e-05, -7.5388e-04, -5.1117e-04,\n",
            "        -9.2447e-05, -1.2474e-03, -1.6708e-03, -3.6478e-04, -9.3162e-05,\n",
            "        -1.0353e-04, -3.3436e-03, -2.4967e-03, -9.2089e-05, -2.7704e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027274267747998238 0.0016183853149414062 0.6485058069229126 0.33819466829299927 tensor(82, device='cuda:0')\n",
            "pred tensor([-1.6797e-04, -3.6068e-03, -3.8395e-03, -6.2513e-04, -2.3594e-03,\n",
            "        -4.7374e-04, -8.1241e-05, -1.0639e-04, -1.9821e-02, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -8.7142e-05, -6.0141e-05, -6.4552e-05,\n",
            "        -1.9729e-05, -1.2815e-05, -7.1526e-06, -5.3704e-05, -1.0848e-04,\n",
            "        -1.2100e-04, -1.5140e-05, -1.0389e-04, -1.8883e-04, -6.3002e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02650568261742592 0.0004096031188964844 0.6978141069412231 0.3346478343009949 tensor(66, device='cuda:0')\n",
            "pred tensor([-2.8467e-04, -2.4974e-05, -2.3687e-04, -9.3985e-04, -1.0462e-03,\n",
            "        -1.3914e-03, -2.6047e-05, -6.0856e-05, -8.0287e-05, -6.7997e-04,\n",
            "        -9.2089e-05, -6.1560e-04, -2.0659e-04, -7.8142e-05, -1.0806e-04,\n",
            "        -7.9691e-05, -1.0073e-04, -2.0683e-05, -1.4150e-04, -5.8055e-05,\n",
            "        -2.5749e-05, -8.5115e-05, -3.6895e-05, -1.4937e-04, -2.3186e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026069266721606255 0.00010156631469726562 0.7194080352783203 0.3481065332889557 tensor(58, device='cuda:0')\n",
            "pred tensor([-1.6212e-05, -6.9499e-05, -2.9254e-04, -2.5415e-04, -4.4703e-06,\n",
            "        -1.0192e-05, -4.2677e-05, -6.5804e-05, -1.4782e-05, -3.7193e-04,\n",
            "        -3.6263e-04, -3.3379e-03, -5.3453e-04, -1.1384e-05, -6.8665e-05,\n",
            "        -2.8348e-04, -6.8665e-05, -3.2187e-05, -1.1325e-04, -7.4100e-04,\n",
            "        -8.5144e-03, -7.1068e-03, -4.7455e-03, -2.3055e-04, -1.2243e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027309998869895935 4.76837158203125e-07 0.7729231715202332 0.34168919920921326 tensor(71, device='cuda:0')\n",
            "pred tensor([-7.4863e-05, -1.1730e-04, -5.0926e-04, -7.7844e-05, -7.0953e-04,\n",
            "        -1.2934e-05, -1.0639e-04, -1.9729e-05, -4.4525e-05, -1.4305e-05,\n",
            "        -9.1791e-06, -3.2115e-04, -3.7727e-03, -3.0184e-04, -6.1572e-05,\n",
            "        -6.4135e-04, -1.2100e-04, -1.1194e-04, -1.5430e-03, -3.4766e-01,\n",
            "        -1.0674e-02, -1.8829e-02, -1.7273e-02, -3.1494e-01, -2.8369e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02846645936369896 0.0 0.7720048427581787 0.35086503624916077 tensor(59, device='cuda:0')\n",
            "pred tensor([-1.5364e-03, -6.3300e-05, -3.4511e-05, -7.8142e-05, -3.9368e-03,\n",
            "        -3.8361e-02, -2.4629e-04, -1.0371e-05, -6.5804e-05, -1.1921e-05,\n",
            "        -3.7909e-05, -4.2975e-05, -4.5738e-03, -2.0142e-03, -6.3848e-04,\n",
            "        -1.2529e-04, -2.0146e-05, -8.1897e-05, -5.8055e-05, -4.4703e-05,\n",
            "        -1.0080e-03, -9.1934e-03, -8.8120e-04, -1.5235e-04, -1.2522e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.023365935310721397 0.00024271011352539062 0.7181018590927124 0.33538463711738586 tensor(60, device='cuda:0')\n",
            "pred tensor([-1.3196e-01, -5.0831e-04, -5.4240e-06, -1.1867e-04, -1.7285e-01,\n",
            "        -9.7656e-01, -1.0000e+00, -1.0000e+00, -3.2482e-03, -1.0147e-03,\n",
            "        -6.0225e-04, -1.9872e-04, -5.5599e-04, -5.3358e-04, -3.7861e-04,\n",
            "        -4.5471e-03, -1.2445e-03, -6.2895e-04, -1.2052e-04, -1.4317e-04,\n",
            "        -3.3970e-03, -1.7405e-04, -1.8883e-04, -1.6584e-03, -1.1820e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02575184591114521 0.0037899017333984375 0.6423060297966003 0.33741438388824463 tensor(74, device='cuda:0')\n",
            "pred tensor([-2.7733e-03, -2.4300e-03, -2.2781e-04, -8.3466e-03, -1.9045e-03,\n",
            "        -6.3848e-04, -7.9632e-04, -5.8293e-05, -3.4571e-06, -4.2319e-05,\n",
            "        -2.4533e-04, -2.9707e-04, -1.2732e-04, -2.3003e-03, -1.7333e-04,\n",
            "        -4.4525e-05, -9.9945e-03, -1.5473e-04, -3.9339e-06, -1.8358e-05,\n",
            "        -3.8981e-05, -4.1962e-05, -6.1095e-05, -2.7490e-04, -5.4312e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02659265510737896 0.002323150634765625 0.6763801574707031 0.3436490595340729 tensor(68, device='cuda:0')\n",
            "pred tensor([-4.5824e-04, -1.9717e-04, -4.6670e-05, -1.4484e-04, -1.0073e-04,\n",
            "        -1.4901e-05, -3.3975e-06, -1.7405e-04, -1.0598e-04, -1.4830e-04,\n",
            "        -4.1161e-03, -2.0587e-04, -1.6737e-04, -1.1146e-04, -9.0003e-06,\n",
            "        -1.3030e-04, -1.1414e-04, -8.1682e-04, -1.4770e-04, -2.4796e-05,\n",
            "        -5.2738e-04, -2.7704e-04, -1.8829e-02, -4.9353e-04, -2.6321e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029029544442892075 0.00011110305786132812 0.7449096441268921 0.3354286849498749 tensor(62, device='cuda:0')\n",
            "pred tensor([-8.1491e-04, -1.4782e-03, -1.3816e-04, -1.8811e-04, -1.7464e-05,\n",
            "        -2.0111e-04, -1.7107e-05, -8.7857e-05, -1.0080e-03, -2.3508e-04,\n",
            "        -1.9646e-03, -7.8888e-03, -1.2482e-02, -1.0605e-03, -6.1810e-05,\n",
            "        -2.4343e-04, -1.9493e-03, -2.7800e-04, -6.7592e-05, -1.2054e-02,\n",
            "        -3.8208e-02, -2.7435e-02, -2.5120e-03, -3.8671e-04, -5.6744e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028890401124954224 3.910064697265625e-05 0.7838355898857117 0.3494037985801697 tensor(80, device='cuda:0')\n",
            "pred tensor([-4.9889e-05, -1.9264e-04, -5.7602e-04, -1.5235e-04, -2.6459e-02,\n",
            "        -2.8553e-03, -3.5429e-04, -2.3055e-04, -1.0639e-04, -7.2174e-03,\n",
            "        -6.6299e-03, -1.2982e-04, -1.2636e-04, -2.1482e-04, -1.0400e-01,\n",
            "        -8.3691e-01, -1.0890e-04, -1.3634e-02, -1.9789e-04, -7.4615e-03,\n",
            "        -2.7523e-03, -2.7490e-04, -9.5463e-04, -7.5102e-04, -3.8457e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028523121029138565 5.91278076171875e-05 0.7819549441337585 0.33626800775527954 tensor(71, device='cuda:0')\n",
            "pred tensor([-3.8004e-04, -1.1915e-04, -1.3647e-03, -1.3800e-03, -5.4312e-04,\n",
            "        -1.2894e-02, -1.9791e-02, -7.1436e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0834e-03, -1.0806e-04, -1.6212e-05, -4.9651e-05, -1.7762e-05,\n",
            "        -9.5367e-07, -1.1444e-05, -1.8969e-03, -2.8253e-04, -3.6836e-04,\n",
            "        -1.6737e-04, -1.0431e-04, -4.7922e-05, -1.0967e-05, -2.1636e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03065449558198452 2.0503997802734375e-05 0.7532651424407959 0.34418004751205444 tensor(97, device='cuda:0')\n",
            "pred tensor([-4.4703e-05, -4.8780e-04, -9.1267e-04, -4.4417e-04, -6.4993e-04,\n",
            "        -1.8382e-04, -3.4571e-06, -2.2054e-06, -1.3185e-04, -3.3975e-05,\n",
            "        -6.6340e-05, -1.4553e-03, -7.0953e-04, -5.2977e-04, -1.5583e-03,\n",
            "        -3.1996e-04, -6.0141e-05, -2.3365e-05, -4.2319e-05, -3.8075e-04,\n",
            "        -3.9215e-03, -1.0080e-03, -8.1558e-03, -1.8967e-02, -3.7861e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03090912289917469 0.0004892349243164062 0.6934760212898254 0.3389435410499573 tensor(76, device='cuda:0')\n",
            "pred tensor([-4.8697e-05, -4.1723e-07, -1.3113e-06, -2.5654e-03, -9.9512e-01,\n",
            "        -1.0000e+00, -4.9390e-01, -1.0016e-01, -7.1168e-05, -1.7703e-05,\n",
            "        -4.4703e-05, -2.6882e-05, -4.7684e-06, -3.8290e-04, -1.2147e-04,\n",
            "        -1.7202e-04, -8.0466e-06, -8.1062e-06, -8.8513e-05, -2.8920e-04,\n",
            "        -3.1590e-05, -3.3617e-05, -6.2752e-04, -1.2386e-04, -1.2140e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02665688842535019 0.0017490386962890625 0.6627491116523743 0.33704379200935364 tensor(67, device='cuda:0')\n",
            "61\n",
            "pred tensor([-2.0909e-04, -3.2723e-05, -1.0931e-04, -2.9812e-03, -1.6260e-03,\n",
            "        -2.3901e-05, -1.0556e-04, -3.9279e-05, -5.3287e-05, -8.3545e-01,\n",
            "        -2.8369e-01, -9.9609e-01, -1.0000e+00, -1.0000e+00, -4.9820e-03,\n",
            "        -4.2796e-04, -1.2045e-03, -3.3021e-04, -2.8849e-05, -5.4240e-06,\n",
            "        -9.8324e-04, -4.2648e-03, -5.7793e-04, -6.3002e-05, -3.8290e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030567068606615067 0.0008792877197265625 0.6719803214073181 0.33735671639442444 tensor(72, device='cuda:0')\n",
            "pred tensor([-2.8667e-03, -7.1096e-04, -6.4850e-04, -1.0151e-04, -7.6008e-04,\n",
            "        -1.8463e-03, -1.6193e-03, -3.4261e-04, -7.6294e-06, -9.1195e-06,\n",
            "        -2.2340e-04, -2.2876e-04, -1.7929e-03, -9.0027e-03, -3.4070e-04,\n",
            "        -5.8508e-04, -3.9756e-05, -1.7679e-04, -1.2243e-04, -1.0765e-04,\n",
            "        -1.0319e-03, -3.4576e-02, -5.0125e-03, -1.1854e-03, -5.0735e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031822383403778076 0.00023746490478515625 0.7106276750564575 0.33808252215385437 tensor(80, device='cuda:0')\n",
            "pred tensor([-1.6415e-04, -4.7028e-05, -2.6166e-05, -4.2319e-06, -4.9248e-03,\n",
            "        -1.1915e-04, -6.6948e-04, -2.3234e-04, -1.4365e-04, -9.7215e-05,\n",
            "        -7.6437e-04, -1.4336e-02, -1.0042e-03, -4.2229e-03, -3.2921e-03,\n",
            "        -8.3506e-05, -1.1504e-05, -1.5497e-05, -1.9491e-04, -1.0710e-03,\n",
            "        -1.0312e-04, -1.5278e-03, -1.2749e-02, -3.4253e-01, -9.9951e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03418927267193794 0.00010824203491210938 0.7478751540184021 0.3366405665874481 tensor(80, device='cuda:0')\n",
            "pred tensor([-9.9902e-01, -1.5676e-05, -7.7486e-07, -4.8697e-05, -1.7536e-04,\n",
            "        -1.1194e-04, -1.4198e-04, -8.6844e-05, -3.5763e-06, -3.1471e-03,\n",
            "        -2.6684e-03, -7.0572e-04, -2.1911e-04, -3.2544e-05, -1.2100e-04,\n",
            "        -2.8172e-03, -2.4676e-05, -7.0000e-03, -6.5565e-05, -6.9714e-04,\n",
            "        -9.3269e-04, -4.3907e-03, -8.4257e-04, -3.0880e-03, -1.3985e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030233178287744522 0.0003962516784667969 0.7024968266487122 0.3352079689502716 tensor(67, device='cuda:0')\n",
            "pred tensor([-9.9182e-05, -2.9325e-05, -2.7239e-05, -1.8954e-05, -5.6624e-06,\n",
            "        -1.2517e-06, -1.3292e-04, -1.5974e-03, -2.7142e-03, -1.7204e-03,\n",
            "        -9.2363e-04, -1.0312e-04, -4.3907e-03, -5.3358e-04, -1.9684e-03,\n",
            "        -5.1117e-04, -2.7409e-03, -4.3068e-03, -3.0339e-05, -4.5300e-06,\n",
            "        -5.1618e-05, -4.4882e-05, -5.2273e-05, -1.2815e-05, -2.0802e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031919367611408234 0.0005626678466796875 0.708452582359314 0.3354465067386627 tensor(69, device='cuda:0')\n",
            "pred tensor([-2.1315e-04, -5.2273e-05, -2.1565e-04, -1.8299e-05, -3.9577e-05,\n",
            "        -4.8518e-05, -1.9181e-04, -1.3709e-04, -1.3816e-04, -1.7941e-05,\n",
            "        -1.5664e-04, -1.7204e-03, -5.3596e-04, -5.5850e-05, -3.0458e-05,\n",
            "        -1.7345e-05, -3.9458e-05, -4.0710e-05, -2.6941e-04, -1.0228e-04,\n",
            "        -1.0900e-03, -1.4782e-05, -4.3809e-05, -5.8413e-06, -9.3162e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02786637470126152 0.0006394386291503906 0.67901211977005 0.3489374816417694 tensor(58, device='cuda:0')\n",
            "pred tensor([-2.6360e-03, -3.9935e-06, -1.2589e-04, -6.9702e-02, -4.9639e-04,\n",
            "        -1.4896e-03, -4.6372e-04, -8.2275e-02, -3.5648e-03, -1.8646e-02,\n",
            "        -9.6226e-04, -1.7059e-04, -2.3246e-06, -3.7193e-04, -5.9187e-05,\n",
            "        -1.5140e-05, -9.7803e-01, -1.0000e+00, -1.0000e+00, -2.2697e-03,\n",
            "        -2.4068e-04, -4.8518e-05, -2.0444e-05, -4.6492e-06, -5.4240e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03391528129577637 0.0006690025329589844 0.6795815229415894 0.33498963713645935 tensor(74, device='cuda:0')\n",
            "pred tensor([-8.2254e-06, -2.2292e-05, -1.1225e-03, -8.2195e-05, -6.0211e-02,\n",
            "        -6.3848e-04, -8.8310e-04, -5.7399e-05, -2.5392e-05, -7.2718e-06,\n",
            "        -8.5235e-06, -1.6689e-05, -1.9014e-05, -1.0848e-04, -6.4790e-05,\n",
            "        -5.8327e-03, -4.0550e-03, -3.6180e-05, -7.0333e-06, -6.6161e-06,\n",
            "        -1.2517e-06, -1.0133e-06, -2.6226e-06, -2.8193e-05, -3.2306e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02937622182071209 0.0004849433898925781 0.6966257095336914 0.3334338963031769 tensor(54, device='cuda:0')\n",
            "pred tensor([-2.1040e-05, -3.5107e-05, -2.5010e-04, -1.4842e-05, -7.0035e-05,\n",
            "        -1.1820e-04, -7.0333e-06, -5.2452e-04, -7.4673e-04, -1.4651e-04,\n",
            "        -2.7835e-05, -2.9206e-05, -2.2078e-04, -1.8711e-03, -3.1257e-04,\n",
            "        -2.9373e-04, -2.3060e-03, -8.4457e-03, -3.5691e-04, -4.3058e-04,\n",
            "        -2.5570e-05, -1.9336e-04, -4.9293e-05, -3.2187e-06, -2.1756e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027859173715114594 0.0007882118225097656 0.6683131456375122 0.35244491696357727 tensor(52, device='cuda:0')\n",
            "pred tensor([-8.5068e-04, -1.0908e-05, -2.5034e-06, -3.3545e-04, -1.6606e-04,\n",
            "        -9.5463e-04, -3.6621e-04, -9.6607e-04, -6.0158e-03, -8.4912e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -8.7500e-05, -6.1321e-04,\n",
            "        -1.3089e-04, -8.7857e-05, -1.4007e-05, -1.1325e-05, -5.6505e-05,\n",
            "        -9.1791e-06, -3.6359e-05, -3.4690e-05, -2.7704e-04, -3.1109e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027920357882976532 0.0005841255187988281 0.6866463422775269 0.3572746515274048 tensor(61, device='cuda:0')\n",
            "pred tensor([-7.5996e-05, -1.5259e-05, -5.5504e-04, -3.3498e-05, -1.7881e-06,\n",
            "        -1.2779e-04, -1.4997e-04, -1.2696e-05, -4.1008e-05, -1.5295e-04,\n",
            "        -8.3447e-06, -3.2723e-05, -4.5955e-05, -4.9472e-06, -2.5320e-04,\n",
            "        -9.3162e-05, -1.0031e-04, -4.4107e-06, -1.3145e-02, -1.0056e-02,\n",
            "        -9.3262e-01, -9.9658e-01, -9.2139e-01, -5.2686e-01, -1.5974e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024901380762457848 0.0036487579345703125 0.6546245813369751 0.3304431736469269 tensor(52, device='cuda:0')\n",
            "pred tensor([-6.7711e-04, -3.3808e-04, -1.7071e-03, -1.6665e-04, -1.1730e-04,\n",
            "        -1.1915e-04, -4.8518e-05, -7.4565e-05, -8.1658e-06, -1.4901e-06,\n",
            "        -8.4043e-06, -5.4240e-06, -6.5565e-07, -1.5497e-06, -5.9605e-08,\n",
            "        -1.1325e-06, -4.1008e-05, -4.5300e-06, -2.6131e-04, -1.8668e-04,\n",
            "        -9.0659e-05, -8.0466e-06, -3.4571e-06, -9.4938e-04, -6.2048e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027344869449734688 0.0002627372741699219 0.7439371347427368 0.3528027832508087 tensor(51, device='cuda:0')\n",
            "pred tensor([-2.8467e-04, -9.9951e-01, -9.9951e-01, -2.0993e-04, -1.1578e-03,\n",
            "        -1.1635e-04, -1.9336e-04, -9.1791e-06, -1.6153e-05, -1.6289e-03,\n",
            "        -2.7962e-03, -9.7215e-05, -2.5451e-05, -1.3456e-03, -1.0151e-04,\n",
            "        -5.2035e-05, -4.5815e-03, -2.7370e-04, -1.4150e-04, -2.8133e-04,\n",
            "        -5.7399e-05, -1.6034e-04, -3.2258e-04, -9.0599e-06, -3.8290e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02864118106663227 3.814697265625e-05 0.7938679456710815 0.3387601971626282 tensor(52, device='cuda:0')\n",
            "pred tensor([-9.3520e-05, -2.7776e-05, -8.2552e-05, -1.0490e-02, -1.4641e-02,\n",
            "        -1.1606e-03, -8.2159e-04, -5.5838e-04, -1.1730e-04, -7.7546e-05,\n",
            "        -2.9588e-04, -1.0681e-04, -1.1963e-04, -1.8969e-03, -3.4928e-04,\n",
            "        -4.0364e-04, -4.2677e-05, -5.2299e-03, -1.7376e-03, -7.7248e-05,\n",
            "        -1.3494e-04, -5.1439e-05, -1.1456e-04, -6.3300e-05, -2.4247e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03051942028105259 0.0 0.7916815280914307 0.33684077858924866 tensor(68, device='cuda:0')\n",
            "pred tensor([-3.1967e-03, -5.9414e-04, -7.9989e-05, -1.9789e-04, -9.5654e-04,\n",
            "        -5.5194e-05, -1.2529e-04, -4.8161e-05, -1.7738e-04, -4.8161e-05,\n",
            "        -7.0286e-04, -6.5029e-05, -6.2287e-05, -1.0848e-05, -3.9458e-05,\n",
            "        -4.7207e-05, -3.7551e-06, -5.6791e-04, -1.0000e+00, -9.7363e-01,\n",
            "        -7.6294e-05, -1.7810e-04, -1.0890e-04, -1.0490e-05, -1.1921e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03208786994218826 0.0002818107604980469 0.7218881845474243 0.33502647280693054 tensor(69, device='cuda:0')\n",
            "pred tensor([-4.5419e-05, -2.6131e-04, -3.3498e-05, -4.5037e-04, -3.4153e-05,\n",
            "        -7.2241e-05, -1.2815e-05, -2.9206e-06, -1.1921e-07, -1.8656e-05,\n",
            "        -1.5259e-05, -1.7285e-06, -2.7418e-06, -3.2783e-06, -6.9737e-05,\n",
            "        -3.7491e-05, -5.0068e-05, -9.4593e-05, -2.8563e-04, -1.3709e-05,\n",
            "        -2.0444e-05, -8.9407e-07, -1.0252e-05, -1.0133e-06, -1.4901e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028902117162942886 0.002208709716796875 0.668229341506958 0.3300580382347107 tensor(49, device='cuda:0')\n",
            "pred tensor([-8.3804e-05, -5.1618e-05, -2.5094e-05, -6.9201e-05, -4.5419e-05,\n",
            "        -7.6294e-06, -1.7583e-05, -9.3877e-05, -4.2677e-05, -3.9935e-05,\n",
            "        -1.5473e-04, -1.1545e-04, -1.3769e-05, -1.0228e-04, -3.5226e-05,\n",
            "        -1.5378e-05, -1.8001e-05, -3.5167e-06, -1.9729e-05, -9.1374e-05,\n",
            "        -2.8348e-04, -3.3331e-04, -6.7115e-05, -5.2452e-04, -1.4191e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03131186217069626 0.0016355514526367188 0.7008907794952393 0.33612290024757385 tensor(67, device='cuda:0')\n",
            "pred tensor([-1.6813e-03, -1.0109e-04, -5.0843e-05, -3.5763e-06, -1.8525e-04,\n",
            "        -1.8668e-04, -6.1393e-06, -2.3234e-04, -1.0073e-05, -1.0166e-03,\n",
            "        -8.5115e-05, -3.1590e-05, -4.9472e-05, -1.9109e-04, -7.8738e-05,\n",
            "        -3.4928e-05, -1.6034e-04, -1.8167e-04, -1.1578e-03, -3.3319e-05,\n",
            "        -1.7059e-04, -1.6284e-04, -3.5620e-04, -9.4557e-04, -4.9472e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03129031881690025 0.00038433074951171875 0.7247240543365479 0.3409159481525421 tensor(58, device='cuda:0')\n",
            "pred tensor([-5.5611e-05, -7.9870e-06, -7.2241e-05, -1.6272e-05, -1.6642e-03,\n",
            "        -6.9737e-06, -1.2338e-05, -5.9426e-05, -1.7685e-02, -4.5395e-04,\n",
            "        -1.0270e-04, -9.5367e-07, -6.1393e-06, -2.2650e-06, -1.8656e-05,\n",
            "        -4.4155e-04, -1.4770e-04, -3.4571e-06, -9.5725e-05, -3.7265e-04,\n",
            "        -4.9472e-06, -8.4817e-05, -1.0042e-03, -5.1022e-05, -4.3154e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03244340047240257 0.0005712509155273438 0.689467191696167 0.3399820625782013 tensor(90, device='cuda:0')\n",
            "pred tensor([-1.3709e-06, -1.7226e-05, -5.7602e-04, -1.9562e-04, -1.9670e-05,\n",
            "        -8.6260e-04, -2.2411e-05, -1.0431e-05, -1.1367e-04, -9.1791e-06,\n",
            "        -2.8014e-06, -2.9206e-06, -1.1921e-07, -6.1810e-05, -2.5725e-04,\n",
            "        -8.6975e-03, -9.7656e-01, -1.0000e+00, -1.0000e+00, -2.1398e-04,\n",
            "        -2.3973e-04, -1.1235e-04, -3.4392e-05, -1.4603e-05, -4.1580e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03386475890874863 0.000316619873046875 0.717104971408844 0.3638225197792053 tensor(93, device='cuda:0')\n",
            "pred tensor([-2.3592e-04, -1.0004e-03, -1.3275e-03, -6.4993e-04, -5.8270e-04,\n",
            "        -4.9651e-05, -2.5272e-05, -3.4809e-05, -4.1664e-05, -2.3687e-04,\n",
            "        -1.0691e-03, -7.9691e-05, -1.5724e-04, -2.3305e-05, -1.9729e-05,\n",
            "        -1.0031e-04, -7.2122e-06, -4.5300e-06, -6.8545e-06, -3.5620e-04,\n",
            "        -1.2817e-03, -3.3855e-04, -1.1425e-03, -2.6846e-04, -1.5414e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03571013733744621 0.0003604888916015625 0.7182695865631104 0.34059038758277893 tensor(91, device='cuda:0')\n",
            "pred tensor([-1.2994e-05, -1.1272e-03, -4.8590e-04, -5.2035e-05, -4.1962e-05,\n",
            "        -3.6776e-05, -1.3137e-04, -1.4198e-02, -2.5902e-03, -1.1504e-04,\n",
            "        -2.8968e-05, -3.6597e-05, -2.6822e-05, -1.1194e-04, -5.6028e-04,\n",
            "        -1.9407e-04, -1.6909e-03, -1.1492e-03, -8.0585e-04, -3.6120e-04,\n",
            "        -2.6531e-03, -6.2764e-05, -2.9683e-05, -3.0937e-03, -7.0000e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03583349287509918 0.0006966590881347656 0.6909775137901306 0.3412392735481262 tensor(88, device='cuda:0')\n",
            "pred tensor([-3.3975e-05, -1.1367e-04, -8.0109e-04, -7.1526e-07, -1.4126e-05,\n",
            "        -3.6061e-05, -9.5367e-05, -6.7949e-06, -1.7059e-04, -9.4727e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.3535e-02, -4.4155e-04, -6.9499e-05,\n",
            "        -2.0564e-05, -5.5432e-06, -1.1206e-05, -5.7399e-05, -2.9583e-03,\n",
            "        -5.1689e-03, -3.5977e-04, -1.1225e-03, -1.5335e-03, -9.4528e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03094414807856083 0.0008883476257324219 0.6703448295593262 0.33924856781959534 tensor(72, device='cuda:0')\n",
            "pred tensor([-5.7869e-03, -2.4109e-03, -6.2764e-05, -2.1994e-04, -1.0192e-04,\n",
            "        -3.6776e-05, -1.2100e-04, -6.9332e-04, -1.1053e-03, -4.5753e-04,\n",
            "        -4.1246e-04, -1.4365e-04, -4.4155e-04, -1.7405e-04, -1.7810e-04,\n",
            "        -6.1798e-04, -1.6797e-04, -8.5831e-05, -3.5187e-02, -4.7638e-02,\n",
            "        -9.9463e-01, -1.2922e-03, -2.8682e-04, -1.7853e-03, -1.2064e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03659027814865112 0.00079345703125 0.6844208240509033 0.3381727337837219 tensor(77, device='cuda:0')\n",
            "pred tensor([-1.2589e-04, -7.5459e-05, -8.7619e-06, -2.8610e-06, -2.2709e-05,\n",
            "        -2.1744e-04, -4.7755e-04, -5.5933e-04, -2.3594e-03, -1.8811e-04,\n",
            "        -1.2887e-04, -1.9760e-03, -4.0221e-04, -1.1456e-04, -1.0973e-04,\n",
            "        -2.2297e-03, -1.6606e-04, -9.9561e-01, -1.0000e+00, -5.7161e-05,\n",
            "        -1.1921e-05, -6.6817e-05, -3.2783e-06, -2.1458e-06, -1.5235e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03378088399767876 4.38690185546875e-05 0.7469591498374939 0.3389332592487335 tensor(80, device='cuda:0')\n",
            "pred tensor([-3.0422e-04, -4.0321e-03, -1.4076e-03, -2.7835e-05, -8.9188e-03,\n",
            "        -9.9945e-03, -8.5592e-04, -2.8496e-03, -2.1911e-04, -6.5029e-05,\n",
            "        -4.2558e-04, -9.5654e-04, -4.4703e-06, -1.0073e-05, -3.8743e-06,\n",
            "        -3.2349e-03, -2.8133e-04, -1.4019e-03, -3.8528e-04, -2.4629e-04,\n",
            "        -1.0920e-03, -2.2697e-04, -2.0826e-04, -5.5850e-05, -2.1279e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.035151321440935135 4.76837158203125e-05 0.7426585555076599 0.35321688652038574 tensor(88, device='cuda:0')\n",
            "pred tensor([-3.1257e-04, -1.4663e-05, -1.7822e-05, -4.7922e-05, -1.7130e-04,\n",
            "        -4.3130e-04, -4.1008e-03, -3.7048e-02, -2.8343e-03, -2.7704e-04,\n",
            "        -7.7486e-07, -5.3644e-07, -2.3842e-07, -1.7881e-07, -1.1921e-07,\n",
            "        -1.0389e-04, -4.1553e-01, -1.0000e+00, -1.0000e+00, -1.1292e-03,\n",
            "        -4.7264e-03, -9.0301e-05, -1.1921e-06, -9.5367e-07, -5.6624e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031229471787810326 0.00017547607421875 0.6913119554519653 0.3523746728897095 tensor(75, device='cuda:0')\n",
            "pred tensor([-7.7486e-07, -9.0003e-06, -9.4175e-06, -6.3181e-06, -5.4836e-06,\n",
            "        -3.4404e-04, -1.8387e-02, -2.7481e-02, -1.0548e-03, -4.8304e-04,\n",
            "        -7.9036e-05, -1.2338e-05, -1.2474e-03, -3.5644e-05, -5.5432e-05,\n",
            "        -3.3545e-04, -9.8515e-04, -9.2089e-05, -7.3195e-04, -3.9673e-03,\n",
            "        -2.3315e-02, -4.0253e-02, -2.6989e-03, -3.8087e-05, -2.1219e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03766603022813797 0.0023860931396484375 0.6424279808998108 0.3482779860496521 tensor(89, device='cuda:0')\n",
            "pred tensor([-6.3229e-04, -2.6417e-04, -1.0204e-03, -7.4673e-04, -4.5061e-05,\n",
            "        -1.5795e-05, -2.7955e-05, -2.5606e-04, -1.6737e-04, -2.5725e-04,\n",
            "        -4.4084e-04, -2.6417e-04, -6.6681e-03, -3.9246e-02, -1.0000e+00,\n",
            "        -1.0000e+00, -6.7711e-04, -1.2398e-03, -2.4490e-03, -4.5395e-03,\n",
            "        -2.7275e-04, -1.3514e-03, -8.1539e-05, -2.3785e-03, -3.0541e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03202661871910095 0.0003910064697265625 0.7218458652496338 0.33369141817092896 tensor(58, device='cuda:0')\n",
            "pred tensor([-1.2779e-04, -1.5843e-04, -1.6928e-04, -5.0812e-03, -3.9291e-03,\n",
            "        -1.6642e-03, -1.3247e-03, -4.4084e-04, -1.0281e-03, -2.8348e-04,\n",
            "        -8.8513e-05, -2.2602e-03, -1.7226e-05, -1.4198e-04, -2.5415e-04,\n",
            "        -3.8290e-04, -3.6478e-04, -1.2159e-03, -5.5771e-03, -5.5265e-04,\n",
            "        -1.8096e-04, -1.9872e-04, -5.9605e-07, -2.9802e-07, -4.3631e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030780499801039696 0.0005054473876953125 0.7080982327461243 0.33171355724334717 tensor(50, device='cuda:0')\n",
            "pred tensor([-1.7107e-05, -4.8876e-06, -4.9472e-06, -4.2498e-05, -5.7817e-06,\n",
            "        -7.7248e-05, -1.6422e-03, -4.2877e-03, -1.0000e+00, -9.9902e-01,\n",
            "        -1.0000e+00, -2.0087e-05, -3.4392e-05, -1.6570e-05, -4.0710e-05,\n",
            "        -4.7684e-06, -2.9802e-05, -1.4603e-05, -3.1471e-05, -8.7142e-05,\n",
            "        -7.7248e-05, -3.8910e-04, -1.2386e-04, -1.2445e-03, -5.0664e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032690223306417465 0.0009169578552246094 0.6491326689720154 0.33562129735946655 tensor(68, device='cuda:0')\n",
            "pred tensor([-2.5690e-05, -2.8610e-05, -3.3617e-05, -3.9339e-06, -4.1842e-05,\n",
            "        -4.2796e-05, -1.3030e-04, -6.5804e-05, -2.2335e-03, -9.2173e-04,\n",
            "        -6.2752e-04, -4.6606e-01, -9.5947e-02, -2.1992e-03, -1.2481e-04,\n",
            "        -2.5988e-05, -5.4240e-06, -2.3842e-07, -1.1921e-07, -1.1921e-07,\n",
            "        -6.3539e-05, -3.8385e-05, -9.7314e-01, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0332709364593029 0.0008091926574707031 0.6661835312843323 0.3491629660129547 tensor(62, device='cuda:0')\n",
            "pred tensor([-4.2725e-04, -1.1915e-04, -2.4915e-04, -8.5473e-05, -2.5392e-05,\n",
            "        -3.8815e-04, -1.5459e-03, -9.9756e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -6.2752e-04, -4.1962e-05, -2.9802e-07, -1.4424e-05,\n",
            "        -3.0458e-05, -4.0233e-05, -3.5346e-05, -2.8014e-06, -4.3511e-06,\n",
            "        -7.7248e-05, -2.3973e-04, -4.8518e-05, -1.1820e-04, -1.5843e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031039521098136902 0.0006551742553710938 0.6836296916007996 0.3582524359226227 tensor(79, device='cuda:0')\n",
            "pred tensor([-2.4974e-05, -1.0848e-05, -5.7817e-06, -8.9169e-04, -3.4928e-04,\n",
            "        -2.3663e-05, -6.1083e-04, -3.6478e-04, -4.9472e-05, -3.5524e-05,\n",
            "        -4.1127e-06, -3.6359e-06, -4.0531e-06, -4.0531e-06, -4.5929e-03,\n",
            "        -9.9902e-01, -1.0000e+00, -1.0000e+00, -1.8604e-01, -4.8208e-04,\n",
            "        -6.3515e-04, -8.5831e-05, -1.6868e-04, -1.0166e-03, -4.8208e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029829777777194977 0.00023365020751953125 0.7013421058654785 0.34142598509788513 tensor(59, device='cuda:0')\n",
            "pred tensor([-7.0801e-03, -3.3140e-04, -1.3924e-04, -3.1018e-04, -1.0319e-03,\n",
            "        -3.2640e-04, -6.2132e-04, -7.9989e-05, -2.3880e-03, -2.3139e-04,\n",
            "        -5.3263e-04, -1.0890e-04, -1.5295e-04, -1.3816e-04, -7.2539e-05,\n",
            "        -6.1333e-05, -5.4538e-05, -4.9162e-04, -2.6131e-04, -1.6384e-03,\n",
            "        -3.4523e-04, -1.5488e-03, -1.0000e+00, -9.9170e-01, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03386698290705681 0.0006041526794433594 0.7056086659431458 0.3493974208831787 tensor(92, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -6.0856e-05, -7.8678e-06, -2.1636e-05,\n",
            "        -2.5010e-04, -5.4955e-05,  0.0000e+00, -1.3113e-06, -1.6689e-06,\n",
            "        -2.9802e-07, -7.2718e-06, -1.1504e-04, -1.5664e-04, -3.2306e-05,\n",
            "        -4.0352e-05, -3.1018e-04, -6.8545e-06, -1.9073e-06, -3.7611e-05,\n",
            "        -1.1325e-04, -5.0843e-05, -1.4901e-05, -4.1723e-07, -2.3842e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031591787934303284 0.0007042884826660156 0.6976563334465027 0.3456996977329254 tensor(74, device='cuda:0')\n",
            "62\n",
            "pred tensor([-9.8705e-04, -5.6458e-04, -7.1526e-07, -6.9141e-06, -2.3782e-04,\n",
            "        -2.7418e-06, -1.1921e-07, -1.1975e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -2.6722e-03, -4.2877e-03, -2.7061e-04,\n",
            "        -1.8282e-03, -1.0031e-04, -1.7953e-04, -3.0828e-04, -2.6536e-04,\n",
            "        -1.4484e-04, -9.8705e-04, -1.0312e-04, -4.2963e-04, -5.1022e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03168187662959099 0.0019321441650390625 0.663921594619751 0.3356459140777588 tensor(74, device='cuda:0')\n",
            "pred tensor([-4.0169e-03, -6.7711e-04, -9.0933e-04, -8.4591e-04, -1.3602e-04,\n",
            "        -9.2363e-04, -1.2077e-02, -6.0368e-04, -2.2256e-04, -3.2723e-05,\n",
            "        -9.1732e-05, -2.5821e-04, -2.3003e-03, -3.3498e-05, -7.9155e-04,\n",
            "        -7.1096e-04, -5.1260e-05, -1.1820e-04, -9.9659e-04, -7.8678e-04,\n",
            "        -5.8949e-05, -1.8299e-05, -1.6630e-05, -4.1485e-04, -1.2398e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03290781006217003 0.00099945068359375 0.6684879660606384 0.3375104069709778 tensor(79, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.3535e-02, -5.8365e-04, -1.0765e-04, -1.6153e-04,\n",
            "        -8.6594e-03, -1.2032e-02, -2.5225e-04, -1.7042e-03, -3.4189e-04,\n",
            "        -1.0204e-03, -4.7374e-04, -1.1950e-03, -1.0431e-04, -4.2558e-04,\n",
            "        -9.1648e-04, -4.5955e-05, -7.6890e-06, -5.1856e-05, -1.5438e-05,\n",
            "        -1.3602e-04, -5.1618e-05, -1.8656e-05, -1.8001e-05, -7.2539e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029847973957657814 0.00014209747314453125 0.7224161624908447 0.3365195393562317 tensor(55, device='cuda:0')\n",
            "pred tensor([-5.6267e-05, -4.1819e-04, -1.0931e-04, -6.5029e-05, -2.2590e-05,\n",
            "        -1.3888e-05, -1.4484e-04, -1.9336e-04, -3.8981e-05, -7.2539e-05,\n",
            "        -3.8075e-04, -9.6858e-05, -1.1511e-03, -2.1782e-03, -4.2558e-04,\n",
            "        -7.0143e-04, -4.7565e-04, -3.5977e-04, -1.7822e-05, -2.7585e-04,\n",
            "        -6.1572e-05, -1.6809e-05, -1.2040e-05, -1.1292e-03, -1.2589e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03394317999482155 0.0001125335693359375 0.7161088585853577 0.34023386240005493 tensor(86, device='cuda:0')\n",
            "pred tensor([-7.6294e-05, -2.8193e-05, -2.1660e-04, -3.8683e-05, -5.5850e-05,\n",
            "        -1.0228e-04, -1.6525e-02, -1.6842e-03, -9.9182e-05, -1.6272e-05,\n",
            "        -2.8801e-04, -6.7592e-05, -8.4591e-04, -2.5902e-03, -1.5671e-02,\n",
            "        -1.9407e-04, -3.1996e-04, -1.3428e-03, -3.1066e-04, -1.4186e-05,\n",
            "        -4.5598e-05, -1.6642e-03, -1.3340e-04, -1.8668e-04, -5.8413e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03461698815226555 0.0008769035339355469 0.667526125907898 0.34530967473983765 tensor(86, device='cuda:0')\n",
            "pred tensor([-1.1921e-06, -4.3511e-06, -4.6670e-05, -2.9354e-03, -9.9512e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -2.8801e-04, -5.8270e-04, -1.5917e-03,\n",
            "        -5.0354e-04, -3.9756e-05, -1.1963e-04, -1.0610e-05, -4.7112e-04,\n",
            "        -1.4603e-05, -5.9187e-05, -4.3333e-05, -1.4937e-04, -2.3823e-03,\n",
            "        -7.3528e-04, -3.8600e-04, -6.2275e-04, -5.0664e-06, -1.9848e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026413295418024063 0.00167083740234375 0.650823712348938 0.33325210213661194 tensor(58, device='cuda:0')\n",
            "pred tensor([-1.8954e-05, -5.3704e-05, -7.0000e-04, -2.7466e-03, -1.7271e-03,\n",
            "        -5.7983e-03, -1.0612e-02, -1.0004e-03, -9.6416e-04, -1.2100e-04,\n",
            "        -3.7193e-04, -4.9257e-04, -5.8949e-05, -3.0212e-03, -9.2363e-04,\n",
            "        -1.9562e-04, -3.3736e-04, -6.4135e-04, -7.4530e-04, -8.6260e-04,\n",
            "        -2.9812e-03, -1.6069e-03, -6.8245e-03, -3.3665e-04, -5.9426e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03226145729422569 0.0004944801330566406 0.7033681273460388 0.34735071659088135 tensor(110, device='cuda:0')\n",
            "pred tensor([-0.0001, -0.0028, -0.0054, -0.0048, -0.0009, -0.0003, -0.0001, -0.0003,\n",
            "        -0.0102, -0.0070, -0.0021, -0.0264, -0.0041, -0.0007, -0.0002, -0.0009,\n",
            "        -0.0003, -0.0023, -0.0023, -0.0002, -0.0003, -0.0009, -0.0013, -0.0005,\n",
            "        -0.0002], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02987106703221798 0.0006504058837890625 0.6874629259109497 0.3576207458972931 tensor(84, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -2.2876e-04, -7.5758e-05, -2.3727e-03,\n",
            "        -2.1114e-03, -7.0870e-05, -4.2140e-05, -1.6332e-05, -6.3372e-04,\n",
            "        -8.3804e-05, -6.0844e-03, -3.0994e-03, -5.2299e-03, -3.5934e-03,\n",
            "        -2.3193e-03, -2.1565e-04, -1.1692e-03, -2.9984e-03, -3.6716e-03,\n",
            "        -4.8218e-03, -2.8114e-03, -8.1711e-03, -7.7197e-01, -9.9805e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030290693044662476 0.0005774497985839844 0.651405930519104 0.34026381373405457 tensor(87, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -1.0000e+00, -9.3896e-01, -9.9951e-01,\n",
            "        -9.9463e-01, -8.5205e-01, -3.7183e-01, -6.6910e-03, -8.7500e-05,\n",
            "        -4.5300e-06, -1.3602e-04, -1.6034e-05, -2.1076e-04, -3.8910e-04,\n",
            "        -1.7703e-05, -4.5547e-03, -1.2934e-04, -3.7365e-03, -7.9036e-05,\n",
            "        -3.1948e-05, -1.8358e-05, -1.4305e-06, -4.4703e-06, -3.2544e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03109472244977951 0.0003466606140136719 0.6890542507171631 0.3405287563800812 tensor(82, device='cuda:0')\n",
            "pred tensor([-8.6784e-04, -8.2254e-06, -9.2983e-06, -1.0431e-04, -1.3794e-02,\n",
            "        -4.9257e-04, -2.5570e-05, -3.9160e-05, -1.5378e-05, -3.0994e-06,\n",
            "        -1.6212e-05, -4.2558e-04, -1.9729e-05, -3.5644e-05, -1.8811e-04,\n",
            "        -2.6822e-06, -7.7486e-07, -7.0286e-04, -3.8981e-05, -1.0729e-05,\n",
            "        -1.5736e-05, -7.9036e-05, -2.9254e-04, -1.8454e-04, -1.2169e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031003467738628387 0.0002646446228027344 0.6892630457878113 0.3410670757293701 tensor(60, device='cuda:0')\n",
            "pred tensor([-1.0270e-04, -1.7853e-03, -3.7932e-04, -1.0598e-04, -1.5097e-03,\n",
            "        -2.2125e-03, -9.7998e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -8.1241e-05, -1.0890e-04, -1.3876e-04, -1.3113e-06, -2.8431e-05,\n",
            "        -1.8775e-05, -3.2723e-05, -2.2054e-06, -7.4506e-06, -5.6624e-06,\n",
            "        -2.6822e-06, -7.9870e-06, -2.2650e-05, -5.3263e-04, -2.3687e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032154183834791183 0.0008711814880371094 0.6747136116027832 0.358796626329422 tensor(69, device='cuda:0')\n",
            "pred tensor([-5.2357e-04, -5.2605e-03, -2.1877e-03, -1.0796e-03, -1.8811e-04,\n",
            "        -7.7009e-04, -3.6979e-04, -5.5859e-01, -1.1826e-02, -2.8000e-03,\n",
            "        -1.5697e-03, -3.3808e-04, -1.6475e-04, -1.3924e-04, -5.7459e-04,\n",
            "        -6.2370e-04, -6.6578e-05, -9.3877e-05, -1.9312e-05, -3.5942e-05,\n",
            "        -2.2964e-03, -1.0000e+00, -1.0000e+00, -6.3992e-04, -3.6478e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027950545772910118 0.0002307891845703125 0.7245712280273438 0.33952879905700684 tensor(48, device='cuda:0')\n",
            "pred tensor([-2.1315e-04, -5.4121e-04, -1.9562e-04, -1.0023e-03, -3.2406e-03,\n",
            "        -2.2259e-03, -1.0973e-04, -1.6415e-04, -1.4696e-03, -8.3506e-05,\n",
            "        -9.3520e-05, -5.7936e-04, -7.9989e-05, -3.5691e-04, -5.5194e-05,\n",
            "        -6.3300e-05, -1.9014e-05, -3.3081e-05, -1.5497e-06, -4.2140e-05,\n",
            "        -5.0735e-04, -1.0848e-04, -4.8161e-05, -1.2934e-05, -5.4359e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029251912608742714 0.00122833251953125 0.6539667844772339 0.34507665038108826 tensor(74, device='cuda:0')\n",
            "pred tensor([-3.3855e-04, -1.8311e-04, -6.1095e-05, -7.3969e-05, -1.0281e-03,\n",
            "        -5.8293e-05, -1.2350e-03, -9.8801e-03, -1.5335e-03, -3.3474e-04,\n",
            "        -2.6417e-04, -3.1829e-05, -3.3081e-05, -1.2684e-04, -4.1485e-04,\n",
            "        -3.8362e-04, -4.7755e-04, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -2.7771e-02, -9.7632e-05, -7.4863e-05, -1.4210e-03, -1.2386e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03149377927184105 0.00046062469482421875 0.7142945528030396 0.3410060405731201 tensor(88, device='cuda:0')\n",
            "pred tensor([-0.0002, -0.0003, -0.0003, -0.0015, -0.0017, -0.0331, -0.0020, -0.0005,\n",
            "        -0.0008, -0.0022, -0.0034, -0.0010, -0.0014, -0.0021, -0.0005, -0.0051,\n",
            "        -0.0024, -0.0013, -0.0002, -0.0003, -0.0015, -0.0007, -0.0019, -0.0043,\n",
            "        -0.0033], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029599254950881004 0.0004906654357910156 0.6845995187759399 0.3445865213871002 tensor(98, device='cuda:0')\n",
            "pred tensor([-3.1495e-04, -1.8967e-02, -3.1257e-04, -3.4928e-04, -2.2519e-04,\n",
            "        -3.6836e-04, -4.7112e-04, -2.0542e-03, -9.1374e-05, -8.3804e-05,\n",
            "        -4.7112e-04, -1.6384e-03, -8.8310e-04, -4.1080e-04, -3.4273e-05,\n",
            "        -1.9038e-04, -8.3447e-07, -1.4544e-04, -2.7847e-03, -3.3975e-06,\n",
            "        -2.7299e-05, -3.1853e-03, -4.5466e-04, -3.0304e-02, -4.0649e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028553316369652748 0.0023250579833984375 0.6651453971862793 0.34065186977386475 tensor(82, device='cuda:0')\n",
            "pred tensor([-5.2441e-01, -5.1239e-02, -7.1487e-03, -7.0114e-03, -6.2683e-02,\n",
            "        -2.2583e-02, -7.6843e-02, -1.2383e-02, -5.5695e-04, -6.7890e-05,\n",
            "        -1.6630e-05, -2.6536e-04, -1.9569e-03, -4.7469e-04, -3.3975e-06,\n",
            "        -8.7142e-05, -4.7684e-06, -6.1333e-05, -7.1487e-03, -9.9170e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.7958e-03, -3.8981e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031181620433926582 0.0002803802490234375 0.7386945486068726 0.34201979637145996 tensor(89, device='cuda:0')\n",
            "pred tensor([-1.9944e-04, -1.0109e-04, -7.7546e-05, -1.0473e-04, -1.9722e-03,\n",
            "        -4.0779e-03, -1.2894e-03, -9.7070e-01, -9.9951e-01, -1.0000e+00,\n",
            "        -4.6372e-04, -2.7955e-05, -6.3002e-05, -8.4925e-04, -1.0556e-04,\n",
            "        -1.4091e-04, -1.3983e-04, -3.9816e-04, -1.3554e-04, -1.3298e-02,\n",
            "        -2.7585e-04, -9.7179e-04, -1.0672e-03, -1.0681e-04, -1.2338e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.033144500106573105 2.6702880859375e-05 0.7740188241004944 0.34645286202430725 tensor(113, device='cuda:0')\n",
            "pred tensor([-2.7275e-04, -1.0765e-04, -5.9986e-04, -1.1772e-04, -4.5395e-04,\n",
            "        -1.5843e-04, -1.7464e-05, -1.0973e-04, -7.1335e-03, -8.4000e-03,\n",
            "        -9.9951e-01, -1.0000e+00, -7.5035e-03, -1.3428e-03, -9.7314e-01,\n",
            "        -9.9316e-01, -3.5522e-01, -3.6621e-02, -2.0504e-04, -5.3978e-04,\n",
            "        -9.3877e-05, -1.5664e-04, -1.3089e-04, -3.5477e-04, -1.4811e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028858555480837822 0.000156402587890625 0.7112118005752563 0.3448847234249115 tensor(82, device='cuda:0')\n",
            "pred tensor([-1.6153e-04, -4.0770e-04, -2.7776e-05, -1.8454e-04, -1.7748e-03,\n",
            "        -1.5533e-04, -5.3704e-05, -4.4107e-06, -2.8610e-05, -1.6809e-05,\n",
            "        -7.5698e-06, -1.0967e-05, -7.6950e-05, -1.3494e-04, -1.3661e-04,\n",
            "        -6.4015e-05, -9.2888e-04, -3.1173e-05, -1.4591e-04, -7.9274e-06,\n",
            "        -5.8770e-05, -2.9354e-03, -4.1351e-03, -1.5414e-04, -3.4928e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03323398157954216 0.0003819465637207031 0.6784806251525879 0.34365665912628174 tensor(96, device='cuda:0')\n",
            "pred tensor([-7.0572e-05, -6.5804e-05, -1.3447e-04, -1.3828e-05, -7.0286e-04,\n",
            "        -4.6849e-05, -1.0118e-03, -6.4790e-05, -1.4925e-03, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -2.2292e-05, -1.8096e-04, -5.6088e-05,\n",
            "        -1.5855e-05, -1.0681e-04, -1.3053e-05, -2.0421e-04, -3.1805e-04,\n",
            "        -2.1565e-04, -4.0829e-05, -2.1660e-04, -1.0848e-04, -1.5724e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030358513817191124 0.0005621910095214844 0.6777917146682739 0.3566124439239502 tensor(85, device='cuda:0')\n",
            "pred tensor([-3.2377e-04, -3.7551e-04, -8.6328e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -4.8876e-04, -5.3287e-05, -6.9737e-05, -2.9778e-04, -7.2122e-06,\n",
            "        -2.9862e-05, -2.1315e-04, -8.8513e-05, -4.9472e-06, -2.0909e-04,\n",
            "        -8.3506e-05, -6.6578e-05, -6.3002e-05, -2.8348e-04, -1.3924e-04,\n",
            "        -1.0967e-05, -1.8835e-05, -1.1950e-03, -5.7364e-04, -3.4738e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029473179951310158 0.0027065277099609375 0.6622438430786133 0.34437692165374756 tensor(102, device='cuda:0')\n",
            "pred tensor([-4.6539e-02, -9.9854e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -5.8770e-05, -6.1321e-04, -1.2598e-03, -1.1625e-03, -2.0866e-03,\n",
            "        -1.3399e-04, -6.3002e-05, -6.1810e-05, -4.0531e-04, -1.9789e-04,\n",
            "        -1.5557e-05, -1.0723e-04, -1.6034e-04, -7.8738e-05, -1.7107e-05,\n",
            "        -4.4882e-05, -2.4247e-04, -1.5593e-04, -5.4836e-04, -1.1623e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032478611916303635 0.0004210472106933594 0.7204259634017944 0.3456301987171173 tensor(107, device='cuda:0')\n",
            "pred tensor([-4.5061e-05, -1.3554e-04, -2.3055e-04, -1.5616e-05, -1.3723e-03,\n",
            "        -1.6475e-04, -4.0674e-04, -2.1875e-05, -1.1768e-03, -1.5783e-04,\n",
            "        -2.5320e-04, -6.3002e-05, -2.6531e-03, -8.9169e-04, -1.7333e-04,\n",
            "        -3.4523e-04, -3.9291e-04, -6.5517e-04, -1.6975e-03, -1.3481e-02,\n",
            "        -2.8467e-04, -1.0073e-04, -6.2207e-01, -9.1406e-01, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0311521515250206 8.487701416015625e-05 0.7369083166122437 0.3419930040836334 tensor(89, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -4.7851e-04, -1.7703e-05, -1.1157e-01,\n",
            "        -1.0353e-04, -2.9683e-05, -9.5010e-05, -1.4267e-03, -5.8413e-06,\n",
            "        -5.4359e-05, -1.1539e-03, -8.9407e-07, -9.0003e-06, -3.6955e-06,\n",
            "        -5.6624e-06, -5.0068e-05, -1.6510e-05, -4.7386e-05, -3.4904e-03,\n",
            "        -1.0000e+00, -1.0000e+00, -7.1487e-03, -1.2054e-02, -1.3752e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030197681859135628 0.00011587142944335938 0.7480179667472839 0.33168846368789673 tensor(56, device='cuda:0')\n",
            "pred tensor([-2.4724e-04, -2.1610e-03, -8.2850e-05, -7.2718e-06, -2.1458e-06,\n",
            "        -1.9073e-06, -1.2875e-05, -1.1384e-05, -1.1325e-06, -9.2447e-05,\n",
            "        -6.1798e-04, -3.1054e-05, -6.9695e-03, -3.1567e-01, -6.9141e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -4.6730e-03, -1.0262e-03, -2.7176e-02, -1.0147e-03, -2.7800e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0291509497910738 0.00012731552124023438 0.7247776389122009 0.3704759180545807 tensor(64, device='cuda:0')\n",
            "pred tensor([-9.7122e-03, -1.0777e-03, -1.0633e-03, -2.7790e-03, -1.6518e-03,\n",
            "        -7.8142e-05, -2.1439e-03, -1.6983e-02, -8.1863e-03, -3.8090e-03,\n",
            "        -5.9175e-04, -3.0899e-04, -1.1692e-03, -1.2016e-03, -9.0742e-04,\n",
            "        -1.3876e-04, -8.7500e-05, -9.6989e-04, -1.3447e-04, -3.5644e-05,\n",
            "        -1.7271e-03, -8.2493e-04, -8.6069e-04, -5.0735e-04, -5.4836e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029996298253536224 0.0006499290466308594 0.679120659828186 0.3381365239620209 tensor(69, device='cuda:0')\n",
            "pred tensor([-8.5596e-01, -2.0504e-04, -1.0723e-04, -1.4901e-05, -3.2496e-04,\n",
            "        -3.1357e-03, -2.3842e-07, -8.9844e-01, -1.0000e+00, -1.4296e-03,\n",
            "        -2.3592e-04, -1.6165e-03, -1.3634e-02, -1.1820e-04, -3.9935e-06,\n",
            "        -1.8835e-05, -1.3113e-05, -9.1732e-05, -1.0729e-05, -1.8656e-05,\n",
            "        -5.4538e-05, -6.1810e-05, -3.2783e-06, -2.9683e-05, -4.2319e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0261684563010931 0.00267791748046875 0.643020510673523 0.36098745465278625 tensor(64, device='cuda:0')\n",
            "pred tensor([-1.0931e-04, -2.5225e-04, -3.0816e-05, -3.2878e-04, -8.8215e-06,\n",
            "        -1.5557e-05, -5.9426e-05, -6.5804e-05, -1.8060e-05, -1.8454e-04,\n",
            "        -3.7193e-05, -2.8610e-05, -2.1148e-04, -7.4565e-05, -9.3985e-04,\n",
            "        -1.0848e-05, -1.5783e-04, -8.3148e-05, -1.5163e-03, -9.2173e-04,\n",
            "        -4.6182e-04, -1.4353e-03, -3.2640e-04, -1.0890e-04, -7.7200e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02349245548248291 0.0008726119995117188 0.7017058730125427 0.3501346707344055 tensor(42, device='cuda:0')\n",
            "pred tensor([-3.8218e-04, -1.7202e-04, -1.6475e-04, -1.9073e-05, -2.4152e-04,\n",
            "        -1.8811e-04, -8.3113e-04, -1.8239e-04, -5.1641e-04, -2.8133e-04,\n",
            "        -8.1062e-04, -9.0283e-01, -9.9951e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0147e-03, -1.0312e-05, -3.9577e-05, -4.1246e-04, -5.1260e-05,\n",
            "        -4.0779e-03, -8.2254e-06, -9.5367e-07, -2.9373e-04, -3.8981e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02751900628209114 5.9604644775390625e-05 0.7650247812271118 0.3347178101539612 tensor(58, device='cuda:0')\n",
            "pred tensor([-6.8963e-05, -2.9898e-04, -1.4961e-05, -1.9073e-06, -8.3447e-06,\n",
            "        -1.1921e-07, -4.6313e-05, -3.0994e-06, -1.4603e-05, -9.9540e-06,\n",
            "        -2.2054e-06, -3.9339e-06, -4.1127e-06, -6.5565e-05, -1.0967e-05,\n",
            "        -9.6858e-05, -9.2089e-05, -1.2522e-03, -1.3340e-04, -3.5763e-05,\n",
            "        -9.2447e-05, -3.1054e-05, -1.8597e-04, -2.9802e-05, -4.2677e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03199698403477669 7.581710815429688e-05 0.7493057250976562 0.3364972770214081 tensor(73, device='cuda:0')\n",
            "pred tensor([-2.7919e-04, -2.9588e-04, -6.1333e-05, -1.2195e-04, -1.0723e-04,\n",
            "        -1.3137e-04, -1.9875e-03, -4.8409e-03, -4.7374e-04, -1.5593e-04,\n",
            "        -9.5272e-04, -3.3927e-04, -6.1572e-05, -8.2550e-03, -8.7023e-06,\n",
            "        -9.6083e-05, -6.6340e-05, -7.5698e-06, -3.6180e-05, -2.0421e-04,\n",
            "        -4.7922e-05, -5.7220e-06, -2.2078e-04, -8.3590e-04, -2.5732e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03334611654281616 0.00015544891357421875 0.7109373807907104 0.34541115164756775 tensor(72, device='cuda:0')\n",
            "pred tensor([-1.0920e-03, -5.0247e-05, -4.5133e-04, -4.9162e-04, -2.0742e-04,\n",
            "        -4.2796e-05, -1.0073e-04, -6.8963e-05, -2.9707e-04, -6.3515e-04,\n",
            "        -1.4937e-04, -3.7193e-05, -1.0586e-03, -1.7548e-03, -1.7138e-03,\n",
            "        -1.0653e-03, -9.0933e-04, -1.7464e-04, -5.5611e-05, -3.9935e-05,\n",
            "        -3.3665e-04, -5.5933e-04, -2.3842e-05, -1.3232e-04, -2.3003e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031067294999957085 0.0011129379272460938 0.6625697016716003 0.33882761001586914 tensor(75, device='cuda:0')\n",
            "pred tensor([-9.6631e-01, -9.9854e-01, -9.7752e-04, -8.6927e-04, -2.5725e-04,\n",
            "        -1.5843e-04, -1.9133e-05, -4.3559e-04, -4.5133e-04, -1.1272e-03,\n",
            "        -1.1146e-04, -3.8505e-05, -9.2089e-05, -3.7193e-05, -1.6332e-05,\n",
            "        -1.7059e-04, -2.4796e-05, -3.5691e-04, -4.7112e-04, -1.0556e-04,\n",
            "        -4.1986e-04, -7.5388e-04, -1.0967e-03, -2.1400e-03, -3.0899e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032223064452409744 0.0014934539794921875 0.6697455644607544 0.3435191214084625 tensor(87, device='cuda:0')\n",
            "pred tensor([-3.7336e-04, -2.1994e-04, -8.5402e-04, -4.6563e-04, -5.0664e-05,\n",
            "        -5.8949e-05, -4.1962e-05, -9.2089e-05, -2.4438e-04, -8.4782e-04,\n",
            "        -9.9951e-01, -9.9561e-01, -9.9951e-01, -6.7472e-04, -7.8869e-04,\n",
            "        -1.8597e-04, -4.6849e-05, -6.1572e-05, -1.1730e-04, -4.5419e-05,\n",
            "        -7.0333e-05, -1.4770e-04, -3.2544e-05, -5.0843e-05, -1.2636e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030324064195156097 0.001552581787109375 0.6669171452522278 0.33566564321517944 tensor(71, device='cuda:0')\n",
            "pred tensor([-1.1963e-01, -1.4467e-03, -8.3923e-04, -3.9444e-03, -2.5558e-03,\n",
            "        -2.2221e-03, -5.3263e-04, -3.4690e-05, -3.0661e-04, -2.2156e-02,\n",
            "        -2.6536e-04, -1.7738e-04, -3.7212e-03, -1.3634e-02, -1.0118e-03,\n",
            "        -7.5245e-04, -1.9109e-04, -1.5354e-04, -5.0068e-05, -8.6260e-04,\n",
            "        -5.1403e-04, -9.9277e-04, -4.3392e-04, -1.7481e-03, -3.2568e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031176578253507614 0.001483917236328125 0.6427967548370361 0.33949217200279236 tensor(83, device='cuda:0')\n",
            "pred tensor([-1.2405e-02, -1.1635e-04, -2.5616e-03, -2.7490e-04, -1.3590e-03,\n",
            "        -3.3665e-04, -8.8215e-05, -6.7353e-05, -3.5942e-05, -6.2037e-04,\n",
            "        -7.6950e-05, -7.0333e-06, -1.4091e-04, -1.5056e-04, -2.2233e-05,\n",
            "        -6.5565e-07, -2.9802e-06, -6.3300e-05, -2.1398e-04, -1.3554e-04,\n",
            "        -1.8826e-03, -3.0136e-02, -4.2456e-01, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02616742253303528 0.00018024444580078125 0.7031253576278687 0.3325999081134796 tensor(55, device='cuda:0')\n",
            "63\n",
            "pred tensor([-2.3687e-04, -5.7526e-03, -1.0000e+00, -1.0000e+00, -1.1146e-05,\n",
            "        -2.5988e-05, -1.2577e-05, -7.9274e-06, -7.8082e-06, -1.0371e-05,\n",
            "        -7.8678e-06, -5.1212e-04, -4.4942e-04, -1.8954e-05, -2.6226e-06,\n",
            "        -2.3973e-04, -4.6492e-05, -2.4724e-04, -1.7202e-04, -2.3484e-05,\n",
            "        -1.1277e-04, -3.8981e-05, -1.2100e-05, -3.7551e-06, -1.0192e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02983226627111435 2.2411346435546875e-05 0.7489262223243713 0.3465858995914459 tensor(60, device='cuda:0')\n",
            "pred tensor([-8.4162e-05, -3.3736e-05, -8.1658e-06, -2.2590e-05, -4.6082e-03,\n",
            "        -1.0389e-04, -3.4273e-05, -1.6327e-03, -9.5367e-05, -2.9802e-06,\n",
            "        -1.1772e-04, -3.6061e-05, -3.3200e-05, -3.4809e-04, -2.8551e-05,\n",
            "        -3.4153e-05, -2.0087e-05, -1.3232e-05, -1.5676e-05, -1.0788e-05,\n",
            "        -4.2462e-04, -3.5214e-04, -1.7166e-05, -2.3723e-05, -1.7953e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03155025467276573 0.00017023086547851562 0.7212541699409485 0.359330415725708 tensor(67, device='cuda:0')\n",
            "pred tensor([-4.0829e-05, -3.6621e-04, -5.4932e-04, -3.3140e-04, -3.3283e-04,\n",
            "        -2.8920e-04, -3.3855e-05, -5.5432e-05, -1.0490e-05, -6.0129e-04,\n",
            "        -1.9038e-04, -1.1721e-03, -7.3671e-05, -4.2975e-05, -2.2256e-04,\n",
            "        -1.2004e-04, -1.2894e-03, -2.1482e-04, -1.0389e-04, -3.5071e-04,\n",
            "        -9.4175e-04, -4.3631e-05, -8.5735e-04, -4.5598e-05, -1.9431e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029158620163798332 0.0005831718444824219 0.6907570362091064 0.36051371693611145 tensor(79, device='cuda:0')\n",
            "pred tensor([-4.9472e-05, -2.6584e-05, -1.0931e-04, -2.8133e-04, -7.6008e-04,\n",
            "        -1.9336e-04, -9.1016e-05, -1.2934e-04, -1.0586e-03, -5.0507e-03,\n",
            "        -5.9426e-05, -4.2892e-04, -1.1091e-03, -2.4343e-04, -1.8167e-04,\n",
            "        -8.2195e-05, -2.5010e-04, -7.3373e-05, -2.1911e-04, -7.5758e-05,\n",
            "        -9.1374e-05, -4.6277e-04, -3.6180e-05, -1.5414e-04, -1.5843e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027249202132225037 0.0009183883666992188 0.7163897752761841 0.33779221773147583 tensor(68, device='cuda:0')\n",
            "pred tensor([-2.2113e-05, -5.3287e-05, -1.2529e-04, -4.8280e-06, -4.9293e-05,\n",
            "        -4.5258e-02, -1.0681e-04, -4.9639e-04, -3.9124e-04, -2.7704e-04,\n",
            "        -5.4777e-05, -1.9670e-06, -6.8545e-06, -3.3736e-05, -1.4484e-05,\n",
            "        -1.0765e-04, -1.6809e-05, -6.1333e-05, -1.4317e-04, -4.5240e-05,\n",
            "        -3.3021e-04, -4.2319e-06, -9.0003e-06, -8.3506e-05, -1.2338e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029653754085302353 0.001071929931640625 0.6855686902999878 0.3350004255771637 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.4238e-03, -5.6458e-04, -5.4893e-03, -7.3314e-06, -1.8120e-05,\n",
            "        -2.4378e-05, -1.9646e-04, -8.7142e-05, -1.5259e-05, -7.3891e-03,\n",
            "        -1.1826e-02, -1.4961e-05, -1.2052e-04, -1.6415e-04, -7.7095e-03,\n",
            "        -9.7179e-04, -7.7248e-05, -9.6083e-05, -2.8467e-04, -2.6779e-03,\n",
            "        -4.2152e-04, -1.2779e-04, -1.0192e-04, -1.1467e-02, -9.8877e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027363048866391182 0.0007491111755371094 0.6608149409294128 0.33669793605804443 tensor(64, device='cuda:0')\n",
            "pred tensor([-6.9618e-04, -2.8563e-04, -7.5758e-05, -1.9336e-04, -1.3661e-04,\n",
            "        -2.9984e-03, -1.6136e-03, -1.0514e-04, -3.7861e-04, -4.1413e-04,\n",
            "        -1.3983e-04, -7.6025e-01, -1.6022e-02, -7.4158e-03, -1.5140e-05,\n",
            "        -4.2796e-05, -2.4974e-05, -1.6489e-03, -5.0259e-04, -9.5367e-05,\n",
            "        -3.6895e-05, -5.0426e-05, -9.2888e-04, -1.2215e-02, -1.2789e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0282449908554554 0.0004029273986816406 0.6929059028625488 0.3382183015346527 tensor(73, device='cuda:0')\n",
            "pred tensor([-2.5253e-02, -2.1698e-02, -2.6727e-04, -1.7583e-05, -2.0146e-05,\n",
            "        -3.2949e-04, -9.7412e-01, -9.9951e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -9.3651e-04, -3.6263e-04, -5.1804e-03, -5.5008e-03,\n",
            "        -2.3975e-03, -3.9005e-03, -6.7329e-04, -1.0118e-03, -3.4504e-03,\n",
            "        -9.3162e-05, -1.5459e-03, -1.2459e-02, -1.6041e-03, -6.4230e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031910475343465805 0.0004940032958984375 0.6940526962280273 0.339127779006958 tensor(79, device='cuda:0')\n",
            "pred tensor([-8.7280e-02, -1.0406e-02, -1.5612e-03, -1.9141e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -4.1318e-04, -7.6723e-04, -1.3275e-03, -3.5763e-05,\n",
            "        -1.2338e-04, -3.2759e-04, -1.8811e-04, -1.2627e-02, -9.4938e-04,\n",
            "        -1.6747e-03, -1.0192e-04, -5.8508e-04, -4.8161e-05, -3.5515e-03,\n",
            "        -1.9159e-03, -2.1744e-04, -3.4504e-03, -2.5806e-03, -7.7248e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030125286430120468 0.0006365776062011719 0.6839689016342163 0.3360068202018738 tensor(81, device='cuda:0')\n",
            "pred tensor([-5.6505e-05, -9.1400e-03, -1.0281e-03, -2.2469e-03, -9.4175e-04,\n",
            "        -5.0354e-04, -5.4777e-05, -1.8311e-04, -1.4651e-04, -3.2377e-04,\n",
            "        -1.8883e-04, -1.7738e-04, -1.0023e-03, -1.0529e-03, -5.6648e-03,\n",
            "        -1.7929e-03, -4.3225e-04, -2.5570e-05, -4.3297e-04, -3.0935e-05,\n",
            "        -3.9935e-06, -4.1652e-04, -1.0723e-04, -3.5548e-04, -1.9431e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027226826176047325 0.00042247772216796875 0.6894433498382568 0.3347024619579315 tensor(59, device='cuda:0')\n",
            "pred tensor([-1.0391e-02, -8.9874e-03, -1.0424e-03, -1.8096e-04, -1.9407e-04,\n",
            "        -8.5068e-04, -1.0366e-03, -1.3914e-03, -3.8319e-03, -7.2098e-04,\n",
            "        -1.0312e-04, -9.5367e-05, -1.6153e-05, -7.0035e-05, -3.2759e-04,\n",
            "        -1.1692e-03, -1.8656e-05, -1.0723e-04, -1.4893e-01, -5.7268e-04,\n",
            "        -1.2550e-03, -9.5725e-05, -2.5415e-04, -6.1572e-05, -5.4777e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027058415114879608 0.0002551078796386719 0.6809064149856567 0.341666042804718 tensor(76, device='cuda:0')\n",
            "pred tensor([-6.1095e-05, -3.6955e-06, -1.8477e-05, -6.7592e-05, -6.7353e-06,\n",
            "        -5.1022e-05, -3.2449e-04, -2.1482e-04, -7.6866e-04, -1.0312e-04,\n",
            "        -2.4014e-03, -8.1897e-05, -6.1560e-04, -3.5980e-02, -9.9365e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.1206e-03, -7.3957e-04, -1.2922e-03,\n",
            "        -7.1487e-03, -6.4230e-04, -1.3769e-04, -4.9448e-04, -4.0054e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027826618403196335 0.0003695487976074219 0.7023658156394958 0.34366703033447266 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.0389e-04, -1.9133e-05, -6.3777e-05, -2.8348e-04, -3.5167e-06,\n",
            "        -2.6226e-06, -1.6093e-06, -1.7285e-06, -7.9036e-05, -2.3246e-06,\n",
            "        -2.7418e-06, -6.3181e-06, -1.5736e-05, -3.0994e-06, -2.3842e-05,\n",
            "        -1.9670e-06, -1.6034e-05, -2.2340e-04, -8.6069e-04, -9.0301e-05,\n",
            "        -4.1306e-05, -7.3910e-06, -2.6703e-05, -7.4565e-05, -2.0337e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029402464628219604 0.0012226104736328125 0.6708752512931824 0.3546898066997528 tensor(74, device='cuda:0')\n",
            "pred tensor([-1.4091e-04, -4.4584e-04, -1.1504e-05, -3.6955e-06, -2.2054e-05,\n",
            "        -5.8770e-05, -3.0184e-04, -4.7183e-04, -1.3137e-04, -1.9872e-04,\n",
            "        -1.4896e-03, -1.6344e-04, -1.5914e-05, -1.2517e-05, -6.4552e-05,\n",
            "        -1.3649e-05, -9.8944e-06, -1.7130e-04, -9.6858e-05, -3.6180e-05,\n",
            "        -2.3484e-05, -2.4021e-05, -5.2452e-06, -1.5795e-05, -7.1526e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02666418068110943 0.001201629638671875 0.6762000322341919 0.3343278467655182 tensor(56, device='cuda:0')\n",
            "pred tensor([-5.0664e-06, -1.4424e-05, -9.3877e-05, -8.3148e-05, -1.1444e-03,\n",
            "        -1.1265e-05, -1.3709e-05, -1.4198e-04, -5.5850e-05, -8.0585e-05,\n",
            "        -6.6042e-05, -6.9332e-04, -9.1016e-05, -8.8811e-06, -2.7299e-05,\n",
            "        -3.7491e-05, -2.1994e-04, -1.7285e-06, -1.4424e-05, -2.3413e-04,\n",
            "        -2.0826e-04, -2.5392e-05, -3.8776e-03, -1.8101e-03, -4.7028e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02733379229903221 0.000659942626953125 0.6595661044120789 0.3537713885307312 tensor(64, device='cuda:0')\n",
            "pred tensor([-4.8161e-05, -2.4676e-05, -1.0147e-02, -1.0000e+00, -1.0000e+00,\n",
            "        -1.5306e-03, -1.5533e-04, -1.3709e-05, -5.3883e-05, -4.8971e-04,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -7.0810e-04, -1.0151e-04,\n",
            "        -6.4230e-04, -9.1267e-04, -1.0509e-03, -7.7343e-04, -2.4247e-04,\n",
            "        -7.3671e-05, -7.7486e-04, -5.4932e-04, -1.6665e-04, -2.4152e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026440391317009926 0.00020360946655273438 0.6899348497390747 0.36658650636672974 tensor(82, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -1.7481e-03, -3.8929e-03, -1.4317e-04,\n",
            "        -5.2118e-04, -1.5783e-04, -1.5175e-04, -1.8811e-04, -3.2568e-04,\n",
            "        -1.2426e-03, -3.4729e-02, -1.2350e-03, -3.3402e-04, -3.3927e-04,\n",
            "        -8.1406e-03, -2.0742e-04, -2.1881e-02, -7.0267e-03, -6.7115e-05,\n",
            "        -2.8000e-03, -9.8486e-01, -9.7021e-01, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02832634374499321 0.0003218650817871094 0.7000877857208252 0.35826200246810913 tensor(94, device='cuda:0')\n",
            "pred tensor([-6.6280e-04, -5.7638e-05, -4.9162e-04, -5.1439e-05, -1.3030e-04,\n",
            "        -1.0900e-03, -2.0111e-04, -1.6606e-04, -2.0993e-04, -1.2481e-04,\n",
            "        -1.4651e-04, -9.0742e-04, -8.1539e-05, -2.0182e-04, -1.1063e-04,\n",
            "        -5.6088e-05, -1.7643e-05, -9.6858e-05, -1.9407e-04, -2.6011e-04,\n",
            "        -1.9944e-04, -7.5674e-04, -2.2423e-04, -1.6475e-04, -2.5225e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027955912053585052 0.00040912628173828125 0.7057418823242188 0.34925809502601624 tensor(84, device='cuda:0')\n",
            "pred tensor([-7.6294e-06, -4.2152e-04, -2.4796e-05, -5.7364e-04, -8.5235e-06,\n",
            "        -2.4796e-05, -3.2723e-05, -3.0816e-05, -3.1757e-04, -8.7142e-05,\n",
            "        -8.0466e-06, -6.0701e-04, -1.6034e-04, -1.9670e-05, -1.5295e-04,\n",
            "        -4.5598e-05, -7.3969e-05, -3.1710e-05, -1.9872e-04, -2.5368e-03,\n",
            "        -1.0834e-03, -2.0798e-02, -1.5625e-01, -3.4523e-04, -1.0473e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02992381900548935 0.00029468536376953125 0.7037639617919922 0.3437502384185791 tensor(94, device='cuda:0')\n",
            "pred tensor([-8.2493e-04, -5.2643e-04, -3.0935e-05, -1.6224e-04, -1.7653e-03,\n",
            "        -3.4666e-04, -1.5473e-04, -7.4100e-04, -4.8943e-03, -1.5640e-03,\n",
            "        -3.1769e-02, -5.7793e-04, -3.6335e-04, -9.6729e-01, -9.9951e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -2.8442e-02, -1.6518e-03, -1.4839e-03,\n",
            "        -7.8888e-03, -7.4816e-04, -2.0466e-03, -1.9872e-04, -1.7762e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.033323727548122406 0.0009746551513671875 0.672640860080719 0.3435421586036682 tensor(106, device='cuda:0')\n",
            "pred tensor([-1.1005e-03, -5.7125e-04, -3.3736e-04, -3.1054e-05, -6.0618e-05,\n",
            "        -7.5698e-06, -1.6272e-05, -1.0931e-04, -2.0587e-04, -2.3687e-04,\n",
            "        -6.7854e-04, -9.7215e-05, -4.4417e-04, -2.3823e-03, -3.6764e-04,\n",
            "        -6.5857e-02, -5.8174e-04, -1.1921e-07, -1.1325e-06, -9.7949e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.7071e-03, -1.0192e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029632119461894035 0.0022029876708984375 0.6291314959526062 0.33915120363235474 tensor(75, device='cuda:0')\n",
            "pred tensor([-0.0316, -0.0125, -0.0002, -0.0008, -0.0011, -0.0004, -0.0466, -0.0005,\n",
            "        -0.0016, -0.0008, -0.0015, -0.0013, -0.0074, -0.0001, -0.0001, -0.0005,\n",
            "        -0.0007, -0.0002, -0.0008, -0.0002, -0.0017, -0.0002, -0.0012, -0.0011,\n",
            "        -0.0011], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0291647519916296 0.0008988380432128906 0.703026294708252 0.33891966938972473 tensor(77, device='cuda:0')\n",
            "pred tensor([-3.3569e-03, -4.6802e-01, -9.7942e-04, -4.6110e-04, -4.9472e-05,\n",
            "        -2.7919e-04, -3.8700e-03, -9.8765e-05, -9.7156e-06, -1.8606e-03,\n",
            "        -5.9319e-04, -4.1246e-04, -1.6928e-04, -1.8966e-04, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -3.2544e-05,\n",
            "        -7.2122e-06, -4.2462e-04, -4.2140e-05, -8.5115e-05, -1.0729e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030069606378674507 0.00012254714965820312 0.7228524088859558 0.36588042974472046 tensor(72, device='cuda:0')\n",
            "pred tensor([-5.4407e-04, -8.3303e-04, -1.9789e-04, -3.4475e-04, -2.3592e-04,\n",
            "        -2.3365e-03, -2.2430e-03, -1.1467e-02, -1.0010e-02, -8.5115e-05,\n",
            "        -1.3661e-04, -2.0266e-06, -5.7817e-05, -2.8431e-05, -1.5793e-03,\n",
            "        -3.0880e-03, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.3914e-03,\n",
            "        -8.0287e-05, -2.0504e-04, -1.2934e-04, -1.1414e-04, -4.4942e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0312807634472847 6.771087646484375e-05 0.7207307815551758 0.35277146100997925 tensor(107, device='cuda:0')\n",
            "pred tensor([-5.5408e-04, -2.8394e-01, -9.3311e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -6.6662e-04, -3.6716e-03, -7.4673e-04, -7.1907e-03, -5.6915e-02,\n",
            "        -3.9978e-03, -1.9226e-03, -9.1457e-04, -3.6354e-03, -8.1825e-04,\n",
            "        -1.0948e-03, -6.7627e-01, -9.8633e-01, -9.9707e-01, -3.6793e-03,\n",
            "        -3.0689e-03, -5.7755e-03, -1.5764e-03, -2.5864e-03, -3.1242e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03337680920958519 0.0004506111145019531 0.6836904883384705 0.36039650440216064 tensor(109, device='cuda:0')\n",
            "pred tensor([-4.4861e-03, -5.1737e-04, -3.0303e-04, -3.5906e-04, -8.5144e-03,\n",
            "        -3.2187e-05, -1.7881e-04, -4.7028e-05, -4.9472e-05, -1.7679e-04,\n",
            "        -3.5346e-05, -8.2552e-05, -4.0233e-05, -7.8142e-05, -2.9087e-05,\n",
            "        -2.9206e-06, -1.1504e-04, -3.1376e-04, -1.7738e-04, -1.8883e-04,\n",
            "        -4.6670e-05, -4.8113e-04, -6.2275e-04, -4.6492e-05, -1.6570e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03215320408344269 0.001239776611328125 0.6515859365463257 0.3389149010181427 tensor(77, device='cuda:0')\n",
            "pred tensor([-2.4247e-04, -1.2982e-04, -4.1187e-05, -7.9036e-05, -2.4343e-04,\n",
            "        -1.2598e-03, -1.3199e-02, -3.2007e-01, -4.6005e-03, -2.3460e-03,\n",
            "        -3.5143e-04, -1.4753e-03, -6.3400e-03, -8.6746e-03, -4.6849e-05,\n",
            "        -9.7215e-05, -2.0542e-03, -4.0234e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -4.4346e-04, -2.8467e-04, -3.6407e-04, -3.0065e-04, -8.8215e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029722804203629494 0.0006427764892578125 0.6954067945480347 0.3467630445957184 tensor(99, device='cuda:0')\n",
            "pred tensor([-1.1005e-03, -9.5978e-03, -1.4591e-04, -7.1239e-04, -6.9499e-05,\n",
            "        -1.5335e-03, -2.0909e-04, -2.7418e-05, -8.6746e-03, -1.5306e-03,\n",
            "        -3.6180e-05, -2.8551e-05, -1.2445e-03, -2.1231e-04, -1.0192e-04,\n",
            "        -1.0598e-04, -1.0598e-04, -2.2650e-05, -2.8801e-04, -1.1235e-04,\n",
            "        -1.7536e-04, -7.1704e-05, -6.1560e-04, -1.6129e-02, -1.0548e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03269144147634506 0.0008935928344726562 0.6718599796295166 0.3524269461631775 tensor(79, device='cuda:0')\n",
            "pred tensor([-1.6630e-05, -1.1492e-03, -4.7821e-02, -1.6289e-03, -2.1911e-04,\n",
            "        -6.7592e-05, -2.6846e-04, -4.0627e-03, -1.3340e-04, -6.9737e-05,\n",
            "        -1.2779e-04, -7.3314e-06, -7.1704e-05, -1.0900e-03, -3.3402e-04,\n",
            "        -1.7405e-04, -1.4770e-04, -4.5753e-04, -2.9588e-04, -2.0542e-03,\n",
            "        -1.0598e-04, -2.9583e-03, -2.9411e-03, -9.9805e-01, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03237089142203331 0.000492095947265625 0.7049498558044434 0.34187430143356323 tensor(95, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -4.2319e-06, -1.4198e-04, -4.0054e-04, -1.9670e-05,\n",
            "        -2.1219e-05, -3.8087e-05, -2.0146e-05, -4.9651e-05, -7.8392e-04,\n",
            "        -4.5395e-04, -3.4828e-03, -3.3283e-04, -1.6534e-04, -3.3855e-04,\n",
            "        -9.3985e-04, -6.2764e-05, -1.7345e-05, -5.2214e-04, -8.3804e-05,\n",
            "        -1.5354e-04, -2.5606e-04, -1.2398e-03, -6.7890e-05, -2.2876e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028677787631750107 0.0008754730224609375 0.6551010608673096 0.3397437334060669 tensor(60, device='cuda:0')\n",
            "pred tensor([-6.8235e-04, -2.2392e-03, -1.5974e-03, -5.1928e-04, -1.9038e-04,\n",
            "        -1.7464e-04, -3.7050e-04, -4.2629e-04, -4.9472e-05, -1.6165e-03,\n",
            "        -8.4305e-03, -1.8740e-04, -1.4830e-04, -4.4775e-04, -4.4594e-03,\n",
            "        -1.1292e-03, -1.0633e-03, -1.1730e-04, -5.8842e-04, -8.3113e-04,\n",
            "        -4.6349e-03, -3.2258e-04, -1.9491e-04, -4.7386e-05, -2.9945e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030157780274748802 0.001224517822265625 0.6515011787414551 0.35652533173561096 tensor(77, device='cuda:0')\n",
            "pred tensor([-3.5477e-04, -4.5466e-04, -8.2159e-04, -1.5140e-05, -1.6475e-04,\n",
            "        -4.3221e-03, -3.3402e-04, -4.2057e-04, -3.4237e-03, -2.6112e-03,\n",
            "        -7.7248e-05, -5.1465e-01, -9.6777e-01, -2.1231e-04, -1.6665e-04,\n",
            "        -4.3726e-04, -3.3975e-05, -5.6171e-04, -4.2992e-03, -2.4929e-03,\n",
            "        -1.0000e+00, -1.0000e+00, -2.5010e-04, -8.5473e-05, -2.5368e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029599642381072044 4.8160552978515625e-05 0.7441844940185547 0.33551114797592163 tensor(63, device='cuda:0')\n",
            "pred tensor([-1.5533e-04, -2.3508e-04, -3.3736e-05, -1.3113e-06, -1.4842e-05,\n",
            "        -9.3460e-04, -9.9756e-01, -1.0000e+00, -1.2577e-05, -5.5432e-05,\n",
            "        -1.6870e-03, -1.0073e-04, -5.4216e-04, -3.8013e-03, -1.1091e-03,\n",
            "        -1.0330e-02, -4.4584e-04, -1.8525e-04, -1.8967e-02, -7.9012e-04,\n",
            "        -4.1008e-03, -6.4125e-03, -2.8348e-04, -5.6648e-03, -2.0828e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02699177712202072 0.00022029876708984375 0.6927248239517212 0.33893200755119324 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.2123e-02, -9.9951e-01, -1.0000e+00, -1.0000e+00, -9.8133e-04,\n",
            "        -3.6926e-03, -3.3998e-04, -1.0080e-03, -1.2665e-03, -2.7637e-03,\n",
            "        -4.1175e-04, -1.2140e-03, -6.7568e-04, -2.8954e-03, -4.0460e-04,\n",
            "        -1.8811e-04, -2.4819e-04, -2.3234e-04, -1.3618e-03, -7.4244e-04,\n",
            "        -5.7364e-04, -7.7915e-04, -9.3520e-05, -1.0920e-03, -1.2398e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026471534743905067 0.00013828277587890625 0.7198520302772522 0.3611590564250946 tensor(62, device='cuda:0')\n",
            "pred tensor([-4.9472e-05, -1.3447e-04, -1.0848e-04, -2.6166e-05, -5.6624e-06,\n",
            "        -3.9053e-04, -7.8142e-05, -1.1325e-04, -1.2684e-04, -5.6953e-03,\n",
            "        -9.9182e-05, -1.0270e-04, -2.4247e-04, -1.3554e-04, -1.8525e-04,\n",
            "        -1.8477e-05, -2.5010e-04, -2.8896e-03, -4.2629e-04, -6.7186e-04,\n",
            "        -5.8937e-04, -1.6224e-04, -8.2552e-05, -8.5473e-05, -2.2423e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025977808982133865 0.0002994537353515625 0.679429292678833 0.35027042031288147 tensor(65, device='cuda:0')\n",
            "pred tensor([-1.5488e-03, -1.8883e-04, -9.5367e-05, -4.0531e-04, -1.1139e-03,\n",
            "        -8.9228e-05, -6.5029e-05, -3.6716e-03, -2.0826e-04, -1.7464e-04,\n",
            "        -1.8139e-03, -2.4068e-04, -6.3777e-05, -1.9944e-04, -2.8133e-04,\n",
            "        -6.9714e-04, -5.0426e-05, -5.2869e-05, -3.5591e-03, -3.3021e-04,\n",
            "        -9.4593e-05, -2.1660e-04, -2.5711e-03, -1.2434e-04, -1.7204e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027307119220495224 0.0005087852478027344 0.6769020557403564 0.36053773760795593 tensor(81, device='cuda:0')\n",
            "pred tensor([-1.1311e-03, -7.9834e-02, -1.0000e+00, -1.0000e+00, -6.0577e-02,\n",
            "        -3.8319e-03, -1.0431e-04, -2.2697e-04, -5.5933e-04, -3.2258e-04,\n",
            "        -1.8060e-05, -4.6670e-05, -4.2319e-05, -7.1168e-05, -2.1148e-04,\n",
            "        -3.0899e-04, -1.6391e-05, -1.1740e-03, -2.0714e-03, -8.9502e-04,\n",
            "        -7.0572e-05, -7.6294e-05, -3.5187e-02, -2.7061e-04, -1.5945e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024449093267321587 0.0017414093017578125 0.6418529152870178 0.3503451347351074 tensor(59, device='cuda:0')\n",
            "pred tensor([-4.0680e-02, -1.3168e-02, -3.2158e-03, -9.2773e-01, -9.9609e-01,\n",
            "        -1.0000e+00, -6.4507e-03, -6.6042e-05, -5.0018e-02, -7.2746e-03,\n",
            "        -1.0023e-03, -8.1491e-04, -3.9597e-03, -2.0103e-03, -2.4681e-03,\n",
            "        -1.5545e-03, -1.1539e-03, -4.7851e-04, -1.2722e-03, -6.9332e-04,\n",
            "        -3.5977e-04, -9.1457e-04, -4.0603e-04, -6.6299e-03, -7.4280e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028682760894298553 0.001583099365234375 0.6558175683021545 0.34087711572647095 tensor(83, device='cuda:0')\n",
            "64\n",
            "pred tensor([-9.9540e-05, -9.6035e-04, -8.1062e-04, -1.1063e-04, -3.8743e-06,\n",
            "        -1.6153e-05, -1.1367e-04, -2.0142e-03, -1.0806e-04, -6.7043e-04,\n",
            "        -8.8440e-02, -5.1117e-04, -1.7464e-05, -2.5988e-05, -7.3135e-05,\n",
            "        -3.6955e-06, -6.2287e-05, -6.3515e-04, -1.8997e-02, -9.9951e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -3.8700e-03, -1.3971e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028780870139598846 0.0005202293395996094 0.7038298845291138 0.347345769405365 tensor(73, device='cuda:0')\n",
            "pred tensor([-1.5808e-02, -3.9196e-04, -1.3661e-04, -2.0828e-03, -4.2224e-04,\n",
            "        -9.5444e-03, -1.0391e-02, -2.9469e-03, -1.1340e-01, -2.7199e-03,\n",
            "        -1.0967e-03, -6.0387e-03, -5.7125e-04, -4.4847e-04, -3.3998e-04,\n",
            "        -5.6000e-03, -2.3975e-03, -1.0473e-04, -1.9058e-02, -1.7761e-02,\n",
            "        -1.3611e-02, -2.5916e-04, -2.4676e-05, -2.1148e-04, -2.4429e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02820218913257122 0.0004405975341796875 0.7041204571723938 0.33587515354156494 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.2827e-04, -4.7684e-06, -1.1206e-05, -1.0300e-03, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -1.1902e-03, -1.4839e-03, -9.9902e-01,\n",
            "        -7.5732e-01, -2.7145e-02, -4.4847e-04, -1.5354e-04, -2.3973e-04,\n",
            "        -1.1444e-03, -3.4070e-04, -2.8467e-04, -3.7003e-03, -2.4815e-03,\n",
            "        -2.5606e-04, -2.0027e-04, -2.6011e-04, -2.6417e-04, -1.8835e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02616480551660061 0.00023126602172851562 0.7222237586975098 0.34109243750572205 tensor(72, device='cuda:0')\n",
            "pred tensor([-1.0729e-05, -1.2426e-03, -1.8883e-04, -3.5143e-04, -1.0004e-03,\n",
            "        -8.1825e-04, -1.8762e-01, -4.9255e-02, -1.0000e+00, -1.0000e+00,\n",
            "        -3.1616e-02, -3.7289e-03, -7.2168e-01, -3.5614e-02, -2.2110e-02,\n",
            "        -1.0696e-02, -1.1688e-02, -9.3985e-04, -1.1721e-03, -4.8409e-03,\n",
            "        -1.6434e-02, -2.3087e-02, -3.6693e-04, -1.0653e-03, -2.4967e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02959778718650341 0.0006170272827148438 0.6883655786514282 0.3462962508201599 tensor(95, device='cuda:0')\n",
            "pred tensor([-4.4891e-02, -3.8671e-04, -1.5533e-04, -1.9722e-03, -2.5010e-04,\n",
            "        -1.2589e-04, -1.4019e-03, -5.2414e-03, -1.2922e-03, -7.0953e-04,\n",
            "        -5.8212e-03, -8.5115e-05, -1.3227e-03, -1.6606e-04, -1.5473e-04,\n",
            "        -1.4889e-04, -7.3013e-03, -2.6531e-03, -1.9608e-03, -8.0338e-03,\n",
            "        -7.9632e-04, -2.4438e-04, -1.4770e-04, -5.6305e-03, -1.2865e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029532266780734062 0.0007877349853515625 0.686947762966156 0.34076428413391113 tensor(75, device='cuda:0')\n",
            "pred tensor([-7.4673e-04, -5.0426e-05, -3.6049e-04, -3.8290e-04, -4.0352e-05,\n",
            "        -1.5533e-04, -8.1491e-04, -9.8348e-06, -3.7909e-05, -2.3782e-04,\n",
            "        -6.0797e-06, -1.5056e-04, -2.9469e-03, -3.5346e-05, -5.9426e-05,\n",
            "        -8.6844e-05, -1.3113e-05, -3.3545e-04, -1.1325e-05, -5.2452e-05,\n",
            "        -4.3333e-05, -9.9805e-01, -1.0000e+00, -1.0443e-03, -4.0674e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02819252945482731 0.0007429122924804688 0.679843544960022 0.34957656264305115 tensor(105, device='cuda:0')\n",
            "pred tensor([-6.7353e-05, -4.9889e-05, -4.1008e-05, -2.5690e-05, -2.3246e-06,\n",
            "        -9.1732e-05, -1.4710e-04, -2.3007e-05, -3.1018e-04, -9.0933e-04,\n",
            "        -1.8525e-04, -6.5374e-04, -2.9981e-05, -1.1742e-05, -1.1021e-04,\n",
            "        -1.8382e-04, -4.8208e-04, -2.7275e-04, -1.0931e-04, -1.6193e-03,\n",
            "        -3.1257e-04, -5.3704e-05, -1.9608e-03, -7.2746e-03, -1.7607e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02834474667906761 0.0020008087158203125 0.6392666101455688 0.33927205204963684 tensor(81, device='cuda:0')\n",
            "pred tensor([-7.1406e-05, -6.9201e-05, -6.2287e-05, -3.4738e-04, -6.0368e-04,\n",
            "        -1.1820e-04, -5.5599e-04, -9.3651e-04, -4.7112e-04, -5.2035e-05,\n",
            "        -2.8348e-04, -1.3030e-04, -1.3661e-04, -1.4038e-02, -4.5133e-04,\n",
            "        -2.9373e-04, -6.3229e-04, -3.6926e-03, -1.0312e-04, -7.8249e-04,\n",
            "        -1.5697e-03, -1.6665e-04, -5.2738e-04, -2.7275e-04, -3.8218e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027754582464694977 0.0005369186401367188 0.679774284362793 0.35593125224113464 tensor(84, device='cuda:0')\n",
            "pred tensor([-7.5996e-05, -3.5644e-05, -4.1485e-05, -1.4324e-03, -9.9658e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.2147e-04, -1.4842e-05, -2.1565e-04,\n",
            "        -1.1997e-03, -7.9298e-04, -7.0333e-05, -2.6584e-05, -3.9816e-04,\n",
            "        -2.8133e-04, -9.0599e-06, -7.7486e-06, -5.8293e-05, -1.6224e-04,\n",
            "        -4.1580e-03, -1.3351e-03, -3.8075e-04, -2.5821e-04, -6.6578e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03184539079666138 2.0503997802734375e-05 0.7678617238998413 0.3438967764377594 tensor(94, device='cuda:0')\n",
            "pred tensor([-9.9121e-01, -9.9902e-01, -1.2243e-04, -3.3617e-05, -6.9141e-06,\n",
            "        -1.2219e-05, -1.3769e-04, -1.0319e-03, -3.9506e-04, -3.8683e-05,\n",
            "        -7.7546e-05, -4.4346e-05, -5.6922e-05, -8.1062e-06, -5.9009e-06,\n",
            "        -1.3530e-05, -4.5300e-06, -1.4424e-05, -2.7704e-04, -1.5557e-05,\n",
            "        -5.5432e-06, -2.3651e-03, -2.1954e-03, -7.5459e-05, -2.8133e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031698454171419144 1.239776611328125e-05 0.7534881830215454 0.3384987711906433 tensor(75, device='cuda:0')\n",
            "pred tensor([-4.4942e-04, -3.8981e-04, -7.0572e-04, -3.9291e-04, -1.6415e-04,\n",
            "        -5.6922e-05, -7.0429e-04, -6.3848e-04, -1.1867e-04, -7.9155e-04,\n",
            "        -3.8600e-04, -1.9431e-05, -2.4819e-04, -7.6592e-05, -1.8775e-05,\n",
            "        -4.8113e-04, -2.0428e-03, -3.6597e-05, -1.3232e-04, -5.3048e-06,\n",
            "        -1.2875e-05, -1.3769e-04, -1.0967e-03, -4.6670e-05, -2.3246e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031312234699726105 9.107589721679688e-05 0.7478793263435364 0.34987831115722656 tensor(104, device='cuda:0')\n",
            "pred tensor([-1.8668e-04, -5.8508e-04, -5.2118e-04, -8.6129e-05, -1.0353e-04,\n",
            "        -1.3428e-03, -1.5593e-04, -4.6921e-03, -3.1719e-03, -3.0899e-04,\n",
            "        -3.2723e-05, -1.8477e-05, -3.4153e-05, -8.7118e-04, -2.3317e-04,\n",
            "        -2.5129e-04, -5.2795e-03, -2.9254e-04, -9.1839e-04, -3.1376e-04,\n",
            "        -1.2732e-04, -4.7660e-04, -5.7602e-04, -3.2640e-04, -1.1772e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02867146022617817 0.00022649765014648438 0.7099904417991638 0.36898571252822876 tensor(87, device='cuda:0')\n",
            "pred tensor([-6.3777e-05, -2.1660e-04, -3.0541e-04, -3.5226e-05, -8.3506e-05,\n",
            "        -3.4118e-04, -8.1825e-04, -1.6909e-03, -2.5010e-04, -1.1578e-03,\n",
            "        -2.3758e-02, -5.0259e-04, -7.1096e-04, -9.0218e-04, -3.8548e-03,\n",
            "        -1.6098e-02, -2.0020e-02, -2.9411e-03, -4.4942e-04, -9.5010e-05,\n",
            "        -4.3559e-04, -1.0473e-04, -1.0192e-04, -1.4317e-04, -9.6416e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03245638310909271 0.0004210472106933594 0.7328047156333923 0.36108896136283875 tensor(111, device='cuda:0')\n",
            "pred tensor([-2.9564e-05, -1.2934e-04, -2.5368e-03, -2.6932e-03, -3.7861e-04,\n",
            "        -2.4521e-02, -5.3314e-02, -6.7078e-02, -2.1155e-01, -1.1246e-02,\n",
            "        -1.1139e-02, -4.2892e-04, -4.0936e-04, -3.7408e-04, -1.2482e-02,\n",
            "        -1.0443e-03, -5.1212e-04, -9.3794e-04, -3.2410e-02, -1.6584e-03,\n",
            "        -5.7817e-05, -1.4544e-04, -9.7266e-01, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029590057209134102 0.0010280609130859375 0.6606667637825012 0.35398802161216736 tensor(74, device='cuda:0')\n",
            "pred tensor([-4.5240e-05, -4.3511e-05, -3.1352e-05, -2.7490e-04, -3.2544e-05,\n",
            "        -1.0729e-06, -3.5226e-05, -3.7789e-05, -1.2064e-03, -1.2887e-04,\n",
            "        -2.3842e-07, -1.1545e-04, -3.8743e-06, -9.9540e-06, -7.7486e-07,\n",
            "        -3.5167e-06, -3.7193e-05, -4.8876e-06, -6.3777e-06, -7.5102e-06,\n",
            "        -7.3969e-05, -2.5868e-05, -2.2340e-04, -3.5167e-06, -1.0639e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029193196445703506 0.0012187957763671875 0.663676917552948 0.3471573293209076 tensor(61, device='cuda:0')\n",
            "pred tensor([-5.6763e-03, -1.7464e-05, -7.9274e-06, -1.9073e-06, -9.9540e-06,\n",
            "        -4.7088e-06, -4.9651e-05, -4.6313e-05, -3.1173e-05, -1.5497e-06,\n",
            "        -5.3644e-07, -6.0618e-05, -1.6737e-04, -1.3292e-05, -1.5140e-05,\n",
            "        -1.7059e-04, -1.3709e-04, -1.9670e-06, -2.3842e-06, -6.1095e-05,\n",
            "        -7.8678e-06, -1.8477e-06, -1.0944e-01, -2.1911e-04, -7.5256e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03214709088206291 0.001537322998046875 0.6533761024475098 0.33930203318595886 tensor(81, device='cuda:0')\n",
            "pred tensor([-4.9114e-05, -5.1856e-06, -4.7922e-05, -8.2552e-05, -8.0943e-05,\n",
            "        -5.4240e-06, -4.7088e-06, -3.8683e-05, -3.5286e-04, -9.9512e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.8060e-05, -1.0353e-04,\n",
            "        -2.6631e-04, -4.1962e-05, -1.0389e-04, -1.8954e-05, -2.7955e-05,\n",
            "        -1.6224e-04, -9.2447e-05, -8.5592e-04, -1.1104e-04, -3.1996e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029353806748986244 0.0001354217529296875 0.741909921169281 0.3544524610042572 tensor(74, device='cuda:0')\n",
            "pred tensor([-7.3195e-04, -5.1260e-05, -1.6809e-05, -5.5850e-05, -2.4300e-03,\n",
            "        -2.7676e-03, -2.3460e-03, -3.2928e-02, -5.6505e-05, -1.0031e-04,\n",
            "        -1.2684e-04, -2.1482e-04, -2.0993e-04, -1.9407e-04, -4.7684e-06,\n",
            "        -2.1994e-04, -3.5763e-04, -2.6846e-04, -4.9472e-05, -3.6061e-05,\n",
            "        -4.3726e-04, -3.6359e-05, -1.2875e-05, -2.3413e-04, -1.1265e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0292283296585083 7.915496826171875e-05 0.7191729545593262 0.35927531123161316 tensor(79, device='cuda:0')\n",
            "pred tensor([-3.4070e-04, -8.7857e-05, -4.9114e-05, -7.7248e-05, -2.9206e-06,\n",
            "        -3.4511e-05, -4.5240e-05, -7.4267e-05, -1.4067e-05, -4.4703e-06,\n",
            "        -2.6727e-04, -4.6134e-05, -1.1146e-05, -8.9407e-07, -5.0068e-06,\n",
            "        -9.3162e-05, -2.0862e-05, -4.5896e-06, -1.1456e-04, -1.4830e-04,\n",
            "        -5.7161e-05, -1.9848e-05, -6.5267e-05, -5.6000e-03, -6.7711e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030499273911118507 0.00021076202392578125 0.6847785711288452 0.3461996018886566 tensor(84, device='cuda:0')\n",
            "pred tensor([-2.1827e-04, -7.4959e-04, -2.1994e-04, -9.1016e-05, -2.1398e-04,\n",
            "        -2.3901e-05, -7.0429e-04, -1.8896e-01, -5.6494e-01, -9.9951e-01,\n",
            "        -1.0000e+00, -1.1358e-03, -4.4525e-05, -2.9240e-03, -1.3298e-02,\n",
            "        -4.2305e-03, -2.6226e-04, -6.4254e-05, -1.6451e-05, -5.3883e-05,\n",
            "        -1.9848e-05, -5.5542e-03, -2.1534e-03, -5.2214e-04, -1.2684e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029109718278050423 0.00032520294189453125 0.6736297607421875 0.3415004312992096 tensor(87, device='cuda:0')\n",
            "pred tensor([-4.9889e-05, -6.2287e-05, -5.9605e-07, -1.0973e-04, -5.3883e-05,\n",
            "        -1.5080e-05, -1.3113e-06, -5.0664e-06, -6.0201e-06, -1.1367e-04,\n",
            "        -5.3644e-07, -9.9540e-06, -1.0598e-04, -1.5724e-04, -4.0603e-04,\n",
            "        -1.2147e-04, -1.2722e-03, -2.1482e-04, -2.3782e-04, -2.1660e-04,\n",
            "        -2.2340e-04, -1.7202e-04, -1.9944e-04, -4.7755e-04, -2.8431e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028465094044804573 0.0008625984191894531 0.6847149729728699 0.3378899097442627 tensor(72, device='cuda:0')\n",
            "pred tensor([-1.4663e-05, -4.1485e-05, -4.6849e-05, -5.4407e-04, -5.0020e-04,\n",
            "        -2.3594e-03, -7.9918e-04, -2.5868e-05, -6.5029e-05, -7.5459e-05,\n",
            "        -7.0930e-06, -2.0444e-05, -2.5749e-05, -5.4836e-06, -3.3200e-05,\n",
            "        -3.5763e-07, -5.0426e-05, -6.6519e-04, -6.4373e-06, -1.0723e-04,\n",
            "        -3.9983e-04, -1.2517e-06, -9.2447e-05, -2.1458e-06, -1.7285e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028030477464199066 0.0027408599853515625 0.6339846253395081 0.3489735722541809 tensor(79, device='cuda:0')\n",
            "pred tensor([-5.5008e-03, -5.5265e-04, -6.4254e-05, -3.2187e-04, -9.6989e-04,\n",
            "        -1.0231e-02, -2.0050e-02, -3.4928e-04, -3.1495e-04, -4.4155e-04,\n",
            "        -2.3460e-03, -1.1894e-02, -7.9298e-04, -1.6489e-03, -3.9673e-04,\n",
            "        -5.2834e-04, -3.5763e-05, -4.4167e-05, -9.6500e-05, -1.1206e-05,\n",
            "        -1.9670e-05, -3.7074e-05, -7.9880e-03, -8.6182e-01, -4.3976e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02862035483121872 0.0012636184692382812 0.6701616644859314 0.3447989821434021 tensor(95, device='cuda:0')\n",
            "pred tensor([-2.4872e-03, -6.0606e-04, -1.0723e-04, -5.2869e-05, -5.7459e-04,\n",
            "        -6.9141e-01, -9.9658e-01, -1.0000e+00, -2.3508e-04, -1.2159e-05,\n",
            "        -1.9908e-05, -8.6427e-06, -1.0473e-04, -1.3232e-04, -6.1178e-04,\n",
            "        -1.1768e-03, -1.4753e-03, -1.0386e-03, -8.5602e-03, -2.4438e-04,\n",
            "        -9.9609e-01, -9.2236e-01, -2.2049e-02, -6.5899e-04, -8.1421e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02896500565111637 0.00012063980102539062 0.742256224155426 0.3461347222328186 tensor(89, device='cuda:0')\n",
            "pred tensor([-2.4629e-04, -4.7278e-04, -1.4954e-03, -5.9891e-04, -8.8215e-05,\n",
            "        -1.0848e-04, -4.3488e-04, -3.8075e-04, -3.1719e-03, -2.6627e-02,\n",
            "        -6.4230e-04, -1.0550e-05, -3.5477e-04, -1.9491e-04, -4.0352e-05,\n",
            "        -6.4015e-05, -1.3561e-03, -2.3139e-04, -4.8685e-04, -4.5562e-04,\n",
            "        -3.4273e-05, -1.3340e-04, -2.1915e-03, -6.5517e-04, -1.9646e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02654508501291275 1.1444091796875e-05 0.8196679353713989 0.34216558933258057 tensor(64, device='cuda:0')\n",
            "pred tensor([-1.6344e-04, -1.4186e-05, -5.3883e-05, -5.9605e-06, -5.1079e-03,\n",
            "        -1.0353e-04, -8.6486e-05, -3.3112e-03, -9.6035e-04, -2.7490e-04,\n",
            "        -2.2602e-03, -5.7526e-03, -3.0200e-01, -6.8506e-01, -2.5977e-01,\n",
            "        -8.9874e-03, -1.0000e+00, -9.4482e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -3.0398e-06, -3.1054e-05, -6.3133e-03, -5.0316e-03, -4.2076e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029060978442430496 7.581710815429688e-05 0.7294376492500305 0.34354251623153687 tensor(87, device='cuda:0')\n",
            "pred tensor([-1.8966e-04, -9.5367e-05, -2.5868e-05, -1.2529e-04, -1.2004e-04,\n",
            "        -5.5599e-04, -1.0908e-05, -2.6941e-04, -2.7676e-03, -8.6844e-05,\n",
            "        -3.4404e-04, -2.5153e-05, -6.0141e-05, -1.4191e-03, -8.8513e-05,\n",
            "        -4.2498e-05, -4.7684e-07, -4.9472e-06, -3.2783e-06, -2.8348e-04,\n",
            "        -1.2827e-04, -7.2837e-05, -5.0247e-05, -5.6028e-06, -1.5116e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027178684249520302 0.00037670135498046875 0.6854974031448364 0.3428085744380951 tensor(93, device='cuda:0')\n",
            "pred tensor([-1.7333e-04, -4.9353e-04, -4.1819e-04, -1.5473e-04, -2.0206e-05,\n",
            "        -7.8678e-06, -9.2387e-06, -9.3520e-05, -7.5698e-06, -5.7817e-05,\n",
            "        -8.2779e-04, -6.7854e-04, -8.6594e-04, -9.6989e-04, -7.0333e-06,\n",
            "        -9.3579e-06, -9.5367e-05, -5.9187e-05, -3.7789e-05, -1.0788e-05,\n",
            "        -9.2089e-05, -2.4343e-04, -1.1921e-06, -5.6624e-06, -2.2471e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031246094033122063 0.0013027191162109375 0.6459205150604248 0.3406170904636383 tensor(90, device='cuda:0')\n",
            "pred tensor([-2.8682e-04, -2.0742e-04, -1.0004e-03, -2.7061e-04, -1.5783e-04,\n",
            "        -1.4937e-04, -4.3488e-03, -5.3883e-04, -1.4353e-03, -1.1692e-03,\n",
            "        -1.3709e-05, -1.0185e-03, -1.2529e-04, -3.1710e-05, -4.2677e-05,\n",
            "        -2.6822e-05, -1.8775e-05, -1.6689e-05, -4.0054e-04, -2.7239e-05,\n",
            "        -1.5497e-06, -6.5267e-05, -1.1734e-02, -1.9038e-04, -3.7265e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025328315794467926 0.00357818603515625 0.6122667193412781 0.35221970081329346 tensor(74, device='cuda:0')\n",
            "pred tensor([-9.6500e-05, -6.3515e-03, -2.8553e-03, -1.5850e-03, -1.5056e-04,\n",
            "        -9.9268e-01, -9.9951e-01, -1.0000e+00, -7.0870e-05, -1.3089e-04,\n",
            "        -2.4724e-04, -1.7405e-04, -4.2992e-03, -3.4809e-04, -9.4938e-04,\n",
            "        -9.7215e-05, -1.3800e-03, -5.8365e-04, -4.6730e-04, -1.5793e-03,\n",
            "        -4.2892e-04, -2.5690e-05, -1.1367e-04, -2.2202e-02, -8.5258e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028521506115794182 0.0005979537963867188 0.6794777512550354 0.34179309010505676 tensor(76, device='cuda:0')\n",
            "pred tensor([-5.5542e-03, -2.7733e-03, -1.9336e-04, -1.5903e-04, -8.7280e-03,\n",
            "        -6.9475e-04, -5.4893e-03, -1.8239e-04, -7.8430e-03, -6.4502e-01,\n",
            "        -1.0980e-01, -3.8853e-03, -1.0080e-03, -1.6809e-01, -1.1034e-03,\n",
            "        -7.7629e-04, -4.1318e-04, -2.3071e-02, -1.7532e-02, -5.3453e-04,\n",
            "        -3.0422e-04, -4.6921e-03, -2.2831e-03, -2.0742e-04, -3.4237e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026566118001937866 0.0004329681396484375 0.6986928582191467 0.33697962760925293 tensor(65, device='cuda:0')\n",
            "pred tensor([-3.5906e-04, -3.7079e-03, -1.5473e-04, -1.1950e-03, -5.9903e-05,\n",
            "        -4.2224e-04, -4.6997e-03, -3.6144e-03, -8.1241e-05, -3.2949e-04,\n",
            "        -2.9297e-03, -5.4407e-04, -1.1034e-03, -1.1414e-04, -2.7955e-05,\n",
            "        -1.9760e-03, -4.6921e-03, -1.7679e-04, -4.6539e-03, -9.8730e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.8167e-04, -1.4043e-04, -7.9632e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02621663734316826 0.00010251998901367188 0.7184953689575195 0.3635048568248749 tensor(84, device='cuda:0')\n",
            "pred tensor([-2.3878e-04, -1.2924e-02, -8.6260e-04, -4.1809e-03, -1.4420e-02,\n",
            "        -3.1495e-04, -4.1246e-04, -8.1539e-05, -3.5942e-05, -2.0337e-04,\n",
            "        -9.7752e-04, -1.2598e-03, -8.2195e-05, -9.6858e-05, -2.9206e-05,\n",
            "        -2.7409e-03, -2.3605e-02, -3.5071e-04, -1.3752e-03, -4.6997e-02,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -3.5834e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026367362588644028 0.00021219253540039062 0.7017187476158142 0.33539333939552307 tensor(64, device='cuda:0')\n",
            "pred tensor([-1.1021e-04, -6.7890e-05, -9.0659e-05, -1.9729e-05, -2.9135e-04,\n",
            "        -4.7755e-04, -2.0337e-04, -7.2539e-05, -1.6737e-04, -7.6294e-05,\n",
            "        -1.9646e-04, -1.1034e-03, -2.5821e-04, -1.8060e-05, -8.6486e-05,\n",
            "        -3.5548e-04, -2.2602e-03, -2.5725e-04, -2.8849e-05, -4.0829e-05,\n",
            "        -4.1318e-04, -1.9407e-04, -2.4915e-04, -8.5831e-05, -8.3804e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02736476995050907 0.0005540847778320312 0.671064019203186 0.3485395610332489 tensor(85, device='cuda:0')\n",
            "pred tensor([-3.0661e-04, -2.9011e-03, -3.0100e-05, -2.9862e-05, -1.3618e-03,\n",
            "        -7.6437e-04, -1.3340e-04, -3.3855e-04, -4.4680e-04, -5.3287e-05,\n",
            "        -1.2646e-03, -2.3499e-03, -1.0900e-03, -6.0380e-05, -3.1796e-03,\n",
            "        -4.5753e-04, -9.0408e-04, -4.9324e-03, -7.3373e-05, -3.2258e-04,\n",
            "        -2.1744e-04, -4.4346e-04, -4.2629e-04, -1.1802e-05, -3.6049e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026394028216600418 0.0007967948913574219 0.6979256272315979 0.33503401279449463 tensor(67, device='cuda:0')\n",
            "pred tensor([-9.8038e-03, -1.0270e-04, -1.4091e-04, -1.2147e-04, -1.8096e-04,\n",
            "        -2.8920e-04, -2.9068e-02, -2.1482e-04, -7.4530e-04, -1.6797e-04,\n",
            "        -3.6192e-04, -3.9291e-04, -1.3769e-04, -9.6083e-05, -1.2875e-05,\n",
            "        -2.5129e-04, -1.0312e-04, -5.4840e-02, -6.1035e-02, -6.8235e-04,\n",
            "        -1.8525e-04, -1.7107e-05, -3.9339e-06, -1.1325e-05, -2.1565e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025978241115808487 0.00021314620971679688 0.7335542440414429 0.3385276794433594 tensor(77, device='cuda:0')\n",
            "pred tensor([-1.6606e-04, -6.5804e-05, -5.4407e-04, -3.8743e-04, -2.1660e-04,\n",
            "        -2.1696e-05, -7.6221e-01, -1.0000e+00, -1.0000e+00, -8.5831e-05,\n",
            "        -8.1539e-05, -3.2306e-04, -6.5899e-04, -1.2779e-04, -9.0742e-04,\n",
            "        -2.1231e-04, -6.9046e-04, -8.8871e-05, -1.1277e-04, -1.8668e-04,\n",
            "        -1.3723e-03, -2.7776e-05, -2.5821e-04, -2.6321e-04, -2.0199e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026483383029699326 0.0011148452758789062 0.661255955696106 0.3342970609664917 tensor(68, device='cuda:0')\n",
            "pred tensor([-9.4833e-03, -1.1757e-02, -4.9639e-04, -2.6932e-03, -7.9155e-04,\n",
            "        -3.0937e-03, -9.6858e-05, -8.3303e-04, -1.4091e-04, -9.7179e-04,\n",
            "        -1.8063e-03, -9.8765e-05, -2.6846e-04, -5.2869e-05, -2.3592e-04,\n",
            "        -1.7679e-04, -3.1967e-03, -1.4186e-05, -3.0398e-06, -5.2035e-05,\n",
            "        -1.0610e-05, -3.1018e-04, -1.8969e-03, -3.0339e-05, -1.7810e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027924198657274246 0.0006771087646484375 0.6696630716323853 0.3435196876525879 tensor(92, device='cuda:0')\n",
            "65\n",
            "pred tensor([-2.5392e-05, -1.9717e-04, -4.1842e-05, -1.2732e-04, -8.4817e-05,\n",
            "        -1.4842e-05, -1.1635e-04, -1.2195e-04, -3.5620e-04, -4.4084e-04,\n",
            "        -5.4836e-06, -7.5161e-05, -6.9737e-05, -3.8981e-04, -1.1504e-04,\n",
            "        -2.7835e-05, -5.0831e-04, -1.8966e-04, -2.0564e-05, -1.2100e-05,\n",
            "        -4.6635e-04, -1.2522e-03, -1.2474e-03, -3.2187e-06, -4.9889e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029236989095807076 0.0004382133483886719 0.6756904125213623 0.3444880247116089 tensor(88, device='cuda:0')\n",
            "pred tensor([-2.9564e-05, -5.0843e-05, -1.1414e-04, -1.0598e-04, -4.2725e-04,\n",
            "        -5.6922e-05, -6.8665e-04, -1.1421e-02, -5.8365e-04, -3.4273e-05,\n",
            "        -4.3154e-05, -2.6264e-03, -3.9577e-04, -6.1178e-04, -2.8193e-05,\n",
            "        -4.8518e-05, -1.1963e-04, -1.0598e-04, -3.1877e-04, -8.9228e-05,\n",
            "        -9.2697e-03, -9.9756e-01, -9.9951e-01, -1.0431e-05, -6.8545e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02886415831744671 0.00017261505126953125 0.6973819136619568 0.3478599786758423 tensor(84, device='cuda:0')\n",
            "pred tensor([-1.0204e-03, -1.2386e-04, -1.6797e-04, -2.7790e-03, -5.4777e-05,\n",
            "        -2.8014e-06, -1.8477e-05, -6.1393e-06, -4.1723e-06, -6.6578e-05,\n",
            "        -1.7932e-01, -1.0000e+00, -1.0000e+00, -4.1723e-07, -5.9605e-08,\n",
            "        -2.1076e-04, -7.2479e-03, -1.1206e-03, -7.8082e-06, -4.5419e-05,\n",
            "        -1.1311e-03, -7.2241e-04, -2.9016e-04, -8.9586e-05, -2.3975e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02521800436079502 0.0002884864807128906 0.659807562828064 0.3350122570991516 tensor(69, device='cuda:0')\n",
            "pred tensor([-1.0292e-02, -5.6000e-03, -5.4741e-04, -2.8667e-03, -5.3444e-03,\n",
            "        -1.8537e-05, -4.7207e-05, -3.6597e-05, -3.8147e-06, -1.4150e-04,\n",
            "        -5.9175e-04, -2.3663e-05, -6.3721e-01, -5.9229e-01, -6.4516e-04,\n",
            "        -1.1692e-03, -3.6836e-04, -1.8835e-05, -1.3924e-04, -4.1842e-05,\n",
            "        -5.5695e-04, -3.3057e-01, -9.3750e-02, -4.6539e-03, -1.6012e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02285058982670307 0.0018796920776367188 0.6376276612281799 0.3477957844734192 tensor(58, device='cuda:0')\n",
            "pred tensor([-9.1374e-05, -9.0301e-05, -1.7242e-03, -2.4378e-05, -5.1260e-06,\n",
            "        -4.2677e-05, -1.0633e-03, -5.1308e-04, -1.3489e-01, -8.8770e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.6129e-02, -3.4561e-03, -2.0504e-04,\n",
            "        -6.3629e-03, -1.5175e-04, -6.5029e-05, -5.9891e-04, -3.5286e-04,\n",
            "        -1.6928e-04, -2.6011e-04, -3.6478e-05, -5.9080e-04, -7.6008e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025200871750712395 0.0005965232849121094 0.667083740234375 0.3367389142513275 tensor(66, device='cuda:0')\n",
            "pred tensor([-1.2741e-03, -2.5511e-04, -3.6049e-04, -1.4365e-04, -5.7817e-05,\n",
            "        -7.7343e-04, -4.7851e-04, -1.2052e-04, -3.1257e-04, -2.2659e-03,\n",
            "        -2.2964e-03, -1.2147e-04, -1.3185e-04, -1.8418e-05, -1.8282e-03,\n",
            "        -4.5037e-04, -1.2207e-03, -6.2513e-04, -1.3769e-04, -8.9693e-04,\n",
            "        -2.4438e-04, -1.3876e-04, -9.5725e-05, -1.4484e-04, -1.2779e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025930948555469513 0.0007815361022949219 0.667799711227417 0.3381642997264862 tensor(74, device='cuda:0')\n",
            "pred tensor([-7.9298e-04, -1.8811e-04, -3.1638e-04, -3.0100e-05, -3.4809e-04,\n",
            "        -5.7602e-04, -1.1692e-03, -1.0514e-04, -4.1413e-04, -1.8892e-03,\n",
            "        -6.5565e-05, -8.0287e-05, -1.0514e-04, -3.0899e-04, -1.3914e-03,\n",
            "        -3.2043e-03, -4.1161e-03, -4.8971e-04, -2.1400e-03, -9.2163e-03,\n",
            "        -8.9502e-04, -8.7142e-05, -6.2525e-05, -1.1551e-02, -2.8253e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026634899899363518 0.0008087158203125 0.6594879627227783 0.3433855473995209 tensor(80, device='cuda:0')\n",
            "pred tensor([-1.6260e-03, -2.3973e-04, -6.5765e-03, -1.6415e-04, -1.5855e-05,\n",
            "        -1.1425e-03, -2.3782e-04, -8.2850e-05, -9.5010e-05, -6.8521e-04,\n",
            "        -7.0000e-04, -3.0220e-05, -1.2871e-02, -7.6008e-04, -3.1257e-04,\n",
            "        -5.5408e-04, -7.6008e-04, -5.5817e-02, -1.0353e-04, -5.0640e-04,\n",
            "        -1.3602e-04, -1.3983e-04, -1.5497e-05, -9.7156e-06, -5.3287e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.022750983014702797 0.00018072128295898438 0.6961391568183899 0.32974135875701904 tensor(43, device='cuda:0')\n",
            "pred tensor([-1.2434e-04, -1.0729e-05, -2.4841e-01, -5.8887e-01, -9.9707e-01,\n",
            "        -1.0000e+00, -2.3246e-06, -9.8348e-06, -4.6082e-03, -2.4724e-04,\n",
            "        -3.8087e-05, -7.1526e-07, -1.1683e-04, -3.1829e-05, -1.3232e-04,\n",
            "        -7.0333e-06, -8.3506e-05, -3.4928e-04, -1.3769e-04, -5.1022e-04,\n",
            "        -5.3358e-04, -4.6468e-04, -5.0449e-04, -1.4238e-03, -1.3618e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024345220997929573 0.0002846717834472656 0.6832373738288879 0.3336240351200104 tensor(64, device='cuda:0')\n",
            "pred tensor([-2.9068e-03, -1.5099e-02, -1.8323e-01, -3.5889e-02, -9.9023e-01,\n",
            "        -8.7402e-01, -4.2572e-02, -2.6221e-01, -2.7800e-04, -7.7148e-02,\n",
            "        -9.2578e-01, -1.4105e-03, -1.1986e-02, -3.9363e-04, -9.1016e-05,\n",
            "        -5.5838e-04, -2.6727e-04, -1.5249e-03, -8.4591e-04, -8.6441e-03,\n",
            "        -7.4365e-01, -9.7900e-01, -5.0316e-03, -5.2719e-03, -8.3148e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02436959743499756 0.000431060791015625 0.6584782600402832 0.3357332646846771 tensor(70, device='cuda:0')\n",
            "pred tensor([-1.2789e-03, -2.5320e-04, -1.2522e-03, -2.4776e-03, -7.1068e-03,\n",
            "        -3.0661e-04, -1.4048e-03, -1.0986e-03, -1.0204e-03, -1.8282e-03,\n",
            "        -4.5204e-04, -9.9658e-01, -1.0000e+00, -1.0000e+00, -4.6313e-05,\n",
            "        -3.8505e-05, -2.3782e-04, -5.2273e-05, -7.5758e-05, -1.7824e-03,\n",
            "        -1.8139e-03, -1.4424e-04, -3.3200e-05, -7.2002e-05, -2.8172e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028823677450418472 0.0010509490966796875 0.6544160842895508 0.3612339496612549 tensor(92, device='cuda:0')\n",
            "pred tensor([-1.6937e-03, -1.3447e-04, -1.9968e-05, -1.2529e-04, -2.6226e-04,\n",
            "        -2.4438e-06, -6.9499e-05, -3.8743e-06, -1.0281e-03, -3.0403e-03,\n",
            "        -5.7161e-05, -7.4863e-05, -1.0166e-03, -6.9618e-04, -1.7202e-04,\n",
            "        -1.1146e-05, -1.0550e-05, -1.4305e-05, -1.6737e-04, -1.2004e-04,\n",
            "        -1.3947e-05, -1.1820e-04, -7.7400e-03, -2.0587e-04, -1.4221e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026596449315547943 0.0005230903625488281 0.6492393016815186 0.3362821042537689 tensor(70, device='cuda:0')\n",
            "pred tensor([-3.6049e-04, -4.0531e-04, -2.1648e-03, -9.0742e-04, -1.5378e-05,\n",
            "        -1.1456e-04, -1.2195e-04, -1.9610e-05, -8.3148e-05, -8.1539e-05,\n",
            "        -6.8235e-04, -2.2827e-02, -7.4316e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -5.1308e-04, -5.1546e-04, -2.9373e-04,\n",
            "        -1.1194e-04, -6.7592e-05, -3.5942e-05, -2.9802e-06, -5.7638e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024190178140997887 0.0023517608642578125 0.6236404180526733 0.33256807923316956 tensor(54, device='cuda:0')\n",
            "pred tensor([-2.3592e-04, -3.6180e-05, -1.2934e-04, -2.6264e-03, -8.3113e-04,\n",
            "        -6.1913e-03, -3.6926e-03, -1.8167e-04, -5.6696e-04, -3.5691e-04,\n",
            "        -1.5783e-04, -3.4475e-04, -5.4359e-05, -1.8167e-04, -1.1593e-04,\n",
            "        -6.7043e-04, -1.6475e-04, -3.4738e-04, -5.4955e-05, -1.1414e-04,\n",
            "        -8.8330e-01, -9.9707e-01, -6.6650e-01, -5.0888e-03, -2.7523e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025579890236258507 0.0005478858947753906 0.6577025651931763 0.33684977889060974 tensor(70, device='cuda:0')\n",
            "pred tensor([-1.9407e-04, -8.1539e-05, -4.9257e-04, -6.9475e-04, -1.6534e-04,\n",
            "        -2.2876e-04, -4.7374e-04, -1.6384e-03, -5.7459e-04, -2.3234e-04,\n",
            "        -1.9181e-04, -2.9016e-04, -1.3340e-04, -1.4305e-06, -1.4091e-04,\n",
            "        -7.3373e-05, -1.5795e-05, -9.2387e-06, -8.0466e-06, -2.5010e-04,\n",
            "        -6.5136e-04, -1.1139e-02, -1.3847e-02, -3.6297e-03, -1.0228e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02629060298204422 5.054473876953125e-05 0.6955763101577759 0.34119150042533875 tensor(64, device='cuda:0')\n",
            "pred tensor([-7.6723e-04, -5.8055e-05, -1.0931e-04, -3.9673e-04, -1.5116e-04,\n",
            "        -1.6093e-04, -3.2949e-04, -3.6693e-04, -2.5725e-04, -5.8784e-03,\n",
            "        -2.3689e-03, -9.1124e-04, -4.0710e-05, -5.5611e-05, -2.5690e-05,\n",
            "        -1.6153e-05, -1.8001e-05, -9.2173e-04, -2.3592e-04, -1.8835e-05,\n",
            "        -4.7278e-04, -6.0272e-03, -6.1768e-01, -3.2788e-01, -5.2214e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027067722752690315 3.910064697265625e-05 0.7105453014373779 0.35155391693115234 tensor(75, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -6.4790e-05, -1.5903e-04, -2.8849e-05, -9.8944e-06,\n",
            "        -7.2122e-06, -2.9945e-04, -2.4147e-03, -1.0490e-02, -1.1504e-04,\n",
            "        -3.5000e-04, -1.0353e-04, -2.2054e-06, -1.9014e-05, -5.7161e-05,\n",
            "        -2.1148e-04, -2.2113e-05, -5.1613e-03, -1.0710e-03, -1.5459e-03,\n",
            "        -1.1683e-05, -1.0598e-04, -1.8387e-02, -9.5068e-01, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024763574823737144 0.00018262863159179688 0.6760839223861694 0.3380179703235626 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -8.7857e-05, -1.1997e-03, -2.9135e-04, -4.7946e-04,\n",
            "        -1.3411e-05, -3.3021e-04, -1.4839e-03, -2.0623e-05, -3.7074e-05,\n",
            "        -1.1921e-03, -5.7817e-05, -6.2525e-05, -1.1861e-05, -1.8668e-04,\n",
            "        -1.4651e-04, -1.4544e-04, -8.2195e-05, -3.6764e-04, -2.0742e-04,\n",
            "        -4.8399e-04, -2.3785e-03, -4.1580e-04, -3.2949e-04, -1.0900e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025670038536190987 0.0005040168762207031 0.676876425743103 0.3337537348270416 tensor(67, device='cuda:0')\n",
            "pred tensor([-0.0460, -0.0126, -0.0164, -0.0012, -0.0058, -0.0092, -0.0882, -0.2285,\n",
            "        -0.0571, -0.1865, -0.0481, -0.9014, -0.4194, -0.0202, -0.3025, -0.0025,\n",
            "        -0.0063, -0.0053, -0.0245, -0.0263, -0.0233, -0.9121, -0.9790, -0.6069,\n",
            "        -0.1888], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02606382593512535 0.0012989044189453125 0.6458548307418823 0.343073308467865 tensor(83, device='cuda:0')\n",
            "pred tensor([-3.9093e-02, -9.8193e-01, -1.3878e-02, -5.4639e-01, -1.0729e-03,\n",
            "        -2.5055e-02, -3.6743e-02, -8.7842e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -5.1022e-04, -2.3484e-05, -1.8024e-04, -2.9588e-04,\n",
            "        -1.4424e-04, -2.4724e-04, -3.9935e-05, -5.6744e-05, -2.4855e-05,\n",
            "        -1.5888e-03, -2.9254e-04, -7.5245e-04, -2.0504e-03, -1.4257e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025327853858470917 0.0023479461669921875 0.6167728304862976 0.34084075689315796 tensor(76, device='cuda:0')\n",
            "pred tensor([-1.1545e-04, -2.3842e-07, -4.1723e-07, -7.2060e-03, -7.0810e-04,\n",
            "        -9.9951e-01, -1.0000e+00, -1.5843e-04, -7.9870e-06, -4.4250e-04,\n",
            "        -1.0000e+00, -2.6684e-03, -3.9887e-04, -1.1473e-03, -1.2922e-03,\n",
            "        -7.1406e-05, -4.4703e-05, -4.3654e-04, -2.4152e-04, -2.6417e-04,\n",
            "        -2.7704e-04, -1.8167e-04, -4.0054e-05, -3.2759e-04, -9.6500e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02729637175798416 0.0003323554992675781 0.6947380900382996 0.33622345328330994 tensor(72, device='cuda:0')\n",
            "pred tensor([-3.4809e-05, -3.7766e-04, -1.7810e-04, -4.7565e-04, -6.4516e-04,\n",
            "        -1.5259e-05, -4.5955e-05, -6.0201e-06, -4.6110e-04, -5.3857e-01,\n",
            "        -3.5248e-03, -2.3718e-01, -9.5068e-01, -2.6169e-03, -9.3002e-03,\n",
            "        -8.2302e-04, -2.5749e-05, -1.3137e-04, -2.9236e-02, -6.4754e-04,\n",
            "        -4.1664e-05, -1.1963e-04, -1.0312e-04, -3.6061e-05, -2.1231e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029559753835201263 5.8650970458984375e-05 0.7189429998397827 0.3419570028781891 tensor(102, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -4.4174e-03, -2.2078e-04, -1.6987e-05, -1.6034e-05,\n",
            "        -1.0598e-04, -4.0531e-04, -6.4373e-06, -4.3988e-04, -9.9945e-03,\n",
            "        -4.6110e-04, -3.0220e-05, -1.7953e-04, -1.6356e-03, -4.8494e-04,\n",
            "        -6.9189e-04, -5.6190e-03, -2.1076e-04, -1.5850e-03, -1.5039e-03,\n",
            "        -1.2891e-01, -9.8828e-01, -1.0000e+00, -1.0000e+00, -1.6813e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026351431384682655 0.00022602081298828125 0.684104323387146 0.3351094722747803 tensor(66, device='cuda:0')\n",
            "pred tensor([-4.7922e-05, -2.2471e-05, -1.8454e-04, -2.7156e-04, -8.3148e-05,\n",
            "        -1.0031e-04, -1.7953e-04, -3.1877e-04, -8.6784e-04, -6.3372e-04,\n",
            "        -1.2100e-04, -8.9943e-05, -1.3399e-04, -5.4121e-04, -5.0664e-05,\n",
            "        -1.0151e-04, -6.5267e-05, -6.5660e-04, -3.8671e-04, -8.7142e-05,\n",
            "        -4.8339e-05, -4.0710e-05, -9.8407e-05, -3.5906e-04, -1.3304e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027125198394060135 0.0001049041748046875 0.7039153575897217 0.3579791486263275 tensor(68, device='cuda:0')\n",
            "pred tensor([-5.7817e-05, -7.7546e-05, -1.3924e-04, -1.5414e-04, -1.6034e-05,\n",
            "        -1.4710e-04, -3.2878e-04, -8.6451e-04, -1.7333e-04, -5.4312e-04,\n",
            "        -5.9903e-05, -2.1660e-04, -5.3048e-05, -5.5265e-04, -5.9080e-04,\n",
            "        -3.0184e-04, -1.4305e-05, -1.8740e-04, -8.5473e-05, -2.7919e-04,\n",
            "        -5.8212e-03, -2.0950e-02, -1.0000e+00, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026185058057308197 0.00025844573974609375 0.656959593296051 0.34508126974105835 tensor(64, device='cuda:0')\n",
            "pred tensor([-2.3317e-04, -3.1066e-04, -7.8869e-04, -5.1022e-04, -1.3800e-03,\n",
            "        -2.6894e-03, -1.5116e-04, -1.9257e-02, -9.8083e-02, -2.8610e-03,\n",
            "        -7.5340e-03, -1.1802e-05, -1.0757e-03, -6.3753e-04, -5.8293e-05,\n",
            "        -2.9087e-05, -5.3644e-06, -6.7115e-05, -1.9302e-03, -7.4506e-06,\n",
            "        -6.4373e-04, -2.0266e-06, -6.2764e-05, -1.4305e-05, -5.1618e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024890581145882607 0.0015573501586914062 0.6412456631660461 0.3370099365711212 tensor(66, device='cuda:0')\n",
            "pred tensor([-5.7793e-04, -1.0900e-03, -1.7309e-03, -3.8090e-03, -2.4533e-04,\n",
            "        -1.1021e-04, -1.5593e-04, -1.3924e-04, -1.2481e-04, -3.5000e-04,\n",
            "        -1.5354e-04, -2.4319e-05, -4.6849e-05, -1.8096e-04, -6.3002e-05,\n",
            "        -7.5459e-05, -2.3234e-04, -1.5843e-04, -1.3151e-03, -8.2703e-03,\n",
            "        -1.1414e-04, -6.4254e-05, -1.2290e-04, -1.3151e-03, -3.0577e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.022761814296245575 0.004032135009765625 0.6448505520820618 0.33708763122558594 tensor(65, device='cuda:0')\n",
            "pred tensor([-6.6948e-04, -8.2016e-04, -1.3292e-04, -4.7183e-04, -2.5320e-04,\n",
            "        -4.9889e-05, -1.3769e-04, -9.7752e-06, -9.8348e-06, -1.1504e-05,\n",
            "        -3.8457e-04, -3.3855e-05, -6.6161e-06, -3.3081e-05, -3.5763e-06,\n",
            "        -3.6621e-04, -1.0000e+00, -1.0000e+00, -3.5648e-03, -7.3338e-04,\n",
            "        -7.9632e-04, -1.2481e-04, -5.2977e-04, -1.0806e-04, -8.0585e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026562096551060677 0.00018262863159179688 0.7302226424217224 0.33972546458244324 tensor(71, device='cuda:0')\n",
            "pred tensor([-2.9445e-05, -3.5620e-04, -1.3494e-04, -9.5367e-07, -4.1664e-05,\n",
            "        -2.2650e-05, -9.4681e-03, -1.2646e-03, -3.8683e-05, -1.4424e-05,\n",
            "        -9.3877e-05, -8.8513e-05, -1.1292e-02, -5.0449e-04, -5.1439e-05,\n",
            "        -4.9472e-06, -3.6478e-05, -4.1986e-04, -4.8780e-04, -1.3399e-04,\n",
            "        -7.5102e-04, -1.5116e-04, -2.6011e-04, -2.2423e-04, -1.4336e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02669173665344715 2.86102294921875e-06 0.7640091180801392 0.35407787561416626 tensor(65, device='cuda:0')\n",
            "pred tensor([-9.8877e-01, -1.0000e+00, -1.0000e+00, -1.0368e-02, -2.3689e-03,\n",
            "        -7.6660e-01, -9.9414e-01, -8.9307e-01, -9.6631e-01, -1.7017e-01,\n",
            "        -9.5459e-01, -8.6963e-01, -2.0266e-04, -4.1246e-04, -3.6192e-04,\n",
            "        -2.5129e-04, -4.4346e-05, -3.8815e-04, -4.0054e-04, -2.6536e-04,\n",
            "        -1.5235e-04, -2.0065e-03, -1.5850e-03, -2.7800e-04, -2.0421e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02764689363539219 0.0 0.7809069156646729 0.3387427031993866 tensor(73, device='cuda:0')\n",
            "pred tensor([-2.6474e-03, -2.4433e-03, -4.6349e-03, -3.0689e-03, -3.7336e-04,\n",
            "        -3.4261e-04, -1.2941e-03, -4.5662e-03, -3.6346e-02, -3.0472e-02,\n",
            "        -4.6826e-01, -9.9902e-01, -9.9316e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -5.4741e-04, -3.4261e-04, -3.0696e-05, -3.8290e-04, -2.8431e-05,\n",
            "        -5.4777e-05, -9.1124e-04, -1.0109e-04, -1.6212e-05, -3.9160e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027686698362231255 0.00011348724365234375 0.7053201198577881 0.3485904037952423 tensor(77, device='cuda:0')\n",
            "pred tensor([-1.4782e-05, -3.1352e-05, -7.0333e-05, -1.7047e-05, -8.5115e-05,\n",
            "        -1.1325e-04, -1.2243e-04, -3.2921e-03, -3.0100e-05, -1.2290e-04,\n",
            "        -7.1955e-04, -1.3232e-05, -1.5497e-06, -2.4676e-05, -1.2970e-03,\n",
            "        -6.1111e-03, -2.7637e-03, -2.6822e-05, -1.2589e-04, -1.6737e-04,\n",
            "        -2.1756e-05, -6.5029e-05, -1.8537e-05, -4.2391e-04, -1.0223e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025969428941607475 0.0005216598510742188 0.698695182800293 0.34408703446388245 tensor(75, device='cuda:0')\n",
            "pred tensor([-2.2697e-03, -2.6138e-02, -7.9498e-03, -9.1162e-01, -9.9268e-01,\n",
            "        -1.0000e+00, -2.3055e-04, -9.7752e-06, -6.4754e-04, -2.2526e-03,\n",
            "        -1.2386e-04, -2.2054e-06, -8.1897e-05, -1.5676e-05, -3.5763e-07,\n",
            "        -1.5318e-05, -1.2517e-05, -4.3511e-06, -1.8120e-05, -1.1235e-04,\n",
            "        -3.8147e-06, -1.0729e-06, -4.2915e-06, -4.1890e-04, -4.6372e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02906530350446701 0.00010347366333007812 0.7033790349960327 0.3530419170856476 tensor(90, device='cuda:0')\n",
            "pred tensor([-4.7112e-04, -5.8270e-04, -6.3002e-05, -6.8378e-04, -1.7941e-05,\n",
            "        -6.7592e-05, -1.2219e-05, -6.3539e-05, -4.0531e-06, -7.0572e-05,\n",
            "        -3.0935e-05, -5.3644e-06, -2.7001e-05, -4.0531e-06, -2.6226e-06,\n",
            "        -1.7881e-07, -1.7285e-06, -9.0599e-06, -3.6359e-06, -2.3842e-07,\n",
            "        -4.7088e-06, -5.1022e-04, -9.9658e-01, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03004598803818226 0.001346588134765625 0.653853178024292 0.35270535945892334 tensor(89, device='cuda:0')\n",
            "pred tensor([-3.3970e-03, -1.5764e-03, -1.4043e-04, -3.4962e-03, -1.4091e-02,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -8.8644e-04, -6.4754e-04,\n",
            "        -3.5644e-05, -3.5346e-05, -4.0126e-04, -7.6294e-04, -1.9038e-04,\n",
            "        -1.9789e-04, -6.6042e-05, -2.8312e-05, -7.0953e-04, -2.5415e-04,\n",
            "        -8.2552e-05, -1.3185e-04, -9.5844e-04, -1.5736e-05, -7.0333e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027243686839938164 0.004913330078125 0.5845524072647095 0.3395425081253052 tensor(70, device='cuda:0')\n",
            "pred tensor([-4.4775e-04, -7.2956e-04, -1.9417e-03, -6.1989e-06, -5.5432e-05,\n",
            "        -6.0940e-04, -5.5838e-04, -1.0185e-03, -1.0605e-03, -1.0151e-04,\n",
            "        -2.5916e-04, -1.7810e-04, -6.5267e-05, -9.7632e-05, -2.3878e-04,\n",
            "        -7.9012e-04, -1.9045e-03, -1.3151e-03, -1.3151e-03, -9.1095e-03,\n",
            "        -1.0424e-03, -1.2779e-04, -2.3270e-03, -4.6338e-01, -9.9951e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02899446152150631 0.0003628730773925781 0.7005141973495483 0.3387811481952667 tensor(84, device='cuda:0')\n",
            "pred tensor([-1.4282e-02, -2.1347e-02, -2.9640e-03, -1.4490e-01, -6.3019e-03,\n",
            "        -1.2337e-02, -2.4052e-03, -1.7376e-03, -5.1308e-03, -1.0986e-03,\n",
            "        -1.3695e-03, -1.2684e-04, -1.5843e-04, -2.2697e-04, -1.2093e-03,\n",
            "        -3.6716e-03, -4.0627e-03, -2.7790e-03, -2.1496e-03, -4.0703e-03,\n",
            "        -2.1992e-03, -6.8893e-03, -2.4078e-02, -1.3741e-02, -6.5269e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03280652314424515 9.1552734375e-05 0.7484663724899292 0.3399827182292938 tensor(83, device='cuda:0')\n",
            "pred tensor([-8.6768e-01, -1.0000e+00, -1.0000e+00, -4.0710e-05, -2.1827e-04,\n",
            "        -1.2386e-04, -1.4484e-04, -1.2887e-04, -3.6597e-05, -1.4710e-04,\n",
            "        -3.6120e-04, -1.1683e-05, -1.9073e-06, -3.5429e-04, -1.9913e-03,\n",
            "        -7.3957e-04, -2.1279e-05, -1.2147e-04, -2.0676e-03, -6.9046e-04,\n",
            "        -4.1413e-04, -7.0429e-04, -2.3317e-04, -3.8207e-05, -7.7248e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029808560386300087 0.0 0.8046330213546753 0.3398241400718689 tensor(81, device='cuda:0')\n",
            "66\n",
            "pred tensor([-1.8997e-02, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -9.7215e-05, -2.2049e-03, -1.3709e-04, -2.8610e-05,\n",
            "        -2.7156e-04, -1.2159e-05, -2.0862e-05, -2.2829e-05, -2.7585e-04,\n",
            "        -9.0003e-06, -4.6277e-04, -3.8087e-05, -1.6534e-04, -7.5817e-04,\n",
            "        -1.3769e-04, -2.9373e-04, -1.6284e-04, -2.2411e-05, -1.7285e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030437488108873367 8.106231689453125e-06 0.7300624251365662 0.3489339053630829 tensor(83, device='cuda:0')\n",
            "pred tensor([-6.8426e-05, -4.6134e-05, -6.3181e-06, -2.6047e-05, -3.6955e-06,\n",
            "        -4.4703e-06, -5.5432e-05, -1.5473e-04, -2.3842e-07, -1.0490e-05,\n",
            "        -1.0353e-04, -7.4673e-04, -9.5367e-06, -1.0000e+00, -1.0000e+00,\n",
            "         0.0000e+00, -1.1921e-07, -7.0068e-02, -1.6093e-06, -2.9802e-07,\n",
            "        -3.7074e-05, -2.5558e-03, -4.5300e-06, -4.3821e-04, -3.1209e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.035160936415195465 2.002716064453125e-05 0.750584065914154 0.3609098792076111 tensor(110, device='cuda:0')\n",
            "pred tensor([-4.7851e-04, -5.0640e-04, -5.3358e-04, -7.8440e-05, -1.0598e-04,\n",
            "        -4.4525e-05, -1.1625e-03, -1.6749e-05, -2.1565e-04, -2.0623e-05,\n",
            "        -1.0723e-04, -1.2722e-03, -3.3927e-04, -2.0905e-03, -2.6321e-03,\n",
            "        -1.5664e-04, -1.0462e-03, -3.6469e-02, -2.8849e-05, -7.8869e-04,\n",
            "        -3.4515e-02, -2.1458e-05, -6.1083e-04, -3.6354e-03, -5.9664e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028193892911076546 0.0004229545593261719 0.6982729434967041 0.34589898586273193 tensor(100, device='cuda:0')\n",
            "pred tensor([-6.7353e-05, -9.8765e-05, -7.4673e-04, -4.8590e-04, -4.0221e-04,\n",
            "        -2.2125e-03, -2.3232e-03, -4.2963e-04, -2.5129e-04, -8.8135e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -5.6419e-03, -7.0068e-02, -5.0018e-02,\n",
            "        -6.6423e-04, -1.0890e-04, -5.0449e-04, -7.1526e-07, -1.8775e-05,\n",
            "        -1.6689e-05, -4.1187e-05, -1.9670e-06, -9.3579e-06, -1.8034e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0327831506729126 0.0017557144165039062 0.6638258695602417 0.3483007848262787 tensor(104, device='cuda:0')\n",
            "pred tensor([-2.6321e-03, -9.1374e-05, -1.2243e-04, -1.3990e-03, -2.1777e-01,\n",
            "        -8.5144e-03, -1.5039e-03, -4.4403e-02, -6.8604e-01, -3.8330e-02,\n",
            "        -1.3428e-02, -2.6727e-04, -9.3651e-04, -3.4738e-04, -2.3687e-04,\n",
            "        -4.5300e-04, -5.4588e-03, -1.6260e-03, -2.1338e-05, -3.8528e-04,\n",
            "        -4.1008e-04, -2.8253e-04, -9.7809e-03, -9.9268e-01, -6.4160e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03131690248847008 0.0023193359375 0.6829320192337036 0.3466167449951172 tensor(103, device='cuda:0')\n",
            "pred tensor([-1.4984e-02, -6.6910e-03, -2.7759e-01, -1.4954e-02, -9.7370e-04,\n",
            "        -2.3305e-05, -7.3671e-05, -3.4332e-04, -5.9540e-02, -7.8278e-03,\n",
            "        -5.2452e-05, -3.9444e-03, -6.3623e-01, -9.4873e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -8.1682e-04, -7.2384e-04, -1.7786e-03, -4.7922e-05,\n",
            "        -2.0587e-04, -2.6112e-03, -8.6427e-06, -6.3300e-05, -5.8031e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.033090826123952866 0.0007429122924804688 0.6945475339889526 0.35605132579803467 tensor(96, device='cuda:0')\n",
            "pred tensor([-4.4174e-03, -2.2659e-03, -2.1148e-04, -1.1625e-03, -5.2357e-04,\n",
            "        -5.8293e-05, -9.1195e-06, -1.2243e-04, -1.7130e-04, -1.2827e-04,\n",
            "        -3.2687e-04, -6.3300e-05, -6.8665e-05, -1.0133e-05, -1.8001e-05,\n",
            "        -3.7694e-04, -1.3983e-04, -1.8597e-04, -1.5903e-04, -5.2977e-04,\n",
            "        -3.9124e-04, -3.9279e-05, -4.6492e-06, -1.4901e-06, -2.2233e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03018302284181118 0.0001049041748046875 0.7233043909072876 0.34251144528388977 tensor(89, device='cuda:0')\n",
            "pred tensor([-1.2875e-05, -5.2869e-05, -1.4133e-03, -6.9201e-05, -1.8024e-04,\n",
            "        -7.9498e-03, -3.4261e-04, -1.2434e-04, -6.3181e-06, -4.5240e-05,\n",
            "        -1.9684e-03, -5.7459e-04, -5.5170e-04, -2.1572e-03, -1.5199e-05,\n",
            "        -6.1333e-05, -2.8610e-03, -3.0577e-05, -5.6088e-05, -1.0669e-05,\n",
            "        -5.6088e-05, -4.1842e-05, -6.2294e-03, -1.2875e-05, -9.0003e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031577348709106445 9.059906005859375e-06 0.7709096670150757 0.34290170669555664 tensor(88, device='cuda:0')\n",
            "pred tensor([-1.6272e-05, -1.5557e-05, -1.1921e-07, -5.4240e-06, -9.8535e-01,\n",
            "        -1.0000e+00, -5.5432e-06, -3.2067e-05, -6.1798e-03, -3.4962e-03,\n",
            "        -1.4937e-04, -5.1737e-04, -1.0109e-04, -1.6451e-03, -1.1244e-03,\n",
            "        -2.6264e-03, -2.4724e-04, -2.0504e-04, -7.4463e-03, -5.6305e-03,\n",
            "        -1.5278e-03, -8.3804e-05, -7.2670e-04, -4.4922e-02, -1.7929e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032439835369586945 6.4849853515625e-05 0.7236828207969666 0.3465467393398285 tensor(85, device='cuda:0')\n",
            "pred tensor([-6.1798e-03, -8.5068e-04, -3.8013e-03, -1.0598e-04, -4.4678e-02,\n",
            "        -6.8896e-01, -3.1586e-02, -2.5988e-05, -1.2865e-03, -1.2335e-01,\n",
            "        -2.0027e-03, -5.4741e-04, -3.4210e-02, -9.5463e-04, -1.3781e-03,\n",
            "        -4.5598e-05, -2.2602e-04, -1.0729e-03, -1.5430e-03, -5.0116e-04,\n",
            "        -3.4666e-04, -1.6797e-04, -6.3848e-04, -1.8826e-03, -8.0252e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032424405217170715 7.390975952148438e-05 0.713804304599762 0.34335020184516907 tensor(86, device='cuda:0')\n",
            "pred tensor([-7.6723e-04, -1.0151e-04, -1.4896e-03, -2.5153e-05, -1.6584e-03,\n",
            "        -6.1798e-04, -3.0403e-03, -1.3983e-04, -4.5598e-05, -6.5804e-05,\n",
            "        -1.3990e-03, -8.1205e-04, -1.0353e-04, -8.5115e-05, -3.8803e-05,\n",
            "        -8.1491e-04, -7.0035e-05, -7.4267e-05, -3.7479e-04, -2.4629e-04,\n",
            "        -6.7043e-04, -2.1248e-03, -4.1187e-05, -3.7611e-05, -6.0463e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026044370606541634 0.0014591217041015625 0.6466443538665771 0.35034623742103577 tensor(65, device='cuda:0')\n",
            "pred tensor([-2.6703e-05, -1.0185e-03, -1.2004e-04, -7.2241e-05, -1.3983e-04,\n",
            "        -1.1492e-03, -1.0931e-04, -7.1704e-05, -2.7800e-04, -3.5429e-04,\n",
            "        -5.4538e-05, -8.7857e-05, -9.5725e-05, -6.6805e-04, -6.9737e-05,\n",
            "        -1.9610e-05, -5.2691e-05, -1.2195e-04, -6.4254e-05, -1.1963e-04,\n",
            "        -3.1376e-04, -1.4296e-03, -1.8835e-05, -6.7472e-04, -2.2590e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03039819933474064 0.0024013519287109375 0.6697373390197754 0.3742987811565399 tensor(73, device='cuda:0')\n",
            "pred tensor([-2.8343e-03, -2.6131e-04, -2.1398e-04, -1.1593e-04, -3.5167e-06,\n",
            "        -5.4646e-04, -6.0940e-04, -3.9673e-04, -1.6868e-04, -6.2294e-03,\n",
            "        -1.6093e-04, -3.4962e-03, -6.5332e-01, -1.0000e+00, -9.9805e-01,\n",
            "        -1.0000e+00, -5.5122e-03, -1.1377e-03, -3.1590e-06, -2.3508e-04,\n",
            "        -1.0586e-03, -8.8215e-05, -3.5114e-03, -3.1376e-04, -1.2887e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026412667706608772 0.0006189346313476562 0.7206017971038818 0.35655835270881653 tensor(65, device='cuda:0')\n",
            "pred tensor([-1.0848e-04, -2.0889e-02, -8.3771e-03, -6.6280e-04, -1.4782e-03,\n",
            "        -4.0869e-01, -9.3079e-04, -6.1646e-02, -4.3732e-02, -1.1938e-01,\n",
            "        -9.6035e-04, -1.3405e-02, -1.0429e-02, -5.4932e-04, -5.4817e-03,\n",
            "        -1.3557e-02, -7.4043e-03, -1.7748e-03, -1.1398e-02, -2.6822e-05,\n",
            "        -1.2684e-04, -4.7112e-04, -1.2722e-03, -5.3704e-05, -2.1660e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025605835020542145 0.00033283233642578125 0.7106618881225586 0.33588123321533203 tensor(61, device='cuda:0')\n",
            "pred tensor([-7.7844e-05, -3.8815e-04, -7.3373e-05, -1.2243e-04, -3.1054e-05,\n",
            "        -3.5214e-04, -1.4484e-04, -1.1194e-04, -1.4603e-05, -1.1730e-04,\n",
            "        -5.6922e-05, -4.4525e-05, -2.9707e-04, -9.9957e-05, -4.0531e-06,\n",
            "        -3.1018e-04, -4.6849e-05, -3.3081e-05, -1.9336e-04, -3.3054e-03,\n",
            "        -1.5962e-04, -5.7697e-04, -4.4703e-06, -5.9605e-08, -4.4107e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02477669157087803 0.00012350082397460938 0.7217397093772888 0.33406367897987366 tensor(39, device='cuda:0')\n",
            "pred tensor([-2.9445e-05, -2.5988e-05, -3.1948e-05, -5.3644e-07, -1.0931e-04,\n",
            "        -1.2426e-03, -8.4162e-05, -4.5300e-06, -2.7580e-03, -2.6047e-05,\n",
            "        -9.9540e-06, -8.3447e-07, -1.0371e-05, -8.3447e-07, -1.6475e-04,\n",
            "        -1.0729e-06, -7.1526e-06, -1.0765e-04, -1.4710e-04, -8.3804e-05,\n",
            "        -9.3689e-02, -9.9707e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03143519163131714 0.000286102294921875 0.7065660953521729 0.33888354897499084 tensor(90, device='cuda:0')\n",
            "pred tensor([-1.5903e-04, -3.5143e-04, -2.6989e-03, -9.2983e-06, -4.2152e-04,\n",
            "        -3.8743e-06, -3.9458e-05, -9.9957e-05, -7.0333e-06, -1.6868e-04,\n",
            "        -2.8551e-05, -4.3511e-05, -1.6665e-04, -7.3671e-04, -6.7854e-04,\n",
            "        -2.6822e-05, -1.4424e-04, -5.9426e-05, -5.1022e-05, -6.2287e-05,\n",
            "        -2.6882e-05, -2.0504e-04, -1.0042e-03, -1.7953e-04, -1.9789e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026551395654678345 0.000705718994140625 0.6934539675712585 0.33921346068382263 tensor(73, device='cuda:0')\n",
            "pred tensor([-1.4929e-01, -7.5867e-02, -1.4900e-02, -4.5441e-02, -7.3586e-03,\n",
            "        -3.4237e-03, -9.2554e-04, -3.3736e-04, -1.7047e-05, -2.4438e-04,\n",
            "        -1.0691e-03, -1.6153e-04, -4.0293e-04, -3.1054e-05, -2.1040e-05,\n",
            "        -1.2875e-05, -6.0797e-06, -2.2233e-05, -5.9605e-08, -5.9605e-07,\n",
            "        -3.2654e-02, -5.2783e-01, -1.0000e+00, -1.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028104443103075027 0.001064300537109375 0.657555103302002 0.3578070402145386 tensor(88, device='cuda:0')\n",
            "pred tensor([-2.6417e-04, -4.4594e-03, -1.4582e-03, -5.0664e-06, -1.3618e-03,\n",
            "        -1.2268e-02, -4.1986e-04, -8.7261e-04, -2.4533e-04, -2.7585e-04,\n",
            "        -2.8496e-03, -1.2493e-03, -3.5763e-04, -3.4392e-05, -4.3809e-05,\n",
            "        -8.1241e-05, -1.1654e-03, -1.5533e-04, -4.1187e-05, -5.0843e-05,\n",
            "        -2.3508e-04, -2.4629e-04, -2.1954e-03, -1.6928e-04, -3.8147e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026819519698619843 0.001544952392578125 0.6320772171020508 0.33403950929641724 tensor(66, device='cuda:0')\n",
            "pred tensor([-2.8563e-04, -2.0206e-05, -3.1757e-04, -7.1526e-06, -4.4703e-06,\n",
            "        -6.3777e-05, -1.5235e-04, -1.2279e-05, -1.1086e-05, -1.4484e-05,\n",
            "        -9.2554e-04, -4.1428e-03, -2.5570e-05, -1.2195e-04, -5.6419e-03,\n",
            "        -1.9968e-05, -2.8014e-06, -2.8014e-06, -2.4378e-05, -1.9431e-05,\n",
            "        -1.7881e-07, -3.9160e-05, -2.5094e-05, -9.5367e-07, -4.0710e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0268748477101326 0.0014476776123046875 0.6427457332611084 0.3368494510650635 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.3185e-04, -4.4155e-04, -3.1605e-03, -3.1052e-03, -2.1082e-01,\n",
            "        -7.5674e-04, -4.0770e-04, -6.7177e-03, -1.1806e-03, -2.8496e-03,\n",
            "        -1.0806e-04, -7.8011e-03, -1.9608e-03, -5.8365e-04, -8.3447e-04,\n",
            "        -1.0834e-03, -3.0541e-04, -2.6417e-03, -1.2195e-04, -6.5029e-05,\n",
            "        -1.1034e-03, -2.7344e-02, -1.1311e-03, -2.4395e-03, -3.2673e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027319980785250664 0.00016546249389648438 0.74272221326828 0.33673346042633057 tensor(56, device='cuda:0')\n",
            "pred tensor([-7.9775e-04, -5.1832e-04, -9.1003e-02, -4.2892e-04, -8.8453e-04,\n",
            "        -7.7915e-04, -2.3055e-04, -2.1398e-04, -4.7374e-04, -1.5533e-04,\n",
            "        -3.1638e-04, -8.5602e-03, -9.0866e-03, -1.4282e-02, -1.0902e-02,\n",
            "        -1.4870e-02, -5.2273e-05, -6.3777e-05, -1.7891e-03, -2.9981e-05,\n",
            "        -2.7835e-05, -1.1504e-04, -4.2498e-05, -4.2140e-05, -4.4942e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026457330211997032 4.76837158203125e-05 0.7282887697219849 0.3478788733482361 tensor(80, device='cuda:0')\n",
            "pred tensor([-5.6305e-03, -3.2291e-03, -4.2295e-04, -1.8063e-03, -1.7443e-03,\n",
            "        -1.1504e-04, -1.0529e-03, -1.6153e-04, -5.1641e-04, -3.0575e-03,\n",
            "        -5.2441e-01, -9.9805e-01, -3.6926e-03, -6.9847e-03, -2.7588e-02,\n",
            "        -1.3769e-04, -8.0466e-06, -2.8172e-03, -1.5664e-04, -1.9729e-05,\n",
            "        -2.9135e-04, -2.9011e-03, -2.8839e-03, -2.7537e-05, -1.0389e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028244661167263985 0.000225067138671875 0.7008097171783447 0.3534345328807831 tensor(76, device='cuda:0')\n",
            "pred tensor([-4.3297e-04, -2.0921e-05, -7.1669e-04, -2.2602e-04, -1.2848e-02,\n",
            "        -5.5450e-02, -3.9825e-03, -8.6260e-04, -2.6428e-02, -2.3918e-03,\n",
            "        -1.0080e-03, -8.6129e-05, -1.2789e-03, -1.1027e-05, -2.2471e-05,\n",
            "        -6.1560e-04, -1.0431e-05, -2.4533e-04, -1.4381e-03, -4.7016e-04,\n",
            "        -4.2295e-04, -5.8270e-04, -9.1267e-04, -1.1963e-02, -3.4738e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029686247929930687 0.0008087158203125 0.6788977384567261 0.35283055901527405 tensor(84, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -3.2406e-03, -5.1439e-05, -4.1318e-04,\n",
            "        -2.0909e-04, -5.9187e-05, -1.2481e-04, -1.0192e-05, -3.2687e-04,\n",
            "        -3.4928e-04, -1.0681e-04, -8.1491e-04, -5.1172e-01, -9.9316e-01,\n",
            "        -4.8126e-02, -9.9854e-01, -9.3765e-03, -1.3824e-02, -3.3545e-01,\n",
            "        -4.1602e-01, -2.0093e-01, -7.9590e-01, -2.0325e-01, -7.7637e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0301972683519125 0.0005655288696289062 0.6991223096847534 0.3496248126029968 tensor(72, device='cuda:0')\n",
            "pred tensor([-3.5071e-04, -9.9087e-04, -7.3914e-02, -8.7142e-05, -2.9683e-05,\n",
            "        -9.1016e-05, -4.6134e-05, -7.9298e-04, -6.1455e-03, -7.7248e-05,\n",
            "        -2.0862e-05, -5.0116e-04, -8.7595e-04, -7.8082e-06, -4.1986e-04,\n",
            "        -5.8937e-04, -1.5354e-04, -5.4955e-05, -7.5698e-06, -6.3229e-04,\n",
            "        -4.7469e-04, -4.2152e-04, -1.0556e-04, -2.2888e-05, -1.2331e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03168774023652077 0.0010776519775390625 0.65400230884552 0.34632551670074463 tensor(69, device='cuda:0')\n",
            "pred tensor([-8.4162e-05, -4.2391e-04, -2.0182e-04, -1.7405e-04, -4.2140e-05,\n",
            "        -1.1593e-04, -6.7822e-01, -1.0000e+00, -1.0000e+00, -5.3287e-05,\n",
            "        -1.1683e-04, -2.3317e-04, -2.0623e-05, -4.2319e-05, -6.3848e-04,\n",
            "        -4.2224e-04, -1.3828e-05, -2.4438e-06, -2.0993e-04, -6.8808e-04,\n",
            "        -1.8418e-05, -1.4126e-05, -1.0550e-05, -3.2425e-05, -3.1877e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027156323194503784 0.0006399154663085938 0.6823891997337341 0.35838639736175537 tensor(68, device='cuda:0')\n",
            "pred tensor([-7.9041e-03, -9.5463e-04, -3.5515e-03, -1.7405e-04, -1.2004e-04,\n",
            "        -4.0674e-04, -3.8910e-04, -1.2517e-06, -1.4544e-05, -3.5906e-04,\n",
            "        -5.5504e-04, -8.6844e-05, -5.6505e-05, -2.7537e-05, -1.9336e-04,\n",
            "        -1.6284e-04, -6.0201e-06, -2.8610e-06, -6.0618e-05, -5.7220e-06,\n",
            "        -2.1458e-05, -5.8842e-04, -1.2529e-04, -4.0531e-06, -1.8239e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026156524196267128 0.0003418922424316406 0.6895899772644043 0.3416346311569214 tensor(66, device='cuda:0')\n",
            "pred tensor([-7.9775e-04, -1.3456e-03, -8.6844e-05, -4.1986e-04, -2.0444e-05,\n",
            "        -8.0287e-05, -1.8826e-03, -6.4790e-05, -1.6922e-02, -3.5492e-02,\n",
            "        -4.1187e-05, -1.0389e-04, -2.8114e-03, -1.1574e-02, -1.1835e-03,\n",
            "        -5.2547e-04, -5.4121e-05, -1.8525e-04, -9.0659e-05, -2.5158e-03,\n",
            "        -1.2426e-03, -3.6359e-05, -4.8518e-05, -3.9887e-04, -6.5804e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028300108388066292 0.000637054443359375 0.6885842680931091 0.35481777787208557 tensor(94, device='cuda:0')\n",
            "pred tensor([-1.1981e-05, -1.2982e-04, -3.1548e-03, -1.3340e-04, -1.6422e-03,\n",
            "        -5.1804e-03, -3.9062e-03, -1.3816e-04, -2.3055e-04, -4.0314e-02,\n",
            "        -5.3453e-04, -5.7364e-04, -2.4490e-03, -2.1660e-04, -1.7242e-03,\n",
            "        -4.1080e-04, -4.1428e-03, -6.1455e-03, -2.0182e-04, -6.8726e-02,\n",
            "        -6.2598e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -2.4967e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026478823274374008 0.0011653900146484375 0.6657379865646362 0.3406848907470703 tensor(79, device='cuda:0')\n",
            "pred tensor([-6.0606e-04, -4.9114e-05, -7.1406e-05, -3.9363e-04, -3.0899e-04,\n",
            "        -2.0266e-04, -3.9825e-02, -7.2327e-03, -1.7532e-02, -2.4395e-03,\n",
            "        -1.1902e-03, -3.8528e-04, -1.2934e-05, -5.7638e-05, -3.3617e-05,\n",
            "        -3.0935e-05, -1.0192e-04, -1.7509e-03, -1.4618e-02, -6.9458e-02,\n",
            "        -2.4052e-03, -8.8120e-04, -1.1692e-03, -8.6260e-04, -4.1246e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028683509677648544 0.00029468536376953125 0.6943880319595337 0.3403002619743347 tensor(81, device='cuda:0')\n",
            "pred tensor([-6.1455e-03, -1.7681e-03, -9.9951e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.3602e-04, -1.1021e-04, -4.6134e-05, -5.2869e-05, -1.4484e-04,\n",
            "        -1.6415e-04, -4.2498e-05, -8.6451e-04, -3.8548e-03, -2.3139e-04,\n",
            "        -4.7028e-05, -7.0667e-04, -3.7491e-05, -3.0994e-06, -1.8311e-04,\n",
            "        -7.0667e-04, -2.4915e-04, -2.8563e-04, -1.1504e-04, -3.1877e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029140932485461235 0.0004968643188476562 0.6619959473609924 0.33938679099082947 tensor(84, device='cuda:0')\n",
            "pred tensor([-2.1482e-04, -8.0585e-04, -4.9744e-03, -2.6846e-04, -6.6042e-04,\n",
            "        -4.4346e-05, -1.4150e-04, -1.7405e-04, -1.3590e-03, -1.5747e-02,\n",
            "        -7.4816e-04, -7.3013e-03, -1.7776e-02, -1.3199e-03, -8.3804e-05,\n",
            "        -4.8676e-03, -3.0880e-03, -1.9045e-03, -3.3894e-03, -5.7817e-05,\n",
            "        -8.5938e-01, -1.1945e-01, -9.6533e-01, -9.9463e-01, -8.2715e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02735864371061325 0.0005068778991699219 0.6766430139541626 0.3411746025085449 tensor(75, device='cuda:0')\n",
            "pred tensor([-9.8779e-01, -4.9744e-03, -1.4091e-04, -2.0428e-03, -5.1613e-03,\n",
            "        -3.4523e-04, -3.0816e-05, -9.0301e-05, -6.2294e-03, -1.7639e-02,\n",
            "        -5.4836e-04, -1.8967e-02, -1.3824e-02, -4.8676e-03, -5.4216e-04,\n",
            "        -1.0765e-04, -1.1963e-02, -1.8382e-04, -1.7822e-05, -2.1881e-02,\n",
            "        -2.2221e-03, -1.2789e-03, -3.0994e-03, -5.9187e-05, -1.9491e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027801785618066788 0.0006632804870605469 0.6606627702713013 0.3531702160835266 tensor(78, device='cuda:0')\n",
            "pred tensor([-7.9775e-04, -5.2023e-04, -6.5660e-04, -5.7161e-05, -2.7299e-05,\n",
            "        -2.3782e-04, -3.0100e-05, -1.6570e-05, -1.2696e-05, -1.7953e-04,\n",
            "        -7.4267e-05, -8.0287e-05, -2.4395e-03, -2.0337e-04, -3.3545e-04,\n",
            "        -1.7405e-04, -7.4816e-04, -3.6955e-06, -5.7817e-06, -7.4816e-04,\n",
            "        -2.1496e-03, -2.3484e-05, -4.5955e-05, -1.5724e-04, -5.6791e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029687443748116493 0.0002789497375488281 0.6850582361221313 0.3433213233947754 tensor(82, device='cuda:0')\n",
            "pred tensor([-8.1539e-05, -3.0422e-04, -4.2462e-04, -2.7800e-04, -4.3988e-05,\n",
            "        -4.2498e-05, -6.9857e-04, -3.4261e-04, -2.9087e-05, -3.5167e-06,\n",
            "        -1.8024e-04, -1.1367e-04, -5.1308e-04, -3.8910e-04, -5.8413e-06,\n",
            "        -5.5611e-05, -9.0659e-05, -3.3855e-05, -4.9114e-05, -2.6166e-05,\n",
            "        -7.4506e-06, -7.5102e-06, -5.4407e-04, -2.9469e-04, -2.6536e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02594582550227642 0.0003266334533691406 0.6980322599411011 0.34909388422966003 tensor(56, device='cuda:0')\n",
            "pred tensor([-5.6791e-04, -3.9673e-04, -7.4565e-05, -1.3554e-04, -3.7491e-05,\n",
            "        -1.1414e-04, -7.1955e-04, -8.2970e-04, -1.8063e-03, -2.3060e-03,\n",
            "        -9.9182e-05, -1.9407e-04, -1.0712e-02, -8.5473e-05, -9.9072e-01,\n",
            "        -9.9902e-01, -9.8633e-01, -8.4257e-04, -3.6507e-03, -2.9588e-04,\n",
            "        -2.8348e-04, -6.4125e-03, -1.2569e-03, -2.3878e-04, -1.3428e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030290624126791954 0.0009059906005859375 0.6546754240989685 0.34578609466552734 tensor(76, device='cuda:0')\n",
            "pred tensor([-1.5669e-03, -1.4651e-04, -5.0020e-04, -6.1646e-02, -1.2302e-03,\n",
            "        -1.0777e-03, -2.3132e-02, -8.2970e-04, -8.4162e-05, -2.7704e-04,\n",
            "        -3.7193e-05, -4.5240e-05, -6.6299e-03, -1.8811e-04, -3.5248e-03,\n",
            "        -5.2738e-04, -2.5253e-03, -3.9053e-04, -3.2787e-03, -1.7262e-04,\n",
            "        -1.0166e-03, -3.4363e-02, -7.5531e-04, -1.0948e-03, -2.4490e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026312846690416336 0.0008258819580078125 0.6358203887939453 0.3538936674594879 tensor(66, device='cuda:0')\n",
            "67\n",
            "pred tensor([-3.9339e-06, -3.1173e-05, -2.2781e-04, -5.7936e-04, -1.0000e+00,\n",
            "        -9.9951e-01, -9.9463e-01, -1.0931e-04, -2.1231e-04, -4.7386e-05,\n",
            "        -3.2306e-05, -1.2386e-04, -1.2004e-04, -1.4365e-05, -9.5654e-04,\n",
            "        -1.6224e-04, -2.7418e-06, -1.9610e-05, -4.1962e-05, -2.9254e-04,\n",
            "        -4.9472e-06, -1.3661e-04, -7.8535e-04, -1.1473e-03, -8.2779e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029315894469618797 0.00019598007202148438 0.6848019957542419 0.3328775465488434 tensor(61, device='cuda:0')\n",
            "pred tensor([-8.8453e-04, -5.8270e-04, -1.1915e-04, -7.0953e-04, -2.1152e-03,\n",
            "        -6.9427e-03, -1.2147e-04, -9.4235e-05, -5.9080e-04, -6.6137e-04,\n",
            "        -4.1187e-05, -2.1744e-04, -3.6836e-04, -5.8770e-05, -4.2915e-06,\n",
            "        -1.0031e-04, -1.4651e-04, -1.1104e-04, -2.2054e-06, -1.1325e-04,\n",
            "        -2.4915e-04, -6.8426e-05, -1.9372e-05, -6.2523e-03, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026697639375925064 0.0005998611450195312 0.6449700593948364 0.33709171414375305 tensor(58, device='cuda:0')\n",
            "pred tensor([-8.4817e-05, -8.1539e-05, -9.2363e-04, -1.1545e-04, -3.5000e-04,\n",
            "        -1.5259e-05, -1.8096e-04, -7.1239e-04, -7.0572e-04, -8.3506e-05,\n",
            "        -2.3413e-04, -2.0659e-04, -2.3317e-04, -5.0843e-05, -2.5225e-04,\n",
            "        -2.7537e-05, -2.3317e-04, -3.2115e-04, -1.7481e-03, -1.0073e-04,\n",
            "        -4.1819e-04, -6.8426e-05, -7.3135e-05, -4.0221e-04, -1.0204e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026920607313513756 0.0017766952514648438 0.6062237024307251 0.33562517166137695 tensor(65, device='cuda:0')\n",
            "pred tensor([-1.0514e-04, -4.3559e-04, -2.7418e-05, -1.7643e-05, -3.0994e-06,\n",
            "        -5.8365e-04, -8.1348e-04, -8.5115e-05, -2.2233e-05, -1.1265e-05,\n",
            "        -6.8426e-05, -1.2589e-04, -1.8811e-04, -2.2602e-04, -2.5868e-05,\n",
            "        -1.1194e-04, -2.3842e-05, -2.7061e-04, -2.4021e-05, -4.1723e-06,\n",
            "        -1.5259e-05, -8.4162e-05, -5.4216e-04, -2.3594e-03, -7.4565e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027261244133114815 0.0002713203430175781 0.677428126335144 0.34691542387008667 tensor(51, device='cuda:0')\n",
            "pred tensor([-1.9944e-04, -1.4133e-03, -4.7851e-04, -2.5868e-05, -4.8876e-06,\n",
            "        -1.8966e-04, -2.3880e-03, -5.8887e-01, -4.6997e-03, -6.6260e-01,\n",
            "        -1.7365e-02, -2.4533e-04, -8.1897e-05, -3.0065e-04, -3.0575e-03,\n",
            "        -2.4967e-03, -1.0353e-04, -2.7919e-04, -8.5592e-04, -4.9805e-01,\n",
            "        -9.9756e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0281438697129488 0.0002589225769042969 0.6701741218566895 0.339095801115036 tensor(79, device='cuda:0')\n",
            "pred tensor([-5.4836e-06, -4.9472e-06, -2.8610e-06, -1.6689e-06, -6.4552e-05,\n",
            "        -2.5153e-05, -5.1022e-05, -3.1471e-05, -7.8678e-06, -3.4511e-05,\n",
            "        -1.0133e-06, -2.9683e-05, -7.4463e-03, -2.5988e-05, -6.6042e-05,\n",
            "        -2.8801e-04, -8.0585e-05, -1.6987e-05, -6.8665e-05, -4.7386e-05,\n",
            "        -9.0599e-06, -4.3809e-05, -1.4997e-04, -1.4365e-05, -2.5570e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02936144359409809 0.0002880096435546875 0.6674758195877075 0.3392411470413208 tensor(57, device='cuda:0')\n",
            "pred tensor([-3.4161e-03, -1.3876e-04, -7.3193e-01, -9.9902e-01, -8.6487e-02,\n",
            "        -4.1779e-02, -7.2813e-04, -1.1339e-03, -3.4294e-03, -2.2960e-04,\n",
            "        -7.8142e-05, -4.3511e-05, -1.2934e-04, -8.9169e-04, -1.4238e-03,\n",
            "        -1.4198e-04, -2.6321e-04, -7.9691e-05, -2.5821e-04, -1.9968e-05,\n",
            "        -2.8458e-03, -3.3283e-04, -4.9734e-04, -8.1205e-04, -1.5306e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027857555076479912 0.00039386749267578125 0.642730712890625 0.3477148115634918 tensor(60, device='cuda:0')\n",
            "pred tensor([-7.9155e-04, -3.5114e-03, -2.1114e-03, -3.5071e-04, -5.4359e-05,\n",
            "        -1.0468e-02, -9.3002e-03, -1.2887e-04, -2.0742e-04, -5.8532e-05,\n",
            "        -4.2140e-05, -2.5129e-04, -6.3300e-05, -7.9489e-04, -3.7079e-03,\n",
            "        -2.0981e-02, -4.8041e-04, -1.0598e-04, -8.7646e-01, -1.3168e-02,\n",
            "        -1.1528e-02, -5.2214e-04, -1.9133e-05, -5.8413e-06, -1.2219e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026392042636871338 0.0006999969482421875 0.6462721228599548 0.34676826000213623 tensor(55, device='cuda:0')\n",
            "pred tensor([-3.1376e-04, -1.0834e-03, -2.2221e-03, -2.1660e-04, -1.1504e-05,\n",
            "        -1.7822e-05, -2.5988e-05, -7.8440e-05, -1.7953e-04, -1.9717e-04,\n",
            "        -1.3542e-03, -8.8835e-04, -1.7822e-05, -7.5674e-04, -9.6035e-04,\n",
            "        -4.4861e-03, -5.7030e-04, -4.6313e-05, -2.4343e-04, -5.6267e-04,\n",
            "        -1.3411e-05, -7.9334e-05, -9.6560e-06, -1.1593e-04, -2.1534e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028799699619412422 0.0012798309326171875 0.6336159110069275 0.3413151800632477 tensor(85, device='cuda:0')\n",
            "pred tensor([-9.2447e-05, -2.4109e-03, -5.8842e-04, -8.3148e-05, -4.4882e-05,\n",
            "        -2.0266e-04, -7.2837e-05, -6.1798e-04, -1.4467e-03, -1.4105e-03,\n",
            "        -1.1635e-04, -6.7329e-04, -1.0424e-03, -6.9499e-05, -1.0723e-04,\n",
            "        -4.5380e-02, -4.4897e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -2.5299e-02, -2.2876e-04, -1.4424e-04, -1.2894e-03, -2.3139e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026265393942594528 0.002201080322265625 0.6434465646743774 0.3448936641216278 tensor(74, device='cuda:0')\n",
            "pred tensor([-6.3229e-04, -2.8920e-04, -8.4782e-04, -3.1796e-03, -5.4955e-05,\n",
            "        -1.0550e-05, -2.7704e-04, -7.1096e-04, -3.0065e-04, -7.0035e-05,\n",
            "        -1.4544e-04, -1.0242e-03, -3.2961e-05, -6.7568e-04, -3.4070e-04,\n",
            "        -9.8348e-06, -2.6941e-04, -1.3409e-03, -6.0141e-05, -1.1444e-05,\n",
            "        -2.8014e-04, -1.2188e-03, -8.9493e-03, -1.7941e-05, -1.3709e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02752874605357647 0.0004444122314453125 0.6851428151130676 0.33788764476776123 tensor(83, device='cuda:0')\n",
            "pred tensor([-2.4815e-03, -9.7990e-05, -3.0100e-05, -4.2877e-03, -3.7050e-04,\n",
            "        -6.3992e-04, -1.0405e-03, -2.1652e-02, -2.4929e-03, -3.7789e-05,\n",
            "        -1.9264e-04, -7.1526e-04, -2.1875e-05, -1.3137e-04, -1.4091e-04,\n",
            "        -3.6180e-05, -1.3030e-04, -2.7275e-04, -4.9472e-05, -1.4317e-04,\n",
            "        -3.5591e-03, -1.6584e-03, -7.3969e-05, -9.5367e-05, -7.6950e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026160700246691704 0.00022602081298828125 0.7035341262817383 0.33909815549850464 tensor(76, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -2.8610e-05, -4.7684e-06, -1.2195e-04,\n",
            "        -5.6624e-06, -4.4107e-06, -2.6226e-06, -1.1820e-04, -9.1016e-05,\n",
            "        -2.6226e-06, -3.5763e-05, -2.5916e-04, -2.9802e-07, -1.6630e-05,\n",
            "        -5.8293e-05, -2.0385e-05, -1.8656e-05, -4.7374e-04, -3.2544e-05,\n",
            "        -1.1325e-06, -2.6703e-05, -5.9903e-05, -8.8811e-06, -1.6689e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02562648430466652 0.00010919570922851562 0.7060202360153198 0.33588650822639465 tensor(70, device='cuda:0')\n",
            "pred tensor([-4.5776e-05, -8.9228e-05, -3.6597e-05, -1.0848e-05, -1.5533e-04,\n",
            "        -1.2732e-04, -5.9414e-04, -3.3379e-06, -8.2850e-05, -1.2815e-05,\n",
            "        -1.7285e-06, -9.6560e-06, -1.2894e-03, -3.8981e-04, -4.5061e-05,\n",
            "        -8.1658e-06, -4.0531e-06, -1.8358e-05, -4.2462e-04, -2.5225e-04,\n",
            "        -5.4216e-04, -3.2842e-05, -4.6468e-04, -3.6621e-04, -3.5644e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027144111692905426 3.1948089599609375e-05 0.695663571357727 0.33861038088798523 tensor(71, device='cuda:0')\n",
            "pred tensor([-1.1072e-03, -1.4043e-04, -1.0242e-03, -4.6936e-02, -3.3069e-04,\n",
            "        -5.0640e-04, -6.6662e-04, -1.4365e-04, -2.0993e-04, -4.5419e-05,\n",
            "        -3.8242e-03, -2.8193e-05, -8.7118e-04, -5.5771e-03, -6.4969e-06,\n",
            "        -2.1877e-03, -9.1492e-02, -5.3287e-05, -7.7546e-05, -2.3193e-03,\n",
            "        -1.3828e-03, -5.9748e-04, -3.2842e-05, -6.2695e-01, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026938432827591896 0.00023794174194335938 0.698320209980011 0.33307480812072754 tensor(59, device='cuda:0')\n",
            "pred tensor([-9.9609e-01, -8.3506e-05, -1.0712e-02, -1.3494e-04, -1.7881e-04,\n",
            "        -2.5368e-03, -4.2498e-05, -1.2052e-04, -4.1580e-03, -1.8477e-06,\n",
            "        -4.7088e-06, -3.7372e-05, -4.4513e-04, -1.0192e-05, -2.4247e-04,\n",
            "        -2.5368e-03, -9.9182e-05, -1.4365e-04, -1.0431e-04, -1.4317e-04,\n",
            "        -1.6284e-04, -2.1338e-05, -2.9981e-05, -2.5589e-02, -3.7479e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025563184171915054 0.0013074874877929688 0.6264286637306213 0.3507266044616699 tensor(69, device='cuda:0')\n",
            "pred tensor([-9.9170e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -2.8074e-05,\n",
            "        -7.0333e-05, -3.5346e-05, -8.9586e-05, -1.0639e-04, -2.6822e-06,\n",
            "        -4.7386e-05, -8.8871e-05, -4.2963e-04, -1.3494e-04, -2.9373e-04,\n",
            "        -4.6182e-04, -5.1618e-05, -1.7107e-05, -2.5988e-05, -1.4043e-04,\n",
            "        -1.6689e-06, -6.8963e-05, -3.1137e-04, -3.7336e-04, -2.2054e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02734403870999813 0.0008368492126464844 0.6639684438705444 0.3389646112918854 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.2636e-04, -9.5062e-03, -5.4312e-04, -5.6088e-05, -3.5286e-04,\n",
            "        -1.0061e-03, -9.5725e-05, -4.0936e-04, -1.1539e-03, -2.2934e-02,\n",
            "        -8.3984e-01, -9.9805e-01, -9.9951e-01, -8.6069e-04, -7.0930e-06,\n",
            "        -7.0870e-05, -2.3305e-05, -2.9016e-04, -4.3130e-04, -1.2846e-03,\n",
            "        -7.2718e-06, -3.5346e-05, -9.6607e-04, -9.6500e-05, -8.2970e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02867172844707966 0.0007963180541992188 0.6587530970573425 0.34798958897590637 tensor(86, device='cuda:0')\n",
            "pred tensor([-2.0920e-02, -1.5039e-03, -8.2159e-04, -1.1915e-04, -5.5838e-04,\n",
            "        -8.3303e-04, -5.0507e-03, -1.4668e-03, -1.7202e-04, -2.5616e-03,\n",
            "        -5.3072e-04, -9.4299e-03, -3.4475e-04, -2.1954e-03, -1.2573e-02,\n",
            "        -7.9498e-03, -3.6907e-04, -3.1300e-03, -1.1997e-03, -3.6049e-04,\n",
            "        -2.0027e-04, -1.7166e-02, -3.6507e-03, -2.9826e-04, -1.9252e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027023963630199432 0.0008168220520019531 0.6869245767593384 0.34014636278152466 tensor(80, device='cuda:0')\n",
            "pred tensor([-2.9206e-06, -1.4997e-04, -5.3048e-06, -5.1618e-05, -1.6928e-04,\n",
            "        -4.2152e-04, -4.7379e-03, -1.0147e-03, -1.3554e-04, -1.6093e-04,\n",
            "        -3.3069e-04, -1.1673e-03, -2.3973e-04, -2.0742e-04, -2.4068e-04,\n",
            "        -8.2850e-06, -2.4629e-04, -4.4942e-04, -2.2650e-06, -1.9014e-05,\n",
            "        -8.5592e-04, -1.2243e-04, -2.7156e-04, -7.4267e-05, -6.6757e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02518766187131405 0.0009474754333496094 0.6559935808181763 0.34193477034568787 tensor(80, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -8.6927e-04, -2.7657e-05, -1.4961e-05,\n",
            "        -3.3998e-04, -4.2319e-06, -3.5524e-05, -6.8235e-04, -2.3842e-05,\n",
            "        -3.1352e-05, -1.3199e-02, -1.7881e-06, -2.1994e-04, -1.5697e-03,\n",
            "        -1.0473e-04, -7.6294e-05, -3.4332e-04, -1.2827e-04, -2.0146e-05,\n",
            "        -9.2173e-04, -2.8496e-03, -3.5477e-04, -2.6822e-05, -7.0143e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026540888473391533 9.965896606445312e-05 0.6963018774986267 0.3351527452468872 tensor(68, device='cuda:0')\n",
            "pred tensor([-3.2759e-04, -3.2687e-04, -2.4204e-03, -5.7220e-06, -1.4770e-04,\n",
            "        -6.6681e-03, -4.2796e-05, -3.4962e-03, -2.0337e-04, -8.2947e-02,\n",
            "        -1.0000e+00, -1.0000e+00, -6.4373e-06, -2.1565e-04, -8.4925e-04,\n",
            "        -1.6534e-04, -8.2636e-04, -1.3145e-02, -2.6016e-03, -3.3545e-04,\n",
            "        -1.8969e-03, -3.4428e-03, -3.7789e-05, -2.0826e-04, -2.1782e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02592417597770691 0.00014352798461914062 0.696729302406311 0.35444825887680054 tensor(67, device='cuda:0')\n",
            "pred tensor([-2.1458e-06, -1.3113e-05, -6.4790e-05, -3.4273e-05, -1.2100e-04,\n",
            "        -8.6427e-06, -5.3883e-05, -4.7088e-06, -2.5153e-05, -1.8525e-04,\n",
            "        -1.2550e-03, -1.4048e-03, -4.5133e-04, -3.3593e-04, -1.1915e-04,\n",
            "        -1.1504e-05, -1.0462e-03, -2.1482e-04, -2.7418e-06, -6.5565e-06,\n",
            "        -1.3901e-02, -4.1723e-06, -1.3983e-04, -1.8024e-04, -2.0266e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029880324378609657 0.00026226043701171875 0.688593864440918 0.339907169342041 tensor(79, device='cuda:0')\n",
            "pred tensor([-8.4817e-05, -8.8120e-03, -7.3969e-05, -1.1027e-05, -3.6120e-04,\n",
            "        -1.1867e-04, -1.6689e-06, -9.3162e-05, -5.3644e-06, -1.5080e-05,\n",
            "        -4.2915e-06, -6.7890e-05, -6.4969e-06, -1.5295e-04, -1.6212e-05,\n",
            "        -1.9908e-05, -1.5664e-04, -2.5511e-04, -5.1856e-06, -1.7464e-05,\n",
            "        -2.7239e-05, -1.3709e-06, -1.2243e-04, -5.5432e-06, -1.2052e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028755635023117065 7.486343383789062e-05 0.7160939574241638 0.3411855697631836 tensor(93, device='cuda:0')\n",
            "pred tensor([-2.8968e-05, -6.6161e-06, -3.0577e-05, -2.0683e-05, -2.7704e-04,\n",
            "        -7.9691e-05, -8.8811e-06, -1.1921e-07, -1.3876e-04, -4.7088e-06,\n",
            "        -8.2254e-06, -7.8418e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.2887e-04, -1.3113e-06, -6.8545e-06, -1.7333e-04, -4.6468e-04,\n",
            "        -4.6272e-03, -6.4373e-06, -1.0586e-03, -1.0848e-04, -1.0353e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024794358760118484 0.000640869140625 0.6522059440612793 0.33702394366264343 tensor(52, device='cuda:0')\n",
            "pred tensor([-4.7660e-04, -4.9324e-03, -7.9346e-03, -1.4252e-02, -1.5414e-04,\n",
            "        -1.7202e-04, -6.9714e-04, -3.0577e-05, -3.0591e-01, -7.3242e-02,\n",
            "        -2.4673e-02, -1.4937e-04, -3.1757e-04, -1.3971e-03, -2.6207e-03,\n",
            "        -1.1563e-05, -6.9499e-05, -1.0073e-05, -5.3465e-05, -4.3631e-05,\n",
            "        -4.2295e-04, -1.2887e-04, -1.6193e-03, -5.3444e-03, -1.3718e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024063481017947197 0.00298309326171875 0.597547173500061 0.35822638869285583 tensor(62, device='cuda:0')\n",
            "pred tensor([-1.5249e-03, -2.5392e-05, -1.1806e-03, -2.1756e-05, -1.8775e-05,\n",
            "        -3.5834e-04, -1.1504e-05, -5.5504e-04, -3.3200e-05, -8.1539e-05,\n",
            "        -6.4611e-04, -7.8869e-04, -2.8348e-04, -8.7786e-04, -1.9989e-03,\n",
            "        -2.7490e-04, -8.0948e-03, -5.7817e-05, -4.7684e-06, -1.5962e-04,\n",
            "        -2.2519e-04, -3.3665e-04, -9.3520e-05, -4.5466e-04, -1.0931e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027358941733837128 0.0007987022399902344 0.6460636854171753 0.3477724492549896 tensor(90, device='cuda:0')\n",
            "pred tensor([-3.9139e-03, -8.2195e-05, -6.8779e-03, -4.3564e-03, -1.5783e-04,\n",
            "        -2.8193e-05, -9.9951e-01, -2.7275e-04, -2.3246e-06, -4.1504e-03,\n",
            "        -2.2564e-03, -6.2287e-05, -1.5011e-03, -1.1456e-04, -4.7743e-05,\n",
            "        -1.0242e-03, -8.5592e-04, -4.8876e-06, -4.7565e-05, -2.1877e-03,\n",
            "        -8.1539e-05, -3.2830e-04, -1.8845e-02, -6.8457e-01, -1.3332e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024927394464612007 0.0006504058837890625 0.6624594926834106 0.347041517496109 tensor(76, device='cuda:0')\n",
            "pred tensor([-7.9155e-04, -2.1915e-03, -2.6226e-04, -3.0184e-04, -1.4651e-04,\n",
            "        -2.1744e-04, -1.5783e-04, -9.1457e-04, -9.4748e-04, -2.3246e-06,\n",
            "        -5.7030e-04, -8.9693e-04, -6.2585e-06, -2.2340e-04, -1.1158e-03,\n",
            "        -7.7248e-03, -7.0667e-04, -1.1414e-04, -2.8442e-02, -1.4770e-04,\n",
            "        -3.9816e-04, -1.5593e-04, -9.7632e-05, -2.8467e-04, -2.2221e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027132945135235786 0.0003452301025390625 0.700135350227356 0.3554348349571228 tensor(81, device='cuda:0')\n",
            "pred tensor([-6.4373e-04, -4.6631e-01, -4.5239e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.7881e-04, -1.8835e-05, -2.1572e-03, -1.8883e-04, -1.6737e-04,\n",
            "        -1.2636e-05, -9.8407e-05, -4.0364e-04, -9.3651e-04, -2.5879e-01,\n",
            "        -4.8304e-04, -1.4424e-04, -1.7071e-03, -9.2447e-05, -5.2273e-05,\n",
            "        -2.3007e-05, -2.1565e-04, -4.3221e-03, -1.4229e-02, -3.2949e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02753560245037079 0.0007982254028320312 0.6917380094528198 0.3381490409374237 tensor(79, device='cuda:0')\n",
            "pred tensor([-1.8201e-01, -1.8930e-03, -8.6441e-03, -5.8838e-01, -9.9902e-01,\n",
            "        -9.9854e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -9.3794e-04, -5.4121e-04, -3.3140e-04, -2.9588e-04, -2.0909e-04,\n",
            "        -1.8382e-04, -3.3379e-06, -1.2577e-05, -2.5451e-05, -1.6665e-04,\n",
            "        -1.6165e-03, -3.1757e-04, -8.5068e-04, -4.2305e-03, -1.1803e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027772607281804085 0.00040912628173828125 0.6946133375167847 0.344365656375885 tensor(85, device='cuda:0')\n",
            "pred tensor([-3.3905e-02, -3.1567e-01, -9.8730e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0691e-03, -1.4889e-04, -2.3918e-03, -1.1921e-05, -7.8142e-05,\n",
            "        -1.0931e-04, -6.4552e-05, -2.4533e-04, -3.4118e-04, -2.3880e-03,\n",
            "        -6.7329e-04, -4.2892e-04, -1.0556e-04, -6.5565e-05, -1.2789e-03,\n",
            "        -1.6153e-04, -1.6415e-04, -5.0640e-04, -3.3855e-04, -1.9121e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029845446348190308 0.00020933151245117188 0.6962539553642273 0.3404223918914795 tensor(79, device='cuda:0')\n",
            "pred tensor([-2.0993e-04, -2.0421e-04, -1.5962e-04, -4.7998e-01, -9.9854e-01,\n",
            "        -9.5654e-01, -6.2370e-04, -9.7990e-05, -3.3617e-05, -2.3782e-04,\n",
            "        -1.8024e-04, -5.3024e-03, -8.3506e-05, -8.9943e-05, -1.0691e-03,\n",
            "        -2.4819e-04, -2.4438e-04, -4.8304e-04, -9.9957e-05, -7.9691e-05,\n",
            "        -1.7881e-04, -1.5080e-05, -3.4523e-04, -1.4439e-03, -8.4043e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028217080980539322 0.0016155242919921875 0.6583613753318787 0.36901888251304626 tensor(120, device='cuda:0')\n",
            "pred tensor([-1.6546e-03, -4.1723e-04, -4.1842e-05, -2.8729e-05, -8.7452e-04,\n",
            "        -4.8518e-05, -3.4189e-04, -5.8441e-03, -9.8407e-05, -7.2670e-04,\n",
            "        -2.2292e-05, -1.0031e-04, -4.7913e-03, -3.7336e-04, -5.1618e-05,\n",
            "        -1.1206e-03, -2.2078e-04, -9.3579e-06, -1.0765e-04, -5.9187e-05,\n",
            "        -4.4417e-04, -6.8521e-04, -1.9670e-06, -9.9335e-03, -2.3975e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026928944513201714 0.0006036758422851562 0.6801110506057739 0.34419652819633484 tensor(68, device='cuda:0')\n",
            "pred tensor([-4.7469e-04, -1.5962e-04, -1.9872e-04, -3.3212e-04, -1.2779e-04,\n",
            "        -4.8876e-04, -4.7028e-05, -4.8339e-05, -1.1563e-05, -7.2539e-05,\n",
            "        -3.4738e-04, -2.0909e-04, -1.7607e-04, -6.8665e-05, -1.7738e-04,\n",
            "        -1.0881e-03, -8.3447e-06, -2.6321e-04, -1.7762e-05, -1.1325e-06,\n",
            "        -9.2387e-06, -5.9664e-05, -2.4140e-05, -3.9053e-04, -2.8849e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027283301576972008 0.000286102294921875 0.6994882822036743 0.39551034569740295 tensor(87, device='cuda:0')\n",
            "pred tensor([-2.0862e-06, -1.2207e-03, -1.8966e-04, -1.2827e-04, -2.8610e-05,\n",
            "        -2.7299e-05, -2.4676e-05, -1.6689e-05, -5.0843e-05, -5.5170e-04,\n",
            "        -2.0182e-04, -3.7789e-05, -6.0797e-06, -6.3539e-05, -2.0504e-04,\n",
            "        -2.8193e-05, -1.4043e-04, -1.0073e-04, -1.2052e-04, -4.9448e-04,\n",
            "        -5.0888e-03, -5.2414e-03, -3.5934e-03, -2.2078e-04, -5.0735e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032721955329179764 9.1552734375e-05 0.717289924621582 0.34116584062576294 tensor(93, device='cuda:0')\n",
            "pred tensor([-9.9951e-01, -1.2934e-05, -1.2887e-04, -1.2517e-06, -2.4438e-06,\n",
            "        -2.4719e-03, -2.0826e-04, -3.3283e-04, -2.0580e-03, -5.5432e-05,\n",
            "        -9.8348e-06, -3.6120e-04, -8.0585e-05, -1.9014e-05, -1.1225e-03,\n",
            "        -1.1367e-04, -2.3186e-05, -2.2400e-02, -6.6042e-05, -9.2804e-05,\n",
            "        -2.8729e-05, -2.1458e-06, -2.3687e-04, -2.4855e-05, -2.5988e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02996307611465454 0.00013875961303710938 0.7306029796600342 0.34147998690605164 tensor(88, device='cuda:0')\n",
            "pred tensor([-4.7684e-07, -7.7486e-07, -5.9605e-07, -1.5080e-05, -5.6028e-06,\n",
            "        -6.7353e-06, -1.6868e-04, -7.7009e-04, -2.6840e-02, -9.3460e-04,\n",
            "        -9.2447e-05, -1.6093e-06, -6.0701e-04, -9.7632e-05, -5.3048e-06,\n",
            "        -1.2982e-04, -1.1921e-06, -4.5300e-06, -2.3365e-05, -2.6011e-04,\n",
            "        -7.0810e-04, -1.1915e-04, -3.0541e-04, -2.3055e-04, -1.2982e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.033722322434186935 0.00014925003051757812 0.7342697381973267 0.3415262699127197 tensor(95, device='cuda:0')\n",
            "68\n",
            "pred tensor([-5.1737e-04, -1.5533e-04, -6.7186e-04, -3.2830e-04, -2.9254e-04,\n",
            "        -3.7479e-04, -2.1756e-05, -7.8430e-03, -4.5037e-04, -4.2877e-03,\n",
            "        -1.6665e-04, -4.2057e-04, -3.5381e-03, -1.6327e-03, -4.9515e-03,\n",
            "        -7.2479e-03, -2.8496e-03, -4.5037e-04, -1.2569e-03, -1.1146e-05,\n",
            "        -7.4565e-05, -8.5235e-06, -6.1893e-04, -6.2656e-04, -1.7130e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03179740160703659 0.0002727508544921875 0.7414886355400085 0.3578200042247772 tensor(92, device='cuda:0')\n",
            "pred tensor([-2.8000e-03, -8.4448e-04, -2.6286e-05, -1.6475e-04, -3.2759e-04,\n",
            "        -1.6606e-04, -3.5763e-06, -2.8968e-05, -1.7953e-04, -1.7464e-05,\n",
            "        -6.9475e-04, -3.9291e-03, -5.7755e-03, -9.3765e-03, -2.1827e-04,\n",
            "        -1.4591e-04, -6.5660e-04, -4.0841e-04, -3.5248e-03, -1.7871e-01,\n",
            "        -2.9068e-03, -3.9597e-03, -5.5762e-01, -9.9902e-01, -9.9902e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032007113099098206 0.0017547607421875 0.6767866611480713 0.3473780155181885 tensor(101, device='cuda:0')\n",
            "pred tensor([-2.9707e-04, -1.9908e-05, -4.1723e-07, -2.6047e-05, -4.0054e-04,\n",
            "        -5.0843e-05, -5.6267e-05, -1.3447e-04, -9.9182e-05, -1.7345e-05,\n",
            "        -6.3049e-02, -2.6405e-05, -3.7611e-05, -3.6693e-04, -1.2112e-03,\n",
            "        -5.1022e-05, -1.3030e-04, -5.9187e-05, -4.1664e-05, -6.5565e-07,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -5.0812e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032657042145729065 0.0004477500915527344 0.7463098764419556 0.34177741408348083 tensor(77, device='cuda:0')\n",
            "pred tensor([-7.1669e-04, -8.9693e-04, -9.3079e-04, -7.6580e-04, -2.2960e-04,\n",
            "        -1.3100e-02, -1.8060e-05, -5.1439e-05, -4.8399e-04, -1.1635e-04,\n",
            "        -7.6580e-04, -8.1825e-04, -5.9748e-04, -2.7481e-02, -1.2064e-03,\n",
            "        -3.4428e-03, -4.4942e-04, -1.4880e-01, -8.6260e-04, -6.3248e-03,\n",
            "        -1.5116e-04, -1.3971e-03, -1.8875e-02, -1.6309e-01, -6.4148e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03171725571155548 0.00019788742065429688 0.7354103326797485 0.35950982570648193 tensor(83, device='cuda:0')\n",
            "pred tensor([-1.8034e-03, -1.2617e-03, -8.5115e-05, -2.4068e-04, -3.0935e-05,\n",
            "        -1.8477e-06, -1.2517e-06, -5.7161e-05, -3.5524e-05, -6.7353e-05,\n",
            "        -8.0109e-04, -5.8949e-05, -1.9717e-04, -9.5010e-05, -4.5300e-06,\n",
            "        -3.8853e-03, -1.4641e-02, -1.2302e-03, -2.2926e-03, -1.5251e-02,\n",
            "        -2.9932e-01, -9.2041e-01, -5.1709e-01, -7.7844e-05, -4.3631e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03396494686603546 0.0003256797790527344 0.7071696519851685 0.3545435667037964 tensor(80, device='cuda:0')\n",
            "pred tensor([-5.0664e-06, -1.7285e-06, -2.4974e-05, -3.5167e-06, -2.3842e-07,\n",
            "        -4.1127e-06, -1.1921e-07, -1.8656e-05, -2.0444e-05, -4.8494e-04,\n",
            "        -2.4204e-03, -3.6049e-04, -8.7857e-05, -5.2023e-04, -2.6207e-03,\n",
            "        -1.8454e-04, -1.2036e-01, -2.8725e-03, -9.5963e-06, -4.2224e-04,\n",
            "        -7.2241e-05, -6.4373e-06, -4.1723e-07, -2.0742e-04, -1.9491e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029687082394957542 0.000362396240234375 0.6822644472122192 0.3376021385192871 tensor(69, device='cuda:0')\n",
            "pred tensor([-8.8513e-05, -2.8610e-06, -7.1406e-05, -3.4392e-05, -9.7156e-06,\n",
            "        -2.4629e-04, -1.1635e-04, -1.4019e-03, -1.9908e-05, -1.5235e-04,\n",
            "        -7.5996e-05, -6.7854e-04, -1.3185e-04, -7.5161e-05, -1.6422e-03,\n",
            "        -9.5010e-05, -9.2888e-04, -4.9324e-03, -6.7383e-01, -9.5605e-01,\n",
            "        -9.9951e-01, -5.9605e-07, -3.6955e-06, -1.4305e-06, -2.8014e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027152840048074722 0.001346588134765625 0.6603888273239136 0.3462260961532593 tensor(53, device='cuda:0')\n",
            "pred tensor([-2.2054e-06, -3.2783e-06, -1.1146e-05, -1.1367e-04, -6.4254e-05,\n",
            "        -1.4305e-05, -8.4043e-06, -2.4557e-05, -4.7684e-07, -1.1235e-04,\n",
            "        -4.2975e-05, -7.1526e-07, -5.8746e-04, -2.0862e-06, -2.6846e-04,\n",
            "        -4.3333e-05, -2.3842e-07, -3.2187e-05, -1.0669e-05, -1.6093e-06,\n",
            "        -3.2878e-04, -2.3901e-05, -2.4378e-05, -2.9981e-05, -3.0339e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029824404045939445 0.0008211135864257812 0.6669553518295288 0.3720891773700714 tensor(71, device='cuda:0')\n",
            "pred tensor([-3.5107e-05, -9.2173e-04, -8.0795e-03, -6.4790e-05, -4.2140e-05,\n",
            "        -1.9038e-04, -2.1219e-05, -1.5116e-04, -4.7088e-06, -1.6272e-05,\n",
            "        -5.9605e-07, -1.8568e-03, -7.3242e-02, -9.4531e-01, -9.9463e-01,\n",
            "        -1.0000e+00, -9.4235e-05, -1.0080e-03, -4.0283e-01, -3.5691e-04,\n",
            "        -2.9037e-02, -2.3973e-04, -6.0797e-06, -5.8949e-05, -2.1911e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025227613747119904 0.001087188720703125 0.6601933240890503 0.37986814975738525 tensor(53, device='cuda:0')\n",
            "pred tensor([-1.8740e-04, -1.3247e-03, -1.7242e-03, -2.8133e-04, -3.6407e-04,\n",
            "        -2.1648e-03, -2.8625e-02, -1.6983e-02, -1.9872e-04, -1.1772e-04,\n",
            "        -9.9659e-04, -8.7023e-06, -1.0806e-04, -4.1187e-05, -1.1325e-06,\n",
            "        -1.1654e-03, -1.0815e-03, -2.2256e-04, -5.7983e-03, -8.2855e-03,\n",
            "        -7.5245e-04, -5.7793e-04, -9.2447e-05, -5.8270e-04, -3.0899e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02991297096014023 0.0013580322265625 0.693938136100769 0.34073424339294434 tensor(89, device='cuda:0')\n",
            "pred tensor([-1.2188e-03, -6.1095e-05, -1.3709e-06, -1.3769e-05, -5.1022e-05,\n",
            "        -4.3297e-04, -3.3975e-05, -5.9426e-05, -1.4420e-02, -1.1104e-04,\n",
            "        -5.6028e-04, -1.5783e-04, -1.8616e-02, -5.7520e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -7.6294e-06, -9.6083e-05,\n",
            "        -1.8477e-06, -1.8716e-05, -8.5831e-05, -1.3232e-05, -1.8477e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03266235813498497 0.00101470947265625 0.7949329018592834 0.3651624023914337 tensor(122, device='cuda:0')\n",
            "pred tensor([-3.0398e-06, -2.1338e-05, -1.8120e-05, -6.0797e-06, -8.9228e-05,\n",
            "        -3.6263e-04, -1.0192e-05, -1.0931e-04, -1.5366e-02, -1.2815e-05,\n",
            "        -2.0623e-05, -4.3907e-03, -4.1652e-04, -4.5776e-05, -5.5432e-05,\n",
            "        -2.9469e-04, -5.9903e-05, -1.5640e-03, -2.1100e-05, -5.0068e-06,\n",
            "        -3.4273e-05, -3.4809e-05, -1.1194e-04, -2.3842e-06, -5.2691e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032893355935811996 0.0005369186401367188 0.8057741522789001 0.3621917963027954 tensor(137, device='cuda:0')\n",
            "pred tensor([-1.7536e-04, -9.7370e-04, -2.2934e-02, -2.8343e-03, -1.3695e-03,\n",
            "        -2.6016e-03, -1.6422e-03, -9.6500e-05, -1.3983e-04, -1.2159e-05,\n",
            "        -1.5855e-05, -3.0632e-03, -3.8362e-04, -2.3365e-03, -6.1455e-03,\n",
            "        -7.4100e-04, -4.0829e-05, -6.8235e-04, -1.2100e-04, -3.3112e-03,\n",
            "        -8.3203e-01, -9.9902e-01, -6.3133e-04, -8.8513e-05, -2.2602e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03152838721871376 0.0015316009521484375 0.7664144039154053 0.35085275769233704 tensor(118, device='cuda:0')\n",
            "pred tensor([-6.3629e-03, -1.2522e-03, -1.7464e-04, -2.0943e-03, -1.1246e-02,\n",
            "        -1.6584e-03, -3.6144e-03, -5.4703e-03, -1.5541e-02, -4.9744e-03,\n",
            "        -2.6077e-02, -1.9670e-05, -2.2697e-04, -5.1546e-04, -4.1580e-04,\n",
            "        -3.5834e-04, -1.7643e-05, -1.1683e-05, -4.3988e-05, -2.0337e-04,\n",
            "        -5.7602e-04, -2.3918e-03, -2.2471e-05, -5.7129e-02, -1.4198e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03547331318259239 0.0004343986511230469 0.8025816679000854 0.35682711005210876 tensor(131, device='cuda:0')\n",
            "pred tensor([-2.1231e-04, -2.8667e-03, -1.5080e-05, -1.0986e-03, -1.6165e-03,\n",
            "        -1.9312e-05, -1.3769e-05, -6.9714e-04, -7.6580e-04, -8.1205e-04,\n",
            "        -1.2672e-02, -1.1292e-03, -2.0027e-04, -7.8249e-04, -1.0473e-04,\n",
            "        -3.3498e-05, -3.1257e-04, -7.5758e-05, -1.8454e-04, -1.4267e-03,\n",
            "        -2.8133e-04, -2.8920e-04, -4.8409e-03, -9.9121e-01, -3.7964e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029775697737932205 0.0005536079406738281 0.7403044700622559 0.34648069739341736 tensor(104, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -9.4336e-01, -9.4580e-01, -9.8730e-01, -8.4668e-01,\n",
            "        -9.9756e-01, -9.9951e-01, -1.0000e+00, -1.3232e-04, -6.1798e-03,\n",
            "        -1.7443e-03, -1.3494e-04, -2.2259e-03, -1.0366e-03, -1.9717e-04,\n",
            "        -1.7958e-03, -1.5039e-03, -3.5357e-04, -2.4438e-04, -3.2842e-05,\n",
            "        -1.3411e-05, -1.4067e-05, -3.3808e-04, -1.8096e-04, -5.6744e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03070066310465336 0.0002503395080566406 0.7261437177658081 0.3436395525932312 tensor(89, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -1.2934e-05, -5.0843e-05, -3.1471e-05,\n",
            "        -8.3506e-05, -8.6260e-04, -7.8738e-05, -1.1146e-04, -2.7103e-03,\n",
            "        -3.6430e-03, -5.0843e-05, -8.4114e-04, -8.3804e-05, -8.4460e-05,\n",
            "        -2.0504e-04, -9.7156e-06, -1.5497e-05, -1.1473e-03, -1.0598e-04,\n",
            "        -6.8128e-05, -5.0735e-04, -2.9254e-04, -6.8903e-04, -1.2159e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031125104054808617 0.0004048347473144531 0.7117242217063904 0.34881529211997986 tensor(94, device='cuda:0')\n",
            "pred tensor([-3.8147e-04, -3.5458e-03, -1.0900e-03, -1.3800e-03, -9.0027e-04,\n",
            "        -9.9170e-01, -9.9951e-01, -1.0000e+00, -1.0000e+00, -1.6012e-03,\n",
            "        -5.3692e-04, -5.3596e-04, -7.0870e-05, -2.9007e-02, -2.4426e-01,\n",
            "        -1.4868e-03, -1.6165e-03, -2.7370e-04, -6.7043e-04, -2.0027e-04,\n",
            "        -5.4550e-04, -7.6866e-04, -2.1152e-03, -7.5388e-04, -8.7500e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031896770000457764 0.0008478164672851562 0.7148193120956421 0.35267162322998047 tensor(102, device='cuda:0')\n",
            "pred tensor([-8.7595e-04, -1.3428e-02, -7.8106e-04, -1.0735e-02, -1.2147e-04,\n",
            "        -7.6592e-05, -3.5801e-03, -3.1495e-04, -1.5764e-03, -1.1444e-05,\n",
            "        -1.0192e-04, -1.5717e-02, -2.8253e-04, -6.2180e-03, -1.2147e-04,\n",
            "        -3.0696e-05, -4.6825e-04, -7.9036e-05, -2.2590e-05, -1.9493e-03,\n",
            "        -1.7107e-05, -1.6630e-05, -9.7156e-06, -6.1572e-05, -7.3099e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029560817405581474 0.0004277229309082031 0.739890456199646 0.3657315969467163 tensor(83, device='cuda:0')\n",
            "pred tensor([-3.2830e-04, -3.0098e-03, -2.0504e-04, -1.7285e-05, -2.2340e-04,\n",
            "        -4.6635e-04, -3.3545e-04, -8.5592e-04, -7.9632e-04, -3.9577e-04,\n",
            "        -5.5170e-04, -5.7399e-05, -1.9312e-05, -2.0826e-04, -1.0806e-04,\n",
            "        -5.3704e-05, -4.7183e-04, -1.3447e-04, -2.9588e-04, -3.9935e-05,\n",
            "        -3.9756e-05, -4.6921e-03, -7.2837e-05, -5.6088e-05, -1.1921e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03378612548112869 0.00031280517578125 0.7463023662567139 0.37104225158691406 tensor(107, device='cuda:0')\n",
            "pred tensor([-8.4817e-05, -1.8167e-04, -5.0259e-04, -2.5269e-02, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0031e-04, -5.5838e-04, -2.8656e-02,\n",
            "        -4.8041e-04, -1.9908e-05, -4.7374e-04, -3.4666e-04, -8.3804e-05,\n",
            "        -1.1950e-03, -8.6129e-05, -9.7632e-05, -1.2052e-04, -3.3200e-05,\n",
            "        -6.6578e-05, -3.3212e-04, -1.8096e-04, -1.0723e-04, -5.4538e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03143720701336861 0.0006818771362304688 0.7856407761573792 0.33595407009124756 tensor(74, device='cuda:0')\n",
            "pred tensor([-1.6737e-04, -3.0220e-05, -1.0389e-04, -1.7703e-05, -2.3508e-04,\n",
            "        -8.4817e-05, -3.9196e-04, -5.5194e-05, -1.8120e-05, -2.9945e-04,\n",
            "        -2.1935e-05, -3.4153e-05, -4.1664e-05, -3.9935e-06, -1.6344e-04,\n",
            "        -1.3709e-04, -3.1376e-04, -1.4553e-03, -6.6817e-05, -9.2804e-05,\n",
            "        -3.7694e-04, -4.3983e-03, -5.4240e-06, -2.8610e-05, -1.0073e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030351800844073296 0.00017452239990234375 0.7572295665740967 0.34951627254486084 tensor(80, device='cuda:0')\n",
            "pred tensor([-6.2525e-05, -5.1832e-04, -1.2109e-01, -4.6563e-04, -2.3823e-03,\n",
            "        -5.4893e-03, -9.1858e-02, -3.5645e-02, -2.8172e-03, -2.4011e-01,\n",
            "        -4.0321e-03, -4.3152e-02, -2.2564e-03, -1.1635e-04, -2.0826e-04,\n",
            "        -5.3704e-05, -4.1842e-05, -4.6921e-04, -2.7955e-05, -6.8963e-05,\n",
            "        -4.3392e-04, -1.9407e-04, -1.9014e-05, -7.9334e-05, -1.8537e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032771382480859756 0.00188446044921875 0.6824719309806824 0.35082074999809265 tensor(119, device='cuda:0')\n",
            "pred tensor([-7.0035e-05, -1.8787e-01, -1.6224e-04, -2.6054e-03, -3.9816e-04,\n",
            "        -7.5378e-02, -2.1744e-02, -2.1517e-05, -1.7929e-03, -2.1076e-04,\n",
            "        -8.6427e-06, -7.8082e-06, -6.5565e-06, -2.2411e-05, -4.3809e-05,\n",
            "        -3.6621e-04, -4.4703e-06, -1.3447e-04, -6.5029e-05, -2.0862e-06,\n",
            "        -2.3842e-07, -1.7881e-07, -4.7684e-07, -4.3511e-06, -2.5690e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029995238408446312 0.0003876686096191406 0.7330297827720642 0.362374484539032 tensor(90, device='cuda:0')\n",
            "pred tensor([-1.2934e-05, -2.1458e-06, -1.1802e-05, -3.8683e-05, -1.6689e-06,\n",
            "        -2.0182e-04, -6.6101e-02, -1.5295e-04, -3.8290e-04, -1.1963e-04,\n",
            "        -2.3365e-05, -2.4498e-05, -5.9605e-06, -5.1641e-04, -9.2447e-05,\n",
            "        -5.7817e-05, -2.2602e-04, -5.6763e-02, -1.7285e-06, -3.4153e-05,\n",
            "        -1.0908e-05, -2.3842e-06, -4.4513e-04, -4.0829e-05, -1.1194e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030548404902219772 0.00015211105346679688 0.7548125982284546 0.3365902006626129 tensor(75, device='cuda:0')\n",
            "pred tensor([-9.2447e-05, -2.9869e-03, -9.2163e-03, -8.4043e-06, -3.3627e-03,\n",
            "        -3.7551e-04, -3.5226e-05, -2.4768e-01, -9.8765e-05, -3.0816e-05,\n",
            "        -1.3222e-02, -1.0000e+00, -1.0000e+00, -1.0000e+00, -2.2340e-04,\n",
            "        -1.0192e-05, -8.3862e-02, -4.7088e-06, -2.4438e-06, -2.8920e-04,\n",
            "        -1.2517e-06, -2.6226e-06, -5.8949e-05, -2.6226e-04, -1.5736e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03398345410823822 0.00022125244140625 0.7383083701133728 0.3420186936855316 tensor(98, device='cuda:0')\n",
            "pred tensor([-2.7776e-05, -1.7285e-06, -1.2243e-04, -9.9854e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -1.1492e-03, -3.6061e-05, -3.0696e-05, -4.1723e-06,\n",
            "        -4.5061e-05, -6.0201e-06, -8.2195e-05, -2.5225e-04, -1.0014e-05,\n",
            "        -7.9691e-05, -1.1820e-04, -9.8348e-06, -2.8920e-04, -3.2544e-05,\n",
            "        -2.5988e-05, -6.3896e-03, -2.3365e-05, -5.4836e-06, -1.1384e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03552742674946785 0.0002465248107910156 0.7225223779678345 0.35511261224746704 tensor(113, device='cuda:0')\n",
            "pred tensor([-8.3506e-05, -3.7766e-04, -4.5240e-05, -4.7851e-04, -1.2100e-04,\n",
            "        -1.8060e-05, -3.6597e-05, -1.9610e-05, -3.0220e-05, -2.2519e-04,\n",
            "        -7.8142e-05, -3.5346e-05, -7.3314e-06, -1.5855e-05, -1.5664e-04,\n",
            "        -1.4365e-04, -6.2525e-05, -7.4506e-06, -1.2457e-05, -1.7881e-06,\n",
            "        -2.2709e-05, -1.0031e-04, -2.7704e-04, -4.3511e-06, -4.7743e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0335981659591198 0.0007066726684570312 0.6764661073684692 0.3404385447502136 tensor(88, device='cuda:0')\n",
            "pred tensor([-2.7537e-05, -2.7299e-05, -2.9206e-06, -2.2054e-06, -3.3498e-05,\n",
            "        -3.3081e-05, -4.4703e-06, -7.6294e-05, -6.5565e-06, -9.6083e-05,\n",
            "        -1.7548e-02, -9.9951e-01, -9.9951e-01, -6.3539e-05, -1.1444e-03,\n",
            "        -6.2048e-05, -6.0380e-05, -5.6028e-06, -1.1921e-07, -1.6212e-05,\n",
            "        -1.3030e-04, -7.1406e-05, -2.0862e-06, -4.5300e-06, -1.9550e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032030776143074036 0.0010843276977539062 0.6921173930168152 0.34638237953186035 tensor(69, device='cuda:0')\n",
            "pred tensor([-1.0228e-04, -5.8293e-05, -1.0073e-04, -2.0447e-02, -3.6793e-03,\n",
            "        -1.0729e-06, -9.2804e-05, -4.3893e-04, -5.5611e-05, -7.2594e-03,\n",
            "        -2.7657e-05, -8.7976e-04, -2.2263e-02, -5.8899e-02, -1.7957e-01,\n",
            "        -7.2241e-05, -2.1935e-05, -2.9016e-04, -3.8457e-04, -2.0802e-05,\n",
            "        -3.1257e-04, -8.5735e-04, -6.1810e-05, -4.9925e-04, -9.9468e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.033884432166814804 0.002079010009765625 0.6494916081428528 0.3412044644355774 tensor(79, device='cuda:0')\n",
            "pred tensor([-1.8096e-04, -6.3777e-06, -1.3447e-04, -2.0146e-05, -4.5300e-06,\n",
            "        -1.6570e-05, -1.6665e-04, -9.9951e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -7.0930e-06, -3.9339e-06, -6.5565e-07, -5.3644e-06, -5.9605e-08,\n",
            "        -2.3842e-07, -5.2035e-05, -1.1206e-05, -4.1127e-06, -6.2752e-04,\n",
            "        -6.8545e-06, -3.1590e-06, -5.6505e-05, -3.0339e-05, -5.7161e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02988450415432453 0.001895904541015625 0.6753987073898315 0.3412344455718994 tensor(86, device='cuda:0')\n",
            "pred tensor([-1.4126e-05, -1.6093e-06, -6.7949e-06, -7.1526e-07, -5.9605e-08,\n",
            "        -2.3842e-06, -1.2100e-05, -1.7881e-07, -2.9802e-07,  0.0000e+00,\n",
            "        -1.1921e-07, -3.5763e-07,  0.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -4.4703e-06, -8.6486e-05, -9.1016e-01, -1.1206e-03, -4.8876e-05,\n",
            "        -1.0071e-02, -2.9373e-04, -1.0729e-06, -3.6955e-06, -1.4439e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.036692000925540924 0.0003237724304199219 0.7235798239707947 0.3410646319389343 tensor(96, device='cuda:0')\n",
            "pred tensor([-1.7881e-04, -2.5606e-04, -1.1802e-05, -2.9862e-05, -1.1635e-04,\n",
            "        -6.9043e-01, -3.3855e-04, -8.1205e-04, -1.2481e-04, -5.4955e-05,\n",
            "        -7.5817e-04, -2.6464e-05, -1.1206e-05, -1.4365e-04, -2.1636e-05,\n",
            "        -5.4777e-05, -1.3590e-05, -1.9717e-04, -3.7372e-05, -5.2273e-05,\n",
            "        -9.7990e-05, -9.2089e-05, -5.3704e-05, -7.4816e-04, -1.0473e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030815253034234047 9.775161743164062e-05 0.723263144493103 0.3474569320678711 tensor(77, device='cuda:0')\n",
            "pred tensor([-3.4153e-05, -6.1989e-06, -8.2552e-05, -1.5354e-04, -1.5616e-05,\n",
            "        -1.2255e-03, -1.7548e-03, -1.0633e-03, -1.4257e-04, -2.2113e-05,\n",
            "        -1.3123e-03, -1.5962e-04, -1.9562e-04, -5.7125e-04, -1.9038e-04,\n",
            "        -3.1376e-04, -2.7256e-03, -2.5606e-04, -4.0531e-06, -4.8027e-03,\n",
            "        -7.7486e-07, -3.4738e-04, -2.3529e-02, -1.3351e-05, -5.6934e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02792852371931076 9.5367431640625e-06 0.7610065937042236 0.3503716289997101 tensor(81, device='cuda:0')\n",
            "pred tensor([-1.3399e-04, -1.5125e-01, -3.8242e-03, -7.1068e-03, -5.5115e-02,\n",
            "        -1.3094e-03, -6.9046e-04, -4.7028e-05, -1.0757e-03, -4.6806e-03,\n",
            "        -2.5129e-04, -5.3072e-04, -8.4610e-03, -5.5008e-03, -3.9282e-01,\n",
            "        -1.2693e-03, -3.7694e-04, -7.9274e-06, -5.3358e-04, -2.3663e-05,\n",
            "        -6.5374e-04, -2.3592e-04, -4.8518e-05, -8.2254e-06, -1.4901e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03238470107316971 0.0001983642578125 0.7211095094680786 0.3399915397167206 tensor(80, device='cuda:0')\n",
            "pred tensor([-2.0325e-05, -1.9407e-04, -8.3545e-01, -1.1384e-05, -1.9670e-05,\n",
            "        -4.8780e-04, -1.1563e-05, -4.8518e-05, -2.2960e-04, -1.3123e-03,\n",
            "        -1.4150e-04, -9.1016e-05, -1.8811e-04, -5.9414e-04, -4.0192e-02,\n",
            "        -5.3497e-02, -3.8362e-04, -9.7217e-01, -9.9805e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.8239e-04, -8.2302e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029767177999019623 0.001194000244140625 0.6834204196929932 0.3402889668941498 tensor(80, device='cuda:0')\n",
            "pred tensor([-1.8740e-04, -2.5129e-04, -1.3089e-04, -1.6093e-04, -4.3654e-04,\n",
            "        -1.9491e-04, -1.9550e-05, -3.2187e-05, -1.8060e-05, -1.0633e-03,\n",
            "        -2.0909e-04, -2.0802e-05, -2.6011e-04, -5.5850e-05, -2.0325e-05,\n",
            "        -2.0182e-04, -1.3137e-04, -1.0729e-06, -8.6927e-04, -4.6313e-05,\n",
            "        -7.6723e-04, -2.8968e-05, -1.6272e-05, -4.8494e-04, -2.4724e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029676489531993866 0.0015659332275390625 0.689403235912323 0.3557761013507843 tensor(97, device='cuda:0')\n",
            "pred tensor([-2.6417e-04, -5.7161e-05, -1.3647e-03, -9.8145e-02, -1.0000e+00,\n",
            "        -1.0000e+00, -3.0689e-03, -3.6550e-04, -1.1986e-02, -1.4639e-03,\n",
            "        -3.7074e-05, -3.9458e-05, -6.3372e-04, -5.2910e-03, -9.7370e-04,\n",
            "        -6.7177e-03, -1.8295e-02, -2.3139e-04, -1.8668e-04, -1.3590e-03,\n",
            "        -2.0742e-04, -1.9836e-03, -3.0880e-03, -3.2115e-04, -1.1635e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029550885781645775 0.0023784637451171875 0.6560226082801819 0.3661584258079529 tensor(90, device='cuda:0')\n",
            "69\n",
            "pred tensor([-7.3338e-04, -6.1321e-04, -2.1875e-05, -6.5804e-05, -2.2049e-03,\n",
            "        -3.3975e-06, -1.3447e-04, -3.7861e-04, -7.0572e-04, -4.3511e-06,\n",
            "        -1.6870e-03, -9.7632e-05, -1.6212e-05, -3.6776e-05, -3.0303e-04,\n",
            "        -2.7790e-03, -2.0587e-04, -1.8668e-04, -1.5917e-03, -5.3644e-06,\n",
            "        -6.8426e-05, -6.1333e-05, -6.1572e-05, -8.2195e-05, -1.0712e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03130311146378517 0.0007634162902832031 0.7144083380699158 0.3514612913131714 tensor(84, device='cuda:0')\n",
            "pred tensor([-1.7405e-04, -2.2960e-04, -6.4240e-03, -4.7646e-03, -1.8954e-05,\n",
            "        -1.4591e-04, -8.7595e-04, -4.1723e-06, -5.1260e-05, -1.9121e-03,\n",
            "        -1.1194e-04, -5.5611e-05, -1.5221e-03, -6.9737e-05, -3.8803e-05,\n",
            "        -1.8382e-04, -2.7790e-03, -1.9646e-04, -4.2391e-04, -1.7953e-04,\n",
            "        -3.3736e-05, -1.4782e-05, -9.9805e-01, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031348079442977905 0.00022125244140625 0.7468350529670715 0.3452300429344177 tensor(96, device='cuda:0')\n",
            "pred tensor([-3.7193e-04, -4.7028e-05, -5.1260e-06, -4.4346e-05, -5.8770e-05,\n",
            "        -8.6486e-05, -9.1791e-06, -1.0556e-04, -6.6805e-04, -1.1063e-04,\n",
            "        -3.6263e-04, -1.9717e-04, -3.6895e-05, -2.9683e-05, -3.4332e-04,\n",
            "        -2.7490e-04, -6.7353e-05, -8.8811e-06, -9.9658e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -1.5378e-05, -8.8215e-05, -3.6061e-05, -2.6703e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03238004073500633 0.00017261505126953125 0.7163215279579163 0.34404176473617554 tensor(97, device='cuda:0')\n",
            "pred tensor([-1.7881e-06, -1.5354e-04, -1.8239e-04, -3.2830e-04, -3.4273e-05,\n",
            "        -2.9564e-05, -2.4676e-05, -1.0723e-04, -1.9789e-04, -2.1148e-04,\n",
            "        -1.5533e-04, -9.2554e-04, -4.1504e-03, -1.7881e-06, -2.3842e-06,\n",
            "        -2.8275e-02, -6.1083e-04, -1.3232e-05, -9.1791e-06, -1.7262e-04,\n",
            "        -1.4233e-01, -7.2718e-06, -1.7175e-01, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031725190579891205 0.00058746337890625 0.6775937676429749 0.3606662154197693 tensor(114, device='cuda:0')\n",
            "pred tensor([-6.3610e-04, -2.1231e-04, -6.2065e-03, -1.7059e-04, -1.5736e-05,\n",
            "        -1.7333e-04, -1.4931e-02, -2.2960e-04, -5.1439e-05, -2.3849e-02,\n",
            "        -2.1155e-01, -5.1212e-04, -9.5367e-07, -1.8969e-03, -2.6685e-01,\n",
            "        -2.8193e-05, -4.1723e-07, -4.0352e-05, -3.6068e-03, -5.7817e-06,\n",
            "        -4.7684e-07, -1.9800e-01, -9.9951e-01, -1.8477e-06, -5.4240e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03218337148427963 0.0001544952392578125 0.7204548120498657 0.34214404225349426 tensor(91, device='cuda:0')\n",
            "pred tensor([-1.2100e-04, -9.2346e-02, -2.0802e-05, -4.1842e-05, -1.8454e-04,\n",
            "        -3.8940e-02, -2.5225e-04, -3.9339e-06, -1.1325e-05, -5.1022e-05,\n",
            "        -7.1526e-06, -1.5557e-05, -2.6464e-05, -2.7418e-05, -1.7262e-04,\n",
            "        -1.1861e-05, -1.0262e-03, -2.0428e-03, -2.2292e-05, -1.1963e-04,\n",
            "        -2.3723e-05, -4.2319e-06, -5.1439e-05, -6.3515e-04, -1.4937e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03309348598122597 8.821487426757812e-05 0.718896210193634 0.33977365493774414 tensor(82, device='cuda:0')\n",
            "pred tensor([-2.2471e-05, -2.5129e-04, -7.3318e-03, -7.6065e-03, -2.7585e-04,\n",
            "        -1.4842e-05, -8.2850e-05, -2.0564e-05, -3.6895e-05, -8.9404e-01,\n",
            "        -7.0703e-01, -5.4980e-01, -1.3666e-03, -1.1835e-03, -6.5576e-01,\n",
            "        -4.8494e-04, -1.3447e-04, -1.2338e-04, -2.2049e-03, -1.7481e-03,\n",
            "        -2.0993e-04, -4.6492e-05, -7.7248e-05, -2.8014e-06, -6.1333e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03302399069070816 0.0013523101806640625 0.6575893759727478 0.34312573075294495 tensor(92, device='cuda:0')\n",
            "pred tensor([-9.3877e-05, -2.0493e-02, -1.6391e-05, -6.7890e-05, -7.2060e-03,\n",
            "        -5.2414e-03, -6.5117e-03, -2.4378e-05, -1.6153e-05, -1.2052e-04,\n",
            "        -1.0550e-05, -2.5868e-05, -4.7028e-05, -1.3661e-04, -2.3365e-05,\n",
            "        -1.9848e-05, -1.7583e-05, -4.9114e-05, -2.3592e-04, -3.7804e-03,\n",
            "        -7.7844e-05, -1.5903e-04, -1.5821e-03, -1.6344e-04, -1.9789e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03295280039310455 0.0011882781982421875 0.6509441137313843 0.3552657663822174 tensor(86, device='cuda:0')\n",
            "pred tensor([-1.1139e-03, -5.2977e-04, -4.5955e-05, -3.6979e-04, -7.4863e-05,\n",
            "        -8.5258e-04, -4.3297e-04, -6.3229e-04, -2.3878e-04, -4.6563e-04,\n",
            "        -1.2195e-04, -5.7161e-05, -1.4722e-05, -2.6822e-05, -1.4591e-04,\n",
            "        -1.8525e-04, -4.7088e-06, -1.4889e-04, -1.9550e-05, -8.9359e-04,\n",
            "        -5.1618e-05, -1.6809e-05, -7.4267e-05, -2.4557e-05, -4.1008e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03144591301679611 0.0006651878356933594 0.6846883296966553 0.35946065187454224 tensor(89, device='cuda:0')\n",
            "pred tensor([-1.2827e-04, -1.1444e-05, -5.5432e-06, -5.4538e-05, -1.2779e-04,\n",
            "        -2.6822e-06, -6.1572e-05, -5.7268e-04, -7.0333e-05, -5.0664e-06,\n",
            "        -5.3644e-07, -9.8348e-06, -1.4198e-02, -8.1682e-04, -3.6895e-05,\n",
            "        -2.9683e-05, -1.1688e-02, -2.5024e-03, -2.2888e-05, -8.1152e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -7.8082e-06, -5.9605e-08, -3.3975e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031488921493291855 0.00018262863159179688 0.714439868927002 0.3442104756832123 tensor(86, device='cuda:0')\n",
            "pred tensor([-5.1403e-04, -3.5226e-05, -1.9038e-04, -1.3292e-05, -3.1352e-05,\n",
            "        -5.0831e-04, -1.6737e-04, -4.1723e-06, -5.6624e-06, -4.1962e-05,\n",
            "        -1.2636e-05, -1.1325e-06, -1.5438e-05, -8.3804e-05, -2.4819e-04,\n",
            "        -5.8055e-05, -2.6166e-05, -5.7817e-05, -4.9651e-05, -5.9605e-06,\n",
            "        -1.0192e-05, -7.8440e-05, -2.0826e-04, -1.4842e-05, -1.1772e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03193707391619682 0.000385284423828125 0.7055661678314209 0.3322126865386963 tensor(55, device='cuda:0')\n",
            "pred tensor([-3.1471e-05, -2.5630e-06, -2.0266e-06, -5.4932e-04, -1.0000e+00,\n",
            "        -1.0000e+00, -4.6670e-05, -4.5598e-05, -7.8082e-06, -6.6161e-06,\n",
            "        -8.5235e-06, -9.6560e-06, -1.6809e-05, -1.3030e-04, -5.9664e-05,\n",
            "        -8.7142e-05, -2.3317e-04, -3.2187e-04, -6.8128e-05, -3.7611e-05,\n",
            "        -5.9009e-06, -8.5473e-05, -6.8808e-04, -9.3162e-05, -2.8610e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03127830848097801 0.00028514862060546875 0.6931607723236084 0.3370935320854187 tensor(72, device='cuda:0')\n",
            "pred tensor([-2.1827e-04, -1.3847e-02, -2.3592e-04, -1.9188e-03, -5.4169e-03,\n",
            "        -4.8676e-03, -6.5660e-04, -9.0301e-05, -1.5545e-03, -7.3671e-04,\n",
            "        -2.0921e-05, -5.7638e-05, -1.2636e-05, -1.0151e-04, -2.6631e-04,\n",
            "        -1.3227e-03, -2.4719e-03, -1.2848e-02, -6.1684e-03, -4.9438e-03,\n",
            "        -1.5039e-03, -5.7526e-03, -1.3618e-03, -1.5516e-03, -3.5906e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03203433379530907 0.00020933151245117188 0.7019680738449097 0.33589065074920654 tensor(69, device='cuda:0')\n",
            "pred tensor([-1.2755e-05, -1.9264e-04, -4.8485e-03, -1.7333e-04, -9.3520e-05,\n",
            "        -2.1696e-05, -9.4235e-05, -8.9407e-07, -4.4703e-05, -3.3283e-04,\n",
            "        -1.8349e-03, -4.0627e-03, -9.0283e-01, -4.3140e-01, -6.3539e-05,\n",
            "        -1.1820e-04, -4.2796e-04, -2.6417e-04, -4.4281e-02, -7.3099e-04,\n",
            "        -1.2077e-02, -4.5052e-03, -6.9946e-02, -8.3130e-02, -1.4901e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03154339641332626 0.0008955001831054688 0.6540861129760742 0.3425643742084503 tensor(67, device='cuda:0')\n",
            "pred tensor([-3.0065e-04, -5.9319e-04, -2.0027e-04, -5.5408e-04, -5.6171e-04,\n",
            "        -3.0816e-05, -4.6670e-05, -8.4460e-05, -1.2779e-04, -5.3635e-03,\n",
            "        -3.6263e-04, -2.3687e-04, -2.1782e-03, -1.0307e-02, -2.5320e-04,\n",
            "        -9.8584e-01, -9.5947e-01, -9.2089e-05, -8.0252e-04, -5.6505e-05,\n",
            "        -1.0890e-04, -6.8903e-04, -7.4883e-03, -5.9903e-05, -6.6042e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03296684846282005 0.0012416839599609375 0.6653333902359009 0.35251715779304504 tensor(83, device='cuda:0')\n",
            "pred tensor([-4.4155e-04, -9.6226e-04, -5.0888e-03, -1.0586e-03, -6.3610e-04,\n",
            "        -2.9683e-05, -1.1683e-04, -3.3927e-04, -4.5955e-05, -2.2423e-04,\n",
            "        -1.5056e-04, -6.4015e-05, -7.8738e-05, -9.0003e-06, -5.3048e-05,\n",
            "        -3.4106e-01, -9.9951e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -3.6926e-03, -7.8392e-04, -8.6594e-04, -1.6968e-02, -2.7523e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03016980178654194 0.0005335807800292969 0.6792489290237427 0.3476596772670746 tensor(57, device='cuda:0')\n",
            "pred tensor([-5.2691e-05, -7.7844e-05, -1.8024e-04, -5.8413e-06, -2.0802e-05,\n",
            "        -1.1772e-04, -1.0228e-04, -3.7622e-04, -5.2490e-01, -9.9951e-01,\n",
            "        -1.0000e+00, -3.6776e-05, -4.3511e-06, -2.8014e-06, -1.0133e-05,\n",
            "        -1.6272e-05, -2.0266e-06, -1.3530e-05, -4.8876e-04, -4.9515e-03,\n",
            "        -8.8215e-05, -3.7611e-05, -5.9605e-06, -1.1921e-06, -5.0664e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02851768583059311 0.0006895065307617188 0.6623380184173584 0.38094282150268555 tensor(55, device='cuda:0')\n",
            "pred tensor([-8.9943e-05, -6.9499e-05, -1.0473e-04, -1.9562e-04, -2.0233e-02,\n",
            "        -7.2937e-02, -4.7922e-05, -3.3021e-04, -2.8896e-03, -4.0352e-05,\n",
            "        -2.4597e-01, -1.2894e-03, -2.4529e-03, -5.1439e-05, -2.3901e-05,\n",
            "        -5.0507e-03, -3.9941e-01, -1.0000e+00, -1.0000e+00, -9.9609e-01,\n",
            "        -1.7881e-07, -1.1325e-06, -2.9802e-07, -1.1921e-07, -5.9605e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029275255277752876 0.0005235671997070312 0.6692886352539062 0.35406187176704407 tensor(63, device='cuda:0')\n",
            "pred tensor([-1.3924e-04, -2.1820e-03, -5.5265e-04, -1.4770e-04, -1.1194e-04,\n",
            "        -6.8963e-05, -2.7156e-04, -2.5821e-04, -1.4313e-02, -1.3151e-03,\n",
            "        -1.5545e-03, -1.0185e-03, -1.5249e-03, -1.0109e-04, -7.2174e-03,\n",
            "        -8.7857e-05, -6.4774e-03, -2.3193e-03, -8.1641e-01, -6.5269e-03,\n",
            "        -5.7640e-03, -4.6921e-04, -2.2173e-04, -5.5408e-04, -4.7469e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029633348807692528 0.0002541542053222656 0.704850971698761 0.35948577523231506 tensor(57, device='cuda:0')\n",
            "pred tensor([-5.5695e-04, -3.2640e-04, -1.2159e-03, -8.7142e-05, -2.5659e-01,\n",
            "        -9.8145e-01, -1.2474e-03, -1.4038e-02, -4.6272e-03, -1.6041e-03,\n",
            "        -1.0339e-01, -9.1943e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -3.9935e-06, -1.3769e-05, -4.9472e-06, -8.0585e-05,\n",
            "        -8.9693e-04, -2.1398e-04, -4.2295e-04, -2.4498e-05, -7.2122e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026905439794063568 0.00070953369140625 0.6736365556716919 0.34710177779197693 tensor(49, device='cuda:0')\n",
            "pred tensor([-4.7565e-05, -4.0054e-04, -4.5037e-04, -7.0333e-06, -1.5116e-04,\n",
            "        -9.6416e-04, -1.0109e-04, -7.9036e-05, -2.5690e-05, -4.0710e-05,\n",
            "        -7.5161e-05, -3.3331e-04, -1.6999e-04, -6.3992e-04, -6.7592e-05,\n",
            "        -1.7748e-03, -1.1272e-03, -1.7493e-01, -9.9609e-01, -9.9609e-01,\n",
            "        -1.0000e+00, -8.2195e-05, -6.7949e-06, -3.1590e-06, -7.8678e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029501160606741905 0.0009417533874511719 0.6658289432525635 0.33393198251724243 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.3709e-04, -3.6180e-05, -3.7551e-06, -5.4240e-06, -2.7299e-05,\n",
            "        -3.8981e-05, -7.7844e-05, -4.5776e-05, -1.8120e-05, -1.6606e-04,\n",
            "        -2.3127e-05, -1.0908e-05, -8.2254e-06, -3.6359e-05, -4.2114e-02,\n",
            "        -7.4609e-01, -2.8496e-03, -2.0676e-03, -1.0806e-04, -4.7088e-06,\n",
            "        -4.7684e-06, -1.2934e-05, -3.6597e-05, -7.3135e-05, -6.5565e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028349345549941063 0.0007586479187011719 0.679561197757721 0.3456117510795593 tensor(65, device='cuda:0')\n",
            "pred tensor([-2.2659e-03, -1.1473e-03, -2.6962e-02, -2.3880e-03, -4.7469e-04,\n",
            "        -2.0752e-03, -2.6112e-03, -4.8494e-04, -8.4000e-03, -2.7275e-04,\n",
            "        -1.1559e-03, -4.8399e-04, -1.5473e-04, -3.4189e-04, -3.9887e-04,\n",
            "        -1.1104e-04, -5.5265e-04, -9.3985e-04, -1.3170e-03, -2.2113e-05,\n",
            "        -1.0514e-04, -2.4776e-03, -2.5725e-04, -2.5010e-04, -7.7539e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02798440307378769 0.001495361328125 0.6643906831741333 0.3614349961280823 tensor(78, device='cuda:0')\n",
            "pred tensor([-9.6094e-01, -9.9951e-01, -2.0587e-04, -7.2384e-04, -5.1856e-05,\n",
            "        -5.5194e-05, -6.0380e-05, -1.6868e-04, -2.3139e-04, -2.6631e-04,\n",
            "        -1.7202e-04, -1.1104e-04, -2.0337e-04, -5.7030e-04, -3.4928e-05,\n",
            "        -5.9605e-06, -2.3007e-05, -1.1146e-05, -3.0398e-06, -1.2457e-05,\n",
            "        -9.9182e-05, -1.0973e-04, -4.8518e-05, -4.0352e-05, -6.4254e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02879142388701439 0.000919342041015625 0.6840132474899292 0.3690452575683594 tensor(74, device='cuda:0')\n",
            "pred tensor([-4.7469e-04, -9.2387e-06, -8.6129e-05, -6.4993e-04, -1.5056e-04,\n",
            "        -1.8597e-04, -1.0967e-05, -1.7226e-05, -1.2529e-04, -5.9903e-05,\n",
            "        -5.7399e-05, -7.0930e-06, -8.4043e-06, -2.1279e-05, -1.2732e-04,\n",
            "        -2.4533e-04, -1.3924e-04, -1.4484e-04, -4.1008e-04, -1.8311e-04,\n",
            "        -9.7156e-06, -1.6153e-04, -8.6844e-05, -8.9407e-07, -2.8968e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028969062492251396 0.0005135536193847656 0.6834501028060913 0.3786580264568329 tensor(69, device='cuda:0')\n",
            "pred tensor([-2.0182e-04, -1.2934e-04, -5.4646e-04, -5.3167e-04, -1.2474e-03,\n",
            "        -5.4741e-04, -2.7776e-05, -2.4974e-05, -7.6592e-05, -9.1195e-06,\n",
            "        -2.1076e-04, -9.9561e-01, -9.9902e-01, -1.2732e-04, -1.3173e-05,\n",
            "        -3.6550e-04, -1.3590e-05, -1.7536e-04, -1.8966e-04, -1.7107e-05,\n",
            "        -9.7632e-05, -8.6844e-05, -4.7386e-05, -3.7611e-05, -3.7193e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030350370332598686 0.0007691383361816406 0.688828706741333 0.3447583317756653 tensor(80, device='cuda:0')\n",
            "pred tensor([-7.8711e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -9.9805e-01,\n",
            "        -3.2425e-05, -2.9248e-01, -7.0381e-03, -1.2789e-03, -2.8348e-04,\n",
            "        -3.0065e-04, -1.6332e-05, -3.5286e-04, -3.2115e-04, -1.2283e-03,\n",
            "        -3.6764e-04, -2.8563e-04, -5.0926e-04, -6.2037e-04, -1.9287e-02,\n",
            "        -4.2496e-03, -5.6396e-02, -1.1206e-03, -6.4790e-05, -2.4796e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03362568840384483 0.0003676414489746094 0.717227578163147 0.3533543348312378 tensor(99, device='cuda:0')\n",
            "pred tensor([-9.0599e-06, -8.9407e-07, -1.1504e-05, -2.6846e-04, -9.5725e-05,\n",
            "        -1.8063e-03, -1.0986e-03, -2.5768e-03, -1.1593e-04, -8.4817e-05,\n",
            "        -9.1791e-06, -5.1618e-05, -6.4790e-05, -2.4438e-04, -4.9162e-04,\n",
            "        -2.6779e-03, -4.5204e-04, -2.7637e-03, -2.7905e-03, -5.7182e-03,\n",
            "        -6.2065e-03, -1.1981e-05, -2.1458e-06, -5.9605e-07, -1.9717e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02796386182308197 0.00115203857421875 0.6725330352783203 0.34108465909957886 tensor(76, device='cuda:0')\n",
            "pred tensor([-6.1893e-04, -5.3167e-04, -7.0000e-03, -1.8051e-02, -9.5444e-03,\n",
            "        -9.9902e-01, -9.9902e-01, -5.1994e-03, -1.2770e-03, -9.9957e-05,\n",
            "        -2.0862e-05, -7.6294e-06, -2.7239e-05, -2.3973e-04, -2.4445e-02,\n",
            "        -3.5114e-03, -2.8920e-04, -5.1641e-04, -2.0428e-03, -2.1576e-02,\n",
            "        -7.4816e-04, -7.3891e-03, -1.7105e-02, -1.5793e-03, -1.8152e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03056890331208706 0.0009627342224121094 0.6718695759773254 0.3383341133594513 tensor(85, device='cuda:0')\n",
            "pred tensor([-4.0550e-03, -6.6340e-05, -2.9016e-04, -1.8784e-02, -9.8926e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -4.4847e-04, -1.2517e-06, -6.5565e-07,\n",
            "        -1.3113e-06, -5.9605e-07, -1.2934e-05, -2.7001e-05, -8.4460e-05,\n",
            "        -6.2585e-06, -1.3828e-05, -1.4186e-05, -4.5776e-05, -3.3975e-05,\n",
            "        -4.0221e-04, -6.6423e-04, -2.9984e-03, -7.0667e-04, -4.1127e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02856171503663063 0.0007085800170898438 0.6762845516204834 0.3458729684352875 tensor(85, device='cuda:0')\n",
            "pred tensor([-2.6264e-03, -6.0225e-04, -4.5776e-05, -1.9264e-03, -9.9072e-01,\n",
            "        -9.9951e-01, -1.0000e+00, -4.6468e-04, -4.0293e-04, -1.6284e-04,\n",
            "        -8.1825e-04, -6.9189e-04, -1.0890e-04, -5.9319e-04, -3.5548e-04,\n",
            "        -6.3777e-05, -6.5565e-07, -4.4703e-06, -1.1921e-07, -5.9605e-08,\n",
            "        -6.1989e-06, -4.0352e-05, -1.3351e-03, -1.8299e-05, -7.6592e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026125751435756683 0.00021409988403320312 0.6927467584609985 0.3359968960285187 tensor(63, device='cuda:0')\n",
            "pred tensor([-3.5644e-05, -1.7583e-05, -5.9009e-06, -1.4305e-06, -4.0233e-05,\n",
            "        -5.9605e-07, -5.0068e-05, -2.7537e-05, -1.4663e-05, -9.9540e-06,\n",
            "        -5.3644e-06, -2.9445e-05, -6.9737e-05, -4.1008e-05, -2.6207e-03,\n",
            "        -9.8515e-04, -1.5855e-05, -2.3365e-05, -3.4392e-05, -3.7193e-05,\n",
            "        -1.1325e-05, -7.3135e-05, -9.3985e-04, -7.4530e-04, -1.7464e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03180861100554466 5.9604644775390625e-05 0.7091238498687744 0.3389151096343994 tensor(85, device='cuda:0')\n",
            "pred tensor([-0.0036, -0.0005, -0.0004, -0.0033, -0.0076, -0.0026, -0.0064, -0.0014,\n",
            "        -0.0015, -0.0021, -0.0166, -0.0011, -0.0004, -0.0001, -0.0004, -0.0015,\n",
            "        -0.0024, -0.0004, -0.0004, -0.0046, -0.0196, -0.0043, -0.0003, -0.0002,\n",
            "        -0.0012], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03128618001937866 0.00019741058349609375 0.6773721575737 0.35615214705467224 tensor(94, device='cuda:0')\n",
            "pred tensor([-3.3855e-04, -1.4365e-04, -9.8407e-05, -1.6665e-04, -1.8454e-04,\n",
            "        -1.3232e-04, -3.5644e-05, -4.7565e-04, -1.0551e-02, -1.7929e-03,\n",
            "        -2.4529e-03, -2.9068e-03, -5.6648e-03, -3.2539e-03, -3.9795e-02,\n",
            "        -9.8926e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -3.7265e-04,\n",
            "        -7.0035e-05, -6.3002e-05, -3.8743e-06, -8.8215e-06, -1.3561e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02743258886039257 0.0010118484497070312 0.6752009391784668 0.33443713188171387 tensor(64, device='cuda:0')\n",
            "pred tensor([-1.9944e-04, -2.0111e-04, -4.8208e-04, -1.0473e-04, -5.0640e-04,\n",
            "        -1.6909e-03, -1.2195e-04, -2.8348e-04, -1.2779e-04, -4.1485e-04,\n",
            "        -5.7697e-04, -3.6478e-05, -4.7207e-05, -6.2048e-05, -1.5783e-04,\n",
            "        -2.8431e-05, -1.5795e-05, -5.8413e-06, -2.8467e-04, -5.2910e-03,\n",
            "        -1.3232e-04, -2.1231e-04, -2.8467e-04, -1.1146e-04, -3.5107e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028412675485014915 0.0015010833740234375 0.6653014421463013 0.33974847197532654 tensor(67, device='cuda:0')\n",
            "pred tensor([-8.2254e-06, -5.6505e-05, -3.3665e-04, -1.9083e-03, -6.6423e-04,\n",
            "        -5.4777e-05, -1.1367e-04, -3.3975e-05, -2.8193e-05, -2.5162e-02,\n",
            "        -1.4830e-04, -7.9989e-05, -6.3848e-04, -6.5660e-04, -1.1492e-03,\n",
            "        -1.5533e-04, -2.5094e-05, -1.4091e-04, -1.0371e-05, -2.0943e-03,\n",
            "        -9.3604e-01, -1.0000e+00, -1.8387e-03, -6.6817e-05, -1.1456e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027873843908309937 0.0006461143493652344 0.6849380731582642 0.33373817801475525 tensor(66, device='cuda:0')\n",
            "70\n",
            "pred tensor([-1.2589e-04, -1.3137e-04, -1.3983e-04, -9.2447e-05, -8.4460e-05,\n",
            "        -3.0899e-04, -1.8537e-05, -3.8600e-04, -6.0463e-04, -1.6224e-04,\n",
            "        -5.5075e-04, -1.6983e-02, -6.5527e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -3.6907e-04, -1.0192e-04,\n",
            "        -1.5974e-05, -9.5367e-07, -2.6822e-05, -3.6061e-05, -4.4346e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027987122535705566 0.0004277229309082031 0.6766756772994995 0.333672434091568 tensor(60, device='cuda:0')\n",
            "pred tensor([-8.5235e-06, -2.8563e-04, -2.6727e-04, -9.5367e-05, -3.2544e-05,\n",
            "        -1.2827e-04, -1.1545e-04, -1.2457e-05, -3.9816e-04, -1.5295e-04,\n",
            "        -1.6344e-04, -7.5161e-05, -1.7607e-04, -5.8508e-04, -1.0973e-04,\n",
            "        -8.9586e-05, -1.0681e-04, -7.3195e-04, -2.1338e-05, -2.2888e-05,\n",
            "        -1.5056e-04, -9.8407e-05, -6.3992e-04, -1.0389e-04, -8.2850e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029002070426940918 5.340576171875e-05 0.7501407861709595 0.3400559425354004 tensor(57, device='cuda:0')\n",
            "pred tensor([-7.7248e-05, -6.4230e-04, -2.9011e-03, -1.4937e-04, -1.9188e-03,\n",
            "        -9.2173e-04, -1.2529e-04, -2.1820e-02, -4.3893e-04, -1.9083e-03,\n",
            "        -4.1306e-05, -1.4484e-05, -2.9206e-06, -4.1723e-07, -3.0065e-04,\n",
            "        -3.2715e-01, -1.0000e+00, -1.0000e+00, -7.8735e-03, -1.5497e-06,\n",
            "        -2.1458e-06, -1.2755e-05, -1.2279e-05, -7.1812e-04, -2.5988e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030967649072408676 0.00014066696166992188 0.7164346575737 0.3370560109615326 tensor(72, device='cuda:0')\n",
            "pred tensor([-5.7817e-05, -7.2527e-04, -6.0141e-05, -8.0585e-05, -2.4140e-05,\n",
            "        -1.7881e-06, -4.6492e-05, -5.7364e-04, -5.4359e-05, -5.6458e-04,\n",
            "        -1.7333e-04, -4.6670e-05, -4.7207e-05, -4.1127e-06, -5.0664e-06,\n",
            "        -5.6267e-05, -3.1590e-06, -7.1526e-06, -1.5736e-03, -7.9632e-04,\n",
            "        -2.5272e-05, -1.1384e-05, -1.6332e-05, -8.5831e-05, -1.9670e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02838519774377346 0.000408172607421875 0.6751582026481628 0.3371664583683014 tensor(60, device='cuda:0')\n",
            "pred tensor([-9.6500e-05, -1.6415e-04, -1.3399e-04, -3.1948e-05, -1.9431e-05,\n",
            "        -2.8312e-05, -1.2589e-04, -4.7207e-05, -1.0000e+00, -1.0000e+00,\n",
            "        -3.5114e-03, -2.0206e-05, -2.5451e-05, -5.9414e-04, -9.8765e-05,\n",
            "        -8.1205e-04, -2.5010e-04, -3.8776e-03, -2.2564e-03, -1.1146e-05,\n",
            "        -2.8610e-05, -1.7762e-05, -1.8418e-05, -2.9802e-05, -1.4150e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027835585176944733 0.0016689300537109375 0.6354043483734131 0.339028537273407 tensor(74, device='cuda:0')\n",
            "pred tensor([-7.3671e-05, -4.5955e-05, -1.2684e-04, -4.5061e-05, -2.1315e-04,\n",
            "        -8.3506e-05, -1.2887e-04, -3.2306e-05, -8.3506e-05, -9.0027e-04,\n",
            "        -4.4882e-05, -5.3287e-05, -1.9729e-05, -5.5194e-05, -1.6689e-06,\n",
            "        -8.3447e-07, -2.4199e-05, -6.5565e-06, -1.0270e-04, -2.2411e-05,\n",
            "        -2.4438e-06, -1.0073e-05, -1.1563e-05, -1.6928e-05, -5.0068e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025495821610093117 0.001659393310546875 0.6287839412689209 0.3470096290111542 tensor(57, device='cuda:0')\n",
            "pred tensor([-1.3170e-03, -7.3471e-03, -1.0443e-03, -1.7047e-05, -4.3511e-05,\n",
            "        -4.5955e-05, -8.8811e-06, -7.3373e-05, -1.7891e-03, -1.2665e-03,\n",
            "        -3.6621e-04, -1.1683e-04, -9.5844e-04, -1.5533e-04, -3.2187e-05,\n",
            "        -4.1890e-04, -9.4748e-04, -1.2732e-04, -1.1963e-04, -9.5844e-04,\n",
            "        -5.4407e-04, -4.6082e-03, -2.8920e-04, -1.2934e-05, -1.5843e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027995701879262924 0.0002875328063964844 0.6620761752128601 0.335237979888916 tensor(70, device='cuda:0')\n",
            "pred tensor([-1.2684e-04, -1.3471e-05, -1.7643e-05, -4.5466e-04, -1.3983e-04,\n",
            "        -1.8418e-05, -4.1485e-05, -6.3002e-05, -8.3780e-04, -1.3018e-03,\n",
            "        -6.6519e-04, -2.7370e-04, -1.4019e-03, -5.5850e-05, -8.5473e-05,\n",
            "        -4.4584e-04, -2.3055e-04, -2.5511e-04, -4.5929e-03, -9.6436e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -2.2960e-04, -5.2273e-05, -6.9499e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028715748339891434 8.440017700195312e-05 0.7066382169723511 0.3331344425678253 tensor(64, device='cuda:0')\n",
            "pred tensor([-6.6578e-05, -3.4118e-04, -6.7353e-06, -1.0073e-05, -1.9610e-05,\n",
            "        -4.4525e-05, -1.3089e-04, -3.4962e-03, -1.1272e-03, -3.2234e-03,\n",
            "        -5.1117e-04, -1.7941e-05, -8.2552e-05, -4.5240e-05, -4.2496e-03,\n",
            "        -4.5419e-05, -1.7107e-05, -8.3804e-05, -5.9664e-05, -1.9073e-05,\n",
            "        -7.9785e-01, -9.6436e-01, -1.2695e-01, -8.8215e-06, -1.1206e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026117142289876938 0.00020503997802734375 0.6749563217163086 0.3465973436832428 tensor(54, device='cuda:0')\n",
            "pred tensor([-4.1008e-05, -2.3782e-04, -3.3736e-05, -1.2290e-04, -1.6475e-04,\n",
            "        -3.1257e-04, -1.4091e-04, -8.5473e-05, -1.1593e-04, -8.9407e-07,\n",
            "        -2.0683e-05, -4.9448e-04, -4.5593e-02, -8.2159e-04, -2.3901e-05,\n",
            "        -1.5855e-05, -3.3319e-05, -5.3883e-05, -8.3447e-06, -1.8954e-05,\n",
            "        -1.8740e-04, -2.4021e-05, -6.7890e-05, -4.2677e-05, -4.9651e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024775320664048195 0.0003871917724609375 0.6470982432365417 0.3385693430900574 tensor(65, device='cuda:0')\n",
            "pred tensor([-7.5161e-05, -1.0796e-03, -2.0182e-04, -1.0765e-04, -1.9908e-05,\n",
            "        -1.8418e-05, -3.2783e-06, -1.2290e-04, -9.7510e-01, -9.9609e-01,\n",
            "        -5.5170e-04, -3.0458e-05, -2.9826e-04, -5.3465e-05, -4.1723e-07,\n",
            "        -3.7491e-05, -5.9986e-04, -1.9188e-03, -2.3422e-03, -4.0674e-04,\n",
            "        -1.8656e-05, -8.6844e-05, -6.1572e-05, -1.9336e-04, -2.4343e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02699810266494751 0.0007448196411132812 0.6501703858375549 0.35264158248901367 tensor(69, device='cuda:0')\n",
            "pred tensor([-1.5640e-02, -2.8553e-03, -1.1606e-03, -4.5598e-05, -3.7551e-04,\n",
            "        -5.0259e-04, -4.3154e-05, -1.4484e-04, -2.6016e-03, -2.3193e-03,\n",
            "        -6.4373e-04, -1.0338e-03, -7.7844e-05, -9.7752e-04, -9.9707e-01,\n",
            "        -9.9951e-01, -1.7679e-04, -1.2887e-04, -9.7215e-05, -2.9802e-06,\n",
            "        -6.6042e-05, -8.1348e-04, -8.7500e-05, -3.3975e-06, -5.8413e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02417236752808094 0.001613616943359375 0.6474013328552246 0.33471670746803284 tensor(63, device='cuda:0')\n",
            "pred tensor([-3.6049e-04, -1.5616e-05, -1.5199e-05, -5.2834e-04, -7.0870e-05,\n",
            "        -7.5698e-06, -2.4819e-04, -3.0182e-02, -1.0000e+00, -1.0000e+00,\n",
            "        -1.3481e-02, -9.8896e-04, -5.1498e-03, -2.3499e-03, -1.1034e-03,\n",
            "        -3.9887e-04, -1.2236e-03, -5.8293e-05, -6.6340e-05, -1.0931e-04,\n",
            "        -2.6011e-04, -1.4191e-03, -9.9512e-01, -9.9609e-01, -9.6143e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02382771484553814 0.0018901824951171875 0.6484121084213257 0.3551136553287506 tensor(65, device='cuda:0')\n",
            "pred tensor([-8.4925e-04, -6.1321e-04, -1.6870e-03, -2.5606e-04, -7.7343e-04,\n",
            "        -3.1555e-02, -5.2185e-03, -1.8597e-04, -2.0752e-03, -2.8149e-01,\n",
            "        -1.5076e-02, -8.1299e-01, -2.2058e-01, -2.6989e-03, -5.2910e-03,\n",
            "        -3.9139e-03, -1.2894e-02, -3.2056e-01, -5.0812e-03, -1.3971e-03,\n",
            "        -2.4586e-03, -7.2956e-04, -1.2398e-03, -2.0065e-03, -3.8815e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024015165865421295 0.0003590583801269531 0.6883363723754883 0.3382047116756439 tensor(61, device='cuda:0')\n",
            "pred tensor([-6.3753e-04, -3.8743e-04, -3.0816e-05, -1.7226e-05, -7.0035e-05,\n",
            "        -2.0993e-04, -1.8454e-04, -9.6560e-06, -3.5763e-07, -1.4961e-05,\n",
            "        -6.9737e-05, -2.3782e-04, -1.6422e-03, -4.4775e-04, -4.6730e-04,\n",
            "        -3.0994e-06, -3.6955e-06, -9.4593e-05, -1.9336e-04, -5.7638e-05,\n",
            "        -1.0192e-05, -8.9407e-07, -1.0806e-04, -2.3842e-07, -1.7679e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029798392206430435 0.0003027915954589844 0.7097145318984985 0.3809291422367096 tensor(96, device='cuda:0')\n",
            "pred tensor([-3.2687e-04, -1.9181e-04, -6.7568e-04, -7.9334e-05, -4.7207e-05,\n",
            "        -8.0287e-05, -1.1802e-05, -5.1260e-06, -2.9981e-05, -7.6723e-04,\n",
            "        -6.0201e-06, -1.9073e-05, -2.9087e-05, -3.2251e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -8.1253e-03, -1.1139e-03, -1.2894e-02, -1.0967e-03,\n",
            "        -2.7585e-04, -1.7509e-03, -1.3741e-02, -2.6989e-03, -7.9651e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02835949882864952 0.0003085136413574219 0.7142240405082703 0.3471502959728241 tensor(103, device='cuda:0')\n",
            "pred tensor([-1.9913e-03, -1.6909e-03, -2.1000e-03, -6.4516e-04, -8.7452e-04,\n",
            "        -6.9737e-05, -1.0598e-04, -4.9829e-04, -1.1692e-03, -2.5708e-01,\n",
            "        -5.1453e-02, -2.4744e-01, -2.8954e-03, -5.9319e-04, -6.3629e-03,\n",
            "        -9.5361e-01, -6.0596e-01, -9.5068e-01, -5.9814e-03, -8.5693e-01,\n",
            "        -2.2297e-03, -6.8521e-04, -2.8458e-03, -2.6962e-02, -7.6790e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030279332771897316 0.00026035308837890625 0.7006924152374268 0.3420618176460266 tensor(88, device='cuda:0')\n",
            "pred tensor([-2.7585e-04, -3.8362e-04, -4.4584e-04, -6.5231e-04, -3.1471e-03,\n",
            "        -1.1482e-02, -1.2693e-03, -1.4410e-03, -2.8896e-03, -4.6635e-04,\n",
            "        -1.4896e-03, -6.0225e-04, -1.5278e-03, -2.5120e-03, -1.1559e-03,\n",
            "        -8.3780e-04, -8.9359e-04, -6.2656e-04, -3.2406e-03, -1.1997e-03,\n",
            "        -9.9854e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -3.3975e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029834356158971786 0.0008831024169921875 0.6565781831741333 0.3408397138118744 tensor(87, device='cuda:0')\n",
            "pred tensor([-5.0354e-04, -7.1406e-05, -8.2850e-05, -3.3998e-04, -1.0890e-04,\n",
            "        -2.5916e-04, -5.5408e-04, -1.4126e-05, -3.9577e-04, -1.0902e-02,\n",
            "        -1.5080e-05, -1.6987e-05, -1.1683e-04, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -4.0576e-01, -3.4821e-02, -4.6730e-03,\n",
            "        -2.4967e-03, -4.5471e-03, -5.7526e-03, -1.2283e-03, -1.2947e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.030024021863937378 0.0004773139953613281 0.6693880558013916 0.3433704376220703 tensor(94, device='cuda:0')\n",
            "pred tensor([-2.8735e-01, -3.4790e-02, -8.1348e-04, -8.5068e-04, -3.1605e-03,\n",
            "        -1.8101e-03, -5.6791e-04, -2.0599e-02, -1.6434e-02, -2.2659e-03,\n",
            "        -2.5024e-03, -9.5749e-03, -1.8101e-03, -1.4410e-03, -7.1239e-04,\n",
            "        -6.4015e-05, -5.2452e-05, -1.2481e-04, -9.4366e-04, -2.8014e-04,\n",
            "        -8.5402e-04, -1.4389e-02, -3.2043e-03, -1.1358e-03, -7.3671e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03031911700963974 0.0002865791320800781 0.7036045789718628 0.3447696566581726 tensor(97, device='cuda:0')\n",
            "pred tensor([-3.4475e-04, -5.8746e-04, -2.7490e-04, -2.6846e-04, -1.5305e-02,\n",
            "        -4.0936e-04, -8.3447e-07, -1.7822e-05, -1.5076e-02, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -3.9124e-04, -1.5199e-05, -1.3232e-04,\n",
            "        -1.4544e-04, -3.0220e-05, -1.0848e-05, -3.3474e-04, -2.2964e-03,\n",
            "        -3.2406e-03, -5.6362e-04, -1.2982e-04, -3.1877e-04, -2.5916e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.032273441553115845 0.0013942718505859375 0.6492165923118591 0.3404204547405243 tensor(92, device='cuda:0')\n",
            "pred tensor([-1.0118e-03, -1.8139e-03, -1.5488e-03, -7.1096e-04, -8.3506e-05,\n",
            "        -5.1994e-03, -1.4544e-04, -1.4091e-04, -4.0770e-04, -1.8167e-04,\n",
            "        -7.5459e-05, -5.3048e-05, -1.0312e-04, -3.3665e-04, -1.5888e-03,\n",
            "        -1.5764e-03, -5.4538e-05, -3.3212e-04, -1.8883e-04, -1.0639e-04,\n",
            "        -4.2462e-04, -8.9502e-04, -7.3671e-04, -7.1669e-04, -8.7118e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029329592362046242 0.0004367828369140625 0.6726058721542358 0.345123291015625 tensor(96, device='cuda:0')\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -2.4052e-03, -1.2848e-02, -7.5161e-05,\n",
            "        -4.9353e-04, -3.3081e-05, -2.9087e-05, -9.2163e-03, -1.3245e-02,\n",
            "        -1.2772e-02, -9.0625e-01, -9.3457e-01, -2.6569e-03, -1.5640e-02,\n",
            "        -7.4158e-03, -4.5753e-04, -1.2398e-03, -6.2525e-05, -4.6134e-05,\n",
            "        -3.0780e-04, -1.1563e-05, -2.3901e-05, -1.0151e-04, -1.5497e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03212336078286171 0.0004892349243164062 0.670820415019989 0.3535270690917969 tensor(96, device='cuda:0')\n",
            "pred tensor([-7.8249e-04, -1.8845e-02, -1.2482e-01, -2.2141e-02, -6.6299e-03,\n",
            "        -2.4974e-05, -3.5763e-07, -1.8477e-06, -1.2517e-06, -3.3081e-05,\n",
            "        -9.9951e-01, -1.0000e+00, -3.5000e-04, -3.1137e-04, -5.8098e-03,\n",
            "        -2.4915e-04, -6.5565e-05, -1.9550e-05, -2.3127e-05, -2.0351e-03,\n",
            "        -2.4533e-04, -5.3024e-03, -1.3199e-03, -3.9756e-05, -1.3914e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029471687972545624 0.0001010894775390625 0.7192717790603638 0.34764793515205383 tensor(98, device='cuda:0')\n",
            "pred tensor([-8.7354e-01, -2.0742e-04, -1.2636e-04, -3.4273e-05, -7.9691e-05,\n",
            "        -6.3539e-05, -2.8610e-05, -9.3579e-06, -1.8668e-04, -2.8467e-04,\n",
            "        -1.8835e-05, -6.3181e-06, -3.3498e-05, -6.1989e-06, -7.2718e-06,\n",
            "        -6.8963e-05, -1.2934e-05, -4.6492e-06, -4.7028e-05, -3.6550e-04,\n",
            "        -1.4710e-04, -2.6882e-05, -1.1104e-04, -6.4790e-05, -1.4043e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02803112380206585 0.00021600723266601562 0.7032719254493713 0.33857056498527527 tensor(62, device='cuda:0')\n",
            "pred tensor([-1.9058e-02, -1.3538e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -3.2067e-04, -1.9491e-04, -6.0368e-04, -3.3245e-03,\n",
            "        -2.0742e-04, -2.6917e-02, -1.1292e-03, -3.8910e-04, -6.0844e-04,\n",
            "        -2.7919e-04, -1.9610e-05, -9.1732e-05, -2.5511e-04, -7.3671e-04,\n",
            "        -3.9053e-04, -2.9469e-04, -6.3539e-05, -1.0550e-05, -1.2331e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02928311750292778 0.00027561187744140625 0.6980637311935425 0.34711337089538574 tensor(67, device='cuda:0')\n",
            "pred tensor([-9.5703e-01, -9.9658e-01, -1.0000e+00, -1.0000e+00, -9.0933e-04,\n",
            "        -2.4915e-04, -4.3058e-04, -1.8454e-04, -3.0458e-05, -5.8293e-05,\n",
            "        -3.5286e-04, -1.0151e-04, -7.1096e-04, -3.4571e-06, -1.5020e-05,\n",
            "        -4.5598e-05, -2.7657e-05, -6.0618e-05, -6.0730e-03, -6.1393e-06,\n",
            "        -1.1861e-05, -5.3287e-05, -7.2122e-06, -8.7500e-05, -9.1791e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03209458664059639 0.00034999847412109375 0.6919823884963989 0.3400854170322418 tensor(71, device='cuda:0')\n",
            "pred tensor([-6.5565e-07, -2.5392e-05, -1.0967e-05, -5.1260e-06, -9.0003e-06,\n",
            "        -2.6226e-04, -3.0935e-05, -5.6744e-05, -4.3392e-04, -2.8431e-05,\n",
            "        -1.0389e-04, -2.2650e-05, -2.0826e-04, -2.1231e-04, -1.1969e-03,\n",
            "        -1.1963e-02, -6.4230e-04, -9.2697e-04, -3.2687e-04, -6.6578e-05,\n",
            "        -2.3098e-03, -9.3872e-02, -2.1744e-04, -5.5265e-04, -1.6868e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028856439515948296 0.0012025833129882812 0.6328967809677124 0.35423749685287476 tensor(76, device='cuda:0')\n",
            "pred tensor([-1.4043e-04, -3.6407e-04, -3.6550e-04, -2.8074e-05, -9.1736e-02,\n",
            "        -9.8926e-01, -9.9902e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.9646e-04, -1.3113e-06, -1.9181e-04, -1.9133e-05, -3.8981e-05,\n",
            "        -9.5081e-04, -7.8142e-05, -4.7016e-04, -1.4067e-05, -1.9550e-05,\n",
            "        -2.1219e-05, -9.4175e-06, -1.3292e-05, -6.1393e-06, -5.1022e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02933342568576336 0.001678466796875 0.6378687620162964 0.3666204512119293 tensor(73, device='cuda:0')\n",
            "pred tensor([-2.2233e-05, -4.3333e-05, -1.3769e-05, -1.0815e-03, -3.0339e-05,\n",
            "        -4.2319e-06, -5.4538e-05, -7.2718e-06, -3.5763e-06, -1.7464e-05,\n",
            "        -7.0953e-04, -1.4305e-05, -1.3137e-04, -2.2519e-04, -2.8193e-05,\n",
            "        -1.4186e-05, -1.4484e-04, -3.6359e-06, -9.4593e-05, -2.6131e-04,\n",
            "        -1.6534e-04, -2.0111e-04, -3.5286e-04, -7.3671e-05, -4.9651e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02661924622952938 0.0008654594421386719 0.6532618403434753 0.33159151673316956 tensor(55, device='cuda:0')\n",
            "pred tensor([-1.0598e-04, -6.6757e-06, -3.7074e-05, -2.4796e-05, -1.6928e-05,\n",
            "        -2.5034e-06, -4.0531e-05, -1.2517e-06, -4.3511e-06, -1.8883e-04,\n",
            "        -2.9540e-04, -1.8954e-05, -1.8835e-05, -1.5903e-04, -3.0398e-06,\n",
            "        -1.0729e-06, -1.4126e-05, -1.5533e-04, -8.7976e-04, -2.4438e-06,\n",
            "        -1.4305e-05, -4.0829e-05, -4.1127e-06, -2.0266e-06, -7.0333e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02652660571038723 0.00013685226440429688 0.69936203956604 0.3649577498435974 tensor(62, device='cuda:0')\n",
            "pred tensor([-3.8981e-05, -1.7881e-07, -1.3947e-05, -3.6955e-06, -3.2425e-05,\n",
            "        -1.8954e-05, -3.9339e-06, -2.1279e-05, -2.9445e-05, -4.9889e-05,\n",
            "        -1.3828e-05, -2.0862e-06, -2.0444e-05, -3.4571e-06, -3.8147e-06,\n",
            "        -6.3539e-05, -2.0862e-06, -1.2100e-05, -5.3644e-07, -3.0994e-06,\n",
            "        -3.5107e-05, -2.1338e-05, -9.9540e-06, -1.3232e-04, -5.7220e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026954082772135735 0.00010395050048828125 0.7019327282905579 0.348488450050354 tensor(65, device='cuda:0')\n",
            "pred tensor([-3.6180e-05, -1.0639e-04, -2.4915e-04, -1.5488e-03, -1.9989e-03,\n",
            "        -1.7345e-05, -1.3089e-04, -4.9114e-05, -9.5963e-06, -1.1742e-05,\n",
            "        -1.1921e-05, -7.7486e-07, -3.0947e-04, -3.1376e-04, -1.1021e-04,\n",
            "        -6.0141e-05, -2.7490e-04, -5.0247e-05, -1.9944e-04, -1.7524e-05,\n",
            "        -1.8477e-06, -1.8835e-05, -4.2975e-05, -6.1393e-06, -1.0610e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02828257344663143 0.000316619873046875 0.6717618107795715 0.3381761908531189 tensor(80, device='cuda:0')\n",
            "pred tensor([-6.7043e-04, -7.8535e-04, -1.6415e-04, -1.0848e-04, -5.1856e-06,\n",
            "        -6.6161e-06, -5.5408e-04, -1.4496e-03, -1.9264e-04, -1.6613e-03,\n",
            "        -1.6384e-03, -5.5170e-04, -2.6611e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -2.1100e-05, -2.0921e-05, -1.0669e-05, -1.1623e-05, -3.8743e-06,\n",
            "        -7.6294e-05, -2.1148e-04, -1.6510e-05, -6.8140e-04, -2.3782e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027340106666088104 0.0010213851928710938 0.6408957242965698 0.3387264609336853 tensor(79, device='cuda:0')\n",
            "pred tensor([-1.6928e-04, -8.7619e-06, -1.0431e-04, -2.5129e-04, -1.6510e-05,\n",
            "        -5.4777e-05, -9.2387e-06, -4.9472e-05, -9.8133e-04, -6.4230e-04,\n",
            "        -6.9678e-01, -1.0000e+00, -1.0000e+00, -5.6922e-05, -1.2434e-04,\n",
            "        -2.4915e-04, -1.5914e-05, -1.5249e-03, -8.3313e-03, -1.7130e-04,\n",
            "        -5.3358e-04, -1.0223e-03, -2.7370e-04, -1.0556e-04, -9.0659e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026662588119506836 0.0006017684936523438 0.670677661895752 0.3464553952217102 tensor(60, device='cuda:0')\n",
            "pred tensor([-9.9182e-05, -7.3671e-04, -6.6414e-03, -3.1757e-04, -9.4557e-04,\n",
            "        -2.4719e-03, -1.2970e-03, -2.0504e-04, -5.2452e-05, -7.3528e-04,\n",
            "        -1.1414e-04, -7.5459e-05, -8.6451e-04, -8.8215e-06, -1.4091e-04,\n",
            "        -1.1730e-04, -1.1772e-04, -1.3292e-04, -6.4502e-01, -9.8682e-01,\n",
            "        -9.9609e-01, -2.4395e-03, -3.1376e-04, -1.0788e-05, -4.0829e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026581410318613052 0.0005993843078613281 0.6514058113098145 0.35020390152931213 tensor(60, device='cuda:0')\n",
            "pred tensor([-2.9981e-05, -2.1458e-05, -1.5616e-05, -3.8683e-05, -5.3072e-04,\n",
            "        -3.9279e-05, -3.4273e-05, -2.3413e-04, -1.4668e-03, -2.1777e-01,\n",
            "        -2.2522e-01, -3.2187e-05, -1.0431e-05, -3.3069e-04, -1.1235e-04,\n",
            "        -4.2140e-05, -1.9109e-04, -7.9334e-05, -4.1080e-04, -5.7399e-05,\n",
            "        -3.5942e-05, -7.2539e-05, -6.3002e-05, -3.2115e-04, -3.8600e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026420800015330315 0.00013780593872070312 0.685599148273468 0.3506397604942322 tensor(46, device='cuda:0')\n",
            "pred tensor([-4.2975e-05, -1.3709e-06, -1.1277e-04, -2.9683e-05, -1.3113e-06,\n",
            "        -1.2517e-05, -4.8943e-03, -1.4424e-04, -1.0000e+00, -1.0000e+00,\n",
            "        -5.4955e-05, -3.5071e-04, -1.6589e-01, -1.3876e-04, -1.8537e-05,\n",
            "        -1.0967e-05, -1.2636e-05, -6.3300e-05, -2.2078e-04, -1.6093e-06,\n",
            "        -3.5107e-05, -2.1994e-04, -7.2718e-06, -5.4538e-05, -1.9431e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025883687660098076 0.000946044921875 0.651585578918457 0.3325212895870209 tensor(52, device='cuda:0')\n",
            "71\n",
            "pred tensor([-1.4381e-03, -9.9854e-01, -9.7656e-01, -3.4118e-04, -1.5039e-03,\n",
            "        -5.0020e-04, -5.7697e-04, -1.4048e-03, -2.0828e-03, -3.9673e-04,\n",
            "        -4.7379e-03, -4.6277e-04, -7.9334e-05, -5.5611e-05, -3.7551e-04,\n",
            "        -6.8426e-05, -1.1915e-04, -2.8782e-03, -3.0422e-04, -4.8676e-03,\n",
            "        -2.5010e-04, -3.9983e-04, -5.1856e-05, -3.4118e-04, -5.0697e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02683638595044613 0.0002841949462890625 0.6814055442810059 0.32961562275886536 tensor(54, device='cuda:0')\n",
            "pred tensor([-9.9951e-01, -9.9951e-01, -1.0000e+00, -1.0000e+00, -1.4257e-04,\n",
            "        -1.3075e-03, -6.1798e-04, -1.9038e-04, -2.5010e-04, -1.8239e-04,\n",
            "        -7.8106e-04, -5.0621e-03, -5.2109e-03, -4.7188e-03, -6.0501e-03,\n",
            "        -3.7265e-04, -5.7364e-04, -3.0422e-04, -1.5175e-04, -1.9109e-04,\n",
            "        -3.6354e-03, -7.8142e-05, -1.9951e-03, -5.3749e-03, -2.6131e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028409967198967934 0.001941680908203125 0.6523923873901367 0.3359668552875519 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.2290e-04, -3.4771e-03, -3.0696e-05, -6.2752e-04, -2.0826e-04,\n",
            "        -6.6161e-06, -6.0797e-06, -1.3769e-04, -1.1730e-04, -2.4915e-04,\n",
            "        -5.4741e-04, -2.7156e-04, -2.1148e-04, -2.3234e-04, -8.1205e-04,\n",
            "        -3.9887e-04, -7.2670e-04, -2.5010e-04, -2.5916e-04, -1.1072e-03,\n",
            "        -2.9869e-03, -2.1994e-04, -1.1692e-03, -1.0312e-04, -3.2687e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026941223070025444 0.0022430419921875 0.6318399310112 0.33707958459854126 tensor(62, device='cuda:0')\n",
            "pred tensor([-7.0667e-04, -5.7459e-04, -5.3263e-04, -4.2892e-04, -5.9187e-05,\n",
            "        -8.0032e-03, -2.0444e-05, -3.5167e-06, -5.3139e-03, -9.9951e-01,\n",
            "        -8.9111e-01, -1.0000e+00, -1.0000e+00, -5.4955e-05, -1.2457e-05,\n",
            "        -1.9717e-04, -6.4135e-04, -1.0109e-04, -1.1146e-05, -3.3855e-04,\n",
            "        -6.1810e-05, -4.6670e-05, -1.2934e-05, -2.3973e-04, -3.8815e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0243397019803524 0.0001811981201171875 0.6910432577133179 0.32993146777153015 tensor(45, device='cuda:0')\n",
            "pred tensor([-3.9577e-05, -4.1664e-05, -2.8968e-05, -2.7776e-05, -1.3876e-04,\n",
            "        -1.2457e-05, -9.1195e-06, -1.3292e-05, -1.6451e-05, -2.0266e-06,\n",
            "        -1.9610e-05, -2.6631e-04, -7.4565e-05, -6.2525e-05, -9.0551e-04,\n",
            "        -1.5903e-04, -3.0899e-04, -4.5300e-06, -2.9206e-06, -1.2040e-05,\n",
            "        -4.8697e-05, -9.5963e-06, -6.0856e-05, -2.0027e-04, -3.4809e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02789703942835331 0.00017404556274414062 0.7108592987060547 0.3439355194568634 tensor(63, device='cuda:0')\n",
            "pred tensor([-7.7844e-05, -1.5020e-05, -3.6597e-05, -1.4524e-03, -4.6563e-04,\n",
            "        -2.0337e-04, -9.7215e-05, -1.0000e+00, -1.0000e+00, -1.0815e-03,\n",
            "        -1.5414e-04, -2.3139e-04, -5.7364e-04, -1.0151e-04, -4.8494e-04,\n",
            "        -1.0598e-04, -1.2188e-03, -1.3137e-04, -6.2895e-04, -5.9426e-05,\n",
            "        -2.7955e-05, -1.3123e-03, -2.4378e-05, -1.3113e-06, -5.6922e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025488518178462982 0.00010061264038085938 0.7042837142944336 0.3316052556037903 tensor(55, device='cuda:0')\n",
            "pred tensor([-5.2691e-05, -1.8966e-04, -4.5776e-05, -8.3804e-05, -4.5598e-05,\n",
            "        -1.7881e-07, -4.7684e-07, -3.2377e-04, -1.0639e-04, -1.0000e+00,\n",
            "        -9.9951e-01, -8.3447e-04, -5.2185e-03, -1.2255e-03, -1.7204e-03,\n",
            "        -1.0509e-03, -4.7946e-04, -1.4900e-02, -3.2673e-03, -2.0421e-04,\n",
            "        -2.3723e-05, -1.2386e-04, -6.4790e-05, -3.1257e-04, -3.2640e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025653695687651634 0.00023746490478515625 0.7003874778747559 0.33031678199768066 tensor(47, device='cuda:0')\n",
            "pred tensor([-1.0023e-03, -6.7043e-04, -6.6519e-04, -3.1300e-03, -6.9141e-06,\n",
            "        -2.6727e-04, -1.0635e-02, -2.1315e-04, -1.6708e-03, -3.3188e-03,\n",
            "        -2.1076e-04, -3.2187e-05, -1.8349e-03, -5.6028e-06, -1.6737e-04,\n",
            "        -4.8208e-04, -2.9540e-04, -1.2505e-02, -3.8166e-03, -8.4460e-05,\n",
            "        -2.1148e-04, -2.6822e-05, -1.1325e-05, -9.9902e-01, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026054492220282555 0.0005192756652832031 0.6668972969055176 0.3460628092288971 tensor(54, device='cuda:0')\n",
            "pred tensor([-7.6294e-05, -1.4544e-05, -9.5367e-05, -3.0577e-05, -7.1406e-05,\n",
            "        -1.1325e-06, -7.5698e-06, -4.4346e-05, -4.4703e-06, -1.4305e-06,\n",
            "        -1.1806e-03, -2.9707e-04, -5.1856e-06, -2.9206e-06, -1.3888e-05,\n",
            "        -2.3723e-05, -1.0151e-04, -2.7490e-04, -2.5463e-03, -4.3488e-04,\n",
            "        -3.5644e-05, -3.2187e-05, -5.3048e-05, -2.8563e-04, -4.9114e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026246795430779457 0.0010747909545898438 0.6480146646499634 0.33114486932754517 tensor(52, device='cuda:0')\n",
            "pred tensor([-6.4790e-05, -1.9670e-06, -9.5367e-07, -1.1444e-02, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -7.2527e-04, -7.1526e-06, -3.1109e-03,\n",
            "        -9.0742e-04, -1.2887e-04, -2.0866e-03, -8.0872e-04, -1.6813e-03,\n",
            "        -1.5278e-03, -9.8407e-05, -1.4889e-04, -3.9506e-04, -9.3651e-04,\n",
            "        -5.2035e-05, -2.0714e-03, -4.9927e-02, -3.4027e-02, -1.5056e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026608586311340332 0.0005517005920410156 0.6621594429016113 0.34132251143455505 tensor(65, device='cuda:0')\n",
            "pred tensor([-8.4460e-05, -5.0843e-05, -2.3842e-05, -4.9829e-04, -4.9472e-06,\n",
            "        -2.8014e-06, -2.1954e-03, -4.1485e-05, -1.0000e+00, -1.0000e+00,\n",
            "        -3.1638e-04, -9.8348e-06, -1.0948e-03, -2.5034e-06, -1.1921e-06,\n",
            "        -2.7905e-03, -1.2064e-03, -2.7704e-04, -2.7585e-04, -2.9683e-05,\n",
            "        -2.1992e-03, -2.1660e-04, -1.8096e-04, -7.1669e-04, -4.9353e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026814384385943413 0.0009508132934570312 0.6353142857551575 0.3726591169834137 tensor(69, device='cuda:0')\n",
            "pred tensor([-9.5010e-05, -1.1806e-03, -2.1231e-04, -1.4091e-04, -3.5942e-05,\n",
            "        -1.2159e-05, -3.8207e-05, -2.2650e-06, -4.9472e-05, -1.8711e-03,\n",
            "        -6.6299e-03, -1.0000e+00, -1.0000e+00, -2.6727e-04, -3.9458e-05,\n",
            "        -8.6427e-06, -1.8525e-04, -6.6757e-06, -2.2650e-06, -1.2577e-05,\n",
            "        -2.7418e-05, -2.5916e-04, -3.3140e-04, -2.6941e-04, -2.0421e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024279581382870674 0.0021724700927734375 0.6235162615776062 0.33457475900650024 tensor(67, device='cuda:0')\n",
            "pred tensor([-4.7016e-04, -6.7711e-04, -1.3828e-03, -3.9756e-05, -6.2585e-06,\n",
            "        -9.9540e-06, -4.0126e-04, -5.4240e-06, -1.0000e+00, -1.0000e+00,\n",
            "        -5.7399e-05, -7.9334e-05, -8.0872e-04, -1.2112e-03, -1.0848e-04,\n",
            "        -1.8311e-03, -6.0380e-05, -2.6882e-05, -6.0701e-04, -2.0146e-05,\n",
            "        -2.6417e-04, -2.5063e-03, -3.1719e-03, -5.1117e-04, -4.8943e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025618571788072586 7.009506225585938e-05 0.7001850605010986 0.3418903350830078 tensor(64, device='cuda:0')\n",
            "pred tensor([-2.2423e-04, -6.0380e-05, -1.0948e-02, -8.0338e-03, -9.6500e-05,\n",
            "        -1.8096e-04, -3.3512e-03, -5.5194e-05, -7.4565e-05, -2.2519e-04,\n",
            "        -2.8667e-03, -8.5402e-04, -6.7017e-02, -1.4048e-03, -2.9125e-03,\n",
            "        -1.1311e-03, -1.6012e-03, -4.3640e-03, -2.0714e-03, -2.8931e-02,\n",
            "        -3.6144e-03, -5.8098e-03, -1.4839e-03, -2.7156e-04, -9.9182e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024098677560687065 5.0067901611328125e-05 0.7318035364151001 0.34170880913734436 tensor(49, device='cuda:0')\n",
            "pred tensor([-9.1629e-03, -4.7379e-03, -7.3013e-03, -6.0501e-03, -7.5391e-01,\n",
            "        -6.5369e-02, -1.2598e-03, -2.4438e-04, -1.0779e-01, -9.9951e-01,\n",
            "        -5.8136e-02, -1.2817e-03, -2.4152e-04, -1.5249e-03, -3.6297e-03,\n",
            "        -2.5768e-03, -1.5056e-04, -1.3332e-03, -1.5917e-03, -5.8327e-03,\n",
            "        -6.0129e-04, -6.6519e-04, -3.5906e-04, -3.7336e-04, -5.6171e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02921692281961441 5.7697296142578125e-05 0.7284383773803711 0.34252628684043884 tensor(91, device='cuda:0')\n",
            "pred tensor([-1.3030e-04, -2.4445e-02, -1.4732e-02, -6.4969e-06, -1.0000e+00,\n",
            "        -1.0000e+00, -7.7844e-05, -2.6226e-06, -2.6836e-03, -5.4283e-03,\n",
            "        -1.1444e-05, -3.1300e-03, -1.4725e-03, -1.9722e-03, -1.6153e-04,\n",
            "        -3.3054e-03, -8.0729e-04, -2.0266e-04, -8.6069e-04, -1.3914e-03,\n",
            "        -2.6535e-02, -2.6226e-04, -3.9673e-03, -3.9062e-03, -4.8876e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028397148475050926 0.0010614395141601562 0.6552891731262207 0.34312060475349426 tensor(94, device='cuda:0')\n",
            "pred tensor([-3.8004e-04, -3.6359e-06, -9.6858e-05, -5.9903e-05, -3.0054e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -9.1016e-05, -3.3975e-06, -5.5656e-03,\n",
            "        -2.8253e-04, -9.5654e-04, -2.6131e-04, -1.4830e-04, -6.8235e-04,\n",
            "        -9.6416e-04, -3.0935e-05, -1.2398e-05, -1.9872e-04, -6.0606e-04,\n",
            "        -4.7207e-05, -5.4359e-05, -1.3351e-03, -2.4724e-04, -4.3154e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025804750621318817 0.00093841552734375 0.6410922408103943 0.34000635147094727 tensor(74, device='cuda:0')\n",
            "pred tensor([-7.7009e-04, -1.7509e-03, -1.1820e-04, -4.2992e-03, -3.0493e-01,\n",
            "        -4.7264e-03, -4.7264e-03, -4.5824e-04, -6.7186e-04, -5.9664e-05,\n",
            "        -1.7703e-05, -1.8311e-03, -1.7762e-05, -4.4525e-05, -1.4484e-04,\n",
            "        -1.9562e-04, -8.8215e-06, -1.4317e-04, -6.6578e-05, -2.6941e-04,\n",
            "        -8.6441e-03, -8.3590e-04, -3.8600e-04, -6.0501e-03, -4.0054e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02603030577301979 0.0005946159362792969 0.6699715852737427 0.3481213450431824 tensor(74, device='cuda:0')\n",
            "pred tensor([-1.3876e-04, -3.3855e-05, -1.6928e-05, -5.2273e-05, -1.0672e-03,\n",
            "        -3.8910e-04, -1.4663e-05, -1.4544e-05, -2.6169e-03, -8.2195e-05,\n",
            "        -9.0301e-05, -1.0890e-04, -8.1863e-03, -2.5415e-04, -4.0233e-05,\n",
            "        -7.3910e-06, -6.8963e-05, -5.2547e-04, -9.7179e-04, -3.5107e-05,\n",
            "        -3.0184e-04, -1.5533e-04, -1.0890e-04, -3.3545e-04, -3.0880e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026855310425162315 0.0008254051208496094 0.6753247976303101 0.3769710958003998 tensor(70, device='cuda:0')\n",
            "pred tensor([-2.8732e-02, -2.7256e-03, -1.2598e-03, -1.7719e-03, -3.3593e-04,\n",
            "        -1.7524e-05, -4.0770e-04, -5.2273e-05, -6.0616e-03, -2.4438e-04,\n",
            "        -6.2048e-05, -6.8521e-04, -7.7705e-03, -6.1572e-05, -7.1406e-05,\n",
            "        -1.5364e-03, -7.5817e-04, -2.1458e-05, -1.9407e-04, -5.2023e-04,\n",
            "        -1.3053e-05, -1.0729e-06, -1.4305e-06, -3.2783e-06, -9.0599e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02657751552760601 0.0005145072937011719 0.6639434099197388 0.34233358502388 tensor(67, device='cuda:0')\n",
            "pred tensor([-3.2496e-04, -1.1063e-04, -1.1835e-03, -1.1072e-03, -1.1158e-03,\n",
            "        -6.4270e-02, -2.8610e-03, -1.9491e-04, -3.9520e-03, -3.3021e-04,\n",
            "        -9.6858e-05, -6.3753e-04, -5.9692e-02, -3.5071e-04, -9.0218e-04,\n",
            "        -6.3992e-04, -3.1686e-04, -2.2233e-05, -7.1239e-04, -1.2665e-03,\n",
            "        -1.6842e-03, -1.2665e-03, -2.2469e-03, -8.1348e-04, -1.4830e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028494002297520638 0.0006723403930664062 0.6624315977096558 0.3411523997783661 tensor(89, device='cuda:0')\n",
            "pred tensor([-1.2941e-03, -1.7953e-04, -1.5945e-03, -8.1491e-04, -6.8808e-04,\n",
            "        -9.3162e-05, -7.1168e-05, -8.1491e-04, -2.5415e-04, -6.6340e-05,\n",
            "        -1.1772e-04, -6.5029e-05, -1.5793e-03, -2.1994e-04, -3.2187e-05,\n",
            "        -9.0714e-03, -3.3736e-04, -1.0000e+00, -1.0000e+00, -1.6534e-04,\n",
            "        -2.4438e-06, -9.9540e-06, -2.2256e-04, -7.7844e-05, -2.8074e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02702143043279648 0.0002875328063964844 0.6827855110168457 0.35632872581481934 tensor(84, device='cuda:0')\n",
            "pred tensor([-1.9944e-04, -2.0993e-04, -5.7638e-05, -1.4150e-04, -3.4881e-04,\n",
            "        -7.2241e-05, -1.5140e-05, -6.6042e-05, -5.4955e-05, -1.0862e-03,\n",
            "        -2.6264e-03, -2.2697e-04, -2.3727e-03, -1.3447e-04, -1.0061e-03,\n",
            "        -4.8027e-03, -1.9646e-04, -3.1018e-04, -3.3998e-04, -5.2869e-05,\n",
            "        -3.0065e-04, -1.2598e-03, -2.2054e-06, -4.1485e-05, -4.4250e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026233656331896782 0.0015773773193359375 0.6148689985275269 0.33975398540496826 tensor(82, device='cuda:0')\n",
            "pred tensor([-3.9279e-05, -8.6784e-04, -2.5010e-04, -1.7059e-04, -1.0681e-04,\n",
            "        -4.5204e-03, -9.7998e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -3.3402e-04, -5.2834e-04, -1.7824e-03, -6.3539e-05, -9.0301e-05,\n",
            "        -7.9298e-04, -2.4052e-03, -2.2590e-05, -6.0856e-05, -1.1593e-04,\n",
            "        -4.6825e-04, -3.1590e-05, -2.3365e-03, -6.9618e-04, -5.8770e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028562018647789955 0.0003857612609863281 0.6777099370956421 0.3537857234477997 tensor(99, device='cuda:0')\n",
            "pred tensor([-3.0065e-04, -7.6592e-05, -5.1546e-04, -7.3738e-03, -2.0826e-04,\n",
            "        -2.4915e-04, -1.0151e-04, -2.4933e-02, -9.9854e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -1.9038e-04, -4.9472e-06, -2.5690e-05, -2.5311e-03,\n",
            "        -2.9254e-04, -2.9135e-04, -9.3162e-05, -1.6034e-05, -1.2517e-06,\n",
            "        -5.5432e-06, -1.2636e-05, -4.7278e-04, -4.4107e-06, -1.0806e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0263524129986763 0.00012922286987304688 0.7207983136177063 0.36623477935791016 tensor(76, device='cuda:0')\n",
            "pred tensor([-4.9957e-02, -2.1942e-02, -6.7725e-01, -9.5654e-01, -6.5967e-01,\n",
            "        -4.0796e-01, -2.7222e-02, -1.4043e-04, -3.2496e-04, -9.1400e-03,\n",
            "        -1.0967e-05, -5.9187e-05, -5.4054e-03, -9.5367e-06, -6.1989e-06,\n",
            "        -8.5735e-04, -1.0353e-04, -8.6260e-04, -1.0031e-04, -3.6979e-04,\n",
            "        -1.0270e-04, -6.4790e-05, -2.9254e-04, -3.3875e-02, -2.4247e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03170797601342201 9.441375732421875e-05 0.6955142617225647 0.3559355139732361 tensor(106, device='cuda:0')\n",
            "pred tensor([-4.1723e-04, -3.2377e-04, -1.9038e-04, -5.2452e-06, -2.0676e-03,\n",
            "        -3.3200e-05, -4.2796e-05, -2.3413e-04, -3.8075e-04, -2.4819e-04,\n",
            "        -1.7614e-03, -5.8887e-01, -1.0000e+00, -8.7305e-01, -6.7177e-03,\n",
            "        -1.6870e-03, -9.5010e-05, -1.5764e-03, -1.1194e-04, -8.2195e-05,\n",
            "        -5.5432e-05, -9.4175e-04, -4.8876e-05, -2.0993e-04, -1.0231e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03170187026262283 0.0006875991821289062 0.651373028755188 0.3482440710067749 tensor(114, device='cuda:0')\n",
            "pred tensor([-9.5898e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -8.5831e-05,\n",
            "        -2.0337e-04, -2.4033e-02, -5.1384e-03, -2.8729e-05, -9.7215e-05,\n",
            "        -1.2338e-05, -4.3333e-05, -1.3924e-04, -6.7353e-05, -1.0729e-03,\n",
            "        -2.0142e-03, -1.0639e-04, -3.6621e-04, -2.0421e-04, -1.3030e-04,\n",
            "        -6.0380e-05, -2.7523e-03, -1.7958e-03, -9.0408e-04, -2.0587e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027368785813450813 0.0008931159973144531 0.6307345032691956 0.33806514739990234 tensor(80, device='cuda:0')\n",
            "pred tensor([-4.3297e-04, -1.8167e-04, -1.4091e-04, -1.7953e-04, -8.0994e-02,\n",
            "        -6.4941e-01, -1.0000e+00, -1.0000e+00, -1.3199e-03, -2.3687e-04,\n",
            "        -2.6264e-03, -2.2829e-05, -2.9016e-04, -5.9357e-03, -4.3988e-04,\n",
            "        -3.3545e-04, -2.8114e-03, -5.7869e-03, -3.5038e-03, -3.8395e-03,\n",
            "        -4.6134e-05, -8.6136e-03, -8.2016e-04, -4.6492e-05, -4.0936e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.031453199684619904 8.106231689453125e-05 0.709247350692749 0.3747116029262543 tensor(104, device='cuda:0')\n",
            "pred tensor([-7.4100e-04, -1.9872e-04, -6.6948e-04, -4.0054e-04, -4.4525e-05,\n",
            "        -3.8505e-05, -7.8440e-05, -7.2002e-05, -2.0065e-03, -9.9951e-01,\n",
            "        -1.0000e+00, -3.7050e-04, -1.4651e-04, -8.3923e-04, -3.0899e-04,\n",
            "        -5.2452e-05, -7.2002e-05, -1.6136e-03, -8.4817e-05, -2.0504e-04,\n",
            "        -5.9586e-03, -1.3170e-03, -1.3709e-05, -4.2975e-05, -1.2846e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02763441950082779 0.00021314620971679688 0.692430853843689 0.3618410527706146 tensor(65, device='cuda:0')\n",
            "pred tensor([-6.1095e-05, -1.6344e-04, -2.3878e-04, -2.3331e-02, -2.5158e-03,\n",
            "        -7.9918e-04, -1.6689e-05, -6.9847e-03, -1.1810e-01, -1.4038e-02,\n",
            "        -1.9623e-02, -8.8013e-02, -9.5844e-04, -8.8310e-04, -1.6999e-04,\n",
            "        -9.3520e-05, -4.3058e-04, -4.1080e-04, -3.9795e-02, -2.7370e-04,\n",
            "        -8.1711e-03, -7.5912e-03, -2.1000e-03, -4.9651e-05, -1.6260e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025753064081072807 0.0006513595581054688 0.6457523703575134 0.36219802498817444 tensor(73, device='cuda:0')\n",
            "pred tensor([-1.0729e-03, -6.4015e-05, -2.9707e-04, -6.6528e-03, -1.3411e-05,\n",
            "        -6.1810e-05, -7.5758e-05, -9.3269e-04, -6.3515e-03, -8.0859e-01,\n",
            "        -8.9355e-01, -8.8120e-04, -1.7593e-02, -2.0782e-02, -1.3168e-02,\n",
            "        -4.4441e-03, -6.6040e-02, -6.7711e-03, -2.1338e-01, -7.8906e-01,\n",
            "        -6.2549e-01, -9.2888e-04, -9.0218e-04, -2.6846e-04, -5.2357e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026480285450816154 0.00261688232421875 0.6069778203964233 0.3365548253059387 tensor(71, device='cuda:0')\n",
            "pred tensor([-8.6486e-05, -2.1219e-05, -1.4198e-04, -3.8013e-01, -3.9844e-01,\n",
            "        -9.2334e-01, -3.7769e-01, -2.1069e-01, -9.6416e-04, -1.6708e-03,\n",
            "        -2.4719e-03, -5.3215e-03, -4.2163e-01, -8.3374e-02, -4.4703e-05,\n",
            "        -7.8678e-06, -4.2305e-03, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -2.0623e-05, -1.3554e-04, -6.1083e-04, -6.2525e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026325302198529243 0.0006780624389648438 0.652153491973877 0.33988139033317566 tensor(75, device='cuda:0')\n",
            "pred tensor([-9.5367e-07, -2.2650e-05, -1.1021e-04, -5.1928e-04, -5.6505e-05,\n",
            "        -1.8024e-04, -3.6836e-04, -2.5821e-04, -1.4305e-06, -5.5432e-06,\n",
            "        -4.2992e-03, -3.0661e-04, -8.9943e-05, -2.9135e-04, -6.0616e-03,\n",
            "        -8.1897e-05, -1.0353e-04, -2.8682e-04, -9.4175e-06, -2.3413e-04,\n",
            "        -3.6955e-06, -4.1485e-05, -9.5367e-07, -2.6584e-05, -1.3914e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028193604201078415 5.7220458984375e-05 0.7017359137535095 0.3405054807662964 tensor(90, device='cuda:0')\n",
            "pred tensor([-1.0151e-04, -4.0829e-05, -4.3988e-05, -6.7353e-05, -4.9472e-05,\n",
            "        -3.0184e-04, -5.8949e-05, -1.3769e-04, -5.6601e-04, -4.5598e-05,\n",
            "        -1.3494e-04, -2.7835e-05, -1.3661e-04, -1.1683e-04, -9.2447e-05,\n",
            "        -1.5557e-05, -1.4424e-05, -1.4901e-05, -1.7576e-03, -8.9943e-05,\n",
            "        -2.1935e-05, -2.5606e-04, -1.5783e-04, -5.2691e-05, -3.3593e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03130991384387016 4.1484832763671875e-05 0.7373918294906616 0.34111037850379944 tensor(91, device='cuda:0')\n",
            "pred tensor([-5.5599e-04, -8.9228e-05, -2.1660e-04, -4.7028e-05, -7.4530e-04,\n",
            "        -5.6458e-04, -5.8770e-05, -5.3704e-05, -1.9717e-04, -9.8047e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.3602e-04, -2.9411e-03, -7.2479e-03,\n",
            "        -6.8426e-05, -2.5630e-06, -1.9717e-04, -7.6294e-05, -1.0192e-05,\n",
            "        -2.0564e-05, -1.7285e-06, -2.1231e-04, -6.2525e-05, -1.9848e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028173932805657387 0.00014972686767578125 0.6788414716720581 0.34064981341362 tensor(76, device='cuda:0')\n",
            "pred tensor([-1.0338e-03, -5.9357e-03, -1.1272e-03, -2.3918e-03, -1.0633e-03,\n",
            "        -2.0580e-03, -5.6696e-04, -2.6474e-03, -1.1272e-03, -5.1022e-05,\n",
            "        -2.5725e-04, -1.7285e-05, -2.1827e-04, -9.0933e-04, -1.3340e-04,\n",
            "        -4.8399e-04, -7.8678e-04, -2.7370e-04, -6.3372e-04, -1.0862e-03,\n",
            "        -2.3782e-04, -1.0223e-03, -1.0000e+00, -1.0000e+00, -3.0458e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.03114924393594265 0.00011014938354492188 0.6775021553039551 0.33725064992904663 tensor(89, device='cuda:0')\n",
            "pred tensor([-2.5272e-05, -3.5524e-05, -2.6131e-04, -5.7697e-04, -7.4565e-05,\n",
            "        -2.0337e-04, -1.2290e-04, -3.4511e-05, -3.0935e-05, -1.3471e-05,\n",
            "        -1.4997e-04, -1.8001e-05, -1.3554e-04, -2.8348e-04, -1.4603e-05,\n",
            "        -3.8385e-05, -5.0449e-04, -1.9872e-04, -6.4015e-05, -4.3988e-04,\n",
            "        -5.3072e-04, -9.8765e-05, -3.5346e-05, -2.5690e-05, -1.3661e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027590801939368248 0.0017566680908203125 0.6151943206787109 0.35488006472587585 tensor(80, device='cuda:0')\n",
            "72\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -3.9887e-04, -1.3876e-04, -1.0073e-04,\n",
            "        -5.6922e-05, -3.0184e-04, -3.1948e-05, -1.8299e-05, -4.3631e-05,\n",
            "        -6.9160e-03, -2.7313e-03, -5.7364e-04, -9.7752e-04, -1.0633e-03,\n",
            "        -1.0633e-03, -1.0556e-04, -3.7932e-04, -8.4448e-04, -4.8208e-04,\n",
            "        -3.0541e-04, -6.0043e-03, -9.5725e-05, -4.1485e-05, -2.2960e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028526468202471733 0.0023651123046875 0.6215004324913025 0.3392655551433563 tensor(90, device='cuda:0')\n",
            "pred tensor([-4.0770e-04, -1.0948e-03, -1.2999e-03, -3.6011e-03, -7.2241e-04,\n",
            "        -5.5611e-05, -3.7193e-04, -3.1877e-04, -3.8981e-04, -1.9569e-03,\n",
            "        -1.6868e-04, -4.7183e-04, -7.5758e-05, -7.8142e-05, -2.2173e-04,\n",
            "        -6.4969e-06, -6.3777e-06, -5.0068e-06, -1.2636e-05, -2.6226e-06,\n",
            "        -6.6161e-06, -9.9957e-05, -2.2256e-04, -2.1038e-03, -1.1780e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02655479870736599 0.000438690185546875 0.6812977194786072 0.3352823257446289 tensor(68, device='cuda:0')\n",
            "pred tensor([-2.6321e-04, -1.4043e-04, -1.7405e-04, -1.4722e-05, -4.7028e-05,\n",
            "        -1.4603e-05, -2.9325e-05, -7.7844e-05, -5.0116e-04, -5.6088e-05,\n",
            "        -1.2636e-05, -6.7115e-05, -2.6882e-05, -5.7638e-05, -2.6584e-05,\n",
            "        -1.7738e-04, -5.9187e-05, -1.6689e-06, -3.0577e-05, -2.3687e-04,\n",
            "        -1.3113e-06, -1.0556e-04, -1.7536e-04, -5.3465e-05, -3.0398e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02944949083030224 4.57763671875e-05 0.7147760391235352 0.35506513714790344 tensor(73, device='cuda:0')\n",
            "pred tensor([-4.6313e-05, -4.7028e-05, -2.2233e-05, -1.8177e-03, -3.0899e-04,\n",
            "        -1.2934e-04, -4.3893e-04, -3.7909e-05, -6.3539e-05, -3.3736e-05,\n",
            "        -1.2195e-04, -2.1565e-04, -1.6415e-04, -3.5620e-04, -2.5320e-04,\n",
            "        -1.7464e-04, -1.0514e-04, -3.5763e-06, -4.2915e-06, -7.7844e-05,\n",
            "        -3.1018e-04, -8.9228e-05, -9.8765e-05, -3.8683e-05, -1.3649e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025435231626033783 3.337860107421875e-06 0.712458074092865 0.3352214992046356 tensor(52, device='cuda:0')\n",
            "pred tensor([-2.5320e-04, -8.9228e-05, -3.0994e-06, -1.1921e-06, -1.6868e-04,\n",
            "        -4.0710e-05, -9.7752e-06, -6.2158e-01, -1.8030e-01, -4.2114e-02,\n",
            "        -2.7061e-04, -1.4210e-03, -5.6419e-03, -3.7479e-04, -1.2338e-04,\n",
            "        -5.1856e-05, -5.1260e-05, -1.0639e-04, -2.0862e-06, -1.4126e-05,\n",
            "        -1.2123e-02, -4.1175e-04, -8.2552e-05, -1.2398e-03, -9.3079e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028746681287884712 9.5367431640625e-06 0.7521224617958069 0.3548838496208191 tensor(60, device='cuda:0')\n",
            "pred tensor([-3.4153e-05, -5.7161e-05, -2.2960e-04, -5.4121e-05, -1.5140e-05,\n",
            "        -1.3947e-05, -4.6492e-06, -9.5367e-06, -1.4901e-06, -1.6689e-06,\n",
            "        -4.3511e-05, -2.7585e-04, -6.2370e-04, -1.2569e-03, -1.3530e-05,\n",
            "        -2.2888e-05, -3.6895e-05, -4.1890e-04, -1.9336e-04, -1.0386e-03,\n",
            "        -5.2738e-04, -5.1856e-06, -9.9540e-06, -7.0870e-05, -5.9319e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028453415259718895 0.00014829635620117188 0.6926565766334534 0.33698979020118713 tensor(76, device='cuda:0')\n",
            "pred tensor([-4.8590e-04, -3.6764e-04, -1.5056e-04, -1.6034e-04, -3.4404e-04,\n",
            "        -1.1883e-03, -1.4439e-03, -7.4100e-04, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -6.5565e-05, -6.5029e-05, -5.2452e-06, -2.3842e-05,\n",
            "        -1.3709e-06, -1.5783e-04, -1.5278e-03, -4.6670e-05, -1.1292e-03,\n",
            "        -3.8207e-05, -3.7861e-04, -4.7922e-05, -2.4261e-02, -1.5099e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02651199698448181 0.00153350830078125 0.6359959840774536 0.33330681920051575 tensor(68, device='cuda:0')\n",
            "pred tensor([-3.0098e-03, -1.3885e-03, -1.8525e-04, -1.6224e-04, -5.4121e-04,\n",
            "        -3.3081e-05, -2.9802e-07, -2.9640e-03, -6.3002e-05, -5.4359e-05,\n",
            "        -2.7001e-05, -5.4836e-06, -6.2764e-05, -1.1986e-02, -7.8320e-01,\n",
            "        -9.9902e-01, -1.0000e+00, -9.9902e-01, -6.1810e-05, -8.0943e-05,\n",
            "        -3.0577e-05, -1.2290e-04, -1.2934e-04, -1.6344e-04, -1.2941e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026327580213546753 0.0013303756713867188 0.6482398509979248 0.3368663191795349 tensor(68, device='cuda:0')\n",
            "pred tensor([-1.6327e-03, -1.4925e-03, -9.3520e-05, -4.6997e-03, -9.0866e-03,\n",
            "        -2.0905e-03, -8.1062e-04, -2.3270e-03, -8.9188e-03, -1.4257e-04,\n",
            "        -5.9891e-04, -3.8815e-04, -3.2496e-04, -1.2846e-03, -7.0534e-03,\n",
            "        -5.9652e-04, -3.0422e-04, -4.1504e-03, -2.5010e-04, -2.4343e-04,\n",
            "        -1.3867e-01, -1.7981e-01, -7.4565e-05, -9.2983e-06, -1.2481e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.0270914938300848 0.0012454986572265625 0.6563374996185303 0.3493468165397644 tensor(67, device='cuda:0')\n",
            "pred tensor([-7.6950e-05, -2.5129e-04, -7.0000e-04, -1.4651e-04, -3.8971e-02,\n",
            "        -3.6068e-03, -1.2434e-04, -6.7043e-04, -9.7122e-03, -1.5235e-04,\n",
            "        -1.2100e-05, -1.7810e-04, -1.9045e-03, -9.9540e-06, -4.2975e-05,\n",
            "        -4.7946e-04, -6.9695e-03, -5.4407e-04, -5.4836e-04, -1.0931e-04,\n",
            "        -3.9749e-03, -9.9951e-01, -1.0000e+00, -2.0504e-04, -7.8440e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.023521099239587784 0.000759124755859375 0.6446704268455505 0.3345642685890198 tensor(57, device='cuda:0')\n",
            "pred tensor([-6.0940e-04, -6.6340e-05, -2.6011e-04, -6.3539e-05, -9.3579e-06,\n",
            "        -1.0777e-03, -1.8454e-04, -2.2876e-04, -1.1009e-02, -1.6747e-03,\n",
            "        -7.7248e-05, -3.7491e-05, -2.9707e-04, -3.6297e-03, -1.9264e-04,\n",
            "        -5.5170e-04, -6.0606e-04, -1.1325e-04, -2.6631e-04, -2.5511e-04,\n",
            "        -1.6284e-04, -3.2067e-04, -1.5056e-04, -1.3292e-04, -6.7854e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02825845405459404 0.0004482269287109375 0.6837348937988281 0.3452063798904419 tensor(87, device='cuda:0')\n",
            "pred tensor([-9.4678e-01, -1.0000e+00, -1.0000e+00, -4.4847e-04, -3.9062e-03,\n",
            "        -1.3661e-04, -8.4448e-04, -5.1212e-04, -2.3139e-04, -1.8101e-03,\n",
            "        -2.0659e-04, -1.3947e-05, -9.3579e-06, -2.0337e-04, -9.3520e-05,\n",
            "        -8.1348e-04, -1.4191e-03, -9.4593e-05, -1.8966e-04, -2.2876e-04,\n",
            "        -1.4732e-02, -1.5593e-04, -3.5942e-05, -1.9455e-03, -5.5933e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027589483186602592 0.00014448165893554688 0.703826904296875 0.3558186888694763 tensor(94, device='cuda:0')\n",
            "pred tensor([-3.5071e-04, -4.8876e-05, -6.6817e-05, -3.3855e-05, -1.0890e-04,\n",
            "        -9.6858e-05, -6.8545e-06, -8.2850e-06, -1.5097e-03, -4.2558e-04,\n",
            "        -9.4116e-02, -1.0000e+00, -1.0000e+00, -1.6613e-03, -7.0801e-03,\n",
            "        -1.9045e-03, -5.5504e-04, -1.1635e-04, -6.2513e-04, -1.2636e-04,\n",
            "        -4.6825e-04, -8.9233e-02, -2.5864e-02, -2.6846e-04, -1.4257e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.027911676093935966 0.0010509490966796875 0.638596773147583 0.33271411061286926 tensor(62, device='cuda:0')\n",
            "pred tensor([-1.4296e-03, -9.1374e-05, -6.0616e-03, -1.4136e-01, -7.5245e-04,\n",
            "        -7.6723e-04, -1.1367e-04, -2.8729e-05, -2.0921e-05, -8.3923e-04,\n",
            "        -1.6870e-03, -6.8808e-04, -1.2093e-03, -1.2529e-04, -8.5473e-05,\n",
            "        -2.4247e-04, -3.0184e-04, -2.5158e-03, -2.4855e-05, -2.9445e-05,\n",
            "        -1.5724e-04, -2.5010e-04, -5.8949e-05, -3.0065e-04, -1.1673e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025541407987475395 0.0005168914794921875 0.6673719882965088 0.3469494581222534 tensor(73, device='cuda:0')\n",
            "pred tensor([-4.0936e-04, -3.7265e-04, -1.2195e-04, -5.0664e-05, -7.0035e-05,\n",
            "        -8.6844e-05, -1.3816e-04, -5.4646e-04, -1.6606e-04, -1.8525e-04,\n",
            "        -1.3661e-04, -4.9651e-05, -1.9312e-05, -2.7120e-05, -4.7112e-04,\n",
            "        -2.5129e-04, -2.9254e-04, -1.7738e-04, -4.3631e-05, -8.7500e-05,\n",
            "        -3.1352e-05, -9.5010e-05, -2.7490e-04, -1.5259e-05, -3.4809e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.029343387112021446 0.0003066062927246094 0.678276777267456 0.34488239884376526 tensor(96, device='cuda:0')\n",
            "pred tensor([-2.4068e-04, -1.4651e-04, -2.4974e-05, -1.1963e-04, -1.9717e-04,\n",
            "        -2.6166e-05, -2.7199e-03, -8.5592e-04, -3.3081e-05, -3.0184e-04,\n",
            "        -1.3924e-04, -5.9903e-05, -3.7861e-04, -8.5115e-05, -3.3081e-05,\n",
            "        -1.3089e-04, -2.5094e-05, -1.0042e-03, -3.6478e-05, -1.2636e-05,\n",
            "        -3.3998e-04, -1.1683e-04, -6.6817e-05, -1.1187e-03, -8.0943e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.028185520321130753 0.0006074905395507812 0.6441079378128052 0.3373640477657318 tensor(75, device='cuda:0')\n",
            "pred tensor([-1.6451e-03, -4.5919e-04, -1.3292e-04, -2.5768e-03, -4.4441e-03,\n",
            "        -2.1332e-02, -9.9951e-01, -9.9951e-01, -6.1798e-04, -4.5240e-05,\n",
            "        -1.4663e-05, -1.5903e-04, -4.2224e-04, -4.7565e-04, -1.2314e-02,\n",
            "        -6.5517e-04, -1.8463e-03, -9.2697e-03, -1.9798e-03, -1.4839e-03,\n",
            "        -4.3511e-05, -7.2098e-04, -1.4484e-04, -4.3654e-04, -4.9448e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.023712800815701485 0.0007100105285644531 0.6244461536407471 0.33781665563583374 tensor(63, device='cuda:0')\n",
            "pred tensor([-3.2482e-03, -6.7353e-05, -3.0746e-03, -1.8597e-04, -6.7329e-04,\n",
            "        -2.6894e-03, -1.3514e-03, -2.4152e-04, -1.2922e-03, -3.7651e-03,\n",
            "        -2.7704e-04, -5.0926e-04, -9.3985e-04, -4.2892e-04, -2.0905e-03,\n",
            "        -2.2256e-04, -7.2412e-01, -9.9756e-01, -1.0000e+00, -5.6028e-06,\n",
            "        -9.0659e-05, -9.2387e-06, -1.5020e-05, -2.2602e-04, -6.7353e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026565006002783775 0.0007810592651367188 0.6330568790435791 0.35476651787757874 tensor(82, device='cuda:0')\n",
            "pred tensor([-1.0973e-04, -9.0301e-05, -1.0462e-03, -7.9691e-05, -1.0481e-03,\n",
            "        -4.7569e-03, -4.4594e-03, -5.6601e-04, -4.4250e-04, -6.7353e-05,\n",
            "        -4.6635e-04, -1.6749e-05, -4.1723e-07, -1.8525e-04, -2.6025e-01,\n",
            "        -9.9951e-01, -1.0000e+00, -1.0000e+00, -3.3212e-04, -1.0973e-04,\n",
            "        -1.2236e-03, -9.5520e-02, -4.2285e-01, -4.6997e-03, -1.4710e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026950325816869736 0.001026153564453125 0.6513363718986511 0.34402576088905334 tensor(72, device='cuda:0')\n",
            "pred tensor([-4.4847e-04, -3.7932e-04, -2.0421e-04, -3.7789e-05, -6.6578e-05,\n",
            "        -3.9577e-05, -7.8738e-05, -1.1194e-04, -1.7309e-03, -1.2865e-03,\n",
            "        -9.9805e-01, -1.0000e+00, -1.0000e+00, -3.5691e-04, -6.7444e-03,\n",
            "        -2.8286e-03, -6.8521e-04, -1.9226e-03, -6.9885e-02, -9.5062e-03,\n",
            "        -3.7079e-03, -1.5545e-03, -3.5691e-04, -2.0676e-03, -4.6806e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.024530334398150444 0.0015869140625 0.6248607635498047 0.3357032239437103 tensor(58, device='cuda:0')\n",
            "pred tensor([-5.9891e-04, -3.6640e-03, -6.6414e-03, -1.5354e-04, -1.8477e-05,\n",
            "        -9.2447e-05, -6.7890e-05, -7.3314e-06, -3.5763e-07, -1.0389e-04,\n",
            "        -9.9658e-01, -9.7510e-01, -1.0000e+00, -1.0000e+00, -9.3079e-04,\n",
            "        -6.1572e-05, -5.0354e-04, -2.0027e-03, -4.9133e-03, -1.2093e-03,\n",
            "        -5.2357e-04, -5.3787e-04, -1.3914e-03, -1.9562e-04, -1.0023e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02668900042772293 0.00035953521728515625 0.6517009139060974 0.33698347210884094 tensor(69, device='cuda:0')\n",
            "pred tensor([-3.6359e-05, -9.6083e-05, -8.7118e-04, -3.0065e-04, -7.0143e-04,\n",
            "        -7.1220e-03, -1.4410e-03, -3.8600e-04, -9.8896e-04, -2.0714e-03,\n",
            "        -8.4114e-04, -1.3494e-04, -1.4365e-04, -8.2302e-04, -1.3232e-04,\n",
            "        -5.4550e-04, -1.1104e-04, -1.0099e-03, -3.7622e-04, -1.1997e-03,\n",
            "        -4.5562e-04, -1.4424e-05, -7.1955e-04, -1.9188e-03, -2.2831e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026079872623085976 0.00013303756713867188 0.6679214835166931 0.33849963545799255 tensor(72, device='cuda:0')\n",
            "pred tensor([-2.7370e-04, -2.6703e-05, -6.4254e-05, -3.9577e-04, -1.2982e-04,\n",
            "        -4.5598e-05, -3.5172e-03, -6.3539e-05, -1.3304e-03, -1.4296e-03,\n",
            "        -3.7365e-03, -7.2718e-06, -7.1704e-05, -2.6584e-05, -1.1915e-04,\n",
            "        -1.5962e-04, -1.8120e-05, -2.7061e-04, -1.4048e-03, -8.5115e-05,\n",
            "        -5.0843e-05, -1.7679e-04, -1.7824e-03, -1.5056e-04, -9.6989e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.026101287454366684 7.486343383789062e-05 0.6945926547050476 0.3356381356716156 tensor(76, device='cuda:0')\n",
            "pred tensor([-6.6817e-05, -1.0473e-04, -6.9714e-04, -5.9113e-02, -9.8096e-01,\n",
            "        -9.9805e-01, -3.4161e-03, -5.7182e-03, -1.7681e-03, -2.8400e-03,\n",
            "        -1.6344e-04, -3.7479e-04, -8.8453e-04, -3.0828e-04, -9.9945e-03,\n",
            "        -3.5172e-03, -5.0697e-03, -3.6507e-03, -4.4346e-04, -3.2379e-02,\n",
            "        -7.0534e-03, -3.5977e-04, -1.1683e-05, -8.6594e-04, -3.7109e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.02267005853354931 6.0558319091796875e-05 0.6986211538314819 0.3416294753551483 tensor(51, device='cuda:0')\n",
            "pred tensor([-2.6631e-04, -3.3545e-04, -2.6779e-03, -4.7207e-05, -4.4098e-03,\n",
            "        -2.4109e-01, -1.2794e-02, -3.7537e-02, -6.9160e-03, -3.8548e-03,\n",
            "        -8.4305e-03, -1.2350e-03, -1.5664e-04, -3.1853e-03, -1.8525e-04,\n",
            "        -6.8903e-04, -1.2569e-03, -2.7370e-04, -1.0765e-04, -2.4624e-03,\n",
            "        -2.4819e-04, -9.9957e-05, -1.0228e-04, -1.7941e-05, -7.8392e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "repr, std, cov, clossl, wrong 0.025143755599856377 0.0003566741943359375 0.6701511740684509 0.33689776062965393 tensor(62, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "for i in range(100):\n",
        "    print(i)\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "31c1755c-ef2a-4f8c-ba23-806590aaa7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "# ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n",
        "\n",
        "\n",
        "# # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "# 2  5/11 8\n",
        "# 1/10 4 7/9\n",
        "# 0  3/12 6\n",
        "\n",
        "# 13 11 14\n",
        "# 10 12 9\n",
        "\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "env = TimeLimit(env, max_episode_steps=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PraFUAPB3j7v",
        "outputId": "329ef0aa-3ac2-43e9-8784-567b3a45bd4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-ff80b479d892>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-42-ff80b479d892>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dided\n",
            "time\n",
            "[13, 13, 2, 1, 0, 4, 13, 13, 0, 13, 12, 0, 12, 13, 13, 12, 13, 13, 13, 4, 13, 13, 13, 13, 10, 10, 0, 11, 7, 12, 4, 12, 13, 12, 14, 4, 12, 4, 4, 12, 6, 1, 9, 9, 14, 4, 13, 13, 13, 13, 13, 13, 4, 13, 11, 13, 13, 0, 13, 13, 4, 4, 13, 13, 13, 13, 12, 12, 0, 12, 1, 1, 1, 1, 6, 11, 13, 12, 14, 13, 12, 4, 13, 13, 13, 12, 12, 0, 4, 1, 6, 6, 7, 13, 14, 0, 12, 13, 13, 13, 13, 12, 4, 13, 13, 4, 12, 13, 12, 13, 4, 12, 13, 13, 13, 11, 4, 13, 4, 4, 4, 13, 13, 10, 13, 0, 13, 13, 4, 13, 13, 13, 13, 4, 13, 4, 4, 13, 13, 6, 13, 4, 13, 13, 13, 13, 1, 1, 12, 13, 4, 13, 13, 13, 13, 13, 4, 10, 13, 11, 13, 13, 13, 4, 13, 13, 13, 13, 12, 4, 13, 4, 5, 1, 1, 2, 13, 0, 13, 12, 4, 13, 13, 1, 13, 12, 13, 12, 4, 13, 13, 10, 13, 4, 13, 4, 13, 13, 13, 1, 12, 13, 12, 4, 13, 12, 13, 13, 10, 6, 1, 1, 1, 6, 6, 6, 0, 13, 13, 13, 4, 12, 13, 13, 13, 13, 4, 13, 4, 4, 13, 4, 0, 13, 13, 13, 13, 4, 13, 4, 4, 13, 13, 13, 13, 13, 4, 4, 13, 4, 12, 13, 0, 12, 4, 13, 10, 6, 0, 14, 1, 1, 5, 1, 12, 13, 13, 4, 4, 13, 13, 1, 12, 0, 12, 13, 4, 12, 13, 13, 12, 13, 12, 13, 4, 4, 0, 13, 13, 4, 13, 13, 4, 13, 13, 0, 13, 13, 13, 13, 13, 7, 4, 13, 13, 13, 13, 13, 1, 1, 10, 1, 4, 4, 4, 13, 4, 10, 4, 0, 13]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    lstate=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6], [1, 256]\n",
        "        lstate.append(state)\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # act = agent(state, k)\n",
        "            act = agent(lstate, k=k)\n",
        "            lstate=[]\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"dided\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cm6KjvBrnNO",
        "outputId": "38db3cb7-fc99-44b0-ac57-5ceb0109f8ca",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-d80e75a9d669>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-47-d80e75a9d669>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dided\n",
            "time\n",
            "[8, 8, 4, 5, 3, 5, 8, 8, 14, 8, 8, 5, 4, 8, 8, 13, 6, 8, 10, 8, 13, 8, 9, 3, 0, 4, 14, 0, 9, 4, 5, 4, 3, 3, 8, 5, 3, 8, 3, 8, 11, 8, 0, 8, 0, 8, 4, 6, 4, 8, 2, 14, 8, 14, 8, 8, 8, 8, 8, 3, 0, 14, 14, 8, 8, 8, 3, 2, 3, 0, 0, 4, 3, 3, 11, 7, 4, 5, 5, 8, 4, 8, 0, 8, 14, 6, 2, 9, 0, 8, 0, 5, 2, 8, 4, 6, 8, 2, 0, 6, 1, 2, 4, 14, 12, 8, 14, 2, 8, 8, 5, 8, 8, 0, 7, 8, 2, 8, 3, 5, 2, 13, 3, 8, 8, 4, 13, 8, 4, 2, 5, 8, 11, 3, 4, 3, 2, 14, 9, 8, 14, 8, 3, 8, 8, 3, 3, 13, 8, 2, 11, 4, 3, 3, 8, 2, 3, 5, 14, 4, 8, 4, 8, 8, 4, 5, 8, 4, 9, 3, 13, 5, 13, 8, 3, 5, 3, 8, 14, 9, 11, 4, 5, 4, 3, 14, 2, 8, 3, 8, 9, 8, 8, 8, 0, 3, 8, 4, 5, 8, 3, 8, 5, 14, 3, 13, 8, 8, 14, 13, 5, 8, 3, 11, 8, 8, 8, 4, 5, 8, 8, 8, 4, 3, 8, 2, 8, 3, 3, 14, 13, 2, 8, 2, 8, 8, 14, 2, 0, 11, 4, 3, 11, 13, 4, 8, 4, 8, 9, 8, 3, 5, 8, 9, 9, 6, 9, 3, 11, 11, 8, 11, 5, 2, 3, 4, 3, 11, 4, 3, 8, 3, 3, 8, 8, 4, 13, 3, 8, 6, 4, 4, 8, 9, 0, 8, 13, 8, 3, 3, 8, 0, 11, 2, 8, 2, 3, 8, 8, 8, 14, 14, 2, 8, 11, 2, 2, 6, 14, 8, 2, 11, 4, 8, 14, 5, 9, 3, 8, 9, 0, 8, 14, 3, 8, 13, 8, 3, 14, 8, 6, 0, 8, 8, 3, 8, 8, 3, 5, 14, 14, 9, 4, 14, 2, 3, 8, 11, 3, 3, 8, 8, 0, 5, 4, 2, 2, 13, 4, 13, 2, 2, 2, 5, 11, 9, 3, 5, 4, 8, 0, 2, 4, 5, 8, 4, 8, 8, 9, 2, 11, 13, 11, 3, 11, 4, 4, 8, 8, 8, 8, 4, 6, 4, 8, 2, 9, 3, 8, 3, 14, 11, 2, 8, 4, 4, 8, 8, 8, 2, 2, 4, 3, 14, 0, 5, 2, 8, 0, 0, 9, 8, 8, 4, 8, 4, 0, 11, 8, 9, 5, 5, 8, 5, 8, 13, 8, 2, 8, 8, 11, 14, 8, 0, 0, 8, 4, 2, 6, 3, 2, 6, 2, 8, 8, 14, 2, 14, 2, 4, 8, 8, 4, 4, 8, 14, 3, 14, 8, 8, 8, 4, 2, 0, 3, 5, 4, 2, 8, 14, 3, 3, 9, 14, 0, 5, 8, 6, 11, 8, 8, 2, 5, 14, 3, 11, 8, 0, 2, 13, 5, 8, 8, 8, 5, 3, 8, 13, 3, 8, 11, 0, 3, 5, 8, 5, 13, 11, 8, 3, 5, 0, 8, 3, 4, 3, 4, 0, 8, 14, 3, 8, 8, 4, 4, 3, 3, 5, 0, 3, 0, 0, 8, 11, 13, 8, 9, 11, 14, 8, 2, 13, 8, 11, 8, 4, 0, 11, 8, 8, 8, 14, 3, 5, 5, 2, 8, 3, 4, 0, 11, 9, 8, 3, 14, 8, 8, 14, 3, 8, 2, 11, 8, 8, 3, 11, 14, 5, 8, 8, 3, 11, 3, 3, 2, 0, 8, 14, 8, 3, 0, 8, 3, 3, 8, 3, 0, 3, 11, 8, 2, 3, 0, 3, 8, 8, 13, 8, 14]\n",
            "dided\n",
            "time\n",
            "[8, 14, 4, 4, 4, 2, 8, 11, 8, 8, 8, 3, 2, 9, 8, 0, 2, 8, 8, 5, 8, 0, 2, 14, 3, 8, 8, 8, 8, 2, 3, 8, 8, 8, 13]\n",
            "dided\n",
            "time\n",
            "[8, 13, 8, 6, 8, 8, 11, 6, 13, 8, 8, 4, 8, 8, 2, 3, 9, 13, 8, 11, 5, 5, 3, 4, 8, 3, 8, 3, 12, 8, 14, 8, 0, 3, 4, 2, 0, 6, 4, 2, 4, 14, 4, 3, 3, 11, 11, 8, 13, 8, 8, 4, 2, 8, 8, 13, 3, 8, 5, 4, 3, 14, 9, 8, 14, 11, 4, 8, 3, 6, 8, 5, 3, 8, 5, 0, 4, 2, 9, 2, 3, 0, 8, 8, 8, 8, 0, 9, 3, 3, 8, 14, 8, 8, 13, 8, 14, 3, 3, 8, 4, 8, 9, 14, 8, 3, 3, 8, 8, 8, 9, 11, 3, 14, 8, 9, 8, 4, 8, 3, 3, 6, 8, 8, 13, 14, 0, 4, 3, 5, 13, 4, 2, 0, 8, 2, 2, 13, 14, 0, 2, 4, 8, 5, 3, 6, 8, 3, 3, 0, 3, 8, 3, 3, 8, 8, 8, 4, 14, 14, 5, 14, 8, 0, 8, 4, 8, 3, 3, 3, 8, 8, 3, 3, 2, 14, 11, 3, 13, 5, 3, 3, 11, 13, 8, 8, 8, 8, 8, 8, 8, 3, 3, 2, 11, 2, 14, 14, 5, 8, 0, 4, 0, 14, 2, 4, 14, 8, 4, 14, 13, 5, 3, 8, 8, 4, 3, 3, 0]\n",
            "dided\n",
            "time\n",
            "[3, 0, 11, 4, 10, 3, 2, 8, 6, 8, 8, 8, 4, 3, 4, 8, 8, 3, 8, 14, 8, 8, 14, 8, 9, 11, 9, 8, 0, 14, 14, 8, 8, 5, 4, 11, 0, 9, 14, 0, 3, 4]\n",
            "dided\n",
            "time\n",
            "[4, 0, 8, 3, 5, 14, 3, 8, 5, 8, 4, 3, 8, 11, 8, 4, 8, 4, 8, 3, 8, 8, 5, 14, 11, 8, 8, 8, 8, 3, 9, 4, 4, 4, 13, 14, 6, 11, 5, 9, 2, 8, 8, 8, 9, 8, 5, 14, 8, 3, 4, 2, 8, 3, 8, 0, 0, 8, 9, 0, 0, 14, 8, 5, 3, 8, 8, 14, 14, 13, 14, 8, 0, 5, 3, 4, 4, 4, 11, 8, 5, 3, 13, 8, 8, 8, 13, 6, 2, 3]\n",
            "0 #### train ####\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-d80e75a9d669>:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-47-d80e75a9d669>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "repr, std, cov, clossl, z, norm 0.050199687480926514 0.2283935546875 3.4165687561035156 0.49619200825691223 1.3102490901947021 2.627256393432617\n",
            "repr, std, cov, clossl, z, norm 0.05339137092232704 0.2294921875 3.3471217155456543 0.4295380711555481 1.2595643997192383 2.339510202407837\n",
            "repr, std, cov, clossl, z, norm 0.04236960411071777 0.2298583984375 3.330925464630127 0.42237308621406555 1.278914213180542 2.836348533630371\n",
            "repr, std, cov, clossl, z, norm 0.05475496128201485 0.2296142578125 3.3375391960144043 0.4159761965274811 1.2828510999679565 2.350651264190674\n",
            "repr, std, cov, clossl, z, norm 0.04160648211836815 0.2293701171875 3.3557305335998535 0.45039573311805725 1.4270884990692139 2.6106982231140137\n",
            "repr, std, cov, clossl, z, norm 0.0648849755525589 0.2291259765625 3.3698673248291016 0.5053079724311829 1.3031196594238281 2.460367202758789\n",
            "repr, std, cov, clossl, z, norm 0.040094297379255295 0.2308349609375 3.2985994815826416 0.43139466643333435 1.2930095195770264 2.6224358081817627\n",
            "repr, std, cov, clossl, z, norm 0.05146126076579094 0.231201171875 3.274540424346924 0.4848814308643341 1.2869971990585327 2.209646701812744\n",
            "repr, std, cov, clossl, z, norm 0.040287576615810394 0.22607421875 3.504601001739502 0.4350287914276123 1.2827478647232056 2.573362350463867\n",
            "repr, std, cov, clossl, z, norm 0.04332108423113823 0.228515625 3.4079105854034424 0.4363424777984619 1.2271088361740112 2.102628231048584\n",
            "repr, std, cov, clossl, z, norm 0.030417054891586304 0.2265625 3.5241854190826416 0.40806660056114197 1.3688805103302002 2.1776013374328613\n",
            "repr, std, cov, clossl, z, norm 0.03740246966481209 0.2271728515625 3.4766132831573486 0.3913949728012085 1.2893091440200806 2.1527504920959473\n",
            "train_data.data 20482\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 4, 2, 2, 8, 4, 8, 14, 6, 8, 3, 3, 8, 8, 8, 4, 2, 8, 8, 0, 8, 9, 2, 2, 3, 8, 3, 8, 8, 14, 8, 2, 3, 3, 13, 4, 2, 3, 3, 0, 13, 8, 8, 4, 14, 2, 2, 6, 4, 8, 8, 8, 14, 6, 14, 2, 3, 3, 8, 3, 3, 3, 9, 3, 3, 3, 3, 4, 3, 5, 11, 13, 3, 3, 3, 3, 4, 2, 8, 5, 4, 3, 5, 11, 8, 4, 9, 13, 3, 8, 2, 3, 4, 8, 0, 14, 3, 5, 2, 14, 14, 4, 9, 8, 8, 3, 8, 0, 3, 3, 0, 6]\n",
            "dided\n",
            "time\n",
            "[6, 8, 2, 8, 14, 3, 3, 8, 4, 11, 9, 8, 3, 11, 6, 8, 3, 14, 8, 14]\n",
            "dided\n",
            "time\n",
            "[14, 8, 14, 8, 8, 13, 0, 8, 8, 9, 8, 6, 4, 3, 8, 2, 3, 8, 8, 8, 8, 13, 0, 2, 8, 11, 4, 8, 8, 3, 8, 8, 9, 8, 11, 14, 8, 3, 5, 8, 0, 8, 13, 11, 3, 8, 8, 14, 8, 3, 8, 2, 14, 8, 14, 4, 3, 8, 0, 13, 14, 3, 8, 3, 4, 4, 8, 8, 8, 14, 14, 11, 14, 0, 14, 3, 3, 11, 2, 9, 3, 4, 8, 11, 14, 2, 14, 4, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 4, 0, 8, 4, 3, 9, 5, 0, 4, 0, 3, 8, 10, 6, 14, 5, 11, 4, 4, 8, 0, 14, 2, 14, 13, 8, 2, 0, 8, 8, 8, 8, 14, 3, 8, 4, 8, 5, 8, 8, 3, 4, 4, 8, 14, 4, 8, 3]\n",
            "dided\n",
            "time\n",
            "[3, 6, 0, 8, 8, 4, 3, 2, 0, 2, 5, 8, 0, 3, 5, 5, 8, 8, 5, 2, 3, 8, 0, 8, 0, 14, 3, 8, 8, 8, 4, 13, 3, 4, 3, 2, 8, 5, 5, 8, 14, 8, 8, 8, 2, 5, 2, 8, 11, 13, 8, 14, 4, 8, 14, 8, 8, 11, 8, 11, 3, 8, 5, 13, 8, 3, 2, 3, 13, 8, 5, 5, 3, 13, 9, 8, 2, 14, 0, 13, 14, 4, 3, 2, 8, 2, 4, 0, 8, 4, 2, 5, 11, 3, 14, 3, 3, 4, 14, 8, 14, 3, 14, 3, 3, 14, 7, 8, 8, 4, 3, 8, 9, 14, 8, 8, 2, 8, 8, 8, 8, 3, 5, 4, 3, 3, 3, 9, 8, 9, 3, 4, 4, 8, 0, 8, 14, 8, 8, 13, 13, 14, 13, 14]\n",
            "1 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.030239468440413475 0.233642578125 3.158824920654297 0.37288448214530945 1.2757306098937988 2.152578115463257\n",
            "repr, std, cov, clossl, z, norm 0.03477417677640915 0.2314453125 3.265927791595459 0.46659520268440247 1.2961010932922363 1.7384819984436035\n",
            "repr, std, cov, clossl, z, norm 0.03046225570142269 0.2266845703125 3.5106117725372314 0.4347151815891266 1.3484346866607666 2.3537771701812744\n",
            "repr, std, cov, clossl, z, norm 0.04478132724761963 0.229736328125 3.3402657508850098 0.4005786180496216 1.2879937887191772 2.1635429859161377\n",
            "repr, std, cov, clossl, z, norm 0.029292959719896317 0.2308349609375 3.2981433868408203 0.4605409502983093 1.2740751504898071 1.8021148443222046\n",
            "repr, std, cov, clossl, z, norm 0.03841055557131767 0.22412109375 3.6301350593566895 0.5085026025772095 1.293576717376709 2.092989683151245\n",
            "repr, std, cov, clossl, z, norm 0.02933657541871071 0.2210693359375 3.804314613342285 0.41870516538619995 1.343421220779419 1.6218377351760864\n",
            "repr, std, cov, clossl, z, norm 0.03793816640973091 0.2293701171875 3.377295970916748 0.47742098569869995 1.2621781826019287 1.9240930080413818\n",
            "repr, std, cov, clossl, z, norm 0.031285952776670456 0.2401123046875 2.877171277999878 0.3724302649497986 1.2581950426101685 1.878142237663269\n",
            "repr, std, cov, clossl, z, norm 0.03870415687561035 0.2303466796875 3.3250019550323486 0.45123210549354553 1.2791039943695068 2.3636653423309326\n",
            "repr, std, cov, clossl, z, norm 0.027417577803134918 0.2257080078125 3.551722764968872 0.35597702860832214 1.2955002784729004 2.0038914680480957\n",
            "repr, std, cov, clossl, z, norm 0.03728378191590309 0.227294921875 3.4771690368652344 0.4645523726940155 1.319761037826538 2.1804027557373047\n",
            "train_data.data 20482\n",
            "dided\n",
            "time\n",
            "[14, 13, 14, 8, 14, 8, 3, 4, 8, 9, 3, 8, 5, 8, 4, 5, 4, 0, 13, 8, 4, 11, 8, 4, 8, 4, 4, 2, 0, 8, 3, 2, 8, 3, 0, 8, 11, 14, 14, 14, 14, 4, 3, 8, 11, 6, 3, 8, 8, 2, 11, 8, 8, 13, 5, 11, 8, 3, 14, 13, 8, 5, 14, 8, 13, 5, 8, 0, 6, 2, 8, 4, 0, 2, 14, 8, 8, 6, 8, 2, 14, 8, 8, 9, 4, 3, 3, 8, 8, 8, 3, 5, 0, 8, 8, 8, 9, 8, 5, 13, 3, 11, 4, 9, 5, 8, 9, 9, 3, 3, 2, 0, 4, 8, 8, 9, 13, 14, 14, 0, 0, 8, 8, 5, 8, 6, 8, 2, 8, 3, 8, 8, 5, 3, 8, 14, 14, 9, 3, 14, 5, 4, 0, 9, 2, 2, 8, 4, 3, 13, 8, 5, 6, 11, 13, 3, 8, 9, 4, 5, 14, 8, 8, 8, 8, 4, 2, 3, 3, 8, 0, 2, 8, 2, 3, 3, 14, 14, 2, 8, 8, 8, 0, 4, 11, 3, 0, 2, 4, 8, 8, 8, 8, 2, 3, 14, 14, 8, 2, 14, 8, 4, 14, 5, 2, 5, 14, 3, 0, 0, 3, 8, 14, 8, 3, 8, 8, 8, 14, 8, 8, 8, 4, 3, 2, 8, 14, 5, 4, 0, 0, 0, 3, 2, 8, 14, 5, 4, 8, 8, 14, 0, 14, 8, 8, 3, 6, 8, 2, 2, 10, 3, 0, 2, 8, 4, 4, 5, 4, 14, 8, 8, 5, 3, 0, 3, 8, 4, 11, 2, 8, 8, 8, 8, 8, 14, 14, 4, 8, 8, 3, 8, 3, 3, 4, 5, 8, 8, 0, 5, 4, 8, 8, 6, 13, 3, 5, 8, 14, 8, 9, 8, 4, 4, 8, 14, 5, 8, 13, 8, 14, 13, 7, 8, 2, 0, 0, 4, 5, 3, 4, 3, 8, 3, 2, 5, 8, 3, 14, 8, 4, 3, 2, 4, 14, 0, 3, 0, 4, 0, 6, 8, 9, 8, 8, 8, 2, 2, 0, 8, 3, 8, 3, 3, 14, 14, 2, 3, 13, 8, 5, 8, 8, 3, 8, 9, 11, 8, 8, 3, 13, 0, 3, 8, 3, 8, 2, 13, 2, 0, 4, 0, 8, 3, 4, 3, 3, 2, 14, 8, 0, 9, 3, 5, 8, 8, 8, 14, 4, 5, 10, 2, 2, 14, 8, 2, 14, 9, 5, 8, 8, 0, 8, 8, 8, 2, 8, 11, 8, 4, 2, 5, 3, 3, 8, 8, 8, 0, 4, 3, 11, 14, 4, 3, 3, 0, 6, 5, 4, 4, 4, 6, 4, 4, 3, 14, 14, 13, 4, 2, 14, 8, 0, 5, 11, 8, 3, 11, 8, 2, 13, 9, 14, 8, 5, 3, 8, 8, 9, 9, 4, 8, 8, 9, 8, 0, 3, 14, 13, 8, 11, 11, 5, 3, 5, 6, 8, 8, 2, 11, 8, 4, 13, 4, 3, 3, 6, 14, 3, 8, 8, 2, 4, 3, 14, 8, 11, 8, 4, 8, 14, 3, 8, 4, 9, 8, 5, 3, 13, 0, 3, 11, 5, 2, 2, 9, 3, 8, 5, 3, 8, 8, 8, 5, 8, 3, 4, 8, 8, 3, 8, 0, 3, 4, 3, 9, 13, 4, 14, 8, 2, 6, 4, 8, 9, 3, 3, 2, 2, 4, 2, 0, 14, 3, 14, 0, 8, 4, 2, 8, 3, 8, 8, 8, 4, 6, 11, 5, 3, 2, 14, 3, 3, 8, 3, 3, 8, 0, 3, 14, 3, 3, 8, 4, 3, 13, 4, 2, 3, 4, 9, 11, 2, 5, 11, 14, 8, 0, 0, 3, 4, 14, 14, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 14, 8, 8, 4, 3, 4, 8, 8, 9, 8, 8, 13, 13, 8, 14, 3, 2, 8, 2, 8, 3, 3, 11]\n",
            "dided\n",
            "time\n",
            "[9, 14, 5, 8, 0, 14, 5, 4, 4, 3, 8, 8, 8, 3, 8, 0, 9, 8, 3, 8, 8, 3, 8, 8, 13, 0, 2, 8, 9, 5, 14, 8, 2, 11, 14, 8, 8, 2, 8, 8, 14, 0, 2, 8, 8, 14, 0, 5, 8, 14, 5, 4, 2, 14, 0, 11, 3, 8, 8, 6, 0, 11, 3, 9, 3, 8, 9, 2, 8, 13, 4, 11, 3, 5, 8, 8, 8, 3, 0, 8, 14, 3, 8, 2, 6, 3, 13, 2, 8]\n",
            "dided\n",
            "time\n",
            "[8, 13, 4, 14, 2, 8, 11, 3, 2, 3, 8, 4, 3, 3, 3, 0, 0, 13, 2, 13, 2, 5, 8, 3, 14, 2, 3, 8, 2, 0, 12, 8, 4, 11, 3, 2, 8, 5, 8, 4, 9, 5]\n",
            "dided\n",
            "time\n",
            "[5, 2, 11, 3, 11, 14, 3, 3, 2, 3, 6, 11, 3, 8, 8, 8, 14, 3, 14, 14, 2, 8, 8, 8, 14, 2, 14, 2, 8, 3, 0, 3, 8, 2, 13, 4, 9, 14, 8, 2, 8, 3, 8, 14, 8, 4, 8, 3, 5, 0, 14, 5, 9, 8, 14, 8, 8, 0, 11, 14, 3, 4, 2, 8, 4, 5, 5, 10, 3, 3, 11, 8, 8, 8, 8, 13, 5, 14, 9, 1, 14, 3, 8, 2, 6, 0, 8, 8, 8, 8, 5, 4, 11, 3, 11, 8, 8, 8, 3, 8, 8, 13, 0, 3, 3, 8, 8, 0, 4, 8, 9, 8, 8, 4, 5, 14, 11, 5, 14, 8, 3, 8, 9, 4, 8, 9, 11, 2, 8, 9, 8, 8, 0, 2, 0, 0, 0, 14, 5, 14, 8, 8, 14, 3, 9, 0, 3, 8, 4, 11, 14, 5, 8, 8, 9, 14, 8, 0, 13, 8, 5, 4, 2, 3, 8, 0, 3, 0, 0, 0, 8, 8, 0, 0, 3, 8, 0, 2, 8, 9, 4, 8, 4, 8, 5, 14, 8, 14, 3, 3, 8, 5, 4, 8, 13, 0, 4, 2, 4, 2, 14, 0, 8, 14, 4, 0, 2, 8, 11, 0, 6, 5, 2, 8, 2, 8, 2, 3, 2, 8, 8, 13, 8, 3, 14, 14, 4, 14, 3, 0, 0, 14, 11, 8, 3, 0, 4, 3, 2, 3, 3, 8, 3, 2, 5, 2, 8, 3, 4, 8, 2, 2, 2, 8, 5, 8, 14, 2, 3, 5, 5, 8, 8, 2, 2, 14, 3, 3, 11, 3, 3, 4, 4, 13, 8, 11, 2, 6, 3, 14, 3, 9, 11, 3, 4, 8, 8, 3, 3, 14, 8, 8, 2, 11, 8, 4, 14, 8, 6, 8, 0, 11, 3, 14, 3, 8, 3, 8, 13, 0, 8, 8, 5, 6, 14, 4, 4, 3, 8, 0, 5, 14, 14, 14, 8, 2, 5, 14, 4, 9, 8, 8, 11, 14, 8, 8, 5, 4, 8, 8, 8, 8, 2, 4, 8, 14, 6, 0, 14, 9, 3, 8, 6, 3, 8, 2, 5, 8, 8, 8, 5, 14, 8, 8, 11, 2, 8, 0, 8, 3, 2, 14, 8, 8, 11, 8, 3, 3, 8, 2, 5, 4, 4, 8, 14, 0, 8, 13, 8, 2, 3, 2, 3, 13, 8, 2, 4, 5, 14, 0, 8, 8, 5, 8, 14, 2, 3, 8, 0, 5, 2, 8, 8, 8, 8, 4, 6, 3, 0, 8, 3, 3, 3, 4, 8, 4, 14, 8, 3, 3, 11, 8, 2, 0, 8, 0, 5, 8, 8, 14, 8, 3, 4, 14, 0, 8, 8, 4, 13, 2, 8, 9, 3, 8, 5, 4, 14, 4, 8, 0, 11, 14, 13, 8, 8, 6, 7, 4, 8, 8, 8, 8, 0, 3, 2, 8, 8, 3, 4, 14, 2, 8, 3, 8, 9, 6, 6, 14, 8, 3, 8, 3, 8, 14, 14, 8, 11, 11, 4, 8, 2, 5, 2, 8, 3, 14, 13, 8, 14, 14, 14, 2, 2, 13, 3, 4, 13, 14, 3, 5, 4, 9, 8, 0, 5, 4, 8, 3, 4, 3, 4, 3, 8, 2, 14, 3, 5, 2, 4, 8, 5, 2, 2, 5, 9, 0, 6, 13, 0, 2, 4, 3, 8, 4, 4, 4, 4, 9, 8, 0, 5, 3, 2, 8, 4, 8, 8, 3, 8, 14, 14, 3, 2, 8, 5, 3, 8, 4, 8, 8, 5, 8, 8, 3, 11, 4, 4, 3, 13, 0, 4, 8, 8, 8, 2, 4, 11, 2, 2, 8, 2, 8, 11, 8, 9, 3, 11, 8, 0, 8, 0, 14, 8, 8, 14, 8, 7, 2, 3, 8, 8, 0, 14, 5, 6, 4, 8, 3, 8, 4, 3, 11, 10, 8, 0, 8, 8, 8, 8, 8, 3, 5, 14, 4, 13, 8, 2, 14, 4, 8, 8, 8, 8, 8, 3, 4, 4, 11, 2, 8, 14, 8, 4, 0, 8, 14, 8, 4, 3, 14, 3, 8, 8, 4, 3, 0, 8, 3, 14, 4, 13, 0, 0]\n",
            "2 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02485060878098011 0.2252197265625 3.570621967315674 0.41615286469459534 1.2404062747955322 2.3992795944213867\n",
            "repr, std, cov, clossl, z, norm 0.034216366708278656 0.2286376953125 3.384977340698242 0.4774250388145447 1.2245571613311768 2.2174956798553467\n",
            "repr, std, cov, clossl, z, norm 0.026624483987689018 0.229248046875 3.360844612121582 0.5352165699005127 1.3333603143692017 2.3448619842529297\n",
            "repr, std, cov, clossl, z, norm 0.03183343634009361 0.2344970703125 3.1166858673095703 0.5451011657714844 1.242904782295227 2.495314121246338\n",
            "repr, std, cov, clossl, z, norm 0.02659958228468895 0.234375 3.123640298843384 0.4627368748188019 1.2942672967910767 1.9189338684082031\n",
            "repr, std, cov, clossl, z, norm 0.03252717852592468 0.2275390625 3.4733314514160156 0.4439626634120941 1.2708508968353271 1.9212883710861206\n",
            "repr, std, cov, clossl, z, norm 0.025751007720828056 0.2264404296875 3.5179672241210938 0.4248899221420288 1.2907782793045044 2.0556976795196533\n",
            "repr, std, cov, clossl, z, norm 0.03526955470442772 0.2283935546875 3.4046435356140137 0.45062950253486633 1.300011157989502 1.7587661743164062\n",
            "repr, std, cov, clossl, z, norm 0.024946225807070732 0.2294921875 3.3561367988586426 0.4758056700229645 1.2846043109893799 2.214874505996704\n",
            "repr, std, cov, clossl, z, norm 0.03324524313211441 0.229736328125 3.3520689010620117 0.44731205701828003 1.2911038398742676 2.0093228816986084\n",
            "repr, std, cov, clossl, z, norm 0.027985719963908195 0.229736328125 3.3436312675476074 0.40503257513046265 1.2876981496810913 2.3253231048583984\n",
            "repr, std, cov, clossl, z, norm 0.03943905979394913 0.225830078125 3.5496597290039062 0.47621434926986694 1.265323281288147 2.156860589981079\n",
            "train_data.data 20466\n",
            "dided\n",
            "time\n",
            "[0, 0, 2, 3, 8, 2, 3, 3, 8, 8, 8, 4, 3, 8, 3, 13, 2, 4, 2, 0, 9, 8, 3, 3, 9, 8, 8, 14, 14, 0, 3, 2, 4, 4, 6, 8, 2, 9, 3, 8, 9, 9, 12, 14, 0, 5, 3, 8, 2, 11, 2, 8, 0, 0, 4, 14, 4, 3, 3, 4, 6, 5, 1, 9, 9, 8, 5, 14, 4, 8, 3, 8, 8, 5, 2, 8, 9, 13, 4, 8, 3, 3, 8, 8, 4, 5, 8, 8, 9, 3, 4, 3, 9, 4, 8, 0, 9, 9, 8, 3, 0, 8, 3, 14, 4, 3, 2, 3, 9, 14, 8, 8, 6, 11, 10, 12, 8, 8, 5, 8, 9, 11, 11, 13, 9, 3, 5, 5, 6, 9, 12, 4, 8, 8, 3, 0, 0, 8, 4, 4, 9, 5, 4, 2, 8, 13, 0, 5, 6, 10, 12, 12, 9, 2, 5, 14, 9, 9, 12, 12, 3, 9, 9, 14, 9, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 0, 2, 6, 4, 12, 12, 6, 11, 12, 11, 9, 4, 14, 14, 3, 9, 10, 1, 6, 12, 1, 12, 9, 3, 13, 2, 4, 3, 5, 11, 9, 8, 14, 9, 9, 8, 2]\n",
            "dided\n",
            "time\n",
            "[3, 9, 14, 12, 9, 9, 4, 14, 4, 9, 3, 8, 4, 8, 5, 8, 9, 4, 8]\n",
            "dided\n",
            "time\n",
            "[4, 8, 8, 4, 9, 3, 8, 14, 9, 8, 13, 14, 6, 12, 1, 10, 8, 8, 0, 8, 6, 11, 12, 12, 4, 14, 11, 3, 9, 8, 2, 8, 8, 3, 8, 8, 8, 8, 2, 13, 9, 3, 8, 5, 2, 3, 5, 14, 8, 8, 3, 14, 2, 4, 8, 0, 3, 9, 8, 3, 9, 14, 8, 5, 8, 13, 3, 3, 14, 9, 8, 0, 3, 8, 8, 9, 8, 11, 8, 13, 5, 6, 3, 3, 8, 8, 9, 8, 4, 9, 8, 2, 8, 8, 11, 3, 6, 12, 10, 12, 8, 8, 5, 14, 9, 9, 11, 0, 8, 3, 4, 3, 6, 0, 9, 3, 8, 2, 2, 2, 9, 9, 12, 12, 6, 9, 12, 12, 9, 4, 3, 4, 4, 5, 14, 3, 9, 2, 11, 2, 11, 8, 3, 3, 4, 4, 8, 8, 0, 8, 3, 3, 14, 3, 14, 9, 9, 3, 8, 6, 6, 11, 12, 8, 0, 8, 0, 8, 14, 8, 3, 3, 13, 9, 9, 5, 3, 3, 2, 14, 9, 14, 3, 8, 9, 9, 8, 8, 8, 3]\n",
            "dided\n",
            "time\n",
            "[4, 3, 3, 14, 9, 9, 8, 2, 4, 8, 8, 8, 4, 8, 8, 2, 2, 14, 8, 8, 3, 3, 8, 8, 9, 4, 9, 4, 9, 14, 2, 5, 9, 2, 3, 5, 6, 11, 12, 12, 8, 3, 8, 0, 9, 14, 4, 8, 8, 13, 4, 8, 0, 8, 14, 8, 6, 1, 12, 10, 11, 3, 8, 6, 0, 3, 0, 3, 6, 4, 12, 12, 4, 8, 11, 14, 13, 14, 9, 8, 9, 3, 2, 3, 6, 9, 12, 12, 2, 0, 8, 0, 4, 8, 9, 8, 6, 1, 1, 10, 9, 4, 12, 12, 9, 6, 2, 3, 3, 3, 4, 0, 0, 4, 0, 8, 8, 3, 8, 8, 4, 11, 8, 8, 0, 3, 8, 8, 9, 3, 13, 9, 6, 9, 11, 4, 9, 3, 8, 3, 8, 11, 13, 4, 9, 8, 8, 14, 14, 13, 8, 4, 0, 13, 11, 2, 4, 2, 8, 4, 3, 8, 9, 4, 9, 11, 5, 11, 2, 8, 3, 9, 9, 9, 11, 6, 3, 3, 4, 2, 8, 14, 4, 8, 3, 9, 5, 14, 4, 3, 0, 14, 8, 2, 4, 3, 9, 6, 14, 3, 4, 8, 8, 14, 4, 4, 11, 2, 9, 14, 8, 3, 9, 8, 11, 6, 9, 9, 8]\n",
            "3 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.028400344774127007 0.2269287109375 3.486931085586548 0.43777284026145935 1.3294031620025635 2.2941055297851562\n",
            "repr, std, cov, clossl, z, norm 0.03544149175286293 0.2296142578125 3.352665901184082 0.46403688192367554 1.2812387943267822 2.236327648162842\n",
            "repr, std, cov, clossl, z, norm 0.030852526426315308 0.23046875 3.2988719940185547 0.4497179687023163 1.2915910482406616 2.0718281269073486\n",
            "repr, std, cov, clossl, z, norm 0.044823843985795975 0.2337646484375 3.142090320587158 0.45316165685653687 1.2850888967514038 1.9388556480407715\n",
            "repr, std, cov, clossl, z, norm 0.030817998573184013 0.22998046875 3.32828426361084 0.4037620425224304 1.2815935611724854 2.031543254852295\n",
            "repr, std, cov, clossl, z, norm 0.04572347179055214 0.2249755859375 3.5834121704101562 0.49382033944129944 1.3481073379516602 1.7467930316925049\n",
            "repr, std, cov, clossl, z, norm 0.03056401200592518 0.2252197265625 3.571194648742676 0.5417723655700684 1.2204015254974365 1.8776277303695679\n",
            "repr, std, cov, clossl, z, norm 0.04324404522776604 0.231201171875 3.266833782196045 0.4957757592201233 1.3077532052993774 1.773760199546814\n",
            "repr, std, cov, clossl, z, norm 0.030661718919873238 0.228515625 3.413473129272461 0.439273864030838 1.335067868232727 2.3735575675964355\n",
            "repr, std, cov, clossl, z, norm 0.03359704092144966 0.23046875 3.2964115142822266 0.4832916855812073 1.292873501777649 2.089559316635132\n",
            "repr, std, cov, clossl, z, norm 0.028415238484740257 0.2281494140625 3.4093284606933594 0.48129555583000183 1.286649465560913 2.1321864128112793\n",
            "repr, std, cov, clossl, z, norm 0.033155038952827454 0.2286376953125 3.383457660675049 0.4053097069263458 1.3547136783599854 1.5568451881408691\n",
            "train_data.data 20630\n",
            "dided\n",
            "time\n",
            "[9, 9, 8, 13, 11, 8, 3, 4, 3, 0, 8, 3, 8, 3, 4, 7, 3, 3, 2, 8, 3, 2, 11, 2, 14, 8, 9, 3, 4, 6, 13, 14, 0, 3, 3, 8, 8, 2, 11, 14, 8, 8, 3, 8, 14, 14, 6, 4, 8, 0, 4, 6, 3, 3, 4, 3, 11, 5, 2, 3, 3, 4, 6, 2, 8, 8, 8, 3, 8, 11, 13, 14, 8, 3, 3, 3, 3, 3, 8, 8, 2, 13, 3, 14, 3]\n",
            "dided\n",
            "time\n",
            "[8, 8, 9, 0, 8, 8, 8, 9, 4, 2, 13, 3, 14, 14, 9, 8, 4, 2, 3, 9, 8, 11, 14, 3, 8, 11, 3, 3, 8, 11, 8, 3, 8, 2, 0, 2, 13, 2, 4, 2, 8, 14, 8, 8, 14, 8, 4, 8, 14, 3, 4, 3, 2, 8, 8, 4, 2, 3, 3, 8, 8, 8, 14, 5, 2, 8, 2, 5, 14, 2, 11, 4, 8, 8, 3, 3, 8, 2, 4, 11, 9, 4, 8, 11, 2, 14, 0, 8, 3, 5, 14, 11, 8, 9, 3, 5, 13, 2, 5, 0, 8, 4, 3, 8, 8, 14, 0, 3, 8, 14, 3, 3, 4, 3, 11, 2, 4, 14, 2, 8, 2, 11, 3, 14, 4, 4, 11, 8, 8, 9, 11, 5, 3, 8, 3, 3, 3, 5, 8, 2, 14, 8, 14, 14, 4, 14, 5, 5, 2, 8, 13, 8, 4, 9, 14, 3, 3, 14, 3, 3, 5, 4, 8, 4, 10, 0, 4, 5, 14, 4, 11, 0, 3, 3, 5, 5, 13, 14, 4, 13, 2, 3, 0, 3, 14, 8, 13, 5, 7, 3, 4, 8, 3, 0, 3, 2, 14, 3, 3, 5, 2, 14, 8, 8, 3, 2, 8, 2, 2, 14, 6, 3, 0, 5, 3, 13, 2, 3, 9, 8, 4, 3, 4, 8, 9, 3, 8, 8, 8, 0, 14, 9, 14, 0, 8, 4, 8, 4, 0, 8, 8, 8, 14, 2, 2, 5, 13, 0, 13, 14, 8, 0, 5, 11, 3, 2, 4, 9, 4, 2, 3, 8, 8, 8, 4, 3, 8, 8, 8, 14, 3, 8, 14, 14, 3, 8, 4, 6, 8, 8, 8, 6, 13, 0, 8, 3, 3, 14, 8, 14, 3, 2, 0, 8, 8, 14, 4, 8, 0, 11, 0, 14, 4, 14, 8, 9, 9, 8, 3, 8, 11, 8, 4, 3, 9, 2, 13, 3, 5, 14, 5, 5, 0, 0, 13, 5, 2, 3, 5, 4, 2, 0, 5, 3, 11, 8, 13, 0, 3, 4, 5, 13, 8, 2, 4, 3, 11, 14, 14, 9, 2, 8, 3, 5, 3, 9, 8, 14, 11, 3, 3, 6, 6, 3, 9, 5, 8, 8, 11, 8, 3, 11, 8, 0, 2, 8, 8, 14, 3, 4, 5, 8, 4, 8, 6, 0, 0, 13, 9, 8, 8, 5, 8, 8, 5, 13, 9, 8, 8, 13, 8, 8, 3, 8, 4, 14, 2, 9, 8, 14, 13, 2, 8, 0, 4, 11, 14, 14, 5, 5, 2, 0, 4, 2, 0, 13, 8, 5, 2, 3, 3, 8, 14, 9, 11, 8, 4, 5, 3, 8, 8, 3, 4, 11, 8, 2, 4, 5, 3, 11, 14, 8, 2, 2, 0, 8, 2, 6, 0, 8, 14, 14, 0, 2, 9, 4, 8, 0, 4, 4, 14, 8, 8, 8, 11, 14, 2, 10, 3, 8, 8, 14, 9, 14, 8, 8, 8, 8, 7, 2, 4, 8, 5, 8, 8, 14, 3, 8, 3, 8, 5, 0, 3, 14, 13, 3, 3, 3, 5, 11, 8, 4, 8, 8, 6, 2, 5, 8, 3, 8, 5, 13, 8]\n",
            "dided\n",
            "time\n",
            "[13, 8, 8, 9, 8, 5, 4, 4, 13, 3, 3, 3, 8, 2, 8, 5, 9, 5, 9, 0, 3, 3, 8, 11, 8, 8, 3, 3, 5, 3, 4, 3, 8, 11, 2, 8, 4, 8, 13, 8, 3, 8, 3, 3, 7, 14, 3, 3, 8, 0, 3, 8, 14, 8, 0, 8, 8, 3, 6, 3, 14, 14, 3, 3, 3, 3, 0, 4, 14, 0, 8, 14, 9, 13, 9, 8, 5, 4, 8, 0, 8]\n",
            "dided\n",
            "time\n",
            "[14, 11, 9, 14, 4, 2, 4, 3, 8, 3, 11, 9, 3, 14, 0, 5, 8, 13, 8, 0, 14, 13, 3, 8, 4, 8, 14, 9, 5, 5, 9, 5, 4, 8, 11, 8, 0, 8, 2, 8, 13, 8, 8, 8, 8, 8, 8, 8, 11, 3, 2, 3, 4, 13, 8, 8, 9, 3, 9, 14, 8, 0, 5, 8, 3, 2, 2, 14, 2, 8, 9, 8, 2, 4, 8, 8, 14, 11, 14, 14, 8, 3, 3, 4, 0, 4, 8, 8, 13, 3, 14, 8, 8, 14, 8]\n",
            "dided\n",
            "time\n",
            "[14, 8, 8, 2, 8, 13, 3, 3, 2, 8, 3, 2, 2, 8, 2, 8, 6, 2, 0, 0, 8, 3, 4, 2, 3, 8, 3, 8, 2, 3, 4, 8, 3, 3, 8, 13, 5, 4, 8, 5]\n",
            "4 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.0311942957341671 0.2257080078125 3.5400681495666504 0.3710521161556244 1.313453197479248 2.2909963130950928\n",
            "repr, std, cov, clossl, z, norm 0.034628208726644516 0.228271484375 3.4176981449127197 0.47709402441978455 1.406011939048767 1.9091883897781372\n",
            "repr, std, cov, clossl, z, norm 0.026986509561538696 0.2349853515625 3.0763564109802246 0.4736819863319397 1.3321717977523804 1.5894745588302612\n",
            "repr, std, cov, clossl, z, norm 0.02898566424846649 0.23046875 3.2977192401885986 0.44847533106803894 1.3652222156524658 1.703845500946045\n",
            "repr, std, cov, clossl, z, norm 0.023403799161314964 0.2257080078125 3.5274717807769775 0.5070616602897644 1.333919644355774 1.4394172430038452\n",
            "repr, std, cov, clossl, z, norm 0.02746584638953209 0.2275390625 3.4510254859924316 0.4651363790035248 1.3729664087295532 1.8376718759536743\n",
            "repr, std, cov, clossl, z, norm 0.031055893748998642 0.2303466796875 3.312014579772949 0.3948083221912384 1.2820547819137573 1.4970263242721558\n",
            "repr, std, cov, clossl, z, norm 0.0325605683028698 0.229248046875 3.3555150032043457 0.5294002890586853 1.302607774734497 1.6181520223617554\n",
            "repr, std, cov, clossl, z, norm 0.029630431905388832 0.223876953125 3.6282968521118164 0.46388307213783264 1.3106380701065063 1.807602047920227\n",
            "repr, std, cov, clossl, z, norm 0.039077166467905045 0.2232666015625 3.691403865814209 0.4701252579689026 1.3213098049163818 1.6997398138046265\n",
            "repr, std, cov, clossl, z, norm 0.027829106897115707 0.2257080078125 3.5676097869873047 0.4553568363189697 1.3319454193115234 1.880733609199524\n",
            "repr, std, cov, clossl, z, norm 0.0342973954975605 0.2310791015625 3.2725796699523926 0.395526260137558 1.2677297592163086 2.1700873374938965\n",
            "train_data.data 20276\n",
            "dided\n",
            "time\n",
            "[4, 8, 5, 8, 14, 4, 8, 4, 2, 8, 8, 4, 12, 3, 2, 3, 8, 5, 8, 7, 8, 8, 8, 6, 14, 11, 8, 5, 8, 14, 8, 0, 14, 8, 9, 6, 2]\n",
            "dided\n",
            "time\n",
            "[5, 9, 8, 14, 0, 11, 4, 9, 8, 3, 8, 13, 0, 8, 13, 13, 11, 3, 9, 8, 9, 8, 0, 4, 11, 9, 4, 9, 0, 2, 6, 3, 9, 4, 8, 9, 4, 5, 8, 8, 8, 2, 4, 8, 14, 8, 12, 5, 3, 3, 3, 8, 8, 14, 3, 3, 2, 4, 8, 4, 8, 8, 3, 3, 14, 2, 9, 8, 2, 8, 3, 8, 4, 2, 8, 8, 3, 4, 3, 5, 8, 0, 2, 11, 8, 8, 8, 0, 14, 8, 14, 0, 3, 14, 3, 3, 0, 0, 4, 5, 0, 8, 8, 8, 8, 4, 9, 8, 3, 3, 4, 11, 9, 4, 4, 14, 8, 13, 4, 3, 2, 4, 4, 8, 4, 6, 8, 2, 14, 8, 3, 5, 8, 14, 5, 8, 5, 8, 0, 3, 5, 8, 3, 8, 0, 2, 5, 4, 0, 8, 14, 3, 2, 9, 3, 3, 3, 13, 8, 13, 8, 14, 8, 2, 9]\n",
            "dided\n",
            "time\n",
            "[2, 8, 14, 0, 3, 8, 11, 0, 2, 2, 3, 3, 3, 2, 8, 4, 3, 3, 0, 8, 8, 8, 3, 0, 5, 8, 8, 4, 9, 14, 4, 4, 8, 8, 3, 2, 13, 0, 8, 2, 3, 4, 8, 4, 5, 8, 14, 13, 5, 8, 8, 0, 8, 0, 4, 2, 9, 2, 0, 8, 3, 8, 3, 8, 8, 8, 3, 11, 0, 3, 14, 5, 4, 2, 0, 8, 4, 2, 0, 14, 3, 14, 3, 3, 3, 2, 9, 0, 3, 14, 2, 3, 8, 0, 8, 5, 3, 2, 5, 14, 3, 5, 8, 3, 8, 0, 8, 4, 8, 8, 0, 3, 8, 8, 2, 14, 14, 2, 3, 3, 0, 14, 8, 2, 3, 3, 5, 3, 4, 2, 8, 5, 3, 0, 5, 5, 0, 11, 3, 0, 13, 0, 14, 8, 4, 0, 11, 5, 7, 14, 8, 13, 13, 9, 4, 14, 8, 14, 3, 3, 5, 8, 3, 14, 3, 5, 4, 3, 2, 8, 8, 14, 3, 2, 8, 4, 0, 3, 8, 13, 8, 14, 8, 4, 4, 8, 2, 8, 7, 13, 3, 12, 14, 3, 8, 4, 3, 0, 13, 3, 8, 14, 8, 3, 8, 3, 2, 0, 6, 8, 3, 8, 8, 14, 8, 14, 8, 3, 9, 4, 3, 8, 3, 3, 3, 8, 8, 8, 14, 14, 14, 4, 8, 8, 8, 8, 11, 3, 4, 4, 11, 8, 2, 9, 8, 4, 3, 11, 8, 4, 4, 8, 8, 8, 3, 4, 11, 3, 8, 8, 4, 8, 8, 5, 4, 10, 3, 8, 2, 14, 3, 8, 3, 8, 3, 8, 0, 5, 14, 0, 5, 9, 3, 2, 8, 3, 4, 8, 9, 14, 8, 11, 2, 8, 13, 14, 4, 0, 3, 5, 13, 0, 2, 11, 8, 8, 3, 0, 3, 2, 8, 8, 2, 8, 8, 3, 4, 3, 2, 13, 3, 8, 8, 0, 4, 13, 3, 5, 9, 6, 8, 8, 8, 8, 8, 3, 2, 13, 3, 14, 2, 14, 5, 4, 3, 4, 3, 13, 3, 14, 3, 9, 2, 3, 8, 3, 8, 8, 2, 3, 8, 11, 13, 5, 10, 8, 8, 0, 11, 8, 3, 5, 8, 3, 0, 8, 2, 8, 11, 2, 4, 4, 8, 8, 4, 3, 2, 3, 2, 9, 4, 5, 0, 4, 3, 8, 8, 8, 3, 3, 2, 2, 11, 14, 3, 2, 4, 2, 8, 3, 8, 2, 0, 9, 8, 4, 11, 11, 4, 8, 4, 8, 3, 8, 14, 8, 4, 8, 5, 8, 4, 8, 3, 13, 11, 8, 13, 5, 14, 14, 4, 4, 8, 2, 0, 8, 4, 8, 8, 8, 3, 9, 14, 0, 2, 2, 8, 3, 8, 8, 5, 8, 9, 4, 8, 4, 9, 3, 0, 8, 8, 8, 3, 8, 3, 6, 3, 8, 3, 0, 11, 9, 14, 2, 4, 14, 11, 8, 14, 2, 13, 8, 2, 0, 3, 0, 8, 3, 9, 14, 7, 4, 11, 5, 2, 2, 8, 0, 8, 5, 10, 14, 5, 8, 3, 8, 9, 3, 8, 3, 8, 8, 8, 6, 8, 8, 8, 2, 4, 3, 14, 8, 4, 5, 14, 0, 3, 3, 14, 4, 14, 8, 3, 14, 2, 8, 14, 2, 8, 3, 5, 5, 8, 2, 4, 8, 9, 14, 5, 6, 2, 9, 3, 8, 8, 5, 8, 13, 8, 8, 5, 3, 14, 3, 5, 14, 3, 3, 8, 8, 3, 2, 2, 3, 9, 4, 8, 9, 14, 8, 0, 13, 2, 8, 8, 6, 14, 3, 3, 14, 8, 8, 2, 4, 8, 11, 3, 13, 3, 8, 0, 3, 11, 8, 8, 8, 5, 8, 8, 14, 8, 4, 14, 9, 14, 3, 4, 8, 10, 3, 9, 4, 2]\n",
            "dided\n",
            "time\n",
            "[8, 3, 14, 3, 4, 0, 4, 14, 5, 14, 3, 5, 2, 14, 8, 8, 8, 8, 3, 0, 14, 0, 14, 8, 4, 9, 8, 5, 2, 3, 8, 0, 13, 8, 8, 6, 3, 8, 8, 6, 14, 6, 14, 8, 2, 5, 5, 5, 3, 0, 9, 6, 8, 8, 8, 8, 0, 0, 8, 3, 3, 9, 4, 11, 5, 11, 8, 14, 2, 8, 14, 14, 2, 3, 8, 4, 6, 9, 4, 8, 3, 3, 14]\n",
            "dided\n",
            "time\n",
            "[3, 14, 11, 4, 5, 11, 4, 8, 5, 3, 3, 8, 5, 4, 8, 4, 11, 3, 8, 3, 8, 5, 8, 3, 8, 0, 11, 6, 8, 8, 8, 8, 3, 3, 13, 11, 5, 8, 8, 4, 8, 9, 8, 4, 5, 4, 2, 13, 9, 3, 8, 6, 4, 4, 2, 5, 8, 2, 8, 8]\n",
            "5 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.025653153657913208 0.23046875 3.3082075119018555 0.4155210852622986 1.3001651763916016 2.130786657333374\n",
            "repr, std, cov, clossl, z, norm 0.027127452194690704 0.2308349609375 3.2883830070495605 0.4032193124294281 1.3106248378753662 2.2478461265563965\n",
            "repr, std, cov, clossl, z, norm 0.02475205808877945 0.227783203125 3.451923370361328 0.41745004057884216 1.3748835325241089 2.457810401916504\n",
            "repr, std, cov, clossl, z, norm 0.02816041372716427 0.2264404296875 3.5183775424957275 0.410548597574234 1.294135332107544 2.136397123336792\n",
            "repr, std, cov, clossl, z, norm 0.029010510072112083 0.226318359375 3.509275436401367 0.43223094940185547 1.30239999294281 2.283470630645752\n",
            "repr, std, cov, clossl, z, norm 0.02462456002831459 0.228515625 3.3926775455474854 0.4781641364097595 1.2572433948516846 2.2518720626831055\n",
            "repr, std, cov, clossl, z, norm 0.025441214442253113 0.2296142578125 3.3522233963012695 0.4748712182044983 1.2501469850540161 2.338423013687134\n",
            "repr, std, cov, clossl, z, norm 0.03299768269062042 0.22900390625 3.38329815864563 0.5019852519035339 1.285364031791687 2.3010568618774414\n",
            "repr, std, cov, clossl, z, norm 0.0250054020434618 0.2305908203125 3.32021427154541 0.4333431124687195 1.302261471748352 2.5980048179626465\n",
            "repr, std, cov, clossl, z, norm 0.02971210703253746 0.2296142578125 3.3685755729675293 0.5492796301841736 1.3319751024246216 1.7825355529785156\n",
            "repr, std, cov, clossl, z, norm 0.025565313175320625 0.2314453125 3.272334337234497 0.4548315703868866 1.3447856903076172 2.063746452331543\n",
            "repr, std, cov, clossl, z, norm 0.03279685229063034 0.2313232421875 3.2709717750549316 0.4525006413459778 1.2465431690216064 1.9043481349945068\n",
            "train_data.data 20459\n",
            "dided\n",
            "time\n",
            "[2, 8, 8, 4, 3, 13, 2, 8, 14, 5, 3, 9, 3, 13, 3, 4, 9, 3, 4, 4, 2, 8, 3, 8, 14, 8, 3, 2, 3, 8, 5, 5, 3, 3, 8, 9, 0, 14, 2, 0, 8, 0, 0, 4, 8, 3, 3, 9, 8, 4, 11, 5, 8, 11, 3, 11, 9, 0, 0, 3, 2, 3, 0, 5, 8, 9, 8, 3, 3, 13, 3, 4, 5, 3, 6, 11, 8, 8, 13, 8]\n",
            "dided\n",
            "time\n",
            "[8, 13, 8, 4, 8, 3, 5, 8, 3, 8, 4, 8, 5, 2, 11, 8, 2, 2, 0, 8, 3, 8, 3, 0, 3, 14, 14, 5, 2, 6, 3, 5, 8, 13, 8, 4, 2, 8, 11, 11, 8, 8, 6, 3, 2, 8, 0, 2, 0, 11, 4, 11, 3, 8, 8, 8, 8, 3, 3, 5, 6, 2, 11, 4, 4, 8, 8, 14, 9, 2, 8, 11, 2, 2, 8, 8, 5, 0, 0, 8, 5, 4, 8, 4, 4, 0, 5, 3, 8, 3, 2, 8, 2, 13, 14, 9, 9, 2, 11, 11, 3, 8, 14, 2, 13, 8, 8, 14, 11, 8, 8, 6, 14, 0, 4, 0, 8, 14, 8, 10, 3, 9, 3, 9, 8, 8, 2, 3, 6, 2, 8, 2, 8, 5, 14, 14, 14, 4, 2, 3, 8, 2, 4, 5, 0]\n",
            "dided\n",
            "time\n",
            "[8, 3, 4, 12, 9, 2, 13, 3, 8, 13, 2, 2, 5, 8, 0, 3, 2, 11, 4, 0, 8, 3, 4, 0, 14, 3, 3, 9, 5, 14, 3, 3, 8, 8, 5, 2, 13, 5, 5, 5, 14, 3, 6, 4, 8, 8, 4, 9, 8, 9, 4, 14, 2, 11, 8, 8, 14, 8, 8, 3, 8, 9, 2, 3, 3, 3, 8, 4, 9, 11, 0, 13, 8]\n",
            "dided\n",
            "time\n",
            "[13, 3, 8, 9, 8, 14, 8, 2, 8, 0, 4, 8, 3, 3, 3, 5, 7, 8, 3, 2, 11, 5, 2, 8, 4, 13, 4, 14, 0, 3, 8, 3, 10, 3, 3, 4, 3, 14, 14, 3, 8, 8, 8, 2, 8, 0, 8, 4, 8, 4, 14, 8, 3, 5, 2, 2, 11, 9, 2, 2, 3, 8, 9, 8, 5, 14, 6, 8, 5, 4, 2, 4, 14, 8, 14, 14, 4, 8]\n",
            "dided\n",
            "time\n",
            "[8, 14, 8, 4, 8, 8, 8, 2, 9, 11, 3, 13, 5, 3, 8, 6, 8, 4, 9, 2, 14, 14, 2, 11, 4, 3, 4, 2, 2, 0, 4, 4, 6, 14, 0, 8, 13, 14, 8, 3, 5, 6, 8, 8, 14, 8, 8, 0, 8, 14, 8, 8, 5, 9, 8, 2, 8, 3, 3, 4, 4, 8, 5, 13, 8, 8, 12, 13, 8, 8, 14, 4, 8, 5, 9, 0, 8, 3, 9, 8, 5, 8, 2, 8, 14, 3, 5, 3, 8, 13, 3, 3, 2, 4, 8, 4, 8, 14, 14, 3, 3, 4, 14, 8, 2, 14, 3, 3, 4, 8, 8, 9, 8, 3, 9, 0, 8, 3, 8, 3, 4, 7, 8, 0, 8, 8, 5, 8, 8, 14, 14, 5, 2, 8, 13, 13, 13, 6, 8, 4, 14, 14, 8, 8, 8, 14, 3, 5, 9, 4, 8, 5, 4, 0, 4, 3, 14, 14, 8, 4, 4, 8, 3, 4, 5, 11, 9, 3, 3, 2, 3, 8, 2, 3, 0, 2, 8, 8, 8, 2, 3, 8, 14, 2, 5, 4, 8, 8, 8, 8, 8, 8, 8, 5, 4, 4, 9, 8, 5, 4, 8, 11, 9, 5, 4, 4, 3, 8, 14, 0, 3, 8, 0, 0, 3, 2, 8, 14, 8, 13, 2, 14, 8, 5, 11, 5, 8, 8, 4, 8, 3, 4, 3, 8, 8, 3, 4, 8, 3, 4, 11, 2, 3, 11, 4, 11, 8, 11, 4, 4, 9, 3, 4, 5, 8, 2, 3, 8, 0, 3, 4, 14, 3, 14, 8, 14, 4, 14, 2, 8, 4, 5, 5, 3, 13, 8, 0, 3, 4, 2, 8, 11, 3, 14, 14, 14, 3, 2, 11, 5, 3, 2, 3, 9, 9, 9, 8, 8, 3, 4, 8, 8, 14, 8, 4, 0, 3, 13, 0, 11, 5, 3, 9, 14, 0, 8, 6, 8, 3, 14, 4, 3, 3, 8, 6, 11, 8, 14, 2, 3, 8, 14, 9, 4, 0, 14, 4, 14, 8, 4, 13, 8, 5, 8, 13, 3, 3, 3, 6, 2, 8, 3, 14, 14, 8, 3, 8, 8, 8, 2, 0, 8, 0, 3, 8, 8, 8, 4, 8, 5, 3, 14, 2, 8, 8, 0, 3, 8, 2, 8, 5, 9, 8, 9, 3, 5, 0, 8, 3, 2, 8, 9, 8, 11, 3, 0, 0, 8, 9, 4, 4, 2, 2, 5, 4, 0, 3, 2, 11, 3, 3, 3, 4, 2, 8, 3, 5, 4, 14, 8, 3, 3, 8, 3, 2, 5, 2, 14, 2, 14, 4, 8, 4, 14, 4, 8, 3, 2, 13, 8, 3, 8, 0, 3, 14, 8, 3, 3, 8, 14, 3, 14, 4, 8, 11, 2, 13, 3, 8, 5, 0, 8, 11, 5, 0, 8, 14, 14, 3, 8, 8, 5, 11, 3, 0, 8, 14, 8, 3, 8, 5, 8, 5, 2, 3, 8, 14, 8, 4, 5, 8, 6, 9, 2, 8, 8, 3, 11, 3, 2, 14, 14, 4, 3, 8, 0, 8, 14, 14, 14, 1, 3, 11, 3, 2, 11, 8, 0, 8, 8, 0, 2, 3, 3, 3, 8, 0, 11, 5, 0, 3, 6, 2, 8, 3, 8, 8, 8, 6, 9, 8, 8, 8, 14, 3, 3, 13, 3, 8, 14, 8, 13, 8, 3, 8, 8, 3, 4, 8, 0, 8, 6, 8, 3, 2, 11, 11, 3, 8, 8, 4, 8, 0, 8, 14, 2, 5, 13, 3, 14, 8, 5, 14, 2, 8, 14, 8, 14, 0, 2, 13, 3, 8, 2, 2, 13, 8, 2, 6, 8, 8, 6, 4, 13, 8, 2, 3, 11, 3, 3, 10, 2, 9, 8, 2, 8, 0, 4, 8, 14, 5, 8, 2, 4, 3, 3, 8, 4, 11, 4, 4, 11, 14, 8, 2, 3, 9, 5, 8, 3, 3, 11, 3, 8, 4, 2, 11, 6, 2, 8, 9, 14, 3, 8, 3, 3, 8, 8, 8, 9, 3, 11, 5, 8, 3, 3, 8, 4, 3, 5, 3, 8, 8, 2, 2, 14, 3, 8, 8, 8, 4, 9, 0, 14, 9, 13, 5, 3, 5, 8, 8, 11, 8, 9, 14, 8, 3, 13, 3, 3, 8, 5, 14, 3, 0, 3, 14, 3, 8, 8, 9, 8, 8, 8, 2, 3, 13, 8, 3, 0, 3, 2, 5, 11, 8, 14, 4, 5, 11, 9, 8, 3, 5, 0, 2, 3, 11, 8, 11, 4, 3, 13, 4, 2, 4, 4, 14, 3, 0, 5, 13, 8, 8, 14, 8, 8, 8, 13, 8, 8, 3, 8, 14, 9, 4, 4, 8, 0, 2, 8, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 6, 5, 8, 8, 3, 3, 14, 4, 8, 3, 14, 8, 9, 11, 11, 8, 8, 8, 5, 14, 11, 8, 3, 8, 14, 14, 2, 8, 3, 9, 3, 2, 14, 3, 8, 8, 8, 4, 11, 4, 3, 3, 8, 3, 13, 3, 11, 0, 0]\n",
            "6 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02697022818028927 0.231689453125 3.2644808292388916 0.4925228953361511 1.359649419784546 2.1612114906311035\n",
            "repr, std, cov, clossl, z, norm 0.03276905044913292 0.2276611328125 3.460308074951172 0.5408653020858765 1.2971036434173584 2.255065679550171\n",
            "repr, std, cov, clossl, z, norm 0.03154568746685982 0.220947265625 3.826835870742798 0.45222553610801697 1.27215576171875 2.63155460357666\n",
            "repr, std, cov, clossl, z, norm 0.04745533689856529 0.2276611328125 3.449700355529785 0.47329533100128174 1.2379229068756104 1.8823297023773193\n",
            "repr, std, cov, clossl, z, norm 0.03267407417297363 0.2340087890625 3.144285202026367 0.521720826625824 1.204567313194275 1.8904998302459717\n",
            "repr, std, cov, clossl, z, norm 0.046236805617809296 0.233642578125 3.1448073387145996 0.4995023310184479 1.2746907472610474 2.156045436859131\n",
            "repr, std, cov, clossl, z, norm 0.027550216764211655 0.22900390625 3.3801212310791016 0.49986574053764343 1.3610988855361938 2.3447418212890625\n",
            "repr, std, cov, clossl, z, norm 0.037958987057209015 0.225341796875 3.580761671066284 0.427501380443573 1.252273678779602 1.7984979152679443\n",
            "repr, std, cov, clossl, z, norm 0.027440419420599937 0.22705078125 3.5003225803375244 0.4202691912651062 1.320556402206421 1.937322974205017\n",
            "repr, std, cov, clossl, z, norm 0.03774913400411606 0.2281494140625 3.426877975463867 0.4869043529033661 1.2336136102676392 2.47320818901062\n",
            "repr, std, cov, clossl, z, norm 0.02843860723078251 0.230224609375 3.336027145385742 0.44254642724990845 1.2718772888183594 2.1140024662017822\n",
            "repr, std, cov, clossl, z, norm 0.03367422893643379 0.23046875 3.3074960708618164 0.43532735109329224 1.3143543004989624 2.0123331546783447\n",
            "train_data.data 20472\n",
            "dided\n",
            "time\n",
            "[0, 11, 8, 5, 8, 8, 2, 11, 0, 6, 8, 11, 2, 8, 0, 5, 2, 14, 3, 8, 3, 4, 0, 0, 5, 3, 8, 4, 13, 14, 14, 8, 3, 4, 4, 8, 9, 14, 2, 8, 8, 5, 8, 3, 5, 4, 8, 4, 8, 13, 3, 14, 0, 11, 2, 8, 3, 6, 8, 0, 8, 8, 8, 8, 8, 8, 14, 8, 0, 14, 8, 6, 4, 8, 8, 5, 8, 3, 3, 3, 11, 0, 2, 8, 14, 5, 11, 9, 2, 6, 9, 2, 8, 4, 8, 5, 6, 8, 9, 8, 11, 2, 5, 0, 8, 13, 8, 9, 9, 4, 0, 14, 6, 14, 4, 0, 2, 8, 3, 11, 8, 3, 13, 14, 3, 2, 4, 4, 3, 8, 14, 8, 8, 8, 5, 8, 8, 9, 0, 0]\n",
            "dided\n",
            "time\n",
            "[9, 0, 0, 3, 8, 8, 8, 8, 8, 5, 3, 8, 8, 8, 8, 3, 0, 3, 3, 11, 3, 6, 11, 3, 13, 8, 3, 8, 8, 8, 8, 3, 8, 9, 5, 8, 8, 2, 2, 14, 8, 4, 8, 14, 8, 3, 5, 3, 13, 8, 8, 4, 3, 4, 8, 8, 3, 8, 2, 8, 8, 0, 8, 14, 13, 5, 0, 8, 8, 5, 8, 8, 14, 8, 0, 3, 3, 11, 0, 8, 3, 2, 13, 11, 6, 8, 3, 8, 0, 14, 8, 0, 0, 8, 2, 2, 0, 8, 9, 6, 0, 8, 9, 4, 3, 3, 8, 4, 8, 3, 14, 11, 3, 2, 8, 8, 6, 3, 3, 3, 2, 11, 14, 14, 11, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 4, 0, 8, 8, 3, 2, 6, 3, 13, 8, 4, 14, 13, 3, 0, 4, 0, 11, 8, 8, 3, 8, 3, 13, 4, 13, 8, 2, 0, 5, 4, 2, 3, 8, 8, 13, 8, 4, 6, 8, 14, 8, 8, 3, 3, 9, 4, 2, 4, 8, 2, 0, 11, 9, 8, 0, 3, 2, 8, 4, 0, 8, 8, 3, 4, 8, 8, 2, 8, 9, 5, 8, 3, 4, 8, 2, 2, 9, 8, 8, 13, 8, 8, 8, 4]\n",
            "dided\n",
            "time\n",
            "[8, 8, 4, 3, 6, 2, 13, 8, 8, 14, 2, 14, 2, 3, 8, 3, 0, 8, 3, 8, 2, 2, 6, 0, 3, 0, 8, 8, 3, 3, 5, 9, 11, 3, 8, 8, 8, 9, 4, 11, 8, 9, 5, 11, 8, 8, 11, 13, 4, 0, 8, 8, 2, 3, 3, 2, 14, 3, 14, 4, 8, 4, 2, 9, 3, 8, 0, 8, 9, 9, 8, 4, 8, 8, 2, 8, 0, 9, 3, 2, 3, 0, 5, 4, 9, 8, 2, 8, 2, 8, 8, 5, 8, 8, 5, 8, 0, 8, 8, 0, 13, 6, 3, 11, 3, 14, 8, 2, 3, 5, 2, 2, 9, 8, 13, 4, 3, 3, 4, 8, 8, 8, 3, 3, 3, 3, 3, 2, 0, 11, 3, 11, 8, 3, 8, 0, 5, 3, 2, 13, 4, 6, 4, 6, 11, 3, 8, 8, 9, 5, 2, 3, 5, 14, 8, 8, 14, 2, 8, 4, 11, 8, 5, 14, 2, 3, 8, 4, 2, 2, 3, 13, 11, 5, 4, 14, 8, 3, 0, 8, 2, 3, 2, 9, 8, 8, 2, 0, 11, 4, 2, 0, 3, 8, 3, 2, 4, 8, 0, 2, 9, 0, 8, 2, 9, 9, 3, 5, 8, 11, 0, 3, 8, 4, 3, 6]\n",
            "dided\n",
            "time\n",
            "[4, 3, 6, 13, 3, 13, 8, 8, 8, 11, 3, 5, 8, 8, 0, 0, 11, 9, 0, 3, 8, 6, 3, 13, 4, 9, 14, 11, 3, 8, 6, 3, 14, 8, 14, 3, 14, 8, 3, 9, 3, 8, 13, 8, 3, 3, 8, 2, 8, 2, 3, 9, 5, 8, 14, 14, 11, 4, 2, 11, 3, 13, 14, 8, 5, 0, 5, 14, 2, 8, 3, 13, 3, 11, 8, 0, 3, 5, 8, 3, 13]\n",
            "7 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02529161609709263 0.230224609375 3.293968677520752 0.4067527949810028 1.2936476469039917 2.290499687194824\n",
            "repr, std, cov, clossl, z, norm 0.02853996679186821 0.228271484375 3.407203197479248 0.48104020953178406 1.3312346935272217 2.292938470840454\n",
            "repr, std, cov, clossl, z, norm 0.026361674070358276 0.225341796875 3.5615830421447754 0.46963992714881897 1.3557754755020142 2.665540933609009\n",
            "repr, std, cov, clossl, z, norm 0.03563272953033447 0.2293701171875 3.354449987411499 0.4139714241027832 1.2849993705749512 2.190217971801758\n",
            "repr, std, cov, clossl, z, norm 0.032700929790735245 0.2301025390625 3.3230786323547363 0.46217742562294006 1.2461278438568115 2.0056896209716797\n",
            "repr, std, cov, clossl, z, norm 0.04110687971115112 0.22607421875 3.5243024826049805 0.4124694764614105 1.3766555786132812 2.0813512802124023\n",
            "repr, std, cov, clossl, z, norm 0.03246361017227173 0.231689453125 3.268521547317505 0.44153982400894165 1.2649328708648682 2.123563766479492\n",
            "repr, std, cov, clossl, z, norm 0.042338915169239044 0.23193359375 3.2406258583068848 0.40385377407073975 1.265582799911499 2.4556877613067627\n",
            "repr, std, cov, clossl, z, norm 0.0317966602742672 0.227294921875 3.471864700317383 0.43255615234375 1.3836562633514404 2.122664451599121\n",
            "repr, std, cov, clossl, z, norm 0.043594684451818466 0.2293701171875 3.3553714752197266 0.46285712718963623 1.3038065433502197 1.827567219734192\n",
            "repr, std, cov, clossl, z, norm 0.028892196714878082 0.2254638671875 3.5413172245025635 0.44284680485725403 1.2382973432540894 2.1728177070617676\n",
            "repr, std, cov, clossl, z, norm 0.03858564794063568 0.2291259765625 3.35714054107666 0.48350459337234497 1.2579731941223145 1.9115179777145386\n",
            "train_data.data 20437\n",
            "dided\n",
            "time\n",
            "[13, 5, 3, 11, 2, 3, 3, 4, 8, 14, 8, 5, 2, 5, 3, 3, 2, 8, 8, 5, 8, 5, 5, 0, 9, 0, 0, 14, 8, 11, 5, 5, 3, 6, 2, 8, 3, 8, 3, 8, 5, 14, 8, 0, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 8, 11, 8, 0, 8, 2, 5, 8, 8, 0, 11, 3, 3, 0, 4, 0, 4, 14, 5, 8, 8, 4, 8, 8, 0, 8, 11, 9, 14, 3, 8, 0, 0, 5, 13, 3, 5, 4, 9, 3, 13]\n",
            "dided\n",
            "time\n",
            "[9, 3, 13, 8, 8, 2, 14, 4, 2, 8, 8, 4, 8, 8, 9, 8, 14, 0, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 11, 4, 14, 8, 5, 8, 3, 8, 3, 11, 8, 8, 3, 13, 11, 3, 4, 2, 3, 14, 9, 14, 5, 8, 0, 5, 14, 8, 8, 14, 8, 8, 3, 9]\n",
            "dided\n",
            "time\n",
            "[3, 9, 14, 2, 3, 8, 2, 0, 14, 8, 13, 3, 8, 9, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 13, 4, 9, 3, 8, 8, 3, 14, 13, 3, 11, 3, 0, 8, 8, 11, 2, 2, 8, 8, 0, 13, 8, 4, 8, 0, 4, 8, 0, 14, 2, 3, 2, 8, 4, 14, 3, 8, 4, 3, 8, 8, 4, 3, 8, 8, 2, 8, 14, 5, 9, 3, 11, 14, 6, 4, 8, 14, 2, 4, 8, 8, 8, 14, 2, 8, 8, 3, 2, 3, 4, 3, 13, 8, 13, 14, 8, 3, 9, 4, 3]\n",
            "8 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.027535337954759598 0.2318115234375 3.253821849822998 0.4935227334499359 1.3559530973434448 2.0962300300598145\n",
            "repr, std, cov, clossl, z, norm 0.03542451933026314 0.231689453125 3.25034499168396 0.40910160541534424 1.2663933038711548 2.156846046447754\n",
            "repr, std, cov, clossl, z, norm 0.027811046689748764 0.228271484375 3.4312307834625244 0.4875829815864563 1.3317739963531494 2.032907009124756\n",
            "repr, std, cov, clossl, z, norm 0.03236374631524086 0.228271484375 3.422123432159424 0.39517977833747864 1.3277982473373413 2.3153598308563232\n",
            "repr, std, cov, clossl, z, norm 0.024814920499920845 0.2315673828125 3.247011184692383 0.3973684012889862 1.3370606899261475 2.2721199989318848\n",
            "repr, std, cov, clossl, z, norm 0.03095058910548687 0.228759765625 3.39389705657959 0.42764031887054443 1.3120304346084595 1.8533393144607544\n",
            "repr, std, cov, clossl, z, norm 0.026144327595829964 0.228515625 3.3976991176605225 0.3933686912059784 1.2324497699737549 1.657750129699707\n",
            "repr, std, cov, clossl, z, norm 0.029161935672163963 0.226806640625 3.4892876148223877 0.45284825563430786 1.2874681949615479 2.031932830810547\n",
            "repr, std, cov, clossl, z, norm 0.027773873880505562 0.2294921875 3.3525028228759766 0.4065997302532196 1.3440889120101929 2.0134143829345703\n",
            "repr, std, cov, clossl, z, norm 0.03323023393750191 0.2320556640625 3.2193610668182373 0.3845211863517761 1.2537084817886353 1.7776256799697876\n",
            "repr, std, cov, clossl, z, norm 0.02911260724067688 0.22998046875 3.3366453647613525 0.35964760184288025 1.2630667686462402 1.452840805053711\n",
            "repr, std, cov, clossl, z, norm 0.039123669266700745 0.2298583984375 3.322871685028076 0.4407927393913269 1.2950830459594727 1.7353590726852417\n",
            "train_data.data 20367\n",
            "dided\n",
            "time\n",
            "[3, 3, 8, 9, 13, 0, 2, 14, 8, 5, 8, 4, 2, 0, 14, 4, 6, 3, 4, 8, 8, 8, 4, 4, 3, 3, 0, 3, 3, 11, 9, 2, 8]\n",
            "dided\n",
            "time\n",
            "[4, 11, 2, 10, 8, 2, 0, 2, 8, 9, 0, 8, 11, 14, 0, 8, 2, 3, 13, 8, 3, 0, 3, 9, 4, 8, 9, 3, 2, 8, 3, 4, 5, 8, 11, 4, 14, 8, 13, 13, 11, 8, 3, 8, 13, 8, 8, 5, 3, 8, 2, 9, 8, 7, 11, 3, 8]\n",
            "dided\n",
            "time\n",
            "[5, 5, 2, 14, 9, 8, 9, 13, 0, 5, 8, 14, 5, 0, 11, 2, 14, 0, 8, 11, 8, 4, 8, 5, 6, 8, 2, 3, 8, 14, 5, 0, 3, 2, 4, 8, 5, 4, 4, 3, 8, 8, 5, 8, 3, 0, 3, 4, 4, 3, 11, 8, 2, 13, 3, 8, 3, 2, 13, 4, 13, 3, 2, 8, 4, 9, 8, 3, 9, 8, 11, 3, 8, 8, 11, 3, 8, 3, 3, 5, 3, 4, 5, 8, 4, 3, 11, 2, 8, 8, 8, 5, 4, 3, 3, 5, 0, 8, 13, 8, 3, 0, 8, 14, 9, 11, 9, 4, 8, 8, 8, 4, 9, 8, 6, 14, 2, 8, 0, 9]\n",
            "dided\n",
            "time\n",
            "[8, 0, 9, 13, 3, 4, 2, 3, 3, 8, 14, 14, 3, 4, 0, 2, 4, 8, 8, 9, 14, 2, 4, 2, 11, 8, 5, 3, 14, 8, 8, 6, 9, 11, 3, 14, 8, 14, 6, 8, 5, 2, 0, 9, 13, 11, 5, 9, 11, 13, 9, 13, 2, 3, 13, 8, 8, 8, 6, 8, 13, 4, 5, 13, 8, 4, 3, 9, 2, 5, 8, 0, 5, 8, 8, 13, 8, 3]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 4, 14, 14, 5, 13, 11, 3, 14, 8, 14, 5, 5, 2, 3, 4, 3, 4, 2, 2, 5, 11, 5, 9, 8, 2, 13, 0, 0, 8, 8, 2, 9, 9, 5, 14, 3, 8, 0, 4, 8, 8, 3, 4, 11, 3, 2, 8, 0, 4, 3, 13, 4, 5, 8, 3, 2, 2, 0, 2, 14, 8, 11, 5, 8, 2, 4, 3, 3, 9, 8, 4, 8, 3, 0, 4, 4, 0, 3, 3, 4, 4]\n",
            "9 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.03285463899374008 0.225341796875 3.548342227935791 0.4537407457828522 1.2645691633224487 1.8226011991500854\n",
            "repr, std, cov, clossl, z, norm 0.05688956007361412 0.2279052734375 3.417818546295166 0.6210587024688721 1.302134394645691 1.9298256635665894\n",
            "repr, std, cov, clossl, z, norm 0.050459764897823334 0.228515625 3.4082279205322266 0.5347555875778198 1.3444961309432983 1.505632996559143\n",
            "repr, std, cov, clossl, z, norm 0.06199505180120468 0.2276611328125 3.442476749420166 0.556046724319458 1.3320510387420654 1.767141342163086\n",
            "repr, std, cov, clossl, z, norm 0.034959398210048676 0.229736328125 3.3292887210845947 0.4771645665168762 1.2901837825775146 1.8243484497070312\n",
            "repr, std, cov, clossl, z, norm 0.03577859327197075 0.2291259765625 3.373295307159424 0.5231065154075623 1.2766019105911255 1.4206011295318604\n",
            "repr, std, cov, clossl, z, norm 0.023236390203237534 0.2264404296875 3.50321626663208 0.43756356835365295 1.3103573322296143 2.142808437347412\n",
            "repr, std, cov, clossl, z, norm 0.02874409779906273 0.2242431640625 3.6166112422943115 0.40265458822250366 1.3412151336669922 1.7538363933563232\n",
            "repr, std, cov, clossl, z, norm 0.02087746560573578 0.229248046875 3.3692805767059326 0.4540559649467468 1.3028069734573364 1.6774609088897705\n",
            "repr, std, cov, clossl, z, norm 0.02320466935634613 0.234130859375 3.149522542953491 0.40547430515289307 1.3552206754684448 1.6417490243911743\n",
            "repr, std, cov, clossl, z, norm 0.018481500446796417 0.228759765625 3.4026637077331543 0.37409916520118713 1.3152979612350464 1.2702903747558594\n",
            "repr, std, cov, clossl, z, norm 0.020538538694381714 0.2281494140625 3.419999599456787 0.44452807307243347 1.4025516510009766 1.5230441093444824\n",
            "train_data.data 20438\n",
            "dided\n",
            "time\n",
            "[3, 4, 4, 3, 0, 8, 3, 14, 0, 3, 3, 2, 3, 3, 8, 4, 3, 3, 8, 9, 2, 4, 2, 14, 14, 3, 14, 2, 11, 0, 8, 13, 3, 5, 4, 8, 8, 8, 4, 13, 3, 4, 8, 8, 3, 9, 3]\n",
            "dided\n",
            "time\n",
            "[9, 3, 3, 8, 13, 3, 5, 9, 0, 13, 8, 14, 3, 2, 3, 8, 3, 4, 9, 8, 13, 9, 2, 2, 8, 4, 4, 14, 8, 8, 4, 2, 4, 8, 8, 14, 14, 8, 8, 8, 5, 8, 2, 3, 5, 8, 1, 2, 3, 0, 5, 8, 2, 13, 3, 3, 4, 3, 8, 3, 11, 8, 0, 8, 4, 3, 8, 0, 5, 2, 4, 3, 8, 2, 3, 8, 13, 2, 7, 8, 4, 14, 8, 8, 3, 5, 3, 3, 13, 13, 8, 0, 9, 8, 5, 14, 4, 8, 8, 8, 8, 5, 8, 2, 14, 8, 5, 5, 11, 8, 13, 8, 11, 8, 11, 0, 5, 4, 11]\n",
            "dided\n",
            "time\n",
            "[4, 11, 3, 3, 11, 3, 4, 8, 5, 14, 5, 3, 8, 14, 2, 2, 13, 3, 9, 8, 2, 14, 13, 4, 8, 2, 0, 3, 14, 3, 8, 3, 5, 13, 14, 13, 8, 14, 8, 2, 14, 8, 8, 8, 4, 8, 5, 2, 0, 14, 8, 13, 3, 8, 8, 2, 8, 4, 8, 8, 8, 3, 8, 8, 8, 8, 9, 8, 13, 14, 4, 9, 2, 14, 3, 5, 0, 2, 14, 4, 8, 8, 2, 8, 2, 3, 8, 9, 13, 9, 3, 9, 14, 0, 8, 5, 8, 5, 11, 2, 5, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 14, 8, 3, 2, 3, 8, 5, 4, 8, 6, 14, 13, 3, 8, 3, 5, 5, 8, 3, 14, 2, 0, 8]\n",
            "dided\n",
            "time\n",
            "[2, 6, 8, 8, 0, 9, 8, 8, 8, 9, 4, 8, 2, 3, 8, 6, 9, 5, 11, 0, 14, 8, 8, 4, 11, 8]\n",
            "10 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.019288092851638794 0.228515625 3.42250919342041 0.40786629915237427 1.277739405632019 1.8817790746688843\n",
            "repr, std, cov, clossl, z, norm 0.025751683861017227 0.229736328125 3.3578295707702637 0.39026567339897156 1.2673442363739014 1.0805763006210327\n",
            "repr, std, cov, clossl, z, norm 0.024631526321172714 0.232177734375 3.224140167236328 0.41401633620262146 1.287062168121338 1.5086588859558105\n",
            "repr, std, cov, clossl, z, norm 0.030223293229937553 0.2347412109375 3.095271110534668 0.42309099435806274 1.2339106798171997 1.9256068468093872\n",
            "repr, std, cov, clossl, z, norm 0.026220722123980522 0.230712890625 3.295884370803833 0.5028176307678223 1.2349964380264282 1.805087924003601\n",
            "repr, std, cov, clossl, z, norm 0.0392114594578743 0.2247314453125 3.580421209335327 0.41025614738464355 1.3073540925979614 1.8063111305236816\n",
            "repr, std, cov, clossl, z, norm 0.027116145938634872 0.2220458984375 3.7339706420898438 0.46508556604385376 1.2537026405334473 1.8443785905838013\n",
            "repr, std, cov, clossl, z, norm 0.037259720265865326 0.226318359375 3.505466938018799 0.49323156476020813 1.2690428495407104 2.08884859085083\n",
            "repr, std, cov, clossl, z, norm 0.03228272497653961 0.2305908203125 3.3099758625030518 0.5064743161201477 1.3354697227478027 2.0891852378845215\n",
            "repr, std, cov, clossl, z, norm 0.036937348544597626 0.2371826171875 3.0199060440063477 0.5306951403617859 1.3403384685516357 2.1062183380126953\n",
            "repr, std, cov, clossl, z, norm 0.028434786945581436 0.2337646484375 3.16462779045105 0.48779428005218506 1.2838062047958374 2.0845956802368164\n",
            "repr, std, cov, clossl, z, norm 0.03737659007310867 0.228271484375 3.4386417865753174 0.37281301617622375 1.313633680343628 2.2090351581573486\n",
            "train_data.data 20887\n",
            "dided\n",
            "time\n",
            "[8, 13, 2, 8, 0, 5, 5, 14, 8, 8, 6, 2, 14, 9, 2, 8, 8, 11, 4, 8, 14, 14, 14, 2, 8, 3, 9, 9, 8, 8, 8, 14, 3, 3, 8, 2, 8, 8, 5, 8, 0, 13, 2, 3, 3, 8, 3, 3, 4, 14, 3, 8, 14, 9, 4, 3, 6, 4, 2, 8, 6, 0, 0, 4, 0, 4, 4, 14, 8, 14, 8, 0, 11, 4, 8, 3, 4, 3, 8, 8, 5, 11]\n",
            "dided\n",
            "time\n",
            "[11, 3, 11, 2, 0, 4, 3, 0, 14, 3, 3, 8, 5, 8, 3, 3, 8, 3, 14, 11, 3, 5, 4, 8, 13, 3, 14, 14, 2, 11, 5, 3, 3, 3, 0, 13, 0, 8, 9, 2, 4, 0, 3, 8, 5, 8, 3, 8, 3, 8, 5, 14, 14, 4, 14, 5, 8, 3, 5, 3, 8, 0, 0, 3, 14, 3, 0, 11, 8, 2, 8]\n",
            "dided\n",
            "time\n",
            "[2, 8, 0, 5, 3, 8, 3, 5, 6, 13, 3, 3, 4, 13, 8, 8, 9, 8, 8, 5, 8, 8, 3, 8, 8, 8, 3, 3, 2, 14, 8, 4, 13, 3, 3, 8, 8, 8, 0, 3, 0, 8, 3, 8, 2, 2, 8, 11, 6, 13, 2, 3, 5, 4, 4, 8, 9, 13, 2, 14, 2, 11, 8]\n",
            "dided\n",
            "time\n",
            "[11, 8, 14, 4, 5, 3, 5, 9, 8, 5, 11, 13, 8, 3, 3, 3, 3, 8, 11, 3, 8, 5, 13, 8, 8, 8, 8, 11, 3, 13, 8, 11, 5, 8, 8, 8, 3, 2, 8, 8, 5, 8, 11, 8, 13, 8, 2, 0, 3, 8, 14, 8, 6, 3, 14, 8, 5, 8, 5]\n",
            "dided\n",
            "time\n",
            "[8, 5, 8, 8, 13, 5, 2, 5, 8, 2, 9, 2, 2, 8, 3, 4, 8, 3, 3, 8, 3, 11, 3, 14, 4, 5, 11, 8, 3, 8, 13, 4, 14, 0, 14, 0, 3, 3, 6, 3, 9, 5, 13, 8, 3, 3, 4, 5, 2, 8, 3, 3, 14, 8, 3, 14, 13, 0, 3, 9, 4, 6, 3, 13, 14, 3, 8, 2, 0, 11, 8, 4, 5, 8, 11, 8, 3, 5, 11, 0, 3, 0, 13, 4, 3, 5, 14, 8, 14, 13, 8, 13, 14, 8]\n",
            "11 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.025879941880702972 0.226318359375 3.5534629821777344 0.44258517026901245 1.3106061220169067 1.8573557138442993\n",
            "repr, std, cov, clossl, z, norm 0.03880420699715614 0.2293701171875 3.3777365684509277 0.45318588614463806 1.2641345262527466 1.6532984972000122\n",
            "repr, std, cov, clossl, z, norm 0.026705309748649597 0.22705078125 3.4821314811706543 0.43028679490089417 1.2184116840362549 2.088620185852051\n",
            "repr, std, cov, clossl, z, norm 0.03282112628221512 0.2257080078125 3.5484251976013184 0.41128799319267273 1.2395641803741455 2.193774938583374\n",
            "repr, std, cov, clossl, z, norm 0.02338799461722374 0.2340087890625 3.127133369445801 0.4133242964744568 1.3184369802474976 2.397143840789795\n",
            "repr, std, cov, clossl, z, norm 0.03119928576052189 0.2310791015625 3.263500213623047 0.4401763677597046 1.3142106533050537 2.065401554107666\n",
            "repr, std, cov, clossl, z, norm 0.022632351145148277 0.2291259765625 3.3516180515289307 0.42313435673713684 1.3154141902923584 2.2273969650268555\n",
            "repr, std, cov, clossl, z, norm 0.03159492090344429 0.2266845703125 3.5005102157592773 0.43147698044776917 1.3123064041137695 1.9186749458312988\n",
            "repr, std, cov, clossl, z, norm 0.033055905252695084 0.2257080078125 3.560577154159546 0.45142820477485657 1.2710825204849243 2.2102763652801514\n",
            "repr, std, cov, clossl, z, norm 0.034277692437171936 0.226318359375 3.497537612915039 0.5019078850746155 1.2720979452133179 2.123542308807373\n",
            "repr, std, cov, clossl, z, norm 0.031199177727103233 0.22705078125 3.485637664794922 0.4231764078140259 1.2453856468200684 2.3008506298065186\n",
            "repr, std, cov, clossl, z, norm 0.03920168802142143 0.23095703125 3.272953987121582 0.5411730408668518 1.2210584878921509 2.0102438926696777\n",
            "train_data.data 20358\n",
            "dided\n",
            "time\n",
            "[8, 14, 3, 8, 8, 14, 8, 9, 0, 2, 3, 9, 4, 8, 3, 3, 3, 8, 8, 8, 3, 8, 9, 8, 3, 14, 0, 0, 3, 5, 2, 8, 9, 14, 11, 2, 4, 5, 1, 4, 13, 2, 0, 8, 4, 2, 8, 8, 8, 8, 14, 3, 5, 2, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[10, 3, 8, 8, 2, 0, 3, 3, 4, 3, 5, 3, 3, 8, 14, 2, 8, 8, 3, 6, 3, 6, 5, 2, 11, 3, 3, 8, 5, 3, 5, 2, 4, 13, 9, 3, 8, 8, 8, 3, 14, 9, 8, 0, 3, 2, 3, 0, 4, 8, 5, 13, 8, 8, 8, 0, 0, 8, 2, 8, 2, 3, 11, 8, 8, 8, 8, 5, 4, 0, 2, 3]\n",
            "dided\n",
            "time\n",
            "[0, 2, 3, 13, 14, 3, 11, 0, 2, 2, 3, 11, 2, 13, 3, 3, 5, 0, 5, 8, 4, 8, 8, 14, 14, 8, 8, 3, 0, 0, 2, 3, 8, 13, 3, 5, 9, 4, 13, 8, 4, 9, 4, 3, 9, 2, 3, 4, 0, 9, 4, 14, 2, 3, 3, 5, 2, 2, 4, 2, 4, 3, 5, 14, 8, 4, 4, 2, 14, 11, 14, 8, 3, 11, 14, 3, 3, 11, 14, 8, 2, 4, 0, 3, 8, 8, 3, 7]\n",
            "dided\n",
            "time\n",
            "[8, 3, 7, 14, 11, 2, 8, 3, 13, 9, 2, 8, 8, 3, 13, 8, 9, 8, 8, 8, 8, 3, 8, 0, 9, 10, 11, 4, 0, 4, 8, 3, 8, 11, 8, 8, 8, 0, 8, 8, 3, 13, 8, 8, 2, 2, 0, 8, 3, 9, 14, 0, 8, 8, 8, 8, 2, 8, 8, 3, 8, 11, 11, 3, 8, 9, 14, 2, 3, 4, 8, 13, 0, 13, 13, 8, 5, 11, 0, 3, 3, 5, 2, 8, 4, 8, 8, 14, 8, 2, 8, 5, 4, 3]\n",
            "dided\n",
            "time\n",
            "[3, 5, 8, 8, 3, 8, 8, 13, 11, 9, 3, 8, 8, 3, 3, 13, 2, 5, 0, 4, 8, 13, 8, 5, 4, 3, 9, 0, 8, 3, 14, 2, 14, 0, 2, 3, 8, 8, 2, 14, 5, 11, 0, 14, 8, 4, 6]\n",
            "12 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.03225032612681389 0.228759765625 3.405928611755371 0.43773147463798523 1.2447561025619507 2.1359410285949707\n",
            "repr, std, cov, clossl, z, norm 0.03105231188237667 0.2281494140625 3.409276008605957 0.5922204256057739 1.2769789695739746 1.9330388307571411\n",
            "repr, std, cov, clossl, z, norm 0.029794203117489815 0.22998046875 3.311598300933838 0.35890060663223267 1.2261682748794556 1.920636773109436\n",
            "repr, std, cov, clossl, z, norm 0.02655383199453354 0.2333984375 3.1672844886779785 0.5363924503326416 1.3083597421646118 2.0056028366088867\n",
            "repr, std, cov, clossl, z, norm 0.025296581909060478 0.2291259765625 3.3878302574157715 0.4147712290287018 1.2600289583206177 2.264611005783081\n",
            "repr, std, cov, clossl, z, norm 0.02273322083055973 0.2301025390625 3.329771041870117 0.4610450565814972 1.2969169616699219 2.173818588256836\n",
            "repr, std, cov, clossl, z, norm 0.024185264483094215 0.2333984375 3.1608011722564697 0.46052029728889465 1.268160343170166 2.191113233566284\n",
            "repr, std, cov, clossl, z, norm 0.026441466063261032 0.2301025390625 3.312941074371338 0.41941896080970764 1.3000435829162598 2.1260852813720703\n",
            "repr, std, cov, clossl, z, norm 0.022723941132426262 0.2249755859375 3.5804691314697266 0.4905291199684143 1.2317447662353516 2.0518798828125\n",
            "repr, std, cov, clossl, z, norm 0.024076083675026894 0.2232666015625 3.6644861698150635 0.48506179451942444 1.265976905822754 2.2575573921203613\n",
            "repr, std, cov, clossl, z, norm 0.0208742655813694 0.2293701171875 3.3530988693237305 0.4145384728908539 1.2544831037521362 1.9956964254379272\n",
            "repr, std, cov, clossl, z, norm 0.026194920763373375 0.2291259765625 3.3827171325683594 0.4584912955760956 1.2787845134735107 2.473085641860962\n",
            "train_data.data 20411\n",
            "dided\n",
            "time\n",
            "[4, 6, 3, 3, 5, 5, 14, 0, 0, 8, 3, 11, 3, 8, 11, 5, 8, 5, 0, 6, 0, 14, 8, 8, 14, 3, 6, 8, 4, 3, 14, 8, 8, 3, 14, 8, 14, 11, 5, 3, 3, 3, 8, 4, 3, 3, 3, 0, 6, 8, 13, 3, 2, 8, 8, 3, 9, 8, 5, 2]\n",
            "dided\n",
            "time\n",
            "[8, 5, 2, 3, 4, 8, 5, 14, 5, 9, 3, 3, 8, 5, 4, 8, 8, 4, 2, 5, 13, 2, 8, 14, 11, 2, 11, 0, 13, 9, 3, 3, 8, 4, 3, 5, 5, 3, 8, 4, 2, 14, 8, 9, 14, 2, 3, 8, 0, 2, 0, 8, 8, 8, 9, 4, 11, 3, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 2, 4, 14, 8, 8, 3, 8, 3, 2, 8, 2, 8, 2, 9, 2, 11, 3, 11, 3, 14, 8, 13, 3, 2, 13, 4, 8, 8, 8, 0, 4, 0, 2, 5, 4, 3, 2, 4, 6, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 3, 0, 0, 8, 0, 8, 9, 2, 9, 8, 8, 5, 9, 8, 3, 3, 3, 14, 2, 9, 8, 4, 11, 5, 8, 3, 2, 8, 0, 8, 14, 4, 2, 4, 3, 2, 6, 5, 5, 8, 8, 5, 8, 4, 8, 2, 8]\n",
            "dided\n",
            "time\n",
            "[8, 2, 8, 2, 8, 2, 3, 11, 3, 13, 8, 9, 0, 2, 8, 0, 14, 14, 8, 0, 3, 8, 13, 0, 13, 13, 0, 3, 0, 3, 3, 11, 2, 5, 3, 3, 6, 3, 11, 8, 0, 3, 8, 8, 11, 3, 8, 3, 3, 8, 13, 13, 8, 11, 2, 3, 8, 0, 2, 8, 13, 13, 5, 8, 14, 0, 11, 2, 8, 13, 5, 8, 3, 14, 2, 4, 11, 2, 8, 8, 14, 3, 6, 9, 3, 4, 8, 8, 8, 4, 3, 8, 0, 3, 8, 8, 6, 14, 5, 3, 4, 2, 3, 2, 13, 11, 9, 11, 9, 8, 0, 8, 13, 8, 8, 0, 4, 8, 8, 9, 8, 2, 4, 9, 2, 14, 0, 5]\n",
            "13 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02187514677643776 0.228759765625 3.396859645843506 0.3839642107486725 1.2615669965744019 2.0930469036102295\n",
            "repr, std, cov, clossl, z, norm 0.031162079423666 0.2310791015625 3.272585868835449 0.4363974630832672 1.3027626276016235 1.741248369216919\n",
            "repr, std, cov, clossl, z, norm 0.021702919155359268 0.232666015625 3.219752311706543 0.46014463901519775 1.2653924226760864 2.0654659271240234\n",
            "repr, std, cov, clossl, z, norm 0.03773898631334305 0.226318359375 3.5038247108459473 0.502004086971283 1.2092418670654297 1.7271286249160767\n",
            "repr, std, cov, clossl, z, norm 0.0205391738563776 0.228271484375 3.4037842750549316 0.5209517478942871 1.4299256801605225 1.5948947668075562\n",
            "repr, std, cov, clossl, z, norm 0.03666379675269127 0.2259521484375 3.5287351608276367 0.5154902338981628 1.2781010866165161 2.0456554889678955\n",
            "repr, std, cov, clossl, z, norm 0.020000604912638664 0.2266845703125 3.52177095413208 0.5232344269752502 1.2954944372177124 1.6748274564743042\n",
            "repr, std, cov, clossl, z, norm 0.034434109926223755 0.2275390625 3.4617509841918945 0.45190560817718506 1.2726932764053345 2.144838333129883\n",
            "repr, std, cov, clossl, z, norm 0.02553306519985199 0.2266845703125 3.4875741004943848 0.44280681014060974 1.2670040130615234 2.1852550506591797\n",
            "repr, std, cov, clossl, z, norm 0.036681707948446274 0.2237548828125 3.6485509872436523 0.45783674716949463 1.276994228363037 2.2472198009490967\n",
            "repr, std, cov, clossl, z, norm 0.02001381665468216 0.2274169921875 3.4502451419830322 0.43978986144065857 1.3065952062606812 2.1573169231414795\n",
            "repr, std, cov, clossl, z, norm 0.028085479512810707 0.2269287109375 3.4677700996398926 0.47941991686820984 1.296722650527954 1.7442176342010498\n",
            "train_data.data 21118\n",
            "dided\n",
            "time\n",
            "[14, 0, 5, 3, 11, 0, 8, 8, 4, 8, 8, 5, 8, 9, 3, 2, 3, 8, 14, 8, 13, 6, 14, 3, 8, 14, 3, 8, 8, 0, 2, 8, 11, 3, 5, 4, 0, 2, 3, 3, 3, 14, 0, 8, 14, 4, 2, 0, 4, 5, 4, 8, 8, 6, 8, 6, 3, 9, 6, 8, 11, 4, 8, 3, 8, 4, 8, 8, 14, 2, 14, 3, 8, 4, 3, 4, 3, 0, 3, 2, 11, 4, 8, 3, 14, 11, 0, 4, 2, 0, 3, 2, 4, 3, 4, 2, 0, 3, 3, 2, 2, 11, 4, 2]\n",
            "dided\n",
            "time\n",
            "[11, 4, 2, 3, 3, 8, 4, 3, 3, 5, 9, 14, 13, 3, 3, 0, 8, 3, 14, 4, 9, 8, 8, 14, 3, 3, 3, 9, 8, 2, 4, 0, 8, 8, 5, 3, 5, 8, 4, 5, 11, 8, 4, 8, 5, 8, 2, 2, 13, 8, 5, 14, 14, 3, 9, 5, 11, 4, 13, 3, 4, 8, 3, 9, 5, 8, 8, 3, 5, 8, 8, 13, 8, 4, 4, 3, 0, 4, 8, 3, 0, 3, 3, 8, 8, 11, 0, 4, 3, 8, 8, 0, 11, 4, 13, 14, 5, 5, 8, 4, 11, 5, 3, 8, 3, 3, 11, 8, 8, 8, 8, 8, 3, 11, 14, 5, 4, 8, 4, 13, 0, 13, 8, 8, 5, 14, 2, 2, 6, 8, 2]\n",
            "dided\n",
            "time\n",
            "[8, 2, 8, 8, 8, 3, 3, 4, 3, 14, 8, 9, 11, 11, 5, 8, 8, 13, 8, 11, 4, 3, 9, 3, 8, 4, 8, 13, 3, 8, 0, 4, 3, 9, 8, 13, 3, 14, 3, 13, 0, 8, 9, 3, 8, 13, 3, 4, 0, 5, 4, 5, 0, 8, 8, 5, 3, 8, 8, 9, 9, 5, 0, 8, 8, 3, 0, 11, 9, 2, 14, 4, 13, 14, 8, 8, 14, 3, 9, 3, 3, 3, 3, 2, 8, 8, 4, 2, 14, 5, 0, 2, 8, 0, 4, 8, 8, 8, 5, 3, 8, 3, 9, 0, 5, 8, 8, 11, 14, 8, 13, 8, 8, 2, 4, 0, 8, 3]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 3, 3, 2, 3, 0, 13, 3, 9, 4, 9, 4, 8, 14, 8, 8, 5, 3, 6, 11, 2, 2, 4, 14, 4, 9, 3, 2, 8, 14, 3, 8, 6, 5, 14, 8, 0, 3, 5, 3, 2, 0, 6, 9, 14, 2, 5, 14, 0, 3, 2, 14, 3, 14, 8, 14, 4, 3, 9, 14, 13, 11, 8, 5, 2, 8, 5, 9, 4, 8, 3, 5, 9, 9, 3, 8, 8, 0, 8, 0, 3, 13, 8, 3, 5, 2, 8, 4, 8, 2, 8, 2, 13, 8, 8, 4, 3, 3, 5, 8, 8, 8, 4, 14, 14, 0, 4, 4, 5, 8, 3, 4, 13, 8, 3, 0, 11, 5, 3, 8, 13, 5]\n",
            "dided\n",
            "time\n",
            "[8, 13, 5, 3, 0, 8, 3, 3, 2, 4, 14, 8, 0, 3, 8, 8, 0, 3, 8, 8, 5, 4]\n",
            "14 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02437947690486908 0.231201171875 3.2809979915618896 0.4301820397377014 1.247361660003662 1.7831627130508423\n",
            "repr, std, cov, clossl, z, norm 0.03531284257769585 0.2294921875 3.354597568511963 0.39313244819641113 1.3660346269607544 1.8947935104370117\n",
            "repr, std, cov, clossl, z, norm 0.020517107099294662 0.2265625 3.499293804168701 0.3787553608417511 1.2747420072555542 1.9595407247543335\n",
            "repr, std, cov, clossl, z, norm 0.02752295508980751 0.2347412109375 3.0961108207702637 0.4459184408187866 1.3064440488815308 1.8070266246795654\n",
            "repr, std, cov, clossl, z, norm 0.02444656565785408 0.23095703125 3.275557279586792 0.4454728364944458 1.2629624605178833 1.8854553699493408\n",
            "repr, std, cov, clossl, z, norm 0.02910182811319828 0.2230224609375 3.6681504249572754 0.46763041615486145 1.2850626707077026 1.6914260387420654\n",
            "repr, std, cov, clossl, z, norm 0.022898567840456963 0.2269287109375 3.488372802734375 0.5159032344818115 1.2373052835464478 2.0106794834136963\n",
            "repr, std, cov, clossl, z, norm 0.036140237003564835 0.2239990234375 3.6511638164520264 0.47886189818382263 1.28842294216156 2.145960807800293\n",
            "repr, std, cov, clossl, z, norm 0.02291843295097351 0.22705078125 3.4806106090545654 0.5037558078765869 1.3356167078018188 2.0981903076171875\n",
            "repr, std, cov, clossl, z, norm 0.02942216955125332 0.2303466796875 3.297726631164551 0.49155572056770325 1.356189250946045 1.813692331314087\n",
            "repr, std, cov, clossl, z, norm 0.023903867229819298 0.230712890625 3.3025388717651367 0.4151118993759155 1.2687759399414062 2.151236057281494\n",
            "repr, std, cov, clossl, z, norm 0.02791311964392662 0.23046875 3.3067235946655273 0.43619030714035034 1.3231369256973267 2.125526189804077\n",
            "train_data.data 20400\n",
            "dided\n",
            "time\n",
            "[4, 2, 8, 3, 11, 8, 11, 4, 4, 8, 9, 8, 8, 6, 3, 5, 9, 14, 11, 8, 3, 8, 6, 14, 3, 2, 8, 11, 8, 9, 11, 5, 8, 8, 9, 7, 2, 3, 9, 3, 0, 3, 13, 11, 8, 3, 3, 8, 11, 9, 11, 8, 3, 2, 8, 0, 5, 8, 9, 0, 4, 4, 8, 14, 3, 14, 0, 9, 11, 11, 5, 0, 0, 0, 8, 14, 14, 4, 8, 3, 8, 3, 8, 4, 6, 4, 3, 0, 9, 14, 2, 3, 8, 2, 8, 3, 8, 8, 0, 9, 8, 0, 8, 9, 8, 14, 4, 8, 3, 8]\n",
            "dided\n",
            "time\n",
            "[8, 0, 8, 8, 3, 3, 2, 8, 2, 2, 8, 0, 4, 9, 9, 3, 14, 14, 0, 11, 5, 9, 2, 11, 2, 5, 3, 3, 13, 9, 5, 4, 8, 8, 8, 3, 13, 11, 3, 3, 4, 8, 3, 2, 3, 4, 8, 14, 11, 4, 11, 14, 11, 14, 14, 14, 0, 0, 8, 8, 2, 11, 3, 8, 14, 1, 5, 6, 0, 4, 0, 8, 5, 4, 0, 8, 8, 3, 4, 5, 3, 4, 14, 4, 3, 3, 2, 3, 2, 3, 3, 8]\n",
            "dided\n",
            "time\n",
            "[3, 3, 8, 4, 4, 8, 3, 9, 5, 6, 3, 11, 8, 14, 2, 8, 9, 14, 14, 9, 4, 13, 3, 8, 3, 8, 9, 8, 3, 11, 3, 3, 8, 0, 4, 11, 4, 14, 8, 8, 4, 3, 3, 14, 2, 2, 6, 3, 8, 8, 14, 2, 3, 8, 0, 4, 5, 0, 9, 0, 8, 14, 11, 14, 11, 14, 11, 3, 3, 2, 8, 5, 3, 0, 8, 3, 0, 3, 8, 2, 11, 0, 11, 8, 14, 2, 4, 13, 14, 8, 9, 4, 2, 8, 14, 5, 3, 8, 3, 3, 3, 8, 8, 8, 8, 6, 2, 4, 8, 5, 13, 8, 0, 3, 8, 0, 11, 3, 0, 5, 13, 8, 8, 3, 0, 3, 3, 5, 2, 5, 9, 8, 0, 8, 8, 11, 8, 3, 0, 6, 3, 0, 8, 2, 13, 3, 8, 0, 8, 3, 3, 3, 8, 4, 14, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 3, 11, 4, 8, 4, 4, 0, 3, 2, 2, 8, 2, 0, 8, 4, 14, 14, 3, 2, 14, 5, 8, 8, 13, 9, 8, 8, 2, 5, 2, 9, 8, 14, 4, 0, 3, 11, 9, 8, 8, 11, 8, 8, 3, 3, 0, 8, 8, 0, 2, 0, 5, 14]\n",
            "dided\n",
            "time\n",
            "[5, 14, 3, 14, 11, 8, 5, 8, 8, 8, 3, 5, 0, 9, 3, 11, 3, 9, 3, 3, 2, 14, 8, 8, 11, 2, 8, 3, 4, 14, 11, 8, 2, 5, 5, 3, 8, 8, 11, 4, 3, 3, 3, 11, 3, 8, 0, 4, 2, 8, 13, 13, 13, 9, 3, 9, 8, 5, 2, 3, 13, 0, 3, 8, 8, 13, 8, 2, 4, 8, 0, 9, 0, 8, 14, 5, 9, 0, 8, 4, 3, 11, 2, 4, 2, 3, 4, 5, 2, 3, 8, 0, 3, 2, 3, 2, 3, 11, 11, 5, 4, 3, 14, 11, 8, 8, 14, 14, 8, 3, 3, 8, 8, 2, 8, 0, 8, 8, 11, 14, 8, 13, 3, 14, 8, 3, 14, 8, 5, 8, 14, 3, 4, 2, 0, 3, 3, 2, 0, 13, 8, 0, 8, 8, 11, 4, 3, 0, 0, 14, 4, 8, 8, 9, 14, 9, 0, 9, 9, 4, 3, 14, 3, 9, 4, 3]\n",
            "15 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.020977236330509186 0.2298583984375 3.3232827186584473 0.36448484659194946 1.301919937133789 1.987878441810608\n",
            "repr, std, cov, clossl, z, norm 0.02747534215450287 0.232421875 3.2218880653381348 0.4181075692176819 1.2439197301864624 2.283031463623047\n",
            "repr, std, cov, clossl, z, norm 0.02402099035680294 0.23046875 3.2869863510131836 0.43465039134025574 1.286439061164856 2.1509432792663574\n",
            "repr, std, cov, clossl, z, norm 0.026652196422219276 0.228759765625 3.385120391845703 0.36790454387664795 1.371635913848877 2.078566789627075\n",
            "repr, std, cov, clossl, z, norm 0.021263260394334793 0.2269287109375 3.4835879802703857 0.48935577273368835 1.3052496910095215 2.1578361988067627\n",
            "repr, std, cov, clossl, z, norm 0.027790173888206482 0.22216796875 3.7202839851379395 0.45131513476371765 1.318672776222229 2.0589406490325928\n",
            "repr, std, cov, clossl, z, norm 0.02371656894683838 0.22705078125 3.4748783111572266 0.367085337638855 1.3010472059249878 1.9193986654281616\n",
            "repr, std, cov, clossl, z, norm 0.027812592685222626 0.2318115234375 3.264193058013916 0.4336718022823334 1.3512810468673706 1.9602322578430176\n",
            "repr, std, cov, clossl, z, norm 0.022719761356711388 0.2313232421875 3.2524633407592773 0.3619276285171509 1.2709378004074097 1.7862149477005005\n",
            "repr, std, cov, clossl, z, norm 0.039839424192905426 0.2308349609375 3.2813141345977783 0.45170801877975464 1.3562549352645874 1.7275642156600952\n",
            "repr, std, cov, clossl, z, norm 0.02965380996465683 0.2230224609375 3.7004494667053223 0.4716455340385437 1.3278156518936157 1.837412714958191\n",
            "repr, std, cov, clossl, z, norm 0.032107360661029816 0.2254638671875 3.5868406295776367 0.47098198533058167 1.3311599493026733 1.8693937063217163\n",
            "train_data.data 20758\n",
            "dided\n",
            "time\n",
            "[3, 6, 8, 3, 0, 8, 8, 8, 4, 9, 5, 8, 9, 3, 4, 3, 8, 8, 0, 11, 3, 8, 0, 3, 14, 0, 14, 3, 3, 3, 8, 8, 8, 8, 4, 4, 8, 2, 2, 8, 8, 9, 11, 14, 3, 8, 8, 4, 6, 13, 11, 14, 4, 14, 3, 4, 0, 8, 8, 2, 4, 4, 9, 8, 8, 4, 8, 14, 0, 0, 8, 8, 8, 14, 14, 6, 14, 11, 3, 0, 0, 6, 3, 3, 2, 14, 8, 3, 8, 3, 8, 8, 3, 8, 4, 3, 8, 4, 14, 14]\n",
            "dided\n",
            "time\n",
            "[4, 14, 14, 11, 3, 3, 14, 5, 3, 5, 0, 8, 9, 3, 4, 9, 0, 11, 3, 9, 14, 6, 8, 3, 8, 8, 8, 8, 5, 4, 2, 14, 2, 9, 8, 8, 14, 8, 3, 13, 11, 8, 4, 8, 8, 9, 14, 8, 8, 11, 3, 8, 14, 8, 3, 0, 0, 3, 0]\n",
            "dided\n",
            "time\n",
            "[3, 0, 4, 9, 4, 2, 13, 14, 13, 8, 4, 4, 8, 14, 5, 8, 8, 8, 8, 2, 8, 3, 14, 14, 8, 8, 8, 2, 0, 5, 2, 8, 11, 8, 3, 3, 11, 14, 3, 8, 8, 4, 2, 9, 8, 8, 4, 2, 8, 6, 0, 4, 3, 14, 8, 5, 5, 3, 3, 5, 8, 2, 8, 14, 8, 2, 3, 5, 3, 8, 8, 8, 8, 13, 4, 8, 0, 3, 3, 5, 4, 8, 9, 2, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 14, 5, 5, 8, 13, 3, 11, 0, 8, 0, 11, 14, 14, 11, 0, 0, 4, 8, 8, 0, 5, 3, 3, 5, 8, 14, 8, 8, 8, 9, 3, 3, 4, 8, 8, 3, 3, 8, 7, 9, 9, 8, 3, 8, 3, 5, 14, 8, 8, 2, 8, 14, 2, 4, 8, 4, 4, 2, 8, 3, 13, 4, 5, 8]\n",
            "dided\n",
            "time\n",
            "[4, 13, 3, 2, 3, 3, 8, 5, 3, 4, 8, 14, 8, 3, 8, 13, 5, 11, 2, 0, 8, 5, 14, 8, 8, 8, 8, 9, 0, 0, 2, 3, 8, 0, 2, 9, 3, 3, 2, 14, 3, 3, 2]\n",
            "16 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.024294691160321236 0.225341796875 3.569840669631958 0.5277284383773804 1.2646580934524536 2.042667865753174\n",
            "repr, std, cov, clossl, z, norm 0.026289023458957672 0.227783203125 3.4497556686401367 0.47850605845451355 1.3417924642562866 1.9674650430679321\n",
            "repr, std, cov, clossl, z, norm 0.022961463779211044 0.2322998046875 3.2270543575286865 0.46260130405426025 1.3597140312194824 1.7836543321609497\n",
            "repr, std, cov, clossl, z, norm 0.030287496745586395 0.233642578125 3.15311861038208 0.39152660965919495 1.3361769914627075 2.006941556930542\n",
            "repr, std, cov, clossl, z, norm 0.024870017543435097 0.2332763671875 3.1824541091918945 0.3868386149406433 1.3773373365402222 1.8010441064834595\n",
            "repr, std, cov, clossl, z, norm 0.0380508117377758 0.228759765625 3.4139461517333984 0.4021066725254059 1.259276270866394 1.4389879703521729\n",
            "repr, std, cov, clossl, z, norm 0.025079570710659027 0.227294921875 3.4711952209472656 0.42464399337768555 1.2834886312484741 1.415871262550354\n",
            "repr, std, cov, clossl, z, norm 0.040034521371126175 0.228271484375 3.4127354621887207 0.3849217891693115 1.351056694984436 1.845922589302063\n",
            "repr, std, cov, clossl, z, norm 0.027253607288002968 0.2281494140625 3.421943187713623 0.45948362350463867 1.3817365169525146 1.4622697830200195\n",
            "repr, std, cov, clossl, z, norm 0.0711519718170166 0.2283935546875 3.4075145721435547 0.423287957906723 1.1947643756866455 1.538454532623291\n",
            "repr, std, cov, clossl, z, norm 0.02456655539572239 0.231201171875 3.265091896057129 0.4486384689807892 1.2921463251113892 2.089559316635132\n",
            "repr, std, cov, clossl, z, norm 0.04561242833733559 0.2281494140625 3.431511163711548 0.491487056016922 1.2590749263763428 1.6870824098587036\n",
            "train_data.data 20906\n",
            "dided\n",
            "time\n",
            "[3, 2, 3, 2, 11, 14, 3, 8, 3, 8, 8, 14, 9, 8, 11, 8, 8, 8, 4, 8, 8, 8, 9, 3, 14, 7, 8, 3, 11, 8, 9, 11, 3, 3, 13, 11, 3, 0, 3, 6, 4, 0, 8, 6, 3, 11, 2, 4, 14, 3, 11, 8, 2, 8, 0, 3, 2, 4, 8, 3, 14, 3, 11, 5, 5, 2, 8, 0]\n",
            "dided\n",
            "time\n",
            "[2, 8, 0, 0, 4, 5, 8, 14, 4, 3, 9, 3, 0, 3, 11, 5, 8, 3, 5, 3, 3, 4, 9, 14, 8, 13, 5, 14, 2]\n",
            "dided\n",
            "time\n",
            "[0, 3, 8, 11, 4, 14, 8, 2, 0, 0, 6, 0, 11, 9, 11, 3, 9, 0, 8, 5, 13, 4, 14, 3, 8, 14, 0, 4, 11, 8, 8, 3, 13, 2, 8, 0, 3, 0, 5, 3, 3, 9, 2, 3, 9, 3, 13, 8, 0, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 8, 3, 7, 2, 8, 8, 13, 3, 3, 8, 8, 8, 0, 3, 3, 14, 8, 0, 7, 8, 14, 9, 8, 5, 11, 6, 2, 3, 3, 0, 8, 13, 8, 8, 8, 8, 8, 4, 6, 3, 8, 14, 8, 8, 2, 3, 8, 3, 11, 3, 14, 8, 4, 8, 0, 9, 8, 13, 9]\n",
            "dided\n",
            "time\n",
            "[9, 6, 8, 8, 8, 3, 8, 4, 8, 2, 0, 8, 14, 8, 3, 3, 8, 3, 14, 8, 4, 8, 2, 3, 8, 3, 0, 8, 14, 8, 2, 8, 3, 8, 14, 14, 5, 6, 8, 8, 3, 8, 0, 8, 3, 8, 5, 0, 4, 11, 8, 9, 8, 5, 8, 13, 8]\n",
            "17 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.023466838523745537 0.22900390625 3.366807222366333 0.515995442867279 1.2225959300994873 1.5330109596252441\n",
            "repr, std, cov, clossl, z, norm 0.04107869043946266 0.234130859375 3.1402230262756348 0.483138769865036 1.2821487188339233 1.973456621170044\n",
            "repr, std, cov, clossl, z, norm 0.023654531687498093 0.2340087890625 3.122159004211426 0.4752632975578308 1.2608734369277954 1.8046022653579712\n",
            "repr, std, cov, clossl, z, norm 0.03086654655635357 0.2305908203125 3.3024635314941406 0.43918848037719727 1.370351791381836 1.6662547588348389\n",
            "repr, std, cov, clossl, z, norm 0.026233749464154243 0.225830078125 3.5465087890625 0.43216514587402344 1.315356731414795 2.0865516662597656\n",
            "repr, std, cov, clossl, z, norm 0.036562371999025345 0.2269287109375 3.4650001525878906 0.5337997674942017 1.2842727899551392 1.789681315422058\n",
            "repr, std, cov, clossl, z, norm 0.02032703533768654 0.2261962890625 3.515479326248169 0.44217243790626526 1.273699164390564 1.4879751205444336\n",
            "repr, std, cov, clossl, z, norm 0.023647738620638847 0.2293701171875 3.374129295349121 0.4228993356227875 1.3182744979858398 1.9930247068405151\n",
            "repr, std, cov, clossl, z, norm 0.01877673901617527 0.228759765625 3.3915042877197266 0.40035468339920044 1.2813011407852173 2.026860237121582\n",
            "repr, std, cov, clossl, z, norm 0.025161854922771454 0.2274169921875 3.455472946166992 0.41651949286460876 1.25357985496521 1.6905580759048462\n",
            "repr, std, cov, clossl, z, norm 0.01693815551698208 0.2269287109375 3.476558208465576 0.40641582012176514 1.292720913887024 1.7340694665908813\n",
            "repr, std, cov, clossl, z, norm 0.021028095856308937 0.2281494140625 3.4382641315460205 0.4072420001029968 1.3802651166915894 1.8028185367584229\n",
            "train_data.data 20019\n",
            "dided\n",
            "time\n",
            "[3, 3, 3, 0, 2, 2, 0, 9, 5, 8, 14, 14, 8, 8, 0, 5, 2, 3, 3, 0, 2, 14, 5, 0, 2, 9]\n",
            "dided\n",
            "time\n",
            "[9, 3, 8, 8, 3, 4, 8, 9, 0, 9, 3, 4, 8, 13, 8, 0, 4, 5, 5, 3, 11, 6, 5, 14, 8, 0, 14, 0, 8, 8, 11, 9, 8, 11, 5, 13, 8, 8, 6, 0, 8, 2, 8, 5, 8, 3, 11, 14, 3, 8, 8, 3, 11, 2, 5, 8, 9, 9, 0, 8, 4, 4, 5, 5, 8, 3, 11, 8, 3, 14, 3, 8, 0, 8, 14, 8, 8, 14, 3, 8, 2, 9, 11, 8, 11, 4, 5, 0, 8, 3, 4, 2, 8, 8, 7, 14, 6, 3, 14, 3, 14, 8, 8, 0, 2, 13, 3, 3, 11, 5, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[2, 8, 8, 14, 8, 11, 8, 5, 8, 13, 11, 3, 8, 9, 8, 0, 14, 2, 9, 3, 7, 3, 0, 8, 11, 14, 3, 14, 9, 3, 3, 8, 8, 0, 8, 13, 0, 8, 4, 0, 8, 13, 8, 8, 8, 5, 4, 4, 8, 2, 0, 0, 8, 4, 9, 2, 2, 2, 3, 3, 8, 9, 0, 0, 8, 3, 3, 14, 3, 4, 14, 4, 14, 8, 9, 8, 0, 5, 9, 8, 8, 3, 14, 4, 3, 0, 9, 4, 0, 13, 8, 3, 8, 4, 2, 3, 8, 4, 3, 8, 3, 3, 3, 0, 4, 3, 8, 8, 8, 2, 0, 9, 0, 3, 2, 14, 13, 8, 8, 9, 5, 8, 8, 8, 9, 14, 4, 4, 3, 4, 3, 4]\n",
            "dided\n",
            "time\n",
            "[4, 3, 4, 8, 14, 4, 6, 8, 13, 14, 4, 8, 0, 3, 11, 5, 14, 8, 0, 8, 3, 2, 4, 14, 11, 14, 14, 5, 5, 8, 8, 8, 4, 2, 2, 8, 4, 2, 8, 13, 11, 11, 8, 4, 14, 4, 11, 3, 4, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 5, 0, 8, 0, 4, 8, 0, 11, 2, 2, 3, 8, 3, 8, 5, 5, 5, 8, 8, 3, 4, 0, 2, 3, 5, 8, 3, 8, 3, 5, 8, 3, 5, 2, 9, 13, 4, 8, 6, 8, 8, 5, 0, 6, 8, 11, 3, 8, 3, 4, 2, 9, 5, 5, 8, 8, 11, 8, 14, 6, 3, 0, 0, 3, 8, 8, 8, 8, 8, 8, 0, 2, 9, 0, 13, 3, 14, 8, 0, 2, 3, 8, 4, 0, 8, 2, 9, 8, 0, 8, 14, 8, 6, 3, 9, 8, 8, 8, 6, 8, 8, 2, 8, 7, 5, 0, 3, 13, 5]\n",
            "18 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.017246661707758904 0.2303466796875 3.328307628631592 0.4835886061191559 1.3987395763397217 1.7570000886917114\n",
            "repr, std, cov, clossl, z, norm 0.024950847029685974 0.229248046875 3.3590779304504395 0.40031901001930237 1.292405366897583 1.5847060680389404\n",
            "repr, std, cov, clossl, z, norm 0.02466520667076111 0.2249755859375 3.5738823413848877 0.41676703095436096 1.2633843421936035 1.7478103637695312\n",
            "repr, std, cov, clossl, z, norm 0.028408493846654892 0.2335205078125 3.1347603797912598 0.468270480632782 1.3021049499511719 1.9980974197387695\n",
            "repr, std, cov, clossl, z, norm 0.020517826080322266 0.232666015625 3.1898202896118164 0.42578232288360596 1.3599793910980225 2.020515203475952\n",
            "repr, std, cov, clossl, z, norm 0.027846364304423332 0.2293701171875 3.366260290145874 0.4385825991630554 1.2541158199310303 2.0198841094970703\n",
            "repr, std, cov, clossl, z, norm 0.023566145449876785 0.2283935546875 3.4151759147644043 0.4691988527774811 1.3691104650497437 2.2562737464904785\n",
            "repr, std, cov, clossl, z, norm 0.03453052043914795 0.2264404296875 3.5332231521606445 0.5341334342956543 1.2803033590316772 2.36316180229187\n",
            "repr, std, cov, clossl, z, norm 0.027334587648510933 0.228759765625 3.392683267593384 0.46608951687812805 1.3313984870910645 2.1342358589172363\n",
            "repr, std, cov, clossl, z, norm 0.02851269580423832 0.2340087890625 3.134028434753418 0.4676482677459717 1.2697921991348267 1.9373341798782349\n",
            "repr, std, cov, clossl, z, norm 0.028001166880130768 0.2344970703125 3.1253061294555664 0.4434750974178314 1.3311618566513062 2.017575263977051\n",
            "repr, std, cov, clossl, z, norm 0.03205769881606102 0.22998046875 3.3373899459838867 0.376984566450119 1.349541187286377 2.515272617340088\n",
            "train_data.data 20926\n",
            "dided\n",
            "time\n",
            "[13, 5, 8, 11, 8, 3, 5, 9, 11, 8, 6, 8, 0, 3, 9, 3, 3, 14, 14, 14, 5, 8, 4, 3, 3, 11, 2, 8, 7, 3, 8, 14, 3, 2, 14, 5, 3, 3, 14, 8, 8, 0, 8, 5, 0, 8, 11, 8, 0, 8, 3, 9, 8, 3, 0, 8, 4, 6, 8, 3, 14, 0, 0, 8, 4, 8, 8, 8, 8, 5, 9, 14, 3, 14, 3, 5, 8, 0, 8, 14, 14, 3, 8, 9, 7, 0, 2, 3, 8, 8, 4, 11, 3, 11, 13, 9, 8, 8, 8, 8, 2, 3, 4, 13, 6, 4, 8, 14, 3, 3, 3, 8, 8, 9, 8, 8, 2, 0, 4, 3, 6, 9, 8, 4, 0, 8, 8, 4, 5, 8, 8, 3, 5, 8, 2, 3, 4, 8, 4, 14, 5, 4, 3, 4, 11, 8, 8, 4, 3, 14, 14, 14, 8, 5, 0, 0, 0, 8, 0, 4, 3, 2, 4, 8, 13, 8, 13, 8, 13, 8, 8, 3, 8, 13, 9, 4, 2, 2, 3, 8, 9, 0, 13, 0, 8, 9, 8, 8, 2, 3, 11, 11, 14, 3, 4, 4, 13, 4, 14, 5, 9, 11]\n",
            "dided\n",
            "time\n",
            "[11, 8, 2, 14, 14, 8, 5, 3, 14, 11, 0, 8, 5, 3, 1, 8, 13, 4, 14, 8, 2, 8, 0, 8, 14, 4, 6, 3, 8, 5, 2, 13, 9, 3, 11, 8, 8]\n",
            "dided\n",
            "time\n",
            "[9, 4, 2, 8, 14, 0, 5, 3, 3, 5, 3, 3, 3, 4, 5, 14, 3, 8, 8, 8, 3, 8, 4, 13, 5, 9, 8, 13, 0, 13, 8, 4, 2, 3, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 5, 14, 8, 13, 8, 8, 5, 4, 8, 3, 8, 11, 0, 11, 0, 5, 4, 14, 13, 9, 9, 8, 2, 7, 4, 8, 8, 14, 8, 8, 4, 12, 14, 9, 8, 8, 11, 0, 9, 3, 2, 8, 8, 13, 8, 3, 5, 8, 9, 8, 11, 0, 5, 14, 8, 4, 0, 3, 0, 8, 3, 8, 14, 9, 9, 9, 14, 8, 13, 8, 11, 0, 8, 8, 14, 3, 11, 4, 3, 3, 2, 2, 0, 5, 3, 3, 11, 3, 3, 3, 8, 9, 3, 2, 3, 0, 4, 0, 4, 4, 8, 8, 4, 8, 6, 8, 5, 8, 13, 0, 8, 13]\n",
            "dided\n",
            "time\n",
            "[13, 3, 3, 4, 3, 8, 5, 8, 3, 3, 8, 14, 4, 0, 3, 5, 14, 8, 14, 8, 8, 4, 3, 2, 8, 13, 9, 0, 3, 2, 3, 8, 5, 8, 4, 9, 3, 3, 2, 14, 4, 0, 8, 0, 8, 3, 5, 5, 3, 8, 14, 9, 4, 0, 2, 5, 14, 13, 13, 4, 8, 8, 9, 6, 8]\n",
            "19 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.024933762848377228 0.2255859375 3.565504550933838 0.5004581212997437 1.2164186239242554 2.114684820175171\n",
            "repr, std, cov, clossl, z, norm 0.03034784458577633 0.2276611328125 3.4396204948425293 0.4123971164226532 1.2977079153060913 2.0305938720703125\n",
            "repr, std, cov, clossl, z, norm 0.023089248687028885 0.23388671875 3.1452174186706543 0.47649627923965454 1.2881978750228882 2.094391345977783\n",
            "repr, std, cov, clossl, z, norm 0.030095521360635757 0.2332763671875 3.17612886428833 0.4541093409061432 1.2339366674423218 1.609165072441101\n",
            "repr, std, cov, clossl, z, norm 0.025256173685193062 0.232177734375 3.217586040496826 0.4029996693134308 1.3611725568771362 2.3058602809906006\n",
            "repr, std, cov, clossl, z, norm 0.03661514073610306 0.2271728515625 3.469517707824707 0.47426488995552063 1.3455173969268799 1.9371910095214844\n",
            "repr, std, cov, clossl, z, norm 0.021340884268283844 0.222412109375 3.7425897121429443 0.4768086373806 1.3596042394638062 2.1334705352783203\n",
            "repr, std, cov, clossl, z, norm 0.03191789984703064 0.22705078125 3.4732842445373535 0.3830142617225647 1.2853831052780151 2.1155343055725098\n",
            "repr, std, cov, clossl, z, norm 0.024222053587436676 0.2318115234375 3.2401041984558105 0.42102402448654175 1.3167425394058228 2.068986177444458\n",
            "repr, std, cov, clossl, z, norm 0.0344453863799572 0.2314453125 3.2547049522399902 0.40722358226776123 1.4051547050476074 1.8861316442489624\n",
            "repr, std, cov, clossl, z, norm 0.02276887558400631 0.226318359375 3.5096869468688965 0.3725924789905548 1.3491655588150024 1.9576566219329834\n",
            "repr, std, cov, clossl, z, norm 0.038470882922410965 0.224609375 3.6012871265411377 0.5065472722053528 1.3450250625610352 1.9645743370056152\n",
            "train_data.data 20071\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 8, 11, 4, 8, 8, 11, 3, 8, 0, 3, 3, 14, 0, 4, 5, 3, 9, 9, 8, 8, 13, 2, 2, 3, 14, 3, 5, 8, 10, 3, 8, 9, 14, 4, 8, 8, 8, 0, 1, 13, 2, 14, 3, 14, 4, 13, 0, 9, 2, 13, 8, 14, 4]\n",
            "dided\n",
            "time\n",
            "[8, 14, 4, 0, 8, 13, 8, 14, 14, 14, 3, 5, 8, 0, 3, 0, 14, 4, 5, 14, 8, 4, 8, 3, 8, 6, 8]\n",
            "dided\n",
            "time\n",
            "[6, 8, 5, 0, 3, 11, 10, 8, 4, 3, 6, 8, 8, 9, 0, 14, 5, 8, 0, 11, 4, 3, 8, 0, 3, 14, 13, 14, 8, 0, 8, 8, 13, 3, 3, 8, 4, 8, 3, 0, 8, 11, 0, 4, 3, 5, 5, 0, 4, 0, 0, 8, 9, 3, 3, 14, 2, 3, 0, 14, 8, 3, 8, 3, 0, 8, 2, 9, 0, 5, 8, 11, 8, 5, 8, 8, 8, 14]\n",
            "dided\n",
            "time\n",
            "[14, 8, 8, 8, 2, 2, 8, 8, 4, 5, 14, 3, 8, 3, 5, 0, 2, 2, 5, 5, 5, 5, 0, 14, 14, 9, 14, 3, 4, 8, 3, 0, 8, 8, 3, 13, 8, 8, 3, 4, 9, 5, 0, 3, 2, 14, 13, 5, 14, 13, 3, 0, 11, 5, 2, 0, 3, 14, 3, 8, 8, 9, 13, 14, 3, 9, 8, 2]\n",
            "dided\n",
            "time\n",
            "[9, 8, 2, 2, 8, 8, 8, 14, 5, 4, 9, 3, 3, 8, 5, 8, 8, 3, 5, 13, 3, 13, 3, 8, 8, 0, 5, 8, 3, 0, 8, 5, 3, 5, 8, 3, 8, 9, 14, 2, 3, 8, 0, 0, 8, 8, 8, 5, 13, 13, 11, 11, 9, 8, 4, 14, 9, 3, 5, 2, 7, 13, 6, 3, 14, 2, 3, 14, 0, 8, 5, 13, 8, 4, 8, 4, 13, 8, 0, 8, 0, 4, 8, 5, 8, 8, 14, 9, 8, 4, 3, 2, 8, 13, 2, 14, 2, 11, 14, 5, 2, 8, 8, 3, 0, 3, 3]\n",
            "20 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02728155627846718 0.2232666015625 3.688267230987549 0.46608516573905945 1.2211158275604248 1.7544885873794556\n",
            "repr, std, cov, clossl, z, norm 0.03384743258357048 0.2333984375 3.173039436340332 0.5269307494163513 1.389276385307312 2.2289750576019287\n",
            "repr, std, cov, clossl, z, norm 0.023262662813067436 0.239990234375 2.8769888877868652 0.5003501772880554 1.3654299974441528 2.407559633255005\n",
            "repr, std, cov, clossl, z, norm 0.037291985005140305 0.229248046875 3.3747520446777344 0.47995778918266296 1.25258207321167 2.4777474403381348\n",
            "repr, std, cov, clossl, z, norm 0.023737534880638123 0.2218017578125 3.7849512100219727 0.41109898686408997 1.3681731224060059 2.4242453575134277\n",
            "repr, std, cov, clossl, z, norm 0.030081788077950478 0.2230224609375 3.6858887672424316 0.4313943088054657 1.3383785486221313 2.052652597427368\n",
            "repr, std, cov, clossl, z, norm 0.022475847974419594 0.22900390625 3.368110418319702 0.4138678312301636 1.3609281778335571 1.8419240713119507\n",
            "repr, std, cov, clossl, z, norm 0.035794246941804886 0.236083984375 3.026798963546753 0.4680648446083069 1.3347749710083008 2.258965015411377\n",
            "repr, std, cov, clossl, z, norm 0.023970140144228935 0.226806640625 3.4996232986450195 0.48758265376091003 1.3172321319580078 2.3088417053222656\n",
            "repr, std, cov, clossl, z, norm 0.03288242593407631 0.223876953125 3.6492974758148193 0.5425607562065125 1.3156743049621582 1.8763271570205688\n",
            "repr, std, cov, clossl, z, norm 0.02571525238454342 0.22265625 3.7068729400634766 0.4410010278224945 1.443307876586914 1.5562124252319336\n",
            "repr, std, cov, clossl, z, norm 0.03074602596461773 0.2261962890625 3.5165281295776367 0.5097915530204773 1.2995706796646118 2.2290518283843994\n",
            "train_data.data 20775\n",
            "dided\n",
            "time\n",
            "[3, 3, 9, 8, 9, 3, 0, 0, 4, 3, 11, 4, 5, 4, 14, 8, 2, 13, 9, 13, 8, 0, 8, 8, 9, 2, 8, 8, 3, 4, 8, 3, 2, 6, 8, 8, 4, 8, 8, 9, 4, 3, 14, 8, 9, 8, 3, 3, 4, 13, 8, 8, 4, 13, 0, 14, 14, 3, 4, 8, 9, 8, 14, 5, 4, 8, 3, 2, 14, 11, 3, 8, 2, 11, 4, 9, 8, 2, 3, 8, 4, 8, 8, 8, 13, 8, 14, 8, 4, 11, 10, 11, 4, 9, 11, 8, 4, 2, 8, 9, 8, 8, 2, 8, 12, 4, 8, 8, 14, 4, 14, 8, 13, 9, 3, 6, 14, 8]\n",
            "dided\n",
            "time\n",
            "[4, 9, 11, 14, 4, 4, 14, 13, 4, 8, 8, 2, 13, 3, 8, 8, 2, 14, 11, 4, 7, 3, 2, 8, 4, 3, 6, 0, 4, 2, 8, 3, 8, 8, 8, 3, 3, 6, 8, 5, 2, 8, 3, 5, 2, 3, 4, 8, 14, 5, 0, 0, 5, 0, 8, 3, 4, 5, 3, 8, 8, 3, 3, 14, 9, 4, 8, 0, 8, 4, 8, 8, 9, 0, 9, 3, 4, 2, 2, 8, 6, 9, 3, 8, 9, 3, 8, 2, 0, 8, 4, 4, 2, 3, 8, 8, 4, 11, 5, 2, 13, 14, 2, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 2, 8, 2, 9, 8, 3, 9, 2, 5, 3, 6, 4, 11, 5, 11, 9, 11, 2, 8, 8, 8, 13, 13, 9, 11, 0, 8, 4, 4, 13, 9, 2, 0, 14, 13, 8, 8, 8, 8, 4, 8, 13, 3, 9, 0, 8, 8, 11, 9, 12, 8, 4, 11, 14, 0, 4]\n",
            "dided\n",
            "time\n",
            "[8, 3, 8, 8, 0, 4, 3, 2, 9, 14, 8, 8, 4, 14, 8, 6, 4, 6, 8, 9, 9, 8, 0, 3, 9, 13, 4, 3, 14, 13, 0, 4, 9, 4, 5, 4, 8, 2, 3, 3, 9, 2, 13, 3, 2, 8, 0, 14, 9, 8, 14, 3, 3, 4, 0, 5, 3, 8, 13, 5, 0, 4, 9, 4, 4, 4, 8, 0, 5, 8, 8, 4]\n",
            "dided\n",
            "time\n",
            "[8, 8, 4, 4, 4, 0, 3, 4, 3, 3, 9, 3, 3, 3, 3, 8, 14, 3, 2, 0, 3, 11, 3, 0, 9, 0, 5, 3, 3, 0, 3, 0, 9, 0, 5, 14, 13, 8, 3, 5, 8, 13, 8, 8, 3, 8, 14, 8, 3, 8, 13, 2, 8, 0, 2, 14, 8, 9, 14, 2, 9, 8, 0, 14, 3, 9, 14, 3, 6, 2, 3, 3, 9, 5, 3, 3, 0, 3, 3, 0, 4, 9, 11, 12, 3, 8, 8, 8, 5]\n",
            "21 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.023045295849442482 0.23486328125 3.1099088191986084 0.5126065611839294 1.2998348474502563 1.7309684753417969\n",
            "repr, std, cov, clossl, z, norm 0.025246523320674896 0.23583984375 3.0608558654785156 0.4837666153907776 1.246376872062683 1.669614553451538\n",
            "repr, std, cov, clossl, z, norm 0.02153889089822769 0.237548828125 2.9864211082458496 0.42868679761886597 1.315906047821045 1.3042830228805542\n",
            "repr, std, cov, clossl, z, norm 0.02366502769291401 0.2303466796875 3.321394443511963 0.4394482970237732 1.2784290313720703 2.0489280223846436\n",
            "repr, std, cov, clossl, z, norm 0.022049764171242714 0.2235107421875 3.6534669399261475 0.4309993386268616 1.2101138830184937 1.74065363407135\n",
            "repr, std, cov, clossl, z, norm 0.027891777455806732 0.2252197265625 3.55330753326416 0.40661248564720154 1.3313133716583252 1.6604773998260498\n",
            "repr, std, cov, clossl, z, norm 0.022057494148612022 0.22607421875 3.5475869178771973 0.3654165267944336 1.3471630811691284 1.6217163801193237\n",
            "repr, std, cov, clossl, z, norm 0.03017408773303032 0.227294921875 3.4715704917907715 0.42498278617858887 1.3242844343185425 1.783899188041687\n",
            "repr, std, cov, clossl, z, norm 0.020509976893663406 0.2320556640625 3.245546340942383 0.3810482919216156 1.3411835432052612 1.5959025621414185\n",
            "repr, std, cov, clossl, z, norm 0.026557328179478645 0.23095703125 3.279473066329956 0.43807947635650635 1.2417871952056885 1.6035734415054321\n",
            "repr, std, cov, clossl, z, norm 0.022357039153575897 0.2293701171875 3.3595263957977295 0.4418973922729492 1.2719016075134277 1.8073726892471313\n",
            "repr, std, cov, clossl, z, norm 0.02832426317036152 0.2325439453125 3.2003979682922363 0.5413550138473511 1.3072071075439453 1.7887438535690308\n",
            "train_data.data 21201\n",
            "dided\n",
            "time\n",
            "[4, 8, 11, 0, 3, 3, 2, 0, 4, 8, 8, 8, 3, 6, 4, 3, 8, 4, 5, 13, 5, 13, 3, 5, 4, 8, 2, 8, 13, 2, 0, 0, 0, 14, 2, 7, 8, 4, 14, 4, 8, 3, 8, 3, 14, 0, 8, 9, 0, 13, 4, 5, 0, 13, 9, 2, 14, 8, 3, 14, 11, 14, 3]\n",
            "dided\n",
            "time\n",
            "[14, 3, 6, 2, 3, 2, 8, 5, 4, 3, 8, 13, 8, 4, 8, 2, 0, 14, 14, 2, 3, 11, 8, 3, 3, 4, 4, 3, 13, 4, 8, 8, 8, 0, 3, 9, 3, 14, 2, 8, 3, 5, 14, 13, 14, 8, 3, 8, 0, 3, 3, 4, 8, 8, 3, 3, 11, 2, 2, 2, 8, 3, 2, 8, 0, 6, 0, 0, 3, 0, 8, 3, 3, 8, 8, 8, 0, 3, 0, 8, 9, 14, 8, 3, 3, 5, 3, 8, 5, 8, 8, 5, 0, 8, 4, 11, 14, 8, 8, 14, 3, 14, 5, 3, 0, 11, 5, 5, 5, 11, 5]\n",
            "dided\n",
            "time\n",
            "[11, 5, 13, 8, 0, 8, 2, 8, 0, 8, 5, 8, 14, 8, 3, 3, 8, 13, 3, 14, 0, 14, 4, 14, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 0, 3, 3, 14, 11, 3, 14, 8, 0, 8, 3, 8, 4, 0, 5, 14, 14, 0, 3, 14, 8, 4, 14, 0, 8, 8, 2, 8, 8, 8, 0, 8, 0, 8, 3, 8, 13, 3, 5, 3, 8, 5, 0, 8, 14, 0, 3, 2, 4, 4, 11, 3, 14, 8, 13, 3, 8, 13, 8, 8, 8, 0, 8, 8, 0, 8, 5, 8, 0, 3, 8, 3, 3, 8, 0, 0, 8, 3, 0, 13, 14, 6, 8, 3, 4, 5, 4, 8, 0, 8, 9, 8, 8, 1, 14, 8, 3, 8, 8, 14, 4, 5, 2, 0, 8, 14, 8, 5, 2, 3, 2, 8, 6, 8, 14, 3, 4, 0, 0, 5, 13, 11, 2, 8, 8, 3, 13, 14, 9, 11, 4, 8, 8, 5, 0, 2, 3, 8, 5, 8, 8, 8, 3]\n",
            "dided\n",
            "time\n",
            "[8, 8, 3, 4, 5, 0, 0, 8, 2, 4, 3, 5, 3, 3, 8, 3, 3, 8, 8, 3, 13, 1, 3, 4, 8, 14, 9, 3, 11, 0, 2, 5, 13, 4, 3, 8, 8, 4, 9, 3, 3, 14, 1, 5, 0, 0, 9, 9, 5, 3, 13, 0, 14, 13, 2, 4, 8, 8, 13, 8, 4, 8, 3, 3, 8, 8, 4, 8, 3, 6, 3, 5, 3, 3, 8, 5, 8, 14, 9, 8, 8, 8, 3, 3, 0]\n",
            "22 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02372095361351967 0.2337646484375 3.158536911010742 0.4487737715244293 1.3295519351959229 1.9668608903884888\n",
            "repr, std, cov, clossl, z, norm 0.03644609451293945 0.232177734375 3.2391180992126465 0.46034348011016846 1.2835816144943237 1.731926679611206\n",
            "repr, std, cov, clossl, z, norm 0.024783451110124588 0.224609375 3.6262259483337402 0.4048144221305847 1.33124840259552 1.7884809970855713\n",
            "repr, std, cov, clossl, z, norm 0.03362453728914261 0.2265625 3.4930009841918945 0.487104594707489 1.3465477228164673 2.17256760597229\n",
            "repr, std, cov, clossl, z, norm 0.023741120472550392 0.2298583984375 3.342524528503418 0.4357489049434662 1.288879156112671 2.3531131744384766\n",
            "repr, std, cov, clossl, z, norm 0.02626609057188034 0.2247314453125 3.60383939743042 0.4742712676525116 1.3734444379806519 2.109625816345215\n",
            "repr, std, cov, clossl, z, norm 0.022400101646780968 0.2230224609375 3.6712653636932373 0.36849290132522583 1.3203048706054688 2.1400489807128906\n",
            "repr, std, cov, clossl, z, norm 0.024556845426559448 0.2275390625 3.446584463119507 0.401516318321228 1.2459361553192139 1.9040441513061523\n",
            "repr, std, cov, clossl, z, norm 0.021040014922618866 0.2305908203125 3.2880921363830566 0.42639145255088806 1.3925672769546509 2.077164888381958\n",
            "repr, std, cov, clossl, z, norm 0.024976953864097595 0.22998046875 3.315284013748169 0.4119904041290283 1.220847249031067 1.946879267692566\n",
            "repr, std, cov, clossl, z, norm 0.026847243309020996 0.2286376953125 3.389211654663086 0.5536951422691345 1.2730265855789185 2.1911585330963135\n",
            "repr, std, cov, clossl, z, norm 0.02426161803305149 0.2301025390625 3.3182902336120605 0.4066815674304962 1.2859920263290405 2.455853223800659\n",
            "train_data.data 20651\n",
            "dided\n",
            "time\n",
            "[8, 3, 8, 3, 0, 11, 8, 2, 9, 8, 5, 8, 5, 3, 11, 8, 5, 5, 8, 11, 3, 8, 3, 3, 8, 5, 2, 5, 8, 4, 8, 8, 3, 0, 8, 0, 8, 9, 4, 3, 13, 13, 3, 8, 3, 9, 13, 4, 0, 8, 0, 8, 4, 3, 3, 13, 9, 3, 8, 0, 3, 11, 9, 8, 0, 4, 5, 8, 0, 8, 8, 3, 0, 2, 5, 11, 8, 4, 4, 3, 3, 4, 14, 8, 6, 2, 0, 13, 8, 4, 3, 14, 8, 3, 11, 8, 3, 14, 8, 3, 3, 8, 3, 0, 4, 13, 13, 8, 3, 14, 8, 0, 8, 8, 8, 14, 8, 0, 3, 8, 8, 5, 4, 8, 0, 3, 5, 9, 11, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 0, 11, 0, 3, 8, 14, 0, 8, 9, 11, 0, 8, 0, 0, 2, 3, 8, 2, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 3, 14, 3, 8, 3, 8, 0, 0, 8, 4, 3, 8, 0, 3, 5, 3, 9, 8, 8, 6, 0, 8, 8, 5, 3, 8, 8, 8, 8, 14, 14, 14, 8, 3, 14, 0, 3, 13, 3, 2, 8, 14, 11, 8, 14, 14, 4, 4, 8, 3, 2, 7, 5, 2, 11, 8]\n",
            "dided\n",
            "time\n",
            "[8, 14, 0, 8, 8, 0, 13, 3, 8, 4, 13, 8, 13, 8, 3, 8, 3, 5, 13, 0, 0, 5, 2, 8, 9, 11, 5, 5, 2, 11, 3, 8, 8, 8, 8, 0, 8, 3, 8, 14, 8, 4, 3, 14, 0, 3, 2, 5, 2, 13, 8, 13, 14, 8, 8, 5, 0, 8, 3, 0, 4, 14, 11, 3, 5, 13, 3, 5, 8, 13, 8, 8, 8, 2, 3, 5, 0, 5, 3, 11, 6, 8, 3, 8, 2, 2, 4, 8, 8, 8, 0, 8, 13, 8, 9]\n",
            "dided\n",
            "time\n",
            "[8, 9, 2, 0, 4, 14, 2, 4, 8, 11, 8, 8, 5, 13, 3, 8, 13, 8, 0, 9, 4, 0, 5, 8, 8, 14, 3, 5, 8, 8, 3, 8, 0, 2, 4, 14, 8, 8, 8, 0, 14, 8, 8, 5, 6, 7, 8, 3, 3, 8, 9, 9, 4, 0, 8, 8, 8, 4, 3, 5, 3, 0, 8, 2, 8, 5, 0, 3, 0, 3, 14, 11, 4, 3, 8, 8, 0, 8, 8, 11, 8, 4, 8, 3, 3, 12, 3, 4, 8, 8, 3, 2, 8, 5, 4, 8, 4, 4, 8, 3, 8, 3, 2, 8, 3, 4, 8, 5, 3, 8, 11, 0, 0, 11, 3, 8, 8, 8, 11, 8, 0, 3, 0, 4, 0, 3, 6, 5, 8, 3, 13, 2, 8, 5, 6, 14, 1, 5, 8, 8, 3, 3, 0, 14, 14, 11, 0, 4, 11, 3]\n",
            "dided\n",
            "time\n",
            "[3, 4, 14, 0, 0, 5, 13, 3, 9, 8, 3, 3, 3, 2, 4, 8, 6, 5, 3, 3, 0, 3, 14, 5, 9, 8, 0, 0, 2, 3, 2, 2, 3, 8, 3, 0, 3, 14, 8, 8, 11, 8, 4, 13, 4, 14, 14, 3, 8, 3, 11, 0, 6, 3, 14, 5, 8, 3, 11, 6, 3, 3, 14, 3, 8, 8, 8, 4, 14, 8, 0, 3, 8, 8]\n",
            "23 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.026243416592478752 0.224365234375 3.6151230335235596 0.402402400970459 1.2675752639770508 1.8843278884887695\n",
            "repr, std, cov, clossl, z, norm 0.025536593049764633 0.23388671875 3.161813259124756 0.4784758388996124 1.3600014448165894 1.85051429271698\n",
            "repr, std, cov, clossl, z, norm 0.02613375149667263 0.232421875 3.2312934398651123 0.419685035943985 1.3177595138549805 1.6833291053771973\n",
            "repr, std, cov, clossl, z, norm 0.024857496842741966 0.2305908203125 3.2942514419555664 0.40796786546707153 1.3456127643585205 2.020928144454956\n",
            "repr, std, cov, clossl, z, norm 0.025885455310344696 0.2288818359375 3.383279323577881 0.40228739380836487 1.2058104276657104 1.4542815685272217\n",
            "repr, std, cov, clossl, z, norm 0.02898908592760563 0.2230224609375 3.688650369644165 0.4211835563182831 1.3423256874084473 1.5003700256347656\n",
            "repr, std, cov, clossl, z, norm 0.028950223699212074 0.2279052734375 3.4316306114196777 0.4584487974643707 1.2955317497253418 1.5271599292755127\n",
            "repr, std, cov, clossl, z, norm 0.03010626509785652 0.22802734375 3.433180332183838 0.5244534611701965 1.2972042560577393 1.2842460870742798\n",
            "repr, std, cov, clossl, z, norm 0.027153365314006805 0.2369384765625 2.9944472312927246 0.4685797691345215 1.4204896688461304 1.2989107370376587\n",
            "repr, std, cov, clossl, z, norm 0.03460448980331421 0.2325439453125 3.1953787803649902 0.5523991584777832 1.3945318460464478 1.8061696290969849\n",
            "repr, std, cov, clossl, z, norm 0.026112746447324753 0.225341796875 3.56815242767334 0.49178484082221985 1.3000528812408447 2.0129032135009766\n",
            "repr, std, cov, clossl, z, norm 0.028097935020923615 0.223388671875 3.6813526153564453 0.46233931183815 1.283265471458435 1.5510749816894531\n",
            "train_data.data 21391\n",
            "dided\n",
            "time\n",
            "[8, 3, 4, 3, 3, 8, 13, 14, 8, 8, 11, 3, 5, 4, 3, 8, 9, 8, 3, 3, 4, 5, 13, 8, 13, 3, 14, 3, 11, 6, 5, 3, 14, 11, 14, 14, 8, 4, 8, 3, 11, 8, 3, 8, 2, 11, 3, 0, 9, 8, 11, 2, 13, 3, 3]\n",
            "dided\n",
            "time\n",
            "[3, 3, 4, 8, 11, 3, 8, 8, 11, 14, 8, 2, 14, 5, 4, 6, 0, 8, 5, 8, 8, 8, 3, 3, 3, 14, 14, 4, 4, 5, 2, 8, 5, 8, 9, 8, 11, 0, 3, 5, 8, 3, 8, 3, 8, 8, 5, 11, 14, 4, 14, 8, 0, 8, 0, 2, 12, 8, 3, 5, 11, 8, 8, 8, 4, 8, 8, 8, 3, 5, 12, 0, 8, 14, 14, 11, 8, 3, 8, 3, 8, 0, 5, 8, 8, 14, 3, 3, 2, 3, 5, 5, 4, 9, 0, 9, 8, 11, 4, 8, 8, 14, 3, 8, 8, 11, 4, 4, 9, 0, 14, 0, 5, 0, 3, 0, 3, 8, 3]\n",
            "dided\n",
            "time\n",
            "[8, 3, 5, 11, 0, 13, 3, 6, 0, 0, 5, 8, 3, 11, 3, 8, 0, 5, 8, 8, 3, 13, 3, 8, 3, 5, 14, 0, 3, 9, 8, 11, 8, 13, 5, 0, 4, 5, 8, 3, 11, 8, 8, 8, 14, 8, 8, 8, 9, 8, 8, 2, 2, 14, 8, 4, 3, 3, 2, 5, 8, 8, 8, 3, 14, 14, 8, 0, 0, 2, 14, 14, 4, 8, 8, 8, 4, 8, 9, 11, 3, 8, 14, 2, 4, 11, 13, 8, 8, 3, 9, 4, 4]\n",
            "dided\n",
            "time\n",
            "[8, 8, 2, 8, 0, 5, 5, 2, 8, 3, 3, 2, 4, 8, 14, 13, 8, 3, 8, 3, 8, 8, 13, 3, 0, 12, 4, 3, 0, 4, 8, 14, 4, 8, 3, 3, 8, 2, 14, 8, 3, 3, 8, 4, 4, 4, 8, 8, 2, 0, 14, 8, 8, 4, 3, 13, 11, 5, 4, 9, 13, 3, 2, 14, 2, 0, 8, 8, 2, 5, 8, 14, 2, 2, 11, 8, 0, 4, 11, 5, 4, 11, 3, 4, 5, 4, 14, 2, 3, 3, 14, 14, 2, 2, 3, 13, 8, 0, 3, 2, 4, 8, 3, 2, 14, 8, 8, 8, 2]\n",
            "dided\n",
            "time\n",
            "[8, 11, 8, 4, 4, 11, 0, 5, 5, 4, 8, 8, 11, 13, 2, 3, 8, 3, 3, 8, 0, 13, 8, 8, 8, 8, 4, 3, 0, 8, 8, 9, 0, 8, 3, 5, 14, 9, 0, 13, 8, 3, 1, 6, 0, 8, 8, 9, 3, 0, 11, 3, 9, 14, 13, 3, 8, 8, 13, 3, 9, 3, 3, 3, 9, 5, 14, 8, 3, 8, 8, 0, 3, 3, 9, 4, 9, 5, 9, 2, 11, 4, 0, 4, 13, 11, 5, 9, 11, 4, 0, 4, 0, 8, 0, 6, 2, 3, 8, 5, 8, 2, 8, 8, 6, 8, 8, 8, 12, 8, 5, 5, 5, 14, 13, 4, 2, 3, 9, 8]\n",
            "24 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.024647904559969902 0.2230224609375 3.7013540267944336 0.41108518838882446 1.2157845497131348 1.618037223815918\n",
            "repr, std, cov, clossl, z, norm 0.028036437928676605 0.228271484375 3.408921241760254 0.4354279935359955 1.3527742624282837 1.803377628326416\n",
            "repr, std, cov, clossl, z, norm 0.025847002863883972 0.23046875 3.316978693008423 0.4124022126197815 1.3277186155319214 1.7546449899673462\n",
            "repr, std, cov, clossl, z, norm 0.025975791737437248 0.2315673828125 3.2379136085510254 0.4409123957157135 1.3312314748764038 1.6378402709960938\n",
            "repr, std, cov, clossl, z, norm 0.0256448183208704 0.232177734375 3.217319965362549 0.4064624011516571 1.2857848405838013 1.8995352983474731\n",
            "repr, std, cov, clossl, z, norm 0.03494228422641754 0.2298583984375 3.341538429260254 0.38790246844291687 1.2743734121322632 2.028505563735962\n",
            "repr, std, cov, clossl, z, norm 0.02635328657925129 0.2230224609375 3.700814962387085 0.4468190670013428 1.3448596000671387 2.2826340198516846\n",
            "repr, std, cov, clossl, z, norm 0.033584173768758774 0.230712890625 3.2909796237945557 0.44645991921424866 1.2903645038604736 1.9476808309555054\n",
            "repr, std, cov, clossl, z, norm 0.03052740916609764 0.23095703125 3.2785284519195557 0.5454992651939392 1.3312662839889526 2.016977071762085\n",
            "repr, std, cov, clossl, z, norm 0.03999946266412735 0.23388671875 3.126436710357666 0.4804595410823822 1.2762075662612915 2.0871541500091553\n",
            "repr, std, cov, clossl, z, norm 0.03518514707684517 0.224853515625 3.591188430786133 0.47744834423065186 1.2834596633911133 2.1123359203338623\n",
            "repr, std, cov, clossl, z, norm 0.046341657638549805 0.226318359375 3.5178070068359375 0.48355501890182495 1.279280424118042 2.2077512741088867\n",
            "train_data.data 20390\n",
            "dided\n",
            "time\n",
            "[3, 9, 8, 5, 0, 4, 9, 8, 3, 3, 2, 8, 8, 3, 8, 8, 0, 3, 4, 8, 3, 14, 2, 5, 0, 0, 8, 14, 2, 6, 3, 9, 14, 8, 0, 9, 6, 8, 3, 5, 8, 11, 3, 2, 14]\n",
            "dided\n",
            "time\n",
            "[0, 4, 14, 4, 9, 8, 8, 14, 13, 3, 14, 9, 2, 1, 8, 8, 8, 8, 14, 13, 3, 4, 8, 3, 8, 14, 11, 3, 13, 11, 0, 8, 8, 5, 8, 3, 2, 0, 3, 3, 0, 8, 0, 8, 5, 8, 8, 0, 5, 11, 4, 14, 5, 2]\n",
            "dided\n",
            "time\n",
            "[2, 3, 11, 9, 8, 5, 4, 4, 4, 8, 0, 0, 0, 12, 2, 10, 3, 8, 5, 8, 8, 14, 13, 8, 8, 2, 8, 4, 0, 13, 0, 4, 4, 3, 14, 4, 8, 2, 8, 4, 8, 14, 8, 5, 3, 4, 3, 8, 2, 8, 8, 4, 2, 0, 3, 8, 8, 14, 14, 8, 8, 8, 0, 0, 11, 9, 4, 14, 2, 3, 3, 8, 3, 8, 2, 7, 8, 8, 3, 2, 8, 14, 8, 8, 13, 7, 14, 8, 0, 8, 8, 8, 4]\n",
            "dided\n",
            "time\n",
            "[3, 3, 8, 5, 4, 13, 5, 14, 3, 5, 8, 3, 5, 11, 8, 8, 4, 14, 9, 9, 0, 13, 8, 3, 13, 4, 0]\n",
            "dided\n",
            "time\n",
            "[4, 0, 3, 2, 13, 0, 6, 3, 6, 9, 8, 14, 0, 3, 5, 2, 0, 3, 14, 8, 8, 8, 9, 8, 3, 0, 2, 8, 8, 5, 14, 6, 8, 3, 8, 3, 0, 9, 2, 8, 8, 14, 9, 3, 14, 3, 3, 8, 8, 11, 0, 4, 8, 8, 8, 4, 9, 3, 8, 8, 5, 3, 5, 0, 4, 8, 8, 0, 11, 3, 5, 4, 5, 8, 2, 3, 0, 8, 8, 9, 14, 13, 8, 3, 8, 13, 8, 8, 3, 4, 8, 8, 9, 4, 0, 14]\n",
            "25 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.03701063245534897 0.231201171875 3.258319616317749 0.47457388043403625 1.3107072114944458 2.1034998893737793\n",
            "repr, std, cov, clossl, z, norm 0.04949948191642761 0.2294921875 3.3582370281219482 0.4508177638053894 1.3784891366958618 2.0436275005340576\n",
            "repr, std, cov, clossl, z, norm 0.03761470317840576 0.2296142578125 3.336409330368042 0.38842567801475525 1.3172645568847656 1.8408739566802979\n",
            "repr, std, cov, clossl, z, norm 0.05767747759819031 0.22802734375 3.418154716491699 0.5718715190887451 1.386889934539795 2.2220218181610107\n",
            "repr, std, cov, clossl, z, norm 0.02805946208536625 0.222900390625 3.7168068885803223 0.4837803244590759 1.3507007360458374 1.5681427717208862\n",
            "repr, std, cov, clossl, z, norm 0.0341489240527153 0.228271484375 3.4089431762695312 0.46100446581840515 1.3156962394714355 1.9124231338500977\n",
            "repr, std, cov, clossl, z, norm 0.022987820208072662 0.2322998046875 3.207927703857422 0.3994975686073303 1.2899467945098877 1.882367730140686\n",
            "repr, std, cov, clossl, z, norm 0.0509551502764225 0.2381591796875 2.966928243637085 0.4171815812587738 1.3347669839859009 1.669110894203186\n",
            "repr, std, cov, clossl, z, norm 0.020398693159222603 0.236328125 3.044895887374878 0.4126386344432831 1.2615079879760742 1.7760692834854126\n",
            "repr, std, cov, clossl, z, norm 0.02934098243713379 0.2354736328125 3.090085506439209 0.4841810166835785 1.2994786500930786 1.7976096868515015\n",
            "repr, std, cov, clossl, z, norm 0.027197277173399925 0.2294921875 3.36256742477417 0.47042885422706604 1.3279019594192505 1.7772778272628784\n",
            "repr, std, cov, clossl, z, norm 0.03984888643026352 0.2215576171875 3.7678065299987793 0.47373566031455994 1.3473737239837646 1.7732572555541992\n",
            "train_data.data 20494\n",
            "dided\n",
            "time\n",
            "[4, 0, 14, 8, 14, 8, 3, 9, 8, 2, 4, 11, 8, 0, 0, 8, 3, 8, 2, 14, 8, 8, 14, 14, 4, 8, 14, 2, 13, 8, 8, 3, 3, 3, 9, 4, 5, 5, 8, 8, 8, 0, 3, 14, 8, 8, 3, 9, 2, 2, 3, 8, 3, 8, 12, 13, 8, 5, 2, 4, 8, 14, 4, 5, 5, 0, 8, 2, 0, 0, 4, 0, 13, 5, 8]\n",
            "dided\n",
            "time\n",
            "[5, 8, 8, 8, 0, 4, 2, 8, 3, 9, 8, 5, 0, 3, 8, 3, 4, 11, 8, 2, 0, 8, 8, 2, 4, 6, 4, 3, 11, 4, 14, 0, 2, 11, 3, 3, 8, 0, 4, 3, 8, 8, 9, 8]\n",
            "dided\n",
            "time\n",
            "[8, 9, 8, 8, 5, 3, 13, 0, 3, 0, 8, 8, 5, 14, 2, 0, 2, 11, 2, 8, 8, 8, 3, 8, 13, 11, 8, 5, 8, 4, 2, 5, 4, 4, 8, 11, 3, 9, 8, 2, 0, 8, 8, 8, 8, 13, 8, 0, 3, 0, 3, 0, 8, 3, 0, 5, 8, 3, 13, 8, 11, 4, 8, 13, 4, 0, 5, 13, 5, 3, 3, 0, 4, 4, 11, 14, 11, 5, 13, 0]\n",
            "dided\n",
            "time\n",
            "[5, 13, 0, 8, 8, 3, 8, 14, 4, 4, 8, 2, 3, 3, 5, 12, 14, 4, 13, 3, 13, 5, 2, 0, 6, 3, 13, 8, 3, 8, 14, 8, 8, 0, 2, 11, 0, 11, 3, 0, 3, 3, 14, 3, 14, 8, 5, 0, 3, 3, 5, 3, 4, 5, 14, 5, 4, 3, 8, 2, 3, 11, 9, 3, 0, 9, 3, 3, 8, 0, 3, 9, 8, 13, 5, 3, 11, 0, 3, 3, 14, 5, 3, 0, 8, 8, 3, 0, 13, 3, 0, 3, 11]\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 14, 8, 9, 13, 3, 8, 8, 3, 5, 5, 6, 9, 3, 14, 3, 3, 5, 13, 3, 2, 2, 0, 4, 5, 14, 8, 0, 11, 8, 2, 4, 8, 8, 3, 3, 9, 3, 14, 5, 13, 8, 5, 3, 4, 4, 11, 5, 3, 8, 3, 0, 14, 9, 14, 9, 5, 13, 3, 0, 8, 0, 8, 13, 8, 3, 3, 0, 5, 11, 11, 0, 0, 4, 7, 8, 6, 8, 5, 14, 8, 5, 2, 4, 3, 9, 8, 3, 8, 8, 8, 0, 0, 3, 8, 2, 5, 8, 3, 6, 9, 0, 8, 3, 9, 2, 3, 2, 8, 3, 11, 8, 3, 8, 8, 9, 3, 0, 9, 4, 8, 13, 8, 11, 8, 8, 8, 9, 0, 6, 6, 2, 3, 13, 8, 8, 4, 11, 3, 4, 2, 8, 0, 14, 9, 8, 2, 8, 3, 9, 0, 4, 6, 3, 3, 6, 8, 8, 3, 8, 8, 14, 0, 2, 8, 14, 8, 3, 8, 8, 4, 8, 8, 4, 7, 14, 3, 8, 4, 8, 8, 6, 3, 0]\n",
            "26 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02379721589386463 0.2225341796875 3.688401222229004 0.5267829895019531 1.3613427877426147 1.7369550466537476\n",
            "repr, std, cov, clossl, z, norm 0.026794495061039925 0.229736328125 3.3504395484924316 0.4616785943508148 1.3183425664901733 1.6978027820587158\n",
            "repr, std, cov, clossl, z, norm 0.022178998216986656 0.227294921875 3.4558608531951904 0.5152245759963989 1.2883515357971191 1.7255586385726929\n",
            "repr, std, cov, clossl, z, norm 0.02344086579978466 0.229248046875 3.360377073287964 0.5211493372917175 1.279618263244629 1.6993705034255981\n",
            "repr, std, cov, clossl, z, norm 0.019063856452703476 0.22705078125 3.4887828826904297 0.5355604887008667 1.2219146490097046 1.623456597328186\n",
            "repr, std, cov, clossl, z, norm 0.021260101348161697 0.22607421875 3.546919822692871 0.4451504051685333 1.3063459396362305 1.6334339380264282\n",
            "repr, std, cov, clossl, z, norm 0.017973577603697777 0.2344970703125 3.1132800579071045 0.42259863018989563 1.3108819723129272 1.574385404586792\n",
            "repr, std, cov, clossl, z, norm 0.018595919013023376 0.23193359375 3.2337851524353027 0.4191119968891144 1.3803056478500366 1.5161817073822021\n",
            "repr, std, cov, clossl, z, norm 0.01769280433654785 0.2332763671875 3.178919792175293 0.4383815824985504 1.3683942556381226 1.5456457138061523\n",
            "repr, std, cov, clossl, z, norm 0.019029326736927032 0.230224609375 3.319291830062866 0.3825113773345947 1.3334290981292725 1.564587116241455\n",
            "repr, std, cov, clossl, z, norm 0.017018292099237442 0.2265625 3.4934940338134766 0.45227551460266113 1.2359315156936646 2.0317182540893555\n",
            "repr, std, cov, clossl, z, norm 0.020038437098264694 0.2255859375 3.5531718730926514 0.4943931996822357 1.3553874492645264 1.5574604272842407\n",
            "train_data.data 20649\n",
            "dided\n",
            "time\n",
            "[0, 11, 13, 8, 4, 8, 2, 8, 3, 0, 3, 0, 8, 11, 0, 8, 3, 2, 8, 4, 11, 14, 14, 3, 3, 0, 8, 3, 8, 5, 11, 9, 3, 3, 2, 8, 9, 8, 8, 0, 14, 8, 3, 3, 8, 9, 3, 8, 8, 2, 8, 9, 3, 8, 3, 0, 14, 14, 14, 3, 8, 8, 8, 5, 8, 3, 8, 8, 3, 14, 8, 9, 8, 11, 8, 0, 3, 5, 11, 3, 4, 3, 8, 9, 0, 8, 8, 3, 0, 11, 11, 4, 8, 13, 13, 8, 8, 8, 0, 3, 0, 8, 3, 8, 8, 0, 4]\n",
            "dided\n",
            "time\n",
            "[0, 4, 13, 8, 13, 9, 8, 9, 6, 8, 8, 8, 5, 5, 8, 8, 8, 14, 8, 3, 0, 5, 8, 14, 14, 5, 3, 8, 11, 8, 0, 3, 9, 8, 9, 4, 0, 14, 3, 8, 4, 3, 5, 6, 3, 3, 0, 14, 4, 4, 11, 2, 4, 4, 9, 10, 3, 3, 8, 4, 4, 9, 0, 2, 9, 8, 2, 13]\n",
            "dided\n",
            "time\n",
            "[8, 2, 13, 3, 2, 14, 13, 11, 9, 4, 8, 8, 4, 9, 9, 4, 12, 4, 12, 6, 9, 0, 14, 11, 9, 6, 14, 8, 4, 6, 6, 14, 13, 14, 0, 8, 4, 4, 8, 11, 9, 8, 6, 8, 5, 8, 8, 3, 14, 8, 8, 8, 3, 8, 8, 8, 9, 8, 4, 0, 9, 4, 5, 4, 4, 14, 3, 11, 3, 5, 0, 8, 5, 8, 2, 3, 3, 3, 13, 8, 8, 3, 2, 2, 1, 3, 4, 14, 8, 5, 4, 8, 4, 4, 8, 8, 8, 8, 8, 3, 9, 4, 8, 9, 5, 11, 8, 3, 4, 14, 3, 4]\n",
            "dided\n",
            "time\n",
            "[9, 3, 4, 3, 9, 9, 4, 8, 9, 8, 5, 8, 4, 8, 14, 11, 5, 8, 4, 8, 3, 11, 8, 8, 4, 8, 4, 11, 3, 3, 8, 5, 9, 8, 6, 13, 8, 0, 3, 13, 3, 14, 8, 9, 14, 2, 8, 11, 2, 8, 3, 3, 9, 4, 8, 14, 9, 6, 8, 2, 8, 7, 4, 11, 6, 14, 14, 3, 3, 8, 4, 14, 9, 14, 0, 3, 4, 5, 9, 8, 4, 8, 5, 13, 8, 9, 4, 3, 6, 5, 2, 8, 8, 8, 9, 0, 4, 11, 14, 2, 14, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 11, 5, 0, 8, 2, 0, 0, 5, 5, 3, 9, 4, 13, 5, 9, 0, 14, 13, 9, 13, 3, 4, 9, 9, 0, 8, 3, 8, 5, 2, 3, 14, 3, 5, 9, 6, 0, 2, 5, 5, 9, 4, 9, 3, 9, 3, 4, 8, 9, 4, 9, 8, 11, 2, 8, 8, 14, 5, 13, 4, 3, 14, 9, 14, 2, 4, 8, 3, 6, 11, 8, 4, 3, 5, 5, 11, 3, 0, 4, 11, 4, 11, 13, 3, 3, 4, 9, 11, 3, 0, 9, 3, 8, 6, 8, 3, 14, 8, 8, 4, 4, 3, 4, 4, 3, 8, 3, 5, 8, 8, 8, 3, 9, 3, 8]\n",
            "27 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.01841879077255726 0.2296142578125 3.360733985900879 0.4649631679058075 1.3014483451843262 1.4700942039489746\n",
            "repr, std, cov, clossl, z, norm 0.023475833237171173 0.2279052734375 3.4356181621551514 0.45710527896881104 1.3181370496749878 1.3696155548095703\n",
            "repr, std, cov, clossl, z, norm 0.018583066761493683 0.2276611328125 3.4823856353759766 0.4592522978782654 1.4001048803329468 1.8748183250427246\n",
            "repr, std, cov, clossl, z, norm 0.02406950853765011 0.2279052734375 3.434276580810547 0.4483586549758911 1.3131276369094849 1.8840276002883911\n",
            "repr, std, cov, clossl, z, norm 0.020343512296676636 0.232421875 3.218306541442871 0.42588332295417786 1.2464425563812256 1.478369951248169\n",
            "repr, std, cov, clossl, z, norm 0.02409624680876732 0.2315673828125 3.255371570587158 0.4492309093475342 1.286125898361206 1.8212605714797974\n",
            "repr, std, cov, clossl, z, norm 0.020694930106401443 0.22705078125 3.4794044494628906 0.5341665744781494 1.2191118001937866 1.6515188217163086\n",
            "repr, std, cov, clossl, z, norm 0.024300817400217056 0.22509765625 3.5793356895446777 0.5911680459976196 1.3896896839141846 2.07415771484375\n",
            "repr, std, cov, clossl, z, norm 0.02170434407889843 0.225341796875 3.569383144378662 0.503913938999176 1.272733211517334 1.9937058687210083\n",
            "repr, std, cov, clossl, z, norm 0.027342259883880615 0.2275390625 3.4800519943237305 0.43107810616493225 1.4307961463928223 1.9183398485183716\n",
            "repr, std, cov, clossl, z, norm 0.021681467071175575 0.236572265625 3.043321132659912 0.41569647192955017 1.3643990755081177 1.7460455894470215\n",
            "repr, std, cov, clossl, z, norm 0.027275847271084785 0.2279052734375 3.4508190155029297 0.46312201023101807 1.2852996587753296 1.9193638563156128\n",
            "train_data.data 21111\n",
            "dided\n",
            "time\n",
            "[3, 9, 5, 0, 5, 3, 14, 8, 13, 5, 8, 11, 5, 8, 8, 3, 5, 0, 3, 8, 5, 8, 11, 3, 3, 3, 8, 11, 14, 5, 3, 13, 8, 8, 5, 3, 14, 9, 5, 4, 3, 2, 3, 8, 5, 5, 0, 2, 13, 8, 4, 3, 3, 5, 2, 3, 14, 3, 3, 3, 14, 14, 3, 8, 5, 3, 8, 8, 5, 3, 8, 14, 14, 3, 8, 3, 8, 8, 14, 5, 5, 8, 0, 4, 4, 5, 5, 5, 2]\n",
            "dided\n",
            "time\n",
            "[3, 0, 14, 8, 8, 2, 2, 4, 6, 0, 0, 5, 2, 8, 8, 3, 9, 8, 8, 6, 8, 14, 8, 4, 8, 8, 8, 8, 3, 3, 11, 2, 11, 6, 8, 2, 8, 8, 8, 8, 2, 14, 13, 5, 2, 3, 3, 2, 14, 4, 8, 14, 14, 5, 8, 8, 4, 0, 2, 3, 0, 3, 3, 8, 3, 0, 2, 8, 3, 8, 4, 13, 8, 2, 5, 14, 2, 13, 5, 8, 0, 4, 0, 0, 8, 3, 8, 4, 2, 0, 8, 8, 8, 8, 14, 5, 3, 3, 8, 3, 3, 14, 3, 14, 4, 3, 5, 3, 13, 3, 8, 3, 5, 3, 14, 3, 13, 14, 13, 10, 8, 3, 3, 3, 11, 5, 11, 14, 0, 11, 8, 8, 4, 4, 2, 2, 8, 14, 3, 4, 0, 5, 11, 3, 2, 13, 8, 11, 11, 9, 3, 8, 13, 13, 0, 5, 3, 5, 4, 8, 13, 13, 3, 11, 13, 11, 0, 8, 5, 3, 4, 0, 8, 8, 14, 3, 5, 11, 3, 9, 11, 0, 0, 3, 14, 11, 8, 0, 14, 8, 8, 5, 8, 8, 2, 4, 4, 3, 8, 8, 9, 8, 2, 8, 13, 8]\n",
            "dided\n",
            "time\n",
            "[8, 5, 8, 5, 3, 3, 3, 8, 3, 2, 9, 14, 14, 3, 3, 8, 0, 14, 3, 14, 8, 11, 4, 4, 13, 5, 8, 8, 8, 14, 5, 14, 4, 8, 3, 3, 11, 5, 13, 5, 9, 3, 8, 3, 3, 3, 0, 0, 0, 8, 4, 3, 2, 3, 2, 8, 5, 4, 0, 0, 3, 3, 14, 5, 8, 14, 8, 5, 8]\n",
            "dided\n",
            "time\n",
            "[5, 3, 3, 8, 3, 3, 4, 0, 3, 2, 14, 3, 8, 8, 3, 8, 8, 8, 13, 3, 5, 4, 13, 8, 6, 8, 14, 8, 6, 9, 14, 14, 5, 3, 14, 8, 13, 3, 11, 6, 8, 5, 8, 14, 0, 8, 13, 8, 0, 13, 8, 11, 0, 8, 2]\n",
            "dided\n",
            "time\n",
            "[8, 2, 14, 3, 4, 11, 8, 8, 8, 4, 2, 7, 5, 8, 13, 8, 13, 0, 8, 9, 14, 6, 14, 8, 2, 8, 0, 11, 13, 8, 0, 3, 8, 14, 3, 8, 11, 0, 3, 11, 14, 0, 5, 2, 0, 2, 11, 11, 8, 8, 5, 8, 8, 8, 4, 8, 2]\n",
            "28 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.020987696945667267 0.2266845703125 3.508538007736206 0.4265114963054657 1.3396061658859253 2.3164641857147217\n",
            "repr, std, cov, clossl, z, norm 0.024953018873929977 0.23193359375 3.237173318862915 0.4317145347595215 1.3731818199157715 2.1052756309509277\n",
            "repr, std, cov, clossl, z, norm 0.02079167775809765 0.2288818359375 3.3971800804138184 0.4212593138217926 1.26012122631073 1.9103931188583374\n",
            "repr, std, cov, clossl, z, norm 0.022995855659246445 0.2310791015625 3.2669320106506348 0.4050820767879486 1.3431206941604614 1.8580576181411743\n",
            "repr, std, cov, clossl, z, norm 0.02038145810365677 0.2294921875 3.3403725624084473 0.48781904578208923 1.2739883661270142 1.9503915309906006\n",
            "repr, std, cov, clossl, z, norm 0.024179605767130852 0.22607421875 3.5315918922424316 0.44509801268577576 1.4157648086547852 2.0605506896972656\n",
            "repr, std, cov, clossl, z, norm 0.021964630112051964 0.2220458984375 3.761442184448242 0.4633539915084839 1.3328526020050049 1.4658037424087524\n",
            "repr, std, cov, clossl, z, norm 0.027426524087786674 0.2264404296875 3.537684202194214 0.4313427209854126 1.3060301542282104 1.8141334056854248\n",
            "repr, std, cov, clossl, z, norm 0.024817006662487984 0.2303466796875 3.3631296157836914 0.4349774122238159 1.249772548675537 1.662079095840454\n",
            "repr, std, cov, clossl, z, norm 0.026666175574064255 0.2314453125 3.2828564643859863 0.470552533864975 1.2648862600326538 1.4886819124221802\n",
            "repr, std, cov, clossl, z, norm 0.020515108481049538 0.2342529296875 3.1390364170074463 0.4788002371788025 1.3686423301696777 1.3659600019454956\n",
            "repr, std, cov, clossl, z, norm 0.02667139284312725 0.2291259765625 3.384145498275757 0.38581734895706177 1.3348289728164673 1.5239684581756592\n",
            "train_data.data 21323\n",
            "dided\n",
            "time\n",
            "[5, 8, 8, 6, 11, 3, 8, 2, 4, 0, 13, 8, 3, 3, 3, 13, 11, 8, 5, 2, 2, 14, 14, 8, 9, 3, 14, 14, 3, 14, 9, 11, 2]\n",
            "dided\n",
            "time\n",
            "[8, 2, 14, 5, 8, 0, 8, 8, 8, 3, 4, 8, 5, 2, 2, 0, 3, 8, 3, 8, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 0, 8, 9, 3, 8, 8, 11, 13, 8, 9, 11, 11, 13, 3, 11, 3, 3, 13, 2, 8, 4, 2, 0, 5, 2, 4, 5, 2, 8]\n",
            "dided\n",
            "time\n",
            "[5, 2, 8, 3, 6, 13, 4, 13, 3, 8, 4, 8, 14, 13, 8, 8, 8, 8, 4, 14, 4, 4, 13, 0, 8, 8, 8, 8, 2, 14, 8, 14, 3, 9, 3, 14, 3, 9, 3, 8, 8, 4, 9, 8, 2, 3, 4, 14, 2, 14, 2, 8, 11, 8, 1, 4, 13, 5, 9, 4, 0, 4, 2, 9]\n",
            "dided\n",
            "time\n",
            "[4, 2, 9, 11, 14, 10, 8, 14, 14, 0, 3, 3, 4, 8, 3, 8, 8, 4, 2, 8, 5, 8, 3, 2, 2, 8, 8, 3, 13, 8, 14, 5, 3, 8, 3, 2, 9, 8, 4, 8, 3, 13, 8, 13, 3, 0, 0, 8, 8, 2, 5, 14, 11, 8, 5, 11, 14, 8, 3, 9, 3, 2, 8, 8, 2, 2, 8, 3, 3, 8, 6, 2, 8, 5, 8, 5, 14, 5, 5, 11, 8, 3, 9, 3, 13, 8, 4, 14, 3, 3, 3, 0, 3, 8, 0, 8, 2, 13, 8, 3]\n",
            "29 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.021770944818854332 0.2264404296875 3.5072193145751953 0.43344029784202576 1.310856819152832 1.369931697845459\n",
            "repr, std, cov, clossl, z, norm 0.027471289038658142 0.224609375 3.612905502319336 0.5068073272705078 1.1580207347869873 1.5093032121658325\n",
            "repr, std, cov, clossl, z, norm 0.0220493171364069 0.2332763671875 3.204843044281006 0.4027479588985443 1.2936636209487915 1.6034127473831177\n",
            "repr, std, cov, clossl, z, norm 0.025327539071440697 0.2340087890625 3.1614251136779785 0.40424466133117676 1.3781753778457642 1.3302680253982544\n",
            "repr, std, cov, clossl, z, norm 0.021977560594677925 0.2335205078125 3.155837059020996 0.4709432125091553 1.3158124685287476 1.625071406364441\n",
            "repr, std, cov, clossl, z, norm 0.023118607699871063 0.2313232421875 3.269291400909424 0.42773446440696716 1.256413459777832 1.2356858253479004\n",
            "repr, std, cov, clossl, z, norm 0.019976038485765457 0.2294921875 3.3560123443603516 0.4385949671268463 1.3552229404449463 1.44846510887146\n",
            "repr, std, cov, clossl, z, norm 0.03140540048480034 0.2261962890625 3.5214619636535645 0.4365968406200409 1.4420056343078613 1.183523416519165\n",
            "repr, std, cov, clossl, z, norm 0.02232544869184494 0.224853515625 3.592114210128784 0.44299647212028503 1.4148402214050293 1.4849271774291992\n",
            "repr, std, cov, clossl, z, norm 0.03191094845533371 0.224853515625 3.575329303741455 0.4512440264225006 1.342373251914978 1.5122851133346558\n",
            "repr, std, cov, clossl, z, norm 0.020772689953446388 0.22998046875 3.316129207611084 0.4315887987613678 1.3275035619735718 1.5180906057357788\n",
            "repr, std, cov, clossl, z, norm 0.031139571219682693 0.2271728515625 3.461928367614746 0.5656669735908508 1.3641165494918823 1.483857274055481\n",
            "train_data.data 20384\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(30):\n",
        "    # print(\"#### simulate ####\")\n",
        "    # buffer_=[]\n",
        "    for _ in range(5):\n",
        "        buffer = simulate(agent, buffer)\n",
        "        # buffer_ = simulate(agent, buffer_)\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # torch.save(checkpoint, folder+'agentoptim1.pkl')\n",
        "\n",
        "    # buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "    # with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "    print(\"train_data.data\",len(train_data.data))\n",
        "    while len(train_data.data)>20000: # 10000:6.9gb, 20000:5.5gb\n",
        "        buffer.pop(random.randrange(len(buffer)))\n",
        "        train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "b8zxYU9jpE8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "9a3cbf99-0034-44f6-d61c-1af446e408c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772,
          "referenced_widgets": [
            "ac79a874852a47d2b6c2ff83806aba05",
            "a977c0be35ff41b09d034eaaa91d4da2",
            "a5e76a14f46b4c9a8a41da8f027b730a",
            "ec9c7426e9bf42d7ae0c96f458401efd",
            "ce20d6ecc8dc4b1893a03f9bcf866050",
            "a128d10441c24c6b8f470f9f76cd78ea",
            "1f8c7a8e6b884f71873b4a236a3f6984",
            "8c5e762b3f544b67b4408b521cfdf1cd"
          ]
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "11068300-f8ac-4d8c-8161-f9629636fc9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:pgn8scjd) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac79a874852a47d2b6c2ff83806aba05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▂█▂▄▂▅▃▅▁▂▃▃▄▂▃▂▂▁▃▂▃▂▃▃▁▆▄▅▃▂▃▃▂▂▃▃▁▃▃▁</td></tr><tr><td>cov</td><td>▄▄▁▂▇▆▄▆█▅▆▇▇▅▃▂▄▅▇▇▆▅▅▄▃▃▃▃▄▅▄▃▃▃▄▄▄▅▃▄</td></tr><tr><td>repr</td><td>▆▄▄▆▁▅█▅█▃▄▆▆▆▇▆▄▅▄▆▆▅▅▅▇▅▃▄▅▄▄▃▄▃▃▃▄▁▄▅</td></tr><tr><td>std</td><td>▁▁█▂▁▂▁▁▁▁▁▂▄▃▂▁▁▂▁▁▁▁▁▁▃▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>0.35301</td></tr><tr><td>cov</td><td>332.71164</td></tr><tr><td>repr</td><td>0.0125</td></tr><tr><td>std</td><td>0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">giddy-dew-73</strong> at: <a href='https://wandb.ai/bobdole/procgen/runs/pgn8scjd' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/pgn8scjd</a><br/> View project at: <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240926_043504-pgn8scjd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:pgn8scjd). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240926_043827-lls9gx19</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/lls9gx19' target=\"_blank\">restful-aardvark-74</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/lls9gx19' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/lls9gx19</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(project=\"procgen\",\n",
        "    config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        _,T,_ = sx.shape\n",
        "        batch = 1\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            # print(i,x.data, z.squeeze(), loss.item())\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mY7BITKjSKC",
        "outputId": "cbad0ffc-e8e9-4a4d-bd3c-d6d9ddaf6e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2336301\n",
            "1278976\n",
            "399360\n",
            "1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-76-e20d23bca149>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=3, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # e = d_model**-0.5\n",
        "        # self.h0 = torch.empty((self.jepa.pred.num_layers, 1, d_model), device=device).uniform_(-e, e) # [num_layers, batch, d_model]\n",
        "        # self.h0 = torch.normal(mean=0, std=e, size=(self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # torch.nn.init.xavier_uniform_(self.h0) # xavier_uniform_, kaiming_normal_\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "        state = torch.zeros((1, 3,64,64), device=device)\n",
        "        self.sx = self.jepa.enc(state)\n",
        "\n",
        "    # def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        self.update_h0(lstate, laction)\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "            # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.h0=lh0[k].unsqueeze(1) # [num_layers, 1, d_model]\n",
        "        # self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        self.lx, self.lz = lx[k:], lz[k:] # [T, dim_a], [T, dim_z]\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "\n",
        "                # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "                # print(torch.cat(lstate, dim=0).shape)\n",
        "                # lsx = self.jepa.enc(torch.stack(lstate, dim=0))#.unsqueeze(0)\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx-torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                # batch, seq_len, _ = lstate.shape\n",
        "                # seq_len, _ = lstate.shape\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    try: la = self.emb(self.la[:seq_len])\n",
        "                    except:\n",
        "                        print(\"err self.la\")\n",
        "                        # la = self.emb([0]*seq_len)\n",
        "                        la = self.emb(torch.zeros(seq_len, dtype=int, device=device))\n",
        "\n",
        "        # lz = nn.Parameter(torch.zeros((batch, seq_len, self.dim_z),device=device))\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        # optim_z = torch.optim.SGD([lz], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([lz], 1e0, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "\n",
        "        for i in range(20): # num epochs\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                # print(sxaz.shape, self.h0.shape)\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                # sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                loss = F.mse_loss(out_, out)\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            print(lz.data)\n",
        "            with torch.no_grad(): lz.clamp_(min=-1, max=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1]\n",
        "        self.la = la[k:]\n",
        "        return h0\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                # x[:,:self.lx.shape[1]], z[:,:self.lz.shape[1]] = self.lx[:,:T], self.lz[:,:T]\n",
        "                x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].repeat(batch,1,1), self.lz[:T].repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            # print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            # print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i, \"loss\", loss.squeeze().data)\n",
        "            # print(x.shape,torch.argmax(-dist,dim=-1).shape,z.shape,loss.shape) # [16, 6, 3], [16, 6], [16, 6, 1], [16, 1]\n",
        "            # print(i, torch.cat([x,torch.argmax(-dist,dim=-1),z],dim=-1).squeeze().data)\n",
        "            print(i, \"x act z\", torch.cat([x[0],torch.argmax(-dist,dim=-1)[0].unsqueeze(-1),z[0]],dim=-1).squeeze().data)\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, t, dim_a], [batch, t, dim_z]\n",
        "\n",
        "\n",
        "    def search_optimxz(self, sx, T=6, h0=None):\n",
        "        self.eval()\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 4 # 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1 ; sgd\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_uniform_(z) # xavier_normal_, xavier_uniform_\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        # optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e1, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].unsqueeze(0).repeat(batch,1,1), self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        print(\"search\", z[0].squeeze())\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        for i in range(10): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.sum().backward()\n",
        "            # optim_x.step(); optim_z.step()\n",
        "            # optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            print(i, \"search loss\", loss.squeeze().data)\n",
        "            # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "            print(i, \"search z\", z[0].squeeze().data)\n",
        "            # print(torch.argmin(dist,dim=-1).int())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        print(\"c\",torch.stack(c)[:,idx])\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        c=[]\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            # print(sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            c.append(tcost)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    # imshow(state[0].cpu())\n",
        "                    # print(\"norm\", torch.norm(sy[0]-sy_[0], dim=-1))\n",
        "                    # # if torch.norm(sy[0]-sy_[0], dim=-1) > 1:\n",
        "                    # print(i, reward[0])\n",
        "                    # print(sy)\n",
        "                    # print(sy_)\n",
        "                    # print(sy[0]-sy_[0])\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "\n",
        "                    # cost loss\n",
        "                    # syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = 100*clossl\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batc h_size, d_model]\n",
        "            # sx=sy_\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "\n",
        "                    # z = self.jepa.argm(sy_, a, sy)\n",
        "                    z = self.argm(sy, sy_, h0, a, reward)\n",
        "                    with torch.no_grad(): z.mul_(torch.rand_like(z)).mul_((torch.rand_like(z)>0.5).bool()) # dropout without scailing\n",
        "\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "\n",
        "                    # cost loss\n",
        "                    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    # syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    # print(h0.requires_grad)\n",
        "                    # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "                    # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "                    # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    # torch.norm(sy-sx, dim=-1)\n",
        "                    # sx=sy\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    norm = torch.norm(sy, dim=-1)[0].item()\n",
        "                    z_norm = torch.norm(z)\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "\n",
        "                # lh0 = torch.zeros((rwd.shape[1],)+h0.shape, device=device)\n",
        "                # lz = torch.zeros((lsy.shape[0], lsy.shape[1], self.dim_z), device=device)\n",
        "                    # for name, param in agent.tcost.named_parameters():\n",
        "                    #     print(\"param.data\",param.max().item(),param.min().item())\n",
        "                    #     print(\"agent.tcost\",param.data)\n",
        "\n",
        "\n",
        "# # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "# ema_model = torch.optim.swa_utils.AveragedModel(model, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n",
        "# for epoch in range(300):\n",
        "#       for input, target in loader:\n",
        "#           optimizer.zero_grad()\n",
        "#           loss_fn(model(input), target).backward()\n",
        "#           optimizer.step()\n",
        "#           ema_model.update_parameters(model)\n",
        "# # Update bn statistics for the ema_model at the end\n",
        "# torch.optim.swa_utils.update_bn(loader, ema_model)\n",
        "# # Use ema_model to make predictions on test data\n",
        "# preds = ema_model(test_input)\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "\n",
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cg0BI2TwY9-p"
      },
      "outputs": [],
      "source": [
        "# @title z.grad.data = -z.grad.data\n",
        "\n",
        "# self.eval()\n",
        "batch = 4 # 16\n",
        "x = nn.Parameter(torch.empty((batch, T, agent.dim_a),device=device))\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# optim_ = torch.optim.SGD([x,z], lr=1e1) # 3e3\n",
        "optim_ = torch.optim.AdamW([x,z], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "print(\"search z\", z[0].squeeze())\n",
        "print(\"search x\", x[0].squeeze())\n",
        "sx, h0 = sx.detach(), h0.detach()\n",
        "for i in range(10): # num epochs\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    z.grad.data = -z.grad.data\n",
        "    optim_.step()\n",
        "    optim_.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "    print(i, \"search loss\", loss.squeeze().data)\n",
        "    print(i, \"search z\", z[0].squeeze().data)\n",
        "    print(i, \"search x\", x[0].squeeze().data)\n",
        "dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "print(\"c\",torch.stack(c)[:,idx])\n",
        "# return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "# print(lact[idx], lh0[:,:,idx,:], x[idx], z[idx])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5VMebkQ1mJtD"
      },
      "outputs": [],
      "source": [
        "# @title argm agent.rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def argm(sx, x,h0, lr=3e3): # 3e3\n",
        "    # agent.eval()\n",
        "    # batch_size, T, _ = sx.shape\n",
        "    batch = 16 # 16\n",
        "    z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "    torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "    optim_z = torch.optim.SGD([z], lr=1e3, maximize=True) # 3e3\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.LBFGS([z], max_iter=5, lr=1)\n",
        "\n",
        "    # print(\"argm\", z[0].squeeze())\n",
        "    sx, h0 = sx.detach(), h0.detach()\n",
        "    x = x.detach().repeat(batch,1,1)\n",
        "    for i in range(5): # num epochs\n",
        "        # print(sx.shape, x.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "        loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "        loss.sum().backward()\n",
        "        optim_z.step()\n",
        "        optim_z.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "        # print(i, \"argm loss\", loss.squeeze().data)\n",
        "        # print(i, \"argm z\", z[0].squeeze().data)\n",
        "    idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "    return z[idx].unsqueeze(0)\n",
        "\n",
        "\n",
        "T=1\n",
        "xx = torch.empty((1, T, agent.dim_a))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "# print(x.shape)\n",
        "optim_x = torch.optim.SGD([x], lr=1e1) # 1e-1,1e-0,1e4 ; 1e2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "\n",
        "state = torch.zeros((1, 3,64,64))\n",
        "with torch.no_grad():\n",
        "    sx = agent.jepa.enc(state).detach()\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "print(time.time()-start)\n",
        "\n",
        "print(\"search\",x.squeeze().data)\n",
        "for i in range(20): # 5\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    z = argm(sx, x_,h0)\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [1, 1, 3], [1, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "    print(i, \"search loss\", x.squeeze().data, loss.item())\n",
        "    # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "\n",
        "# z sgd 1e3\n",
        "# 9 search loss tensor([0.0142, 0.0142, 0.0142, 0.0142])\n",
        "# 9 search z tensor([-0.3381, -0.7005, -0.5877, -0.0664, -0.1439,  0.0283,  0.0541, -0.1439])\n",
        "\n",
        "# x sgd 1e2\n",
        "# 1 tensor([0.3561, 0.3059, 0.8830]) 0.014148875139653683\n",
        "# 9 tensor([0.3560, 0.3064, 0.8828]) 2.328815611463142e-07\n",
        "\n",
        "# 1e0\n",
        "# 19 tensor([-0.5768,  0.5778,  0.5774]) 6.543130552927323e-07\n",
        "# 19 tensor([0.3570, 0.6689, 0.6521]) 2.474381801675918e-07\n",
        "# 19 tensor([0.5783, 0.5765, 0.5772]) 1.519319567933053e-07\n",
        "# 19 tensor([0.3427, 0.6795, 0.6487]) 4.220427456402831e-07\n",
        "#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gcvgdCB1h1_E"
      },
      "outputs": [],
      "source": [
        "# @title torch.optim.LBFGS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Example of a deep nonlinear model f(x)\n",
        "class DeepNonlinearModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNonlinearModel, self).__init__()\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(10, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "f = DeepNonlinearModel()\n",
        "# x = torch.randn(1, 10, requires_grad=True)\n",
        "# xx = torch.randn((1,10))\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "\n",
        "# Define loss function (mean squared error for this example)\n",
        "target = torch.tensor([[0.0]])  # Target output\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "def closure():\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    return loss\n",
        "\n",
        "optimizer = torch.optim.LBFGS([x], lr=1.0, max_iter=5)  # Limit to 2-3 iterations for speed\n",
        "start_time = time.time()\n",
        "for _ in range(2):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step(closure)  # Perform a step of optimisation\n",
        "    print(loss.item())\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "start_time = time.time()\n",
        "optimizer = torch.optim.SGD([x], lr=1e1, maximize=True) # 3e3\n",
        "for _ in range(5):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step()  # Perform a step of optimisation\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    print(loss.item())\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "# optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def argm(self, sx, lr=3e3): # 3e3\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch_size, T, _ = sx.shape\n",
        "        batch = 16\n",
        "        # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "        z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "        optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "        sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "        # sx = sx.detach()\n",
        "        for i in range(20): # 10\n",
        "            # print(sx.shape,z.shape)\n",
        "            sxz = torch.cat([sx, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                cost = model(sxz)\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "            # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "            # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        # return z.detach()\n",
        "        # print(\"argm z\",z.squeeze().data)\n",
        "        # print(\"cost\",cost.squeeze())\n",
        "        idx = torch.argmax(loss)\n",
        "        # return z[idx].detach().unsqueeze(0)\n",
        "        return z[:,idx].detach()\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=1\n",
        "        T=1\n",
        "        x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "\n",
        "        lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "        # ratio = 6e0\n",
        "        lr = 1e-1 # adamw 1e-1\n",
        "        ratio = 4\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        # optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze())\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "        for i in range(50):\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = argm(x)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "            # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "            # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                x.clamp_(min=-1, max=1)\n",
        "            # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "        idx = torch.argmax(loss)\n",
        "        print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd):\n",
        "        self.tcost.eval()\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "            lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "                for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                    syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                    out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                    lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                    lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "                repr_loss = F.mse_loss(lsy, lsy_)\n",
        "                syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "                # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "                cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        return lz.detach()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HcOidvtW9KAH"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx0k_ndHOEMe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "N2TGs69fnrZo",
        "outputId": "e7624553-e17a-4a9f-85a4-512720ed329a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcost.1.weight torch.Size([2, 512])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU(267, 256, batch_first=True, dropout=0.2)\n"
          ]
        }
      ],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "# for name, param in agent.emb.named_parameters():\n",
        "for name, param in agent.tcost.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjEWGq2WGi9a",
        "outputId": "649e3612-f156-496e-d8d5-fc576110e2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0015, -0.0132,  0.0280,  ...,  0.0297,  0.0289,  0.0152],\n",
            "        [ 0.0168,  0.0031, -0.0288,  ..., -0.0064, -0.0137, -0.0085]])\n"
          ]
        }
      ],
      "source": [
        "# print(vars(agent.jepa.pred.))\n",
        "# print(vars(agent.tcost.state_dict()))\n",
        "# print(agent.jepa.pred._parameters.keys())\n",
        "# print(agent.jepa.pred._parameters['weight_ih_l0'])\n",
        "# print(agent.jepa.pred._parameters['weight_hh_l2']) # weight_hh_l0, weight_hh_l2\n",
        "# print(agent.tcost.state_dict().keys())\n",
        "print(agent.tcost.state_dict()['tcost.1.weight']) # tcost.2.bias, tcost.4.bias\n",
        "# print(agent.tcost.named_parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_xnBFjXVxgz"
      },
      "outputs": [],
      "source": [
        "# @title transfer weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 10  # size of the first dimension\n",
        "a = 5   # size of the extra nodes to omit\n",
        "m = 8   # output dimension\n",
        "\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "target_layer = nn.Linear(d, m)\n",
        "# source_layer = nn.Linear(d, m)\n",
        "# target_layer = nn.Linear(d+a, m)\n",
        "\n",
        "def transfer(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt.weight[:, :src.weight.shape[1]].copy_(src.weight[:, :tgt.weight.shape[1]])\n",
        "        tgt.bias.copy_(src.bias)\n",
        "    return tgt,src\n",
        "\n",
        "target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "src_sd = source_layer.state_dict()\n",
        "tgt_sd = target_layer.state_dict()\n",
        "\n",
        "def transfersd(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt['weight'][:, :src['weight'].shape[1]].copy_(src['weight'][:, :tgt['weight'].shape[1]])\n",
        "        tgt['bias'].copy_(src['bias'])\n",
        "    return tgt\n",
        "\n",
        "tgt_sd = transfersd(tgt_sd, src_sd)\n",
        "target_layer.load_state_dict(tgt_sd)\n",
        "\n",
        "\n",
        "agent_src = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "\n",
        "# agent.tcost = TCost((1+agent.jepa.pred.num_layers)*agent.d_model) # replace tcost\n",
        "\n",
        "agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# agent.jepa.pred\n",
        "# target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(vars(agent.jepa.pred))\n",
        "# gru = agent.jepa.pred\n",
        "# gru = agent_src.jepa.pred\n",
        "# for wht_name in gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, gru._parameters[wht_name].shape)\n",
        "\n",
        "# weight_ih_l0 dim_z=3: [768, 262] , dim_z=1: [768, 260]\n",
        "# weight_hh_l0 torch.Size([768, 256])\n",
        "# bias_ih_l0 torch.Size([768])\n",
        "# bias_hh_l0 torch.Size([768])\n",
        "\n",
        "# tgt_gru = agent.jepa.pred\n",
        "# src_gru = agent_src.jepa.pred\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "\n",
        "tgt_gru[]\n",
        "def transfer_gru(tgt_gru, src_gru): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(len(tgt_gru._all_weights), len(src_gru._all_weights))):\n",
        "        # for lyr in tgt_gru._all_weights:\n",
        "            lyr = tgt_gru._all_weights[i]\n",
        "            for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "                # print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "                tgt_wht, src_wht = tgt_gru._parameters[wht_name], src_gru._parameters[wht_name]\n",
        "                if len(tgt_wht.shape)==2:\n",
        "                    tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "                elif len(tgt_wht.shape)==1:\n",
        "                    tgt_gru._parameters[wht_name] = src_wht\n",
        "    return tgt_gru\n",
        "tgt_gru = transfer_gru(tgt_gru, src_gru)\n",
        "\n",
        "# for wht_name in tgt_gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d_model=256; dim_a=3; dim_z=1; dim_v=512\n",
        "\n",
        "pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "# pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "print(pred._all_weights)\n",
        "for lyr in pred._all_weights:\n",
        "    for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "        print(wht_name, pred._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(pred.state_dict().keys())\n",
        "\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "print(tgt_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "print(src_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "\n",
        "print(tgt_gru.state_dict()['bias_ih_l0'][:10])\n",
        "print(src_gru.state_dict()['bias_ih_l0'][:10])\n",
        "tgt_gru.state_dict().keys()\n",
        "src_gru.state_dict().keys()\n",
        "\n",
        "# tgt_gru\n",
        "# src_gru\n",
        "for wht_name in tgt_gru.state_dict().keys():\n",
        "    if not wht_name in src_gru.state_dict().keys(): continue\n",
        "    print(wht_name)\n",
        "    # print(tgt_gru.state_dict()[wht_name])\n",
        "    # tgt_gru.state_dict()[wht_name].copy_(src_gru.state_dict()[wht_name])\n",
        "\n",
        "tgt_sd = tgt_gru.state_dict()\n",
        "src_sd = src_gru.state_dict()\n",
        "def transfer_sd(tgt_sd, src_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            # print(wht_name)\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            if len(tgt_wht.shape)==2:\n",
        "                tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "            elif len(tgt_wht.shape)==1:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "    return tgt_sd\n",
        "tgt_sd = transfer_sd(tgt_sd, src_sd)\n",
        "print(tgt_sd['weight_ih_l0'][0][:10])\n",
        "print(tgt_sd['bias_ih_l0'][:10])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CwApoQMMKzB",
        "outputId": "98f67f91-ef5b-406f-b852-5a93130f9e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0012018680572509766\n",
            "tensor([0.2797, 0.2218, 0.2731, 0.3268, 0.2632, 0.2914, 0.3217, 0.2845])\n"
          ]
        }
      ],
      "source": [
        "# @title test init norm\n",
        "print(agent.emb.state_dict()['weight'].norm(dim=-1))\n",
        "\n",
        "# x = torch.rand(16)\n",
        "x = torch.rand(8,16)\n",
        "# print(x)\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1.0)\n",
        "# torch.nn.init.xavier_normal_(x)\n",
        "import time\n",
        "start = time.time()\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # 0.00966, 0.000602, 0.0004\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1./x.shape[-1]**0.5)\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "print(time.time()-start)\n",
        "# std = ((Sum (xi-mean)^2)/ N)^(1/2)\n",
        "# print(x)\n",
        "# print(((x**2).sum())**(0.5))\n",
        "print(torch.norm(x, dim=-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B1yvJkX89C_o"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein\n",
        "import torch\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    # cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    # dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # cs = (x-y).cumsum(dim=-1)\n",
        "    cs = (x-y) @ torch.tril(torch.ones(x.shape[0], x.shape[0]))\n",
        "    # dist = weight * torch.abs(cs)\n",
        "    dist = weight * cs**2\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "\n",
        "def soft_wasserstein_loss(x, y, smoothing=0.1):\n",
        "    # Normalise distributions\n",
        "    x = x / x.sum()\n",
        "    y = y / y.sum()\n",
        "    # Compute the cumulative distributions (CDFs) with a small smoothing factor\n",
        "    cdf_x = torch.cumsum(x, dim=-1) + smoothing\n",
        "    cdf_y = torch.cumsum(y, dim=-1) + smoothing\n",
        "    # Compute smooth Wasserstein distance (L2 distance between CDFs)\n",
        "    distance = torch.norm(cdf_x - cdf_y, p=2)  # L2 distance instead of L1 for smoother gradients\n",
        "    return distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5], dtype=float))\n",
        "x = nn.Parameter(torch.tensor([-0.01, -0.0, -0.99], dtype=torch.float))\n",
        "y = torch.tensor([0.0, 0.0, -1.0], dtype=torch.float)\n",
        "\n",
        "# x = nn.Parameter(torch.rand(1024, dtype=float))\n",
        "# y = torch.rand(1024, dtype=float)\n",
        "# a = len(train_data.buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "a=1/45\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "print(weight)\n",
        "dist = wasserstein(x, y, weight=weight)\n",
        "print(time.time() - start)\n",
        "print(dist)  # Should output 0.7\n",
        "# dist.backward()\n",
        "\n",
        "# 0.0004496574401855469\n",
        "# 0.000331878662109375\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3nfZRhVc9Ssp"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein sinkhorn train\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# agent.eval()\n",
        "# batch_size, T, _ = sx.shape\n",
        "x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3) # 3e3\n",
        "# optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.999)) # ? 1e0 ; 3e-2 1e-1\n",
        "# optim = torch.optim.AdamW([x], 1e-0, (0.9, 0.95)) # ? 1e0 ; 3e-2 1e-1\n",
        "y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=torch.float)\n",
        "a=1/45\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "# print(weight)\n",
        "\n",
        "# loss = wasserstein(x, y, weight=weight)\n",
        "# loss = wasserstein(x, y)\n",
        "# loss = sinkhorn(x, y)\n",
        "# loss.backward()\n",
        "# print(x.grad)\n",
        "\n",
        "\n",
        "for i in range(50): # num epochs\n",
        "    loss = wasserstein(x, y, weight=weight)\n",
        "    # loss = sinkhorn(x, y)\n",
        "    # loss = sinkhorn(x, y,0.05,80)\n",
        "    loss.sum().backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(x.data, loss.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def sinkhorn(x, y, epsilon=0.05, max_iters=100):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "\n",
        "    # Compute the cost matrix: here the cost is the squared distance between indices\n",
        "    # (|i-j|^2 for each position i, j)\n",
        "    posx = torch.arange(x.shape[-1], dtype=torch.float).unsqueeze(1)\n",
        "    posy = torch.arange(y.shape[-1], dtype=torch.float).unsqueeze(0)\n",
        "    cost_matrix = (posx - posy).pow(2)  # squared distance\n",
        "\n",
        "    # Initialize the dual variables\n",
        "    u = torch.zeros_like(x)\n",
        "    v = torch.zeros_like(y)\n",
        "\n",
        "    # Sinkhorn iterations\n",
        "    K = torch.exp(-cost_matrix / epsilon)  # Kernel matrix, regularised with epsilon\n",
        "    for _ in range(max_iters):\n",
        "        u = x / (K @ (y / (K.t() @ u + 1e-8)) + 1e-8)\n",
        "        v = y / (K.t() @ (x / (K @ v + 1e-8)) + 1e-8)\n",
        "    # print(K,u.data,v.data)\n",
        "    plan = torch.diag(u) @ K @ torch.diag(v)\n",
        "    dist = torch.sum(plan * cost_matrix)\n",
        "    return dist\n",
        "\n",
        "# Example\n",
        "x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float, requires_grad=True)\n",
        "y = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float)\n",
        "# x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "# y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=float)\n",
        "\n",
        "# dist = sinkhorn(x, y)\n",
        "dist = sinkhorn(x, y, 0.05,80)\n",
        "dist.backward()  # To compute gradients with respect to x\n",
        "\n",
        "print(dist.item())\n",
        "print(x.grad)\n",
        "\n",
        "# [2.0000e+07, 3.0000e+07, 1.0000e-08]) tensor([       0.,        0., 49999996.] episodes>=80\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s1_GgDzoDyYB"
      },
      "outputs": [],
      "source": [
        "# @title torchrl.data.PrioritizedReplayBuffer\n",
        "from torchrl.data import LazyMemmapStorage, LazyTensorStorage, ListStorage\n",
        "buffer_lazytensor = ReplayBuffer(storage=LazyTensorStorage(size))\n",
        "\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "buffer_lazymemmap = ReplayBuffer(storage=LazyMemmapStorage(size), batch_size=32, sampler=SamplerWithoutReplacement())\n",
        "\n",
        "\n",
        "from torchrl.data import ListStorage, PrioritizedReplayBuffer\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "rb = PrioritizedReplayBuffer(alpha=0.7, beta=0.9, storage=ListStorage(10))\n",
        "data = range(10)\n",
        "rb.extend(data)\n",
        "# rb.extend(buffer)\n",
        "\n",
        "\n",
        "sample = rb.sample(3)\n",
        "print(sample)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_UlGz6Xoq3"
      },
      "source": [
        "## plot 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VX5IExbRriwm"
      },
      "outputs": [],
      "source": [
        "# @title sklearn RBF\n",
        "# https://gist.github.com/eljost/2c4e1af652ef02b2989da341c5569af7\n",
        "# from nn_plot.ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import scipy.stats as st\n",
        "\n",
        "# np.random.seed(1)\n",
        "def func(x):\n",
        "    # print(x.shape)\n",
        "    # x= np.sum(x**2, axis=-1)\n",
        "    x=np.random.rand(x.shape[0])\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "res = 50\n",
        "num_pts=15\n",
        "X=np.random.rand(num_pts,2)*res\n",
        "# Y = func(X)\n",
        "Y=np.random.rand(num_pts)\n",
        "# print(X);print(Y)\n",
        "\n",
        "lim = 1\n",
        "# lin = np.linspace(-lim, lim, res)\n",
        "lin = np.linspace(0, res, res)\n",
        "x1, x2 = np.meshgrid(lin, lin)\n",
        "xx = np.vstack((x1.flatten(), x2.flatten())).T\n",
        "\n",
        "kernel = RBF()\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gp.fit(X, Y)\n",
        "# print(\"Learned kernel\", gp.kernel_)\n",
        "y_mean, y_cov = gp.predict(xx, return_cov=True)\n",
        "\n",
        "posteriors = st.multivariate_normal.rvs(mean=y_mean, cov=y_cov, size=1)\n",
        "\n",
        "ax = plt.figure().add_subplot(projection='3d')\n",
        "Z=posteriors.reshape(-1, res)\n",
        "# ax.plot_surface(x1, x2, Z)\n",
        "ax.plot_surface(x1, x2, Z, cmap='rainbow', alpha=0.7)\n",
        "\n",
        "# ax.plot_surface(x1, x2, posteriors.reshape(-1, res))\n",
        "ax.contour(x1, x2, Z, zdir='z', offset=-1, cmap='coolwarm') # https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html#sphx-glr-gallery-mplot3d-contour3d-3-py\n",
        "# ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-0.4, 0.5))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-1, 2))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, c=zdata, cmap='Greens');\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, cmap='Greens');\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SusX7gpzxFNL",
        "outputId": "9f14a9da-e188-49ba-f5f5-70192ff33134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-2.6321], grad_fn=<AddBackward0>)\n",
            "tensor([2.7358]) tensor([-4.7358])\n"
          ]
        }
      ],
      "source": [
        "# @title chatgpt RBFKernelLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBFKernelLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        self.centres = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        dists = torch.cdist(x, self.centres, p=2) ** 2\n",
        "        return torch.exp(-dists / (2 * self.sigma ** 2))\n",
        "\n",
        "class SaddlePointNetwork(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(SaddlePointNetwork, self).__init__()\n",
        "        self.rbf_layer = RBFKernelLayer(in_features, out_features, sigma)\n",
        "        self.linear = nn.Linear(out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rbf_output = self.rbf_layer(x)\n",
        "        # Introduce a saddle point structure\n",
        "        linear_output = self.linear(rbf_output)\n",
        "        # Example saddle function: x^2 - y^2\n",
        "        saddle_output = torch.sum(linear_output[:, :1]**2 - linear_output[:, 1:]**2, dim=1, keepdim=True)\n",
        "        return saddle_output\n",
        "\n",
        "# sin(ax)sin(bx)\n",
        "# (x^2 - y^2)\n",
        "import torch\n",
        "\n",
        "def rbf_saddle(x, y, gamma=1.0, a=1.0, b=1.0):\n",
        "    # RBF-like term\n",
        "    rbf_term = torch.exp(-gamma * torch.norm(x - y, p=2)**2)\n",
        "    # Saddle point term\n",
        "    saddle_term = (a * x)**2 - (b * y)**2\n",
        "    return rbf_term + saddle_term\n",
        "\n",
        "# Example usage\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "output = rbf_saddle(x, y)\n",
        "print(output)\n",
        "\n",
        "# Compute gradients\n",
        "output.backward()\n",
        "print(x.grad, y.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rTmCo7pm0NxL"
      },
      "outputs": [],
      "source": [
        "# @title plot 3d\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "y = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "X, Y = torch.meshgrid(x, y)\n",
        "Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fWZaQTDFg1",
        "outputId": "4c5ced88-54f1-436e-89f9-66f1c8396373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000, -0.7231,  0.3792,  0.0000]]) tensor([0.3362])\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title shape NN\n",
        "num_pts=1\n",
        "\n",
        "# X=torch.rand(num_pts,4)*2-1\n",
        "# X=torch.cat([torch.tensor([0,0]).unsqueeze(0),torch.rand(num_pts,2)*2-1], dim=-1)\n",
        "X=torch.cat([torch.zeros(1,1),torch.rand(num_pts,2)*2-1,torch.zeros(1,1)], dim=-1)\n",
        "Y=torch.rand(num_pts)\n",
        "print(X,Y)\n",
        "optim = torch.optim.SGD(model.parameters(), 1e-1)\n",
        "\n",
        "# model.train()\n",
        "pred = model(X)\n",
        "# print(Y.shape,pred.shape)\n",
        "# loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "loss = F.mse_loss(Y, pred.squeeze(-1))\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "boDd__PE2sGy"
      },
      "outputs": [],
      "source": [
        "# @title plot NN\n",
        "\n",
        "xx = torch.linspace(-1, 1, 100)\n",
        "yy = torch.linspace(-1, 1, 100)\n",
        "X, Y = torch.meshgrid(xx, yy) # [100,100]\n",
        "xy = torch.cat([X.unsqueeze(-1), torch.zeros(X.shape+(2,)), Y.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "with torch.no_grad(): Z = model(xy).squeeze(-1)\n",
        "# print(Z)\n",
        "# print(Z.shape)\n",
        "\n",
        "# Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qW6BYoXsX57o"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle same time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def f(x, y):\n",
        "    return x ** 2 - y ** 2 + x * y\n",
        "# (x-y)(x+y)+xy\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "dim_x, dim_z = 3, 8\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "xx = torch.empty((1, T, dim_x))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "# x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "\n",
        "# tensor([[0.6478, 0.0531, 0.0861]]) tensor([[-1.,  1.]]) 0.2974517047405243\n",
        "# tensor([-0.9419, -1.0000,  0.4416, -1.0000,  1.0000,  0.2963])\n",
        "\n",
        "# x = nn.Parameter(torch.tensor([[0.6478, 0.0531, 0.0861]]))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "z = nn.Parameter(torch.empty((batch, T, dim_z)))\n",
        "# z = torch.empty((1, T, 1))\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# z = nn.Parameter(z.repeat(batch,1,1))\n",
        "# z = nn.Parameter(torch.tensor([[-1.,  1.]]))\n",
        "# optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "# optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.95), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "# .95,1e-1,3e-1\n",
        "# .99,\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(10): # num epochs\n",
        "    # loss = f(x,z)\n",
        "    # loss = f(x.sum(-1),z)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step(); optim_z.step()\n",
        "    optim_x.zero_grad(); optim_z.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x.squeeze()[0].data, z[0].squeeze().data, loss[0].squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "        z.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "# idx = torch.argmax(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GJdFpDr2wIMT"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle argm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def argm(sx, lr=3e3): # 3e3\n",
        "    # batch=sx.size(dim=0)\n",
        "    batch_size, T, _ = sx.shape\n",
        "    batch = 16\n",
        "    # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "    # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "    z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "    torch.nn.init.xavier_uniform_(z)\n",
        "    # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "    # sx = sx.detach()\n",
        "    for i in range(20): # 10\n",
        "        # print(sx.shape,z.shape)\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            cost = model(sxz)\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "        # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    # return z.detach()\n",
        "    print(cost.squeeze().data)\n",
        "    idx = torch.argmax(cost.squeeze(), dim=1)\n",
        "    return z[torch.arange(z.shape[0]),idx].detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(xx.clone())\n",
        "x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50):\n",
        "    z = argm(x)\n",
        "    # print(x.shape,z.shape)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "# print(\"z\",z)\n",
        "# print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjIJP6RlEv2",
        "outputId": "447fdefd-452b-437d-c228-1847492b36f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = torch.randn(16, 16)\n",
        "# print((b==torch.max(b)).nonzero())\n",
        "x = torch.randn(10, 3)\n",
        "idx = torch.randint(3,(10,))\n",
        "# print(x[:,idx].shape)\n",
        "print(x[torch.arange(x.shape[0]),idx].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cX71EprCMSNG"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim bad?\n",
        "\n",
        "import torch\n",
        "\n",
        "def transfer_optim(src_optim, tgt_optim, param_mapping):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    tgt_sd = tgt_optim.state_dict()\n",
        "\n",
        "    # Iterate over each parameter in the target optimizer\n",
        "    for (tgt_idx, target_param) in enumerate(tgt_optim.param_groups[0]['params']):\n",
        "        target_id = id(target_param)\n",
        "\n",
        "        # Find the corresponding source parameter using param_mapping\n",
        "        if target_id in param_mapping:\n",
        "            source_param = param_mapping[target_id]\n",
        "            source_id = id(source_param)\n",
        "\n",
        "            # If there's an existing state for the source parameter, transfer it\n",
        "            if source_id in src_sd['state']:\n",
        "                source_state = src_sd['state'][source_id]\n",
        "                target_state = {}\n",
        "\n",
        "                # Handle momentum/first and second moments (e.g., `exp_avg`, `exp_avg_sq` in Adam)\n",
        "                for key in source_state.keys():\n",
        "                    if source_state[key].shape == target_param.shape: target_state[key] = source_state[key].clone()\n",
        "                    # If size doesn't match, either copy what you can or initialise new values\n",
        "                    elif key in ['exp_avg', 'exp_avg_sq']:  # Momentums (specific to Adam-like optimizers)\n",
        "                        target_state[key] = torch.zeros_like(target_param)\n",
        "                        target_state[key][:source_param.numel()] = source_state[key].flatten()[:target_param.numel()]\n",
        "                    else: target_state[key] = torch.zeros_like(target_param) # init\n",
        "                tgt_sd['state'][target_id] = target_state\n",
        "\n",
        "    # Load the updated state dict back into the target optimizer\n",
        "    tgt_optim.load_state_dict(tgt_sd)\n",
        "    return tgt_optim\n",
        "# {'state': {0: {'step': tensor(1.), 'exp_avg': tensor, 'exp_avg_sq': tensor}, 1: }}\n",
        "\n",
        "\n",
        "\n",
        "model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "source_optimizer = optim.AdamW(model_src.parameters())\n",
        "target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "dummy_input = torch.randn(3, 10)\n",
        "dummy_target = torch.randn(3, 5)\n",
        "criterion = torch.nn.MSELoss()\n",
        "output = model_src(dummy_input)\n",
        "loss = criterion(output, dummy_target)\n",
        "loss.backward()\n",
        "source_optimizer.step()\n",
        "\n",
        "param_mapping = {id(tgt_param): src_param for src_param, tgt_param in zip(model_src.parameters(), model_tgt.parameters())}\n",
        "target_optimizer = transfer_optim(source_optimizer, target_optimizer, param_mapping)\n",
        "\n",
        "print(source_optimizer.state_dict())\n",
        "print(target_optimizer.state_dict())\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title transfer_optim bad? 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    opt_state_dict = optimizer.state_dict()\n",
        "    for group in opt_state_dict['param_groups']:\n",
        "        # For each parameter index (p in param group refers to the layer parameters)\n",
        "        for param_idx, p in enumerate(group['params']):\n",
        "            print(p,source_layer.weight)\n",
        "            if p == source_layer.weight:\n",
        "                # Find the corresponding target layer parameter (in this case, target_layer.weight)\n",
        "                target_param = target_layer.weight\n",
        "                source_state = optimizer.state[p]  # Get the state for the source parameter\n",
        "\n",
        "                # If the parameter is found in the optimizer's state dict\n",
        "                if 'exp_avg' in source_state and 'exp_avg_sq' in source_state:\n",
        "                    exp_avg = source_state['exp_avg']  # First moment (momentum)\n",
        "                    exp_avg_sq = source_state['exp_avg_sq']  # Second moment (variance)\n",
        "\n",
        "                    # Handle input dimension mismatch (copy/truncate or pad)\n",
        "                    source_in_dim = source_layer.weight.shape[1]\n",
        "                    target_in_dim = target_layer.weight.shape[1]\n",
        "\n",
        "                    # Copy optimizer state (exp_avg and exp_avg_sq) accordingly\n",
        "                    with torch.no_grad():\n",
        "                        # Copy the available part and initialize new dimensions to zero\n",
        "                        new_exp_avg = torch.zeros_like(target_param)\n",
        "                        new_exp_avg_sq = torch.zeros_like(target_param)\n",
        "                        # new_exp_avg[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        # new_exp_avg_sq[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        new_exp_avg[:, :source_in_dim] = exp_avg[:, :target_in_dim]\n",
        "                        new_exp_avg_sq[:, :source_in_dim] = exp_avg_sq[:, :target_in_dim]\n",
        "\n",
        "                    # Update the target layer's optimizer state\n",
        "                    optimizer.state[target_param] = {\n",
        "                        'exp_avg': new_exp_avg,\n",
        "                        'exp_avg_sq': new_exp_avg_sq,\n",
        "                        'step': source_state['step']  # Keep the same step count\n",
        "                    }\n",
        "\n",
        "                # Handle the bias (if it exists)\n",
        "                if hasattr(source_layer, 'bias') and hasattr(target_layer, 'bias'):\n",
        "                    source_bias = optimizer.state[source_layer.bias]\n",
        "                    target_bias = target_layer.bias\n",
        "\n",
        "                    optimizer.state[target_bias] = source_bias\n",
        "    return optimizer\n",
        "\n",
        "# Example usage:\n",
        "d = 10  # Input dimension of the source layer\n",
        "a = 5   # Extra nodes to be omitted or added in the target layer\n",
        "m = 8   # Output dimension (same for both)\n",
        "\n",
        "# Source layer (input dimension d+a)\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "\n",
        "# Target layer (input dimension d, or d+a, or arbitrary)\n",
        "target_layer = nn.Linear(d, m)\n",
        "\n",
        "# Optimizer (using AdamW in this case)\n",
        "optimizer = torch.optim.AdamW(source_layer.parameters())\n",
        "\n",
        "# Perform weight transfer (from d+a to d or vice versa) here (assumed done already)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "# Transfer optimizer states\n",
        "optimizer = transfer_optimizer_state(source_layer, target_layer, optimizer)\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    state_dict = optimizer.state_dict()\n",
        "    for old_param, new_param in zip(source_layer.parameters(), target_layer.parameters()):\n",
        "        # If old_param exists in optimizer state\n",
        "        if old_param in state_dict['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = state_dict['state'][old_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                if key in ['exp_avg', 'exp_avg_sq']:  # for Adam or AdamW momentum estimates\n",
        "                    # Handle the shape adjustment (copy, shrink, or randomly initialise the extra nodes)\n",
        "                    new_state[key] = torch.zeros_like(new_param)  # Initialise with zeros\n",
        "                    new_state[key][:old_param.shape[0]] = value[:new_param.shape[0]]  # Copy old values\n",
        "                    # else:\n",
        "                    #     new_state[key] = value.clone()  # Copy directly if shapes match\n",
        "                else:\n",
        "                    new_state[key] = value  # Copy other states directly if they exist\n",
        "\n",
        "            # Set the new parameter in optimizer state\n",
        "            state_dict['state'][new_param] = new_state\n",
        "            # Remove the old parameter from the optimizer state\n",
        "            del state_dict['state'][old_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(state_dict)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optim(src_model, tgt_model, src_optim, tgt_optim):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    for src_param, tgt_param in zip(src_model.parameters(), tgt_model.parameters()):\n",
        "        # If src_param exists in optimizer state\n",
        "        if src_param in src_sd['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = src_sd['state'][src_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                new_state[key] = torch.zeros_like(tgt_param)  # Initialise with zeros\n",
        "                new_state[key][:src_param.shape[0]] = value[:tgt_param.shape[0]]  # Copy old values\n",
        "\n",
        "            src_sd['state'][tgt_param] = new_state\n",
        "            del src_sd['state'][src_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(src_sd)\n",
        "    return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "LKUSzmYLLuRh",
        "outputId": "07ca4b89-257b-4205-c5c8-6a96474ae82a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-186620617543>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# j=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mwht_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwht_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(o)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title rename wht_name\n",
        "# wht_name='jepa.enc.cnn.0.weight'\n",
        "wht_name='jepa.pred.weight_ih_l0'\n",
        "# wht_name='emb.weight'\n",
        "# print(o.isnumeric())\n",
        "# mask = [x.isnumeric() for x in o]\n",
        "# print(o[mask])\n",
        "na_=''\n",
        "# j=0\n",
        "\n",
        "for wht_name in agent.state_dict().keys():\n",
        "    o=wht_name.split('.')\n",
        "    # print(o)\n",
        "    name=wht_name\n",
        "    print(\"####\", wht_name)\n",
        "    for i in range(len(o)):\n",
        "        c = o[i]\n",
        "        if c.isnumeric():\n",
        "            na = '.'.join(o[:i])\n",
        "            me = '.'.join(o[i+1:])\n",
        "            # print(c_,c, c_<c, )\n",
        "            c=int(c)\n",
        "            if na!=na_: # param name diff\n",
        "                j=0 # reset num\n",
        "                c_=c # track wht_name num\n",
        "                na_=na # track param name\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('1', name)\n",
        "            elif c_<c: # same param name, diff num\n",
        "                j+=1\n",
        "                c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('2', name)\n",
        "            else: # same param name, same num\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('3', name)\n",
        "    print('4', name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CACQCCaxA_Y",
        "outputId": "b5d127cd-18ce-49e5-b1e2-d883cb34125a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1746836772511624\n"
          ]
        }
      ],
      "source": [
        "# @title geomloss, Python Optimal Transport\n",
        "# !pip install geomloss[full]\n",
        "\n",
        "import torch\n",
        "from geomloss import SamplesLoss  # See also ImagesLoss, VolumesLoss\n",
        "\n",
        "# # Create some large point clouds in 3D\n",
        "# x = torch.randn(100000, 3, requires_grad=True).cuda()\n",
        "# y = torch.randn(200000, 3).cuda()\n",
        "\n",
        "# x = torch.rand(1000, 1)\n",
        "# y = torch.rand(1000, 1)\n",
        "x = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "y = torch.tensor([0, 1, 0]).float().unsqueeze(-1)\n",
        "# k=1.\n",
        "# y = torch.tensor([k, k, k]).float().unsqueeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01) # 0.05, quadratic, Wasserstein-2. low blur => closer to true Wasserstein dist but slower compute\n",
        "\n",
        "loss = loss_fn(x, y)  # By default, use constant weights = 1/number of samples\n",
        "print(loss)\n",
        "# g_x, = torch.autograd.grad(L, [x])\n",
        "\n",
        "# [0, 1, 0]: 2.4253e-12, 2.4253e-12\n",
        "# [0, 0, 0.1]: 0.1350; [0, 0, 0.5]: 0.0417; [0, 0, 1]: 0\n",
        "# k=0.: 0.1666; k=0.1: 0.1383; k=0.333: 0.1111; k=0.5: 0.1250; k=1.: 0.3333\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "# Define x and y as n-dimensional tensors representing mass distributions\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# y = torch.tensor([0, 0, 1], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# x = torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1))\n",
        "x = nn.Parameter(torch.tensor([0,1.5,0]).float().unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "\n",
        "# Create a position tensor representing the index of each element\n",
        "positions_x = torch.arange(x.shape[0], dtype=float).unsqueeze(1)\n",
        "positions_y = torch.arange(y.shape[0], dtype=float).unsqueeze(1)\n",
        "\n",
        "# Sinkhorn loss using GeomLoss\n",
        "loss_fn = SamplesLoss(\"sinkhorn\", p=1, blur=0.05)  # p=1 for Wasserstein-1\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=1, blur=0.05, scaling=0.9, debias=True)\n",
        "\n",
        "transport_cost = loss_fn(positions_x, x, positions_y, y)\n",
        "\n",
        "print(transport_cost.item())\n",
        "# 1.298424361328248\n",
        "\n",
        "transport_cost.backward()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install POT\n",
        "\n",
        "import ot\n",
        "import numpy as np\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / np.sum(x)\n",
        "    # y = y / np.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = np.abs(np.arange(n)[:, None] - np.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = np.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "x = np.array([0.2, 0.3, 0.5])\n",
        "y = np.array([0, 0, 1])\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "# distance.backward()\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / torch.sum(x)\n",
        "    # y = y / torch.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = torch.abs(torch.arange(n)[:, None] - torch.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = torch.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "# x = np.array([0.2, 0.3, 0.5])\n",
        "# y = np.array([0, 0, 1])\n",
        "x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float())#.unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float()#.unsqueeze(-1)\n",
        "\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "distance.backward()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MqBL9hljvW-5"
      },
      "outputs": [],
      "source": [
        "# @title batchify argm train\n",
        "\n",
        "def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "    self.jepa.pred.train()\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "    lsx=sx.unsqueeze(1)\n",
        "    h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "    lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "    # print(lsx.shape, la.shape, lz.shape)\n",
        "    c=[]\n",
        "    for t in range(seq_len):\n",
        "        a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "        # print(sx.shape, a.shape, z.shape)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            tcost = -self.tcost(syh0)\n",
        "        c.append(tcost)\n",
        "        lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "        lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        cost += (tcost + icost)*gamma**t\n",
        "    return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "\n",
        "def argm(self, sy, sy_, h0, a, reward, lr=3e3): # 3e3\n",
        "    self.tcost.eval()\n",
        "    batch_size = sy.shape[0] # [batch_size, d_model]\n",
        "    z = nn.Parameter(torch.zeros((batch_size, self.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(z)\n",
        "    torch.nn.init.normal_(z, mean=0., std=.3/z.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([z], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    sy, sy_ = sy.detach(), sy_.detach()\n",
        "    out = sy - sy_\n",
        "    h0, a, reward = h0.detach(), a.detach(), reward.detach()\n",
        "    for i in range(10): # 10\n",
        "        with torch.amp.autocast('cuda'):\n",
        "\n",
        "\n",
        "\n",
        "            syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "            out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            repr_loss = F.mse_loss(out, out_[:, -1, :])\n",
        "            # syh0 = torch.cat([sy.flatten(1),F.dropout(h0_, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            syh0 = torch.cat([sy.flatten(1),h0_.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "            z_loss = torch.abs(z).sum() # z_loss = torch.norm(z)\n",
        "            print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def argm(lsy, sy, h0, la, rwd):\n",
        "    # lz = agent.argm(out, h0, la, reward)\n",
        "    agent.tcost.eval()\n",
        "    batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "    lz = nn.Parameter(torch.zeros((batch_size, bptt, agent.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(lz)\n",
        "    torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "    for i in range(3): # 10\n",
        "        sy_, h0_ = sy.detach(), h0.detach()\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "        with torch.cuda.amp.autocast():\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0_ = agent.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl + agent.zloss_coeff * z_loss\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    agent.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return lz.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# closs_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01)\n",
        "bptt = 25\n",
        "for batch, Sar in enumerate(train_loader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "    state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "    sy_ = agent.jepa.enc(state).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    # sx=sy_\n",
        "    state, action, reward = Sar # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "    state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "    for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "        with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "            lsy = agent.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "            la = agent.emb(act) # [batch_size, bptt, dim_a]\n",
        "            out = lsy - torch.cat([sy_, lsy[:,:-1]], dim=1)\n",
        "            # lz = agent.argm(out, h0, la, reward)\n",
        "            lz = argm(lsy, sy_, h0, la, rwd)\n",
        "            # lz = torch.zeros((batch_size, bptt, agent.dim_z), device=device)\n",
        "\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0 = agent.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            std_loss, cov_loss = agent.jepa.v_creg(agent.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "            jloss = agent.jepa.sim_coeff * repr_loss + agent.jepa.std_coeff * std_loss + agent.jepa.cov_coeff * cov_loss\n",
        "\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            # print(\"syh0, rwd\",syh0.shape,rwd.shape)\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            # reward_ = agent.tcost(syh0)\n",
        "            # clossl = wasserstein(rwd, reward_)#.squeeze(-1)\n",
        "            closs = agent.closs_coeff * clossl\n",
        "\n",
        "            # print(h0.requires_grad)\n",
        "            # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "            # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "            # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "            # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "            # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "            loss = jloss + closs\n",
        "\n",
        "            # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "            norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "            z_norm = torch.norm(z)\n",
        "            # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "            # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "            print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            optim.zero_grad()\n",
        "            sy_, h0 = sy_.detach(), h0.detach()\n",
        "    break\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mhkK_9AQm8_q",
        "Jt_UlGz6Xoq3",
        "wUhKd009Qvk3"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac79a874852a47d2b6c2ff83806aba05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a977c0be35ff41b09d034eaaa91d4da2",
              "IPY_MODEL_a5e76a14f46b4c9a8a41da8f027b730a"
            ],
            "layout": "IPY_MODEL_ec9c7426e9bf42d7ae0c96f458401efd"
          }
        },
        "a977c0be35ff41b09d034eaaa91d4da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce20d6ecc8dc4b1893a03f9bcf866050",
            "placeholder": "​",
            "style": "IPY_MODEL_a128d10441c24c6b8f470f9f76cd78ea",
            "value": "0.067 MB of 0.067 MB uploaded\r"
          }
        },
        "a5e76a14f46b4c9a8a41da8f027b730a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f8c7a8e6b884f71873b4a236a3f6984",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c5e762b3f544b67b4408b521cfdf1cd",
            "value": 1
          }
        },
        "ec9c7426e9bf42d7ae0c96f458401efd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce20d6ecc8dc4b1893a03f9bcf866050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a128d10441c24c6b8f470f9f76cd78ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f8c7a8e6b884f71873b4a236a3f6984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c5e762b3f544b67b4408b521cfdf1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}