{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "ceb04208-0658-4658-90ca-1f138fec5deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m681.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq procgen faiss-cpu vector-quantize-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nEY9MmwZhA8a"
      },
      "outputs": [],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# conv = Conv().to(device)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# # input = torch.rand((4,1,256,256), device=device)\n",
        "# out = conv(input)\n",
        "# print(out.shape)\n",
        "\n",
        "# conv = Deconv(256).to(device)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# input = torch.rand((4,256), device=device)\n",
        "# out = conv(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "cbd7dc49-2465-481d-94c6-741be1614c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FuA25qQknUAX"
      },
      "outputs": [],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model)\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        # self.pred = gru(emb_dim, rnn_units, num_layers)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=1e5): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        num_steps = 5 # 10\n",
        "        for i in range(num_steps):\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                sy_ = self.pred(sxaz)\n",
        "                # print(\"y_, y\",y_.shape, y.shape)\n",
        "                loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z = torch.clamp(z, min=-1, max=1)\n",
        "            # print(\"argm in\",loss.item())\n",
        "        # print(z.squeeze())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCD647ZpPrGf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cIF--UQMEEFx"
      },
      "outputs": [],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(3): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "                sx_ = sx_.detach()\n",
        "                # print(loss.item(), lact)\n",
        "        # print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29O1eyvhnRSD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=1) # 20\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(3): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "                sx_ = sx_.detach()\n",
        "                # print(loss.item(), lact)\n",
        "            # print(x)\n",
        "        print(\"search\",loss.item())\n",
        "        # print(lact)\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=32):\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(sxaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy).squeeze(-1)\n",
        "                    clossl = F.mse_loss(stt, r)\n",
        "                    print(\"reward_\",reward.shape,reward_.shape)\n",
        "                    try:\n",
        "                        st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    # closs = F.mse_loss(reward_, reward) + F.mse_loss(stt, r)\n",
        "                    clossb = F.mse_loss(reward_, reward)\n",
        "                    closs = clossl + clossb\n",
        "\n",
        "                    # loss = loss + jloss + conv_loss + closs\n",
        "                    loss = loss + jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss = loss + closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "!gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "69ab0a3d-4041-4b7e-a390-f3babceb08ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_\n",
            "From (redirected): https://drive.google.com/uc?id=12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_&confirm=t&uuid=7a45c8ca-269c-4a6a-845c-a79ddbc78173\n",
            "To: /content/agentoptim.pkl\n",
            "100% 44.2M/44.2M [00:01<00:00, 38.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ShHQ_ynlwoyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67771f40-5520-4398-d0fc-22c3783d40dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "def save(folder, name='agent.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    # agent.mem.save(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer512.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# save(folder, name='agent_jepa753333256.pth')\n",
        "# buffer = load(folder)\n",
        "# save('/content/')\n",
        "# buffer = load('/content/')\n",
        "\n",
        "# name='agent.pth'\n",
        "# print(folder+name)\n",
        "# torch.load(folder+name, map_location='o')\n",
        "# with open(folder+'buffer512down.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "with open(folder+'buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl').values()\n",
        "modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "agent.load_state_dict(modelsd)\n",
        "optim.load_state_dict(optimsd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = self.data_process(buffer)\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    # def data_process(self, data): # str 10780437\n",
        "    #     return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "        # return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        return state, action, reward\n",
        "        # state, action, reward = self.data[idx]\n",
        "        # # print(\"__getitem__\",state)\n",
        "        # state = self.transform(state)\n",
        "        # # print(\"__getitem__\",type(state))\n",
        "        # return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e3fpbtNOiz1",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "ba7daeaf-0622-4ad9-a01b-28b64b95eb99"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1., -1.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.])\n",
            "tensor([-1., -1.,  0.,  0., -1.,  0., -1.,  0., -1.,  0.])\n",
            "tensor([ 0.,  0., -1.,  0., -1., -1.,  0., -1., -1.,  0.])\n",
            "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  0., -1.])\n",
            "tensor([-1.,  0., -1., -1., -1.,  0., -1., -1., -1., -1.])\n",
            "tensor([ 0.,  0.,  0., -1.,  0.,  0., -1.,  0., -1.,  0.])\n",
            "tensor([ 0.,  0.,  0., -1.])\n",
            "tensor([-9.8974e-01, -9.8857e-01,  2.3056e-03, -9.9022e-01, -9.8437e-01,\n",
            "        -9.8829e-01, -8.7404e-04, -1.0284e-02, -8.4426e-03,  2.6559e-03])\n",
            "tensor([-0.9697, -0.9913,  0.0031, -0.0119, -0.9905, -0.0034, -0.9898, -0.0086,\n",
            "        -0.9899, -0.0057])\n",
            "tensor([-1.7535e-04,  2.9385e-03, -9.9157e-01, -1.7301e-03, -9.9056e-01,\n",
            "        -9.9026e-01, -1.2313e-02, -9.8259e-01, -9.9146e-01, -7.7463e-03])\n",
            "tensor([-0.9888, -0.9904, -0.9907, -0.9901, -0.9909, -0.9852, -0.9901, -0.9851,\n",
            "        -0.0024, -0.9383])\n",
            "tensor([-0.9899, -0.0027, -0.9884, -0.9842, -0.9879,  0.0012, -0.9899, -1.0217,\n",
            "        -0.9916, -0.9917])\n",
            "tensor([ 1.0985e-03, -4.5651e-02,  2.3623e-04, -9.9015e-01, -2.6837e-03,\n",
            "        -1.8158e-03, -9.9100e-01,  3.3083e-03, -9.9026e-01,  4.6624e-04])\n",
            "tensor([-0.0091, -0.0077,  0.0027, -0.9888])\n"
          ]
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "train_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "train_data = Datasetme(train_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    train_data=list(zip(state,reward))\n",
        "    train_data = Datasetme(train_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range((len(labels)//10)+1):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "        pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "        # print(pred)\n",
        "        for x in range((len(pred)//10)+1):\n",
        "            print(pred[10*x:10*x+10])\n",
        "        # print((labels==pred).sum())\n",
        "except: pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OksdjCeJYpYh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    # agent.train_ae(train_loader, optim)\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "# 10 epochs 15m23s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PraFUAPB3j7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "b08a516a-5f4f-4dcb-dd5b-933ef05e02ea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x256 and 260x256)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-b50b90272db4>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# buffer = simulate(agent, buffer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-b50b90272db4>\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(agent, buffer)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mst_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mstt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x256 and 260x256)"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    agent.eval()\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # out = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        with torch.no_grad():\n",
        "            st = agent.jepa.enc(state)\n",
        "            # st_ = agent.jepa.pred(st)\n",
        "            stt = agent.tcost(st_).squeeze(-1)\n",
        "            imshow(state.detach().cpu().squeeze(0))\n",
        "            print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = nn.Parameter(torch.empty((4,5,3),device=device)) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "xx = torch.split(x, 5, dim=1)\n",
        "for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    la, lact = quantizer(x)\n",
        "# print(xx)\n",
        "print(x)\n",
        "print(la)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "59a873c2-964c-408a-e827-a71735049d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[ 0.0802,  0.3557,  0.4046],\n",
            "         [ 0.4610,  0.2339, -0.2854],\n",
            "         [-0.4339,  0.3213, -0.1077],\n",
            "         [-0.2197,  0.1823,  0.4601],\n",
            "         [ 0.0789,  0.1214, -0.0021]],\n",
            "\n",
            "        [[ 0.3509, -0.3400, -0.0577],\n",
            "         [-0.3121, -0.3316,  0.1402],\n",
            "         [-0.3047,  0.1295, -0.0393],\n",
            "         [-0.1396, -0.0857, -0.3933],\n",
            "         [ 0.2932, -0.0498,  0.4478]],\n",
            "\n",
            "        [[-0.0607,  0.2992,  0.2298],\n",
            "         [ 0.1823, -0.0217,  0.3734],\n",
            "         [ 0.3363, -0.1199, -0.3895],\n",
            "         [ 0.4462,  0.1263, -0.3820],\n",
            "         [ 0.0985, -0.0637, -0.1477]],\n",
            "\n",
            "        [[ 0.4375, -0.0122, -0.4023],\n",
            "         [-0.3738, -0.0780, -0.3988],\n",
            "         [-0.3659, -0.1077,  0.4109],\n",
            "         [ 0.2406,  0.0619,  0.2601],\n",
            "         [ 0.0410, -0.3293,  0.3457]]], device='cuda:0', requires_grad=True)\n",
            "tensor([[[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.],\n",
            "         [0., 0., 0.]]], device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9cm6KjvBrnNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8dca2d4-faaa-4a64-e3fd-442b1d0d86f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "search 0.4267578125\n",
            "tcost icost 0.0068817138671875 0.4227501153945923\n",
            "tcost icost 0.0068817138671875 0.42038026452064514\n",
            "tcost icost 0.0068817138671875 0.4203834533691406\n",
            "search 0.42724609375\n",
            "tcost icost 0.00701141357421875 0.41959837079048157\n",
            "tcost icost 0.00701141357421875 0.4229119122028351\n",
            "tcost icost 0.00701141357421875 0.41988620162010193\n",
            "search 0.427001953125\n",
            "tcost icost 0.0071563720703125 0.42026636004447937\n",
            "tcost icost 0.006908416748046875 0.4208675026893616\n",
            "tcost icost 0.006908416748046875 0.42065346240997314\n",
            "search 0.427490234375\n",
            "tcost icost 0.007720947265625 0.4204835295677185\n",
            "tcost icost 0.006778717041015625 0.4207017421722412\n",
            "tcost icost 0.006778717041015625 0.42370063066482544\n",
            "search 0.430419921875\n",
            "tcost icost 0.007720947265625 0.41666579246520996\n",
            "tcost icost 0.00717926025390625 0.41639062762260437\n",
            "tcost icost 0.00717926025390625 0.41873085498809814\n",
            "search 0.42578125\n",
            "tcost icost 0.007602691650390625 0.41461166739463806\n",
            "tcost icost 0.007038116455078125 0.4142364263534546\n",
            "tcost icost 0.007038116455078125 0.4192182123661041\n",
            "search 0.42626953125\n",
            "tcost icost 0.00725555419921875 0.4157907962799072\n",
            "tcost icost 0.007198333740234375 0.41566720604896545\n",
            "tcost icost 0.007198333740234375 0.4143744111061096\n",
            "search 0.42138671875\n",
            "tcost icost 0.00766754150390625 0.4199468493461609\n",
            "tcost icost 0.007152557373046875 0.4162716269493103\n",
            "tcost icost 0.007152557373046875 0.41685113310813904\n",
            "search 0.423828125\n",
            "tcost icost 0.007740020751953125 0.41392895579338074\n",
            "tcost icost 0.00731658935546875 0.4155501425266266\n",
            "tcost icost 0.00731658935546875 0.4139297306537628\n",
            "search 0.421142578125\n",
            "tcost icost 0.00806427001953125 0.4149298667907715\n",
            "tcost icost 0.0074005126953125 0.41616058349609375\n",
            "tcost icost 0.0074005126953125 0.4179653525352478\n",
            "search 0.42529296875\n",
            "tcost icost 0.00792694091796875 0.4189217686653137\n",
            "tcost icost 0.007717132568359375 0.41556206345558167\n",
            "tcost icost 0.007717132568359375 0.41895371675491333\n",
            "search 0.4267578125\n",
            "tcost icost 0.0079498291015625 0.4184076189994812\n",
            "tcost icost 0.007381439208984375 0.4161761403083801\n",
            "tcost icost 0.007381439208984375 0.4159010946750641\n",
            "search 0.42333984375\n",
            "tcost icost 0.00800323486328125 0.418245792388916\n",
            "tcost icost 0.007080078125 0.41718530654907227\n",
            "tcost icost 0.007080078125 0.4172084927558899\n",
            "search 0.42431640625\n",
            "tcost icost 0.008026123046875 0.41619911789894104\n",
            "tcost icost 0.007404327392578125 0.41738802194595337\n",
            "tcost icost 0.007404327392578125 0.41722044348716736\n",
            "search 0.424560546875\n",
            "tcost icost 0.00789642333984375 0.4176567792892456\n",
            "tcost icost 0.0080413818359375 0.4150907099246979\n",
            "tcost icost 0.0080413818359375 0.41692984104156494\n",
            "search 0.425048828125\n",
            "tcost icost 0.00792694091796875 0.41322049498558044\n",
            "tcost icost 0.00753021240234375 0.4144574701786041\n",
            "tcost icost 0.00753021240234375 0.41423463821411133\n",
            "search 0.421875\n",
            "tcost icost 0.00785064697265625 0.41652828454971313\n",
            "tcost icost 0.0080108642578125 0.4162776470184326\n",
            "tcost icost 0.0080108642578125 0.4128851890563965\n",
            "search 0.4208984375\n",
            "tcost icost 0.0074462890625 0.4190939962863922\n",
            "tcost icost 0.00713348388671875 0.4169134199619293\n",
            "tcost icost 0.00713348388671875 0.41814470291137695\n",
            "search 0.42529296875\n",
            "tcost icost 0.008056640625 0.4156309962272644\n",
            "tcost icost 0.00737762451171875 0.4189251959323883\n",
            "tcost icost 0.00737762451171875 0.41939181089401245\n",
            "search 0.4267578125\n",
            "tcost icost 0.0079498291015625 0.41759106516838074\n",
            "tcost icost 0.00691986083984375 0.4181256890296936\n",
            "tcost icost 0.00691986083984375 0.4192677438259125\n",
            "search 0.426025390625\n",
            "tcost icost 0.00730133056640625 0.42225831747055054\n",
            "tcost icost 0.00737762451171875 0.4192284345626831\n",
            "tcost icost 0.00737762451171875 0.4182921051979065\n",
            "search 0.425537109375\n",
            "tcost icost 0.00768280029296875 0.41918236017227173\n",
            "tcost icost 0.006923675537109375 0.4189715087413788\n",
            "tcost icost 0.006923675537109375 0.4235202968120575\n",
            "search 0.430419921875\n",
            "tcost icost 0.007396697998046875 0.4192553460597992\n",
            "tcost icost 0.007396697998046875 0.41872844099998474\n",
            "tcost icost 0.007396697998046875 0.4189111292362213\n",
            "search 0.42626953125\n",
            "tcost icost 0.007442474365234375 0.42024558782577515\n",
            "tcost icost 0.00720977783203125 0.42002010345458984\n",
            "tcost icost 0.00720977783203125 0.41982418298721313\n",
            "search 0.42724609375\n",
            "tcost icost 0.007244110107421875 0.42314401268959045\n",
            "tcost icost 0.0069580078125 0.4230886399745941\n",
            "tcost icost 0.0069580078125 0.4235512316226959\n",
            "search 0.4306640625\n",
            "tcost icost 0.007610321044921875 0.416993647813797\n",
            "tcost icost 0.006595611572265625 0.41648155450820923\n",
            "tcost icost 0.006595611572265625 0.41679590940475464\n",
            "search 0.42333984375\n",
            "tcost icost 0.00650787353515625 0.422334760427475\n",
            "tcost icost 0.00640106201171875 0.4203425347805023\n",
            "tcost icost 0.00640106201171875 0.4215894639492035\n",
            "search 0.427978515625\n",
            "tcost icost 0.00710296630859375 0.4193476736545563\n",
            "tcost icost 0.006786346435546875 0.4192248284816742\n",
            "tcost icost 0.006786346435546875 0.4192248284816742\n",
            "search 0.426025390625\n",
            "tcost icost 0.006793975830078125 0.41673094034194946\n",
            "tcost icost 0.006793975830078125 0.41673097014427185\n",
            "tcost icost 0.006793975830078125 0.41673094034194946\n",
            "search 0.423583984375\n",
            "tcost icost 0.006511688232421875 0.4189456105232239\n",
            "tcost icost 0.0076446533203125 0.424368679523468\n",
            "tcost icost 0.0076446533203125 0.41993582248687744\n",
            "search 0.427490234375\n",
            "tcost icost 0.007297515869140625 0.4174036979675293\n",
            "tcost icost 0.007022857666015625 0.41758114099502563\n",
            "tcost icost 0.007022857666015625 0.41846704483032227\n",
            "search 0.425537109375\n",
            "tcost icost 0.00682830810546875 0.4193837642669678\n",
            "tcost icost 0.00644683837890625 0.4197289049625397\n",
            "tcost icost 0.00644683837890625 0.41971537470817566\n",
            "search 0.426025390625\n",
            "tcost icost 0.0071868896484375 0.4226478040218353\n",
            "tcost icost 0.006931304931640625 0.41913333535194397\n",
            "tcost icost 0.006931304931640625 0.41878893971443176\n",
            "search 0.425537109375\n",
            "tcost icost 0.0074462890625 0.4186985194683075\n",
            "tcost icost 0.007144927978515625 0.42009586095809937\n",
            "tcost icost 0.007144927978515625 0.4188660979270935\n",
            "search 0.426025390625\n",
            "tcost icost 0.00664520263671875 0.4219304621219635\n",
            "tcost icost 0.006374359130859375 0.4219818115234375\n",
            "tcost icost 0.006374359130859375 0.4221377968788147\n",
            "search 0.428466796875\n",
            "tcost icost 0.007236480712890625 0.4220695197582245\n",
            "tcost icost 0.006450653076171875 0.423829048871994\n",
            "tcost icost 0.006450653076171875 0.42165690660476685\n",
            "search 0.427978515625\n",
            "tcost icost 0.007579803466796875 0.42182132601737976\n",
            "tcost icost 0.006542205810546875 0.42193758487701416\n",
            "tcost icost 0.006542205810546875 0.42191287875175476\n",
            "search 0.428466796875\n",
            "tcost icost 0.007282257080078125 0.4224070906639099\n",
            "tcost icost 0.006511688232421875 0.4217614531517029\n",
            "tcost icost 0.006511688232421875 0.4223577380180359\n",
            "search 0.428955078125\n",
            "tcost icost 0.006534576416015625 0.42422762513160706\n",
            "tcost icost 0.006366729736328125 0.42273619771003723\n",
            "tcost icost 0.006366729736328125 0.4216693043708801\n",
            "search 0.427978515625\n",
            "tcost icost 0.00688934326171875 0.42376285791397095\n",
            "tcost icost 0.00666046142578125 0.4214319884777069\n",
            "tcost icost 0.00666046142578125 0.42151010036468506\n",
            "search 0.42822265625\n",
            "tcost icost 0.00640869140625 0.4223572015762329\n",
            "tcost icost 0.00640869140625 0.4225218892097473\n",
            "tcost icost 0.00640869140625 0.4223330020904541\n",
            "search 0.4287109375\n",
            "tcost icost 0.00716400146484375 0.4222349226474762\n",
            "tcost icost 0.00641632080078125 0.42222511768341064\n",
            "tcost icost 0.00641632080078125 0.42228925228118896\n",
            "search 0.4287109375\n",
            "tcost icost 0.007476806640625 0.4246533215045929\n",
            "tcost icost 0.00667572021484375 0.42219316959381104\n",
            "tcost icost 0.00667572021484375 0.42290395498275757\n",
            "search 0.429443359375\n",
            "tcost icost 0.007274627685546875 0.4230166971683502\n",
            "tcost icost 0.006988525390625 0.4248340129852295\n",
            "tcost icost 0.006988525390625 0.4218064844608307\n",
            "search 0.428955078125\n",
            "tcost icost 0.0075836181640625 0.4219200909137726\n",
            "tcost icost 0.006511688232421875 0.4266873002052307\n",
            "tcost icost 0.006511688232421875 0.4222583472728729\n",
            "search 0.428955078125\n",
            "tcost icost 0.006378173828125 0.4229772984981537\n",
            "tcost icost 0.006671905517578125 0.4232023358345032\n",
            "tcost icost 0.006671905517578125 0.4228248596191406\n",
            "search 0.429443359375\n",
            "tcost icost 0.00742340087890625 0.4234439432621002\n",
            "tcost icost 0.00637054443359375 0.4300937056541443\n",
            "tcost icost 0.00637054443359375 0.4282096326351166\n",
            "search 0.4345703125\n",
            "tcost icost 0.00762939453125 0.4234260320663452\n",
            "tcost icost 0.00650787353515625 0.42343243956565857\n",
            "tcost icost 0.00650787353515625 0.42340245842933655\n",
            "search 0.429931640625\n",
            "tcost icost 0.00727081298828125 0.423720121383667\n",
            "tcost icost 0.006488800048828125 0.4252817630767822\n",
            "tcost icost 0.006488800048828125 0.42335039377212524\n",
            "search 0.429931640625\n",
            "tcost icost 0.007114410400390625 0.4262552261352539\n",
            "tcost icost 0.006885528564453125 0.4252522885799408\n",
            "tcost icost 0.006885528564453125 0.42324891686439514\n",
            "search 0.43017578125\n",
            "tcost icost 0.006832122802734375 0.4231823980808258\n",
            "tcost icost 0.006443023681640625 0.4232022166252136\n",
            "tcost icost 0.006443023681640625 0.423202246427536\n",
            "search 0.429443359375\n",
            "tcost icost 0.00778961181640625 0.4227321147918701\n",
            "tcost icost 0.006439208984375 0.4243660867214203\n",
            "tcost icost 0.006439208984375 0.4227844774723053\n",
            "search 0.42919921875\n",
            "tcost icost 0.00652313232421875 0.42323410511016846\n",
            "tcost icost 0.006435394287109375 0.42406007647514343\n",
            "tcost icost 0.006435394287109375 0.42299360036849976\n",
            "search 0.429443359375\n",
            "tcost icost 0.0074005126953125 0.4225831925868988\n",
            "tcost icost 0.00659942626953125 0.42323774099349976\n",
            "tcost icost 0.00659942626953125 0.42578256130218506\n",
            "search 0.432373046875\n",
            "tcost icost 0.006984710693359375 0.42775121331214905\n",
            "tcost icost 0.00635528564453125 0.42777490615844727\n",
            "tcost icost 0.00635528564453125 0.4275726079940796\n",
            "search 0.433837890625\n",
            "tcost icost 0.00690460205078125 0.4279848337173462\n",
            "tcost icost 0.006378173828125 0.42810165882110596\n",
            "tcost icost 0.006378173828125 0.4279903173446655\n",
            "search 0.434326171875\n",
            "tcost icost 0.00688934326171875 0.4291456639766693\n",
            "tcost icost 0.006381988525390625 0.43085557222366333\n",
            "tcost icost 0.006381988525390625 0.42777204513549805\n",
            "search 0.43408203125\n",
            "tcost icost 0.006847381591796875 0.42776861786842346\n",
            "tcost icost 0.006420135498046875 0.4288776218891144\n",
            "tcost icost 0.006420135498046875 0.42926791310310364\n",
            "search 0.435546875\n",
            "tcost icost 0.00716400146484375 0.427966445684433\n",
            "tcost icost 0.00641632080078125 0.42944076657295227\n",
            "tcost icost 0.00641632080078125 0.4292048513889313\n",
            "search 0.435546875\n",
            "tcost icost 0.0070648193359375 0.43091028928756714\n",
            "tcost icost 0.0068511962890625 0.4292543828487396\n",
            "tcost icost 0.0068511962890625 0.42946159839630127\n",
            "search 0.436279296875\n",
            "tcost icost 0.00717926025390625 0.4294414222240448\n",
            "tcost icost 0.00717926025390625 0.4294414222240448\n",
            "tcost icost 0.00717926025390625 0.4324800968170166\n",
            "search 0.439453125\n",
            "tcost icost 0.006900787353515625 0.42963123321533203\n",
            "tcost icost 0.006443023681640625 0.43057766556739807\n",
            "tcost icost 0.006443023681640625 0.429871529340744\n",
            "search 0.436279296875\n",
            "tcost icost 0.0065155029296875 0.42978930473327637\n",
            "tcost icost 0.006458282470703125 0.43049684166908264\n",
            "tcost icost 0.006458282470703125 0.42984873056411743\n",
            "search 0.436279296875\n",
            "tcost icost 0.006961822509765625 0.42971161007881165\n",
            "tcost icost 0.006771087646484375 0.4302694797515869\n",
            "tcost icost 0.006771087646484375 0.42954665422439575\n",
            "search 0.436279296875\n",
            "tcost icost 0.007114410400390625 0.43086808919906616\n",
            "tcost icost 0.006397247314453125 0.4313150644302368\n",
            "tcost icost 0.006397247314453125 0.4299645721912384\n",
            "search 0.436279296875\n",
            "tcost icost 0.006832122802734375 0.43276211619377136\n",
            "tcost icost 0.006439208984375 0.43254554271698\n",
            "tcost icost 0.006439208984375 0.43073025345802307\n",
            "search 0.43701171875\n",
            "tcost icost 0.006885528564453125 0.43429234623908997\n",
            "tcost icost 0.006885528564453125 0.43133899569511414\n",
            "tcost icost 0.006885528564453125 0.4323219656944275\n",
            "search 0.439208984375\n",
            "tcost icost 0.00647735595703125 0.43114563822746277\n",
            "tcost icost 0.0064239501953125 0.43102964758872986\n",
            "tcost icost 0.0064239501953125 0.43072840571403503\n",
            "search 0.43701171875\n",
            "tcost icost 0.00714111328125 0.4312603771686554\n",
            "tcost icost 0.00641632080078125 0.43119367957115173\n",
            "tcost icost 0.00641632080078125 0.4331497848033905\n",
            "search 0.439453125\n",
            "tcost icost 0.006938934326171875 0.43150562047958374\n",
            "tcost icost 0.00634765625 0.4317573308944702\n",
            "tcost icost 0.00634765625 0.4317573308944702\n",
            "search 0.43798828125\n",
            "tcost icost 0.007167816162109375 0.43216472864151\n",
            "tcost icost 0.00666046142578125 0.43483495712280273\n",
            "tcost icost 0.00666046142578125 0.432024210691452\n",
            "search 0.438720703125\n",
            "tcost icost 0.0080718994140625 0.432360976934433\n",
            "tcost icost 0.007755279541015625 0.4320928454399109\n",
            "tcost icost 0.007755279541015625 0.4353296160697937\n",
            "search 0.443115234375\n",
            "tcost icost 0.007442474365234375 0.4341348111629486\n",
            "tcost icost 0.007442474365234375 0.4332088828086853\n",
            "tcost icost 0.007442474365234375 0.4358179569244385\n",
            "search 0.443115234375\n",
            "tcost icost 0.0079345703125 0.43486326932907104\n",
            "tcost icost 0.007480621337890625 0.4322868883609772\n",
            "tcost icost 0.007480621337890625 0.43298840522766113\n",
            "search 0.440673828125\n",
            "tcost icost 0.0079345703125 0.43534088134765625\n",
            "tcost icost 0.007511138916015625 0.4322958290576935\n",
            "tcost icost 0.007511138916015625 0.43361514806747437\n",
            "search 0.441162109375\n",
            "tcost icost 0.00791168212890625 0.4369770884513855\n",
            "tcost icost 0.006805419921875 0.4410897493362427\n",
            "tcost icost 0.006805419921875 0.4366415739059448\n",
            "search 0.443359375\n",
            "tcost icost 0.007598876953125 0.44060951471328735\n",
            "tcost icost 0.00679779052734375 0.4394606053829193\n",
            "tcost icost 0.00679779052734375 0.4408029019832611\n",
            "search 0.44775390625\n",
            "tcost icost 0.0079803466796875 0.4407057464122772\n",
            "tcost icost 0.0078125 0.44017791748046875\n",
            "tcost icost 0.0078125 0.44072815775871277\n",
            "search 0.448486328125\n",
            "tcost icost 0.00748443603515625 0.4408409893512726\n",
            "tcost icost 0.006694793701171875 0.4409260153770447\n",
            "tcost icost 0.006694793701171875 0.4429701566696167\n",
            "search 0.449462890625\n",
            "tcost icost 0.007801055908203125 0.4410158395767212\n",
            "tcost icost 0.00691986083984375 0.44499486684799194\n",
            "tcost icost 0.00691986083984375 0.44094786047935486\n",
            "search 0.44775390625\n",
            "tcost icost 0.0080108642578125 0.4412671625614166\n",
            "tcost icost 0.00807952880859375 0.4407764673233032\n",
            "tcost icost 0.00807952880859375 0.4404815435409546\n",
            "search 0.448486328125\n",
            "tcost icost 0.00812530517578125 0.4420020878314972\n",
            "tcost icost 0.0080718994140625 0.4408130347728729\n",
            "tcost icost 0.0080718994140625 0.44081300497055054\n",
            "search 0.448974609375\n",
            "tcost icost 0.0080413818359375 0.4413681924343109\n",
            "tcost icost 0.007518768310546875 0.44124919176101685\n",
            "tcost icost 0.007518768310546875 0.44124919176101685\n",
            "search 0.44873046875\n",
            "tcost icost 0.00797271728515625 0.4410771131515503\n",
            "tcost icost 0.00809478759765625 0.44190284609794617\n",
            "tcost icost 0.00809478759765625 0.44212326407432556\n",
            "search 0.4501953125\n",
            "tcost icost 0.007373809814453125 0.44114699959754944\n",
            "tcost icost 0.007373809814453125 0.44261589646339417\n",
            "tcost icost 0.007373809814453125 0.44236963987350464\n",
            "search 0.44970703125\n",
            "tcost icost 0.00775909423828125 0.4413229525089264\n",
            "tcost icost 0.007381439208984375 0.44431355595588684\n",
            "tcost icost 0.007381439208984375 0.44203445315361023\n",
            "search 0.449462890625\n",
            "tcost icost 0.0079193115234375 0.44124579429626465\n",
            "tcost icost 0.007354736328125 0.4409184455871582\n",
            "tcost icost 0.007354736328125 0.4411141872406006\n",
            "search 0.448486328125\n",
            "tcost icost 0.00794219970703125 0.44080960750579834\n",
            "tcost icost 0.007747650146484375 0.44098785519599915\n",
            "tcost icost 0.007747650146484375 0.44098785519599915\n",
            "search 0.44873046875\n",
            "tcost icost 0.00768280029296875 0.4408440887928009\n",
            "tcost icost 0.00768280029296875 0.4408177137374878\n",
            "tcost icost 0.00768280029296875 0.4410296380519867\n",
            "search 0.448486328125\n",
            "tcost icost 0.0079498291015625 0.4408077001571655\n",
            "tcost icost 0.00785064697265625 0.44100213050842285\n",
            "tcost icost 0.00785064697265625 0.4402454197406769\n",
            "search 0.447998046875\n",
            "tcost icost 0.007564544677734375 0.4415290653705597\n",
            "tcost icost 0.007232666015625 0.4405279755592346\n",
            "tcost icost 0.007232666015625 0.44151678681373596\n",
            "search 0.44873046875\n",
            "tcost icost 0.007732391357421875 0.44351711869239807\n",
            "tcost icost 0.007274627685546875 0.44306710362434387\n",
            "tcost icost 0.007274627685546875 0.44306710362434387\n",
            "search 0.450439453125\n",
            "tcost icost 0.007843017578125 0.4437074661254883\n",
            "tcost icost 0.00710296630859375 0.4434514343738556\n",
            "tcost icost 0.00710296630859375 0.4438481330871582\n",
            "search 0.450927734375\n",
            "tcost icost 0.007801055908203125 0.444305956363678\n",
            "tcost icost 0.007450103759765625 0.44403019547462463\n",
            "tcost icost 0.007450103759765625 0.4442180395126343\n",
            "search 0.451904296875\n",
            "tcost icost 0.00762939453125 0.4443499743938446\n",
            "tcost icost 0.006832122802734375 0.447020024061203\n",
            "tcost icost 0.006832122802734375 0.4447745382785797\n",
            "search 0.45166015625\n",
            "tcost icost 0.007175445556640625 0.44360414147377014\n",
            "tcost icost 0.00685882568359375 0.446120947599411\n",
            "tcost icost 0.00685882568359375 0.44405481219291687\n",
            "search 0.450927734375\n",
            "tcost icost 0.00748443603515625 0.4451983571052551\n",
            "tcost icost 0.00716400146484375 0.4446212947368622\n",
            "tcost icost 0.00716400146484375 0.4446433484554291\n",
            "search 0.45166015625\n",
            "tcost icost 0.007732391357421875 0.44771960377693176\n",
            "tcost icost 0.0079498291015625 0.4444754123687744\n",
            "tcost icost 0.0079498291015625 0.4448660612106323\n",
            "search 0.452880859375\n",
            "tcost icost 0.007503509521484375 0.44475820660591125\n",
            "tcost icost 0.0069732666015625 0.44470447301864624\n",
            "tcost icost 0.0069732666015625 0.44425395131111145\n",
            "search 0.451416015625\n",
            "tcost icost 0.006832122802734375 0.4439508020877838\n",
            "tcost icost 0.006439208984375 0.4426433742046356\n",
            "tcost icost 0.006439208984375 0.4423884451389313\n",
            "search 0.44873046875\n",
            "tcost icost 0.006450653076171875 0.44785451889038086\n",
            "tcost icost 0.006435394287109375 0.44555649161338806\n",
            "tcost icost 0.006435394287109375 0.44555649161338806\n",
            "search 0.451904296875\n",
            "tcost icost 0.00682830810546875 0.44807642698287964\n",
            "tcost icost 0.0064849853515625 0.4453577399253845\n",
            "tcost icost 0.0064849853515625 0.4453577399253845\n",
            "search 0.451904296875\n",
            "tcost icost 0.0066680908203125 0.44589027762413025\n",
            "tcost icost 0.006351470947265625 0.446077436208725\n",
            "tcost icost 0.006351470947265625 0.446077436208725\n",
            "search 0.452392578125\n",
            "tcost icost 0.006832122802734375 0.44116759300231934\n",
            "tcost icost 0.00640106201171875 0.44104328751564026\n",
            "tcost icost 0.00640106201171875 0.44104328751564026\n",
            "search 0.447509765625\n",
            "tcost icost 0.007335662841796875 0.4478681981563568\n",
            "tcost icost 0.006778717041015625 0.44729894399642944\n",
            "tcost icost 0.006778717041015625 0.4472759962081909\n",
            "search 0.4541015625\n",
            "tcost icost 0.0074920654296875 0.4482922852039337\n",
            "tcost icost 0.007213592529296875 0.44925928115844727\n",
            "tcost icost 0.007213592529296875 0.44834384322166443\n",
            "search 0.45556640625\n",
            "tcost icost 0.007366180419921875 0.4485343098640442\n",
            "tcost icost 0.007080078125 0.44847339391708374\n",
            "tcost icost 0.007080078125 0.4487372636795044\n",
            "search 0.455810546875\n",
            "tcost icost 0.008026123046875 0.44766756892204285\n",
            "tcost icost 0.00716400146484375 0.44750162959098816\n",
            "tcost icost 0.00716400146484375 0.4501399099826813\n",
            "search 0.457275390625\n",
            "tcost icost 0.007488250732421875 0.4471262991428375\n",
            "tcost icost 0.00637054443359375 0.44942936301231384\n",
            "tcost icost 0.00637054443359375 0.4472704827785492\n",
            "search 0.45361328125\n",
            "tcost icost 0.00751495361328125 0.44205036759376526\n",
            "tcost icost 0.00751495361328125 0.44205036759376526\n",
            "tcost icost 0.00751495361328125 0.44205036759376526\n",
            "search 0.44970703125\n",
            "tcost icost 0.0079345703125 0.43862515687942505\n",
            "tcost icost 0.00785064697265625 0.4386361241340637\n",
            "tcost icost 0.00785064697265625 0.44065579771995544\n",
            "search 0.448486328125\n",
            "ded\n",
            "time\n",
            "21 #### train ####\n",
            "repr, std, cov, conv, closs 0.02945471554994583 0.327880859375 0.31918826699256897 0.0634469985961914 0.01601552590727806\n",
            "46.925490473840185 7.392640060334186 1.0\n",
            "repr, std, cov, conv, closs 0.03686666488647461 0.331787109375 0.2648024559020996 0.0675748884677887 0.01562986522912979\n",
            "47.11347417642725 7.526844710428666 1.0\n",
            "repr, std, cov, conv, closs 0.032552897930145264 0.333984375 0.3008521795272827 0.059804193675518036 0.015541963279247284\n",
            "47.53919549255947 7.717291288961148 1.0\n",
            "repr, std, cov, conv, closs 0.034827157855033875 0.334228515625 0.291976660490036 0.06965071707963943 0.015591872856020927\n",
            "47.77736733760687 7.818220237111288 1.0\n",
            "repr, std, cov, conv, closs 0.03266805782914162 0.333740234375 0.3150567412376404 0.05731627345085144 0.030288174748420715\n",
            "47.82514470494447 7.976079054470414 1.0\n",
            "repr, std, cov, conv, closs 0.03368587791919708 0.334228515625 0.266230970621109 0.06390094757080078 0.0006099424208514392\n",
            "48.11281390756523 8.088472895719768 1.0\n",
            "repr, std, cov, conv, closs 0.036261703819036484 0.333740234375 0.307547390460968 0.060426995158195496 0.02121959812939167\n",
            "48.257296735842445 8.161561017257375 1.0\n",
            "repr, std, cov, conv, closs 0.03900273144245148 0.330078125 0.3315250873565674 0.06306350231170654 0.029244715347886086\n",
            "48.35385958661085 8.169722578274632 1.0\n",
            "repr, std, cov, conv, closs 0.036994680762290955 0.32470703125 0.3308931887149811 0.06045500561594963 0.004722394049167633\n",
            "47.968763662318544 7.960150792734154 1.0\n",
            "repr, std, cov, conv, closs 0.04432150721549988 0.320556640625 0.3577211797237396 0.07007986307144165 0.0015397153329104185\n",
            "47.63432142274007 7.833864495805746 1.0\n",
            "repr, std, cov, conv, closs 0.034891240298748016 0.32421875 0.33216166496276855 0.06898332387208939 0.0025457600131630898\n",
            "47.160587650603674 7.610055223119162 1.0\n",
            "repr, std, cov, conv, closs 0.028554361313581467 0.33154296875 0.3294844627380371 0.0631723552942276 0.003680998459458351\n",
            "46.73825682997955 7.526844710428666 1.0\n",
            "repr, std, cov, conv, closs 0.028072871267795563 0.326904296875 0.30789661407470703 0.05858589708805084 0.015906376764178276\n",
            "46.69156526471484 7.407432733094913 1.0\n",
            "repr, std, cov, conv, closs 0.042742770165205 0.322998046875 0.3534260392189026 0.05965282768011093 0.035275474190711975\n",
            "47.01938838027833 7.363143278898984 1.0\n",
            "repr, std, cov, conv, closs 0.03479210287332535 0.32861328125 0.30674540996551514 0.07145294547080994 0.008913529105484486\n",
            "47.44425952924147 7.2246328144569665 1.0\n",
            "repr, std, cov, conv, closs 0.036143794655799866 0.328857421875 0.33126452565193176 0.06972857564687729 0.0015431403880938888\n",
            "47.72963769990697 7.275357214562424 1.0\n",
            "repr, std, cov, conv, closs 0.04687732085585594 0.32861328125 0.3312234878540039 0.05352654308080673 0.01165681704878807\n",
            "48.16092672147279 7.304502324672662 1.0\n",
            "repr, std, cov, conv, closs 0.030400503426790237 0.33203125 0.2882832884788513 0.05183185264468193 0.014480320736765862\n",
            "48.35385958661085 7.319118633824331 1.0\n",
            "repr, std, cov, conv, closs 0.04044190049171448 0.332763671875 0.3120619058609009 0.05857604369521141 0.0008362189400941133\n",
            "49.03523147134977 7.451988589244286 1.0\n",
            "repr, std, cov, conv, closs 0.0411936491727829 0.330078125 0.30816513299942017 0.06591726839542389 0.0008208859944716096\n",
            "49.08426670282111 7.496812449760183 1.0\n",
            "repr, std, cov, conv, closs 0.030319757759571075 0.3310546875 0.31061336398124695 0.05225894972681999 0.002719379495829344\n",
            "49.4783179480689 7.519325385043623 1.0\n",
            "repr, std, cov, conv, closs 0.03644060716032982 0.32861328125 0.3259038031101227 0.06254765391349792 0.023322559893131256\n",
            "49.330179370090356 7.579690950843743 1.0\n",
            "repr, std, cov, conv, closs 0.04108539596199989 0.330810546875 0.31582677364349365 0.06975989788770676 0.00037092319689691067\n",
            "49.23166680481393 7.678820322319725 1.0\n",
            "repr, std, cov, conv, closs 0.026471637189388275 0.33349609375 0.3041272759437561 0.0687018483877182 0.02218148671090603\n",
            "49.182484320493444 7.7714746614764 1.0\n",
            "repr, std, cov, conv, closs 0.038767144083976746 0.332763671875 0.3002321720123291 0.0661235898733139 0.030859429389238358\n",
            "49.62690138634525 7.976079054470414 1.0\n",
            "repr, std, cov, conv, closs 0.04006534069776535 0.329345703125 0.307106077671051 0.0630357414484024 0.021613191813230515\n",
            "50.02530893325793 8.064255927106414 1.0\n",
            "repr, std, cov, conv, closs 0.03023051656782627 0.32958984375 0.3597344756126404 0.05400259420275688 0.03212813660502434\n",
            "49.72620481601932 8.088472895719768 1.0\n",
            "repr, std, cov, conv, closs 0.03837570175528526 0.3359375 0.29680752754211426 0.05799705907702446 0.0004911364521831274\n",
            "49.72620481601932 8.056199727379036 1.0\n",
            "repr, std, cov, conv, closs 0.03707669675350189 0.3310546875 0.3132837414741516 0.06232535466551781 0.0016815703129395843\n",
            "49.52779626601696 8.13712522207835 1.0\n",
            "repr, std, cov, conv, closs 0.03809582442045212 0.32666015625 0.3387763500213623 0.062223926186561584 0.00026720052119344473\n",
            "49.379509549460444 8.040111464338896 1.0\n",
            "repr, std, cov, conv, closs 0.03729349374771118 0.326416015625 0.35562995076179504 0.06047782301902771 0.028797514736652374\n",
            "48.93730791820545 7.865246988319233 1.0\n",
            "repr, std, cov, conv, closs 0.045051224529743195 0.327880859375 0.34283334016799927 0.06388239562511444 0.011346714571118355\n",
            "48.693353728846894 7.787025382274013 1.0\n",
            "repr, std, cov, conv, closs 0.046617504209280014 0.324462890625 0.32345515489578247 0.06425592303276062 0.025990765541791916\n",
            "48.59611290692016 7.648181675925595 1.0\n",
            "repr, std, cov, conv, closs 0.033263564109802246 0.328857421875 0.36280959844589233 0.061373718082904816 0.0005924502620473504\n",
            "48.45061565964365 7.526844710428666 1.0\n",
            "repr, std, cov, conv, closs 0.037466369569301605 0.328125 0.29813316464424133 0.07321497052907944 0.0021549267694354057\n",
            "48.11281390756523 7.377876928600059 1.0\n",
            "repr, std, cov, conv, closs 0.045208126306533813 0.330322265625 0.3489632308483124 0.07453795522451401 0.014980498701334\n",
            "48.35385958661085 7.392640060334186 1.0\n",
            "tcost icost 0.002780914306640625 0.41897228360176086\n",
            "tcost icost 0.003284454345703125 0.4196327328681946\n",
            "tcost icost 0.003284454345703125 0.4195226728916168\n",
            "search 0.422607421875\n",
            "tcost icost 0.0034351348876953125 0.41905882954597473\n",
            "tcost icost 0.0028972625732421875 0.4205518364906311\n",
            "tcost icost 0.0028972625732421875 0.41834715008735657\n",
            "search 0.42138671875\n",
            "tcost icost 0.0031261444091796875 0.4171995222568512\n",
            "tcost icost 0.0028972625732421875 0.4160756468772888\n",
            "tcost icost 0.0028972625732421875 0.41729936003685\n",
            "search 0.420166015625\n",
            "tcost icost 0.002780914306640625 0.41583094000816345\n",
            "tcost icost 0.003284454345703125 0.41564837098121643\n",
            "tcost icost 0.003284454345703125 0.41482698917388916\n",
            "search 0.41796875\n",
            "tcost icost 0.0031337738037109375 0.4172188341617584\n",
            "tcost icost 0.00290679931640625 0.415690541267395\n",
            "tcost icost 0.00290679931640625 0.4166392982006073\n",
            "search 0.419677734375\n",
            "tcost icost 0.00363922119140625 0.4155253469944\n",
            "tcost icost 0.0033168792724609375 0.41327551007270813\n",
            "tcost icost 0.0033168792724609375 0.41495242714881897\n",
            "search 0.41845703125\n",
            "tcost icost 0.003757476806640625 0.4129236042499542\n",
            "tcost icost 0.00339508056640625 0.4109572470188141\n",
            "tcost icost 0.00339508056640625 0.411373108625412\n",
            "search 0.414794921875\n",
            "tcost icost 0.0029163360595703125 0.4108351171016693\n",
            "tcost icost 0.0034732818603515625 0.4102841913700104\n",
            "tcost icost 0.0034732818603515625 0.4122125208377838\n",
            "search 0.41552734375\n",
            "tcost icost 0.003231048583984375 0.4076095223426819\n",
            "tcost icost 0.0028133392333984375 0.40900489687919617\n",
            "tcost icost 0.0028133392333984375 0.4106142520904541\n",
            "search 0.41357421875\n",
            "tcost icost 0.003322601318359375 0.41091153025627136\n",
            "tcost icost 0.00289154052734375 0.4107944071292877\n",
            "tcost icost 0.00289154052734375 0.4091681241989136\n",
            "search 0.412109375\n",
            "tcost icost 0.0029315948486328125 0.406786173582077\n",
            "tcost icost 0.002674102783203125 0.408995121717453\n",
            "tcost icost 0.002674102783203125 0.40677258372306824\n",
            "search 0.409423828125\n",
            "tcost icost 0.0030574798583984375 0.4053567051887512\n",
            "tcost icost 0.002666473388671875 0.4071384072303772\n",
            "tcost icost 0.002666473388671875 0.4047043025493622\n",
            "search 0.407470703125\n",
            "tcost icost 0.0031948089599609375 0.4041869044303894\n",
            "tcost icost 0.002780914306640625 0.4041258990764618\n",
            "tcost icost 0.002780914306640625 0.4041258692741394\n",
            "search 0.40673828125\n",
            "tcost icost 0.0028171539306640625 0.402678519487381\n",
            "tcost icost 0.00334930419921875 0.4012397229671478\n",
            "tcost icost 0.00334930419921875 0.4026866853237152\n",
            "search 0.406005859375\n",
            "tcost icost 0.0033054351806640625 0.4027179479598999\n",
            "tcost icost 0.0028858184814453125 0.4036640226840973\n",
            "tcost icost 0.0028858184814453125 0.40276867151260376\n",
            "search 0.40576171875\n",
            "tcost icost 0.003131866455078125 0.4020152688026428\n",
            "tcost icost 0.002910614013671875 0.40204110741615295\n",
            "tcost icost 0.002910614013671875 0.4040101170539856\n",
            "search 0.406982421875\n",
            "tcost icost 0.00308990478515625 0.4010876715183258\n",
            "tcost icost 0.00286102294921875 0.40088045597076416\n",
            "tcost icost 0.00286102294921875 0.4027441143989563\n",
            "search 0.40576171875\n",
            "tcost icost 0.0030231475830078125 0.39844632148742676\n",
            "tcost icost 0.002780914306640625 0.39884570240974426\n",
            "tcost icost 0.002780914306640625 0.3987368941307068\n",
            "search 0.4013671875\n",
            "tcost icost 0.00308990478515625 0.4002439081668854\n",
            "tcost icost 0.0028705596923828125 0.39790353178977966\n",
            "tcost icost 0.0028705596923828125 0.39722466468811035\n",
            "search 0.400146484375\n",
            "tcost icost 0.00315093994140625 0.39545875787734985\n",
            "tcost icost 0.0034027099609375 0.3970440626144409\n",
            "tcost icost 0.0034027099609375 0.3950951397418976\n",
            "search 0.3984375\n",
            "tcost icost 0.0032806396484375 0.3932332396507263\n",
            "tcost icost 0.00286102294921875 0.39493033289909363\n",
            "tcost icost 0.00286102294921875 0.3949974775314331\n",
            "search 0.39794921875\n",
            "tcost icost 0.0027828216552734375 0.3920941948890686\n",
            "tcost icost 0.0027828216552734375 0.39169222116470337\n",
            "tcost icost 0.0027828216552734375 0.3929098844528198\n",
            "search 0.3955078125\n",
            "tcost icost 0.0028629302978515625 0.39141592383384705\n",
            "tcost icost 0.0028629302978515625 0.394993394613266\n",
            "tcost icost 0.0028629302978515625 0.3933756649494171\n",
            "search 0.396240234375\n",
            "tcost icost 0.0030765533447265625 0.3917482793331146\n",
            "tcost icost 0.00286102294921875 0.3901401162147522\n",
            "tcost icost 0.00286102294921875 0.3901401162147522\n",
            "search 0.39306640625\n",
            "tcost icost 0.0030765533447265625 0.38804391026496887\n",
            "tcost icost 0.002838134765625 0.3879423141479492\n",
            "tcost icost 0.002838134765625 0.3911936581134796\n",
            "search 0.39404296875\n",
            "tcost icost 0.0033721923828125 0.3896700143814087\n",
            "tcost icost 0.002857208251953125 0.3862791359424591\n",
            "tcost icost 0.002857208251953125 0.3880453407764435\n",
            "search 0.390869140625\n",
            "tcost icost 0.0030879974365234375 0.38804030418395996\n",
            "tcost icost 0.002899169921875 0.3852007985115051\n",
            "tcost icost 0.002899169921875 0.38804057240486145\n",
            "search 0.390869140625\n",
            "tcost icost 0.003063201904296875 0.3864663541316986\n",
            "tcost icost 0.00284576416015625 0.3864695131778717\n",
            "tcost icost 0.00284576416015625 0.388045072555542\n",
            "search 0.390869140625\n",
            "tcost icost 0.003269195556640625 0.38643747568130493\n",
            "tcost icost 0.00286865234375 0.38571491837501526\n",
            "tcost icost 0.00286865234375 0.38804227113723755\n",
            "search 0.390869140625\n",
            "tcost icost 0.002780914306640625 0.3864787220954895\n",
            "tcost icost 0.002780914306640625 0.38603317737579346\n",
            "tcost icost 0.002780914306640625 0.38491421937942505\n",
            "search 0.3876953125\n",
            "tcost icost 0.0045013427734375 0.38384759426116943\n",
            "tcost icost 0.003360748291015625 0.3839404881000519\n",
            "tcost icost 0.003360748291015625 0.3841750919818878\n",
            "search 0.3876953125\n",
            "tcost icost 0.003643035888671875 0.386043518781662\n",
            "tcost icost 0.00333404541015625 0.3858124613761902\n",
            "tcost icost 0.00333404541015625 0.3861130177974701\n",
            "search 0.3896484375\n",
            "tcost icost 0.003879547119140625 0.3866584599018097\n",
            "tcost icost 0.0033054351806640625 0.38457420468330383\n",
            "tcost icost 0.0033054351806640625 0.3851032853126526\n",
            "search 0.388427734375\n",
            "tcost icost 0.003910064697265625 0.3849910497665405\n",
            "tcost icost 0.003330230712890625 0.38516107201576233\n",
            "tcost icost 0.003330230712890625 0.38876381516456604\n",
            "search 0.39208984375\n",
            "tcost icost 0.0035610198974609375 0.3893846869468689\n",
            "tcost icost 0.0035610198974609375 0.3890360891819\n",
            "tcost icost 0.0035610198974609375 0.3875665068626404\n",
            "search 0.39111328125\n",
            "tcost icost 0.003986358642578125 0.3938947021961212\n",
            "tcost icost 0.0031261444091796875 0.3920849561691284\n",
            "tcost icost 0.0031261444091796875 0.390275239944458\n",
            "search 0.3935546875\n",
            "tcost icost 0.0039005279541015625 0.39170584082603455\n",
            "tcost icost 0.0032100677490234375 0.3905240595340729\n",
            "tcost icost 0.0032100677490234375 0.39237579703330994\n",
            "search 0.3955078125\n",
            "tcost icost 0.0038509368896484375 0.39240485429763794\n",
            "tcost icost 0.003139495849609375 0.39630627632141113\n",
            "tcost icost 0.003139495849609375 0.39630627632141113\n",
            "search 0.3994140625\n",
            "tcost icost 0.0037403106689453125 0.39383935928344727\n",
            "tcost icost 0.002864837646484375 0.39237841963768005\n",
            "tcost icost 0.002864837646484375 0.39317360520362854\n",
            "search 0.39599609375\n",
            "tcost icost 0.0033550262451171875 0.3955858051776886\n",
            "tcost icost 0.0033206939697265625 0.39590930938720703\n",
            "tcost icost 0.0033206939697265625 0.39405885338783264\n",
            "search 0.3974609375\n",
            "tcost icost 0.0030765533447265625 0.3951794505119324\n",
            "tcost icost 0.002437591552734375 0.3943566083908081\n",
            "tcost icost 0.002437591552734375 0.39533576369285583\n",
            "search 0.397705078125\n",
            "tcost icost 0.0033473968505859375 0.39813825488090515\n",
            "tcost icost 0.0024394989013671875 0.39585214853286743\n",
            "tcost icost 0.0024394989013671875 0.3978613018989563\n",
            "search 0.400390625\n",
            "tcost icost 0.003376007080078125 0.3955124020576477\n",
            "tcost icost 0.003345489501953125 0.39650148153305054\n",
            "tcost icost 0.003345489501953125 0.3967311382293701\n",
            "search 0.400146484375\n",
            "tcost icost 0.0036563873291015625 0.3967888057231903\n",
            "tcost icost 0.0034332275390625 0.3967476785182953\n",
            "tcost icost 0.0034332275390625 0.3968609869480133\n",
            "search 0.400390625\n",
            "tcost icost 0.0033130645751953125 0.3994900584220886\n",
            "tcost icost 0.0024318695068359375 0.4002421200275421\n",
            "tcost icost 0.0024318695068359375 0.3998960256576538\n",
            "search 0.40234375\n",
            "tcost icost 0.0026569366455078125 0.40120929479599\n",
            "tcost icost 0.0023403167724609375 0.4026812016963959\n",
            "tcost icost 0.0023403167724609375 0.40276098251342773\n",
            "search 0.4052734375\n",
            "tcost icost 0.0026683807373046875 0.40308257937431335\n",
            "tcost icost 0.0024204254150390625 0.402765691280365\n",
            "tcost icost 0.0024204254150390625 0.40316057205200195\n",
            "search 0.405517578125\n",
            "tcost icost 0.002552032470703125 0.40372103452682495\n",
            "tcost icost 0.0020160675048828125 0.4031841456890106\n",
            "tcost icost 0.0020160675048828125 0.4057048559188843\n",
            "search 0.40771484375\n",
            "tcost icost 0.0026988983154296875 0.40792739391326904\n",
            "tcost icost 0.0020999908447265625 0.40572288632392883\n",
            "tcost icost 0.0020999908447265625 0.40675970911979675\n",
            "search 0.408935546875\n",
            "tcost icost 0.0027065277099609375 0.4089095890522003\n",
            "tcost icost 0.0020999908447265625 0.41042113304138184\n",
            "tcost icost 0.0020999908447265625 0.4085812568664551\n",
            "search 0.410888671875\n",
            "tcost icost 0.0031890869140625 0.406612366437912\n",
            "tcost icost 0.0023021697998046875 0.40832385420799255\n",
            "tcost icost 0.0023021697998046875 0.40738779306411743\n",
            "search 0.40966796875\n",
            "tcost icost 0.0030956268310546875 0.4071815311908722\n",
            "tcost icost 0.00279998779296875 0.40725064277648926\n",
            "tcost icost 0.00279998779296875 0.40675437450408936\n",
            "search 0.409423828125\n",
            "tcost icost 0.00323486328125 0.40954968333244324\n",
            "tcost icost 0.003047943115234375 0.40738099813461304\n",
            "tcost icost 0.003047943115234375 0.4070475101470947\n",
            "search 0.409912109375\n",
            "tcost icost 0.0033130645751953125 0.40891629457473755\n",
            "tcost icost 0.003269195556640625 0.40655726194381714\n",
            "tcost icost 0.003269195556640625 0.4107755422592163\n",
            "search 0.4140625\n",
            "tcost icost 0.00311279296875 0.4096551239490509\n",
            "tcost icost 0.0028476715087890625 0.40901052951812744\n",
            "tcost icost 0.0028476715087890625 0.4108906686306\n",
            "search 0.413818359375\n",
            "tcost icost 0.0028820037841796875 0.4106135070323944\n",
            "tcost icost 0.0027942657470703125 0.41108331084251404\n",
            "tcost icost 0.0027942657470703125 0.41108331084251404\n",
            "search 0.413818359375\n",
            "tcost icost 0.0022563934326171875 0.4093724191188812\n",
            "tcost icost 0.0027751922607421875 0.40921205282211304\n",
            "tcost icost 0.0027751922607421875 0.41082772612571716\n",
            "search 0.41357421875\n",
            "tcost icost 0.00270843505859375 0.41491082310676575\n",
            "tcost icost 0.0031795501708984375 0.4140893518924713\n",
            "tcost icost 0.0031795501708984375 0.4140893816947937\n",
            "search 0.417236328125\n",
            "tcost icost 0.003009796142578125 0.4115773141384125\n",
            "tcost icost 0.0030269622802734375 0.413712739944458\n",
            "tcost icost 0.0030269622802734375 0.41451025009155273\n",
            "search 0.41748046875\n",
            "tcost icost 0.0037975311279296875 0.4128337502479553\n",
            "tcost icost 0.003414154052734375 0.4152524173259735\n",
            "tcost icost 0.003414154052734375 0.41341838240623474\n",
            "search 0.416748046875\n",
            "tcost icost 0.0027523040771484375 0.41584208607673645\n",
            "tcost icost 0.002803802490234375 0.41280248761177063\n",
            "tcost icost 0.002803802490234375 0.4158124029636383\n",
            "search 0.41845703125\n",
            "tcost icost 0.0020580291748046875 0.4135366976261139\n",
            "tcost icost 0.002178192138671875 0.415862500667572\n",
            "tcost icost 0.002178192138671875 0.41319605708122253\n",
            "search 0.415283203125\n",
            "tcost icost 0.0027313232421875 0.4133557379245758\n",
            "tcost icost 0.002590179443359375 0.41427189111709595\n",
            "tcost icost 0.002590179443359375 0.4149174690246582\n",
            "search 0.417724609375\n",
            "tcost icost 0.0026493072509765625 0.41216200590133667\n",
            "tcost icost 0.002498626708984375 0.41525593400001526\n",
            "tcost icost 0.002498626708984375 0.41271740198135376\n",
            "search 0.4150390625\n",
            "tcost icost 0.003231048583984375 0.4127248525619507\n",
            "tcost icost 0.002826690673828125 0.41526493430137634\n",
            "tcost icost 0.002826690673828125 0.41376829147338867\n",
            "search 0.416748046875\n",
            "tcost icost 0.0031108856201171875 0.41219615936279297\n",
            "tcost icost 0.0027408599853515625 0.41531750559806824\n",
            "tcost icost 0.0027408599853515625 0.4145366847515106\n",
            "search 0.417236328125\n",
            "tcost icost 0.0025997161865234375 0.41341739892959595\n",
            "tcost icost 0.002506256103515625 0.4124390482902527\n",
            "tcost icost 0.002506256103515625 0.4143352806568146\n",
            "search 0.416748046875\n",
            "tcost icost 0.002960205078125 0.4142829179763794\n",
            "tcost icost 0.002681732177734375 0.4162128269672394\n",
            "tcost icost 0.002681732177734375 0.41425615549087524\n",
            "search 0.4169921875\n",
            "tcost icost 0.0028438568115234375 0.4160541594028473\n",
            "tcost icost 0.00251007080078125 0.4163456857204437\n",
            "tcost icost 0.00251007080078125 0.41583120822906494\n",
            "search 0.418212890625\n",
            "tcost icost 0.002162933349609375 0.41567230224609375\n",
            "tcost icost 0.00203704833984375 0.4156593084335327\n",
            "tcost icost 0.00203704833984375 0.4137886166572571\n",
            "search 0.415771484375\n",
            "tcost icost 0.0036029815673828125 0.41391855478286743\n",
            "tcost icost 0.00335693359375 0.41297242045402527\n",
            "tcost icost 0.00335693359375 0.41421401500701904\n",
            "search 0.417724609375\n",
            "tcost icost 0.00312042236328125 0.41572245955467224\n",
            "tcost icost 0.0025081634521484375 0.41358357667922974\n",
            "tcost icost 0.0025081634521484375 0.4133911430835724\n",
            "search 0.415771484375\n",
            "tcost icost 0.0031337738037109375 0.41441610455513\n",
            "tcost icost 0.0022830963134765625 0.41464102268218994\n",
            "tcost icost 0.0022830963134765625 0.4169537425041199\n",
            "search 0.419189453125\n",
            "tcost icost 0.0035457611083984375 0.41384992003440857\n",
            "tcost icost 0.003467559814453125 0.41314420104026794\n",
            "tcost icost 0.003467559814453125 0.41396793723106384\n",
            "search 0.41748046875\n",
            "tcost icost 0.0035991668701171875 0.41609853506088257\n",
            "tcost icost 0.0033416748046875 0.4174681305885315\n",
            "tcost icost 0.0033416748046875 0.4152606129646301\n",
            "search 0.418701171875\n",
            "tcost icost 0.0026683807373046875 0.41530361771583557\n",
            "tcost icost 0.0025997161865234375 0.41549381613731384\n",
            "tcost icost 0.0025997161865234375 0.4172159731388092\n",
            "search 0.419921875\n",
            "tcost icost 0.0037975311279296875 0.418241024017334\n",
            "tcost icost 0.0029201507568359375 0.41823431849479675\n",
            "tcost icost 0.0029201507568359375 0.42006897926330566\n",
            "search 0.423095703125\n",
            "tcost icost 0.002811431884765625 0.42030757665634155\n",
            "tcost icost 0.002513885498046875 0.4198918342590332\n",
            "tcost icost 0.002513885498046875 0.42093634605407715\n",
            "search 0.42333984375\n",
            "tcost icost 0.0036563873291015625 0.4219169318675995\n",
            "tcost icost 0.00337982177734375 0.42037224769592285\n",
            "tcost icost 0.00337982177734375 0.4207562506198883\n",
            "search 0.424072265625\n",
            "tcost icost 0.0030670166015625 0.4236954152584076\n",
            "tcost icost 0.0030670166015625 0.42361658811569214\n",
            "tcost icost 0.0030670166015625 0.4212351441383362\n",
            "search 0.42431640625\n",
            "tcost icost 0.0037097930908203125 0.42267411947250366\n",
            "tcost icost 0.003452301025390625 0.4227942228317261\n",
            "tcost icost 0.003452301025390625 0.4229004681110382\n",
            "search 0.42626953125\n",
            "tcost icost 0.0040435791015625 0.4261679947376251\n",
            "tcost icost 0.0035381317138671875 0.42242470383644104\n",
            "tcost icost 0.0035381317138671875 0.4221050441265106\n",
            "search 0.425537109375\n",
            "tcost icost 0.00405120849609375 0.4220467507839203\n",
            "tcost icost 0.003978729248046875 0.4202200472354889\n",
            "tcost icost 0.003978729248046875 0.4188731014728546\n",
            "search 0.4228515625\n",
            "tcost icost 0.003025054931640625 0.4211869537830353\n",
            "tcost icost 0.0026702880859375 0.42197564244270325\n",
            "tcost icost 0.0026702880859375 0.42071059346199036\n",
            "search 0.42333984375\n",
            "tcost icost 0.005107879638671875 0.41926053166389465\n",
            "tcost icost 0.003879547119140625 0.4186622202396393\n",
            "tcost icost 0.003879547119140625 0.42082178592681885\n",
            "search 0.4248046875\n",
            "tcost icost 0.0033168792724609375 0.41939449310302734\n",
            "tcost icost 0.00330352783203125 0.4195843040943146\n",
            "tcost icost 0.00330352783203125 0.4199022948741913\n",
            "search 0.42333984375\n",
            "tcost icost 0.0031299591064453125 0.4201720356941223\n",
            "tcost icost 0.002899169921875 0.4216270446777344\n",
            "tcost icost 0.002899169921875 0.42065638303756714\n",
            "search 0.423583984375\n",
            "tcost icost 0.0028896331787109375 0.4205836355686188\n",
            "tcost icost 0.002750396728515625 0.4212048053741455\n",
            "tcost icost 0.002750396728515625 0.4212048053741455\n",
            "search 0.423828125\n",
            "tcost icost 0.0028972625732421875 0.42031049728393555\n",
            "tcost icost 0.0027599334716796875 0.422359436750412\n",
            "tcost icost 0.0027599334716796875 0.4200926423072815\n",
            "search 0.4228515625\n",
            "tcost icost 0.0025310516357421875 0.42105984687805176\n",
            "tcost icost 0.0025310516357421875 0.4229945242404938\n",
            "tcost icost 0.0025310516357421875 0.4215034246444702\n",
            "search 0.423828125\n",
            "tcost icost 0.002838134765625 0.4220496416091919\n",
            "tcost icost 0.00231170654296875 0.42195576429367065\n",
            "tcost icost 0.00231170654296875 0.42401623725891113\n",
            "search 0.42626953125\n",
            "tcost icost 0.0025463104248046875 0.4216478765010834\n",
            "tcost icost 0.0025463104248046875 0.42224636673927307\n",
            "tcost icost 0.0025463104248046875 0.421819269657135\n",
            "search 0.42431640625\n",
            "tcost icost 0.0030498504638671875 0.4251139461994171\n",
            "tcost icost 0.002483367919921875 0.42212969064712524\n",
            "tcost icost 0.002483367919921875 0.4233212471008301\n",
            "search 0.42578125\n",
            "tcost icost 0.002941131591796875 0.4247244894504547\n",
            "tcost icost 0.00252532958984375 0.4232214093208313\n",
            "tcost icost 0.00252532958984375 0.4236133396625519\n",
            "search 0.426025390625\n",
            "tcost icost 0.004001617431640625 0.42134594917297363\n",
            "tcost icost 0.003635406494140625 0.4207617938518524\n",
            "tcost icost 0.003635406494140625 0.4207725524902344\n",
            "search 0.42431640625\n",
            "tcost icost 0.00209808349609375 0.42419666051864624\n",
            "tcost icost 0.0021381378173828125 0.42345792055130005\n",
            "tcost icost 0.0021381378173828125 0.422162801027298\n",
            "search 0.42431640625\n",
            "tcost icost 0.0037097930908203125 0.4223100543022156\n",
            "tcost icost 0.00335693359375 0.4216616153717041\n",
            "tcost icost 0.00335693359375 0.4214839041233063\n",
            "search 0.4248046875\n",
            "tcost icost 0.0029430389404296875 0.42507609724998474\n",
            "tcost icost 0.002552032470703125 0.4230842888355255\n",
            "tcost icost 0.002552032470703125 0.42267701029777527\n",
            "search 0.425048828125\n",
            "tcost icost 0.0029125213623046875 0.42421838641166687\n",
            "tcost icost 0.002590179443359375 0.4284472167491913\n",
            "tcost icost 0.002590179443359375 0.4265631437301636\n",
            "search 0.42919921875\n",
            "tcost icost 0.00235748291015625 0.4244123101234436\n",
            "tcost icost 0.0021915435791015625 0.42377060651779175\n",
            "tcost icost 0.0021915435791015625 0.42385920882225037\n",
            "search 0.426025390625\n",
            "tcost icost 0.0029296875 0.4237489104270935\n",
            "tcost icost 0.0024242401123046875 0.423536092042923\n",
            "tcost icost 0.0024242401123046875 0.4236629009246826\n",
            "search 0.426025390625\n",
            "tcost icost 0.0026302337646484375 0.42471829056739807\n",
            "tcost icost 0.002532958984375 0.4256260395050049\n",
            "tcost icost 0.002532958984375 0.4245634078979492\n",
            "search 0.427001953125\n",
            "tcost icost 0.002655029296875 0.4239284098148346\n",
            "tcost icost 0.002593994140625 0.4244995415210724\n",
            "tcost icost 0.002593994140625 0.4238455295562744\n",
            "search 0.426513671875\n",
            "tcost icost 0.002960205078125 0.4224490523338318\n",
            "tcost icost 0.002826690673828125 0.4249093532562256\n",
            "tcost icost 0.002826690673828125 0.4237573742866516\n",
            "search 0.4267578125\n",
            "tcost icost 0.0033206939697265625 0.4241933226585388\n",
            "tcost icost 0.002593994140625 0.42606547474861145\n",
            "tcost icost 0.002593994140625 0.4260654151439667\n",
            "search 0.4287109375\n",
            "tcost icost 0.00292205810546875 0.4238225519657135\n",
            "tcost icost 0.0025882720947265625 0.42575034499168396\n",
            "tcost icost 0.0025882720947265625 0.42341405153274536\n",
            "search 0.426025390625\n",
            "tcost icost 0.0029468536376953125 0.42301666736602783\n",
            "tcost icost 0.00246429443359375 0.42528849840164185\n",
            "tcost icost 0.00246429443359375 0.42327171564102173\n",
            "search 0.42578125\n",
            "tcost icost 0.0030670166015625 0.42230042815208435\n",
            "tcost icost 0.0019359588623046875 0.4214574992656708\n",
            "tcost icost 0.0019359588623046875 0.4228975176811218\n",
            "search 0.4248046875\n",
            "tcost icost 0.0025463104248046875 0.42502763867378235\n",
            "tcost icost 0.003170013427734375 0.4228896200656891\n",
            "tcost icost 0.003170013427734375 0.4241529405117035\n",
            "search 0.42724609375\n",
            "tcost icost 0.003742218017578125 0.4269607663154602\n",
            "tcost icost 0.003345489501953125 0.42423200607299805\n",
            "tcost icost 0.003345489501953125 0.4251185953617096\n",
            "search 0.428466796875\n",
            "tcost icost 0.0026340484619140625 0.42338287830352783\n",
            "tcost icost 0.0019474029541015625 0.42280587553977966\n",
            "tcost icost 0.0019474029541015625 0.4231320023536682\n",
            "search 0.425048828125\n",
            "tcost icost 0.002521514892578125 0.42421630024909973\n",
            "tcost icost 0.0027217864990234375 0.42592859268188477\n",
            "tcost icost 0.0027217864990234375 0.4235527813434601\n",
            "search 0.42626953125\n",
            "tcost icost 0.002758026123046875 0.42523300647735596\n",
            "tcost icost 0.0025081634521484375 0.42579951882362366\n",
            "tcost icost 0.0025081634521484375 0.42518746852874756\n",
            "search 0.427734375\n",
            "tcost icost 0.0025844573974609375 0.42401123046875\n",
            "tcost icost 0.0030364990234375 0.4245568513870239\n",
            "tcost icost 0.0030364990234375 0.4251498281955719\n",
            "search 0.427978515625\n",
            "tcost icost 0.00266265869140625 0.42212146520614624\n",
            "tcost icost 0.0019550323486328125 0.4246237874031067\n",
            "tcost icost 0.0019550323486328125 0.4227792024612427\n",
            "search 0.4248046875\n",
            "tcost icost 0.0033016204833984375 0.424940824508667\n",
            "tcost icost 0.00262451171875 0.4256051480770111\n",
            "tcost icost 0.00262451171875 0.42411965131759644\n",
            "search 0.4267578125\n",
            "tcost icost 0.003437042236328125 0.4243408441543579\n",
            "tcost icost 0.0030841827392578125 0.423949658870697\n",
            "tcost icost 0.0030841827392578125 0.424617737531662\n",
            "search 0.427734375\n",
            "ded\n",
            "time\n",
            "22 #### train ####\n",
            "repr, std, cov, conv, closs 0.04766629636287689 0.326904296875 0.31004956364631653 0.062166087329387665 0.019123557955026627\n",
            "48.98624522612365 7.444544045199088 1.0\n",
            "repr, std, cov, conv, closs 0.048450618982315063 0.326904296875 0.3304611146450043 0.05790693312883377 0.041942089796066284\n",
            "49.4288890590099 7.5118135714721515 1.0\n",
            "repr, std, cov, conv, closs 0.047737255692481995 0.3291015625 0.34029728174209595 0.07235341519117355 0.038856714963912964\n",
            "49.97533359965828 7.648181675925595 1.0\n",
            "repr, std, cov, conv, closs 0.03665103390812874 0.329833984375 0.305420845746994 0.06435969471931458 0.011218068189918995\n",
            "50.07533424219118 7.717291288961148 1.0\n",
            "repr, std, cov, conv, closs 0.035108525305986404 0.331298828125 0.31541702151298523 0.06118461489677429 0.016659514978528023\n",
            "50.02530893325793 7.740466322419187 1.0\n",
            "repr, std, cov, conv, closs 0.04323992505669594 0.330078125 0.30208754539489746 0.05623108893632889 0.015359197743237019\n",
            "50.225710520995804 7.794812407656286 1.0\n",
            "repr, std, cov, conv, closs 0.05219355598092079 0.326416015625 0.348130464553833 0.049181073904037476 0.0017004769761115313\n",
            "50.32621216774831 7.678820322319725 1.0\n",
            "repr, std, cov, conv, closs 0.04471339285373688 0.32275390625 0.40247178077697754 0.06278635561466217 0.015968533232808113\n",
            "50.12540957643337 7.556997280453546 1.0\n",
            "repr, std, cov, conv, closs 0.03183192387223244 0.328857421875 0.30995091795921326 0.05462568253278732 0.04239051416516304\n",
            "49.72620481601932 7.45944057783353 1.0\n",
            "repr, std, cov, conv, closs 0.04424940049648285 0.324951171875 0.36024898290634155 0.06106546148657799 0.0013762122252956033\n",
            "49.330179370090356 7.385254805528658 1.0\n",
            "repr, std, cov, conv, closs 0.04614788293838501 0.32421875 0.385730504989624 0.055447712540626526 0.06110022962093353\n",
            "49.4288890590099 7.2972051195531105 1.0\n",
            "repr, std, cov, conv, closs 0.036006633192300797 0.33154296875 0.3219258189201355 0.05882048234343529 0.0006421279394999146\n",
            "49.379509549460444 7.282632571776986 1.0\n",
            "repr, std, cov, conv, closs 0.029333502054214478 0.329345703125 0.33158883452415466 0.05136136710643768 0.01637798547744751\n",
            "49.28089847161874 7.26082829713985 1.0\n",
            "repr, std, cov, conv, closs 0.045380398631095886 0.326904296875 0.3454285264015198 0.0589505136013031 0.001192791503854096\n",
            "49.28089847161874 7.311806826997334 1.0\n",
            "repr, std, cov, conv, closs 0.04933296516537666 0.32666015625 0.35993656516075134 0.06255397200584412 0.032116372138261795\n",
            "50.1755349860098 7.392640060334186 1.0\n",
            "repr, std, cov, conv, closs 0.032015588134527206 0.334228515625 0.24019724130630493 0.05442728102207184 0.000180838571395725\n",
            "50.67955426655792 7.4893231266335505 1.0\n",
            "repr, std, cov, conv, closs 0.03588992729783058 0.33349609375 0.28666701912879944 0.0750102549791336 0.02126765251159668\n",
            "50.93345934048234 7.655829857601519 1.0\n",
            "repr, std, cov, conv, closs 0.039827924221754074 0.332275390625 0.3352828025817871 0.07529763877391815 0.002551062498241663\n",
            "51.08641256981525 7.732733588830357 1.0\n",
            "repr, std, cov, conv, closs 0.0372086837887764 0.330078125 0.30663740634918213 0.06661230325698853 0.01584594137966633\n",
            "51.29106494296664 7.8809853475428575 1.0\n",
            "repr, std, cov, conv, closs 0.0361274853348732 0.3369140625 0.2782251834869385 0.07033298909664154 0.015878429636359215\n",
            "51.3936983639175 7.912556606376934 1.0\n",
            "repr, std, cov, conv, closs 0.03471197560429573 0.32666015625 0.3525426983833313 0.07144664227962494 0.0030954815447330475\n",
            "50.47734183321425 7.787025382274013 1.0\n",
            "repr, std, cov, conv, closs 0.03231682628393173 0.330810546875 0.3211514949798584 0.07188694179058075 0.016414042562246323\n",
            "50.225710520995804 7.709581707253895 1.0\n",
            "repr, std, cov, conv, closs 0.032122742384672165 0.326416015625 0.32758277654647827 0.06533943116664886 0.014005325734615326\n",
            "49.52779626601696 7.526844710428666 1.0\n",
            "repr, std, cov, conv, closs 0.03571401536464691 0.32861328125 0.3102800250053406 0.0628712922334671 0.03696573153138161\n",
            "49.23166680481393 7.407432733094913 1.0\n",
            "repr, std, cov, conv, closs 0.036274850368499756 0.32568359375 0.32056546211242676 0.059562064707279205 0.019017228856682777\n",
            "48.693353728846894 7.26082829713985 1.0\n",
            "repr, std, cov, conv, closs 0.03608105704188347 0.323974609375 0.3318638205528259 0.06075882911682129 0.005006336607038975\n",
            "48.79078912965831 7.210205193864046 1.0\n",
            "repr, std, cov, conv, closs 0.037137772887945175 0.3291015625 0.30001625418663025 0.07142852991819382 0.03969646617770195\n",
            "48.93730791820545 7.152782256849166 1.0\n",
            "repr, std, cov, conv, closs 0.047972120344638824 0.329833984375 0.33950376510620117 0.07413230836391449 0.015844525769352913\n",
            "49.03523147134977 7.1171253874511935 1.0\n",
            "repr, std, cov, conv, closs 0.035573266446590424 0.327880859375 0.32291993498802185 0.060880303382873535 0.01178424246609211\n",
            "49.4783179480689 7.145636620228938 1.0\n",
            "repr, std, cov, conv, closs 0.02821793407201767 0.333251953125 0.27660804986953735 0.07191115617752075 0.040173664689064026\n",
            "49.72620481601932 7.268089125436989 1.0\n",
            "repr, std, cov, conv, closs 0.0407525971531868 0.332275390625 0.29770588874816895 0.06446724385023117 0.0190935917198658\n",
            "50.42691491829596 7.429677260999828 1.0\n",
            "repr, std, cov, conv, closs 0.04369562864303589 0.333984375 0.27596163749694824 0.06462782621383667 0.003024906385689974\n",
            "50.984392799822814 7.504309262209943 1.0\n",
            "repr, std, cov, conv, closs 0.03605053573846817 0.33203125 0.2863864302635193 0.056227974593639374 0.001727644819766283\n",
            "51.90995424203243 7.717291288961148 1.0\n",
            "repr, std, cov, conv, closs 0.03892170637845993 0.33203125 0.3240290880203247 0.06131424754858017 0.011870958842337132\n",
            "52.01382606047073 7.810409827284005 1.0\n",
            "repr, std, cov, conv, closs 0.047343939542770386 0.322509765625 0.3850674033164978 0.05754683166742325 0.029781263321638107\n",
            "52.11790572641772 7.8809853475428575 1.0\n",
            "repr, std, cov, conv, closs 0.035921625792980194 0.33349609375 0.27573296427726746 0.06641885638237 0.028681235387921333\n",
            "52.01382606047073 7.944254339800216 1.0\n",
            "tcost icost 0.01788330078125 0.4228866994380951\n",
            "tcost icost 0.017425537109375 0.42260149121284485\n",
            "tcost icost 0.017425537109375 0.42469316720962524\n",
            "search 0.442138671875\n",
            "tcost icost 0.01788330078125 0.4232829213142395\n",
            "tcost icost 0.017425537109375 0.42573294043540955\n",
            "tcost icost 0.017425537109375 0.4234558939933777\n",
            "search 0.440673828125\n",
            "tcost icost 0.017303466796875 0.4221888780593872\n",
            "tcost icost 0.017333984375 0.42654916644096375\n",
            "tcost icost 0.017333984375 0.4216132164001465\n",
            "search 0.43896484375\n",
            "tcost icost 0.017303466796875 0.4214840233325958\n",
            "tcost icost 0.017333984375 0.4212837517261505\n",
            "tcost icost 0.017333984375 0.42161110043525696\n",
            "search 0.43896484375\n",
            "tcost icost 0.0172576904296875 0.4201594591140747\n",
            "tcost icost 0.017425537109375 0.42201679944992065\n",
            "tcost icost 0.017425537109375 0.42090392112731934\n",
            "search 0.438232421875\n",
            "tcost icost 0.017333984375 0.4212837517261505\n",
            "tcost icost 0.017303466796875 0.41869378089904785\n",
            "tcost icost 0.017303466796875 0.42028167843818665\n",
            "search 0.4375\n",
            "tcost icost 0.0172271728515625 0.41957738995552063\n",
            "tcost icost 0.0174102783203125 0.41892147064208984\n",
            "tcost icost 0.0174102783203125 0.4189395010471344\n",
            "search 0.436279296875\n",
            "tcost icost 0.017242431640625 0.4174554944038391\n",
            "tcost icost 0.017425537109375 0.41940662264823914\n",
            "tcost icost 0.017425537109375 0.42074668407440186\n",
            "search 0.43798828125\n",
            "tcost icost 0.0171356201171875 0.41819554567337036\n",
            "tcost icost 0.0173492431640625 0.4182303547859192\n",
            "tcost icost 0.0173492431640625 0.4207230806350708\n",
            "search 0.43798828125\n",
            "tcost icost 0.018096923828125 0.420567125082016\n",
            "tcost icost 0.0180511474609375 0.42014750838279724\n",
            "tcost icost 0.0180511474609375 0.4201892614364624\n",
            "search 0.438232421875\n",
            "tcost icost 0.0177459716796875 0.4203096628189087\n",
            "tcost icost 0.0178375244140625 0.42025694251060486\n",
            "tcost icost 0.0178375244140625 0.42284369468688965\n",
            "search 0.440673828125\n",
            "tcost icost 0.018310546875 0.4232791066169739\n",
            "tcost icost 0.0177154541015625 0.42064565420150757\n",
            "tcost icost 0.0177154541015625 0.4204804301261902\n",
            "search 0.438232421875\n",
            "tcost icost 0.0185394287109375 0.42319488525390625\n",
            "tcost icost 0.0177001953125 0.4217829704284668\n",
            "tcost icost 0.0177001953125 0.42044636607170105\n",
            "search 0.43798828125\n",
            "tcost icost 0.0185089111328125 0.42066940665245056\n",
            "tcost icost 0.01849365234375 0.4203275442123413\n",
            "tcost icost 0.01849365234375 0.4217148721218109\n",
            "search 0.440185546875\n",
            "tcost icost 0.018218994140625 0.41730937361717224\n",
            "tcost icost 0.0180206298828125 0.4164832532405853\n",
            "tcost icost 0.0180206298828125 0.4205516576766968\n",
            "search 0.438720703125\n",
            "tcost icost 0.0176239013671875 0.4167346656322479\n",
            "tcost icost 0.0177154541015625 0.41675788164138794\n",
            "tcost icost 0.0177154541015625 0.41696327924728394\n",
            "search 0.434814453125\n",
            "tcost icost 0.0172576904296875 0.41239598393440247\n",
            "tcost icost 0.017486572265625 0.4141807556152344\n",
            "tcost icost 0.017486572265625 0.4148405194282532\n",
            "search 0.432373046875\n",
            "tcost icost 0.01837158203125 0.4145853817462921\n",
            "tcost icost 0.01837158203125 0.4168960154056549\n",
            "tcost icost 0.01837158203125 0.41993504762649536\n",
            "search 0.438232421875\n",
            "tcost icost 0.0183258056640625 0.4070080518722534\n",
            "tcost icost 0.017913818359375 0.4059327244758606\n",
            "tcost icost 0.017913818359375 0.40882399678230286\n",
            "search 0.4267578125\n",
            "tcost icost 0.0185394287109375 0.4113624393939972\n",
            "tcost icost 0.0177001953125 0.41144034266471863\n",
            "tcost icost 0.0177001953125 0.41144034266471863\n",
            "search 0.42919921875\n",
            "tcost icost 0.0177001953125 0.40324872732162476\n",
            "tcost icost 0.017913818359375 0.40312209725379944\n",
            "tcost icost 0.017913818359375 0.4027497172355652\n",
            "search 0.420654296875\n",
            "tcost icost 0.0183258056640625 0.4113122820854187\n",
            "tcost icost 0.0182952880859375 0.40752679109573364\n",
            "tcost icost 0.0182952880859375 0.4119966924190521\n",
            "search 0.430419921875\n",
            "tcost icost 0.017669677734375 0.4060693681240082\n",
            "tcost icost 0.0171661376953125 0.40503570437431335\n",
            "tcost icost 0.0171661376953125 0.4031553864479065\n",
            "search 0.420166015625\n",
            "tcost icost 0.018096923828125 0.4058978259563446\n",
            "tcost icost 0.0180511474609375 0.40812474489212036\n",
            "tcost icost 0.0180511474609375 0.405383437871933\n",
            "search 0.42333984375\n",
            "tcost icost 0.0178680419921875 0.4069069027900696\n",
            "tcost icost 0.017730712890625 0.4068504869937897\n",
            "tcost icost 0.017730712890625 0.40695691108703613\n",
            "search 0.4248046875\n",
            "tcost icost 0.017669677734375 0.4021536409854889\n",
            "tcost icost 0.0178070068359375 0.4044029414653778\n",
            "tcost icost 0.0178070068359375 0.4038561284542084\n",
            "search 0.421630859375\n",
            "tcost icost 0.018096923828125 0.4085948169231415\n",
            "tcost icost 0.0181427001953125 0.4065941572189331\n",
            "tcost icost 0.0181427001953125 0.40728867053985596\n",
            "search 0.42529296875\n",
            "tcost icost 0.017120361328125 0.40221408009529114\n",
            "tcost icost 0.0164031982421875 0.40220966935157776\n",
            "tcost icost 0.0164031982421875 0.40342071652412415\n",
            "search 0.419677734375\n",
            "tcost icost 0.01617431640625 0.4031689465045929\n",
            "tcost icost 0.01666259765625 0.4011177718639374\n",
            "tcost icost 0.01666259765625 0.40250736474990845\n",
            "search 0.419189453125\n",
            "tcost icost 0.0162506103515625 0.4022432863712311\n",
            "tcost icost 0.0162506103515625 0.4026394486427307\n",
            "tcost icost 0.0162506103515625 0.4015082120895386\n",
            "search 0.41796875\n",
            "tcost icost 0.01480865478515625 0.4016810357570648\n",
            "tcost icost 0.01474761962890625 0.40185490250587463\n",
            "tcost icost 0.01474761962890625 0.4028686285018921\n",
            "search 0.41748046875\n",
            "tcost icost 0.01611328125 0.4023471772670746\n",
            "tcost icost 0.016876220703125 0.4014786183834076\n",
            "tcost icost 0.016876220703125 0.40319138765335083\n",
            "search 0.419921875\n",
            "tcost icost 0.0159149169921875 0.4032004773616791\n",
            "tcost icost 0.01666259765625 0.3998909294605255\n",
            "tcost icost 0.01666259765625 0.4008644223213196\n",
            "search 0.41748046875\n",
            "tcost icost 0.0157928466796875 0.3999852240085602\n",
            "tcost icost 0.0164947509765625 0.399745374917984\n",
            "tcost icost 0.0164947509765625 0.3995181620121002\n",
            "search 0.416015625\n",
            "tcost icost 0.017242431640625 0.4021940529346466\n",
            "tcost icost 0.0168914794921875 0.40362945199012756\n",
            "tcost icost 0.0168914794921875 0.4077223241329193\n",
            "search 0.424560546875\n",
            "tcost icost 0.0163726806640625 0.40555936098098755\n",
            "tcost icost 0.01561737060546875 0.404541015625\n",
            "tcost icost 0.01561737060546875 0.40386301279067993\n",
            "search 0.41943359375\n",
            "tcost icost 0.017120361328125 0.4038148820400238\n",
            "tcost icost 0.0167236328125 0.4044015407562256\n",
            "tcost icost 0.0167236328125 0.4041411280632019\n",
            "search 0.4208984375\n",
            "tcost icost 0.01543426513671875 0.4062286913394928\n",
            "tcost icost 0.01531982421875 0.40654870867729187\n",
            "tcost icost 0.01531982421875 0.40601006150245667\n",
            "search 0.42138671875\n",
            "tcost icost 0.0168304443359375 0.4051412045955658\n",
            "tcost icost 0.0168304443359375 0.40576332807540894\n",
            "tcost icost 0.0168304443359375 0.40552419424057007\n",
            "search 0.42236328125\n",
            "tcost icost 0.0166473388671875 0.40815988183021545\n",
            "tcost icost 0.0171966552734375 0.40397581458091736\n",
            "tcost icost 0.0171966552734375 0.4041835367679596\n",
            "search 0.42138671875\n",
            "tcost icost 0.0170745849609375 0.4094488024711609\n",
            "tcost icost 0.01751708984375 0.4067094624042511\n",
            "tcost icost 0.01751708984375 0.4072994291782379\n",
            "search 0.4248046875\n",
            "tcost icost 0.017242431640625 0.4095538258552551\n",
            "tcost icost 0.0172119140625 0.408840537071228\n",
            "tcost icost 0.0172119140625 0.40876200795173645\n",
            "search 0.42578125\n",
            "tcost icost 0.01751708984375 0.4060220420360565\n",
            "tcost icost 0.0175323486328125 0.4042019844055176\n",
            "tcost icost 0.0175323486328125 0.408145010471344\n",
            "search 0.42578125\n",
            "tcost icost 0.0174102783203125 0.4073532521724701\n",
            "tcost icost 0.017333984375 0.40719443559646606\n",
            "tcost icost 0.017333984375 0.40741416811943054\n",
            "search 0.4248046875\n",
            "tcost icost 0.017425537109375 0.40726369619369507\n",
            "tcost icost 0.0173187255859375 0.4088020622730255\n",
            "tcost icost 0.0173187255859375 0.40954822301864624\n",
            "search 0.427001953125\n",
            "tcost icost 0.0175933837890625 0.4077050983905792\n",
            "tcost icost 0.0177001953125 0.408657044172287\n",
            "tcost icost 0.0177001953125 0.40806272625923157\n",
            "search 0.42578125\n",
            "tcost icost 0.01617431640625 0.4081408679485321\n",
            "tcost icost 0.015716552734375 0.40738800168037415\n",
            "tcost icost 0.015716552734375 0.4084092676639557\n",
            "search 0.424072265625\n",
            "tcost icost 0.0176239013671875 0.4084412455558777\n",
            "tcost icost 0.0177001953125 0.408694863319397\n",
            "tcost icost 0.0177001953125 0.4089842438697815\n",
            "search 0.4267578125\n",
            "tcost icost 0.0174102783203125 0.4126664400100708\n",
            "tcost icost 0.017333984375 0.4108016788959503\n",
            "tcost icost 0.017333984375 0.40996357798576355\n",
            "search 0.42724609375\n",
            "tcost icost 0.0174713134765625 0.4123954772949219\n",
            "tcost icost 0.01739501953125 0.41061902046203613\n",
            "tcost icost 0.01739501953125 0.4095739424228668\n",
            "search 0.427001953125\n",
            "tcost icost 0.017364501953125 0.4080715775489807\n",
            "tcost icost 0.01751708984375 0.40889793634414673\n",
            "tcost icost 0.01751708984375 0.40741005539894104\n",
            "search 0.425048828125\n",
            "tcost icost 0.017547607421875 0.41349461674690247\n",
            "tcost icost 0.0175018310546875 0.41082578897476196\n",
            "tcost icost 0.0175018310546875 0.4113004207611084\n",
            "search 0.428955078125\n",
            "tcost icost 0.0176544189453125 0.40894371271133423\n",
            "tcost icost 0.0175628662109375 0.4088006913661957\n",
            "tcost icost 0.0175628662109375 0.4090367257595062\n",
            "search 0.426513671875\n",
            "tcost icost 0.01715087890625 0.4087256193161011\n",
            "tcost icost 0.01739501953125 0.40685391426086426\n",
            "tcost icost 0.01739501953125 0.4085439443588257\n",
            "search 0.42578125\n",
            "tcost icost 0.0174560546875 0.41262808442115784\n",
            "tcost icost 0.0174102783203125 0.4116966128349304\n",
            "tcost icost 0.0174102783203125 0.41073963046073914\n",
            "search 0.427978515625\n",
            "tcost icost 0.0171051025390625 0.41264644265174866\n",
            "tcost icost 0.0172119140625 0.41243234276771545\n",
            "tcost icost 0.0172119140625 0.41121143102645874\n",
            "search 0.42822265625\n",
            "tcost icost 0.0176239013671875 0.41256412863731384\n",
            "tcost icost 0.0175018310546875 0.4082300066947937\n",
            "tcost icost 0.0175018310546875 0.40815719962120056\n",
            "search 0.42578125\n",
            "tcost icost 0.01800537109375 0.4081004858016968\n",
            "tcost icost 0.0183563232421875 0.4050341546535492\n",
            "tcost icost 0.0183563232421875 0.40483832359313965\n",
            "search 0.423095703125\n",
            "tcost icost 0.017822265625 0.41021227836608887\n",
            "tcost icost 0.0179901123046875 0.4101879894733429\n",
            "tcost icost 0.0179901123046875 0.4077061712741852\n",
            "search 0.42578125\n",
            "tcost icost 0.01751708984375 0.41374996304512024\n",
            "tcost icost 0.0176544189453125 0.41381439566612244\n",
            "tcost icost 0.0176544189453125 0.4139702022075653\n",
            "search 0.431640625\n",
            "tcost icost 0.0174102783203125 0.41735562682151794\n",
            "tcost icost 0.0174407958984375 0.41614240407943726\n",
            "tcost icost 0.0174407958984375 0.4162120521068573\n",
            "search 0.43359375\n",
            "tcost icost 0.0174102783203125 0.41662126779556274\n",
            "tcost icost 0.0174102783203125 0.4180634915828705\n",
            "tcost icost 0.0174102783203125 0.41662126779556274\n",
            "search 0.433837890625\n",
            "tcost icost 0.0174713134765625 0.41532424092292786\n",
            "tcost icost 0.0175628662109375 0.4121629595756531\n",
            "tcost icost 0.0175628662109375 0.41071441769599915\n",
            "search 0.42822265625\n",
            "tcost icost 0.01739501953125 0.41457417607307434\n",
            "tcost icost 0.017486572265625 0.4123091697692871\n",
            "tcost icost 0.017486572265625 0.41252055764198303\n",
            "search 0.43017578125\n",
            "tcost icost 0.017486572265625 0.4112851619720459\n",
            "tcost icost 0.017486572265625 0.4103356599807739\n",
            "tcost icost 0.017486572265625 0.41239839792251587\n",
            "search 0.429931640625\n",
            "tcost icost 0.0175323486328125 0.411801278591156\n",
            "tcost icost 0.0175628662109375 0.410794734954834\n",
            "tcost icost 0.0175628662109375 0.4093499183654785\n",
            "search 0.427001953125\n",
            "tcost icost 0.0179443359375 0.40678760409355164\n",
            "tcost icost 0.0182647705078125 0.4034228026866913\n",
            "tcost icost 0.0182647705078125 0.40233007073402405\n",
            "search 0.420654296875\n",
            "tcost icost 0.0177459716796875 0.406072199344635\n",
            "tcost icost 0.0180511474609375 0.4022820293903351\n",
            "tcost icost 0.0180511474609375 0.4024142622947693\n",
            "search 0.42041015625\n",
            "tcost icost 0.01763916015625 0.4009152948856354\n",
            "tcost icost 0.01763916015625 0.40089723467826843\n",
            "tcost icost 0.01763916015625 0.40174633264541626\n",
            "search 0.41943359375\n",
            "tcost icost 0.01654052734375 0.4122784435749054\n",
            "tcost icost 0.016357421875 0.4125344753265381\n",
            "tcost icost 0.016357421875 0.41289860010147095\n",
            "search 0.42919921875\n",
            "tcost icost 0.018402099609375 0.39871466159820557\n",
            "tcost icost 0.0175323486328125 0.38578498363494873\n",
            "tcost icost 0.0175323486328125 0.3874218761920929\n",
            "search 0.405029296875\n",
            "tcost icost 0.01763916015625 0.41926783323287964\n",
            "tcost icost 0.01763916015625 0.4192816913127899\n",
            "tcost icost 0.01763916015625 0.41712018847465515\n",
            "search 0.434814453125\n",
            "tcost icost 0.0166168212890625 0.41545459628105164\n",
            "tcost icost 0.0164794921875 0.411345511674881\n",
            "tcost icost 0.0164794921875 0.41248971223831177\n",
            "search 0.42919921875\n",
            "tcost icost 0.0180816650390625 0.4118938148021698\n",
            "tcost icost 0.0184783935546875 0.4057987928390503\n",
            "tcost icost 0.0184783935546875 0.4053650200366974\n",
            "search 0.423828125\n",
            "tcost icost 0.0180511474609375 0.40967512130737305\n",
            "tcost icost 0.0184478759765625 0.4068281352519989\n",
            "tcost icost 0.0184478759765625 0.4055944085121155\n",
            "search 0.424072265625\n",
            "tcost icost 0.019287109375 0.4013044536113739\n",
            "tcost icost 0.0194549560546875 0.3968314826488495\n",
            "tcost icost 0.0194549560546875 0.3981798589229584\n",
            "search 0.417724609375\n",
            "tcost icost 0.01959228515625 0.3986629247665405\n",
            "tcost icost 0.01959228515625 0.39614468812942505\n",
            "tcost icost 0.01959228515625 0.3955899178981781\n",
            "search 0.4150390625\n",
            "tcost icost 0.017578125 0.41411441564559937\n",
            "tcost icost 0.017486572265625 0.41327011585235596\n",
            "tcost icost 0.017486572265625 0.4145829677581787\n",
            "search 0.43212890625\n",
            "tcost icost 0.017669677734375 0.4193201959133148\n",
            "tcost icost 0.0175628662109375 0.4187455475330353\n",
            "tcost icost 0.0175628662109375 0.418558806180954\n",
            "search 0.43603515625\n",
            "tcost icost 0.0176849365234375 0.4181325435638428\n",
            "tcost icost 0.0176239013671875 0.4175644814968109\n",
            "tcost icost 0.0176239013671875 0.41766518354415894\n",
            "search 0.435302734375\n",
            "tcost icost 0.0175323486328125 0.41881364583969116\n",
            "tcost icost 0.0172119140625 0.41705384850502014\n",
            "tcost icost 0.0172119140625 0.4180624186992645\n",
            "search 0.43505859375\n",
            "tcost icost 0.0173492431640625 0.4169130027294159\n",
            "tcost icost 0.0173492431640625 0.4183569550514221\n",
            "tcost icost 0.0173492431640625 0.41664567589759827\n",
            "search 0.43408203125\n",
            "tcost icost 0.017578125 0.42023542523384094\n",
            "tcost icost 0.017333984375 0.41632959246635437\n",
            "tcost icost 0.017333984375 0.41682565212249756\n",
            "search 0.43408203125\n",
            "tcost icost 0.017364501953125 0.41915345191955566\n",
            "tcost icost 0.0175628662109375 0.4193054437637329\n",
            "tcost icost 0.0175628662109375 0.4191581606864929\n",
            "search 0.436767578125\n",
            "tcost icost 0.0177001953125 0.42050856351852417\n",
            "tcost icost 0.0177154541015625 0.418992817401886\n",
            "tcost icost 0.0177154541015625 0.418720543384552\n",
            "search 0.4365234375\n",
            "tcost icost 0.017669677734375 0.4211127460002899\n",
            "tcost icost 0.017486572265625 0.4172869920730591\n",
            "tcost icost 0.017486572265625 0.41890203952789307\n",
            "search 0.4365234375\n",
            "tcost icost 0.0178070068359375 0.42004919052124023\n",
            "tcost icost 0.0176849365234375 0.41998565196990967\n",
            "tcost icost 0.0176849365234375 0.41998565196990967\n",
            "search 0.4375\n",
            "tcost icost 0.0168609619140625 0.41807809472084045\n",
            "tcost icost 0.01715087890625 0.41646790504455566\n",
            "tcost icost 0.01715087890625 0.4157063364982605\n",
            "search 0.432861328125\n",
            "tcost icost 0.0176239013671875 0.4165169298648834\n",
            "tcost icost 0.0175628662109375 0.4118324816226959\n",
            "tcost icost 0.0175628662109375 0.4128313958644867\n",
            "search 0.430419921875\n",
            "tcost icost 0.01849365234375 0.4058256447315216\n",
            "tcost icost 0.01910400390625 0.4021407663822174\n",
            "tcost icost 0.01910400390625 0.4010072648525238\n",
            "search 0.420166015625\n",
            "tcost icost 0.0173187255859375 0.42347288131713867\n",
            "tcost icost 0.01739501953125 0.42095986008644104\n",
            "tcost icost 0.01739501953125 0.42080795764923096\n",
            "search 0.438232421875\n",
            "tcost icost 0.0157928466796875 0.4178842008113861\n",
            "tcost icost 0.0155487060546875 0.4176637530326843\n",
            "tcost icost 0.0155487060546875 0.4178670644760132\n",
            "search 0.43359375\n",
            "tcost icost 0.0175933837890625 0.4208275377750397\n",
            "tcost icost 0.0173492431640625 0.4198567569255829\n",
            "tcost icost 0.0173492431640625 0.41974735260009766\n",
            "search 0.43701171875\n",
            "tcost icost 0.0177459716796875 0.42121225595474243\n",
            "tcost icost 0.0176849365234375 0.4216049611568451\n",
            "tcost icost 0.0176849365234375 0.42108353972435\n",
            "search 0.438720703125\n",
            "tcost icost 0.017578125 0.4204317033290863\n",
            "tcost icost 0.017333984375 0.41879913210868835\n",
            "tcost icost 0.017333984375 0.4202798008918762\n",
            "search 0.4375\n",
            "tcost icost 0.0174407958984375 0.4219852089881897\n",
            "tcost icost 0.0173492431640625 0.4205192029476166\n",
            "tcost icost 0.0173492431640625 0.421113520860672\n",
            "search 0.4384765625\n",
            "tcost icost 0.0174102783203125 0.4244006276130676\n",
            "tcost icost 0.0173797607421875 0.4227679371833801\n",
            "tcost icost 0.0173797607421875 0.42042598128318787\n",
            "search 0.437744140625\n",
            "tcost icost 0.017578125 0.42758938670158386\n",
            "tcost icost 0.0172119140625 0.4245537519454956\n",
            "tcost icost 0.0172119140625 0.42686399817466736\n",
            "search 0.44384765625\n",
            "tcost icost 0.016998291015625 0.42606455087661743\n",
            "tcost icost 0.0169525146484375 0.4269486665725708\n",
            "tcost icost 0.0169525146484375 0.4256027042865753\n",
            "search 0.4423828125\n",
            "tcost icost 0.0175018310546875 0.43056410551071167\n",
            "tcost icost 0.01739501953125 0.4279305040836334\n",
            "tcost icost 0.01739501953125 0.4274503290653229\n",
            "search 0.44482421875\n",
            "tcost icost 0.0177001953125 0.4287710189819336\n",
            "tcost icost 0.01776123046875 0.42825472354888916\n",
            "tcost icost 0.01776123046875 0.4307420551776886\n",
            "search 0.448486328125\n",
            "tcost icost 0.0174102783203125 0.4253891408443451\n",
            "tcost icost 0.0174102783203125 0.42527276277542114\n",
            "tcost icost 0.0174102783203125 0.4253404438495636\n",
            "search 0.442626953125\n",
            "tcost icost 0.0174102783203125 0.42567920684814453\n",
            "tcost icost 0.017364501953125 0.4254436194896698\n",
            "tcost icost 0.017364501953125 0.4254436194896698\n",
            "search 0.44287109375\n",
            "tcost icost 0.0176544189453125 0.4278480112552643\n",
            "tcost icost 0.017333984375 0.4236232340335846\n",
            "tcost icost 0.017333984375 0.42558082938194275\n",
            "search 0.44287109375\n",
            "tcost icost 0.0177001953125 0.42850008606910706\n",
            "tcost icost 0.017364501953125 0.42428433895111084\n",
            "tcost icost 0.017364501953125 0.4254591166973114\n",
            "search 0.44287109375\n",
            "tcost icost 0.0175628662109375 0.42588135600090027\n",
            "tcost icost 0.0173187255859375 0.4231336712837219\n",
            "tcost icost 0.0173187255859375 0.42189010977745056\n",
            "search 0.439208984375\n",
            "tcost icost 0.0174560546875 0.4179641306400299\n",
            "tcost icost 0.0175628662109375 0.41538670659065247\n",
            "tcost icost 0.0175628662109375 0.41596147418022156\n",
            "search 0.43359375\n",
            "tcost icost 0.0174560546875 0.42411938309669495\n",
            "tcost icost 0.01727294921875 0.4218398630619049\n",
            "tcost icost 0.01727294921875 0.4212115406990051\n",
            "search 0.4384765625\n",
            "tcost icost 0.0174102783203125 0.4232919216156006\n",
            "tcost icost 0.0173797607421875 0.4216074049472809\n",
            "tcost icost 0.0173797607421875 0.4198261499404907\n",
            "search 0.437255859375\n",
            "tcost icost 0.0175628662109375 0.4243866205215454\n",
            "tcost icost 0.0174102783203125 0.42440667748451233\n",
            "tcost icost 0.0174102783203125 0.4237189292907715\n",
            "search 0.441162109375\n",
            "tcost icost 0.016815185546875 0.42374587059020996\n",
            "tcost icost 0.016998291015625 0.4232088327407837\n",
            "tcost icost 0.016998291015625 0.42313888669013977\n",
            "search 0.440185546875\n",
            "tcost icost 0.0175323486328125 0.4281647801399231\n",
            "tcost icost 0.0177764892578125 0.42863619327545166\n",
            "tcost icost 0.0177764892578125 0.4282645881175995\n",
            "search 0.446044921875\n",
            "tcost icost 0.0177001953125 0.42916399240493774\n",
            "tcost icost 0.01763916015625 0.42711126804351807\n",
            "tcost icost 0.01763916015625 0.4277464747428894\n",
            "search 0.4453125\n",
            "tcost icost 0.0166168212890625 0.4238576889038086\n",
            "tcost icost 0.016021728515625 0.42074596881866455\n",
            "tcost icost 0.016021728515625 0.41990333795547485\n",
            "search 0.43603515625\n",
            "tcost icost 0.017181396484375 0.42842981219291687\n",
            "tcost icost 0.017333984375 0.4255555272102356\n",
            "tcost icost 0.017333984375 0.42580661177635193\n",
            "search 0.443115234375\n",
            "tcost icost 0.017425537109375 0.4260922968387604\n",
            "tcost icost 0.0173797607421875 0.420990914106369\n",
            "tcost icost 0.0173797607421875 0.4225703477859497\n",
            "search 0.43994140625\n",
            "tcost icost 0.0146331787109375 0.41368040442466736\n",
            "tcost icost 0.01464080810546875 0.4089912176132202\n",
            "tcost icost 0.01464080810546875 0.4098450541496277\n",
            "search 0.424560546875\n",
            "tcost icost 0.0154876708984375 0.4121415913105011\n",
            "tcost icost 0.01490020751953125 0.40608254075050354\n",
            "tcost icost 0.01490020751953125 0.40734362602233887\n",
            "search 0.422119140625\n",
            "tcost icost 0.017120361328125 0.4261772930622101\n",
            "tcost icost 0.01702880859375 0.4261307418346405\n",
            "tcost icost 0.01702880859375 0.4263889789581299\n",
            "search 0.443359375\n",
            "tcost icost 0.016510009765625 0.4241943657398224\n",
            "tcost icost 0.015869140625 0.4235323965549469\n",
            "tcost icost 0.015869140625 0.4254035949707031\n",
            "search 0.441162109375\n",
            "tcost icost 0.017669677734375 0.42855304479599\n",
            "tcost icost 0.01763916015625 0.42576301097869873\n",
            "tcost icost 0.01763916015625 0.42586028575897217\n",
            "search 0.443359375\n",
            "tcost icost 0.0164031982421875 0.4244404435157776\n",
            "tcost icost 0.0164031982421875 0.423647940158844\n",
            "tcost icost 0.0164031982421875 0.4241572618484497\n",
            "search 0.4404296875\n",
            "tcost icost 0.0178070068359375 0.4257207214832306\n",
            "tcost icost 0.0177764892578125 0.4254624843597412\n",
            "tcost icost 0.0177764892578125 0.42830705642700195\n",
            "search 0.446044921875\n",
            "tcost icost 0.0178070068359375 0.4255124628543854\n",
            "tcost icost 0.0176239013671875 0.42578285932540894\n",
            "tcost icost 0.0176239013671875 0.42578285932540894\n",
            "search 0.443359375\n",
            "tcost icost 0.01776123046875 0.42514270544052124\n",
            "tcost icost 0.017608642578125 0.42584720253944397\n",
            "tcost icost 0.017608642578125 0.42629000544548035\n",
            "search 0.44384765625\n",
            "tcost icost 0.0166473388671875 0.42500874400138855\n",
            "tcost icost 0.0169830322265625 0.4252435564994812\n",
            "tcost icost 0.0169830322265625 0.42605945467948914\n",
            "search 0.443115234375\n",
            "tcost icost 0.0185394287109375 0.4261612296104431\n",
            "tcost icost 0.0177001953125 0.42547741532325745\n",
            "tcost icost 0.0177001953125 0.42678338289260864\n",
            "search 0.4443359375\n",
            "tcost icost 0.0172576904296875 0.4218197762966156\n",
            "tcost icost 0.0164947509765625 0.421796053647995\n",
            "tcost icost 0.0164947509765625 0.42171981930732727\n",
            "search 0.438232421875\n",
            "tcost icost 0.0157928466796875 0.4194062054157257\n",
            "tcost icost 0.0157623291015625 0.419156938791275\n",
            "tcost icost 0.0157623291015625 0.4196443259716034\n",
            "search 0.435546875\n",
            "tcost icost 0.015472412109375 0.41875532269477844\n",
            "tcost icost 0.0141754150390625 0.41708555817604065\n",
            "tcost icost 0.0141754150390625 0.4170854985713959\n",
            "search 0.43115234375\n",
            "tcost icost 0.01617431640625 0.41637760400772095\n",
            "tcost icost 0.01617431640625 0.41440925002098083\n",
            "tcost icost 0.01617431640625 0.41448503732681274\n",
            "search 0.4306640625\n",
            "tcost icost 0.0184326171875 0.41704797744750977\n",
            "tcost icost 0.0183868408203125 0.4183759093284607\n",
            "tcost icost 0.0183868408203125 0.41808435320854187\n",
            "search 0.436279296875\n",
            "tcost icost 0.0182342529296875 0.4165399372577667\n",
            "tcost icost 0.0182342529296875 0.4159591495990753\n",
            "tcost icost 0.0182342529296875 0.41747450828552246\n",
            "search 0.435791015625\n",
            "tcost icost 0.017730712890625 0.41813066601753235\n",
            "tcost icost 0.017852783203125 0.4162417948246002\n",
            "tcost icost 0.017852783203125 0.41624176502227783\n",
            "search 0.43408203125\n",
            "tcost icost 0.0178375244140625 0.4185355305671692\n",
            "tcost icost 0.017852783203125 0.4158863425254822\n",
            "tcost icost 0.017852783203125 0.4162418842315674\n",
            "search 0.43408203125\n",
            "tcost icost 0.0179443359375 0.41949620842933655\n",
            "tcost icost 0.01800537109375 0.4179450571537018\n",
            "tcost icost 0.01800537109375 0.4207330644130707\n",
            "search 0.438720703125\n",
            "tcost icost 0.0176849365234375 0.4176425337791443\n",
            "tcost icost 0.0177459716796875 0.41811394691467285\n",
            "tcost icost 0.0177459716796875 0.4190775454044342\n",
            "search 0.43701171875\n",
            "tcost icost 0.0177001953125 0.41800427436828613\n",
            "tcost icost 0.0178985595703125 0.41787731647491455\n",
            "tcost icost 0.0178985595703125 0.41798362135887146\n",
            "search 0.435791015625\n",
            "tcost icost 0.0183258056640625 0.41034314036369324\n",
            "tcost icost 0.017913818359375 0.40940579771995544\n",
            "tcost icost 0.017913818359375 0.40982547402381897\n",
            "search 0.427734375\n",
            "tcost icost 0.0176849365234375 0.41409337520599365\n",
            "tcost icost 0.0178680419921875 0.41629961133003235\n",
            "tcost icost 0.0178680419921875 0.4141922891139984\n",
            "search 0.43212890625\n",
            "tcost icost 0.0177764892578125 0.41333818435668945\n",
            "tcost icost 0.0177764892578125 0.41366103291511536\n",
            "tcost icost 0.0177764892578125 0.41504621505737305\n",
            "search 0.432861328125\n",
            "tcost icost 0.01739501953125 0.4149950444698334\n",
            "tcost icost 0.0178070068359375 0.4147733449935913\n",
            "tcost icost 0.0178070068359375 0.4129927158355713\n",
            "search 0.430908203125\n",
            "tcost icost 0.0173187255859375 0.4151688814163208\n",
            "tcost icost 0.0170440673828125 0.4155602753162384\n",
            "tcost icost 0.0170440673828125 0.4155603349208832\n",
            "search 0.4326171875\n",
            "tcost icost 0.0175628662109375 0.41592180728912354\n",
            "tcost icost 0.017486572265625 0.41569530963897705\n",
            "tcost icost 0.017486572265625 0.417283833026886\n",
            "search 0.434814453125\n",
            "tcost icost 0.0178985595703125 0.4174861013889313\n",
            "tcost icost 0.0178070068359375 0.4177664816379547\n",
            "tcost icost 0.0178070068359375 0.41575315594673157\n",
            "search 0.43359375\n",
            "tcost icost 0.0177764892578125 0.4171577990055084\n",
            "tcost icost 0.017791748046875 0.41797545552253723\n",
            "tcost icost 0.017791748046875 0.4159822165966034\n",
            "search 0.433837890625\n",
            "tcost icost 0.0172119140625 0.4162085950374603\n",
            "tcost icost 0.0171356201171875 0.41567784547805786\n",
            "tcost icost 0.0171356201171875 0.41786816716194153\n",
            "search 0.43505859375\n",
            "tcost icost 0.0176849365234375 0.4204268157482147\n",
            "tcost icost 0.0176849365234375 0.41820627450942993\n",
            "tcost icost 0.0176849365234375 0.41623765230178833\n",
            "search 0.433837890625\n",
            "tcost icost 0.0177764892578125 0.41567885875701904\n",
            "tcost icost 0.017791748046875 0.41711312532424927\n",
            "tcost icost 0.017791748046875 0.4166789650917053\n",
            "search 0.4345703125\n",
            "tcost icost 0.0176544189453125 0.41728055477142334\n",
            "tcost icost 0.0177459716796875 0.4167183041572571\n",
            "tcost icost 0.0177459716796875 0.4164603650569916\n",
            "search 0.434326171875\n",
            "tcost icost 0.017181396484375 0.41223475337028503\n",
            "tcost icost 0.01641845703125 0.4124792814254761\n",
            "tcost icost 0.01641845703125 0.4128067195415497\n",
            "search 0.42919921875\n",
            "tcost icost 0.0169830322265625 0.4121037423610687\n",
            "tcost icost 0.0176239013671875 0.4121885299682617\n",
            "tcost icost 0.0176239013671875 0.4121885299682617\n",
            "search 0.4296875\n",
            "tcost icost 0.01751708984375 0.4091437757015228\n",
            "tcost icost 0.016632080078125 0.4119829535484314\n",
            "tcost icost 0.016632080078125 0.41233131289482117\n",
            "search 0.428955078125\n",
            "tcost icost 0.0182037353515625 0.41582295298576355\n",
            "tcost icost 0.0175933837890625 0.41531556844711304\n",
            "tcost icost 0.0175933837890625 0.41363340616226196\n",
            "search 0.43115234375\n",
            "tcost icost 0.0177154541015625 0.41738229990005493\n",
            "tcost icost 0.017913818359375 0.413593053817749\n",
            "tcost icost 0.017913818359375 0.41546863317489624\n",
            "search 0.433349609375\n",
            "tcost icost 0.01629638671875 0.4147600829601288\n",
            "tcost icost 0.016693115234375 0.4162670075893402\n",
            "tcost icost 0.016693115234375 0.413629949092865\n",
            "search 0.43017578125\n",
            "tcost icost 0.0175628662109375 0.4157547950744629\n",
            "tcost icost 0.0174560546875 0.4166463315486908\n",
            "tcost icost 0.0174560546875 0.41500037908554077\n",
            "search 0.4326171875\n",
            "tcost icost 0.01824951171875 0.41459181904792786\n",
            "tcost icost 0.0177764892578125 0.41442573070526123\n",
            "tcost icost 0.0177764892578125 0.4150134027004242\n",
            "search 0.432861328125\n",
            "tcost icost 0.0184173583984375 0.4154801666736603\n",
            "tcost icost 0.0181121826171875 0.4155382812023163\n",
            "tcost icost 0.0181121826171875 0.4144589304924011\n",
            "search 0.4326171875\n",
            "tcost icost 0.017791748046875 0.415944367647171\n",
            "tcost icost 0.0178375244140625 0.4154021143913269\n",
            "tcost icost 0.0178375244140625 0.4154021143913269\n",
            "search 0.43310546875\n",
            "tcost icost 0.0174560546875 0.4155419170856476\n",
            "tcost icost 0.017425537109375 0.41775789856910706\n",
            "tcost icost 0.017425537109375 0.4176921248435974\n",
            "search 0.43505859375\n",
            "tcost icost 0.017486572265625 0.41478732228279114\n",
            "tcost icost 0.0169219970703125 0.41675105690956116\n",
            "tcost icost 0.0169219970703125 0.414800226688385\n",
            "search 0.431640625\n",
            "tcost icost 0.016845703125 0.41688382625579834\n",
            "tcost icost 0.01708984375 0.4155195951461792\n",
            "tcost icost 0.01708984375 0.41521283984184265\n",
            "search 0.432373046875\n",
            "tcost icost 0.0177001953125 0.4184679687023163\n",
            "tcost icost 0.01800537109375 0.41968220472335815\n",
            "tcost icost 0.01800537109375 0.4174253046512604\n",
            "search 0.435546875\n",
            "tcost icost 0.0181732177734375 0.41693300008773804\n",
            "tcost icost 0.0177001953125 0.4168879985809326\n",
            "tcost icost 0.0177001953125 0.4176459014415741\n",
            "search 0.435546875\n",
            "tcost icost 0.0169525146484375 0.41813892126083374\n",
            "tcost icost 0.0172119140625 0.41903242468833923\n",
            "tcost icost 0.0172119140625 0.41894298791885376\n",
            "search 0.43603515625\n",
            "tcost icost 0.0182037353515625 0.4195635914802551\n",
            "tcost icost 0.018280029296875 0.41748693585395813\n",
            "tcost icost 0.018280029296875 0.4195486009120941\n",
            "search 0.437744140625\n",
            "tcost icost 0.0174102783203125 0.4139559268951416\n",
            "tcost icost 0.017486572265625 0.4151666760444641\n",
            "tcost icost 0.017486572265625 0.41762688755989075\n",
            "search 0.435302734375\n",
            "tcost icost 0.0177001953125 0.4158158600330353\n",
            "tcost icost 0.0178985595703125 0.41534554958343506\n",
            "tcost icost 0.0178985595703125 0.417919397354126\n",
            "search 0.435791015625\n",
            "tcost icost 0.017547607421875 0.41709640622138977\n",
            "tcost icost 0.0176849365234375 0.4151768982410431\n",
            "tcost icost 0.0176849365234375 0.41715553402900696\n",
            "search 0.434814453125\n",
            "tcost icost 0.0180816650390625 0.4168274998664856\n",
            "tcost icost 0.0176849365234375 0.41471096873283386\n",
            "tcost icost 0.0176849365234375 0.41499215364456177\n",
            "search 0.4326171875\n",
            "tcost icost 0.015777587890625 0.41568323969841003\n",
            "tcost icost 0.0156402587890625 0.41611939668655396\n",
            "tcost icost 0.0156402587890625 0.4154132604598999\n",
            "search 0.43115234375\n",
            "tcost icost 0.01543426513671875 0.4140254259109497\n",
            "tcost icost 0.015411376953125 0.4152776598930359\n",
            "tcost icost 0.015411376953125 0.4134168028831482\n",
            "search 0.4287109375\n",
            "tcost icost 0.0145416259765625 0.4093451499938965\n",
            "tcost icost 0.013916015625 0.4089328348636627\n",
            "tcost icost 0.013916015625 0.41026175022125244\n",
            "search 0.424072265625\n",
            "tcost icost 0.01445770263671875 0.4091668128967285\n",
            "tcost icost 0.01412200927734375 0.40865272283554077\n",
            "tcost icost 0.01412200927734375 0.4084794819355011\n",
            "search 0.422607421875\n",
            "tcost icost 0.0151824951171875 0.40280112624168396\n",
            "tcost icost 0.01470947265625 0.4006620943546295\n",
            "tcost icost 0.01470947265625 0.3995053768157959\n",
            "search 0.4140625\n",
            "ded\n",
            "time\n",
            "23 #### train ####\n",
            "repr, std, cov, conv, closs 0.030641168355941772 0.32958984375 0.2910078465938568 0.06161855161190033 0.048106104135513306\n",
            "51.54803369149802 7.794812407656286 1.0\n",
            "repr, std, cov, conv, closs 0.035005949437618256 0.327392578125 0.33858999609947205 0.06375168263912201 0.023156428709626198\n",
            "51.03537719262263 7.671149173146579 1.0\n",
            "repr, std, cov, conv, closs 0.032489046454429626 0.321044921875 0.3519546687602997 0.06044764444231987 0.016060424968600273\n",
            "50.225710520995804 7.429677260999828 1.0\n",
            "repr, std, cov, conv, closs 0.041579969227313995 0.3212890625 0.34089696407318115 0.06315021961927414 0.0004969612928107381\n",
            "49.72620481601932 7.2972051195531105 1.0\n",
            "repr, std, cov, conv, closs 0.04348630830645561 0.32373046875 0.3461635112762451 0.07402476668357849 0.0006364225409924984\n",
            "49.23166680481393 7.145636620228938 1.0\n",
            "repr, std, cov, conv, closs 0.03746216744184494 0.32568359375 0.3613604009151459 0.06478734314441681 0.0006167585961520672\n",
            "48.83957991878796 7.032272437632244 1.0\n",
            "repr, std, cov, conv, closs 0.039081037044525146 0.32763671875 0.31366631388664246 0.06765013933181763 0.013185673393309116\n",
            "49.133350969523924 6.983242844959728 1.0\n",
            "repr, std, cov, conv, closs 0.043675683438777924 0.32373046875 0.3282221555709839 0.06909734010696411 0.0006240357179194689\n",
            "49.4783179480689 7.039304710069875 1.0\n",
            "repr, std, cov, conv, closs 0.0408661849796772 0.329345703125 0.29140323400497437 0.06202470511198044 0.0018220794154331088\n",
            "50.02530893325793 7.239089304718693 1.0\n",
            "repr, std, cov, conv, closs 0.031770970672369 0.332763671875 0.31711024045944214 0.05838094279170036 0.03105308674275875\n",
            "50.275936231516795 7.333764190210612 1.0\n",
            "repr, std, cov, conv, closs 0.04061141610145569 0.33056640625 0.31013205647468567 0.055700525641441345 0.02069578506052494\n",
            "50.780964054645295 7.496812449760183 1.0\n",
            "repr, std, cov, conv, closs 0.04398457705974579 0.33349609375 0.31019240617752075 0.06480877101421356 0.00018474854005035013\n",
            "51.08641256981525 7.6024527703488145 1.0\n",
            "repr, std, cov, conv, closs 0.029765691608190536 0.334716796875 0.2620537281036377 0.060117609798908234 0.002289110329002142\n",
            "51.85809614588655 7.810409827284005 1.0\n",
            "repr, std, cov, conv, closs 0.03648822382092476 0.332275390625 0.27364081144332886 0.06073775887489319 0.0164914820343256\n",
            "52.11790572641772 7.896755199223289 1.0\n",
            "repr, std, cov, conv, closs 0.047114789485931396 0.334716796875 0.2763988971710205 0.06386049091815948 0.015900665894150734\n",
            "52.326690265281464 8.008031259074912 1.0\n",
            "repr, std, cov, conv, closs 0.03962226212024689 0.328125 0.28622132539749146 0.06761237978935242 4.402193735586479e-05\n",
            "52.37901695554674 7.976079054470414 1.0\n",
            "repr, std, cov, conv, closs 0.040808260440826416 0.32421875 0.3987191915512085 0.06249147653579712 0.01803336851298809\n",
            "51.7028324882216 7.802607220063941 1.0\n",
            "repr, std, cov, conv, closs 0.04272880405187607 0.326171875 0.3251381814479828 0.07306604087352753 0.030350374057888985\n",
            "51.2398251178488 7.671149173146579 1.0\n",
            "repr, std, cov, conv, closs 0.046048104763031006 0.322265625 0.3612544536590576 0.07335203886032104 0.003632161533460021\n",
            "50.73023382082447 7.4893231266335505 1.0\n",
            "repr, std, cov, conv, closs 0.03355392813682556 0.3271484375 0.3197792172431946 0.06899666041135788 0.058274127542972565\n",
            "50.42691491829596 7.370506422177882 1.0\n",
            "repr, std, cov, conv, closs 0.037775732576847076 0.323974609375 0.33349132537841797 0.06161458045244217 0.002779762027785182\n",
            "49.72620481601932 7.195806385287089 1.0\n",
            "repr, std, cov, conv, closs 0.04379873722791672 0.324462890625 0.3543877601623535 0.0656275525689125 0.0004874759470112622\n",
            "49.67652828773159 7.110015372079115 1.0\n",
            "repr, std, cov, conv, closs 0.030468925833702087 0.330322265625 0.2823171615600586 0.05345335602760315 0.0010332127567380667\n",
            "49.67652828773159 6.990226087804687 1.0\n",
            "repr, std, cov, conv, closs 0.03942297399044037 0.327880859375 0.3323593735694885 0.05539025366306305 0.015281436033546925\n",
            "49.97533359965828 6.983242844959728 1.0\n",
            "repr, std, cov, conv, closs 0.038300104439258575 0.332763671875 0.31832757592201233 0.059804659336805344 0.001261914148926735\n",
            "50.5783469942225 7.1171253874511935 1.0\n",
            "repr, std, cov, conv, closs 0.03215532749891281 0.33349609375 0.2737354040145874 0.061059944331645966 0.01815541461110115\n",
            "50.780964054645295 7.231857447271422 1.0\n",
            "repr, std, cov, conv, closs 0.04824353754520416 0.330322265625 0.31451117992401123 0.07027283310890198 7.645714504178613e-05\n",
            "51.54803369149802 7.45944057783353 1.0\n",
            "repr, std, cov, conv, closs 0.04647555574774742 0.332275390625 0.2773323953151703 0.06357026100158691 0.00023625046014785767\n",
            "52.17002363214413 7.594857912436379 1.0\n",
            "repr, std, cov, conv, closs 0.047443658113479614 0.32861328125 0.34813693165779114 0.05842521786689758 0.0001768832007655874\n",
            "52.85231815920079 7.7637109505258755 1.0\n",
            "repr, std, cov, conv, closs 0.03288927674293518 0.3359375 0.2481955736875534 0.055804818868637085 0.003643905511125922\n",
            "52.90517047735999 7.857389598720513 1.0\n",
            "repr, std, cov, conv, closs 0.037895653396844864 0.33740234375 0.2589944303035736 0.06414167582988739 0.0011921047698706388\n",
            "52.79951864056024 8.008031259074912 1.0\n",
            "repr, std, cov, conv, closs 0.04630887135863304 0.327880859375 0.31921815872192383 0.07699562609195709 0.047527678310871124\n",
            "52.48382736847478 8.040111464338896 1.0\n",
            "repr, std, cov, conv, closs 0.0428604930639267 0.328125 0.2953483462333679 0.055350106209516525 0.016362007707357407\n",
            "52.17002363214413 7.833864495805746 1.0\n",
            "repr, std, cov, conv, closs 0.03675079345703125 0.326171875 0.3326812982559204 0.0639549046754837 0.0005094219231978059\n",
            "51.7028324882216 7.725008580250108 1.0\n",
            "repr, std, cov, conv, closs 0.040380965918302536 0.322998046875 0.3389771580696106 0.060774948447942734 0.00024412178026977926\n",
            "52.01382606047073 7.564554277733999 1.0\n",
            "repr, std, cov, conv, closs 0.03669891133904457 0.3212890625 0.31688255071640015 0.06309802830219269 0.015432286076247692\n",
            "51.90995424203243 7.474366918429774 1.0\n",
            "tcost icost 0.0203094482421875 0.4117130637168884\n",
            "tcost icost 0.0209503173828125 0.40996357798576355\n",
            "tcost icost 0.0209503173828125 0.411448210477829\n",
            "search 0.432373046875\n",
            "tcost icost 0.022216796875 0.4123598635196686\n",
            "tcost icost 0.0200653076171875 0.4109274446964264\n",
            "tcost icost 0.0200653076171875 0.4126437306404114\n",
            "search 0.4326171875\n",
            "tcost icost 0.0210113525390625 0.40886446833610535\n",
            "tcost icost 0.0200653076171875 0.41168880462646484\n",
            "tcost icost 0.0200653076171875 0.4083830416202545\n",
            "search 0.428466796875\n",
            "tcost icost 0.022216796875 0.4101574718952179\n",
            "tcost icost 0.0200653076171875 0.40848690271377563\n",
            "tcost icost 0.0200653076171875 0.41199374198913574\n",
            "search 0.43212890625\n",
            "tcost icost 0.020050048828125 0.4103359580039978\n",
            "tcost icost 0.0200653076171875 0.4158596992492676\n",
            "tcost icost 0.0200653076171875 0.41077330708503723\n",
            "search 0.430908203125\n",
            "tcost icost 0.0210113525390625 0.41168081760406494\n",
            "tcost icost 0.0200653076171875 0.4131642282009125\n",
            "tcost icost 0.0200653076171875 0.4113257825374603\n",
            "search 0.431396484375\n",
            "tcost icost 0.0210113525390625 0.41085341572761536\n",
            "tcost icost 0.0200653076171875 0.4132040739059448\n",
            "tcost icost 0.0200653076171875 0.4105582535266876\n",
            "search 0.4306640625\n",
            "tcost icost 0.0196075439453125 0.4116957187652588\n",
            "tcost icost 0.0209503173828125 0.4116126596927643\n",
            "tcost icost 0.0209503173828125 0.41104525327682495\n",
            "search 0.43212890625\n",
            "tcost icost 0.02093505859375 0.4131948947906494\n",
            "tcost icost 0.0213775634765625 0.41392621397972107\n",
            "tcost icost 0.0213775634765625 0.41203930974006653\n",
            "search 0.43359375\n",
            "tcost icost 0.0210113525390625 0.41111084818840027\n",
            "tcost icost 0.0200653076171875 0.41630274057388306\n",
            "tcost icost 0.0200653076171875 0.41219186782836914\n",
            "search 0.43212890625\n",
            "tcost icost 0.0225677490234375 0.41219714283943176\n",
            "tcost icost 0.0202484130859375 0.4117453992366791\n",
            "tcost icost 0.0202484130859375 0.41482388973236084\n",
            "search 0.43505859375\n",
            "tcost icost 0.0212860107421875 0.41379600763320923\n",
            "tcost icost 0.0202484130859375 0.4148334562778473\n",
            "tcost icost 0.0202484130859375 0.41398459672927856\n",
            "search 0.434326171875\n",
            "tcost icost 0.020050048828125 0.4119907021522522\n",
            "tcost icost 0.02008056640625 0.4120272099971771\n",
            "tcost icost 0.02008056640625 0.41202718019485474\n",
            "search 0.43212890625\n",
            "tcost icost 0.0211334228515625 0.41270631551742554\n",
            "tcost icost 0.0204010009765625 0.41181397438049316\n",
            "tcost icost 0.0204010009765625 0.41281235218048096\n",
            "search 0.433349609375\n",
            "tcost icost 0.0221099853515625 0.4118291735649109\n",
            "tcost icost 0.020904541015625 0.41392743587493896\n",
            "tcost icost 0.020904541015625 0.4114252030849457\n",
            "search 0.432373046875\n",
            "tcost icost 0.0212249755859375 0.41420993208885193\n",
            "tcost icost 0.0204925537109375 0.41569995880126953\n",
            "tcost icost 0.0204925537109375 0.41407451033592224\n",
            "search 0.4345703125\n",
            "tcost icost 0.0200653076171875 0.4138774573802948\n",
            "tcost icost 0.0207366943359375 0.41472387313842773\n",
            "tcost icost 0.0207366943359375 0.4164251983165741\n",
            "search 0.437255859375\n",
            "tcost icost 0.0210113525390625 0.4138701558113098\n",
            "tcost icost 0.0214691162109375 0.413707435131073\n",
            "tcost icost 0.0214691162109375 0.41624218225479126\n",
            "search 0.437744140625\n",
            "tcost icost 0.02215576171875 0.4181077778339386\n",
            "tcost icost 0.02093505859375 0.4142780601978302\n",
            "tcost icost 0.02093505859375 0.4154762625694275\n",
            "search 0.4365234375\n",
            "tcost icost 0.0218658447265625 0.41778481006622314\n",
            "tcost icost 0.0206756591796875 0.4160180389881134\n",
            "tcost icost 0.0206756591796875 0.41640040278434753\n",
            "search 0.437255859375\n",
            "tcost icost 0.0208740234375 0.4210832118988037\n",
            "tcost icost 0.0203094482421875 0.42035192251205444\n",
            "tcost icost 0.0203094482421875 0.4213976562023163\n",
            "search 0.441650390625\n",
            "tcost icost 0.021148681640625 0.4190608263015747\n",
            "tcost icost 0.0204010009765625 0.4205417037010193\n",
            "tcost icost 0.0204010009765625 0.42144179344177246\n",
            "search 0.44189453125\n",
            "tcost icost 0.0223541259765625 0.4194619059562683\n",
            "tcost icost 0.0201263427734375 0.4229275584220886\n",
            "tcost icost 0.0201263427734375 0.4196709096431732\n",
            "search 0.439697265625\n",
            "tcost icost 0.0201873779296875 0.4201095402240753\n",
            "tcost icost 0.0193023681640625 0.41899573802948\n",
            "tcost icost 0.0193023681640625 0.4198981821537018\n",
            "search 0.439208984375\n",
            "tcost icost 0.01947021484375 0.4216349422931671\n",
            "tcost icost 0.019287109375 0.4226776957511902\n",
            "tcost icost 0.019287109375 0.421595960855484\n",
            "search 0.44091796875\n",
            "tcost icost 0.01947021484375 0.4217586815357208\n",
            "tcost icost 0.0194244384765625 0.4223143458366394\n",
            "tcost icost 0.0194244384765625 0.4202762544155121\n",
            "search 0.439697265625\n",
            "tcost icost 0.0190277099609375 0.4191143214702606\n",
            "tcost icost 0.019500732421875 0.41911569237709045\n",
            "tcost icost 0.019500732421875 0.4187633991241455\n",
            "search 0.438232421875\n",
            "tcost icost 0.01953125 0.42294037342071533\n",
            "tcost icost 0.019378662109375 0.4191194474697113\n",
            "tcost icost 0.019378662109375 0.41911938786506653\n",
            "search 0.4384765625\n",
            "tcost icost 0.020477294921875 0.4191170036792755\n",
            "tcost icost 0.0194549560546875 0.41887375712394714\n",
            "tcost icost 0.0194549560546875 0.4214458167552948\n",
            "search 0.44091796875\n",
            "tcost icost 0.019195556640625 0.42180371284484863\n",
            "tcost icost 0.0193634033203125 0.42102983593940735\n",
            "tcost icost 0.0193634033203125 0.41954681277275085\n",
            "search 0.438720703125\n",
            "tcost icost 0.0195159912109375 0.42148783802986145\n",
            "tcost icost 0.019500732421875 0.4214763045310974\n",
            "tcost icost 0.019500732421875 0.42117732763290405\n",
            "search 0.440673828125\n",
            "tcost icost 0.019622802734375 0.4242146909236908\n",
            "tcost icost 0.0201416015625 0.4245326519012451\n",
            "tcost icost 0.0201416015625 0.4249527156352997\n",
            "search 0.4453125\n",
            "tcost icost 0.01910400390625 0.42224353551864624\n",
            "tcost icost 0.019378662109375 0.4226389527320862\n",
            "tcost icost 0.019378662109375 0.42488622665405273\n",
            "search 0.444091796875\n",
            "tcost icost 0.019378662109375 0.4241787791252136\n",
            "tcost icost 0.0201568603515625 0.42518433928489685\n",
            "tcost icost 0.0201568603515625 0.426540344953537\n",
            "search 0.44677734375\n",
            "tcost icost 0.0198974609375 0.42592698335647583\n",
            "tcost icost 0.019989013671875 0.4232668876647949\n",
            "tcost icost 0.019989013671875 0.42356956005096436\n",
            "search 0.443603515625\n",
            "tcost icost 0.01953125 0.4260461926460266\n",
            "tcost icost 0.019378662109375 0.42499080300331116\n",
            "tcost icost 0.019378662109375 0.4239901900291443\n",
            "search 0.443359375\n",
            "tcost icost 0.019439697265625 0.4243778586387634\n",
            "tcost icost 0.019317626953125 0.42381054162979126\n",
            "tcost icost 0.019317626953125 0.42602670192718506\n",
            "search 0.4453125\n",
            "tcost icost 0.019256591796875 0.42418617010116577\n",
            "tcost icost 0.0201416015625 0.42306283116340637\n",
            "tcost icost 0.0201416015625 0.42306283116340637\n",
            "search 0.443359375\n",
            "tcost icost 0.01947021484375 0.4237479865550995\n",
            "tcost icost 0.0193328857421875 0.42370760440826416\n",
            "tcost icost 0.0193328857421875 0.42370760440826416\n",
            "search 0.443115234375\n",
            "tcost icost 0.01983642578125 0.42192205786705017\n",
            "tcost icost 0.019317626953125 0.42134973406791687\n",
            "tcost icost 0.019317626953125 0.4213509261608124\n",
            "search 0.440673828125\n",
            "tcost icost 0.0193023681640625 0.424487441778183\n",
            "tcost icost 0.0199432373046875 0.42238038778305054\n",
            "tcost icost 0.0199432373046875 0.42184826731681824\n",
            "search 0.44189453125\n",
            "tcost icost 0.019378662109375 0.42499053478240967\n",
            "tcost icost 0.019287109375 0.42395225167274475\n",
            "tcost icost 0.019287109375 0.4234708547592163\n",
            "search 0.44287109375\n",
            "tcost icost 0.0201416015625 0.42559248208999634\n",
            "tcost icost 0.0193328857421875 0.4255010187625885\n",
            "tcost icost 0.0193328857421875 0.4255010187625885\n",
            "search 0.44482421875\n",
            "tcost icost 0.019561767578125 0.4277101755142212\n",
            "tcost icost 0.0192108154296875 0.4273932874202728\n",
            "tcost icost 0.0192108154296875 0.4270451068878174\n",
            "search 0.4462890625\n",
            "tcost icost 0.01959228515625 0.43016475439071655\n",
            "tcost icost 0.0191802978515625 0.43111345171928406\n",
            "tcost icost 0.0191802978515625 0.4301639795303345\n",
            "search 0.449462890625\n",
            "tcost icost 0.0195159912109375 0.4335751533508301\n",
            "tcost icost 0.019378662109375 0.430965781211853\n",
            "tcost icost 0.019378662109375 0.4320896863937378\n",
            "search 0.451416015625\n",
            "tcost icost 0.019134521484375 0.4341661334037781\n",
            "tcost icost 0.0196075439453125 0.4337281584739685\n",
            "tcost icost 0.0196075439453125 0.43400102853775024\n",
            "search 0.45361328125\n",
            "tcost icost 0.0207977294921875 0.4379010498523712\n",
            "tcost icost 0.021728515625 0.43371671438217163\n",
            "tcost icost 0.021728515625 0.433959037065506\n",
            "search 0.45556640625\n",
            "tcost icost 0.01953125 0.435884028673172\n",
            "tcost icost 0.0192413330078125 0.43294355273246765\n",
            "tcost icost 0.0192413330078125 0.43255460262298584\n",
            "search 0.451904296875\n",
            "tcost icost 0.019622802734375 0.43168288469314575\n",
            "tcost icost 0.01947021484375 0.4333447217941284\n",
            "tcost icost 0.01947021484375 0.4336200952529907\n",
            "search 0.453125\n",
            "tcost icost 0.01934814453125 0.4315831661224365\n",
            "tcost icost 0.019195556640625 0.4330320954322815\n",
            "tcost icost 0.019195556640625 0.43204957246780396\n",
            "search 0.451416015625\n",
            "tcost icost 0.0195159912109375 0.43673181533813477\n",
            "tcost icost 0.019256591796875 0.4360958933830261\n",
            "tcost icost 0.019256591796875 0.43646344542503357\n",
            "search 0.455810546875\n",
            "tcost icost 0.0201568603515625 0.43550047278404236\n",
            "tcost icost 0.019317626953125 0.4363192021846771\n",
            "tcost icost 0.019317626953125 0.4363192021846771\n",
            "search 0.45556640625\n",
            "tcost icost 0.0199737548828125 0.3909437358379364\n",
            "tcost icost 0.019439697265625 0.3796321153640747\n",
            "tcost icost 0.019439697265625 0.3796321153640747\n",
            "search 0.399169921875\n",
            "tcost icost 0.0215606689453125 0.3785933256149292\n",
            "tcost icost 0.0206451416015625 0.36365965008735657\n",
            "tcost icost 0.0206451416015625 0.36455586552619934\n",
            "search 0.38525390625\n",
            "ded\n",
            "time\n",
            "24 #### train ####\n",
            "repr, std, cov, conv, closs 0.05142394080758095 0.320068359375 0.38010966777801514 0.053917210549116135 0.0037392396479845047\n",
            "51.7028324882216 7.275357214562424 1.0\n",
            "repr, std, cov, conv, closs 0.04495435953140259 0.322509765625 0.4220435619354248 0.05891769751906395 0.0015670245047658682\n",
            "51.3936983639175 7.174262069119264 1.0\n",
            "repr, std, cov, conv, closs 0.03991799056529999 0.322998046875 0.3494378626346588 0.06422122567892075 0.014303749427199364\n",
            "51.18863648136744 7.039304710069875 1.0\n",
            "repr, std, cov, conv, closs 0.04713984578847885 0.330810546875 0.33002132177352905 0.0742170661687851 0.0025792717933654785\n",
            "51.13749898238506 6.990226087804687 1.0\n",
            "repr, std, cov, conv, closs 0.05453714728355408 0.325927734375 0.3102775514125824 0.06079325079917908 0.030225779861211777\n",
            "51.54803369149802 6.9972163138924905 1.0\n",
            "repr, std, cov, conv, closs 0.041800692677497864 0.332763671875 0.2762823700904846 0.06262365728616714 0.014313495717942715\n",
            "52.01382606047073 7.102912459619496 1.0\n",
            "repr, std, cov, conv, closs 0.04776036739349365 0.329833984375 0.27749955654144287 0.05855082720518112 0.015703748911619186\n",
            "52.588847507039084 7.311806826997334 1.0\n",
            "repr, std, cov, conv, closs 0.03951093554496765 0.331787109375 0.3118816018104553 0.06194622442126274 0.012366010807454586\n",
            "52.74677186869155 7.414840165828007 1.0\n",
            "repr, std, cov, conv, closs 0.03790033981204033 0.334716796875 0.2825608253479004 0.060872890055179596 0.0012938095023855567\n",
            "52.85231815920079 7.564554277733999 1.0\n",
            "repr, std, cov, conv, closs 0.04469282925128937 0.338623046875 0.259376585483551 0.056721340864896774 0.015083893202245235\n",
            "53.06404475720865 7.655829857601519 1.0\n",
            "repr, std, cov, conv, closs 0.03526041656732559 0.33203125 0.3332449793815613 0.0671120434999466 0.00010857157758437097\n",
            "53.59707946563695 7.873112235307551 1.0\n",
            "repr, std, cov, conv, closs 0.037027448415756226 0.331787109375 0.3298119306564331 0.06324117630720139 0.00017261314496863633\n",
            "53.9194669713686 7.928389632146293 1.0\n",
            "repr, std, cov, conv, closs 0.05380428582429886 0.33154296875 0.31392645835876465 0.05786452442407608 0.062430113554000854\n",
            "53.86560136999861 7.849540058661852 1.0\n",
            "repr, std, cov, conv, closs 0.04413045197725296 0.329833984375 0.3083844780921936 0.06085140258073807 0.006317565217614174\n",
            "53.650676545102584 7.755954995530346 1.0\n",
            "repr, std, cov, conv, closs 0.05494919419288635 0.3212890625 0.3481413424015045 0.06219726800918579 0.0007039193878881633\n",
            "52.90517047735999 7.519325385043623 1.0\n",
            "repr, std, cov, conv, closs 0.054614223539829254 0.32275390625 0.34281712770462036 0.05708842724561691 0.0005571712972596288\n",
            "52.641436354546116 7.385254805528658 1.0\n",
            "repr, std, cov, conv, closs 0.05511601269245148 0.318115234375 0.4371660351753235 0.055881135165691376 0.023448489606380463\n",
            "52.222193655776266 7.174262069119264 1.0\n",
            "repr, std, cov, conv, closs 0.0441056489944458 0.32373046875 0.36033374071121216 0.05705402418971062 0.00011727547826012596\n",
            "52.222193655776266 7.088727915061459 1.0\n",
            "repr, std, cov, conv, closs 0.04960772395133972 0.326171875 0.34773069620132446 0.06610918045043945 0.017150459811091423\n",
            "52.326690265281464 6.990226087804687 1.0\n",
            "repr, std, cov, conv, closs 0.044714972376823425 0.325439453125 0.3514483571052551 0.06261052936315536 4.512111627263948e-05\n",
            "52.37901695554674 6.955379566587508 1.0\n",
            "repr, std, cov, conv, closs 0.03263910859823227 0.332763671875 0.2911277711391449 0.05907759070396423 0.015561148524284363\n",
            "52.588847507039084 7.046344014779945 1.0\n",
            "repr, std, cov, conv, closs 0.03865095227956772 0.3310546875 0.3161105811595917 0.06724201887845993 0.029509253799915314\n",
            "52.69407779090066 7.145636620228938 1.0\n",
            "repr, std, cov, conv, closs 0.05807666480541229 0.329833984375 0.32387903332710266 0.0747009739279747 0.0003780921979341656\n",
            "53.276619532815246 7.355787491407577 1.0\n",
            "repr, std, cov, conv, closs 0.047586847096681595 0.334716796875 0.30685219168663025 0.07138809561729431 0.015226870775222778\n",
            "53.650676545102584 7.481841285348203 1.0\n",
            "repr, std, cov, conv, closs 0.03850312903523445 0.333740234375 0.2887856364250183 0.06447117775678635 0.00019461975898593664\n",
            "53.86560136999861 7.709581707253895 1.0\n",
            "repr, std, cov, conv, closs 0.0465695895254612 0.333984375 0.2851974368095398 0.06454078108072281 0.0004208978498354554\n",
            "54.0273598247783 7.818220237111288 1.0\n",
            "repr, std, cov, conv, closs 0.04095521196722984 0.331787109375 0.27478712797164917 0.0587787888944149 0.0003970175748690963\n",
            "54.46109449876867 7.992039188658408 1.0\n",
            "repr, std, cov, conv, closs 0.03605937212705612 0.33056640625 0.32380688190460205 0.0687502846121788 0.00046506713260896504\n",
            "54.352335475482235 8.056199727379036 1.0\n",
            "repr, std, cov, conv, closs 0.0399191752076149 0.33154296875 0.2883502244949341 0.0633181780576706 0.00010608982120174915\n",
            "53.86560136999861 8.1046579299841 1.0\n",
            "repr, std, cov, conv, closs 0.04576631635427475 0.326171875 0.32092058658599854 0.06178335100412369 0.015891365706920624\n",
            "53.70432722164768 8.128996225852498 1.0\n",
            "repr, std, cov, conv, closs 0.05249816179275513 0.32470703125 0.36355942487716675 0.06300924718379974 0.0007171601755544543\n",
            "53.01103372348517 7.8888663328904 1.0\n",
            "repr, std, cov, conv, closs 0.03587445989251137 0.3212890625 0.33831357955932617 0.06531378626823425 0.00022919363982509822\n",
            "52.37901695554674 7.7637109505258755 1.0\n",
            "repr, std, cov, conv, closs 0.03991391137242317 0.326171875 0.37656736373901367 0.060244858264923096 0.025434503331780434\n",
            "52.01382606047073 7.519325385043623 1.0\n",
            "repr, std, cov, conv, closs 0.042062461376190186 0.323486328125 0.40505659580230713 0.07078352570533752 0.00901543814688921\n",
            "51.75453532070982 7.429677260999828 1.0\n",
            "repr, std, cov, conv, closs 0.03801373392343521 0.3232421875 0.3712471127510071 0.06712445616722107 0.043320465832948685\n",
            "51.13749898238506 7.253574722417433 1.0\n",
            "repr, std, cov, conv, closs 0.04352327808737755 0.324951171875 0.35027462244033813 0.06638173758983612 0.01587572693824768\n",
            "51.18863648136744 7.1886177675195695 1.0\n",
            "tcost icost 0.0295257568359375 0.433548241853714\n",
            "tcost icost 0.0283966064453125 0.4323786795139313\n",
            "tcost icost 0.0283966064453125 0.43261459469795227\n",
            "search 0.4609375\n",
            "tcost icost 0.0283966064453125 0.432377427816391\n",
            "tcost icost 0.0283966064453125 0.4327771067619324\n",
            "tcost icost 0.0283966064453125 0.43237748742103577\n",
            "search 0.460693359375\n",
            "tcost icost 0.033203125 0.4386787712574005\n",
            "tcost icost 0.029266357421875 0.43620434403419495\n",
            "tcost icost 0.029266357421875 0.4346832036972046\n",
            "search 0.4638671875\n",
            "tcost icost 0.0283966064453125 0.4323926270008087\n",
            "tcost icost 0.0283966064453125 0.4328157305717468\n",
            "tcost icost 0.0283966064453125 0.43259403109550476\n",
            "search 0.4609375\n",
            "tcost icost 0.032012939453125 0.43532586097717285\n",
            "tcost icost 0.0283966064453125 0.43281570076942444\n",
            "tcost icost 0.0283966064453125 0.43281570076942444\n",
            "search 0.461181640625\n",
            "tcost icost 0.0295257568359375 0.4361855983734131\n",
            "tcost icost 0.0283966064453125 0.43281570076942444\n",
            "tcost icost 0.0283966064453125 0.43281570076942444\n",
            "search 0.461181640625\n",
            "tcost icost 0.032012939453125 0.43797725439071655\n",
            "tcost icost 0.0283966064453125 0.4326401650905609\n",
            "tcost icost 0.0283966064453125 0.4328773617744446\n",
            "search 0.461181640625\n",
            "tcost icost 0.032012939453125 0.4381062686443329\n",
            "tcost icost 0.0283966064453125 0.43538588285446167\n",
            "tcost icost 0.0283966064453125 0.43538588285446167\n",
            "search 0.463623046875\n",
            "tcost icost 0.029266357421875 0.4374975264072418\n",
            "tcost icost 0.033203125 0.4389239549636841\n",
            "tcost icost 0.033203125 0.4389239549636841\n",
            "search 0.47216796875\n",
            "tcost icost 0.032012939453125 0.43786752223968506\n",
            "tcost icost 0.0283966064453125 0.4352227449417114\n",
            "tcost icost 0.0283966064453125 0.43538588285446167\n",
            "search 0.463623046875\n",
            "tcost icost 0.032012939453125 0.4382459223270416\n",
            "tcost icost 0.0283966064453125 0.4353858530521393\n",
            "tcost icost 0.0283966064453125 0.4353502690792084\n",
            "search 0.463623046875\n",
            "tcost icost 0.032012939453125 0.44012606143951416\n",
            "tcost icost 0.0283966064453125 0.43702682852745056\n",
            "tcost icost 0.0283966064453125 0.4366788864135742\n",
            "search 0.465087890625\n",
            "tcost icost 0.032012939453125 0.4465996325016022\n",
            "tcost icost 0.0283966064453125 0.43853795528411865\n",
            "tcost icost 0.0283966064453125 0.43853795528411865\n",
            "search 0.466796875\n",
            "tcost icost 0.0283966064453125 0.43853795528411865\n",
            "tcost icost 0.0283966064453125 0.43853795528411865\n",
            "tcost icost 0.0283966064453125 0.43853795528411865\n",
            "search 0.466796875\n",
            "tcost icost 0.033203125 0.444641649723053\n",
            "tcost icost 0.029266357421875 0.442617267370224\n",
            "tcost icost 0.029266357421875 0.44439074397087097\n",
            "search 0.4736328125\n",
            "tcost icost 0.03607177734375 0.4463473856449127\n",
            "tcost icost 0.0283966064453125 0.44064846634864807\n",
            "tcost icost 0.0283966064453125 0.43903788924217224\n",
            "search 0.46728515625\n",
            "tcost icost 0.029266357421875 0.4422849714756012\n",
            "tcost icost 0.029266357421875 0.443927526473999\n",
            "tcost icost 0.029266357421875 0.44400447607040405\n",
            "search 0.473388671875\n",
            "tcost icost 0.033203125 0.4443201422691345\n",
            "tcost icost 0.029266357421875 0.44250261783599854\n",
            "tcost icost 0.029266357421875 0.4422849416732788\n",
            "search 0.4716796875\n",
            "tcost icost 0.0295257568359375 0.4417506456375122\n",
            "tcost icost 0.0283966064453125 0.43928202986717224\n",
            "tcost icost 0.0283966064453125 0.43928202986717224\n",
            "search 0.467529296875\n",
            "tcost icost 0.03143310546875 0.441482275724411\n",
            "tcost icost 0.0283966064453125 0.43928202986717224\n",
            "tcost icost 0.0283966064453125 0.43928202986717224\n",
            "search 0.467529296875\n",
            "tcost icost 0.029266357421875 0.4453350007534027\n",
            "tcost icost 0.029266357421875 0.44398483633995056\n",
            "tcost icost 0.029266357421875 0.4436255693435669\n",
            "search 0.472900390625\n",
            "tcost icost 0.0360107421875 0.44770118594169617\n",
            "tcost icost 0.0283966064453125 0.44032472372055054\n",
            "tcost icost 0.0283966064453125 0.43990179896354675\n",
            "search 0.46826171875\n",
            "tcost icost 0.032012939453125 0.444904625415802\n",
            "tcost icost 0.0283966064453125 0.44032472372055054\n",
            "tcost icost 0.0283966064453125 0.44141173362731934\n",
            "search 0.4697265625\n",
            "tcost icost 0.031463623046875 0.4428544342517853\n",
            "tcost icost 0.0284271240234375 0.4404645264148712\n",
            "tcost icost 0.0284271240234375 0.4404645264148712\n",
            "search 0.46875\n",
            "tcost icost 0.0361328125 0.4463033974170685\n",
            "tcost icost 0.0284271240234375 0.4404645264148712\n",
            "tcost icost 0.0284271240234375 0.4404645264148712\n",
            "search 0.46875\n",
            "tcost icost 0.032012939453125 0.4448210597038269\n",
            "tcost icost 0.0283966064453125 0.44032472372055054\n",
            "tcost icost 0.0283966064453125 0.4411787986755371\n",
            "search 0.469482421875\n",
            "tcost icost 0.03314208984375 0.44615480303764343\n",
            "tcost icost 0.0291748046875 0.4433748722076416\n",
            "tcost icost 0.0291748046875 0.4435690641403198\n",
            "search 0.47265625\n",
            "tcost icost 0.031646728515625 0.44456979632377625\n",
            "tcost icost 0.0285186767578125 0.44085073471069336\n",
            "tcost icost 0.0285186767578125 0.44085073471069336\n",
            "search 0.469482421875\n",
            "tcost icost 0.0322265625 0.44512540102005005\n",
            "tcost icost 0.02850341796875 0.44312840700149536\n",
            "tcost icost 0.02850341796875 0.4407826364040375\n",
            "search 0.46923828125\n",
            "tcost icost 0.035980224609375 0.44519007205963135\n",
            "tcost icost 0.0284881591796875 0.44073787331581116\n",
            "tcost icost 0.0284881591796875 0.44073787331581116\n",
            "search 0.46923828125\n",
            "tcost icost 0.0298614501953125 0.4444623589515686\n",
            "tcost icost 0.02850341796875 0.4408062994480133\n",
            "tcost icost 0.02850341796875 0.4408062994480133\n",
            "search 0.469482421875\n",
            "tcost icost 0.035614013671875 0.4456951320171356\n",
            "tcost icost 0.028900146484375 0.4474184513092041\n",
            "tcost icost 0.028900146484375 0.4442974328994751\n",
            "search 0.47314453125\n",
            "tcost icost 0.03106689453125 0.4412263333797455\n",
            "tcost icost 0.02825927734375 0.4383403956890106\n",
            "tcost icost 0.02825927734375 0.43871402740478516\n",
            "search 0.467041015625\n",
            "tcost icost 0.0289764404296875 0.44169890880584717\n",
            "tcost icost 0.0282745361328125 0.4386986494064331\n",
            "tcost icost 0.0282745361328125 0.4424985647201538\n",
            "search 0.470703125\n",
            "tcost icost 0.0289154052734375 0.44156593084335327\n",
            "tcost icost 0.0282440185546875 0.44079679250717163\n",
            "tcost icost 0.0282440185546875 0.4382041394710541\n",
            "search 0.466552734375\n",
            "tcost icost 0.031158447265625 0.4429854154586792\n",
            "tcost icost 0.0282745361328125 0.439043253660202\n",
            "tcost icost 0.0282745361328125 0.4387072026729584\n",
            "search 0.467041015625\n",
            "tcost icost 0.02880859375 0.44293543696403503\n",
            "tcost icost 0.02880859375 0.44293543696403503\n",
            "tcost icost 0.02880859375 0.44389697909355164\n",
            "search 0.47265625\n",
            "tcost icost 0.032012939453125 0.4446793794631958\n",
            "tcost icost 0.028411865234375 0.4396471679210663\n",
            "tcost icost 0.028411865234375 0.440143346786499\n",
            "search 0.468505859375\n",
            "tcost icost 0.031402587890625 0.44212424755096436\n",
            "tcost icost 0.0283966064453125 0.43957048654556274\n",
            "tcost icost 0.0283966064453125 0.43923163414001465\n",
            "search 0.467529296875\n",
            "tcost icost 0.02947998046875 0.4462476074695587\n",
            "tcost icost 0.0286865234375 0.44559988379478455\n",
            "tcost icost 0.0286865234375 0.4424039423465729\n",
            "search 0.47119140625\n",
            "tcost icost 0.028228759765625 0.43803077936172485\n",
            "tcost icost 0.028228759765625 0.43803077936172485\n",
            "tcost icost 0.028228759765625 0.43803077936172485\n",
            "search 0.46630859375\n",
            "tcost icost 0.0288238525390625 0.44091588258743286\n",
            "tcost icost 0.0282135009765625 0.43781164288520813\n",
            "tcost icost 0.0282135009765625 0.43781161308288574\n",
            "search 0.466064453125\n",
            "tcost icost 0.0288238525390625 0.44185900688171387\n",
            "tcost icost 0.0282135009765625 0.4385824501514435\n",
            "tcost icost 0.0282135009765625 0.4385824501514435\n",
            "search 0.466796875\n",
            "tcost icost 0.031585693359375 0.44510728120803833\n",
            "tcost icost 0.028289794921875 0.4394364058971405\n",
            "tcost icost 0.028289794921875 0.4394364058971405\n",
            "search 0.4677734375\n",
            "tcost icost 0.0298614501953125 0.44537392258644104\n",
            "tcost icost 0.0291290283203125 0.444764107465744\n",
            "tcost icost 0.0291290283203125 0.4460621476173401\n",
            "search 0.47509765625\n",
            "tcost icost 0.035552978515625 0.4474984109401703\n",
            "tcost icost 0.0288543701171875 0.44391030073165894\n",
            "tcost icost 0.0288543701171875 0.44391030073165894\n",
            "search 0.47265625\n",
            "tcost icost 0.0291748046875 0.4438987970352173\n",
            "tcost icost 0.028350830078125 0.442780077457428\n",
            "tcost icost 0.028350830078125 0.442780077457428\n",
            "search 0.47119140625\n",
            "tcost icost 0.0285797119140625 0.4432276785373688\n",
            "tcost icost 0.0285797119140625 0.4432276785373688\n",
            "tcost icost 0.0285797119140625 0.4456978142261505\n",
            "search 0.474365234375\n",
            "tcost icost 0.028106689453125 0.4420586824417114\n",
            "tcost icost 0.028106689453125 0.4423145651817322\n",
            "tcost icost 0.028106689453125 0.44472959637641907\n",
            "search 0.472900390625\n",
            "tcost icost 0.027374267578125 0.44146448373794556\n",
            "tcost icost 0.027374267578125 0.4391191303730011\n",
            "tcost icost 0.027374267578125 0.44162946939468384\n",
            "search 0.468994140625\n",
            "tcost icost 0.0284271240234375 0.4364355206489563\n",
            "tcost icost 0.02777099609375 0.4316931962966919\n",
            "tcost icost 0.02777099609375 0.4316931962966919\n",
            "search 0.45947265625\n",
            "tcost icost 0.0311126708984375 0.44398197531700134\n",
            "tcost icost 0.02813720703125 0.4372957646846771\n",
            "tcost icost 0.02813720703125 0.4372957646846771\n",
            "search 0.46533203125\n",
            "tcost icost 0.030914306640625 0.445556640625\n",
            "tcost icost 0.0281219482421875 0.43589383363723755\n",
            "tcost icost 0.0281219482421875 0.4358937740325928\n",
            "search 0.4638671875\n",
            "tcost icost 0.031463623046875 0.44481855630874634\n",
            "tcost icost 0.0282440185546875 0.4387059509754181\n",
            "tcost icost 0.0282440185546875 0.4387059509754181\n",
            "search 0.467041015625\n",
            "tcost icost 0.032135009765625 0.44619515538215637\n",
            "tcost icost 0.0289764404296875 0.44345101714134216\n",
            "tcost icost 0.0289764404296875 0.44586458802223206\n",
            "search 0.474853515625\n",
            "tcost icost 0.031524658203125 0.44680169224739075\n",
            "tcost icost 0.029693603515625 0.44663992524147034\n",
            "tcost icost 0.029693603515625 0.4447900354862213\n",
            "search 0.474609375\n",
            "tcost icost 0.0290679931640625 0.44432222843170166\n",
            "tcost icost 0.0277557373046875 0.44263532757759094\n",
            "tcost icost 0.0277557373046875 0.44263532757759094\n",
            "search 0.470458984375\n",
            "tcost icost 0.0288848876953125 0.44180405139923096\n",
            "tcost icost 0.0272979736328125 0.4388373792171478\n",
            "tcost icost 0.0272979736328125 0.4426056742668152\n",
            "search 0.469970703125\n",
            "tcost icost 0.03070068359375 0.44033002853393555\n",
            "tcost icost 0.0281219482421875 0.43609365820884705\n",
            "tcost icost 0.0281219482421875 0.43648427724838257\n",
            "search 0.464599609375\n",
            "tcost icost 0.03314208984375 0.4304526448249817\n",
            "tcost icost 0.0268096923828125 0.41521990299224854\n",
            "tcost icost 0.0268096923828125 0.4151017963886261\n",
            "search 0.44189453125\n",
            "tcost icost 0.0288238525390625 0.42734748125076294\n",
            "tcost icost 0.0267486572265625 0.4190480411052704\n",
            "tcost icost 0.0267486572265625 0.4190480411052704\n",
            "search 0.44580078125\n",
            "tcost icost 0.028411865234375 0.43718743324279785\n",
            "tcost icost 0.0277862548828125 0.4321863651275635\n",
            "tcost icost 0.0277862548828125 0.4341403543949127\n",
            "search 0.4619140625\n",
            "tcost icost 0.0301361083984375 0.43679678440093994\n",
            "tcost icost 0.02703857421875 0.42391058802604675\n",
            "tcost icost 0.02703857421875 0.42452552914619446\n",
            "search 0.45166015625\n",
            "tcost icost 0.03082275390625 0.4420647919178009\n",
            "tcost icost 0.0281219482421875 0.4401961863040924\n",
            "tcost icost 0.0281219482421875 0.4402778744697571\n",
            "search 0.46826171875\n",
            "tcost icost 0.03399658203125 0.4447798430919647\n",
            "tcost icost 0.027435302734375 0.42916616797447205\n",
            "tcost icost 0.027435302734375 0.42926499247550964\n",
            "search 0.45654296875\n",
            "tcost icost 0.027740478515625 0.43236079812049866\n",
            "tcost icost 0.027740478515625 0.43236079812049866\n",
            "tcost icost 0.027740478515625 0.43236079812049866\n",
            "search 0.460205078125\n",
            "tcost icost 0.0302734375 0.4410325884819031\n",
            "tcost icost 0.027252197265625 0.4274452328681946\n",
            "tcost icost 0.027252197265625 0.4274452328681946\n",
            "search 0.454833984375\n",
            "tcost icost 0.026763916015625 0.4208509027957916\n",
            "tcost icost 0.026763916015625 0.41948753595352173\n",
            "tcost icost 0.026763916015625 0.42004990577697754\n",
            "search 0.447021484375\n",
            "tcost icost 0.0289306640625 0.4389807879924774\n",
            "tcost icost 0.0271148681640625 0.4336920380592346\n",
            "tcost icost 0.0271148681640625 0.4336920380592346\n",
            "search 0.460693359375\n",
            "tcost icost 0.029693603515625 0.43190598487854004\n",
            "tcost icost 0.0267486572265625 0.4174162745475769\n",
            "tcost icost 0.0267486572265625 0.41631418466567993\n",
            "search 0.443115234375\n",
            "tcost icost 0.0288238525390625 0.42534303665161133\n",
            "tcost icost 0.0267333984375 0.41842636466026306\n",
            "tcost icost 0.0267333984375 0.4176458418369293\n",
            "search 0.4443359375\n",
            "tcost icost 0.0300140380859375 0.39778199791908264\n",
            "tcost icost 0.029937744140625 0.37766027450561523\n",
            "tcost icost 0.029937744140625 0.37766027450561523\n",
            "search 0.40771484375\n",
            "tcost icost 0.031982421875 0.44171303510665894\n",
            "tcost icost 0.0269317626953125 0.431244432926178\n",
            "tcost icost 0.0269317626953125 0.4309599995613098\n",
            "search 0.457763671875\n",
            "tcost icost 0.0289154052734375 0.4377362132072449\n",
            "tcost icost 0.026947021484375 0.43332594633102417\n",
            "tcost icost 0.026947021484375 0.4316537380218506\n",
            "search 0.45849609375\n",
            "tcost icost 0.028900146484375 0.44375094771385193\n",
            "tcost icost 0.0273895263671875 0.4413391649723053\n",
            "tcost icost 0.0273895263671875 0.4413391947746277\n",
            "search 0.46875\n",
            "tcost icost 0.0309906005859375 0.4207354485988617\n",
            "tcost icost 0.0275726318359375 0.4006709158420563\n",
            "tcost icost 0.0275726318359375 0.4006709158420563\n",
            "search 0.42822265625\n",
            "tcost icost 0.0328369140625 0.43168747425079346\n",
            "tcost icost 0.0269317626953125 0.40776026248931885\n",
            "tcost icost 0.0269317626953125 0.40776026248931885\n",
            "search 0.4345703125\n",
            "tcost icost 0.0289306640625 0.4362224340438843\n",
            "tcost icost 0.0268096923828125 0.430877149105072\n",
            "tcost icost 0.0268096923828125 0.4296155869960785\n",
            "search 0.45654296875\n",
            "tcost icost 0.0298309326171875 0.4333204925060272\n",
            "tcost icost 0.0267333984375 0.41902804374694824\n",
            "tcost icost 0.0267333984375 0.41902804374694824\n",
            "search 0.44580078125\n",
            "tcost icost 0.029266357421875 0.42314139008522034\n",
            "tcost icost 0.0272064208984375 0.40406084060668945\n",
            "tcost icost 0.0272064208984375 0.40406084060668945\n",
            "search 0.43115234375\n",
            "tcost icost 0.02752685546875 0.42121803760528564\n",
            "tcost icost 0.0272216796875 0.3991171419620514\n",
            "tcost icost 0.0272216796875 0.3987326920032501\n",
            "search 0.42578125\n",
            "tcost icost 0.029052734375 0.40849804878234863\n",
            "tcost icost 0.0277862548828125 0.39886409044265747\n",
            "tcost icost 0.0277862548828125 0.39780741930007935\n",
            "search 0.425537109375\n",
            "tcost icost 0.0294952392578125 0.43580710887908936\n",
            "tcost icost 0.0274505615234375 0.43001070618629456\n",
            "tcost icost 0.0274505615234375 0.429477721452713\n",
            "search 0.456787109375\n",
            "tcost icost 0.0288848876953125 0.4409634470939636\n",
            "tcost icost 0.02716064453125 0.4359632432460785\n",
            "tcost icost 0.02716064453125 0.4359632432460785\n",
            "search 0.463134765625\n",
            "tcost icost 0.028564453125 0.4012950658798218\n",
            "tcost icost 0.0294952392578125 0.37514472007751465\n",
            "tcost icost 0.0294952392578125 0.37450531125068665\n",
            "search 0.404052734375\n",
            "tcost icost 0.02880859375 0.409150630235672\n",
            "tcost icost 0.0292816162109375 0.3842303454875946\n",
            "tcost icost 0.0292816162109375 0.38475537300109863\n",
            "search 0.4140625\n",
            "tcost icost 0.02862548828125 0.40542730689048767\n",
            "tcost icost 0.02777099609375 0.39609673619270325\n",
            "tcost icost 0.02777099609375 0.39609673619270325\n",
            "search 0.423828125\n",
            "tcost icost 0.033721923828125 0.44115734100341797\n",
            "tcost icost 0.0268402099609375 0.42218926548957825\n",
            "tcost icost 0.0268402099609375 0.4231579005718231\n",
            "search 0.449951171875\n",
            "tcost icost 0.0268402099609375 0.41485726833343506\n",
            "tcost icost 0.0268402099609375 0.4137246906757355\n",
            "tcost icost 0.0268402099609375 0.4144119620323181\n",
            "search 0.441162109375\n",
            "tcost icost 0.0286865234375 0.4205481708049774\n",
            "tcost icost 0.0268707275390625 0.41237887740135193\n",
            "tcost icost 0.0268707275390625 0.4121110439300537\n",
            "search 0.43896484375\n",
            "tcost icost 0.02789306640625 0.4361945390701294\n",
            "tcost icost 0.02789306640625 0.43427574634552\n",
            "tcost icost 0.02789306640625 0.43427574634552\n",
            "search 0.462158203125\n",
            "tcost icost 0.0294036865234375 0.42587897181510925\n",
            "tcost icost 0.02691650390625 0.40909749269485474\n",
            "tcost icost 0.02691650390625 0.40909749269485474\n",
            "search 0.43603515625\n",
            "tcost icost 0.02935791015625 0.40917709469795227\n",
            "tcost icost 0.02874755859375 0.39004290103912354\n",
            "tcost icost 0.02874755859375 0.3891143500804901\n",
            "search 0.41796875\n",
            "tcost icost 0.032379150390625 0.35202813148498535\n",
            "tcost icost 0.032379150390625 0.353304386138916\n",
            "tcost icost 0.032379150390625 0.353304386138916\n",
            "search 0.3857421875\n",
            "tcost icost 0.034149169921875 0.3369433581829071\n",
            "tcost icost 0.034637451171875 0.33193954825401306\n",
            "tcost icost 0.034637451171875 0.33193954825401306\n",
            "search 0.36669921875\n",
            "tcost icost 0.032623291015625 0.3677365183830261\n",
            "tcost icost 0.03216552734375 0.3550182580947876\n",
            "tcost icost 0.03216552734375 0.3550182580947876\n",
            "search 0.38720703125\n",
            "tcost icost 0.0290374755859375 0.4088003635406494\n",
            "tcost icost 0.027679443359375 0.39821216464042664\n",
            "tcost icost 0.027679443359375 0.398372083902359\n",
            "search 0.426025390625\n",
            "tcost icost 0.032135009765625 0.418865829706192\n",
            "tcost icost 0.0285797119140625 0.39038652181625366\n",
            "tcost icost 0.0285797119140625 0.39038652181625366\n",
            "search 0.4189453125\n",
            "tcost icost 0.0278778076171875 0.401947557926178\n",
            "tcost icost 0.0281829833984375 0.3934193551540375\n",
            "tcost icost 0.0281829833984375 0.3941771388053894\n",
            "search 0.42236328125\n",
            "tcost icost 0.0271148681640625 0.41741466522216797\n",
            "tcost icost 0.0269012451171875 0.409969300031662\n",
            "tcost icost 0.0269012451171875 0.4099693298339844\n",
            "search 0.436767578125\n",
            "tcost icost 0.0321044921875 0.4237389862537384\n",
            "tcost icost 0.0278472900390625 0.395813912153244\n",
            "tcost icost 0.0278472900390625 0.39649879932403564\n",
            "search 0.42431640625\n",
            "tcost icost 0.02862548828125 0.4284020662307739\n",
            "tcost icost 0.02655029296875 0.4193286597728729\n",
            "tcost icost 0.02655029296875 0.4193286597728729\n",
            "search 0.446044921875\n",
            "tcost icost 0.0308685302734375 0.40824413299560547\n",
            "tcost icost 0.02911376953125 0.3750596046447754\n",
            "tcost icost 0.02911376953125 0.3750596344470978\n",
            "search 0.404052734375\n",
            "tcost icost 0.033111572265625 0.4332282245159149\n",
            "tcost icost 0.0268096923828125 0.4146783947944641\n",
            "tcost icost 0.0268096923828125 0.4146783947944641\n",
            "search 0.441650390625\n",
            "tcost icost 0.028656005859375 0.4298313856124878\n",
            "tcost icost 0.026519775390625 0.42097797989845276\n",
            "tcost icost 0.026519775390625 0.42097797989845276\n",
            "search 0.447509765625\n",
            "tcost icost 0.0271759033203125 0.4274594187736511\n",
            "tcost icost 0.0271759033203125 0.427908718585968\n",
            "tcost icost 0.0271759033203125 0.4286127984523773\n",
            "search 0.455810546875\n",
            "tcost icost 0.030029296875 0.37943556904792786\n",
            "tcost icost 0.030487060546875 0.3710859417915344\n",
            "tcost icost 0.030487060546875 0.3710859417915344\n",
            "search 0.401611328125\n",
            "tcost icost 0.0291900634765625 0.41721197962760925\n",
            "tcost icost 0.027801513671875 0.39796632528305054\n",
            "tcost icost 0.027801513671875 0.39796632528305054\n",
            "search 0.42578125\n",
            "tcost icost 0.02960205078125 0.4304647743701935\n",
            "tcost icost 0.0268096923828125 0.41429707407951355\n",
            "tcost icost 0.0268096923828125 0.4154738485813141\n",
            "search 0.4423828125\n",
            "tcost icost 0.0270233154296875 0.43050268292427063\n",
            "tcost icost 0.0270233154296875 0.43045350909233093\n",
            "tcost icost 0.0270233154296875 0.4304267168045044\n",
            "search 0.45751953125\n",
            "tcost icost 0.0255889892578125 0.4262583553791046\n",
            "tcost icost 0.02117919921875 0.3985077142715454\n",
            "tcost icost 0.02117919921875 0.3985077142715454\n",
            "search 0.419677734375\n",
            "tcost icost 0.028411865234375 0.43889763951301575\n",
            "tcost icost 0.027862548828125 0.43372130393981934\n",
            "tcost icost 0.027862548828125 0.4333045780658722\n",
            "search 0.461181640625\n",
            "tcost icost 0.0287017822265625 0.4420030415058136\n",
            "tcost icost 0.02691650390625 0.43958520889282227\n",
            "tcost icost 0.02691650390625 0.4402221441268921\n",
            "search 0.467041015625\n",
            "tcost icost 0.033782958984375 0.43896427750587463\n",
            "tcost icost 0.0269317626953125 0.4215547442436218\n",
            "tcost icost 0.0269317626953125 0.4207003712654114\n",
            "search 0.447509765625\n",
            "tcost icost 0.033905029296875 0.4376082122325897\n",
            "tcost icost 0.027130126953125 0.42503029108047485\n",
            "tcost icost 0.027130126953125 0.42599934339523315\n",
            "search 0.453125\n",
            "tcost icost 0.027923583984375 0.43340864777565\n",
            "tcost icost 0.027923583984375 0.43340864777565\n",
            "tcost icost 0.027923583984375 0.43340864777565\n",
            "search 0.461181640625\n",
            "tcost icost 0.027313232421875 0.4103828966617584\n",
            "tcost icost 0.0271759033203125 0.40295934677124023\n",
            "tcost icost 0.0271759033203125 0.40295934677124023\n",
            "search 0.43017578125\n",
            "ded\n",
            "time\n",
            "25 #### train ####\n",
            "repr, std, cov, conv, closs 0.03719896078109741 0.32275390625 0.3635871112346649 0.061667777597904205 0.01491750217974186\n",
            "51.44509206228141 7.074571697095573 1.0\n",
            "repr, std, cov, conv, closs 0.039232153445482254 0.328857421875 0.3341255784034729 0.06820961833000183 0.0002890495234169066\n",
            "51.496537154343685 7.046344014779945 1.0\n",
            "repr, std, cov, conv, closs 0.05654895678162575 0.326904296875 0.3540871739387512 0.052332233637571335 0.00041948462603613734\n",
            "52.37901695554674 7.124242512838644 1.0\n",
            "repr, std, cov, conv, closs 0.057082369923591614 0.33056640625 0.3100006878376007 0.05332323908805847 0.01535297092050314\n",
            "52.588847507039084 7.231857447271422 1.0\n",
            "repr, std, cov, conv, closs 0.04888679087162018 0.332275390625 0.3057239353656769 0.06843826174736023 8.118060941342264e-05\n",
            "53.3832260485004 7.444544045199088 1.0\n",
            "repr, std, cov, conv, closs 0.05072425678372383 0.335205078125 0.2688084542751312 0.06954894959926605 0.030348248779773712\n",
            "53.758031548869326 7.564554277733999 1.0\n",
            "repr, std, cov, conv, closs 0.0553206242620945 0.33154296875 0.31841957569122314 0.06346669793128967 0.015647366642951965\n",
            "54.0273598247783 7.725008580250108 1.0\n",
            "repr, std, cov, conv, closs 0.04484137147665024 0.330322265625 0.29501765966415405 0.06155058369040489 0.011211706325411797\n",
            "54.46109449876867 7.818220237111288 1.0\n",
            "repr, std, cov, conv, closs 0.052737802267074585 0.326904296875 0.3352513313293457 0.05769335478544235 0.01687300205230713\n",
            "54.679265861229545 7.717291288961148 1.0\n",
            "repr, std, cov, conv, closs 0.03287776559591293 0.330078125 0.3263319134712219 0.06453707814216614 0.013229763135313988\n",
            "54.13546857178767 7.648181675925595 1.0\n",
            "repr, std, cov, conv, closs 0.03717814385890961 0.327392578125 0.3034754991531372 0.07683862745761871 0.0005835203337483108\n",
            "52.90517047735999 7.496812449760183 1.0\n",
            "repr, std, cov, conv, closs 0.032215408980846405 0.3271484375 0.3041626214981079 0.06518547236919403 0.02902791276574135\n",
            "52.48382736847478 7.429677260999828 1.0\n",
            "repr, std, cov, conv, closs 0.030257614329457283 0.325439453125 0.3159688711166382 0.0586082860827446 0.053809717297554016\n",
            "51.90995424203243 7.2463283940234104 1.0\n",
            "repr, std, cov, conv, closs 0.04125692695379257 0.323486328125 0.3472394645214081 0.06605811417102814 0.00015511378296650946\n",
            "51.85809614588655 7.217415399057909 1.0\n",
            "repr, std, cov, conv, closs 0.03908157721161842 0.327392578125 0.32232666015625 0.06180199235677719 0.0013437398010864854\n",
            "51.2398251178488 7.074571697095573 1.0\n",
            "repr, std, cov, conv, closs 0.044704537838697433 0.326171875 0.3579849600791931 0.06404401361942291 0.04722925275564194\n",
            "51.29106494296664 7.032272437632244 1.0\n",
            "repr, std, cov, conv, closs 0.044365569949150085 0.326416015625 0.3058791756629944 0.06372778117656708 0.0007624795543961227\n",
            "51.7028324882216 6.948431135452057 1.0\n",
            "repr, std, cov, conv, closs 0.0505751371383667 0.3310546875 0.31975841522216797 0.06171226501464844 0.0003652437881100923\n",
            "52.065839886531194 6.976266578381347 1.0\n",
            "repr, std, cov, conv, closs 0.04121493548154831 0.3291015625 0.30762094259262085 0.06687009334564209 0.0003542298509273678\n",
            "52.37901695554674 7.124242512838644 1.0\n",
            "repr, std, cov, conv, closs 0.04774376377463341 0.33447265625 0.28775039315223694 0.06753773987293243 0.01479227002710104\n",
            "52.95807564783734 7.231857447271422 1.0\n",
            "repr, std, cov, conv, closs 0.04617458954453468 0.3330078125 0.264184832572937 0.05378212034702301 0.01967048831284046\n",
            "53.9194669713686 7.414840165828007 1.0\n",
            "repr, std, cov, conv, closs 0.0381304994225502 0.33154296875 0.3244030475616455 0.06140640377998352 0.01515603344887495\n",
            "54.13546857178767 7.5118135714721515 1.0\n",
            "repr, std, cov, conv, closs 0.043008118867874146 0.3349609375 0.2696305215358734 0.06568561494350433 0.00042606814531609416\n",
            "54.73394512709077 7.725008580250108 1.0\n",
            "repr, std, cov, conv, closs 0.03855802118778229 0.33203125 0.3016214370727539 0.06632550060749054 0.012257056310772896\n",
            "54.46109449876867 7.678820322319725 1.0\n",
            "repr, std, cov, conv, closs 0.0465913750231266 0.32763671875 0.32076597213745117 0.06388585269451141 0.015660325065255165\n",
            "53.59707946563695 7.564554277733999 1.0\n",
            "repr, std, cov, conv, closs 0.0477459616959095 0.3291015625 0.28759312629699707 0.06151595711708069 0.031417373567819595\n",
            "53.59707946563695 7.5118135714721515 1.0\n",
            "repr, std, cov, conv, closs 0.032723743468523026 0.329345703125 0.30659955739974976 0.05891150236129761 0.02748795598745346\n",
            "53.17022591076781 7.437106938260827 1.0\n",
            "repr, std, cov, conv, closs 0.03775550797581673 0.323974609375 0.36008375883102417 0.0634218081831932 0.009357710368931293\n",
            "52.85231815920079 7.385254805528658 1.0\n",
            "repr, std, cov, conv, closs 0.03369978815317154 0.33056640625 0.2847849130630493 0.05799204111099243 0.0006289373268373311\n",
            "52.43139597250228 7.253574722417433 1.0\n",
            "repr, std, cov, conv, closs 0.0380261167883873 0.326416015625 0.35864758491516113 0.067591592669487 0.0009059166768565774\n",
            "51.90995424203243 7.16709497414512 1.0\n",
            "repr, std, cov, conv, closs 0.04848919436335564 0.329833984375 0.2776573896408081 0.0677393227815628 0.016051044687628746\n",
            "52.065839886531194 7.09581664297652 1.0\n",
            "repr, std, cov, conv, closs 0.037171974778175354 0.330810546875 0.29333215951919556 0.06404707580804825 0.03576525300741196\n",
            "52.17002363214413 7.1171253874511935 1.0\n",
            "repr, std, cov, conv, closs 0.04035623371601105 0.324951171875 0.31674906611442566 0.06177554652094841 0.013801331631839275\n",
            "52.37901695554674 7.09581664297652 1.0\n",
            "repr, std, cov, conv, closs 0.04533596709370613 0.33056640625 0.2771138548851013 0.06657871603965759 0.031053798273205757\n",
            "52.79951864056024 7.2030021916723745 1.0\n",
            "repr, std, cov, conv, closs 0.039616893976926804 0.331787109375 0.2822933793067932 0.07302738726139069 0.017765473574399948\n",
            "53.01103372348517 7.355787491407577 1.0\n",
            "repr, std, cov, conv, closs 0.04080259054899216 0.333740234375 0.2732223868370056 0.07107704132795334 0.0005088144680485129\n",
            "53.329896152348056 7.45944057783353 1.0\n",
            "tcost icost 0.0423583984375 0.4489858150482178\n",
            "tcost icost 0.035675048828125 0.4460476040840149\n",
            "tcost icost 0.035675048828125 0.44629737734794617\n",
            "search 0.48193359375\n",
            "tcost icost 0.04803466796875 0.44997748732566833\n",
            "tcost icost 0.037689208984375 0.4473848342895508\n",
            "tcost icost 0.037689208984375 0.44743502140045166\n",
            "search 0.485107421875\n",
            "tcost icost 0.037689208984375 0.4501498341560364\n",
            "tcost icost 0.044219970703125 0.4551199674606323\n",
            "tcost icost 0.044219970703125 0.4523368179798126\n",
            "search 0.49658203125\n",
            "tcost icost 0.038726806640625 0.45142826437950134\n",
            "tcost icost 0.037689208984375 0.45152756571769714\n",
            "tcost icost 0.037689208984375 0.4501498341560364\n",
            "search 0.48779296875\n",
            "tcost icost 0.044189453125 0.4524073600769043\n",
            "tcost icost 0.03765869140625 0.45018282532691956\n",
            "tcost icost 0.03765869140625 0.45018282532691956\n",
            "search 0.48779296875\n",
            "tcost icost 0.03887939453125 0.45130038261413574\n",
            "tcost icost 0.037872314453125 0.45006707310676575\n",
            "tcost icost 0.037872314453125 0.45006707310676575\n",
            "search 0.48779296875\n",
            "tcost icost 0.03912353515625 0.44994300603866577\n",
            "tcost icost 0.035675048828125 0.4489143490791321\n",
            "tcost icost 0.035675048828125 0.44961756467819214\n",
            "search 0.4853515625\n",
            "tcost icost 0.047882080078125 0.4546225965023041\n",
            "tcost icost 0.03570556640625 0.4485481381416321\n",
            "tcost icost 0.03570556640625 0.4485481381416321\n",
            "search 0.484130859375\n",
            "tcost icost 0.046295166015625 0.45257675647735596\n",
            "tcost icost 0.03570556640625 0.4485480487346649\n",
            "tcost icost 0.03570556640625 0.4485480487346649\n",
            "search 0.484130859375\n",
            "tcost icost 0.038665771484375 0.4526981711387634\n",
            "tcost icost 0.035675048828125 0.4486643373966217\n",
            "tcost icost 0.035675048828125 0.4486643373966217\n",
            "search 0.484375\n",
            "tcost icost 0.038726806640625 0.4500288665294647\n",
            "tcost icost 0.03564453125 0.44852039217948914\n",
            "tcost icost 0.03564453125 0.44867053627967834\n",
            "search 0.484375\n",
            "tcost icost 0.0423583984375 0.4513680338859558\n",
            "tcost icost 0.035675048828125 0.4486415982246399\n",
            "tcost icost 0.035675048828125 0.4486415982246399\n",
            "search 0.484375\n",
            "tcost icost 0.048065185546875 0.4524324834346771\n",
            "tcost icost 0.0377197265625 0.44987952709198\n",
            "tcost icost 0.0377197265625 0.45009374618530273\n",
            "search 0.48779296875\n",
            "tcost icost 0.04833984375 0.4520260691642761\n",
            "tcost icost 0.038055419921875 0.4499339163303375\n",
            "tcost icost 0.038055419921875 0.45250779390335083\n",
            "search 0.490478515625\n",
            "tcost icost 0.037994384765625 0.4499724805355072\n",
            "tcost icost 0.044281005859375 0.45202797651290894\n",
            "tcost icost 0.044281005859375 0.45202797651290894\n",
            "search 0.496337890625\n",
            "tcost icost 0.042877197265625 0.45116347074508667\n",
            "tcost icost 0.035675048828125 0.44891512393951416\n",
            "tcost icost 0.035675048828125 0.44891512393951416\n",
            "search 0.484619140625\n",
            "tcost icost 0.042816162109375 0.4511996805667877\n",
            "tcost icost 0.03564453125 0.44886264204978943\n",
            "tcost icost 0.03564453125 0.44886264204978943\n",
            "search 0.484619140625\n",
            "tcost icost 0.0462646484375 0.45251789689064026\n",
            "tcost icost 0.035736083984375 0.4484668970108032\n",
            "tcost icost 0.035736083984375 0.4484668970108032\n",
            "search 0.484130859375\n",
            "tcost icost 0.0421142578125 0.4542884826660156\n",
            "tcost icost 0.0357666015625 0.45087552070617676\n",
            "tcost icost 0.0357666015625 0.45308083295822144\n",
            "search 0.48876953125\n",
            "tcost icost 0.0377197265625 0.45264720916748047\n",
            "tcost icost 0.04425048828125 0.45789214968681335\n",
            "tcost icost 0.04425048828125 0.4547986388206482\n",
            "search 0.4990234375\n",
            "tcost icost 0.038909912109375 0.45383232831954956\n",
            "tcost icost 0.037933349609375 0.45524492859840393\n",
            "tcost icost 0.037933349609375 0.45258277654647827\n",
            "search 0.490478515625\n",
            "tcost icost 0.03912353515625 0.45246410369873047\n",
            "tcost icost 0.035675048828125 0.4532092213630676\n",
            "tcost icost 0.035675048828125 0.4539870321750641\n",
            "search 0.48974609375\n",
            "tcost icost 0.044281005859375 0.45481178164482117\n",
            "tcost icost 0.037872314453125 0.4526232182979584\n",
            "tcost icost 0.037872314453125 0.4526232182979584\n",
            "search 0.490478515625\n",
            "tcost icost 0.042510986328125 0.45396584272384644\n",
            "tcost icost 0.03564453125 0.45121151208877563\n",
            "tcost icost 0.03564453125 0.45121151208877563\n",
            "search 0.48681640625\n",
            "tcost icost 0.042266845703125 0.4541071057319641\n",
            "tcost icost 0.03570556640625 0.4510590434074402\n",
            "tcost icost 0.03570556640625 0.45382076501846313\n",
            "search 0.489501953125\n",
            "tcost icost 0.03997802734375 0.45457908511161804\n",
            "tcost icost 0.035675048828125 0.4523807764053345\n",
            "tcost icost 0.035675048828125 0.4523807764053345\n",
            "search 0.488037109375\n",
            "tcost icost 0.038787841796875 0.4565485119819641\n",
            "tcost icost 0.03564453125 0.4524441361427307\n",
            "tcost icost 0.03564453125 0.4524441361427307\n",
            "search 0.488037109375\n",
            "tcost icost 0.038482666015625 0.45394086837768555\n",
            "tcost icost 0.03570556640625 0.4522218704223633\n",
            "tcost icost 0.03570556640625 0.4522218704223633\n",
            "search 0.48779296875\n",
            "tcost icost 0.0426025390625 0.45513319969177246\n",
            "tcost icost 0.035614013671875 0.452261745929718\n",
            "tcost icost 0.035614013671875 0.4551066756248474\n",
            "search 0.49072265625\n",
            "tcost icost 0.040496826171875 0.45459985733032227\n",
            "tcost icost 0.035736083984375 0.4527367055416107\n",
            "tcost icost 0.035736083984375 0.4527367055416107\n",
            "search 0.48828125\n",
            "tcost icost 0.03594970703125 0.45280128717422485\n",
            "tcost icost 0.03594970703125 0.4526934325695038\n",
            "tcost icost 0.03594970703125 0.45558691024780273\n",
            "search 0.491455078125\n",
            "tcost icost 0.0357666015625 0.4520944058895111\n",
            "tcost icost 0.0357666015625 0.4520944058895111\n",
            "tcost icost 0.0357666015625 0.4520944058895111\n",
            "search 0.48779296875\n",
            "tcost icost 0.042205810546875 0.455583781003952\n",
            "tcost icost 0.035736083984375 0.45219001173973083\n",
            "tcost icost 0.035736083984375 0.4534958302974701\n",
            "search 0.4892578125\n",
            "tcost icost 0.03948974609375 0.45448121428489685\n",
            "tcost icost 0.03582763671875 0.4520249366760254\n",
            "tcost icost 0.03582763671875 0.4520249366760254\n",
            "search 0.48779296875\n",
            "tcost icost 0.038177490234375 0.4556875228881836\n",
            "tcost icost 0.037384033203125 0.4540131986141205\n",
            "tcost icost 0.037384033203125 0.4543602764606476\n",
            "search 0.49169921875\n",
            "tcost icost 0.039398193359375 0.45435482263565063\n",
            "tcost icost 0.03594970703125 0.4518539011478424\n",
            "tcost icost 0.03594970703125 0.4518539011478424\n",
            "search 0.48779296875\n",
            "tcost icost 0.03802490234375 0.4557788074016571\n",
            "tcost icost 0.037261962890625 0.45450514554977417\n",
            "tcost icost 0.037261962890625 0.45450514554977417\n",
            "search 0.491943359375\n",
            "tcost icost 0.039520263671875 0.4552513659000397\n",
            "tcost icost 0.035797119140625 0.45277321338653564\n",
            "tcost icost 0.035797119140625 0.45277321338653564\n",
            "search 0.48876953125\n",
            "tcost icost 0.047882080078125 0.45751476287841797\n",
            "tcost icost 0.03759765625 0.4561891555786133\n",
            "tcost icost 0.03759765625 0.45480820536613464\n",
            "search 0.492431640625\n",
            "tcost icost 0.039794921875 0.4553256034851074\n",
            "tcost icost 0.035675048828125 0.452976793050766\n",
            "tcost icost 0.035675048828125 0.452976793050766\n",
            "search 0.488525390625\n",
            "tcost icost 0.04241943359375 0.456100195646286\n",
            "tcost icost 0.03564453125 0.45314109325408936\n",
            "tcost icost 0.03564453125 0.45314109325408936\n",
            "search 0.48876953125\n",
            "tcost icost 0.035614013671875 0.4528034031391144\n",
            "tcost icost 0.035614013671875 0.45317378640174866\n",
            "tcost icost 0.035614013671875 0.45317378640174866\n",
            "search 0.48876953125\n",
            "tcost icost 0.048126220703125 0.4573221802711487\n",
            "tcost icost 0.037750244140625 0.4546830654144287\n",
            "tcost icost 0.037750244140625 0.4546830654144287\n",
            "search 0.492431640625\n",
            "tcost icost 0.039581298828125 0.4575527012348175\n",
            "tcost icost 0.035736083984375 0.4528449773788452\n",
            "tcost icost 0.035736083984375 0.4528449773788452\n",
            "search 0.488525390625\n",
            "tcost icost 0.042266845703125 0.4562966823577881\n",
            "tcost icost 0.03570556640625 0.4529767632484436\n",
            "tcost icost 0.03570556640625 0.4529767334461212\n",
            "search 0.488525390625\n",
            "tcost icost 0.038360595703125 0.4543093144893646\n",
            "tcost icost 0.044342041015625 0.45656898617744446\n",
            "tcost icost 0.044342041015625 0.45656898617744446\n",
            "search 0.5009765625\n",
            "tcost icost 0.038299560546875 0.45437753200531006\n",
            "tcost icost 0.0443115234375 0.45665088295936584\n",
            "tcost icost 0.0443115234375 0.45665088295936584\n",
            "search 0.5009765625\n",
            "tcost icost 0.043304443359375 0.4553341865539551\n",
            "tcost icost 0.038421630859375 0.4542350769042969\n",
            "tcost icost 0.038421630859375 0.4542350769042969\n",
            "search 0.49267578125\n",
            "tcost icost 0.0369873046875 0.4536496102809906\n",
            "tcost icost 0.0369873046875 0.4536495804786682\n",
            "tcost icost 0.0369873046875 0.4536495804786682\n",
            "search 0.49072265625\n",
            "tcost icost 0.039398193359375 0.45480427145957947\n",
            "tcost icost 0.0386962890625 0.45553058385849\n",
            "tcost icost 0.0386962890625 0.4536890983581543\n",
            "search 0.4921875\n",
            "tcost icost 0.0394287109375 0.4590426981449127\n",
            "tcost icost 0.035736083984375 0.45622652769088745\n",
            "tcost icost 0.035736083984375 0.45622649788856506\n",
            "search 0.491943359375\n",
            "tcost icost 0.03948974609375 0.45797228813171387\n",
            "tcost icost 0.035797119140625 0.4552771747112274\n",
            "tcost icost 0.035797119140625 0.4552771747112274\n",
            "search 0.4912109375\n",
            "tcost icost 0.04241943359375 0.4589526951313019\n",
            "tcost icost 0.035675048828125 0.4556911587715149\n",
            "tcost icost 0.035675048828125 0.45569121837615967\n",
            "search 0.491455078125\n",
            "tcost icost 0.03875732421875 0.45895034074783325\n",
            "tcost icost 0.03765869140625 0.45745453238487244\n",
            "tcost icost 0.03765869140625 0.45745450258255005\n",
            "search 0.4951171875\n",
            "tcost icost 0.042144775390625 0.45915719866752625\n",
            "tcost icost 0.035736083984375 0.4554499685764313\n",
            "tcost icost 0.035736083984375 0.4554499685764313\n",
            "search 0.4912109375\n",
            "tcost icost 0.038421630859375 0.45703911781311035\n",
            "tcost icost 0.0357666015625 0.455075740814209\n",
            "tcost icost 0.0357666015625 0.455075740814209\n",
            "search 0.49072265625\n",
            "tcost icost 0.047607421875 0.46026626229286194\n",
            "tcost icost 0.0357666015625 0.4549359679222107\n",
            "tcost icost 0.0357666015625 0.4549359679222107\n",
            "search 0.49072265625\n",
            "tcost icost 0.042205810546875 0.4587514400482178\n",
            "tcost icost 0.03570556640625 0.4551454782485962\n",
            "tcost icost 0.03570556640625 0.4551454782485962\n",
            "search 0.49072265625\n",
            "tcost icost 0.03997802734375 0.4576644003391266\n",
            "tcost icost 0.035675048828125 0.45781999826431274\n",
            "tcost icost 0.035675048828125 0.45531973242759705\n",
            "search 0.490966796875\n",
            "tcost icost 0.0435791015625 0.4597717523574829\n",
            "tcost icost 0.035980224609375 0.4557744562625885\n",
            "tcost icost 0.035980224609375 0.4557744562625885\n",
            "search 0.49169921875\n",
            "tcost icost 0.03839111328125 0.45715051889419556\n",
            "tcost icost 0.035888671875 0.4548887014389038\n",
            "tcost icost 0.035888671875 0.4548887014389038\n",
            "search 0.49072265625\n",
            "tcost icost 0.039398193359375 0.45743057131767273\n",
            "tcost icost 0.035980224609375 0.45470744371414185\n",
            "tcost icost 0.035980224609375 0.45470744371414185\n",
            "search 0.490478515625\n",
            "tcost icost 0.042449951171875 0.45855844020843506\n",
            "tcost icost 0.035675048828125 0.4553544521331787\n",
            "tcost icost 0.035675048828125 0.4553544521331787\n",
            "search 0.490966796875\n",
            "tcost icost 0.0472412109375 0.4596365690231323\n",
            "tcost icost 0.035675048828125 0.4556473195552826\n",
            "tcost icost 0.035675048828125 0.4556473195552826\n",
            "search 0.4912109375\n",
            "tcost icost 0.039276123046875 0.45754215121269226\n",
            "tcost icost 0.038543701171875 0.4562791883945465\n",
            "tcost icost 0.038543701171875 0.4562791883945465\n",
            "search 0.494873046875\n",
            "tcost icost 0.03900146484375 0.45828184485435486\n",
            "tcost icost 0.038055419921875 0.4568708837032318\n",
            "tcost icost 0.038055419921875 0.4568708837032318\n",
            "search 0.494873046875\n",
            "tcost icost 0.040924072265625 0.459224134683609\n",
            "tcost icost 0.037017822265625 0.453348308801651\n",
            "tcost icost 0.037017822265625 0.453348308801651\n",
            "search 0.490478515625\n",
            "tcost icost 0.04913330078125 0.45816871523857117\n",
            "tcost icost 0.038726806640625 0.4558323621749878\n",
            "tcost icost 0.038726806640625 0.4559367895126343\n",
            "search 0.494873046875\n",
            "tcost icost 0.041656494140625 0.4579421281814575\n",
            "tcost icost 0.036285400390625 0.45581918954849243\n",
            "tcost icost 0.036285400390625 0.4558192193508148\n",
            "search 0.4921875\n",
            "tcost icost 0.0418701171875 0.4590192437171936\n",
            "tcost icost 0.035888671875 0.45746415853500366\n",
            "tcost icost 0.035888671875 0.4548887014389038\n",
            "search 0.49072265625\n",
            "tcost icost 0.03790283203125 0.45911121368408203\n",
            "tcost icost 0.037200927734375 0.45779144763946533\n",
            "tcost icost 0.037200927734375 0.45779144763946533\n",
            "search 0.494873046875\n",
            "tcost icost 0.038330078125 0.4606023132801056\n",
            "tcost icost 0.0361328125 0.45572569966316223\n",
            "tcost icost 0.0361328125 0.45572569966316223\n",
            "search 0.491943359375\n",
            "tcost icost 0.039337158203125 0.45876243710517883\n",
            "tcost icost 0.036041259765625 0.45594993233680725\n",
            "tcost icost 0.036041259765625 0.45594993233680725\n",
            "search 0.4921875\n",
            "tcost icost 0.0401611328125 0.4576779007911682\n",
            "tcost icost 0.036041259765625 0.4571266174316406\n",
            "tcost icost 0.036041259765625 0.4571266174316406\n",
            "search 0.4931640625\n",
            "tcost icost 0.039215087890625 0.4592745304107666\n",
            "tcost icost 0.0384521484375 0.45760107040405273\n",
            "tcost icost 0.0384521484375 0.45790669322013855\n",
            "search 0.49658203125\n",
            "tcost icost 0.041290283203125 0.4592955708503723\n",
            "tcost icost 0.036041259765625 0.4597398638725281\n",
            "tcost icost 0.036041259765625 0.457126647233963\n",
            "search 0.4931640625\n",
            "tcost icost 0.04156494140625 0.4569178819656372\n",
            "tcost icost 0.036865234375 0.4572485089302063\n",
            "tcost icost 0.036865234375 0.4572485089302063\n",
            "search 0.494140625\n",
            "tcost icost 0.04168701171875 0.4605638086795807\n",
            "tcost icost 0.0360107421875 0.4560288190841675\n",
            "tcost icost 0.0360107421875 0.45589765906333923\n",
            "search 0.49169921875\n",
            "tcost icost 0.0462646484375 0.45649710297584534\n",
            "tcost icost 0.040557861328125 0.4552549123764038\n",
            "tcost icost 0.040557861328125 0.4552549123764038\n",
            "search 0.495849609375\n",
            "tcost icost 0.04345703125 0.45940113067626953\n",
            "tcost icost 0.035919189453125 0.45711106061935425\n",
            "tcost icost 0.035919189453125 0.45711106061935425\n",
            "search 0.492919921875\n",
            "tcost icost 0.03643798828125 0.45514801144599915\n",
            "tcost icost 0.03643798828125 0.45514801144599915\n",
            "tcost icost 0.03643798828125 0.45514801144599915\n",
            "search 0.491455078125\n",
            "tcost icost 0.03814697265625 0.4614108204841614\n",
            "tcost icost 0.0369873046875 0.45975354313850403\n",
            "tcost icost 0.0369873046875 0.46169134974479675\n",
            "search 0.49853515625\n",
            "tcost icost 0.0423583984375 0.4600278437137604\n",
            "tcost icost 0.03564453125 0.4564780294895172\n",
            "tcost icost 0.03564453125 0.4564780294895172\n",
            "search 0.4921875\n",
            "tcost icost 0.0321044921875 0.4494006931781769\n",
            "tcost icost 0.0298309326171875 0.4413079023361206\n",
            "tcost icost 0.0298309326171875 0.4413079023361206\n",
            "search 0.47119140625\n",
            "tcost icost 0.0283355712890625 0.4304562211036682\n",
            "tcost icost 0.029998779296875 0.42821407318115234\n",
            "tcost icost 0.029998779296875 0.42665380239486694\n",
            "search 0.456787109375\n",
            "ded\n",
            "time\n",
            "26 #### train ####\n",
            "repr, std, cov, conv, closs 0.03200172632932663 0.331787109375 0.30713582038879395 0.056596945971250534 0.03200491890311241\n",
            "53.490045883823434 7.526844710428666 1.0\n",
            "repr, std, cov, conv, closs 0.03910031169652939 0.33349609375 0.27553847432136536 0.06932909786701202 0.00016184660489670932\n",
            "53.543535929707254 7.632908226564242 1.0\n",
            "repr, std, cov, conv, closs 0.04930201545357704 0.32373046875 0.36684805154800415 0.049727004021406174 0.0003885492333211005\n",
            "53.490045883823434 7.587270641794586 1.0\n",
            "repr, std, cov, conv, closs 0.03699767589569092 0.332275390625 0.3279382288455963 0.0499543622136116 0.01635384000837803\n",
            "53.17022591076781 7.496812449760183 1.0\n",
            "repr, std, cov, conv, closs 0.03647317737340927 0.330322265625 0.29014068841934204 0.05092880129814148 0.014905708841979504\n",
            "53.22339613667857 7.466900018411363 1.0\n",
            "repr, std, cov, conv, closs 0.03701052442193031 0.329345703125 0.3164323568344116 0.051004961133003235 0.029056282714009285\n",
            "53.11710880196585 7.392640060334186 1.0\n",
            "repr, std, cov, conv, closs 0.037732869386672974 0.330322265625 0.2963505983352661 0.06045518442988396 0.013417378067970276\n",
            "53.11710880196585 7.400032700394519 1.0\n",
            "repr, std, cov, conv, closs 0.043717823922634125 0.326171875 0.35624486207962036 0.06399565935134888 0.01622067391872406\n",
            "53.276619532815246 7.407432733094913 1.0\n",
            "repr, std, cov, conv, closs 0.04538331925868988 0.327880859375 0.3264322280883789 0.07376235723495483 0.02985302358865738\n",
            "53.06404475720865 7.348439052355222 1.0\n",
            "repr, std, cov, conv, closs 0.034615445882081985 0.3310546875 0.2918983995914459 0.07126548886299133 0.00035904059768654406\n",
            "52.90517047735999 7.392640060334186 1.0\n",
            "repr, std, cov, conv, closs 0.03803016245365143 0.329833984375 0.3000525236129761 0.05591902881860733 0.015035728923976421\n",
            "53.3832260485004 7.407432733094913 1.0\n",
            "repr, std, cov, conv, closs 0.04409293085336685 0.32470703125 0.34041154384613037 0.06074494868516922 0.05917617306113243\n",
            "53.490045883823434 7.422255005993835 1.0\n",
            "repr, std, cov, conv, closs 0.04116209223866463 0.32861328125 0.30489861965179443 0.06837095320224762 0.02292776294052601\n",
            "53.650676545102584 7.407432733094913 1.0\n",
            "repr, std, cov, conv, closs 0.03910187631845474 0.32861328125 0.30787843465805054 0.06096977740526199 0.0001395791332470253\n",
            "53.59707946563695 7.370506422177882 1.0\n",
            "repr, std, cov, conv, closs 0.035176441073417664 0.331787109375 0.30036550760269165 0.06371370702981949 0.016015859320759773\n",
            "53.9194669713686 7.45944057783353 1.0\n",
            "repr, std, cov, conv, closs 0.0431942455470562 0.334228515625 0.26296281814575195 0.07115429639816284 0.0004554504994302988\n",
            "54.18960404035945 7.556997280453546 1.0\n",
            "repr, std, cov, conv, closs 0.0431981198489666 0.330322265625 0.3090229034423828 0.05829288065433502 0.001053539919666946\n",
            "54.679265861229545 7.648181675925595 1.0\n",
            "repr, std, cov, conv, closs 0.050129834562540054 0.327392578125 0.30452287197113037 0.06997862458229065 0.0003517775039654225\n",
            "54.46109449876867 7.671149173146579 1.0\n",
            "repr, std, cov, conv, closs 0.04051203280687332 0.32568359375 0.3651488423347473 0.06794749200344086 0.01180794183164835\n",
            "54.0273598247783 7.504309262209943 1.0\n",
            "repr, std, cov, conv, closs 0.04453881084918976 0.326171875 0.3164326548576355 0.06807693839073181 0.010657435283064842\n",
            "53.650676545102584 7.392640060334186 1.0\n",
            "repr, std, cov, conv, closs 0.0365779846906662 0.32568359375 0.35500234365463257 0.061912901699543 0.0002424785343464464\n",
            "52.74677186869155 7.217415399057909 1.0\n",
            "repr, std, cov, conv, closs 0.047579988837242126 0.3251953125 0.3291189670562744 0.06606438755989075 0.00026932594482786953\n",
            "52.48382736847478 7.152782256849166 1.0\n",
            "repr, std, cov, conv, closs 0.04015538468956947 0.325927734375 0.3386293351650238 0.06274829804897308 0.045624665915966034\n",
            "52.53631119584325 7.046344014779945 1.0\n",
            "repr, std, cov, conv, closs 0.04050173610448837 0.325927734375 0.32005777955055237 0.05958493798971176 0.04375302046537399\n",
            "52.641436354546116 7.011217743736587 1.0\n",
            "repr, std, cov, conv, closs 0.043109502643346786 0.33056640625 0.2874649167060852 0.07033219933509827 0.005673195701092482\n",
            "53.70432722164768 7.145636620228938 1.0\n",
            "repr, std, cov, conv, closs 0.06122410297393799 0.327880859375 0.328188955783844 0.07288168370723724 0.014658160507678986\n",
            "53.9194669713686 7.210205193864046 1.0\n",
            "repr, std, cov, conv, closs 0.03439777344465256 0.335693359375 0.25830334424972534 0.06577104330062866 0.015503374859690666\n",
            "54.352335475482235 7.2972051195531105 1.0\n",
            "repr, std, cov, conv, closs 0.05099920183420181 0.326904296875 0.33185577392578125 0.06669499725103378 0.014777084812521935\n",
            "54.57007114886069 7.363143278898984 1.0\n",
            "repr, std, cov, conv, closs 0.03501197695732117 0.33203125 0.29078516364097595 0.07120893895626068 0.012908156029880047\n",
            "54.78867907221785 7.481841285348203 1.0\n",
            "repr, std, cov, conv, closs 0.0517377033829689 0.33154296875 0.3299311399459839 0.06479552388191223 0.0003534192219376564\n",
            "54.95320953026038 7.496812449760183 1.0\n",
            "repr, std, cov, conv, closs 0.06089905649423599 0.32373046875 0.2955361008644104 0.06545781344175339 0.018488651141524315\n",
            "55.339037939659136 7.6024527703488145 1.0\n",
            "repr, std, cov, conv, closs 0.05015547573566437 0.330322265625 0.27715471386909485 0.06299184262752533 0.029113559052348137\n",
            "55.39437697759879 7.6176652783422805 1.0\n",
            "repr, std, cov, conv, closs 0.053272999823093414 0.322998046875 0.37999430298805237 0.05814730376005173 0.06262264400720596\n",
            "55.063170902530416 7.400032700394519 1.0\n",
            "repr, std, cov, conv, closs 0.04818040877580643 0.327880859375 0.3091181218624115 0.06749056279659271 0.010939383879303932\n",
            "55.339037939659136 7.341097954400822 1.0\n",
            "repr, std, cov, conv, closs 0.06734821200370789 0.32568359375 0.33918341994285583 0.06445088982582092 0.012109196744859219\n",
            "54.62464122000954 7.16709497414512 1.0\n",
            "repr, std, cov, conv, closs 0.051041603088378906 0.3251953125 0.3457413911819458 0.06258659064769745 0.0020035274792462587\n",
            "54.13546857178767 7.053390358794724 1.0\n",
            "tcost icost 0.033111572265625 0.4414225220680237\n",
            "tcost icost 0.03082275390625 0.4283594489097595\n",
            "tcost icost 0.03082275390625 0.4283594489097595\n",
            "search 0.459228515625\n",
            "tcost icost 0.03436279296875 0.45116159319877625\n",
            "tcost icost 0.03082275390625 0.4263719916343689\n",
            "tcost icost 0.03082275390625 0.42819637060165405\n",
            "search 0.458984375\n",
            "tcost icost 0.036102294921875 0.43993517756462097\n",
            "tcost icost 0.03082275390625 0.42438453435897827\n",
            "tcost icost 0.03082275390625 0.42438453435897827\n",
            "search 0.455078125\n",
            "tcost icost 0.036102294921875 0.43727752566337585\n",
            "tcost icost 0.03082275390625 0.42239710688591003\n",
            "tcost icost 0.03082275390625 0.42239710688591003\n",
            "search 0.453125\n",
            "tcost icost 0.035430908203125 0.43913260102272034\n",
            "tcost icost 0.03082275390625 0.4204096496105194\n",
            "tcost icost 0.03082275390625 0.4204096496105194\n",
            "search 0.451171875\n",
            "tcost icost 0.033355712890625 0.439579039812088\n",
            "tcost icost 0.030059814453125 0.41350945830345154\n",
            "tcost icost 0.030059814453125 0.4151299297809601\n",
            "search 0.445068359375\n",
            "tcost icost 0.0323486328125 0.42381224036216736\n",
            "tcost icost 0.03021240234375 0.4130662977695465\n",
            "tcost icost 0.03021240234375 0.4130662977695465\n",
            "search 0.443359375\n",
            "tcost icost 0.034027099609375 0.43204593658447266\n",
            "tcost icost 0.030487060546875 0.4147879183292389\n",
            "tcost icost 0.030487060546875 0.4131126403808594\n",
            "search 0.443603515625\n",
            "tcost icost 0.03521728515625 0.4221499562263489\n",
            "tcost icost 0.0301055908203125 0.4086669683456421\n",
            "tcost icost 0.0301055908203125 0.40799084305763245\n",
            "search 0.43798828125\n",
            "tcost icost 0.034423828125 0.4264179468154907\n",
            "tcost icost 0.030853271484375 0.4106365442276001\n",
            "tcost icost 0.030853271484375 0.41263264417648315\n",
            "search 0.443359375\n",
            "tcost icost 0.03564453125 0.42194947600364685\n",
            "tcost icost 0.033416748046875 0.4227086007595062\n",
            "tcost icost 0.033416748046875 0.41994407773017883\n",
            "search 0.453369140625\n",
            "tcost icost 0.03509521484375 0.4170530438423157\n",
            "tcost icost 0.03302001953125 0.41141650080680847\n",
            "tcost icost 0.03302001953125 0.4137175679206848\n",
            "search 0.44677734375\n",
            "tcost icost 0.0445556640625 0.4258537292480469\n",
            "tcost icost 0.0335693359375 0.41241809725761414\n",
            "tcost icost 0.0335693359375 0.41479015350341797\n",
            "search 0.4482421875\n",
            "tcost icost 0.04339599609375 0.4215351343154907\n",
            "tcost icost 0.036224365234375 0.42118099331855774\n",
            "tcost icost 0.036224365234375 0.4174399971961975\n",
            "search 0.45361328125\n",
            "tcost icost 0.050506591796875 0.4247244596481323\n",
            "tcost icost 0.039703369140625 0.4174450933933258\n",
            "tcost icost 0.039703369140625 0.4206428825855255\n",
            "search 0.46044921875\n",
            "tcost icost 0.036651611328125 0.4164658784866333\n",
            "tcost icost 0.034576416015625 0.4097072184085846\n",
            "tcost icost 0.034576416015625 0.41192784905433655\n",
            "search 0.446533203125\n",
            "tcost icost 0.035003662109375 0.4093495309352875\n",
            "tcost icost 0.0302734375 0.39872434735298157\n",
            "tcost icost 0.0302734375 0.4005800485610962\n",
            "search 0.430908203125\n",
            "tcost icost 0.03472900390625 0.41033682227134705\n",
            "tcost icost 0.03472900390625 0.40736374258995056\n",
            "tcost icost 0.03472900390625 0.40736374258995056\n",
            "search 0.442138671875\n",
            "tcost icost 0.04217529296875 0.4048860967159271\n",
            "tcost icost 0.03289794921875 0.4030206799507141\n",
            "tcost icost 0.03289794921875 0.4030206799507141\n",
            "search 0.43603515625\n",
            "tcost icost 0.029998779296875 0.3956678807735443\n",
            "tcost icost 0.029449462890625 0.38778045773506165\n",
            "tcost icost 0.029449462890625 0.38778045773506165\n",
            "search 0.417236328125\n",
            "tcost icost 0.03466796875 0.4017069637775421\n",
            "tcost icost 0.03350830078125 0.4007643759250641\n",
            "tcost icost 0.03350830078125 0.4007643759250641\n",
            "search 0.434326171875\n",
            "tcost icost 0.037994384765625 0.3984745740890503\n",
            "tcost icost 0.03570556640625 0.39547258615493774\n",
            "tcost icost 0.03570556640625 0.39547258615493774\n",
            "search 0.43115234375\n",
            "tcost icost 0.042144775390625 0.3949095606803894\n",
            "tcost icost 0.03570556640625 0.39234742522239685\n",
            "tcost icost 0.03570556640625 0.39234742522239685\n",
            "search 0.427978515625\n",
            "tcost icost 0.037078857421875 0.39166074991226196\n",
            "tcost icost 0.035430908203125 0.39548251032829285\n",
            "tcost icost 0.035430908203125 0.39243122935295105\n",
            "search 0.427734375\n",
            "tcost icost 0.034271240234375 0.38955265283584595\n",
            "tcost icost 0.032073974609375 0.38912713527679443\n",
            "tcost icost 0.032073974609375 0.3900277614593506\n",
            "search 0.422119140625\n",
            "tcost icost 0.03717041015625 0.38960662484169006\n",
            "tcost icost 0.035186767578125 0.39246252179145813\n",
            "tcost icost 0.035186767578125 0.39215198159217834\n",
            "search 0.42724609375\n",
            "tcost icost 0.0296173095703125 0.3677732050418854\n",
            "tcost icost 0.0306549072265625 0.3625447154045105\n",
            "tcost icost 0.0306549072265625 0.36272382736206055\n",
            "search 0.3935546875\n",
            "tcost icost 0.03619384765625 0.38690438866615295\n",
            "tcost icost 0.0274200439453125 0.37931522727012634\n",
            "tcost icost 0.0274200439453125 0.37803974747657776\n",
            "search 0.4052734375\n",
            "tcost icost 0.0386962890625 0.38671252131462097\n",
            "tcost icost 0.03631591796875 0.3845376670360565\n",
            "tcost icost 0.03631591796875 0.3845376670360565\n",
            "search 0.4208984375\n",
            "tcost icost 0.035888671875 0.3883167505264282\n",
            "tcost icost 0.03466796875 0.38507279753685\n",
            "tcost icost 0.03466796875 0.384034663438797\n",
            "search 0.418701171875\n",
            "tcost icost 0.044769287109375 0.38749396800994873\n",
            "tcost icost 0.03662109375 0.39118388295173645\n",
            "tcost icost 0.03662109375 0.3936001658439636\n",
            "search 0.43017578125\n",
            "tcost icost 0.035491943359375 0.3828621208667755\n",
            "tcost icost 0.0279083251953125 0.3821173310279846\n",
            "tcost icost 0.0279083251953125 0.3821173310279846\n",
            "search 0.409912109375\n",
            "tcost icost 0.03778076171875 0.38035422563552856\n",
            "tcost icost 0.035430908203125 0.37863776087760925\n",
            "tcost icost 0.035430908203125 0.37863776087760925\n",
            "search 0.4140625\n",
            "tcost icost 0.036346435546875 0.37967896461486816\n",
            "tcost icost 0.035736083984375 0.38403570652008057\n",
            "tcost icost 0.035736083984375 0.38163989782333374\n",
            "search 0.417236328125\n",
            "tcost icost 0.03662109375 0.38650327920913696\n",
            "tcost icost 0.03662109375 0.38650327920913696\n",
            "tcost icost 0.03662109375 0.38650327920913696\n",
            "search 0.423095703125\n",
            "tcost icost 0.035552978515625 0.3785686790943146\n",
            "tcost icost 0.0333251953125 0.37978312373161316\n",
            "tcost icost 0.0333251953125 0.3825313150882721\n",
            "search 0.416015625\n",
            "tcost icost 0.0350341796875 0.3788396716117859\n",
            "tcost icost 0.0325927734375 0.3833160400390625\n",
            "tcost icost 0.0325927734375 0.3781088590621948\n",
            "search 0.41064453125\n",
            "tcost icost 0.042144775390625 0.3727782666683197\n",
            "tcost icost 0.038299560546875 0.36690282821655273\n",
            "tcost icost 0.038299560546875 0.370341420173645\n",
            "search 0.40869140625\n",
            "tcost icost 0.037322998046875 0.3840174674987793\n",
            "tcost icost 0.033721923828125 0.3833644688129425\n",
            "tcost icost 0.033721923828125 0.3833644688129425\n",
            "search 0.4169921875\n",
            "tcost icost 0.03857421875 0.37304186820983887\n",
            "tcost icost 0.03857421875 0.3763517141342163\n",
            "tcost icost 0.03857421875 0.37304186820983887\n",
            "search 0.41162109375\n",
            "tcost icost 0.040740966796875 0.3812080919742584\n",
            "tcost icost 0.03271484375 0.3738366961479187\n",
            "tcost icost 0.03271484375 0.37438490986824036\n",
            "search 0.406982421875\n",
            "tcost icost 0.039337158203125 0.3805568218231201\n",
            "tcost icost 0.036651611328125 0.38323143124580383\n",
            "tcost icost 0.036651611328125 0.3806619346141815\n",
            "search 0.417236328125\n",
            "tcost icost 0.0433349609375 0.38410425186157227\n",
            "tcost icost 0.037811279296875 0.3844902217388153\n",
            "tcost icost 0.037811279296875 0.38599830865859985\n",
            "search 0.423828125\n",
            "tcost icost 0.0288848876953125 0.38324621319770813\n",
            "tcost icost 0.0298004150390625 0.38067272305488586\n",
            "tcost icost 0.0298004150390625 0.3817567229270935\n",
            "search 0.41162109375\n",
            "tcost icost 0.034637451171875 0.38171032071113586\n",
            "tcost icost 0.0321044921875 0.38298049569129944\n",
            "tcost icost 0.0321044921875 0.38060474395751953\n",
            "search 0.41259765625\n",
            "tcost icost 0.03826904296875 0.38328444957733154\n",
            "tcost icost 0.03564453125 0.3843548595905304\n",
            "tcost icost 0.03564453125 0.38612625002861023\n",
            "search 0.421875\n",
            "tcost icost 0.036865234375 0.38684287667274475\n",
            "tcost icost 0.036163330078125 0.38464832305908203\n",
            "tcost icost 0.036163330078125 0.3845768868923187\n",
            "search 0.420654296875\n",
            "tcost icost 0.03997802734375 0.38778793811798096\n",
            "tcost icost 0.03936767578125 0.37666937708854675\n",
            "tcost icost 0.03936767578125 0.3756101727485657\n",
            "search 0.414794921875\n",
            "tcost icost 0.044219970703125 0.38586148619651794\n",
            "tcost icost 0.037200927734375 0.38884496688842773\n",
            "tcost icost 0.037200927734375 0.3890562057495117\n",
            "search 0.42626953125\n",
            "tcost icost 0.03900146484375 0.38833674788475037\n",
            "tcost icost 0.03900146484375 0.3895331621170044\n",
            "tcost icost 0.03900146484375 0.38833674788475037\n",
            "search 0.427490234375\n",
            "tcost icost 0.03802490234375 0.3868658244609833\n",
            "tcost icost 0.038177490234375 0.3838285207748413\n",
            "tcost icost 0.038177490234375 0.3835550546646118\n",
            "search 0.421630859375\n",
            "tcost icost 0.042877197265625 0.3931806981563568\n",
            "tcost icost 0.039154052734375 0.387500524520874\n",
            "tcost icost 0.039154052734375 0.38922974467277527\n",
            "search 0.42822265625\n",
            "tcost icost 0.03741455078125 0.371997207403183\n",
            "tcost icost 0.033843994140625 0.35654884576797485\n",
            "tcost icost 0.033843994140625 0.35565078258514404\n",
            "search 0.3896484375\n",
            "tcost icost 0.0343017578125 0.3824949264526367\n",
            "tcost icost 0.03466796875 0.3721696734428406\n",
            "tcost icost 0.03466796875 0.3732711970806122\n",
            "search 0.407958984375\n",
            "tcost icost 0.0364990234375 0.3848980963230133\n",
            "tcost icost 0.0362548828125 0.36996421217918396\n",
            "tcost icost 0.0362548828125 0.37184104323387146\n",
            "search 0.408203125\n",
            "tcost icost 0.0401611328125 0.3878079354763031\n",
            "tcost icost 0.0411376953125 0.3809424042701721\n",
            "tcost icost 0.0411376953125 0.3775489628314972\n",
            "search 0.41845703125\n",
            "tcost icost 0.038970947265625 0.3921065330505371\n",
            "tcost icost 0.036529541015625 0.3928581476211548\n",
            "tcost icost 0.036529541015625 0.39203575253486633\n",
            "search 0.4287109375\n",
            "tcost icost 0.0394287109375 0.39181265234947205\n",
            "tcost icost 0.038818359375 0.38524535298347473\n",
            "tcost icost 0.038818359375 0.3852449655532837\n",
            "search 0.424072265625\n",
            "tcost icost 0.044769287109375 0.39460253715515137\n",
            "tcost icost 0.041168212890625 0.37680134177207947\n",
            "tcost icost 0.041168212890625 0.3785785734653473\n",
            "search 0.419921875\n",
            "tcost icost 0.04345703125 0.39804327487945557\n",
            "tcost icost 0.03460693359375 0.3969094157218933\n",
            "tcost icost 0.03460693359375 0.3981599509716034\n",
            "search 0.432861328125\n",
            "tcost icost 0.043670654296875 0.39809349179267883\n",
            "tcost icost 0.041961669921875 0.38694456219673157\n",
            "tcost icost 0.041961669921875 0.3857899606227875\n",
            "search 0.427734375\n",
            "tcost icost 0.04443359375 0.39851221442222595\n",
            "tcost icost 0.042083740234375 0.39982476830482483\n",
            "tcost icost 0.042083740234375 0.39789849519729614\n",
            "search 0.43994140625\n",
            "tcost icost 0.0489501953125 0.39797741174697876\n",
            "tcost icost 0.0418701171875 0.3987865149974823\n",
            "tcost icost 0.0418701171875 0.3987865149974823\n",
            "search 0.4404296875\n",
            "tcost icost 0.048919677734375 0.3978937566280365\n",
            "tcost icost 0.0450439453125 0.3928166329860687\n",
            "tcost icost 0.0450439453125 0.3940984904766083\n",
            "search 0.43896484375\n",
            "tcost icost 0.044708251953125 0.398663729429245\n",
            "tcost icost 0.04364013671875 0.3929476737976074\n",
            "tcost icost 0.04364013671875 0.3929476737976074\n",
            "search 0.436767578125\n",
            "tcost icost 0.04449462890625 0.3955296576023102\n",
            "tcost icost 0.04449462890625 0.39562487602233887\n",
            "tcost icost 0.04449462890625 0.3955296576023102\n",
            "search 0.43994140625\n",
            "tcost icost 0.05120849609375 0.4011988341808319\n",
            "tcost icost 0.042572021484375 0.40216556191444397\n",
            "tcost icost 0.042572021484375 0.4003821015357971\n",
            "search 0.44287109375\n",
            "tcost icost 0.052734375 0.39787566661834717\n",
            "tcost icost 0.04437255859375 0.40028858184814453\n",
            "tcost icost 0.04437255859375 0.40106001496315\n",
            "search 0.445556640625\n",
            "tcost icost 0.04302978515625 0.3962157964706421\n",
            "tcost icost 0.0421142578125 0.3901226222515106\n",
            "tcost icost 0.0421142578125 0.3914138078689575\n",
            "search 0.43359375\n",
            "tcost icost 0.05169677734375 0.4017503261566162\n",
            "tcost icost 0.046875 0.39138123393058777\n",
            "tcost icost 0.046875 0.3924197554588318\n",
            "search 0.439208984375\n",
            "tcost icost 0.045318603515625 0.3979291021823883\n",
            "tcost icost 0.044403076171875 0.3922511339187622\n",
            "tcost icost 0.044403076171875 0.3922511339187622\n",
            "search 0.436767578125\n",
            "tcost icost 0.040008544921875 0.40389227867126465\n",
            "tcost icost 0.038787841796875 0.40108662843704224\n",
            "tcost icost 0.038787841796875 0.4027480185031891\n",
            "search 0.441650390625\n",
            "tcost icost 0.049713134765625 0.39878058433532715\n",
            "tcost icost 0.046600341796875 0.38722652196884155\n",
            "tcost icost 0.046600341796875 0.3867146670818329\n",
            "search 0.433349609375\n",
            "tcost icost 0.046630859375 0.40346843004226685\n",
            "tcost icost 0.042236328125 0.3872218132019043\n",
            "tcost icost 0.042236328125 0.38955920934677124\n",
            "search 0.431884765625\n",
            "tcost icost 0.045074462890625 0.4029187262058258\n",
            "tcost icost 0.04345703125 0.3935859799385071\n",
            "tcost icost 0.04345703125 0.39205655455589294\n",
            "search 0.435546875\n",
            "tcost icost 0.03887939453125 0.4090873897075653\n",
            "tcost icost 0.036224365234375 0.4081372022628784\n",
            "tcost icost 0.036224365234375 0.4100169837474823\n",
            "search 0.446044921875\n",
            "tcost icost 0.0384521484375 0.408751904964447\n",
            "tcost icost 0.036712646484375 0.4049588739871979\n",
            "tcost icost 0.036712646484375 0.4044727683067322\n",
            "search 0.441162109375\n",
            "tcost icost 0.03533935546875 0.4090690314769745\n",
            "tcost icost 0.03424072265625 0.40283554792404175\n",
            "tcost icost 0.03424072265625 0.4045282304286957\n",
            "search 0.438720703125\n",
            "tcost icost 0.0374755859375 0.40610507130622864\n",
            "tcost icost 0.03765869140625 0.39230796694755554\n",
            "tcost icost 0.03765869140625 0.39287158846855164\n",
            "search 0.430419921875\n",
            "tcost icost 0.037445068359375 0.4109548032283783\n",
            "tcost icost 0.033599853515625 0.41248035430908203\n",
            "tcost icost 0.033599853515625 0.41043785214424133\n",
            "search 0.444091796875\n",
            "tcost icost 0.0396728515625 0.4130636751651764\n",
            "tcost icost 0.03704833984375 0.41225892305374146\n",
            "tcost icost 0.03704833984375 0.4124920666217804\n",
            "search 0.44970703125\n",
            "tcost icost 0.03875732421875 0.4101468622684479\n",
            "tcost icost 0.035430908203125 0.41099855303764343\n",
            "tcost icost 0.035430908203125 0.4115024209022522\n",
            "search 0.447021484375\n",
            "tcost icost 0.04046630859375 0.4098794758319855\n",
            "tcost icost 0.035125732421875 0.38591110706329346\n",
            "tcost icost 0.035125732421875 0.3875996470451355\n",
            "search 0.4228515625\n",
            "tcost icost 0.0341796875 0.40907812118530273\n",
            "tcost icost 0.035369873046875 0.40690675377845764\n",
            "tcost icost 0.035369873046875 0.40620431303977966\n",
            "search 0.441650390625\n",
            "tcost icost 0.050384521484375 0.41237592697143555\n",
            "tcost icost 0.044281005859375 0.40094590187072754\n",
            "tcost icost 0.044281005859375 0.4022119641304016\n",
            "search 0.4462890625\n",
            "tcost icost 0.043182373046875 0.4041362702846527\n",
            "tcost icost 0.045318603515625 0.3991428017616272\n",
            "tcost icost 0.045318603515625 0.3958204686641693\n",
            "search 0.441162109375\n",
            "tcost icost 0.04815673828125 0.41229575872421265\n",
            "tcost icost 0.041748046875 0.41219115257263184\n",
            "tcost icost 0.041748046875 0.41313788294792175\n",
            "search 0.454833984375\n",
            "tcost icost 0.05609130859375 0.41277197003364563\n",
            "tcost icost 0.0452880859375 0.4111356735229492\n",
            "tcost icost 0.0452880859375 0.40949398279190063\n",
            "search 0.45458984375\n",
            "tcost icost 0.056732177734375 0.41129857301712036\n",
            "tcost icost 0.049072265625 0.4008006751537323\n",
            "tcost icost 0.049072265625 0.40134838223457336\n",
            "search 0.450439453125\n",
            "tcost icost 0.0460205078125 0.41058897972106934\n",
            "tcost icost 0.0433349609375 0.39170026779174805\n",
            "tcost icost 0.0433349609375 0.3910786509513855\n",
            "search 0.4345703125\n",
            "tcost icost 0.050537109375 0.4078880846500397\n",
            "tcost icost 0.0472412109375 0.39643362164497375\n",
            "tcost icost 0.0472412109375 0.39770182967185974\n",
            "search 0.44482421875\n",
            "tcost icost 0.042144775390625 0.40508779883384705\n",
            "tcost icost 0.040802001953125 0.39693397283554077\n",
            "tcost icost 0.040802001953125 0.39803993701934814\n",
            "search 0.438720703125\n",
            "tcost icost 0.03826904296875 0.414764404296875\n",
            "tcost icost 0.0374755859375 0.41318830847740173\n",
            "tcost icost 0.0374755859375 0.41301754117012024\n",
            "search 0.45068359375\n",
            "tcost icost 0.04278564453125 0.403780460357666\n",
            "tcost icost 0.040130615234375 0.3839898407459259\n",
            "tcost icost 0.040130615234375 0.38397976756095886\n",
            "search 0.424072265625\n",
            "tcost icost 0.04241943359375 0.4124591648578644\n",
            "tcost icost 0.041717529296875 0.4011801481246948\n",
            "tcost icost 0.041717529296875 0.40230533480644226\n",
            "search 0.444091796875\n",
            "tcost icost 0.0430908203125 0.3975974917411804\n",
            "tcost icost 0.039642333984375 0.3790128827095032\n",
            "tcost icost 0.039642333984375 0.3818361163139343\n",
            "search 0.42138671875\n",
            "tcost icost 0.042205810546875 0.39727306365966797\n",
            "tcost icost 0.038543701171875 0.3822157084941864\n",
            "tcost icost 0.038543701171875 0.3822157084941864\n",
            "search 0.4208984375\n",
            "tcost icost 0.0411376953125 0.402669221162796\n",
            "tcost icost 0.037384033203125 0.3841984272003174\n",
            "tcost icost 0.037384033203125 0.38403835892677307\n",
            "search 0.42138671875\n",
            "tcost icost 0.041290283203125 0.420184463262558\n",
            "tcost icost 0.0408935546875 0.40515443682670593\n",
            "tcost icost 0.0408935546875 0.4055885672569275\n",
            "search 0.4462890625\n",
            "tcost icost 0.036773681640625 0.4184615910053253\n",
            "tcost icost 0.036956787109375 0.40638425946235657\n",
            "tcost icost 0.036956787109375 0.40747395157814026\n",
            "search 0.4443359375\n",
            "tcost icost 0.036773681640625 0.42026379704475403\n",
            "tcost icost 0.034210205078125 0.4015826880931854\n",
            "tcost icost 0.034210205078125 0.3990098536014557\n",
            "search 0.43310546875\n",
            "tcost icost 0.0333251953125 0.42022404074668884\n",
            "tcost icost 0.033294677734375 0.4135395884513855\n",
            "tcost icost 0.033294677734375 0.4140530228614807\n",
            "search 0.447265625\n",
            "tcost icost 0.032623291015625 0.41663357615470886\n",
            "tcost icost 0.032196044921875 0.41150084137916565\n",
            "tcost icost 0.032196044921875 0.4103240966796875\n",
            "search 0.442626953125\n",
            "tcost icost 0.0298309326171875 0.4087415039539337\n",
            "tcost icost 0.0272674560546875 0.4094647467136383\n",
            "tcost icost 0.0272674560546875 0.4108055531978607\n",
            "search 0.438232421875\n",
            "tcost icost 0.0341796875 0.40537309646606445\n",
            "tcost icost 0.034576416015625 0.4019168019294739\n",
            "tcost icost 0.034576416015625 0.4012501537799835\n",
            "search 0.43603515625\n",
            "tcost icost 0.0408935546875 0.41954317688941956\n",
            "tcost icost 0.041107177734375 0.40540823340415955\n",
            "tcost icost 0.041107177734375 0.4052404463291168\n",
            "search 0.4462890625\n",
            "tcost icost 0.039306640625 0.407594233751297\n",
            "tcost icost 0.035736083984375 0.39420828223228455\n",
            "tcost icost 0.035736083984375 0.39569994807243347\n",
            "search 0.431396484375\n",
            "tcost icost 0.043212890625 0.413997083902359\n",
            "tcost icost 0.040863037109375 0.39711275696754456\n",
            "tcost icost 0.040863037109375 0.39618822932243347\n",
            "search 0.43701171875\n",
            "tcost icost 0.040802001953125 0.4195858836174011\n",
            "tcost icost 0.04022216796875 0.41203203797340393\n",
            "tcost icost 0.04022216796875 0.4132327735424042\n",
            "search 0.45361328125\n",
            "tcost icost 0.04388427734375 0.416523277759552\n",
            "tcost icost 0.042022705078125 0.40224379301071167\n",
            "tcost icost 0.042022705078125 0.4036348760128021\n",
            "search 0.445556640625\n",
            "tcost icost 0.042266845703125 0.42572852969169617\n",
            "tcost icost 0.040008544921875 0.426066130399704\n",
            "tcost icost 0.040008544921875 0.42624226212501526\n",
            "search 0.46630859375\n",
            "tcost icost 0.04656982421875 0.4199109673500061\n",
            "tcost icost 0.04656982421875 0.4185529053211212\n",
            "tcost icost 0.04656982421875 0.4174641966819763\n",
            "search 0.464111328125\n",
            "tcost icost 0.04376220703125 0.4203498959541321\n",
            "tcost icost 0.04315185546875 0.4115101993083954\n",
            "tcost icost 0.04315185546875 0.411714643239975\n",
            "search 0.454833984375\n",
            "tcost icost 0.034393310546875 0.42618492245674133\n",
            "tcost icost 0.031494140625 0.42212456464767456\n",
            "tcost icost 0.031494140625 0.4237384498119354\n",
            "search 0.455322265625\n",
            "tcost icost 0.04095458984375 0.41422006487846375\n",
            "tcost icost 0.0379638671875 0.4007934033870697\n",
            "tcost icost 0.0379638671875 0.39978352189064026\n",
            "search 0.43798828125\n",
            "tcost icost 0.044921875 0.42787569761276245\n",
            "tcost icost 0.037994384765625 0.4254041612148285\n",
            "tcost icost 0.037994384765625 0.42546674609184265\n",
            "search 0.463623046875\n",
            "tcost icost 0.03826904296875 0.4279143214225769\n",
            "tcost icost 0.037200927734375 0.42043226957321167\n",
            "tcost icost 0.037200927734375 0.42192038893699646\n",
            "search 0.458984375\n",
            "tcost icost 0.035980224609375 0.4274657964706421\n",
            "tcost icost 0.03271484375 0.4247276186943054\n",
            "tcost icost 0.03271484375 0.4264044761657715\n",
            "search 0.459228515625\n",
            "tcost icost 0.04779052734375 0.42175814509391785\n",
            "tcost icost 0.039947509765625 0.4274871051311493\n",
            "tcost icost 0.039947509765625 0.4265446662902832\n",
            "search 0.466552734375\n",
            "tcost icost 0.045501708984375 0.42121216654777527\n",
            "tcost icost 0.045501708984375 0.42417600750923157\n",
            "tcost icost 0.045501708984375 0.42401793599128723\n",
            "search 0.469482421875\n",
            "tcost icost 0.037689208984375 0.42626267671585083\n",
            "tcost icost 0.039154052734375 0.4216446876525879\n",
            "tcost icost 0.039154052734375 0.4214501678943634\n",
            "search 0.46044921875\n",
            "tcost icost 0.041229248046875 0.42408958077430725\n",
            "tcost icost 0.03094482421875 0.4282057285308838\n",
            "tcost icost 0.03094482421875 0.4290258288383484\n",
            "search 0.4599609375\n",
            "tcost icost 0.045684814453125 0.42818889021873474\n",
            "tcost icost 0.0390625 0.43297746777534485\n",
            "tcost icost 0.0390625 0.43297746777534485\n",
            "search 0.471923828125\n",
            "tcost icost 0.033721923828125 0.4304886758327484\n",
            "tcost icost 0.0300140380859375 0.43015459179878235\n",
            "tcost icost 0.0300140380859375 0.4307038187980652\n",
            "search 0.460693359375\n",
            "tcost icost 0.04156494140625 0.42742589116096497\n",
            "tcost icost 0.04156494140625 0.42770934104919434\n",
            "tcost icost 0.04156494140625 0.4274882376194\n",
            "search 0.468994140625\n",
            "tcost icost 0.041259765625 0.42909398674964905\n",
            "tcost icost 0.041259765625 0.42800283432006836\n",
            "tcost icost 0.041259765625 0.42710167169570923\n",
            "search 0.46826171875\n",
            "tcost icost 0.042388916015625 0.4147058427333832\n",
            "tcost icost 0.0443115234375 0.4093678891658783\n",
            "tcost icost 0.0443115234375 0.4108268618583679\n",
            "search 0.455078125\n",
            "tcost icost 0.04656982421875 0.42122769355773926\n",
            "tcost icost 0.047271728515625 0.411771297454834\n",
            "tcost icost 0.047271728515625 0.41420862078666687\n",
            "search 0.461669921875\n",
            "tcost icost 0.04486083984375 0.4163099229335785\n",
            "tcost icost 0.047149658203125 0.41072267293930054\n",
            "tcost icost 0.047149658203125 0.41104280948638916\n",
            "search 0.458251953125\n",
            "tcost icost 0.049041748046875 0.4178493618965149\n",
            "tcost icost 0.0462646484375 0.41457390785217285\n",
            "tcost icost 0.0462646484375 0.4165019690990448\n",
            "search 0.462890625\n",
            "tcost icost 0.041778564453125 0.42477309703826904\n",
            "tcost icost 0.038604736328125 0.4219215512275696\n",
            "tcost icost 0.038604736328125 0.4233655035495758\n",
            "search 0.4619140625\n",
            "tcost icost 0.041961669921875 0.4202532470226288\n",
            "tcost icost 0.038299560546875 0.4187018573284149\n",
            "tcost icost 0.038299560546875 0.41929441690444946\n",
            "search 0.45751953125\n",
            "tcost icost 0.04632568359375 0.41708001494407654\n",
            "tcost icost 0.04632568359375 0.4166127145290375\n",
            "tcost icost 0.04632568359375 0.41883188486099243\n",
            "search 0.46533203125\n",
            "tcost icost 0.041015625 0.42024412751197815\n",
            "tcost icost 0.0426025390625 0.41901251673698425\n",
            "tcost icost 0.0426025390625 0.4150536358356476\n",
            "search 0.45751953125\n",
            "tcost icost 0.040924072265625 0.4133807122707367\n",
            "tcost icost 0.035247802734375 0.42345497012138367\n",
            "tcost icost 0.035247802734375 0.4216199219226837\n",
            "search 0.456787109375\n",
            "tcost icost 0.043243408203125 0.41307592391967773\n",
            "tcost icost 0.036102294921875 0.422024130821228\n",
            "tcost icost 0.036102294921875 0.4219457507133484\n",
            "search 0.4580078125\n",
            "tcost icost 0.038421630859375 0.4161045551300049\n",
            "tcost icost 0.041961669921875 0.41096141934394836\n",
            "tcost icost 0.041961669921875 0.4106769859790802\n",
            "search 0.45263671875\n",
            "tcost icost 0.042022705078125 0.41344520449638367\n",
            "tcost icost 0.0399169921875 0.4119916260242462\n",
            "tcost icost 0.0399169921875 0.4151856303215027\n",
            "search 0.455078125\n",
            "tcost icost 0.03948974609375 0.4138226807117462\n",
            "tcost icost 0.036529541015625 0.41192421317100525\n",
            "tcost icost 0.036529541015625 0.4116361737251282\n",
            "search 0.4482421875\n",
            "tcost icost 0.04376220703125 0.4109203517436981\n",
            "tcost icost 0.03948974609375 0.4045552611351013\n",
            "tcost icost 0.03948974609375 0.4074955880641937\n",
            "search 0.447021484375\n",
            "tcost icost 0.04144287109375 0.40334567427635193\n",
            "tcost icost 0.036224365234375 0.4171258211135864\n",
            "tcost icost 0.036224365234375 0.41207805275917053\n",
            "search 0.4482421875\n",
            "tcost icost 0.040283203125 0.40820828080177307\n",
            "tcost icost 0.03924560546875 0.41147467494010925\n",
            "tcost icost 0.03924560546875 0.4109361469745636\n",
            "search 0.4501953125\n",
            "tcost icost 0.041259765625 0.40287595987319946\n",
            "tcost icost 0.04022216796875 0.4068915545940399\n",
            "tcost icost 0.04022216796875 0.40513190627098083\n",
            "search 0.4453125\n",
            "tcost icost 0.0467529296875 0.40610185265541077\n",
            "tcost icost 0.038726806640625 0.4082188606262207\n",
            "tcost icost 0.038726806640625 0.41062530875205994\n",
            "search 0.449462890625\n",
            "tcost icost 0.050079345703125 0.39788395166397095\n",
            "tcost icost 0.038116455078125 0.4019831717014313\n",
            "tcost icost 0.038116455078125 0.4018460810184479\n",
            "search 0.43994140625\n",
            "tcost icost 0.037017822265625 0.4005524516105652\n",
            "tcost icost 0.04315185546875 0.3966032862663269\n",
            "tcost icost 0.04315185546875 0.3952065706253052\n",
            "search 0.4384765625\n",
            "tcost icost 0.040740966796875 0.40121379494667053\n",
            "tcost icost 0.039398193359375 0.40228959918022156\n",
            "tcost icost 0.039398193359375 0.3999791145324707\n",
            "search 0.439208984375\n",
            "tcost icost 0.050079345703125 0.392850399017334\n",
            "tcost icost 0.038116455078125 0.39427676796913147\n",
            "tcost icost 0.038116455078125 0.39519038796424866\n",
            "search 0.433349609375\n",
            "tcost icost 0.05072021484375 0.3918522000312805\n",
            "tcost icost 0.04022216796875 0.3911646008491516\n",
            "tcost icost 0.04022216796875 0.38789254426956177\n",
            "search 0.42822265625\n",
            "tcost icost 0.037445068359375 0.39014574885368347\n",
            "tcost icost 0.041900634765625 0.38113951683044434\n",
            "tcost icost 0.041900634765625 0.38472771644592285\n",
            "search 0.4267578125\n",
            "tcost icost 0.036468505859375 0.39345431327819824\n",
            "tcost icost 0.03912353515625 0.3795797824859619\n",
            "tcost icost 0.03912353515625 0.3824249505996704\n",
            "search 0.42138671875\n",
            "tcost icost 0.03363037109375 0.3921476900577545\n",
            "tcost icost 0.03118896484375 0.3925389051437378\n",
            "tcost icost 0.03118896484375 0.3914807140827179\n",
            "search 0.4228515625\n",
            "tcost icost 0.040069580078125 0.37648987770080566\n",
            "tcost icost 0.03497314453125 0.38853979110717773\n",
            "tcost icost 0.03497314453125 0.3899554908275604\n",
            "search 0.4248046875\n",
            "tcost icost 0.0347900390625 0.38573163747787476\n",
            "tcost icost 0.03765869140625 0.37324875593185425\n",
            "tcost icost 0.03765869140625 0.37046948075294495\n",
            "search 0.407958984375\n",
            "tcost icost 0.0299530029296875 0.39528048038482666\n",
            "tcost icost 0.037689208984375 0.37860116362571716\n",
            "tcost icost 0.037689208984375 0.37824997305870056\n",
            "search 0.415771484375\n",
            "tcost icost 0.03900146484375 0.3709520995616913\n",
            "tcost icost 0.038299560546875 0.37340980768203735\n",
            "tcost icost 0.038299560546875 0.37340980768203735\n",
            "search 0.41162109375\n",
            "tcost icost 0.043212890625 0.3632482588291168\n",
            "tcost icost 0.035186767578125 0.37484508752822876\n",
            "tcost icost 0.035186767578125 0.37843069434165955\n",
            "search 0.41357421875\n",
            "tcost icost 0.0350341796875 0.3786373436450958\n",
            "tcost icost 0.038055419921875 0.36432722210884094\n",
            "tcost icost 0.038055419921875 0.3617120087146759\n",
            "search 0.39990234375\n",
            "tcost icost 0.03167724609375 0.38408756256103516\n",
            "tcost icost 0.03167724609375 0.3838883936405182\n",
            "tcost icost 0.03167724609375 0.3854333162307739\n",
            "search 0.417236328125\n",
            "tcost icost 0.035919189453125 0.3717322051525116\n",
            "tcost icost 0.0391845703125 0.36177462339401245\n",
            "tcost icost 0.0391845703125 0.3583812117576599\n",
            "search 0.3974609375\n",
            "tcost icost 0.034210205078125 0.3793618679046631\n",
            "tcost icost 0.037628173828125 0.3641873896121979\n",
            "tcost icost 0.037628173828125 0.3688619136810303\n",
            "search 0.406494140625\n",
            "tcost icost 0.038726806640625 0.3709357678890228\n",
            "tcost icost 0.040374755859375 0.3662145733833313\n",
            "tcost icost 0.040374755859375 0.3624516427516937\n",
            "search 0.40283203125\n",
            "tcost icost 0.04290771484375 0.36353880167007446\n",
            "tcost icost 0.04107666015625 0.3545268177986145\n",
            "tcost icost 0.04107666015625 0.3555229604244232\n",
            "search 0.396484375\n",
            "tcost icost 0.045318603515625 0.35312584042549133\n",
            "tcost icost 0.038116455078125 0.36231985688209534\n",
            "tcost icost 0.038116455078125 0.3660339117050171\n",
            "search 0.404052734375\n",
            "tcost icost 0.038909912109375 0.371502161026001\n",
            "tcost icost 0.041595458984375 0.3586454391479492\n",
            "tcost icost 0.041595458984375 0.357254296541214\n",
            "search 0.398681640625\n",
            "tcost icost 0.041748046875 0.3670242726802826\n",
            "tcost icost 0.0440673828125 0.3544498682022095\n",
            "tcost icost 0.0440673828125 0.35550621151924133\n",
            "search 0.3994140625\n",
            "tcost icost 0.04400634765625 0.3510293662548065\n",
            "tcost icost 0.03814697265625 0.36046338081359863\n",
            "tcost icost 0.03814697265625 0.35566946864128113\n",
            "search 0.393798828125\n",
            "tcost icost 0.048614501953125 0.35234567523002625\n",
            "tcost icost 0.036895751953125 0.37973782420158386\n",
            "tcost icost 0.036895751953125 0.3730905055999756\n",
            "search 0.409912109375\n",
            "tcost icost 0.045501708984375 0.3592071235179901\n",
            "tcost icost 0.0421142578125 0.35468044877052307\n",
            "tcost icost 0.0421142578125 0.3579002618789673\n",
            "search 0.39990234375\n",
            "tcost icost 0.040008544921875 0.37261098623275757\n",
            "tcost icost 0.04193115234375 0.3570248782634735\n",
            "tcost icost 0.04193115234375 0.3570132851600647\n",
            "search 0.39892578125\n",
            "tcost icost 0.044281005859375 0.3724926710128784\n",
            "tcost icost 0.04425048828125 0.36055970191955566\n",
            "tcost icost 0.04425048828125 0.36001911759376526\n",
            "search 0.404296875\n",
            "tcost icost 0.04132080078125 0.3775162398815155\n",
            "tcost icost 0.0426025390625 0.36087486147880554\n",
            "tcost icost 0.0426025390625 0.35858142375946045\n",
            "search 0.4013671875\n",
            "tcost icost 0.0382080078125 0.3833329975605011\n",
            "tcost icost 0.03485107421875 0.38685235381126404\n",
            "tcost icost 0.03485107421875 0.38380002975463867\n",
            "search 0.418701171875\n",
            "tcost icost 0.03826904296875 0.36771100759506226\n",
            "tcost icost 0.043121337890625 0.3638753294944763\n",
            "tcost icost 0.043121337890625 0.35871008038520813\n",
            "search 0.40185546875\n",
            "tcost icost 0.0380859375 0.3791455328464508\n",
            "tcost icost 0.041748046875 0.36536267399787903\n",
            "tcost icost 0.041748046875 0.36601534485816956\n",
            "search 0.40771484375\n",
            "tcost icost 0.038482666015625 0.38330376148223877\n",
            "tcost icost 0.0433349609375 0.37393805384635925\n",
            "tcost icost 0.0433349609375 0.3740200400352478\n",
            "search 0.41748046875\n",
            "tcost icost 0.0391845703125 0.37838470935821533\n",
            "tcost icost 0.0413818359375 0.3660472631454468\n",
            "tcost icost 0.0413818359375 0.36276036500930786\n",
            "search 0.404296875\n",
            "tcost icost 0.051422119140625 0.3704972565174103\n",
            "tcost icost 0.048248291015625 0.36956802010536194\n",
            "tcost icost 0.048248291015625 0.36638009548187256\n",
            "search 0.414794921875\n",
            "tcost icost 0.040679931640625 0.3770851492881775\n",
            "tcost icost 0.0421142578125 0.3658234477043152\n",
            "tcost icost 0.0421142578125 0.3612893223762512\n",
            "search 0.4033203125\n",
            "tcost icost 0.04388427734375 0.3751010596752167\n",
            "tcost icost 0.04351806640625 0.3640816807746887\n",
            "tcost icost 0.04351806640625 0.3650280237197876\n",
            "search 0.408447265625\n",
            "tcost icost 0.051239013671875 0.3730027675628662\n",
            "tcost icost 0.051239013671875 0.37279632687568665\n",
            "tcost icost 0.051239013671875 0.37408629059791565\n",
            "search 0.42529296875\n",
            "tcost icost 0.04278564453125 0.37458279728889465\n",
            "tcost icost 0.042755126953125 0.36575549840927124\n",
            "tcost icost 0.042755126953125 0.36571651697158813\n",
            "search 0.408447265625\n",
            "tcost icost 0.0390625 0.38587141036987305\n",
            "tcost icost 0.044677734375 0.37041613459587097\n",
            "tcost icost 0.044677734375 0.368240088224411\n",
            "search 0.412841796875\n",
            "tcost icost 0.042327880859375 0.37226659059524536\n",
            "tcost icost 0.042572021484375 0.36732715368270874\n",
            "tcost icost 0.042572021484375 0.3664250671863556\n",
            "search 0.408935546875\n",
            "tcost icost 0.0355224609375 0.3654317259788513\n",
            "tcost icost 0.0386962890625 0.35865575075149536\n",
            "tcost icost 0.0386962890625 0.35707974433898926\n",
            "search 0.39599609375\n",
            "tcost icost 0.040008544921875 0.38054245710372925\n",
            "tcost icost 0.043731689453125 0.3643572926521301\n",
            "tcost icost 0.043731689453125 0.3679516613483429\n",
            "search 0.41162109375\n",
            "tcost icost 0.04254150390625 0.3723156154155731\n",
            "tcost icost 0.0418701171875 0.3629743158817291\n",
            "tcost icost 0.0418701171875 0.3629245460033417\n",
            "search 0.40478515625\n",
            "tcost icost 0.043243408203125 0.3840833604335785\n",
            "tcost icost 0.051727294921875 0.3701861500740051\n",
            "tcost icost 0.051727294921875 0.37130609154701233\n",
            "search 0.423095703125\n",
            "tcost icost 0.03216552734375 0.3822133243083954\n",
            "tcost icost 0.03668212890625 0.3717923164367676\n",
            "tcost icost 0.03668212890625 0.36885470151901245\n",
            "search 0.405517578125\n",
            "tcost icost 0.037139892578125 0.3593238592147827\n",
            "tcost icost 0.042083740234375 0.3559016287326813\n",
            "tcost icost 0.042083740234375 0.35586217045783997\n",
            "search 0.39794921875\n",
            "tcost icost 0.042724609375 0.37813514471054077\n",
            "tcost icost 0.0426025390625 0.3599645495414734\n",
            "tcost icost 0.0426025390625 0.3600900173187256\n",
            "search 0.40283203125\n",
            "tcost icost 0.0399169921875 0.37471985816955566\n",
            "tcost icost 0.041900634765625 0.36503005027770996\n",
            "tcost icost 0.041900634765625 0.36005324125289917\n",
            "search 0.402099609375\n",
            "tcost icost 0.040618896484375 0.3640572428703308\n",
            "tcost icost 0.041656494140625 0.3598763048648834\n",
            "tcost icost 0.041656494140625 0.3598763048648834\n",
            "search 0.401611328125\n",
            "tcost icost 0.042816162109375 0.37235137820243835\n",
            "tcost icost 0.043853759765625 0.362354576587677\n",
            "tcost icost 0.043853759765625 0.36675357818603516\n",
            "search 0.41064453125\n",
            "tcost icost 0.03826904296875 0.36679068207740784\n",
            "tcost icost 0.041168212890625 0.3654426038265228\n",
            "tcost icost 0.041168212890625 0.3649806082248688\n",
            "search 0.40625\n",
            "tcost icost 0.041900634765625 0.3666943609714508\n",
            "tcost icost 0.043487548828125 0.36272522807121277\n",
            "tcost icost 0.043487548828125 0.3602338433265686\n",
            "search 0.40380859375\n",
            "tcost icost 0.04205322265625 0.3670954406261444\n",
            "tcost icost 0.042694091796875 0.3624684512615204\n",
            "tcost icost 0.042694091796875 0.3604353964328766\n",
            "search 0.403076171875\n",
            "tcost icost 0.042266845703125 0.3801419734954834\n",
            "tcost icost 0.045074462890625 0.3727934956550598\n",
            "tcost icost 0.045074462890625 0.37091079354286194\n",
            "search 0.416015625\n",
            "tcost icost 0.04351806640625 0.3820027709007263\n",
            "tcost icost 0.041229248046875 0.37515440583229065\n",
            "tcost icost 0.041229248046875 0.3754724860191345\n",
            "search 0.416748046875\n",
            "tcost icost 0.037322998046875 0.3742881417274475\n",
            "tcost icost 0.041290283203125 0.3660917282104492\n",
            "tcost icost 0.041290283203125 0.3670545518398285\n",
            "search 0.408203125\n",
            "tcost icost 0.036468505859375 0.3726925253868103\n",
            "tcost icost 0.04010009765625 0.3591725528240204\n",
            "tcost icost 0.04010009765625 0.36765336990356445\n",
            "search 0.40771484375\n",
            "tcost icost 0.045440673828125 0.365247905254364\n",
            "tcost icost 0.042510986328125 0.3642460107803345\n",
            "tcost icost 0.042510986328125 0.3604956269264221\n",
            "search 0.403076171875\n",
            "tcost icost 0.043975830078125 0.380460262298584\n",
            "tcost icost 0.044219970703125 0.3676477372646332\n",
            "tcost icost 0.044219970703125 0.3702346980571747\n",
            "search 0.414306640625\n",
            "tcost icost 0.040191650390625 0.3822963237762451\n",
            "tcost icost 0.042694091796875 0.3695550262928009\n",
            "tcost icost 0.042694091796875 0.374385803937912\n",
            "search 0.4169921875\n",
            "tcost icost 0.0357666015625 0.36313164234161377\n",
            "tcost icost 0.040069580078125 0.3573298156261444\n",
            "tcost icost 0.040069580078125 0.3610169291496277\n",
            "search 0.401123046875\n",
            "tcost icost 0.038604736328125 0.38032546639442444\n",
            "tcost icost 0.041839599609375 0.36644428968429565\n",
            "tcost icost 0.041839599609375 0.37297573685646057\n",
            "search 0.414794921875\n",
            "tcost icost 0.03204345703125 0.38694262504577637\n",
            "tcost icost 0.02935791015625 0.3879823684692383\n",
            "tcost icost 0.02935791015625 0.38539424538612366\n",
            "search 0.414794921875\n",
            "tcost icost 0.029266357421875 0.390675812959671\n",
            "tcost icost 0.0261077880859375 0.38980889320373535\n",
            "tcost icost 0.0261077880859375 0.39064928889274597\n",
            "search 0.416748046875\n",
            "tcost icost 0.0304107666015625 0.3858388364315033\n",
            "tcost icost 0.0278167724609375 0.3837323486804962\n",
            "tcost icost 0.0278167724609375 0.3827684223651886\n",
            "search 0.41064453125\n",
            "ded\n",
            "time\n",
            "27 #### train ####\n",
            "repr, std, cov, conv, closs 0.04233389347791672 0.322265625 0.3475404977798462 0.05670011788606644 0.009024860337376595\n",
            "53.70432722164768 6.899986091363442 1.0\n",
            "repr, std, cov, conv, closs 0.03897339850664139 0.32373046875 0.3374171853065491 0.056633248925209045 0.021178768947720528\n",
            "53.59707946563695 6.824539676930682 1.0\n",
            "repr, std, cov, conv, closs 0.04719869792461395 0.3271484375 0.3036932945251465 0.06378911435604095 0.0010955188190564513\n",
            "53.490045883823434 6.7634248014441125 1.0\n",
            "repr, std, cov, conv, closs 0.04529614374041557 0.328857421875 0.30099985003471375 0.06375471502542496 0.024014225229620934\n",
            "54.0273598247783 6.817721954975707 1.0\n",
            "repr, std, cov, conv, closs 0.04386688023805618 0.32861328125 0.2914009690284729 0.07611584663391113 0.002561361063271761\n",
            "54.13546857178767 6.948431135452057 1.0\n",
            "repr, std, cov, conv, closs 0.04212183505296707 0.33349609375 0.31254875659942627 0.06068272888660431 0.029479146003723145\n",
            "54.57007114886069 7.039304710069875 1.0\n",
            "repr, std, cov, conv, closs 0.046524420380592346 0.331787109375 0.2889711856842041 0.06116523593664169 0.0027800777461379766\n",
            "55.39437697759879 7.239089304718693 1.0\n",
            "repr, std, cov, conv, closs 0.041444696485996246 0.3369140625 0.26762655377388 0.066674143075943 0.02349202334880829\n",
            "55.56072634705688 7.355787491407577 1.0\n",
            "repr, std, cov, conv, closs 0.033313412219285965 0.338134765625 0.24746164679527283 0.07564299553632736 0.0067652324214577675\n",
            "56.23113432206681 7.579690950843743 1.0\n",
            "repr, std, cov, conv, closs 0.04852690547704697 0.33349609375 0.2563703656196594 0.08123201131820679 0.015644090250134468\n",
            "56.343652821845254 7.701879827426469 1.0\n",
            "repr, std, cov, conv, closs 0.05523834377527237 0.3291015625 0.2893785238265991 0.06536852568387985 0.015048978850245476\n",
            "56.62593508620097 7.717291288961148 1.0\n",
            "repr, std, cov, conv, closs 0.06075087934732437 0.3291015625 0.3132830262184143 0.06578322499990463 0.02828093431890011\n",
            "56.682561021287164 7.648181675925595 1.0\n",
            "repr, std, cov, conv, closs 0.055681999772787094 0.32568359375 0.30568188428878784 0.060193877667188644 0.0007408890523947775\n",
            "56.56936572048049 7.466900018411363 1.0\n",
            "repr, std, cov, conv, closs 0.04920557513833046 0.322021484375 0.3438680171966553 0.06520342826843262 0.0040269955061376095\n",
            "56.343652821845254 7.341097954400822 1.0\n",
            "repr, std, cov, conv, closs 0.06287132203578949 0.32568359375 0.3139086961746216 0.060186080634593964 0.016250086948275566\n",
            "56.17495936270411 7.138498122106832 1.0\n",
            "repr, std, cov, conv, closs 0.051759421825408936 0.327392578125 0.3334943652153015 0.06188482046127319 0.0008692772826179862\n",
            "56.00677097346404 7.039304710069875 1.0\n",
            "repr, std, cov, conv, closs 0.05944503843784332 0.318359375 0.3781883716583252 0.06136581301689148 0.016086256131529808\n",
            "55.17335230750636 6.831364216607612 1.0\n",
            "repr, std, cov, conv, closs 0.04688824713230133 0.327880859375 0.3173423707485199 0.06349069625139236 0.0004769470542669296\n",
            "55.28375418547367 6.783735372886271 1.0\n",
            "repr, std, cov, conv, closs 0.03712940961122513 0.330810546875 0.31631696224212646 0.055834006518125534 0.01641666889190674\n",
            "55.339037939659136 6.709560076941027 1.0\n",
            "repr, std, cov, conv, closs 0.056460633873939514 0.330810546875 0.30442100763320923 0.06576785445213318 0.03099701553583145\n",
            "55.22852565981386 6.7634248014441125 1.0\n",
            "repr, std, cov, conv, closs 0.03261128440499306 0.333251953125 0.2988917827606201 0.0622870996594429 0.001021758303977549\n",
            "55.28375418547367 6.913792963532259 1.0\n",
            "repr, std, cov, conv, closs 0.051556363701820374 0.330322265625 0.29539164900779724 0.0545617938041687 0.0283232219517231\n",
            "55.449771354576384 7.032272437632244 1.0\n",
            "repr, std, cov, conv, closs 0.04823387414216995 0.331298828125 0.29334092140197754 0.05220816656947136 0.012135491706430912\n",
            "55.78330283910163 7.2030021916723745 1.0\n",
            "repr, std, cov, conv, closs 0.039009831845760345 0.332275390625 0.265789270401001 0.053941190242767334 0.0406569167971611\n",
            "55.839086141940726 7.2972051195531105 1.0\n",
            "repr, std, cov, conv, closs 0.03478381037712097 0.33544921875 0.25895604491233826 0.05004071444272995 0.00037324681761674583\n",
            "56.00677097346404 7.496812449760183 1.0\n",
            "repr, std, cov, conv, closs 0.05048711225390434 0.33154296875 0.26169270277023315 0.06105493754148483 0.025735262781381607\n",
            "56.00677097346404 7.610055223119162 1.0\n",
            "repr, std, cov, conv, closs 0.04469866678118706 0.331298828125 0.2515122592449188 0.06752853095531464 0.03038160316646099\n",
            "55.78330283910163 7.787025382274013 1.0\n",
            "repr, std, cov, conv, closs 0.052212901413440704 0.329345703125 0.30352556705474854 0.06574074178934097 0.04058528691530228\n",
            "55.39437697759879 7.7714746614764 1.0\n",
            "repr, std, cov, conv, closs 0.046586908400058746 0.32763671875 0.3102768659591675 0.06083244830369949 0.0562618188560009\n",
            "55.008162739790635 7.725008580250108 1.0\n",
            "repr, std, cov, conv, closs 0.04715065658092499 0.322998046875 0.33651667833328247 0.06756412982940674 0.00022723022266291082\n",
            "54.40668781095771 7.6024527703488145 1.0\n",
            "repr, std, cov, conv, closs 0.03895575553178787 0.325439453125 0.351252019405365 0.06295575201511383 0.0002607525675557554\n",
            "53.973386438339965 7.437106938260827 1.0\n",
            "repr, std, cov, conv, closs 0.042481619864702225 0.325927734375 0.3323860764503479 0.06318515539169312 0.0022541910875588655\n",
            "53.490045883823434 7.326437752458155 1.0\n",
            "repr, std, cov, conv, closs 0.04539696127176285 0.323974609375 0.3541546165943146 0.06970348209142685 0.0012239206116646528\n",
            "52.53631119584325 7.16709497414512 1.0\n",
            "repr, std, cov, conv, closs 0.038246989250183105 0.330810546875 0.32579535245895386 0.07452090084552765 0.01583302579820156\n",
            "52.588847507039084 7.09581664297652 1.0\n",
            "repr, std, cov, conv, closs 0.04222918301820755 0.328369140625 0.3354089856147766 0.06003739684820175 0.01649709790945053\n",
            "53.11710880196585 7.060443749153518 1.0\n",
            "repr, std, cov, conv, closs 0.03868556767702103 0.33251953125 0.25420916080474854 0.05615539476275444 0.015798961743712425\n",
            "53.43660927454889 7.145636620228938 1.0\n",
            "tcost icost 0.021148681640625 0.3585366904735565\n",
            "tcost icost 0.01023101806640625 0.35931921005249023\n",
            "tcost icost 0.01023101806640625 0.3627336919307709\n",
            "search 0.373046875\n",
            "tcost icost 0.02105712890625 0.35781002044677734\n",
            "tcost icost 0.006061553955078125 0.38000866770744324\n",
            "tcost icost 0.006061553955078125 0.3775373101234436\n",
            "search 0.383544921875\n",
            "tcost icost 0.01183319091796875 0.36710450053215027\n",
            "tcost icost 0.01482391357421875 0.36251142621040344\n",
            "tcost icost 0.01482391357421875 0.36189836263656616\n",
            "search 0.376708984375\n",
            "tcost icost 0.01273345947265625 0.3663869798183441\n",
            "tcost icost 0.01023101806640625 0.36911141872406006\n",
            "tcost icost 0.01023101806640625 0.37079131603240967\n",
            "search 0.381103515625\n",
            "tcost icost 0.00832366943359375 0.37686416506767273\n",
            "tcost icost 0.021148681640625 0.3701857030391693\n",
            "tcost icost 0.021148681640625 0.36527374386787415\n",
            "search 0.386474609375\n",
            "tcost icost 0.01482391357421875 0.36548829078674316\n",
            "tcost icost 0.006061553955078125 0.3827873766422272\n",
            "tcost icost 0.006061553955078125 0.3824455142021179\n",
            "search 0.388427734375\n",
            "tcost icost 0.021148681640625 0.364802747964859\n",
            "tcost icost 0.01023101806640625 0.3777914345264435\n",
            "tcost icost 0.01023101806640625 0.37294769287109375\n",
            "search 0.38330078125\n",
            "tcost icost 0.01273345947265625 0.37514638900756836\n",
            "tcost icost 0.01023101806640625 0.3762001693248749\n",
            "tcost icost 0.01023101806640625 0.3728030323982239\n",
            "search 0.383056640625\n",
            "tcost icost 0.00832366943359375 0.37691158056259155\n",
            "tcost icost 0.021148681640625 0.36878636479377747\n",
            "tcost icost 0.021148681640625 0.36972615122795105\n",
            "search 0.390869140625\n",
            "tcost icost 0.00832366943359375 0.3770913779735565\n",
            "tcost icost 0.021148681640625 0.3731831908226013\n",
            "tcost icost 0.021148681640625 0.374347448348999\n",
            "search 0.3955078125\n",
            "tcost icost 0.00832366943359375 0.38024359941482544\n",
            "tcost icost 0.021148681640625 0.36828866600990295\n",
            "tcost icost 0.021148681640625 0.37558838725090027\n",
            "search 0.396728515625\n",
            "tcost icost 0.01183319091796875 0.3736860752105713\n",
            "tcost icost 0.006061553955078125 0.38508790731430054\n",
            "tcost icost 0.006061553955078125 0.3892727494239807\n",
            "search 0.395263671875\n",
            "tcost icost 0.01273345947265625 0.37732431292533875\n",
            "tcost icost 0.01023101806640625 0.37381845712661743\n",
            "tcost icost 0.01023101806640625 0.3729616403579712\n",
            "search 0.38330078125\n",
            "tcost icost 0.00841522216796875 0.37688907980918884\n",
            "tcost icost 0.0216064453125 0.368960976600647\n",
            "tcost icost 0.0216064453125 0.373706191778183\n",
            "search 0.3955078125\n",
            "tcost icost 0.01177978515625 0.37615805864334106\n",
            "tcost icost 0.00603485107421875 0.3845962882041931\n",
            "tcost icost 0.00603485107421875 0.3842025399208069\n",
            "search 0.390380859375\n",
            "tcost icost 0.0079345703125 0.38111549615859985\n",
            "tcost icost 0.0186614990234375 0.3775179386138916\n",
            "tcost icost 0.0186614990234375 0.3754142224788666\n",
            "search 0.39404296875\n",
            "tcost icost 0.01861572265625 0.3799280822277069\n",
            "tcost icost 0.00978851318359375 0.3815906345844269\n",
            "tcost icost 0.00978851318359375 0.3865665793418884\n",
            "search 0.396240234375\n",
            "tcost icost 0.01031494140625 0.38124045729637146\n",
            "tcost icost 0.015777587890625 0.3824334442615509\n",
            "tcost icost 0.015777587890625 0.3801698088645935\n",
            "search 0.39599609375\n",
            "tcost icost 0.010528564453125 0.3852372467517853\n",
            "tcost icost 0.016693115234375 0.38202551007270813\n",
            "tcost icost 0.016693115234375 0.3807390630245209\n",
            "search 0.3974609375\n",
            "tcost icost 0.01560211181640625 0.37921690940856934\n",
            "tcost icost 0.005748748779296875 0.39592042565345764\n",
            "tcost icost 0.005748748779296875 0.39367979764938354\n",
            "search 0.399658203125\n",
            "tcost icost 0.01151275634765625 0.388019323348999\n",
            "tcost icost 0.00949859619140625 0.38709399104118347\n",
            "tcost icost 0.00949859619140625 0.38709399104118347\n",
            "search 0.396728515625\n",
            "tcost icost 0.01081085205078125 0.3889332115650177\n",
            "tcost icost 0.0181884765625 0.38689470291137695\n",
            "tcost icost 0.0181884765625 0.38646483421325684\n",
            "search 0.40478515625\n",
            "tcost icost 0.0192108154296875 0.38590872287750244\n",
            "tcost icost 0.006008148193359375 0.39768117666244507\n",
            "tcost icost 0.006008148193359375 0.39691361784935\n",
            "search 0.403076171875\n",
            "tcost icost 0.00595855712890625 0.3994961380958557\n",
            "tcost icost 0.0134735107421875 0.39163869619369507\n",
            "tcost icost 0.0134735107421875 0.3880465626716614\n",
            "search 0.4013671875\n",
            "tcost icost 0.007724761962890625 0.39860787987709045\n",
            "tcost icost 0.0172576904296875 0.3929716944694519\n",
            "tcost icost 0.0172576904296875 0.38935354351997375\n",
            "search 0.40673828125\n",
            "tcost icost 0.01068115234375 0.3976342976093292\n",
            "tcost icost 0.0173492431640625 0.3980497121810913\n",
            "tcost icost 0.0173492431640625 0.394777774810791\n",
            "search 0.412109375\n",
            "tcost icost 0.0106964111328125 0.39775848388671875\n",
            "tcost icost 0.0176849365234375 0.3947872519493103\n",
            "tcost icost 0.0176849365234375 0.3942978084087372\n",
            "search 0.411865234375\n",
            "tcost icost 0.010528564453125 0.40469232201576233\n",
            "tcost icost 0.01654052734375 0.3978081941604614\n",
            "tcost icost 0.01654052734375 0.3978081941604614\n",
            "search 0.414306640625\n",
            "tcost icost 0.01097869873046875 0.39984825253486633\n",
            "tcost icost 0.00909423828125 0.4024636745452881\n",
            "tcost icost 0.00909423828125 0.40232864022254944\n",
            "search 0.411376953125\n",
            "tcost icost 0.0125732421875 0.4013589322566986\n",
            "tcost icost 0.00905609130859375 0.4035976529121399\n",
            "tcost icost 0.00905609130859375 0.40259575843811035\n",
            "search 0.41162109375\n",
            "tcost icost 0.0153961181640625 0.4020000398159027\n",
            "tcost icost 0.005710601806640625 0.4053382873535156\n",
            "tcost icost 0.005710601806640625 0.40589064359664917\n",
            "search 0.41162109375\n",
            "tcost icost 0.007106781005859375 0.4081364572048187\n",
            "tcost icost 0.005584716796875 0.4048655331134796\n",
            "tcost icost 0.005584716796875 0.40721553564071655\n",
            "search 0.412841796875\n",
            "tcost icost 0.00962066650390625 0.40630197525024414\n",
            "tcost icost 0.005496978759765625 0.4048694372177124\n",
            "tcost icost 0.005496978759765625 0.4046366512775421\n",
            "search 0.41015625\n",
            "tcost icost 0.00653839111328125 0.40880441665649414\n",
            "tcost icost 0.005428314208984375 0.4066329598426819\n",
            "tcost icost 0.005428314208984375 0.40437108278274536\n",
            "search 0.40966796875\n",
            "tcost icost 0.00782012939453125 0.40462949872016907\n",
            "tcost icost 0.01361083984375 0.40118855237960815\n",
            "tcost icost 0.01361083984375 0.4056223928928375\n",
            "search 0.419189453125\n",
            "tcost icost 0.010894775390625 0.40352392196655273\n",
            "tcost icost 0.005977630615234375 0.40534722805023193\n",
            "tcost icost 0.005977630615234375 0.4054422080516815\n",
            "search 0.411376953125\n",
            "tcost icost 0.01213836669921875 0.4023374617099762\n",
            "tcost icost 0.0055389404296875 0.4042282998561859\n",
            "tcost icost 0.0055389404296875 0.40652135014533997\n",
            "search 0.412109375\n",
            "tcost icost 0.01171875 0.4042718708515167\n",
            "tcost icost 0.00966644287109375 0.40205302834510803\n",
            "tcost icost 0.00966644287109375 0.4028666019439697\n",
            "search 0.41259765625\n",
            "tcost icost 0.006805419921875 0.40816015005111694\n",
            "tcost icost 0.0054473876953125 0.40415048599243164\n",
            "tcost icost 0.0054473876953125 0.40564486384391785\n",
            "search 0.4111328125\n",
            "tcost icost 0.00612640380859375 0.40802234411239624\n",
            "tcost icost 0.005199432373046875 0.40670081973075867\n",
            "tcost icost 0.005199432373046875 0.40669411420822144\n",
            "search 0.411865234375\n",
            "tcost icost 0.01023101806640625 0.4057716727256775\n",
            "tcost icost 0.005741119384765625 0.4064830541610718\n",
            "tcost icost 0.005741119384765625 0.40681934356689453\n",
            "search 0.41259765625\n",
            "tcost icost 0.0070953369140625 0.4076859951019287\n",
            "tcost icost 0.00559234619140625 0.40833625197410583\n",
            "tcost icost 0.00559234619140625 0.4079038202762604\n",
            "search 0.41357421875\n",
            "tcost icost 0.006885528564453125 0.41158127784729004\n",
            "tcost icost 0.005565643310546875 0.4122353792190552\n",
            "tcost icost 0.005565643310546875 0.40776771306991577\n",
            "search 0.413330078125\n",
            "tcost icost 0.0102386474609375 0.41029757261276245\n",
            "tcost icost 0.005725860595703125 0.41060182452201843\n",
            "tcost icost 0.005725860595703125 0.40987348556518555\n",
            "search 0.41552734375\n",
            "tcost icost 0.00598907470703125 0.4093239903450012\n",
            "tcost icost 0.005191802978515625 0.41120603680610657\n",
            "tcost icost 0.005191802978515625 0.408890962600708\n",
            "search 0.4140625\n",
            "tcost icost 0.00732421875 0.4090464115142822\n",
            "tcost icost 0.0048828125 0.40680524706840515\n",
            "tcost icost 0.0048828125 0.4062076210975647\n",
            "search 0.4111328125\n",
            "tcost icost 0.00609588623046875 0.41441118717193604\n",
            "tcost icost 0.0050811767578125 0.4121217131614685\n",
            "tcost icost 0.0050811767578125 0.4106517434120178\n",
            "search 0.415771484375\n",
            "tcost icost 0.006969451904296875 0.41009193658828735\n",
            "tcost icost 0.00487518310546875 0.40865403413772583\n",
            "tcost icost 0.00487518310546875 0.40813249349594116\n",
            "search 0.4130859375\n",
            "tcost icost 0.00815582275390625 0.4183277189731598\n",
            "tcost icost 0.006015777587890625 0.41378551721572876\n",
            "tcost icost 0.006015777587890625 0.41353893280029297\n",
            "search 0.419677734375\n",
            "tcost icost 0.0156097412109375 0.41292309761047363\n",
            "tcost icost 0.006381988525390625 0.414704293012619\n",
            "tcost icost 0.006381988525390625 0.414352148771286\n",
            "search 0.420654296875\n",
            "tcost icost 0.0136260986328125 0.41515418887138367\n",
            "tcost icost 0.006175994873046875 0.4173373579978943\n",
            "tcost icost 0.006175994873046875 0.4192398488521576\n",
            "search 0.42529296875\n",
            "tcost icost 0.01546478271484375 0.41818422079086304\n",
            "tcost icost 0.00609588623046875 0.41547641158103943\n",
            "tcost icost 0.00609588623046875 0.4198998212814331\n",
            "search 0.426025390625\n",
            "tcost icost 0.016845703125 0.4159092903137207\n",
            "tcost icost 0.006519317626953125 0.4166717529296875\n",
            "tcost icost 0.006519317626953125 0.4166717529296875\n",
            "search 0.42333984375\n",
            "tcost icost 0.005390167236328125 0.41805097460746765\n",
            "tcost icost 0.00423431396484375 0.39978891611099243\n",
            "tcost icost 0.00423431396484375 0.40076878666877747\n",
            "search 0.405029296875\n",
            "tcost icost 0.010498046875 0.41953620314598083\n",
            "tcost icost 0.00795745849609375 0.4220431447029114\n",
            "tcost icost 0.00795745849609375 0.41737985610961914\n",
            "search 0.425537109375\n",
            "tcost icost 0.004970550537109375 0.4162784516811371\n",
            "tcost icost 0.004131317138671875 0.39460593461990356\n",
            "tcost icost 0.004131317138671875 0.3950621485710144\n",
            "search 0.399169921875\n",
            "tcost icost 0.006816864013671875 0.4181455075740814\n",
            "tcost icost 0.004100799560546875 0.39591607451438904\n",
            "tcost icost 0.004100799560546875 0.39653870463371277\n",
            "search 0.400634765625\n",
            "tcost icost 0.0037078857421875 0.39305391907691956\n",
            "tcost icost 0.004398345947265625 0.3639649748802185\n",
            "tcost icost 0.004398345947265625 0.36385777592658997\n",
            "search 0.3681640625\n",
            "tcost icost 0.0035152435302734375 0.38762933015823364\n",
            "tcost icost 0.0040283203125 0.3575730323791504\n",
            "tcost icost 0.0040283203125 0.35940495133399963\n",
            "search 0.36328125\n",
            "tcost icost 0.00377655029296875 0.41091853380203247\n",
            "tcost icost 0.00432586669921875 0.3710744082927704\n",
            "tcost icost 0.00432586669921875 0.3726750910282135\n",
            "search 0.376953125\n",
            "tcost icost 0.00414276123046875 0.4096333980560303\n",
            "tcost icost 0.004276275634765625 0.3796581029891968\n",
            "tcost icost 0.004276275634765625 0.38102829456329346\n",
            "search 0.385498046875\n",
            "tcost icost 0.0093536376953125 0.4191303551197052\n",
            "tcost icost 0.0164794921875 0.41658058762550354\n",
            "tcost icost 0.0164794921875 0.41658055782318115\n",
            "search 0.43310546875\n",
            "tcost icost 0.011474609375 0.42260757088661194\n",
            "tcost icost 0.00949859619140625 0.41887742280960083\n",
            "tcost icost 0.00949859619140625 0.41887742280960083\n",
            "search 0.428466796875\n",
            "tcost icost 0.012115478515625 0.4235548675060272\n",
            "tcost icost 0.005523681640625 0.4166108965873718\n",
            "tcost icost 0.005523681640625 0.41653743386268616\n",
            "search 0.422119140625\n",
            "tcost icost 0.007427215576171875 0.4229053556919098\n",
            "tcost icost 0.005748748779296875 0.419508695602417\n",
            "tcost icost 0.005748748779296875 0.4195306897163391\n",
            "search 0.42529296875\n",
            "tcost icost 0.0106658935546875 0.42162230610847473\n",
            "tcost icost 0.005931854248046875 0.42008548974990845\n",
            "tcost icost 0.005931854248046875 0.4201766550540924\n",
            "search 0.426025390625\n",
            "tcost icost 0.0105743408203125 0.42318278551101685\n",
            "tcost icost 0.0059051513671875 0.4203866422176361\n",
            "tcost icost 0.0059051513671875 0.42492783069610596\n",
            "search 0.430908203125\n",
            "tcost icost 0.00991058349609375 0.4222703278064728\n",
            "tcost icost 0.00559234619140625 0.41827183961868286\n",
            "tcost icost 0.00559234619140625 0.4200283885002136\n",
            "search 0.425537109375\n",
            "tcost icost 0.009490966796875 0.4236585199832916\n",
            "tcost icost 0.0171356201171875 0.4256514012813568\n",
            "tcost icost 0.0171356201171875 0.4302073121070862\n",
            "search 0.447265625\n",
            "tcost icost 0.01122283935546875 0.4328247606754303\n",
            "tcost icost 0.00937652587890625 0.4317386746406555\n",
            "tcost icost 0.00937652587890625 0.4292600154876709\n",
            "search 0.4384765625\n",
            "tcost icost 0.0108642578125 0.4314296245574951\n",
            "tcost icost 0.0059814453125 0.42805957794189453\n",
            "tcost icost 0.0059814453125 0.42563867568969727\n",
            "search 0.431640625\n",
            "tcost icost 0.021820068359375 0.42884713411331177\n",
            "tcost icost 0.006072998046875 0.4291601777076721\n",
            "tcost icost 0.006072998046875 0.42804354429244995\n",
            "search 0.43408203125\n",
            "tcost icost 0.0114898681640625 0.42921486496925354\n",
            "tcost icost 0.006015777587890625 0.42615750432014465\n",
            "tcost icost 0.006015777587890625 0.42615750432014465\n",
            "search 0.432373046875\n",
            "tcost icost 0.0134124755859375 0.43138718605041504\n",
            "tcost icost 0.00616455078125 0.42804577946662903\n",
            "tcost icost 0.00616455078125 0.42708519101142883\n",
            "search 0.43310546875\n",
            "tcost icost 0.011474609375 0.4286729693412781\n",
            "tcost icost 0.00949859619140625 0.4330579936504364\n",
            "tcost icost 0.00949859619140625 0.4282802939414978\n",
            "search 0.437744140625\n",
            "tcost icost 0.01502227783203125 0.4278976321220398\n",
            "tcost icost 0.01163482666015625 0.43788987398147583\n",
            "tcost icost 0.01163482666015625 0.4283377230167389\n",
            "search 0.43994140625\n",
            "tcost icost 0.01007843017578125 0.42894792556762695\n",
            "tcost icost 0.006473541259765625 0.42813950777053833\n",
            "tcost icost 0.006473541259765625 0.42813950777053833\n",
            "search 0.434814453125\n",
            "tcost icost 0.01052093505859375 0.429935485124588\n",
            "tcost icost 0.00661468505859375 0.4279111921787262\n",
            "tcost icost 0.00661468505859375 0.4281039834022522\n",
            "search 0.434814453125\n",
            "tcost icost 0.018829345703125 0.4280894994735718\n",
            "tcost icost 0.00635528564453125 0.4303441643714905\n",
            "tcost icost 0.00635528564453125 0.42747482657432556\n",
            "search 0.433837890625\n",
            "tcost icost 0.014495849609375 0.42873188853263855\n",
            "tcost icost 0.00624847412109375 0.4271761476993561\n",
            "tcost icost 0.00624847412109375 0.4276552200317383\n",
            "search 0.43408203125\n",
            "tcost icost 0.00907135009765625 0.4286291003227234\n",
            "tcost icost 0.0061492919921875 0.4279187023639679\n",
            "tcost icost 0.0061492919921875 0.4267072081565857\n",
            "search 0.432861328125\n",
            "tcost icost 0.01261138916015625 0.43188923597335815\n",
            "tcost icost 0.0101776123046875 0.4290076494216919\n",
            "tcost icost 0.0101776123046875 0.4297741949558258\n",
            "search 0.43994140625\n",
            "tcost icost 0.0135650634765625 0.4289133846759796\n",
            "tcost icost 0.0061798095703125 0.42706719040870667\n",
            "tcost icost 0.0061798095703125 0.4263475239276886\n",
            "search 0.432373046875\n",
            "tcost icost 0.00624847412109375 0.42702507972717285\n",
            "tcost icost 0.00624847412109375 0.42702507972717285\n",
            "tcost icost 0.00624847412109375 0.431691974401474\n",
            "search 0.43798828125\n",
            "tcost icost 0.017425537109375 0.42838433384895325\n",
            "tcost icost 0.00592803955078125 0.4263416528701782\n",
            "tcost icost 0.00592803955078125 0.4235021471977234\n",
            "search 0.429443359375\n",
            "tcost icost 0.0078582763671875 0.43563657999038696\n",
            "tcost icost 0.00598907470703125 0.42710578441619873\n",
            "tcost icost 0.00598907470703125 0.4284336268901825\n",
            "search 0.4345703125\n",
            "tcost icost 0.0174713134765625 0.43228772282600403\n",
            "tcost icost 0.00618743896484375 0.4302200973033905\n",
            "tcost icost 0.00618743896484375 0.42952436208724976\n",
            "search 0.435546875\n",
            "tcost icost 0.020172119140625 0.432159960269928\n",
            "tcost icost 0.006519317626953125 0.4302578270435333\n",
            "tcost icost 0.006519317626953125 0.4327854514122009\n",
            "search 0.439453125\n",
            "tcost icost 0.0093841552734375 0.4322172701358795\n",
            "tcost icost 0.005863189697265625 0.42761892080307007\n",
            "tcost icost 0.005863189697265625 0.42531293630599976\n",
            "search 0.43115234375\n",
            "tcost icost 0.005863189697265625 0.42529061436653137\n",
            "tcost icost 0.005863189697265625 0.42538967728614807\n",
            "tcost icost 0.005863189697265625 0.42538967728614807\n",
            "search 0.43115234375\n",
            "tcost icost 0.01096343994140625 0.43314269185066223\n",
            "tcost icost 0.005985260009765625 0.42946937680244446\n",
            "tcost icost 0.005985260009765625 0.42702922224998474\n",
            "search 0.43310546875\n",
            "tcost icost 0.00833892822265625 0.43259167671203613\n",
            "tcost icost 0.006092071533203125 0.42859187722206116\n",
            "tcost icost 0.006092071533203125 0.4286811649799347\n",
            "search 0.434814453125\n",
            "tcost icost 0.0114288330078125 0.4368637502193451\n",
            "tcost icost 0.00945281982421875 0.4330872893333435\n",
            "tcost icost 0.00945281982421875 0.4365357756614685\n",
            "search 0.446044921875\n",
            "tcost icost 0.0251007080078125 0.4343322217464447\n",
            "tcost icost 0.006183624267578125 0.4294266402721405\n",
            "tcost icost 0.006183624267578125 0.4291493594646454\n",
            "search 0.435302734375\n",
            "tcost icost 0.01776123046875 0.43376970291137695\n",
            "tcost icost 0.0062255859375 0.4298061728477478\n",
            "tcost icost 0.0062255859375 0.42938387393951416\n",
            "search 0.435546875\n",
            "tcost icost 0.019775390625 0.4332408010959625\n",
            "tcost icost 0.006011962890625 0.42668160796165466\n",
            "tcost icost 0.006011962890625 0.4271523356437683\n",
            "search 0.433349609375\n",
            "tcost icost 0.011932373046875 0.43326184153556824\n",
            "tcost icost 0.006053924560546875 0.4275747835636139\n",
            "tcost icost 0.006053924560546875 0.42772889137268066\n",
            "search 0.433837890625\n",
            "tcost icost 0.0101165771484375 0.43306663632392883\n",
            "tcost icost 0.005649566650390625 0.42281433939933777\n",
            "tcost icost 0.005649566650390625 0.4229629337787628\n",
            "search 0.428466796875\n",
            "tcost icost 0.010589599609375 0.43320396542549133\n",
            "tcost icost 0.005889892578125 0.4250052869319916\n",
            "tcost icost 0.005889892578125 0.4252513647079468\n",
            "search 0.43115234375\n",
            "tcost icost 0.0030956268310546875 0.40215903520584106\n",
            "tcost icost 0.0040435791015625 0.34668484330177307\n",
            "tcost icost 0.0040435791015625 0.3447054326534271\n",
            "search 0.348876953125\n",
            "tcost icost 0.003910064697265625 0.40521711111068726\n",
            "tcost icost 0.004489898681640625 0.34871605038642883\n",
            "tcost icost 0.004489898681640625 0.34871605038642883\n",
            "search 0.35302734375\n",
            "ded\n",
            "time\n",
            "28 #### train ####\n",
            "repr, std, cov, conv, closs 0.034366969019174576 0.33251953125 0.27992236614227295 0.056417398154735565 0.0295957550406456\n",
            "53.973386438339965 7.195806385287089 1.0\n",
            "repr, std, cov, conv, closs 0.049728501588106155 0.33251953125 0.2713014483451843 0.06608691811561584 0.031083140522241592\n",
            "54.0273598247783 7.275357214562424 1.0\n",
            "repr, std, cov, conv, closs 0.047672465443611145 0.328857421875 0.3002670705318451 0.06970488280057907 0.0003767026064451784\n",
            "54.51555559326743 7.422255005993835 1.0\n",
            "repr, std, cov, conv, closs 0.03799225389957428 0.33203125 0.3004509508609772 0.069289930164814 0.041181862354278564\n",
            "54.62464122000954 7.429677260999828 1.0\n",
            "repr, std, cov, conv, closs 0.04838591814041138 0.331298828125 0.3253890573978424 0.056253354996442795 0.00033169396920129657\n",
            "54.62464122000954 7.451988589244286 1.0\n",
            "repr, std, cov, conv, closs 0.04181808605790138 0.3349609375 0.25377631187438965 0.06206851825118065 0.0005138638662174344\n",
            "54.95320953026038 7.414840165828007 1.0\n",
            "repr, std, cov, conv, closs 0.04558667913079262 0.332763671875 0.28837019205093384 0.06219363212585449 0.0038585944566875696\n",
            "54.78867907221785 7.341097954400822 1.0\n",
            "repr, std, cov, conv, closs 0.038791559636592865 0.326904296875 0.35585129261016846 0.06356730312108994 0.0028920115437358618\n",
            "54.57007114886069 7.2899152043487625 1.0\n",
            "repr, std, cov, conv, closs 0.046995438635349274 0.32421875 0.32737451791763306 0.05770398676395416 0.00045417415094561875\n",
            "54.46109449876867 7.110015372079115 1.0\n",
            "repr, std, cov, conv, closs 0.04929538071155548 0.32861328125 0.29019027948379517 0.06427042186260223 0.0024931947700679302\n",
            "54.2980374380442 7.011217743736587 1.0\n",
            "repr, std, cov, conv, closs 0.045087218284606934 0.32470703125 0.31951096653938293 0.05544772744178772 0.01582196168601513\n",
            "54.13546857178767 6.872455009100297 1.0\n",
            "repr, std, cov, conv, closs 0.045094557106494904 0.325927734375 0.3031066656112671 0.06065046042203903 0.002069072797894478\n",
            "54.13546857178767 6.858730688991627 1.0\n",
            "repr, std, cov, conv, closs 0.06109971925616264 0.32861328125 0.324063241481781 0.05616679787635803 0.058109089732170105\n",
            "54.2980374380442 6.824539676930682 1.0\n",
            "repr, std, cov, conv, closs 0.04627900570631027 0.329345703125 0.30673396587371826 0.05432386323809624 0.00029975592042319477\n",
            "54.679265861229545 6.906886077454805 1.0\n",
            "repr, std, cov, conv, closs 0.04944483935832977 0.3291015625 0.3093864917755127 0.06823086738586426 0.0009504329646006227\n",
            "54.84346775129006 7.060443749153518 1.0\n",
            "repr, std, cov, conv, closs 0.03608406335115433 0.33203125 0.2687170207500458 0.07063926756381989 0.010081194341182709\n",
            "55.17335230750636 7.1886177675195695 1.0\n",
            "repr, std, cov, conv, closs 0.034852299839258194 0.3330078125 0.2787500321865082 0.07077615708112717 0.016394853591918945\n",
            "55.61628707340393 7.333764190210612 1.0\n",
            "repr, std, cov, conv, closs 0.04535556584596634 0.3369140625 0.27130958437919617 0.060377418994903564 0.03119168058037758\n",
            "55.89492522808266 7.429677260999828 1.0\n",
            "repr, std, cov, conv, closs 0.029621277004480362 0.337890625 0.25579380989074707 0.06989458203315735 0.0007103648968040943\n",
            "56.23113432206681 7.610055223119162 1.0\n",
            "repr, std, cov, conv, closs 0.037588585168123245 0.33251953125 0.26111435890197754 0.07364635914564133 0.000573265366256237\n",
            "56.28736545638887 7.732733588830357 1.0\n",
            "repr, std, cov, conv, closs 0.050712116062641144 0.326171875 0.3149458169937134 0.06129613518714905 0.0008721958729438484\n",
            "56.17495936270411 7.857389598720513 1.0\n",
            "repr, std, cov, conv, closs 0.04871206730604172 0.326171875 0.31052160263061523 0.06365596503019333 0.015682844445109367\n",
            "55.839086141940726 7.794812407656286 1.0\n",
            "repr, std, cov, conv, closs 0.044539641588926315 0.324951171875 0.33307987451553345 0.06152721494436264 0.02369391731917858\n",
            "55.11823407343294 7.572118832011732 1.0\n",
            "repr, std, cov, conv, closs 0.05054573714733124 0.32861328125 0.3298078179359436 0.06410162895917892 0.0010633125202730298\n",
            "54.73394512709077 7.451988589244286 1.0\n",
            "repr, std, cov, conv, closs 0.04274262860417366 0.31982421875 0.3811899721622467 0.0651426836848259 0.014659276232123375\n",
            "54.13546857178767 7.217415399057909 1.0\n",
            "repr, std, cov, conv, closs 0.03710787743330002 0.321044921875 0.38627398014068604 0.061299920082092285 0.028665687888860703\n",
            "53.650676545102584 7.09581664297652 1.0\n",
            "repr, std, cov, conv, closs 0.0416598916053772 0.3212890625 0.3297443091869354 0.057829223573207855 0.022280842065811157\n",
            "52.85231815920079 6.893092998365078 1.0\n",
            "repr, std, cov, conv, closs 0.04985349625349045 0.318603515625 0.3831838369369507 0.057981424033641815 0.003817595075815916\n",
            "52.588847507039084 6.810911043931776 1.0\n",
            "repr, std, cov, conv, closs 0.04125995934009552 0.327880859375 0.3113926351070404 0.06513191759586334 0.007951367646455765\n",
            "52.37901695554674 6.736438601454199 1.0\n",
            "repr, std, cov, conv, closs 0.035113975405693054 0.328857421875 0.31081050634384155 0.0627094954252243 0.013100966811180115\n",
            "52.69407779090066 6.729708892561638 1.0\n",
            "repr, std, cov, conv, closs 0.04271731898188591 0.32763671875 0.2818841338157654 0.06439090520143509 0.0006017228006385267\n",
            "53.81178958041819 6.838195580824219 1.0\n",
            "repr, std, cov, conv, closs 0.04860306531190872 0.33154296875 0.3215867877006531 0.06646893918514252 0.0004910877905786037\n",
            "54.2437936443998 6.858730688991627 1.0\n",
            "repr, std, cov, conv, closs 0.034833088517189026 0.3330078125 0.29207897186279297 0.06849254667758942 0.0011158108245581388\n",
            "54.84346775129006 7.053390358794724 1.0\n",
            "repr, std, cov, conv, closs 0.04394655302166939 0.32958984375 0.3230159878730774 0.07047653943300247 0.0005447104922495782\n",
            "55.22852565981386 7.16709497414512 1.0\n",
            "repr, std, cov, conv, closs 0.035342611372470856 0.338134765625 0.24010193347930908 0.06311117112636566 0.015383497811853886\n",
            "56.343652821845254 7.370506422177882 1.0\n",
            "repr, std, cov, conv, closs 0.04152405261993408 0.338134765625 0.2565256953239441 0.06877033412456512 0.000947131309658289\n",
            "56.56936572048049 7.481841285348203 1.0\n",
            "tcost icost 0.0151824951171875 0.4235573709011078\n",
            "tcost icost 0.0106353759765625 0.4196932315826416\n",
            "tcost icost 0.0106353759765625 0.4196931719779968\n",
            "search 0.430419921875\n",
            "tcost icost 0.0270538330078125 0.41868266463279724\n",
            "tcost icost 0.0106353759765625 0.417079359292984\n",
            "tcost icost 0.0106353759765625 0.41873496770858765\n",
            "search 0.429443359375\n",
            "tcost icost 0.0151824951171875 0.41621553897857666\n",
            "tcost icost 0.0106353759765625 0.4144790768623352\n",
            "tcost icost 0.0106353759765625 0.41470152139663696\n",
            "search 0.425537109375\n",
            "tcost icost 0.0106353759765625 0.41326475143432617\n",
            "tcost icost 0.0106353759765625 0.4124191999435425\n",
            "tcost icost 0.0106353759765625 0.41216006875038147\n",
            "search 0.4228515625\n",
            "tcost icost 0.0162506103515625 0.4158695936203003\n",
            "tcost icost 0.0106353759765625 0.4101369380950928\n",
            "tcost icost 0.0106353759765625 0.4128463566303253\n",
            "search 0.423583984375\n",
            "tcost icost 0.0132293701171875 0.4105730950832367\n",
            "tcost icost 0.0106353759765625 0.40862953662872314\n",
            "tcost icost 0.0106353759765625 0.4086550176143646\n",
            "search 0.41943359375\n",
            "tcost icost 0.0132293701171875 0.4108532965183258\n",
            "tcost icost 0.0106353759765625 0.4059995114803314\n",
            "tcost icost 0.0106353759765625 0.4105640649795532\n",
            "search 0.42138671875\n",
            "tcost icost 0.02423095703125 0.40452930331230164\n",
            "tcost icost 0.01490020751953125 0.40621331334114075\n",
            "tcost icost 0.01490020751953125 0.40423494577407837\n",
            "search 0.419189453125\n",
            "tcost icost 0.0132293701171875 0.40381351113319397\n",
            "tcost icost 0.0106353759765625 0.40371719002723694\n",
            "tcost icost 0.0106353759765625 0.4037172198295593\n",
            "search 0.41455078125\n",
            "tcost icost 0.0270538330078125 0.40398284792900085\n",
            "tcost icost 0.0106353759765625 0.40564480423927307\n",
            "tcost icost 0.0106353759765625 0.40336254239082336\n",
            "search 0.4140625\n",
            "tcost icost 0.0132293701171875 0.40107160806655884\n",
            "tcost icost 0.0106353759765625 0.40235522389411926\n",
            "tcost icost 0.0106353759765625 0.40078112483024597\n",
            "search 0.41162109375\n",
            "tcost icost 0.02423095703125 0.39918532967567444\n",
            "tcost icost 0.01490020751953125 0.4007066786289215\n",
            "tcost icost 0.01490020751953125 0.40110617876052856\n",
            "search 0.416015625\n",
            "tcost icost 0.0151824951171875 0.4001830816268921\n",
            "tcost icost 0.0106353759765625 0.4004206657409668\n",
            "tcost icost 0.0106353759765625 0.3973635137081146\n",
            "search 0.408203125\n",
            "tcost icost 0.01490020751953125 0.39618802070617676\n",
            "tcost icost 0.01490020751953125 0.39552581310272217\n",
            "tcost icost 0.01490020751953125 0.39546269178390503\n",
            "search 0.410400390625\n",
            "tcost icost 0.0132293701171875 0.3930160403251648\n",
            "tcost icost 0.0106353759765625 0.3928714394569397\n",
            "tcost icost 0.0106353759765625 0.39772915840148926\n",
            "search 0.408447265625\n",
            "tcost icost 0.0162506103515625 0.39300528168678284\n",
            "tcost icost 0.0106353759765625 0.3954232633113861\n",
            "tcost icost 0.0106353759765625 0.395508348941803\n",
            "search 0.40625\n",
            "tcost icost 0.0132293701171875 0.3906310796737671\n",
            "tcost icost 0.0106353759765625 0.3912915587425232\n",
            "tcost icost 0.0106353759765625 0.39094382524490356\n",
            "search 0.401611328125\n",
            "tcost icost 0.0106353759765625 0.39016100764274597\n",
            "tcost icost 0.0106353759765625 0.38903477787971497\n",
            "tcost icost 0.0106353759765625 0.3908899426460266\n",
            "search 0.401611328125\n",
            "tcost icost 0.0151824951171875 0.3859978914260864\n",
            "tcost icost 0.0106353759765625 0.38675251603126526\n",
            "tcost icost 0.0106353759765625 0.38903477787971497\n",
            "search 0.399658203125\n",
            "tcost icost 0.0162506103515625 0.39085015654563904\n",
            "tcost icost 0.0106353759765625 0.3886893689632416\n",
            "tcost icost 0.0106353759765625 0.39355969429016113\n",
            "search 0.404296875\n",
            "tcost icost 0.02423095703125 0.38599130511283875\n",
            "tcost icost 0.01490020751953125 0.386383593082428\n",
            "tcost icost 0.01490020751953125 0.38638362288475037\n",
            "search 0.4013671875\n",
            "tcost icost 0.0106353759765625 0.38671284914016724\n",
            "tcost icost 0.020721435546875 0.39022520184516907\n",
            "tcost icost 0.020721435546875 0.3877117335796356\n",
            "search 0.408447265625\n",
            "tcost icost 0.0132293701171875 0.38572958111763\n",
            "tcost icost 0.0106353759765625 0.38760077953338623\n",
            "tcost icost 0.0106353759765625 0.3912774324417114\n",
            "search 0.402099609375\n",
            "tcost icost 0.0162506103515625 0.3860699534416199\n",
            "tcost icost 0.0106353759765625 0.38443055748939514\n",
            "tcost icost 0.0106353759765625 0.38443055748939514\n",
            "search 0.395263671875\n",
            "tcost icost 0.0151824951171875 0.3806578516960144\n",
            "tcost icost 0.0106353759765625 0.3816942870616913\n",
            "tcost icost 0.0106353759765625 0.3808547556400299\n",
            "search 0.3916015625\n",
            "tcost icost 0.030426025390625 0.38068342208862305\n",
            "tcost icost 0.01085662841796875 0.3861808478832245\n",
            "tcost icost 0.01085662841796875 0.38113170862197876\n",
            "search 0.391845703125\n",
            "tcost icost 0.01085662841796875 0.37707245349884033\n",
            "tcost icost 0.0232086181640625 0.37491554021835327\n",
            "tcost icost 0.0232086181640625 0.37972840666770935\n",
            "search 0.40283203125\n",
            "tcost icost 0.0140228271484375 0.3756391108036041\n",
            "tcost icost 0.0111846923828125 0.3770328164100647\n",
            "tcost icost 0.0111846923828125 0.3744391202926636\n",
            "search 0.3857421875\n",
            "tcost icost 0.0215911865234375 0.3749571144580841\n",
            "tcost icost 0.017181396484375 0.3728009760379791\n",
            "tcost icost 0.017181396484375 0.37227076292037964\n",
            "search 0.389404296875\n",
            "tcost icost 0.0157012939453125 0.37091904878616333\n",
            "tcost icost 0.0107574462890625 0.37483125925064087\n",
            "tcost icost 0.0107574462890625 0.3752332329750061\n",
            "search 0.385986328125\n",
            "tcost icost 0.0171356201171875 0.3691503405570984\n",
            "tcost icost 0.0107574462890625 0.37483125925064087\n",
            "tcost icost 0.0107574462890625 0.3729666471481323\n",
            "search 0.3837890625\n",
            "tcost icost 0.0126190185546875 0.3691171705722809\n",
            "tcost icost 0.0107574462890625 0.3704231381416321\n",
            "tcost icost 0.0107574462890625 0.3727916479110718\n",
            "search 0.383544921875\n",
            "tcost icost 0.0215606689453125 0.365144282579422\n",
            "tcost icost 0.0107574462890625 0.3707468807697296\n",
            "tcost icost 0.0107574462890625 0.37020373344421387\n",
            "search 0.380859375\n",
            "tcost icost 0.02252197265625 0.36993473768234253\n",
            "tcost icost 0.0113525390625 0.37327247858047485\n",
            "tcost icost 0.0113525390625 0.3709944486618042\n",
            "search 0.38232421875\n",
            "tcost icost 0.0193328857421875 0.37297746539115906\n",
            "tcost icost 0.0193328857421875 0.3746916651725769\n",
            "tcost icost 0.0193328857421875 0.3706320524215698\n",
            "search 0.389892578125\n",
            "tcost icost 0.01219940185546875 0.3666818141937256\n",
            "tcost icost 0.013824462890625 0.3676041066646576\n",
            "tcost icost 0.013824462890625 0.3649185597896576\n",
            "search 0.37890625\n",
            "tcost icost 0.0123748779296875 0.3641780614852905\n",
            "tcost icost 0.0182342529296875 0.3611851632595062\n",
            "tcost icost 0.0182342529296875 0.36422520875930786\n",
            "search 0.382568359375\n",
            "tcost icost 0.01102447509765625 0.366031676530838\n",
            "tcost icost 0.027679443359375 0.3625072240829468\n",
            "tcost icost 0.027679443359375 0.36760005354881287\n",
            "search 0.395263671875\n",
            "tcost icost 0.0202178955078125 0.3636939227581024\n",
            "tcost icost 0.0165252685546875 0.3661123812198639\n",
            "tcost icost 0.0165252685546875 0.36643412709236145\n",
            "search 0.383056640625\n",
            "tcost icost 0.028900146484375 0.371249794960022\n",
            "tcost icost 0.01312255859375 0.3687269389629364\n",
            "tcost icost 0.01312255859375 0.3709949254989624\n",
            "search 0.38427734375\n",
            "tcost icost 0.028778076171875 0.3730708062648773\n",
            "tcost icost 0.01290130615234375 0.3731021285057068\n",
            "tcost icost 0.01290130615234375 0.3708309531211853\n",
            "search 0.3837890625\n",
            "tcost icost 0.0229644775390625 0.3722861707210541\n",
            "tcost icost 0.01141357421875 0.3706313967704773\n",
            "tcost icost 0.01141357421875 0.37302032113075256\n",
            "search 0.384521484375\n",
            "tcost icost 0.0217742919921875 0.3710513710975647\n",
            "tcost icost 0.011322021484375 0.3714407682418823\n",
            "tcost icost 0.011322021484375 0.3691461682319641\n",
            "search 0.38037109375\n",
            "tcost icost 0.021453857421875 0.3723314702510834\n",
            "tcost icost 0.017486572265625 0.37272194027900696\n",
            "tcost icost 0.017486572265625 0.37041354179382324\n",
            "search 0.387939453125\n",
            "tcost icost 0.028228759765625 0.3667439818382263\n",
            "tcost icost 0.01099395751953125 0.37173107266426086\n",
            "tcost icost 0.01099395751953125 0.36943578720092773\n",
            "search 0.38037109375\n",
            "tcost icost 0.0165252685546875 0.36768069863319397\n",
            "tcost icost 0.01117706298828125 0.37074241042137146\n",
            "tcost icost 0.01117706298828125 0.3736012876033783\n",
            "search 0.384765625\n",
            "tcost icost 0.01108551025390625 0.36863380670547485\n",
            "tcost icost 0.0271148681640625 0.3688794672489166\n",
            "tcost icost 0.0271148681640625 0.36675316095352173\n",
            "search 0.393798828125\n",
            "tcost icost 0.01495361328125 0.3686177134513855\n",
            "tcost icost 0.01189422607421875 0.37235668301582336\n",
            "tcost icost 0.01189422607421875 0.3696838617324829\n",
            "search 0.381591796875\n",
            "tcost icost 0.0189361572265625 0.36493757367134094\n",
            "tcost icost 0.0147552490234375 0.3676583170890808\n",
            "tcost icost 0.0147552490234375 0.3676583170890808\n",
            "search 0.38232421875\n",
            "tcost icost 0.011444091796875 0.37340015172958374\n",
            "tcost icost 0.01509857177734375 0.36596325039863586\n",
            "tcost icost 0.01509857177734375 0.3682907521724701\n",
            "search 0.383544921875\n",
            "tcost icost 0.0126190185546875 0.3699672818183899\n",
            "tcost icost 0.0162811279296875 0.36625298857688904\n",
            "tcost icost 0.0162811279296875 0.3686964213848114\n",
            "search 0.385009765625\n",
            "tcost icost 0.0179443359375 0.36404719948768616\n",
            "tcost icost 0.0110931396484375 0.3724652826786041\n",
            "tcost icost 0.0110931396484375 0.37053748965263367\n",
            "search 0.381591796875\n",
            "tcost icost 0.01418304443359375 0.3687479794025421\n",
            "tcost icost 0.0138092041015625 0.3702239692211151\n",
            "tcost icost 0.0138092041015625 0.3699702322483063\n",
            "search 0.3837890625\n",
            "tcost icost 0.0135040283203125 0.3739526569843292\n",
            "tcost icost 0.0179901123046875 0.3707832992076874\n",
            "tcost icost 0.0179901123046875 0.3672879934310913\n",
            "search 0.38525390625\n",
            "tcost icost 0.022125244140625 0.37270593643188477\n",
            "tcost icost 0.01165771484375 0.3753133714199066\n",
            "tcost icost 0.01165771484375 0.3745283782482147\n",
            "search 0.38623046875\n",
            "tcost icost 0.020172119140625 0.3764035105705261\n",
            "tcost icost 0.013763427734375 0.3806160092353821\n",
            "tcost icost 0.013763427734375 0.3804799020290375\n",
            "search 0.39404296875\n",
            "tcost icost 0.01552581787109375 0.378788024187088\n",
            "tcost icost 0.0261077880859375 0.37500083446502686\n",
            "tcost icost 0.0261077880859375 0.3720218241214752\n",
            "search 0.398193359375\n",
            "tcost icost 0.0135040283203125 0.3769700527191162\n",
            "tcost icost 0.0176849365234375 0.37329524755477905\n",
            "tcost icost 0.0176849365234375 0.37349987030029297\n",
            "search 0.39111328125\n",
            "tcost icost 0.0223846435546875 0.3720940947532654\n",
            "tcost icost 0.01244354248046875 0.37969455122947693\n",
            "tcost icost 0.01244354248046875 0.37859752774238586\n",
            "search 0.39111328125\n",
            "tcost icost 0.01532745361328125 0.3742848336696625\n",
            "tcost icost 0.0200958251953125 0.37308788299560547\n",
            "tcost icost 0.0200958251953125 0.3758573532104492\n",
            "search 0.39599609375\n",
            "tcost icost 0.02264404296875 0.37350139021873474\n",
            "tcost icost 0.0135040283203125 0.38046103715896606\n",
            "tcost icost 0.0135040283203125 0.3816659450531006\n",
            "search 0.39501953125\n",
            "tcost icost 0.0156097412109375 0.3749873638153076\n",
            "tcost icost 0.0257568359375 0.37127596139907837\n",
            "tcost icost 0.0257568359375 0.3715116083621979\n",
            "search 0.3974609375\n",
            "tcost icost 0.0197601318359375 0.37725675106048584\n",
            "tcost icost 0.01528167724609375 0.3792942762374878\n",
            "tcost icost 0.01528167724609375 0.3792942762374878\n",
            "search 0.394775390625\n",
            "tcost icost 0.0263519287109375 0.3749401271343231\n",
            "tcost icost 0.0224609375 0.37805986404418945\n",
            "tcost icost 0.0224609375 0.3778983950614929\n",
            "search 0.400390625\n",
            "tcost icost 0.01558685302734375 0.3815140426158905\n",
            "tcost icost 0.0214385986328125 0.37928926944732666\n",
            "tcost icost 0.0214385986328125 0.3798908591270447\n",
            "search 0.4013671875\n",
            "tcost icost 0.0267486572265625 0.3770856261253357\n",
            "tcost icost 0.0130615234375 0.38546761870384216\n",
            "tcost icost 0.0130615234375 0.3851448893547058\n",
            "search 0.3984375\n",
            "tcost icost 0.0243377685546875 0.37906092405319214\n",
            "tcost icost 0.017242431640625 0.3840493857860565\n",
            "tcost icost 0.017242431640625 0.38101187348365784\n",
            "search 0.3984375\n",
            "tcost icost 0.0159912109375 0.3803253173828125\n",
            "tcost icost 0.022735595703125 0.37814855575561523\n",
            "tcost icost 0.022735595703125 0.3807605803012848\n",
            "search 0.403564453125\n",
            "tcost icost 0.0264892578125 0.37957239151000977\n",
            "tcost icost 0.01358795166015625 0.38800477981567383\n",
            "tcost icost 0.01358795166015625 0.3856331706047058\n",
            "search 0.3994140625\n",
            "tcost icost 0.0159149169921875 0.3835211992263794\n",
            "tcost icost 0.0226287841796875 0.3818667232990265\n",
            "tcost icost 0.0226287841796875 0.3819565773010254\n",
            "search 0.404541015625\n",
            "tcost icost 0.01386260986328125 0.38478440046310425\n",
            "tcost icost 0.017181396484375 0.3831162750720978\n",
            "tcost icost 0.017181396484375 0.380764365196228\n",
            "search 0.39794921875\n",
            "tcost icost 0.0244140625 0.3854861259460449\n",
            "tcost icost 0.01496124267578125 0.39265599846839905\n",
            "tcost icost 0.01496124267578125 0.3914976418018341\n",
            "search 0.406494140625\n",
            "tcost icost 0.0220184326171875 0.38697630167007446\n",
            "tcost icost 0.01453399658203125 0.39136743545532227\n",
            "tcost icost 0.01453399658203125 0.3891632854938507\n",
            "search 0.40380859375\n",
            "tcost icost 0.01535797119140625 0.38983723521232605\n",
            "tcost icost 0.021881103515625 0.3863893747329712\n",
            "tcost icost 0.021881103515625 0.3878025412559509\n",
            "search 0.40966796875\n",
            "tcost icost 0.02008056640625 0.38435524702072144\n",
            "tcost icost 0.01374053955078125 0.38920193910598755\n",
            "tcost icost 0.01374053955078125 0.39191359281539917\n",
            "search 0.405517578125\n",
            "tcost icost 0.01258087158203125 0.38942858576774597\n",
            "tcost icost 0.0151214599609375 0.3861214220523834\n",
            "tcost icost 0.0151214599609375 0.38686710596084595\n",
            "search 0.402099609375\n",
            "tcost icost 0.0164031982421875 0.392591655254364\n",
            "tcost icost 0.0202484130859375 0.38743120431900024\n",
            "tcost icost 0.0202484130859375 0.3890754282474518\n",
            "search 0.409423828125\n",
            "tcost icost 0.01537322998046875 0.38864442706108093\n",
            "tcost icost 0.0119781494140625 0.3937319815158844\n",
            "tcost icost 0.0119781494140625 0.39406818151474\n",
            "search 0.406005859375\n",
            "tcost icost 0.01387786865234375 0.391200989484787\n",
            "tcost icost 0.01654052734375 0.3924018144607544\n",
            "tcost icost 0.01654052734375 0.3895193934440613\n",
            "search 0.406005859375\n",
            "tcost icost 0.01480865478515625 0.38830527663230896\n",
            "tcost icost 0.01561737060546875 0.3883329927921295\n",
            "tcost icost 0.01561737060546875 0.3887816071510315\n",
            "search 0.404296875\n",
            "tcost icost 0.0211334228515625 0.3884199261665344\n",
            "tcost icost 0.01259613037109375 0.3963678479194641\n",
            "tcost icost 0.01259613037109375 0.39746102690696716\n",
            "search 0.41015625\n",
            "tcost icost 0.0163116455078125 0.3932898938655853\n",
            "tcost icost 0.0154266357421875 0.39379462599754333\n",
            "tcost icost 0.0154266357421875 0.3919118344783783\n",
            "search 0.4072265625\n",
            "tcost icost 0.01323699951171875 0.3935997784137726\n",
            "tcost icost 0.0186309814453125 0.38881969451904297\n",
            "tcost icost 0.0186309814453125 0.3908856213092804\n",
            "search 0.409423828125\n",
            "tcost icost 0.0169830322265625 0.3913302421569824\n",
            "tcost icost 0.01314544677734375 0.3985864520072937\n",
            "tcost icost 0.01314544677734375 0.3964976370334625\n",
            "search 0.40966796875\n",
            "tcost icost 0.0155181884765625 0.39627063274383545\n",
            "tcost icost 0.024932861328125 0.3906272351741791\n",
            "tcost icost 0.024932861328125 0.39541757106781006\n",
            "search 0.42041015625\n",
            "tcost icost 0.018096923828125 0.392551451921463\n",
            "tcost icost 0.0131988525390625 0.39821502566337585\n",
            "tcost icost 0.0131988525390625 0.39821240305900574\n",
            "search 0.411376953125\n",
            "tcost icost 0.022552490234375 0.3919602334499359\n",
            "tcost icost 0.0189971923828125 0.39623144268989563\n",
            "tcost icost 0.0189971923828125 0.3934248387813568\n",
            "search 0.412353515625\n",
            "tcost icost 0.01546478271484375 0.39434710144996643\n",
            "tcost icost 0.0201568603515625 0.39026010036468506\n",
            "tcost icost 0.0201568603515625 0.3908022344112396\n",
            "search 0.4111328125\n",
            "tcost icost 0.01485443115234375 0.3925561308860779\n",
            "tcost icost 0.01220703125 0.40011516213417053\n",
            "tcost icost 0.01220703125 0.397809237241745\n",
            "search 0.409912109375\n",
            "tcost icost 0.01500701904296875 0.3942858874797821\n",
            "tcost icost 0.01357269287109375 0.3986787796020508\n",
            "tcost icost 0.01357269287109375 0.39851388335227966\n",
            "search 0.412109375\n",
            "tcost icost 0.01409149169921875 0.39501383900642395\n",
            "tcost icost 0.021087646484375 0.39269953966140747\n",
            "tcost icost 0.021087646484375 0.3953758478164673\n",
            "search 0.416259765625\n",
            "tcost icost 0.0266571044921875 0.3909228444099426\n",
            "tcost icost 0.018768310546875 0.39864426851272583\n",
            "tcost icost 0.018768310546875 0.39264562726020813\n",
            "search 0.411376953125\n",
            "tcost icost 0.022003173828125 0.3922857344150543\n",
            "tcost icost 0.0139312744140625 0.3991750180721283\n",
            "tcost icost 0.0139312744140625 0.39635714888572693\n",
            "search 0.41015625\n",
            "tcost icost 0.0181884765625 0.3967602252960205\n",
            "tcost icost 0.014495849609375 0.4010120630264282\n",
            "tcost icost 0.014495849609375 0.40114831924438477\n",
            "search 0.41552734375\n",
            "tcost icost 0.01482391357421875 0.3933805227279663\n",
            "tcost icost 0.0126495361328125 0.4010271728038788\n",
            "tcost icost 0.0126495361328125 0.3990177512168884\n",
            "search 0.41162109375\n",
            "tcost icost 0.017547607421875 0.3945225477218628\n",
            "tcost icost 0.01629638671875 0.3964318633079529\n",
            "tcost icost 0.01629638671875 0.39494451880455017\n",
            "search 0.411376953125\n",
            "tcost icost 0.0306243896484375 0.3940954804420471\n",
            "tcost icost 0.0147705078125 0.40148770809173584\n",
            "tcost icost 0.0147705078125 0.3983723819255829\n",
            "search 0.4130859375\n",
            "tcost icost 0.0173797607421875 0.3981778621673584\n",
            "tcost icost 0.013458251953125 0.40224504470825195\n",
            "tcost icost 0.013458251953125 0.39976274967193604\n",
            "search 0.4130859375\n",
            "tcost icost 0.026153564453125 0.3972116708755493\n",
            "tcost icost 0.0136566162109375 0.40297746658325195\n",
            "tcost icost 0.0136566162109375 0.3993237018585205\n",
            "search 0.4130859375\n",
            "tcost icost 0.0238037109375 0.3983657658100128\n",
            "tcost icost 0.0148162841796875 0.4008365869522095\n",
            "tcost icost 0.0148162841796875 0.39907366037368774\n",
            "search 0.4140625\n",
            "tcost icost 0.019775390625 0.39813661575317383\n",
            "tcost icost 0.0131683349609375 0.4045850932598114\n",
            "tcost icost 0.0131683349609375 0.40213915705680847\n",
            "search 0.415283203125\n",
            "tcost icost 0.0189208984375 0.4012332558631897\n",
            "tcost icost 0.013397216796875 0.40530750155448914\n",
            "tcost icost 0.013397216796875 0.40529000759124756\n",
            "search 0.418701171875\n",
            "tcost icost 0.02117919921875 0.4042900800704956\n",
            "tcost icost 0.0137939453125 0.40590882301330566\n",
            "tcost icost 0.0137939453125 0.4081551134586334\n",
            "search 0.421875\n",
            "tcost icost 0.024169921875 0.4046841263771057\n",
            "tcost icost 0.0158233642578125 0.4084770083427429\n",
            "tcost icost 0.0158233642578125 0.4060126543045044\n",
            "search 0.421875\n",
            "tcost icost 0.01506805419921875 0.40438181161880493\n",
            "tcost icost 0.01300811767578125 0.4072969853878021\n",
            "tcost icost 0.01300811767578125 0.40779513120651245\n",
            "search 0.420654296875\n",
            "tcost icost 0.0115966796875 0.404845654964447\n",
            "tcost icost 0.013031005859375 0.40324047207832336\n",
            "tcost icost 0.013031005859375 0.4036446809768677\n",
            "search 0.41650390625\n",
            "tcost icost 0.0174560546875 0.40854236483573914\n",
            "tcost icost 0.022064208984375 0.40817975997924805\n",
            "tcost icost 0.022064208984375 0.40697911381721497\n",
            "search 0.428955078125\n",
            "tcost icost 0.0156707763671875 0.4111534655094147\n",
            "tcost icost 0.0219268798828125 0.40302222967147827\n",
            "tcost icost 0.0219268798828125 0.4024859368801117\n",
            "search 0.424560546875\n",
            "tcost icost 0.014068603515625 0.40438684821128845\n",
            "tcost icost 0.0163726806640625 0.40181973576545715\n",
            "tcost icost 0.0163726806640625 0.40361031889915466\n",
            "search 0.419921875\n",
            "tcost icost 0.01177978515625 0.4093434810638428\n",
            "tcost icost 0.0149078369140625 0.4058922529220581\n",
            "tcost icost 0.0149078369140625 0.40525540709495544\n",
            "search 0.420166015625\n",
            "ded\n",
            "time\n",
            "29 #### train ####\n",
            "repr, std, cov, conv, closs 0.038466427475214005 0.333740234375 0.2771410048007965 0.055798422545194626 0.014948442578315735\n",
            "57.423871806136994 7.640541134790805 1.0\n",
            "repr, std, cov, conv, closs 0.03947566822171211 0.33251953125 0.27670687437057495 0.056040894240140915 0.015966692939400673\n",
            "57.76927754439001 7.6864991426420435 1.0\n",
            "repr, std, cov, conv, closs 0.05233405530452728 0.327392578125 0.28999876976013184 0.05559363216161728 0.030051395297050476\n",
            "57.653912066345264 7.579690950843743 1.0\n",
            "repr, std, cov, conv, closs 0.049305375665426254 0.322998046875 0.3609405755996704 0.06676924228668213 0.02156335487961769\n",
            "57.30919610473144 7.496812449760183 1.0\n",
            "repr, std, cov, conv, closs 0.041969411075115204 0.323974609375 0.3275294005870819 0.06284819543361664 0.01565452665090561\n",
            "56.062777744437504 7.282632571776986 1.0\n",
            "repr, std, cov, conv, closs 0.04145583510398865 0.3251953125 0.3154817223548889 0.06594832986593246 0.02220655046403408\n",
            "55.56072634705688 7.159935039106014 1.0\n",
            "repr, std, cov, conv, closs 0.05543459951877594 0.3203125 0.40407079458236694 0.07302896678447723 0.024990368634462357\n",
            "54.46109449876867 6.941489645806252 1.0\n",
            "repr, std, cov, conv, closs 0.041874125599861145 0.322021484375 0.3274407386779785 0.07544147968292236 0.0308917798101902\n",
            "53.973386438339965 6.838195580824219 1.0\n",
            "repr, std, cov, conv, closs 0.05234936624765396 0.324462890625 0.33975541591644287 0.06012018769979477 0.0160643570125103\n",
            "53.650676545102584 6.676112685591701 1.0\n",
            "repr, std, cov, conv, closs 0.04205527901649475 0.32568359375 0.3486916422843933 0.06364671885967255 0.016674213111400604\n",
            "53.650676545102584 6.669443242349352 1.0\n",
            "repr, std, cov, conv, closs 0.04298825562000275 0.32958984375 0.32107463479042053 0.06604930758476257 0.00038949644658714533\n",
            "54.352335475482235 6.702857219721306 1.0\n",
            "repr, std, cov, conv, closs 0.03959781304001808 0.330810546875 0.28346312046051025 0.06395573914051056 0.014473567716777325\n",
            "54.62464122000954 6.804106936994782 1.0\n",
            "repr, std, cov, conv, closs 0.032914940267801285 0.33837890625 0.28052425384521484 0.0635933130979538 0.010639924556016922\n",
            "55.449771354576384 7.011217743736587 1.0\n",
            "repr, std, cov, conv, closs 0.045385174453258514 0.337158203125 0.2333976924419403 0.0644875168800354 0.015224638395011425\n",
            "56.062777744437504 7.138498122106832 1.0\n",
            "repr, std, cov, conv, closs 0.046458806842565536 0.334228515625 0.24983787536621094 0.08085410296916962 0.015452447347342968\n",
            "56.73924358230845 7.363143278898984 1.0\n",
            "repr, std, cov, conv, closs 0.04541816562414169 0.33544921875 0.26863613724708557 0.07011272758245468 0.00019646402506623417\n",
            "57.13761179936037 7.481841285348203 1.0\n",
            "repr, std, cov, conv, closs 0.04371538758277893 0.3330078125 0.2901577949523926 0.0618644654750824 0.020372778177261353\n",
            "57.13761179936037 7.610055223119162 1.0\n",
            "repr, std, cov, conv, closs 0.05778367444872856 0.330078125 0.31413519382476807 0.0588088184595108 0.0004246005555614829\n",
            "57.08053126809229 7.640541134790805 1.0\n",
            "repr, std, cov, conv, closs 0.037828389555215836 0.32861328125 0.3749591112136841 0.06306900084018707 0.05781244486570358\n",
            "56.343652821845254 7.504309262209943 1.0\n",
            "repr, std, cov, conv, closs 0.04253200441598892 0.332275390625 0.31250426173210144 0.06016213446855545 0.03452644869685173\n",
            "56.118840522181934 7.385254805528658 1.0\n",
            "repr, std, cov, conv, closs 0.05379801243543625 0.322509765625 0.3555344343185425 0.06519730389118195 0.019858412444591522\n",
            "55.78330283910163 7.174262069119264 1.0\n",
            "repr, std, cov, conv, closs 0.04595215618610382 0.32373046875 0.3577454090118408 0.0626865029335022 0.015999816358089447\n",
            "55.7275752638378 7.074571697095573 1.0\n",
            "repr, std, cov, conv, closs 0.04386918991804123 0.326416015625 0.32242971658706665 0.062057703733444214 0.01540603581815958\n",
            "54.73394512709077 6.906886077454805 1.0\n",
            "repr, std, cov, conv, closs 0.04321541637182236 0.3251953125 0.3218126893043518 0.06177627295255661 0.0003104759380221367\n",
            "54.40668781095771 6.824539676930682 1.0\n",
            "repr, std, cov, conv, closs 0.04184165596961975 0.326171875 0.3091692328453064 0.058581724762916565 0.00023691933893132955\n",
            "54.13546857178767 6.722985906654984 1.0\n",
            "repr, std, cov, conv, closs 0.04589574411511421 0.32861328125 0.3148902654647827 0.05734734982252121 0.0016037593595683575\n",
            "54.51555559326743 6.709560076941027 1.0\n",
            "repr, std, cov, conv, closs 0.04385332018136978 0.3291015625 0.3042728006839752 0.056313008069992065 0.011259949766099453\n",
            "54.95320953026038 6.7634248014441125 1.0\n",
            "repr, std, cov, conv, closs 0.03989563137292862 0.331298828125 0.25826090574264526 0.06474914401769638 0.02594936080276966\n",
            "55.008162739790635 6.831364216607612 1.0\n",
            "repr, std, cov, conv, closs 0.03693719953298569 0.331787109375 0.28294751048088074 0.056142158806324005 0.015999173745512962\n",
            "55.17335230750636 6.962334946154095 1.0\n",
            "repr, std, cov, conv, closs 0.030375724658370018 0.33447265625 0.2471417933702469 0.06770750880241394 0.02560495026409626\n",
            "55.11823407343294 7.053390358794724 1.0\n",
            "repr, std, cov, conv, closs 0.03598549962043762 0.330322265625 0.3115161657333374 0.04970850050449371 0.0005920643452554941\n",
            "55.95082015331074 7.174262069119264 1.0\n",
            "repr, std, cov, conv, closs 0.039021000266075134 0.336181640625 0.2735431492328644 0.06309972703456879 0.002677932847291231\n",
            "56.062777744437504 7.304502324672662 1.0\n",
            "repr, std, cov, conv, closs 0.031682707369327545 0.337890625 0.2518002986907959 0.057945773005485535 0.0019612987525761127\n",
            "56.85277880871663 7.5118135714721515 1.0\n",
            "repr, std, cov, conv, closs 0.048278652131557465 0.33544921875 0.2547282874584198 0.061261288821697235 0.0006822954164817929\n",
            "57.08053126809229 7.6024527703488145 1.0\n",
            "repr, std, cov, conv, closs 0.04510435461997986 0.333251953125 0.24040432274341583 0.07031062990427017 0.0580255426466465\n",
            "57.02350776033196 7.748206788741605 1.0\n",
            "repr, std, cov, conv, closs 0.04327746853232384 0.3291015625 0.30191919207572937 0.06429348140954971 0.0005810436559841037\n",
            "57.02350776033196 7.725008580250108 1.0\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(10):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "    c_loader = make_weighted(buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    # train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "8dbbd2d3-e929-49c6-c03a-2069da1ac0ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "4ad29030-4c15-4a86-902d-ecce152091f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240823_103908-3o415bps</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/3o415bps' target=\"_blank\">frosty-armadillo-21</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/3o415bps' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/3o415bps</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.cuda.amp.autocast():\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.cuda.amp.autocast():\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjm2kV3H7ZVR",
        "outputId": "d4040132-28f1-4347-8028-2e951476da85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6872065\n"
          ]
        }
      ],
      "source": [
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.cuda.amp.autocast(): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ],
      "metadata": {
        "id": "gJ3X_hQelW2x",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi4ODp-XlZoU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "QYbOgNoZn6JL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kcajtpjr7Io",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.cuda.amp.autocast(): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wUhKd009Qvk3"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}