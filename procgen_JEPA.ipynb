{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "94d22bec-ba35-42a3-d037-1b7fbd6b5fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "53a72602-ec5b-490e-9376-dce488d21d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "9ff134e5-8d6e-4fbe-89b0-4954a26f6c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-3df1573f24c0>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TCost\n",
        "class TCost(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=256): # in_dim=(1+self.jepa.pred.num_layers)*d_model\n",
        "        super().__init__()\n",
        "        self.tc = torch.tensor([-1., 0.], device=device).T\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            nn.Linear(in_dim, 2), nn.Softmax(),\n",
        "            # nn.Linear(in_dim, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, 2), nn.Softmax(),\n",
        "            )\n",
        "        # self.update_loss_weight(train_data)\n",
        "\n",
        "    def update_loss_weight(self, train_data):\n",
        "        a = len(train_data.buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "        # self.data = [step for episode in buffer for step in episode]\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tcost(x)@self.tc\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        out = self.tcost(x)\n",
        "        # print(\"ctost loss\", out, y)\n",
        "        y = torch.where(y < -0.5, 0, 1)\n",
        "        return self.loss_fn(out, y)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AfjFbveH64Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "d519e717-0a84-4b0e-fc1f-fdf5e23f779a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3321903\n",
            "1278976\n",
            "1187328\n",
            "2050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-90f8a63b7978>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        # self.tcost = nn.Sequential( # trained cost\n",
        "        #     # nn.Linear((1+self.jepa.pred.num_layers)*d_model, 1),\n",
        "        #     nn.Linear((1+self.jepa.pred.num_layers)*d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, 1),\n",
        "        #     )\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "        self.h0 = torch.randn((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "\n",
        "    def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.h0=lh0[k].unsqueeze(1) # [num_layers, 1, d_model]\n",
        "        # self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        self.lx, self.lz = lx[k:], lz[k:] # [T, dim_a], [T, dim_z]\n",
        "        return act\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 16\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                # x[:,:self.lx.shape[1]], z[:,:self.lz.shape[1]] = self.lx[:,:T], self.lz[:,:T]\n",
        "                x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].repeat(batch,1,1), self.lz[:T].repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            # print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            # print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i, loss.squeeze().data)\n",
        "            # print(x.shape,torch.argmax(-dist,dim=-1).shape,z.shape,loss.shape) # [16, 6, 3], [16, 6], [16, 6, 1], [16, 1]\n",
        "            # print(i, torch.cat([x,torch.argmax(-dist,dim=-1),z,loss],dim=-1).squeeze().data)\n",
        "            # print(i, torch.cat([x[0],torch.argmax(-dist,dim=-1)[0].unsqueeze(-1),z[0]],dim=-1).squeeze().data)\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, t, dim_a], [batch, t, dim_z]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                # tcost = -self.tcost(sx)\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.randn((self.jepa.pred.num_layers, batch_size, self.d_model), device=device)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0)\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    # reward_ = self.tcost(syh0).squeeze(-1)\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "\n",
        "                    # try: st, r = next(trainiter)\n",
        "                    # except StopIteration:\n",
        "                    #     st, r = next(trainiter)\n",
        "                    #     trainiter = iter(c_loader)\n",
        "                    # st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    # # _, st = self.get(st, world_state=world_zero)\n",
        "                    # # print(\"stt\",st.shape)\n",
        "                    # # stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    # # stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    # h00 = torch.randn((self.jepa.pred.num_layers, batch_size, self.d_model), device=device)\n",
        "                    # syy = self.jepa.enc(st) # [batch_size, d_model]\n",
        "                    # syh0 = torch.cat([syy,h00.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    # stt = self.tcost(syh0).squeeze(-1)\n",
        "                    # clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossl #+ clossb\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "4b8704be-f078-4dc7-acbc-acfabddaab4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB\n",
            "From (redirected): https://drive.google.com/uc?id=1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB&confirm=t&uuid=0b1786e6-b4ac-4a42-b499-7557d8592234\n",
            "To: /content/buffergo.pkl\n",
            "100% 786M/786M [00:09<00:00, 85.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3 convenc4\n",
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "import pickle\n",
        "# !gdown 19VQp7UjXqH8kJjEPABOTHDV8reg8r7Zn -O buffer512down.pkl # B\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1HRwU4u7Y6YjQWmC8xlKrDe4YGcNUWxvQ -O buffer512.pkl # B\n",
        "# with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "!gdown 1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB -O buffergo.pkl # S3\n",
        "with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "d4ce8c41-b71b-4b1b-b19e-e429292643d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-92-26acb02b03e2>:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load('agentoptim.pkl').values()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=[], unexpected_keys=['tcost.loss_fn.weight'])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "def save(folder, name='agent.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    # agent.mem.save(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer512.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# save(folder, name='agent_jepa753333256.pth')\n",
        "# buffer = load(folder)\n",
        "# save('/content/')\n",
        "# buffer = load('/content/')\n",
        "\n",
        "# name='agent.pth'\n",
        "# print(folder+name)\n",
        "# torch.load(folder+name, map_location='o')\n",
        "# with open(folder+'buffer512down.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "# with open(folder+'buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG4Wn3c8IN4V"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = buffer[7][80][0]\n",
        "state = transform(state).unsqueeze(0).to(device)#[0]\n",
        "act = agent(state).cpu()[:1].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBfBomEBnJu0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "df87d7b1-e76e-4554-d408-c82cd07eda5d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bece5c242ab8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'agentoptimgru3tcost1.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# torch.save(checkpoint, 'agentoptim.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "torch.save(checkpoint, folder+'agentoptimgru3tcost1.pkl')\n",
        "# torch.save(checkpoint, 'agentoptim.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVcknabHMxH6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.buffer = self.process(buffer)\n",
        "        self.data = [step for episode in self.buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 80):] for episode in cleaned]\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 25 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e3fpbtNOiz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4827a1b0-5b33-4fe6-dc6c-b765170c14a1",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0., -1.,  0., -1.,  0.,  0.,  0., -1., -1., -1.])\n",
            "tensor([ 0.,  0., -1.,  0.,  0., -1., -1.,  0., -1.,  0.])\n",
            "tensor([ 0., -1.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.])\n",
            "tensor([ 0., -1.,  0., -1., -1.,  0.,  0., -1.,  0., -1.])\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.])\n",
            "tensor([ 0., -1., -1., -1.,  0.,  0., -1.,  0., -1., -1.])\n",
            "tensor([-1., -1.,  0.,  0.])\n",
            "tensor([-2.0264e-04, -1.5202e-05, -2.4620e-05, -4.4141e-02, -3.3871e-07,\n",
            "        -1.2363e-05, -3.2429e-06, -1.0000e+00, -1.0000e+00, -1.0000e+00])\n",
            "tensor([-2.8538e-03, -5.4882e-07, -1.0000e+00, -7.7519e-08, -3.8722e-10,\n",
            "        -1.0000e+00, -9.9998e-01, -3.1855e-05, -1.0000e+00, -6.2057e-07])\n",
            "tensor([-4.5120e-07, -1.0000e+00, -9.0568e-03, -3.6339e-04, -1.0000e+00,\n",
            "        -1.0000e+00, -6.8257e-05, -4.5680e-08, -1.0490e-06, -1.8566e-09])\n",
            "tensor([-1.7961e-04, -8.8757e-04, -2.2199e-06, -1.0000e+00, -1.0000e+00,\n",
            "        -1.1566e-05, -1.0139e-06, -5.2201e-01, -1.9276e-08, -1.0000e+00])\n",
            "tensor([-3.4978e-06, -6.4599e-05, -3.8331e-07, -1.6783e-07, -1.4337e-04,\n",
            "        -5.1830e-06, -1.7692e-04, -1.2529e-04, -9.9998e-01, -1.1291e-08])\n",
            "tensor([-1.2375e-05, -1.0000e+00, -5.3263e-03, -1.0000e+00, -9.8408e-06,\n",
            "        -1.0327e-05, -1.0000e+00, -1.1331e-07, -9.9998e-01, -9.9559e-02])\n",
            "tensor([-7.9367e-02, -9.9004e-01, -5.8856e-05, -9.9673e-08])\n",
            "tensor(0.1217)\n"
          ]
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "ctrain_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    ctrain_data=list(zip(state,reward))\n",
        "    ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range((len(labels)//10)+1):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# # try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    h0 = torch.randn((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device)\n",
        "    torch.nn.init.xavier_normal_(h0)\n",
        "    sy = agent.jepa.enc(images.to(device)) # [batch_size, d_model]\n",
        "    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "    pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "    # print(pred)\n",
        "    for x in range((len(pred)//10)+1):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(F.mse_loss(labels, pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OksdjCeJYpYh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "1924153d-48e2-4147-d9c5-cc0e3e3f88b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-41daf5871243>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_loss_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "\n",
        "for i in range(30):\n",
        "    print(i)\n",
        "    # agent.train_ae(train_loader, optim)\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "# 10 epochs 15m23s\n",
        "\n",
        "\n",
        "\n",
        "# loss 0.00027325598057359457\n",
        "# loss 0.00027538512949831784\n",
        "# loss 0.000279315427178517\n",
        "# loss 0.00028544830274768174\n",
        "# loss 0.00029633755912072957\n",
        "# loss 0.0002964686427731067\n",
        "# loss 0.00030574199627153575\n",
        "# loss 0.00031030713580548763\n",
        "# loss 0.00011697990703396499\n",
        "# loss 0.00012466282350942492\n",
        "\n",
        "# loss 0.0002805441035889089\n",
        "# loss 0.0002813159371726215\n",
        "# loss 0.00028616547933779657\n",
        "# loss 0.00029815093148499727\n",
        "# loss 0.0003055527340620756\n",
        "# loss 0.0002878434315789491\n",
        "# loss 0.0002965773455798626\n",
        "# loss 0.00030164531199261546\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "b89b0a9f-5ed9-44cc-8d9f-48c540d4bbfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PraFUAPB3j7v",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e3bf39-889f-4a0c-b8ac-41e64b1763a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "11 tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
            "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
            "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
            "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
            "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
            "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
            "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
            "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
            "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
            "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([5.9485e-05, 6.6876e-05, 5.0783e-05, 6.1750e-05, 5.0664e-05, 6.9380e-05,\n",
            "        5.0068e-05, 4.8518e-05, 7.8022e-05, 6.5804e-05, 5.9426e-05, 7.4983e-05,\n",
            "        5.8532e-05, 6.6042e-05, 5.4717e-05, 4.7088e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.7220e-05, 5.6267e-05, 5.2989e-05, 6.6578e-05, 6.0320e-05, 6.5029e-05,\n",
            "        4.9293e-05, 4.5598e-05, 9.0480e-05, 7.1883e-05, 5.4955e-05, 7.0035e-05,\n",
            "        6.0737e-05, 5.8591e-05, 5.6565e-05, 4.7266e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([6.6102e-05, 5.8889e-05, 4.7386e-05, 6.3539e-05, 6.0260e-05, 5.5850e-05,\n",
            "        5.1022e-05, 4.8816e-05, 7.9930e-05, 6.9439e-05, 4.6790e-05, 6.2764e-05,\n",
            "        5.6446e-05, 6.7234e-05, 5.1558e-05, 4.6134e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([6.2704e-05, 5.3227e-05, 4.4703e-05, 6.0916e-05, 6.4552e-05, 5.5730e-05,\n",
            "        5.3465e-05, 4.7028e-05, 6.8486e-05, 5.4359e-05, 4.5240e-05, 6.5386e-05,\n",
            "        6.2943e-05, 5.3763e-05, 4.8280e-05, 5.4300e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([4.5300e-05, 4.5538e-05, 4.5121e-05, 4.5002e-05, 4.6313e-05, 4.7147e-05,\n",
            "        4.5478e-05, 4.6790e-05, 4.9055e-05, 4.4823e-05, 4.2737e-05, 5.2273e-05,\n",
            "        4.8399e-05, 4.4227e-05, 4.3094e-05, 4.8161e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.5359e-05, 4.5419e-05, 4.5121e-05, 4.5240e-05, 4.6432e-05, 4.7386e-05,\n",
            "        4.5836e-05, 4.5300e-05, 4.7326e-05, 4.5300e-05, 4.3035e-05, 5.2392e-05,\n",
            "        4.8459e-05, 4.5836e-05, 4.3273e-05, 4.8578e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([4.5359e-05, 4.5359e-05, 4.5121e-05, 4.5180e-05, 4.5538e-05, 4.7565e-05,\n",
            "        4.6074e-05, 4.5300e-05, 4.5180e-05, 4.5300e-05, 4.3094e-05, 4.8935e-05,\n",
            "        4.6968e-05, 4.5300e-05, 4.4107e-05, 4.6670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([4.5359e-05, 4.5359e-05, 4.5121e-05, 4.5061e-05, 4.5598e-05, 4.5657e-05,\n",
            "        4.6253e-05, 4.5300e-05, 4.5180e-05, 4.5300e-05, 4.3213e-05, 4.8399e-05,\n",
            "        4.4882e-05, 4.5240e-05, 4.4167e-05, 4.6670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([4.5538e-05, 4.5478e-05, 4.5121e-05, 4.5240e-05, 4.5717e-05, 4.5776e-05,\n",
            "        4.6253e-05, 4.5300e-05, 4.5180e-05, 4.5240e-05, 4.3690e-05, 4.8399e-05,\n",
            "        4.4882e-05, 4.5300e-05, 4.5300e-05, 4.6670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([4.5657e-05, 4.5538e-05, 4.5121e-05, 4.5359e-05, 4.5836e-05, 4.5776e-05,\n",
            "        4.3988e-05, 4.5419e-05, 4.5180e-05, 4.5240e-05, 4.3869e-05, 4.8399e-05,\n",
            "        4.5419e-05, 4.5419e-05, 4.5300e-05, 4.6670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([4.5717e-05, 4.5657e-05, 4.5121e-05, 4.5419e-05, 4.6074e-05, 4.6015e-05,\n",
            "        4.5478e-05, 4.4048e-05, 4.5180e-05, 4.5359e-05, 4.4048e-05, 4.8399e-05,\n",
            "        4.5538e-05, 4.5538e-05, 4.5300e-05, 4.6670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([4.3213e-05, 4.1127e-05, 4.3392e-05, 4.2796e-05, 4.3571e-05, 4.3511e-05,\n",
            "        4.3154e-05, 4.2081e-05, 4.5180e-05, 4.2796e-05, 4.2200e-05, 4.8459e-05,\n",
            "        4.0948e-05, 4.2915e-05, 4.3571e-05, 4.2260e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([4.3571e-05, 4.1425e-05, 4.3392e-05, 4.1068e-05, 4.1902e-05, 4.3929e-05,\n",
            "        4.2260e-05, 4.2200e-05, 4.0472e-05, 4.0770e-05, 4.2200e-05, 4.8459e-05,\n",
            "        4.1008e-05, 4.1127e-05, 4.3571e-05, 4.2260e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([4.3571e-05, 3.9995e-05, 4.3392e-05, 3.9399e-05, 4.0591e-05, 4.3988e-05,\n",
            "        4.0889e-05, 4.2200e-05, 4.0591e-05, 4.1068e-05, 4.2200e-05, 4.0114e-05,\n",
            "        4.1425e-05, 3.9339e-05, 4.3571e-05, 4.2260e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([4.0174e-05, 4.0352e-05, 4.3571e-05, 3.9637e-05, 4.0472e-05, 4.4227e-05,\n",
            "        4.0889e-05, 4.2200e-05, 4.0650e-05, 4.1068e-05, 4.2200e-05, 4.0114e-05,\n",
            "        4.1842e-05, 3.9637e-05, 4.2200e-05, 4.0889e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([4.0233e-05, 4.0472e-05, 4.3571e-05, 3.9995e-05, 4.0472e-05, 4.2260e-05,\n",
            "        4.0889e-05, 4.2200e-05, 4.0948e-05, 4.1485e-05, 4.2200e-05, 3.8385e-05,\n",
            "        4.2021e-05, 3.9995e-05, 4.2200e-05, 4.0650e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([4.0233e-05, 4.0472e-05, 4.2200e-05, 4.0352e-05, 4.0472e-05, 4.0472e-05,\n",
            "        4.0889e-05, 4.1962e-05, 3.9876e-05, 4.1723e-05, 4.1962e-05, 3.6538e-05,\n",
            "        4.0948e-05, 4.0054e-05, 4.2200e-05, 4.0650e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([4.0233e-05, 4.0233e-05, 4.2200e-05, 4.0472e-05, 4.0472e-05, 4.0472e-05,\n",
            "        4.0889e-05, 4.1068e-05, 3.7909e-05, 4.0174e-05, 4.1962e-05, 3.6716e-05,\n",
            "        3.9041e-05, 4.0233e-05, 4.2200e-05, 4.0650e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([4.0233e-05, 4.0233e-05, 4.1366e-05, 4.0233e-05, 3.8862e-05, 4.0472e-05,\n",
            "        4.0889e-05, 4.1068e-05, 3.8266e-05, 4.0591e-05, 4.1068e-05, 3.6955e-05,\n",
            "        3.9041e-05, 4.0233e-05, 4.2200e-05, 4.0650e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([4.0233e-05, 4.0233e-05, 4.1366e-05, 4.0233e-05, 3.8862e-05, 4.0472e-05,\n",
            "        4.0889e-05, 4.1068e-05, 3.8445e-05, 4.0472e-05, 4.1068e-05, 3.7074e-05,\n",
            "        3.9041e-05, 4.0233e-05, 4.1962e-05, 3.8862e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([1.3530e-04, 1.3065e-04, 1.1843e-04, 1.2064e-04, 1.4019e-04, 1.3995e-04,\n",
            "        1.0061e-04, 1.0508e-04, 9.8944e-05, 9.7632e-05, 1.2708e-04, 1.3220e-04,\n",
            "        1.1957e-04, 1.1879e-04, 1.0574e-04, 1.1498e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([5.0008e-05, 5.1260e-05, 5.1081e-05, 4.1425e-05, 4.2856e-05, 4.7505e-05,\n",
            "        4.6432e-05, 4.8459e-05, 3.8505e-05, 4.8220e-05, 5.0724e-05, 4.5061e-05,\n",
            "        4.3273e-05, 5.0366e-05, 5.5909e-05, 4.7624e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([4.2796e-05, 5.5969e-05, 4.6313e-05, 4.9591e-05, 4.6432e-05, 5.2094e-05,\n",
            "        4.8757e-05, 3.7909e-05, 3.9577e-05, 3.7372e-05, 5.1916e-05, 5.1796e-05,\n",
            "        4.0293e-05, 4.8757e-05, 4.5955e-05, 5.6386e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([5.0604e-05, 5.9843e-05, 4.6611e-05, 4.5598e-05, 4.1127e-05, 5.2691e-05,\n",
            "        5.5254e-05, 4.0591e-05, 4.6313e-05, 4.3094e-05, 4.4107e-05, 5.3763e-05,\n",
            "        4.2260e-05, 4.3392e-05, 4.1366e-05, 4.5538e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.7207e-05, 5.0604e-05, 5.0306e-05, 4.7028e-05, 4.5359e-05, 5.0008e-05,\n",
            "        5.3883e-05, 3.8505e-05, 4.3035e-05, 3.8564e-05, 5.0008e-05, 5.3227e-05,\n",
            "        3.6478e-05, 3.8028e-05, 4.6015e-05, 4.5776e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([4.0352e-05, 3.8087e-05, 3.8326e-05, 4.1485e-05, 3.8385e-05, 3.8743e-05,\n",
            "        3.8922e-05, 3.8981e-05, 3.6478e-05, 3.6299e-05, 3.8981e-05, 3.8564e-05,\n",
            "        3.6359e-05, 3.7968e-05, 3.8385e-05, 3.7849e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.0889e-05, 3.8266e-05, 3.8683e-05, 4.0352e-05, 3.8564e-05, 3.9041e-05,\n",
            "        3.9041e-05, 3.8981e-05, 3.8326e-05, 3.6299e-05, 3.9339e-05, 3.8743e-05,\n",
            "        3.6359e-05, 3.8683e-05, 3.8505e-05, 3.8207e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.9399e-05, 3.8385e-05, 3.8922e-05, 4.0591e-05, 3.8981e-05, 3.9220e-05,\n",
            "        3.9220e-05, 3.8981e-05, 3.8326e-05, 3.7611e-05, 3.9577e-05, 3.8862e-05,\n",
            "        3.6359e-05, 3.9041e-05, 3.8803e-05, 3.8326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.9518e-05, 3.8564e-05, 3.9160e-05, 4.0948e-05, 3.9399e-05, 3.9458e-05,\n",
            "        3.9339e-05, 3.8326e-05, 3.8326e-05, 3.7611e-05, 3.8922e-05, 3.9101e-05,\n",
            "        3.6359e-05, 3.9458e-05, 3.9041e-05, 3.8326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.9518e-05, 3.8683e-05, 3.9399e-05, 3.9518e-05, 3.9518e-05, 3.9518e-05,\n",
            "        3.8803e-05, 3.8326e-05, 3.8326e-05, 3.7611e-05, 3.9101e-05, 3.9279e-05,\n",
            "        3.8326e-05, 3.9518e-05, 3.9279e-05, 3.8624e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.9518e-05, 3.9041e-05, 3.9518e-05, 3.7670e-05, 3.7670e-05, 3.9518e-05,\n",
            "        3.9041e-05, 3.8326e-05, 3.8326e-05, 3.8326e-05, 3.9339e-05, 3.9458e-05,\n",
            "        3.8326e-05, 3.7670e-05, 3.9458e-05, 3.8505e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.9518e-05, 3.9279e-05, 3.9518e-05, 3.7670e-05, 3.7670e-05, 3.9518e-05,\n",
            "        3.9279e-05, 3.8326e-05, 3.7193e-05, 3.8326e-05, 3.9518e-05, 3.9518e-05,\n",
            "        3.8326e-05, 3.7670e-05, 3.9518e-05, 3.8743e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.9518e-05, 3.9399e-05, 3.9518e-05, 3.7670e-05, 3.5763e-05, 3.9518e-05,\n",
            "        3.9518e-05, 3.8326e-05, 3.7193e-05, 3.7193e-05, 3.7670e-05, 3.9518e-05,\n",
            "        3.8326e-05, 3.7670e-05, 3.9518e-05, 3.8981e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.5763e-05, 3.7491e-05, 3.6299e-05, 3.5763e-05, 3.5763e-05, 3.7253e-05,\n",
            "        3.7491e-05, 3.6955e-05, 3.5763e-05, 3.5763e-05, 3.5763e-05, 3.7491e-05,\n",
            "        3.5763e-05, 3.5763e-05, 3.5763e-05, 3.5584e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.5763e-05, 3.7491e-05, 3.6299e-05, 3.5763e-05, 3.5763e-05, 3.7253e-05,\n",
            "        3.5763e-05, 3.5763e-05, 3.5763e-05, 3.5763e-05, 3.5763e-05, 3.7491e-05,\n",
            "        3.5763e-05, 3.5763e-05, 3.5763e-05, 3.5584e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.5763e-05, 3.7491e-05, 3.4332e-05, 3.5763e-05, 3.5763e-05, 3.7253e-05,\n",
            "        3.5763e-05, 3.5763e-05, 3.5763e-05, 3.5763e-05, 3.4630e-05, 3.7491e-05,\n",
            "        3.5763e-05, 3.5763e-05, 3.5763e-05, 3.4630e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.5763e-05, 3.7253e-05, 3.4332e-05, 3.5763e-05, 3.5763e-05, 3.6061e-05,\n",
            "        3.4630e-05, 3.5763e-05, 3.5763e-05, 3.5763e-05, 3.4630e-05, 3.5763e-05,\n",
            "        3.5763e-05, 3.5763e-05, 3.5763e-05, 3.3319e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.5763e-05, 3.6061e-05, 3.4630e-05, 3.5763e-05, 3.5584e-05, 3.6061e-05,\n",
            "        3.3319e-05, 3.5763e-05, 3.4928e-05, 3.5763e-05, 3.4630e-05, 3.5584e-05,\n",
            "        3.5763e-05, 3.4630e-05, 3.5763e-05, 3.3319e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.5763e-05, 3.4332e-05, 3.4630e-05, 3.5763e-05, 3.5584e-05, 3.6061e-05,\n",
            "        3.3319e-05, 3.5584e-05, 3.4809e-05, 3.5763e-05, 3.4630e-05, 3.4392e-05,\n",
            "        3.5763e-05, 3.4630e-05, 3.5763e-05, 3.3319e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.5763e-05, 3.4332e-05, 3.4630e-05, 3.5763e-05, 3.5584e-05, 3.4332e-05,\n",
            "        3.3140e-05, 3.5584e-05, 3.4809e-05, 3.5763e-05, 3.4630e-05, 3.4392e-05,\n",
            "        3.5763e-05, 3.4630e-05, 3.5763e-05, 3.3319e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.4630e-05, 3.3140e-05, 3.4332e-05, 3.5584e-05, 3.5584e-05, 3.4630e-05,\n",
            "        3.3140e-05, 3.5584e-05, 3.4809e-05, 3.4809e-05, 3.4630e-05, 3.3140e-05,\n",
            "        3.5763e-05, 3.4630e-05, 3.5763e-05, 3.3319e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.6134e-05, 3.3200e-05, 4.4942e-05, 3.0160e-05, 3.4988e-05, 4.0352e-05,\n",
            "        3.7193e-05, 3.5822e-05, 4.1246e-05, 5.0306e-05, 3.4451e-05, 3.3379e-05,\n",
            "        3.5286e-05, 4.5121e-05, 3.3498e-05, 3.1173e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([4.4286e-05, 3.9101e-05, 3.3200e-05, 3.5584e-05, 3.4690e-05, 3.8624e-05,\n",
            "        4.1246e-05, 3.3557e-05, 4.1366e-05, 3.7372e-05, 3.9637e-05, 3.6597e-05,\n",
            "        3.2783e-05, 4.3988e-05, 3.4213e-05, 3.6418e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.0398e-05, 3.6299e-05, 3.5346e-05, 3.8683e-05, 3.3140e-05, 3.3498e-05,\n",
            "        3.7491e-05, 3.4034e-05, 3.7014e-05, 3.8087e-05, 3.2961e-05, 3.0339e-05,\n",
            "        3.1233e-05, 5.2691e-05, 4.0710e-05, 3.5167e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.3081e-05, 3.3796e-05, 3.5822e-05, 3.3379e-05, 3.1829e-05, 3.4750e-05,\n",
            "        4.1068e-05, 3.4571e-05, 3.5882e-05, 3.5703e-05, 3.5346e-05, 3.2246e-05,\n",
            "        3.3557e-05, 4.4703e-05, 4.3988e-05, 3.7432e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.8133e-05, 2.9981e-05, 3.4928e-05, 2.9922e-05, 2.8431e-05, 2.9802e-05,\n",
            "        2.9802e-05, 2.9981e-05, 3.1292e-05, 3.1650e-05, 3.1054e-05, 2.7657e-05,\n",
            "        2.9862e-05, 2.8789e-05, 3.2902e-05, 2.9922e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.8491e-05, 3.0160e-05, 3.2961e-05, 3.0100e-05, 2.9147e-05, 3.0816e-05,\n",
            "        3.0220e-05, 3.0160e-05, 3.1590e-05, 3.2187e-05, 2.8491e-05, 2.9802e-05,\n",
            "        2.8729e-05, 2.9147e-05, 3.0220e-05, 3.0160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.8789e-05, 3.0279e-05, 3.1471e-05, 3.0279e-05, 2.9147e-05, 3.0935e-05,\n",
            "        3.0279e-05, 3.0279e-05, 3.1650e-05, 3.0279e-05, 2.8610e-05, 2.9266e-05,\n",
            "        2.8253e-05, 2.9206e-05, 3.0279e-05, 3.0279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.8968e-05, 3.0279e-05, 3.1710e-05, 3.0279e-05, 2.9147e-05, 3.0935e-05,\n",
            "        3.0279e-05, 3.0279e-05, 3.0279e-05, 3.0279e-05, 2.9802e-05, 2.9266e-05,\n",
            "        2.9802e-05, 2.9385e-05, 2.9087e-05, 3.0279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.9087e-05, 3.0279e-05, 3.1888e-05, 3.0279e-05, 2.9147e-05, 3.0935e-05,\n",
            "        3.0279e-05, 3.0279e-05, 3.0279e-05, 3.0279e-05, 3.0100e-05, 2.9266e-05,\n",
            "        2.9266e-05, 2.9624e-05, 2.9087e-05, 3.0279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.9087e-05, 3.0279e-05, 3.1888e-05, 3.0279e-05, 2.9147e-05, 2.9087e-05,\n",
            "        3.0279e-05, 3.0279e-05, 3.0279e-05, 3.0279e-05, 3.0279e-05, 2.9266e-05,\n",
            "        2.9266e-05, 3.0041e-05, 2.9087e-05, 3.0279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.9087e-05, 2.9087e-05, 3.1888e-05, 3.0279e-05, 2.9147e-05, 2.9087e-05,\n",
            "        3.0279e-05, 3.0279e-05, 3.0279e-05, 3.0279e-05, 3.0279e-05, 2.9266e-05,\n",
            "        2.9266e-05, 3.0279e-05, 2.9087e-05, 3.0279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.9087e-05, 2.9087e-05, 3.1888e-05, 2.7716e-05, 2.9147e-05, 2.9087e-05,\n",
            "        3.0279e-05, 3.0279e-05, 3.0279e-05, 3.0279e-05, 2.8789e-05, 2.9266e-05,\n",
            "        2.9266e-05, 3.0279e-05, 2.7716e-05, 3.0279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.7716e-05, 2.7716e-05, 3.0696e-05, 2.7716e-05, 2.7478e-05, 2.6703e-05,\n",
            "        2.8789e-05, 2.7716e-05, 2.8789e-05, 2.7716e-05, 2.6703e-05, 2.8372e-05,\n",
            "        2.7478e-05, 2.8789e-05, 2.7716e-05, 2.8789e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.7537e-05, 2.7537e-05, 3.0696e-05, 2.7716e-05, 2.7478e-05, 2.6703e-05,\n",
            "        2.8789e-05, 2.6524e-05, 2.8789e-05, 2.7716e-05, 2.6703e-05, 2.7657e-05,\n",
            "        2.7478e-05, 2.8610e-05, 2.7716e-05, 2.8610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.7537e-05, 2.7537e-05, 2.6226e-05, 2.6524e-05, 2.7478e-05, 2.6703e-05,\n",
            "        2.7716e-05, 2.6524e-05, 2.7716e-05, 2.7716e-05, 2.6703e-05, 2.7657e-05,\n",
            "        2.6762e-05, 2.8610e-05, 2.7716e-05, 2.7716e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.7537e-05, 2.7537e-05, 2.6226e-05, 2.6524e-05, 2.7478e-05, 2.6703e-05,\n",
            "        2.7716e-05, 2.6524e-05, 2.7716e-05, 2.7716e-05, 2.6703e-05, 2.6762e-05,\n",
            "        2.6762e-05, 2.8610e-05, 2.6703e-05, 2.6524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.7537e-05, 2.7537e-05, 2.6226e-05, 2.6524e-05, 2.7478e-05, 2.6703e-05,\n",
            "        2.7716e-05, 2.6524e-05, 2.6703e-05, 2.7716e-05, 2.6703e-05, 2.6703e-05,\n",
            "        2.6762e-05, 2.8610e-05, 2.6703e-05, 2.6524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.6524e-05, 2.7537e-05, 2.5332e-05, 2.6524e-05, 2.7478e-05, 2.6703e-05,\n",
            "        2.7716e-05, 2.6524e-05, 2.6703e-05, 2.7716e-05, 2.6524e-05, 2.6703e-05,\n",
            "        2.6762e-05, 2.8610e-05, 2.6524e-05, 2.6524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.6524e-05, 2.7537e-05, 2.5332e-05, 2.6524e-05, 2.7478e-05, 2.6703e-05,\n",
            "        2.7537e-05, 2.6524e-05, 2.6524e-05, 2.7716e-05, 2.6524e-05, 2.6703e-05,\n",
            "        2.6762e-05, 2.7537e-05, 2.6524e-05, 2.6524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.6524e-05, 2.6524e-05, 2.5332e-05, 2.6524e-05, 2.6762e-05, 2.6524e-05,\n",
            "        2.7537e-05, 2.6524e-05, 2.6524e-05, 2.6703e-05, 2.6524e-05, 2.6703e-05,\n",
            "        2.6762e-05, 2.7537e-05, 2.6524e-05, 2.6524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([5.4121e-05, 5.2392e-05, 4.8876e-05, 5.3704e-05, 4.7445e-05, 4.7982e-05,\n",
            "        5.2571e-05, 4.2140e-05, 4.5002e-05, 4.4167e-05, 4.5180e-05, 4.2081e-05,\n",
            "        4.2498e-05, 5.1856e-05, 5.0008e-05, 4.8101e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.2691e-05, 5.4121e-05, 4.8637e-05, 5.2094e-05, 4.4644e-05, 4.6909e-05,\n",
            "        5.0187e-05, 4.6909e-05, 3.7551e-05, 4.5955e-05, 4.2379e-05, 4.5836e-05,\n",
            "        4.0889e-05, 5.4598e-05, 4.8161e-05, 4.7207e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([5.0843e-05, 4.8101e-05, 4.8876e-05, 5.2750e-05, 4.3869e-05, 4.3154e-05,\n",
            "        5.1796e-05, 4.4405e-05, 3.5584e-05, 4.6313e-05, 4.3750e-05, 4.4048e-05,\n",
            "        4.1187e-05, 5.9307e-05, 3.7789e-05, 4.9710e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.7803e-05, 4.8935e-05, 4.2617e-05, 4.7684e-05, 4.2319e-05, 4.4346e-05,\n",
            "        4.8697e-05, 4.8161e-05, 4.0174e-05, 4.5300e-05, 4.4703e-05, 4.9233e-05,\n",
            "        4.5836e-05, 5.4300e-05, 4.0889e-05, 4.3452e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.9160e-05, 3.8385e-05, 3.8028e-05, 3.8326e-05, 3.8445e-05, 3.9518e-05,\n",
            "        3.8326e-05, 3.9518e-05, 3.7670e-05, 3.8207e-05, 3.8028e-05, 3.8862e-05,\n",
            "        3.8028e-05, 3.8505e-05, 3.8028e-05, 3.9160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.8624e-05, 3.8624e-05, 3.8326e-05, 3.8564e-05, 3.8624e-05, 3.8207e-05,\n",
            "        3.8505e-05, 3.9518e-05, 3.7670e-05, 3.8207e-05, 3.8028e-05, 3.8862e-05,\n",
            "        3.8028e-05, 3.8505e-05, 3.8028e-05, 3.8564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.8564e-05, 3.8505e-05, 3.8326e-05, 3.8564e-05, 3.8564e-05, 3.8207e-05,\n",
            "        3.8564e-05, 3.8207e-05, 3.7134e-05, 3.8207e-05, 3.7432e-05, 3.8862e-05,\n",
            "        3.8028e-05, 3.8564e-05, 3.8028e-05, 3.8564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.8564e-05, 3.8505e-05, 3.8326e-05, 3.8564e-05, 3.8564e-05, 3.8207e-05,\n",
            "        3.8564e-05, 3.8207e-05, 3.8207e-05, 3.8207e-05, 3.7432e-05, 3.8862e-05,\n",
            "        3.8028e-05, 3.8564e-05, 3.8087e-05, 3.8564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.8564e-05, 3.8505e-05, 3.8385e-05, 3.8564e-05, 3.8564e-05, 3.8207e-05,\n",
            "        3.8564e-05, 3.8207e-05, 3.8207e-05, 3.8207e-05, 3.7432e-05, 3.8207e-05,\n",
            "        3.8028e-05, 3.8564e-05, 3.8028e-05, 3.8564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.8564e-05, 3.8564e-05, 3.8385e-05, 3.8564e-05, 3.8564e-05, 3.8207e-05,\n",
            "        3.8564e-05, 3.8207e-05, 3.8207e-05, 3.8207e-05, 3.7432e-05, 3.8207e-05,\n",
            "        3.8207e-05, 3.8564e-05, 3.8087e-05, 3.8564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.8564e-05, 3.8564e-05, 3.8385e-05, 3.8564e-05, 3.8564e-05, 3.8207e-05,\n",
            "        3.8564e-05, 3.8207e-05, 3.8207e-05, 3.8207e-05, 3.7432e-05, 3.8207e-05,\n",
            "        3.8207e-05, 3.8564e-05, 3.8147e-05, 3.8564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.8564e-05, 3.8564e-05, 3.6895e-05, 3.8564e-05, 3.8564e-05, 3.7432e-05,\n",
            "        3.8564e-05, 3.8207e-05, 3.8207e-05, 3.8207e-05, 3.7432e-05, 3.8207e-05,\n",
            "        3.8207e-05, 3.8564e-05, 3.8087e-05, 3.8564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.8564e-05, 3.8385e-05, 3.6895e-05, 3.8385e-05, 3.6657e-05, 3.7432e-05,\n",
            "        3.8564e-05, 3.8207e-05, 3.6180e-05, 3.8207e-05, 3.8207e-05, 3.8207e-05,\n",
            "        3.8207e-05, 3.8564e-05, 3.8147e-05, 3.6895e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.6895e-05, 3.5286e-05, 3.5286e-05, 3.6657e-05, 3.6657e-05, 3.6180e-05,\n",
            "        3.6895e-05, 3.7134e-05, 3.6180e-05, 3.7134e-05, 3.7134e-05, 3.7134e-05,\n",
            "        3.7134e-05, 3.6895e-05, 3.6776e-05, 3.5524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.5524e-05, 3.5286e-05, 3.5524e-05, 3.5286e-05, 3.5286e-05, 3.6180e-05,\n",
            "        3.5524e-05, 3.7134e-05, 3.6180e-05, 3.7134e-05, 3.6180e-05, 3.7134e-05,\n",
            "        3.7134e-05, 3.6895e-05, 3.5703e-05, 3.5524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.5524e-05, 3.5286e-05, 3.5524e-05, 3.5286e-05, 3.5286e-05, 3.6180e-05,\n",
            "        3.5286e-05, 3.7134e-05, 3.6180e-05, 3.7134e-05, 3.6180e-05, 3.7134e-05,\n",
            "        3.7134e-05, 3.5524e-05, 3.5703e-05, 3.5286e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.5524e-05, 3.5286e-05, 3.4332e-05, 3.5286e-05, 3.5286e-05, 3.6180e-05,\n",
            "        3.5286e-05, 3.7134e-05, 3.6180e-05, 3.7134e-05, 3.6180e-05, 3.7134e-05,\n",
            "        3.6180e-05, 3.5524e-05, 3.5703e-05, 3.5286e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.5286e-05, 3.4094e-05, 3.4332e-05, 3.5286e-05, 3.5286e-05, 3.6180e-05,\n",
            "        3.5286e-05, 3.7134e-05, 3.6180e-05, 3.6895e-05, 3.6180e-05, 3.7134e-05,\n",
            "        3.6180e-05, 3.5524e-05, 3.5107e-05, 3.5286e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.4094e-05, 3.4094e-05, 3.4332e-05, 3.5286e-05, 3.5286e-05, 3.6180e-05,\n",
            "        3.5286e-05, 3.7134e-05, 3.6180e-05, 3.6895e-05, 3.6180e-05, 3.6001e-05,\n",
            "        3.6180e-05, 3.4094e-05, 3.5346e-05, 3.4094e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.4094e-05, 3.4094e-05, 3.4332e-05, 3.5286e-05, 3.5286e-05, 3.6180e-05,\n",
            "        3.5286e-05, 3.6180e-05, 3.6180e-05, 3.6895e-05, 3.6180e-05, 3.6001e-05,\n",
            "        3.6180e-05, 3.4094e-05, 3.5346e-05, 3.4094e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.4107e-05, 4.6670e-05, 5.1260e-05, 4.9710e-05, 4.5240e-05, 6.0022e-05,\n",
            "        5.7876e-05, 4.6432e-05, 4.6372e-05, 6.0499e-05, 5.2452e-05, 4.5598e-05,\n",
            "        6.7651e-05, 4.9710e-05, 5.7161e-05, 4.2677e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.2154e-05, 4.3869e-05, 5.2810e-05, 4.4584e-05, 4.6015e-05, 5.8115e-05,\n",
            "        5.9366e-05, 3.8803e-05, 4.9889e-05, 5.9664e-05, 3.5644e-05, 5.2691e-05,\n",
            "        5.5552e-05, 4.5300e-05, 5.6684e-05, 4.9472e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([5.0783e-05, 5.1379e-05, 5.2512e-05, 4.6790e-05, 5.3406e-05, 4.5180e-05,\n",
            "        6.4611e-05, 4.4405e-05, 4.1783e-05, 4.3750e-05, 4.1425e-05, 4.1723e-05,\n",
            "        6.3241e-05, 4.8220e-05, 5.6207e-05, 4.9114e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.4703e-05, 4.6730e-05, 4.4703e-05, 4.6194e-05, 5.1796e-05, 4.5478e-05,\n",
            "        4.7147e-05, 4.4346e-05, 4.5896e-05, 4.3750e-05, 4.5180e-05, 4.5240e-05,\n",
            "        4.5359e-05, 4.5061e-05, 4.4763e-05, 4.8399e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([4.0591e-05, 4.1783e-05, 4.1664e-05, 4.2140e-05, 4.1783e-05, 4.0114e-05,\n",
            "        4.2558e-05, 4.1008e-05, 4.2140e-05, 4.4346e-05, 4.1664e-05, 4.1842e-05,\n",
            "        4.6134e-05, 4.0174e-05, 3.9160e-05, 4.0293e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.1008e-05, 4.0293e-05, 4.1604e-05, 4.2319e-05, 4.1723e-05, 4.1008e-05,\n",
            "        3.9637e-05, 4.1008e-05, 4.2260e-05, 4.5180e-05, 4.0233e-05, 4.1842e-05,\n",
            "        4.1187e-05, 4.0174e-05, 3.9756e-05, 4.0412e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([4.1246e-05, 4.0293e-05, 4.1246e-05, 4.2677e-05, 4.0233e-05, 4.1246e-05,\n",
            "        3.9935e-05, 4.1008e-05, 4.0770e-05, 4.2737e-05, 4.0352e-05, 4.0829e-05,\n",
            "        4.2021e-05, 4.1008e-05, 3.9935e-05, 4.0412e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([4.1246e-05, 4.0293e-05, 4.1246e-05, 4.1127e-05, 4.0293e-05, 4.1246e-05,\n",
            "        4.0114e-05, 4.1008e-05, 4.0889e-05, 4.2975e-05, 4.0412e-05, 4.0829e-05,\n",
            "        3.9697e-05, 4.0293e-05, 4.0174e-05, 4.0531e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([4.1246e-05, 4.0293e-05, 4.1246e-05, 4.1246e-05, 4.0293e-05, 4.1246e-05,\n",
            "        4.0352e-05, 4.0293e-05, 4.1068e-05, 4.3094e-05, 4.0472e-05, 3.9220e-05,\n",
            "        3.9935e-05, 4.0293e-05, 4.0591e-05, 4.0770e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([4.1246e-05, 4.0293e-05, 3.9637e-05, 4.1246e-05, 4.0293e-05, 4.1246e-05,\n",
            "        4.0829e-05, 4.0293e-05, 4.1246e-05, 4.3154e-05, 4.0531e-05, 3.9279e-05,\n",
            "        4.0293e-05, 4.0293e-05, 4.0948e-05, 4.1068e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([4.1246e-05, 4.0293e-05, 3.9637e-05, 4.1246e-05, 4.0352e-05, 4.1246e-05,\n",
            "        4.1127e-05, 4.0293e-05, 4.1246e-05, 4.2319e-05, 4.0531e-05, 3.9279e-05,\n",
            "        4.0770e-05, 4.0293e-05, 4.1246e-05, 4.1246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([4.1246e-05, 4.0293e-05, 3.9637e-05, 4.1246e-05, 4.0412e-05, 4.1246e-05,\n",
            "        4.1246e-05, 4.0293e-05, 3.9637e-05, 4.2796e-05, 4.0531e-05, 3.9279e-05,\n",
            "        4.1127e-05, 4.0293e-05, 4.1246e-05, 4.1246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.8087e-05, 3.9220e-05, 3.8087e-05, 3.9339e-05, 4.0293e-05, 4.1246e-05,\n",
            "        4.1246e-05, 3.9220e-05, 3.8087e-05, 4.1544e-05, 3.9279e-05, 3.6597e-05,\n",
            "        4.1723e-05, 4.0293e-05, 3.9339e-05, 3.9339e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.8087e-05, 3.9220e-05, 3.8087e-05, 3.9339e-05, 3.9220e-05, 3.8087e-05,\n",
            "        3.8028e-05, 3.9220e-05, 3.8087e-05, 4.1962e-05, 3.7789e-05, 3.6538e-05,\n",
            "        4.1962e-05, 3.9220e-05, 3.8028e-05, 3.9160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.8087e-05, 3.9220e-05, 3.8087e-05, 3.8087e-05, 3.9220e-05, 3.8087e-05,\n",
            "        3.8028e-05, 3.8266e-05, 3.6597e-05, 4.1962e-05, 3.7789e-05, 3.6538e-05,\n",
            "        4.1962e-05, 3.9220e-05, 3.8028e-05, 3.9160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.8087e-05, 3.8266e-05, 3.6597e-05, 3.6359e-05, 3.9220e-05, 3.8087e-05,\n",
            "        3.6597e-05, 3.8266e-05, 3.6597e-05, 4.1962e-05, 3.7968e-05, 3.6597e-05,\n",
            "        3.6001e-05, 3.9220e-05, 3.6597e-05, 3.9160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.8087e-05, 3.8266e-05, 3.6359e-05, 3.6359e-05, 3.9220e-05, 3.8087e-05,\n",
            "        3.6597e-05, 3.8266e-05, 3.6597e-05, 3.6001e-05, 3.8087e-05, 3.6716e-05,\n",
            "        3.6001e-05, 3.9220e-05, 3.6359e-05, 3.9160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.8087e-05, 3.8266e-05, 3.6359e-05, 3.6359e-05, 3.9220e-05, 3.6597e-05,\n",
            "        3.6597e-05, 3.8266e-05, 3.6597e-05, 3.6001e-05, 3.8087e-05, 3.6657e-05,\n",
            "        3.6001e-05, 3.8266e-05, 3.6359e-05, 3.9160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.8087e-05, 3.8266e-05, 3.6359e-05, 3.6359e-05, 3.9220e-05, 3.6597e-05,\n",
            "        3.6597e-05, 3.7313e-05, 3.6597e-05, 3.6001e-05, 3.6359e-05, 3.6597e-05,\n",
            "        3.4928e-05, 3.8266e-05, 3.6359e-05, 3.9160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.6597e-05, 3.8266e-05, 3.6359e-05, 3.6359e-05, 3.8266e-05, 3.6359e-05,\n",
            "        3.6597e-05, 3.7313e-05, 3.6359e-05, 3.4928e-05, 3.6359e-05, 3.6776e-05,\n",
            "        3.4928e-05, 3.7313e-05, 3.6359e-05, 3.7730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0012, 0.0011, 0.0011, 0.0012, 0.0012, 0.0011, 0.0011, 0.0012, 0.0011,\n",
            "        0.0012, 0.0011, 0.0012, 0.0010, 0.0011, 0.0011, 0.0012],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0011, 0.0010, 0.0011, 0.0012,\n",
            "        0.0013, 0.0010, 0.0011, 0.0011, 0.0011, 0.0011, 0.0013],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0011, 0.0011, 0.0012, 0.0012,\n",
            "        0.0012, 0.0011, 0.0012, 0.0011, 0.0012, 0.0011, 0.0013],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0011, 0.0011, 0.0011, 0.0012,\n",
            "        0.0012, 0.0011, 0.0012, 0.0011, 0.0012, 0.0012, 0.0012],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([1.2374e-04, 1.1557e-04, 9.3579e-05, 1.1706e-04, 1.1551e-04, 1.2839e-04,\n",
            "        1.1027e-04, 1.2010e-04, 9.9599e-05, 9.7275e-05, 1.0145e-04, 1.1927e-04,\n",
            "        1.3280e-04, 1.1921e-04, 1.2469e-04, 1.1581e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.3995e-04, 1.0616e-04, 1.0318e-04, 1.1361e-04, 1.1879e-04, 1.2851e-04,\n",
            "        9.4235e-05, 1.1432e-04, 1.1194e-04, 1.0031e-04, 9.5725e-05, 1.2130e-04,\n",
            "        1.3709e-04, 1.3614e-04, 1.1277e-04, 1.2827e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([1.0031e-04, 1.0186e-04, 1.0186e-04, 9.8825e-05, 9.9421e-05, 9.9719e-05,\n",
            "        9.8050e-05, 1.0073e-04, 1.0073e-04, 9.9719e-05, 9.8348e-05, 9.8825e-05,\n",
            "        9.9301e-05, 1.0109e-04, 9.9421e-05, 1.0079e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.0031e-04, 9.9719e-05, 1.0186e-04, 9.9003e-05, 9.9719e-05, 9.9719e-05,\n",
            "        9.8705e-05, 1.0073e-04, 1.0073e-04, 9.9719e-05, 9.8348e-05, 9.8825e-05,\n",
            "        9.9301e-05, 1.0014e-04, 9.9421e-05, 9.9719e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([9.6083e-05, 9.6083e-05, 9.9719e-05, 9.9003e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.8705e-05, 9.6083e-05, 9.7096e-05, 9.9719e-05, 9.8348e-05, 9.5725e-05,\n",
            "        9.6202e-05, 9.5546e-05, 9.6083e-05, 9.6083e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([9.6083e-05, 9.6083e-05, 9.6083e-05, 9.5487e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.5248e-05, 9.6083e-05, 9.7096e-05, 9.6083e-05, 9.4652e-05, 9.6083e-05,\n",
            "        9.6202e-05, 9.5546e-05, 9.6083e-05, 9.6083e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([9.6083e-05, 9.6083e-05, 9.6083e-05, 9.5725e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.5248e-05, 9.6083e-05, 9.7096e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.5546e-05, 9.6083e-05, 9.6083e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([9.6083e-05, 9.6083e-05, 9.6083e-05, 9.5725e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.6083e-05, 9.7096e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.6202e-05, 9.6083e-05, 9.6083e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([9.6083e-05, 9.6083e-05, 9.6083e-05, 9.5725e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.6083e-05, 9.7096e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.7096e-05, 9.6083e-05, 9.6083e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([9.6083e-05, 9.6083e-05, 9.6083e-05, 9.5725e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.7096e-05, 9.6083e-05, 9.6083e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([9.6083e-05, 9.6083e-05, 9.6083e-05, 9.5725e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([9.5725e-05, 9.5725e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05,\n",
            "        9.4116e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05, 9.4116e-05, 9.6083e-05,\n",
            "        9.6083e-05, 9.6083e-05, 9.6083e-05, 9.6083e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([9.5725e-05, 9.3877e-05, 9.4116e-05, 9.4116e-05, 9.4116e-05, 9.5725e-05,\n",
            "        9.4116e-05, 9.6083e-05, 9.4116e-05, 9.3877e-05, 9.3877e-05, 9.6083e-05,\n",
            "        9.4116e-05, 9.4116e-05, 9.4116e-05, 9.4116e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([9.3877e-05, 9.3877e-05, 9.4116e-05, 9.4116e-05, 9.4116e-05, 9.3877e-05,\n",
            "        9.3877e-05, 9.4116e-05, 9.4116e-05, 9.3877e-05, 9.3877e-05, 9.4116e-05,\n",
            "        9.4116e-05, 9.4116e-05, 9.4116e-05, 9.4116e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([9.3877e-05, 9.3877e-05, 9.4116e-05, 9.4116e-05, 9.4116e-05, 9.3877e-05,\n",
            "        9.3877e-05, 9.4116e-05, 9.4116e-05, 9.3877e-05, 9.3877e-05, 9.4116e-05,\n",
            "        9.4116e-05, 9.4116e-05, 9.4116e-05, 9.4116e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([9.3877e-05, 9.3877e-05, 9.4116e-05, 9.4116e-05, 9.4116e-05, 9.3877e-05,\n",
            "        9.3877e-05, 9.4116e-05, 9.4116e-05, 9.3877e-05, 9.3877e-05, 9.4116e-05,\n",
            "        9.3877e-05, 9.4116e-05, 9.4116e-05, 9.4116e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([9.3877e-05, 9.3877e-05, 9.4116e-05, 9.4116e-05, 9.4116e-05, 9.3877e-05,\n",
            "        9.3877e-05, 9.3877e-05, 9.4116e-05, 9.3877e-05, 9.3877e-05, 9.3877e-05,\n",
            "        9.3877e-05, 9.4116e-05, 9.4116e-05, 9.3877e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0004, 0.0003, 0.0004,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([3.7432e-05, 3.8147e-05, 5.8591e-05, 5.3227e-05, 4.3750e-05, 5.0366e-05,\n",
            "        4.0710e-05, 3.1352e-05, 4.1485e-05, 4.9293e-05, 5.1379e-05, 3.3498e-05,\n",
            "        4.1842e-05, 4.4703e-05, 4.5359e-05, 3.8862e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([4.0531e-05, 4.1425e-05, 5.6863e-05, 4.3571e-05, 3.8147e-05, 4.3273e-05,\n",
            "        4.2796e-05, 3.8266e-05, 4.0114e-05, 5.6267e-05, 4.4584e-05, 3.8624e-05,\n",
            "        4.9889e-05, 4.6074e-05, 4.5002e-05, 3.7551e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.3273e-05, 3.7074e-05, 5.2869e-05, 4.0233e-05, 3.8624e-05, 4.8518e-05,\n",
            "        4.4823e-05, 3.6359e-05, 3.5822e-05, 4.8876e-05, 5.0724e-05, 4.2200e-05,\n",
            "        3.9101e-05, 3.9756e-05, 4.6253e-05, 3.7253e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.6776e-05, 3.4571e-05, 5.2154e-05, 3.7074e-05, 3.5465e-05, 3.5763e-05,\n",
            "        3.6657e-05, 3.4690e-05, 3.4273e-05, 4.8757e-05, 4.3809e-05, 3.4809e-05,\n",
            "        3.9399e-05, 3.4094e-05, 4.2260e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.4750e-05, 3.6359e-05, 3.3796e-05, 3.2902e-05, 3.5822e-05, 3.4034e-05,\n",
            "        3.6299e-05, 3.5107e-05, 3.5703e-05, 3.5346e-05, 3.3796e-05, 3.5942e-05,\n",
            "        3.2902e-05, 3.2425e-05, 3.5405e-05, 3.8087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.5048e-05, 3.3438e-05, 3.3736e-05, 3.3259e-05, 3.4750e-05, 3.4213e-05,\n",
            "        3.4988e-05, 3.3915e-05, 3.4273e-05, 3.3796e-05, 3.3915e-05, 3.4571e-05,\n",
            "        3.3081e-05, 3.2604e-05, 3.5763e-05, 3.6776e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.5048e-05, 3.3557e-05, 3.3855e-05, 3.3677e-05, 3.5048e-05, 3.3736e-05,\n",
            "        3.5048e-05, 3.3855e-05, 3.4451e-05, 3.3796e-05, 3.4153e-05, 3.4750e-05,\n",
            "        3.3379e-05, 3.2723e-05, 3.5882e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.4809e-05, 3.3557e-05, 3.3855e-05, 3.3915e-05, 3.5048e-05, 3.3796e-05,\n",
            "        3.5048e-05, 3.3975e-05, 3.4511e-05, 3.3736e-05, 3.4213e-05, 3.5048e-05,\n",
            "        3.3617e-05, 3.4392e-05, 3.4511e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.4809e-05, 3.3557e-05, 3.3855e-05, 3.4213e-05, 3.4809e-05, 3.4034e-05,\n",
            "        3.5048e-05, 3.4034e-05, 3.4750e-05, 3.3915e-05, 3.4153e-05, 3.5048e-05,\n",
            "        3.4988e-05, 3.4630e-05, 3.4809e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.3617e-05, 3.3557e-05, 3.3796e-05, 3.4690e-05, 3.4809e-05, 3.4392e-05,\n",
            "        3.3557e-05, 3.2902e-05, 3.4988e-05, 3.3975e-05, 3.4511e-05, 3.5048e-05,\n",
            "        3.5048e-05, 3.4869e-05, 3.5048e-05, 3.4630e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.3617e-05, 3.3557e-05, 3.3915e-05, 3.4809e-05, 3.4809e-05, 3.4392e-05,\n",
            "        3.3557e-05, 3.3021e-05, 3.5048e-05, 3.4034e-05, 3.4630e-05, 3.5048e-05,\n",
            "        3.5048e-05, 3.3498e-05, 3.3557e-05, 3.4630e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.2127e-05, 3.2067e-05, 3.3975e-05, 3.4988e-05, 3.1829e-05, 3.2067e-05,\n",
            "        3.2067e-05, 3.1769e-05, 3.3319e-05, 3.2246e-05, 3.2961e-05, 3.3200e-05,\n",
            "        3.2067e-05, 3.2067e-05, 3.2067e-05, 3.3081e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.2246e-05, 3.2067e-05, 3.2246e-05, 3.3319e-05, 3.1829e-05, 3.2067e-05,\n",
            "        3.2067e-05, 3.1769e-05, 3.2246e-05, 3.2365e-05, 3.3200e-05, 3.3200e-05,\n",
            "        3.2067e-05, 3.2067e-05, 3.2067e-05, 3.0875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.2246e-05, 3.2067e-05, 3.2425e-05, 3.2067e-05, 3.1829e-05, 3.2067e-05,\n",
            "        3.2067e-05, 3.1888e-05, 3.2246e-05, 3.2663e-05, 3.3319e-05, 3.3200e-05,\n",
            "        3.2067e-05, 3.2067e-05, 3.2067e-05, 3.0875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.0875e-05, 3.2067e-05, 3.2485e-05, 3.2067e-05, 3.2067e-05, 3.2067e-05,\n",
            "        3.0875e-05, 3.1888e-05, 3.0875e-05, 3.2842e-05, 3.3319e-05, 3.1829e-05,\n",
            "        3.0756e-05, 3.2067e-05, 3.2067e-05, 3.0875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.0875e-05, 3.2067e-05, 3.1352e-05, 3.2067e-05, 3.2067e-05, 3.2067e-05,\n",
            "        3.0875e-05, 3.0875e-05, 3.0875e-05, 3.3140e-05, 3.3200e-05, 3.0756e-05,\n",
            "        3.0756e-05, 3.2067e-05, 3.2067e-05, 3.0875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.0875e-05, 3.2067e-05, 3.1710e-05, 3.0875e-05, 3.0875e-05, 3.2067e-05,\n",
            "        3.0875e-05, 3.0756e-05, 3.0875e-05, 3.3319e-05, 3.3200e-05, 3.0756e-05,\n",
            "        3.0756e-05, 3.2067e-05, 3.2067e-05, 3.0875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.0875e-05, 3.0756e-05, 3.1948e-05, 3.0875e-05, 3.0875e-05, 3.2067e-05,\n",
            "        3.0875e-05, 3.0756e-05, 3.0756e-05, 3.2067e-05, 3.1829e-05, 3.0756e-05,\n",
            "        3.0756e-05, 3.2067e-05, 3.0875e-05, 3.0875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.0875e-05, 3.0756e-05, 3.1829e-05, 3.0875e-05, 3.0875e-05, 3.2067e-05,\n",
            "        3.0756e-05, 3.0756e-05, 3.0756e-05, 3.0935e-05, 3.1829e-05, 3.0756e-05,\n",
            "        3.0756e-05, 3.2067e-05, 3.0875e-05, 3.0875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.0875e-05, 3.0756e-05, 3.1829e-05, 3.0875e-05, 3.0875e-05, 3.2067e-05,\n",
            "        3.0756e-05, 3.0756e-05, 3.0756e-05, 3.0935e-05, 3.0696e-05, 3.0756e-05,\n",
            "        3.0756e-05, 3.0756e-05, 3.0875e-05, 3.0875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0006, 0.0005, 0.0006, 0.0006, 0.0006, 0.0005, 0.0006, 0.0006, 0.0007,\n",
            "        0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0007, 0.0005, 0.0005, 0.0006, 0.0006, 0.0005, 0.0006, 0.0006, 0.0007,\n",
            "        0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0006, 0.0006, 0.0005, 0.0006, 0.0006, 0.0005, 0.0006, 0.0007, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0011, 0.0012, 0.0010, 0.0011, 0.0011, 0.0011, 0.0009, 0.0011, 0.0011,\n",
            "        0.0010, 0.0011, 0.0009, 0.0010, 0.0011, 0.0011, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0011, 0.0011, 0.0010, 0.0011, 0.0011, 0.0011, 0.0010, 0.0011, 0.0012,\n",
            "        0.0010, 0.0010, 0.0009, 0.0010, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0011, 0.0011, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0011, 0.0011,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0011, 0.0011, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0010, 0.0011, 0.0010, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0011, 0.0010, 0.0010, 0.0010, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0012, 0.0011, 0.0011, 0.0010, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0012, 0.0011, 0.0011, 0.0011, 0.0012, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0010, 0.0011, 0.0012],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0012, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0012, 0.0012, 0.0011, 0.0011, 0.0011, 0.0011, 0.0012],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0012, 0.0011, 0.0011,\n",
            "        0.0012, 0.0011, 0.0012, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
            "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0061, 0.0064, 0.0061, 0.0064, 0.0065, 0.0062, 0.0061, 0.0064, 0.0061,\n",
            "        0.0061, 0.0062, 0.0063, 0.0061, 0.0060, 0.0064, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0061, 0.0064, 0.0062, 0.0064, 0.0065, 0.0063, 0.0061, 0.0065, 0.0062,\n",
            "        0.0061, 0.0062, 0.0063, 0.0062, 0.0061, 0.0062, 0.0063],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0062, 0.0064, 0.0063, 0.0064, 0.0065, 0.0064, 0.0062, 0.0063, 0.0063,\n",
            "        0.0062, 0.0063, 0.0063, 0.0063, 0.0061, 0.0063, 0.0063],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
            "        0.0062, 0.0062, 0.0062, 0.0062, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0060, 0.0060, 0.0061, 0.0060, 0.0060, 0.0061, 0.0061, 0.0060, 0.0060,\n",
            "        0.0061, 0.0061, 0.0061, 0.0060, 0.0060, 0.0061, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0060, 0.0060, 0.0061, 0.0060, 0.0060, 0.0061, 0.0060, 0.0060, 0.0060,\n",
            "        0.0061, 0.0060, 0.0061, 0.0060, 0.0060, 0.0061, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0061, 0.0060, 0.0060, 0.0060,\n",
            "        0.0060, 0.0060, 0.0061, 0.0060, 0.0060, 0.0061, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060,\n",
            "        0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0060, 0.0060, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
            "        0.0060, 0.0060, 0.0062, 0.0060, 0.0060, 0.0062, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
            "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
            "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
            "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
            "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062,\n",
            "        0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0062, 0.0062, 0.0060, 0.0060, 0.0060, 0.0060, 0.0062, 0.0060, 0.0060,\n",
            "        0.0062, 0.0062, 0.0060, 0.0062, 0.0062, 0.0060, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060,\n",
            "        0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060,\n",
            "        0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060,\n",
            "        0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060,\n",
            "        0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060,\n",
            "        0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060, 0.0060],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([1.7881e-05, 1.7941e-05, 1.5914e-05, 1.1146e-05, 1.6093e-05, 2.0027e-05,\n",
            "        1.7285e-05, 2.7955e-05, 1.9729e-05, 1.6093e-05, 1.5557e-05, 1.4067e-05,\n",
            "        2.3067e-05, 1.7285e-05, 2.0206e-05, 2.3723e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.1458e-05, 1.6689e-05, 1.7345e-05, 1.3173e-05, 1.7345e-05, 2.3186e-05,\n",
            "        1.2457e-05, 2.1517e-05, 1.4544e-05, 1.5795e-05, 1.1683e-05, 1.4722e-05,\n",
            "        2.3961e-05, 2.1875e-05, 2.3544e-05, 2.7120e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.5914e-05, 1.7345e-05, 2.0266e-05, 1.4842e-05, 2.1338e-05, 1.8060e-05,\n",
            "        1.4603e-05, 2.2352e-05, 1.6332e-05, 1.8477e-05, 1.5259e-05, 1.6153e-05,\n",
            "        2.1338e-05, 1.9729e-05, 2.0027e-05, 2.6524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.7107e-05, 1.7166e-05, 1.9193e-05, 1.6451e-05, 1.7703e-05, 1.9073e-05,\n",
            "        1.6510e-05, 1.7464e-05, 1.7941e-05, 1.6928e-05, 1.7881e-05, 1.7822e-05,\n",
            "        1.9073e-05, 1.8120e-05, 1.9670e-05, 2.0444e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.4067e-05, 1.7941e-05, 1.9073e-05, 1.4365e-05, 1.6987e-05, 1.8179e-05,\n",
            "        1.6809e-05, 1.6689e-05, 1.4305e-05, 1.6212e-05, 1.5736e-05, 1.5140e-05,\n",
            "        1.8179e-05, 1.7941e-05, 1.8716e-05, 1.9491e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.4126e-05, 1.8358e-05, 1.9372e-05, 1.4126e-05, 1.7226e-05, 1.6749e-05,\n",
            "        1.4842e-05, 1.5616e-05, 1.4365e-05, 1.5140e-05, 1.6093e-05, 1.5438e-05,\n",
            "        1.6749e-05, 1.6689e-05, 1.7226e-05, 1.7345e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.4782e-05, 1.6928e-05, 1.7405e-05, 1.4246e-05, 1.5020e-05, 1.6272e-05,\n",
            "        1.4544e-05, 1.6689e-05, 1.4484e-05, 1.5914e-05, 1.6272e-05, 1.5438e-05,\n",
            "        1.5676e-05, 1.6749e-05, 1.6630e-05, 1.6689e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.4901e-05, 1.6928e-05, 1.7464e-05, 1.4484e-05, 1.5020e-05, 1.6272e-05,\n",
            "        1.4544e-05, 1.6809e-05, 1.4544e-05, 1.5974e-05, 1.6272e-05, 1.5497e-05,\n",
            "        1.5676e-05, 1.6809e-05, 1.5616e-05, 1.6689e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.4424e-05, 1.7107e-05, 1.7583e-05, 1.4722e-05, 1.5140e-05, 1.6272e-05,\n",
            "        1.4722e-05, 1.5914e-05, 1.4544e-05, 1.6034e-05, 1.6451e-05, 1.5676e-05,\n",
            "        1.5736e-05, 1.6868e-05, 1.5736e-05, 1.5736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.4544e-05, 1.7107e-05, 1.7583e-05, 1.4782e-05, 1.5199e-05, 1.5438e-05,\n",
            "        1.4663e-05, 1.5914e-05, 1.4722e-05, 1.6153e-05, 1.5736e-05, 1.4961e-05,\n",
            "        1.5855e-05, 1.6987e-05, 1.5676e-05, 1.5736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.4603e-05, 1.7107e-05, 1.6809e-05, 1.4782e-05, 1.5259e-05, 1.4901e-05,\n",
            "        1.4663e-05, 1.5318e-05, 1.4126e-05, 1.6093e-05, 1.5736e-05, 1.4901e-05,\n",
            "        1.5855e-05, 1.6987e-05, 1.5736e-05, 1.5736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.4722e-05, 1.7107e-05, 1.6749e-05, 1.2696e-05, 1.5199e-05, 1.5140e-05,\n",
            "        1.4067e-05, 1.5318e-05, 1.4126e-05, 1.6272e-05, 1.4842e-05, 1.4305e-05,\n",
            "        1.5914e-05, 1.5974e-05, 1.5855e-05, 1.5736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.3292e-05, 1.6272e-05, 1.6272e-05, 1.2696e-05, 1.5199e-05, 1.5199e-05,\n",
            "        1.4246e-05, 1.5378e-05, 1.4067e-05, 1.6332e-05, 1.3530e-05, 1.3411e-05,\n",
            "        1.6034e-05, 1.5974e-05, 1.5974e-05, 1.5795e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.2755e-05, 1.6272e-05, 1.6272e-05, 1.2696e-05, 1.5259e-05, 1.5378e-05,\n",
            "        1.3351e-05, 1.5378e-05, 1.3530e-05, 1.6332e-05, 1.3530e-05, 1.3411e-05,\n",
            "        1.6153e-05, 1.5974e-05, 1.6034e-05, 1.5855e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.2755e-05, 1.3828e-05, 1.3828e-05, 1.2636e-05, 1.5318e-05, 1.5438e-05,\n",
            "        1.3411e-05, 1.5378e-05, 1.3530e-05, 1.3888e-05, 1.3530e-05, 1.3411e-05,\n",
            "        1.6093e-05, 1.5974e-05, 1.5259e-05, 1.5974e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.2696e-05, 1.3709e-05, 1.3828e-05, 1.2636e-05, 1.3173e-05, 1.3113e-05,\n",
            "        1.3411e-05, 1.2815e-05, 1.3530e-05, 1.3828e-05, 1.3530e-05, 1.3411e-05,\n",
            "        1.5199e-05, 1.3053e-05, 1.5318e-05, 1.5080e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.2696e-05, 1.3053e-05, 1.3113e-05, 1.2636e-05, 1.3053e-05, 1.3173e-05,\n",
            "        1.3411e-05, 1.2815e-05, 1.2636e-05, 1.3828e-05, 1.3530e-05, 1.3411e-05,\n",
            "        1.3053e-05, 1.3173e-05, 1.3113e-05, 1.3232e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.2636e-05, 1.3053e-05, 1.3113e-05, 1.2636e-05, 1.3053e-05, 1.3113e-05,\n",
            "        1.3411e-05, 1.2875e-05, 1.2636e-05, 1.3828e-05, 1.2696e-05, 1.3411e-05,\n",
            "        1.2994e-05, 1.3113e-05, 1.3173e-05, 1.3173e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.2636e-05, 1.3053e-05, 1.3113e-05, 1.2636e-05, 1.3053e-05, 1.3113e-05,\n",
            "        1.3411e-05, 1.2934e-05, 1.2636e-05, 1.3053e-05, 1.2696e-05, 1.2577e-05,\n",
            "        1.3113e-05, 1.2636e-05, 1.3053e-05, 1.3113e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.2636e-05, 1.3053e-05, 1.3113e-05, 1.2636e-05, 1.3053e-05, 1.3113e-05,\n",
            "        1.2577e-05, 1.2994e-05, 1.2636e-05, 1.3053e-05, 1.2696e-05, 1.2577e-05,\n",
            "        1.2994e-05, 1.2696e-05, 1.3053e-05, 1.3053e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([4.0472e-05, 4.2081e-05, 3.2842e-05, 5.0008e-05, 3.7134e-05, 3.8266e-05,\n",
            "        4.1604e-05, 4.2140e-05, 3.7253e-05, 3.6120e-05, 4.6432e-05, 3.8803e-05,\n",
            "        4.2260e-05, 4.3869e-05, 4.4346e-05, 3.2663e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.0756e-05, 3.5703e-05, 3.5524e-05, 4.5538e-05, 3.9995e-05, 3.5346e-05,\n",
            "        5.0366e-05, 4.6432e-05, 4.1008e-05, 4.0054e-05, 4.1127e-05, 3.8803e-05,\n",
            "        3.9756e-05, 4.0352e-05, 4.4465e-05, 3.2246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.5405e-05, 3.5405e-05, 3.8147e-05, 4.9412e-05, 3.4630e-05, 3.4034e-05,\n",
            "        5.2392e-05, 4.3213e-05, 3.9756e-05, 3.7193e-05, 4.7565e-05, 4.2617e-05,\n",
            "        3.6776e-05, 4.7207e-05, 4.3273e-05, 3.2723e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.4094e-05, 3.4153e-05, 3.5465e-05, 4.0233e-05, 3.3379e-05, 3.2067e-05,\n",
            "        3.6359e-05, 3.4332e-05, 3.6359e-05, 3.5763e-05, 3.5346e-05, 4.2021e-05,\n",
            "        3.2961e-05, 3.6180e-05, 3.5703e-05, 3.3081e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.5167e-05, 3.4750e-05, 3.6597e-05, 3.2127e-05, 3.3379e-05, 3.3021e-05,\n",
            "        3.5048e-05, 3.2604e-05, 3.4511e-05, 3.5703e-05, 3.3557e-05, 3.6180e-05,\n",
            "        3.3438e-05, 3.5048e-05, 3.4928e-05, 3.5167e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.3617e-05, 3.3200e-05, 3.5107e-05, 3.2365e-05, 3.2008e-05, 3.1531e-05,\n",
            "        3.5465e-05, 3.3081e-05, 3.4750e-05, 3.5942e-05, 3.3796e-05, 3.4928e-05,\n",
            "        3.2961e-05, 3.5286e-05, 3.5763e-05, 3.3617e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.3617e-05, 3.3200e-05, 3.3796e-05, 3.2485e-05, 3.3617e-05, 3.2187e-05,\n",
            "        3.5822e-05, 3.3498e-05, 3.4988e-05, 3.6299e-05, 3.4094e-05, 3.4988e-05,\n",
            "        3.2961e-05, 3.3855e-05, 3.5882e-05, 3.3617e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.3796e-05, 3.3200e-05, 3.3796e-05, 3.4094e-05, 3.3617e-05, 3.3796e-05,\n",
            "        3.5882e-05, 3.4988e-05, 3.4988e-05, 3.4988e-05, 3.4392e-05, 3.4988e-05,\n",
            "        3.3736e-05, 3.4988e-05, 3.5882e-05, 3.3617e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.3796e-05, 3.3796e-05, 3.3796e-05, 3.4273e-05, 3.3617e-05, 3.3796e-05,\n",
            "        3.5882e-05, 3.4988e-05, 3.4988e-05, 3.4988e-05, 3.4630e-05, 3.4988e-05,\n",
            "        3.3796e-05, 3.4988e-05, 3.3438e-05, 3.3617e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.3796e-05, 3.2961e-05, 3.3796e-05, 3.3081e-05, 3.3796e-05, 3.3796e-05,\n",
            "        3.5882e-05, 3.4988e-05, 3.4988e-05, 3.4988e-05, 3.4809e-05, 3.4988e-05,\n",
            "        3.3796e-05, 3.4988e-05, 3.3438e-05, 3.3796e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.3796e-05, 3.2961e-05, 3.3796e-05, 3.3319e-05, 3.3796e-05, 3.3796e-05,\n",
            "        3.5882e-05, 3.4988e-05, 3.4988e-05, 3.4988e-05, 3.4988e-05, 3.4988e-05,\n",
            "        3.3796e-05, 3.4988e-05, 3.3438e-05, 3.3796e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.3796e-05, 3.2961e-05, 3.2961e-05, 3.3438e-05, 3.3796e-05, 3.2961e-05,\n",
            "        3.5882e-05, 3.1948e-05, 3.3200e-05, 3.2246e-05, 3.1948e-05, 3.3200e-05,\n",
            "        3.3796e-05, 3.2246e-05, 3.1948e-05, 3.2961e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.2663e-05, 3.1769e-05, 3.1769e-05, 3.1948e-05, 3.2663e-05, 3.1769e-05,\n",
            "        3.3200e-05, 3.1948e-05, 3.3200e-05, 3.2246e-05, 3.1948e-05, 3.2246e-05,\n",
            "        3.1590e-05, 3.2246e-05, 3.1948e-05, 3.1769e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.2663e-05, 3.1769e-05, 3.1054e-05, 3.1948e-05, 3.2663e-05, 3.1590e-05,\n",
            "        3.3200e-05, 3.1948e-05, 3.1948e-05, 3.0816e-05, 3.1710e-05, 3.2246e-05,\n",
            "        3.1590e-05, 3.2246e-05, 3.1948e-05, 3.1769e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.1769e-05, 3.1769e-05, 3.1054e-05, 3.1948e-05, 3.2663e-05, 3.1590e-05,\n",
            "        3.1948e-05, 3.1948e-05, 3.0577e-05, 3.0816e-05, 3.1710e-05, 3.2246e-05,\n",
            "        3.1769e-05, 3.0816e-05, 3.1948e-05, 3.1769e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.1769e-05, 3.1769e-05, 3.1054e-05, 3.1948e-05, 3.2663e-05, 3.1590e-05,\n",
            "        3.1948e-05, 3.1948e-05, 3.0577e-05, 3.0816e-05, 3.1710e-05, 3.0816e-05,\n",
            "        3.1769e-05, 3.0816e-05, 3.1948e-05, 3.1769e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.1769e-05, 3.1769e-05, 3.1054e-05, 3.1710e-05, 3.2663e-05, 3.1590e-05,\n",
            "        3.0816e-05, 3.1948e-05, 3.0577e-05, 3.0816e-05, 3.1710e-05, 3.0577e-05,\n",
            "        3.1769e-05, 3.0816e-05, 3.1948e-05, 3.1769e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.1769e-05, 3.1769e-05, 3.1054e-05, 3.0577e-05, 3.1769e-05, 3.1590e-05,\n",
            "        3.0816e-05, 3.1948e-05, 3.0577e-05, 3.0577e-05, 3.0577e-05, 3.0577e-05,\n",
            "        3.1769e-05, 3.0816e-05, 3.1948e-05, 3.1054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.1769e-05, 3.1590e-05, 3.1054e-05, 3.0577e-05, 3.1769e-05, 3.1590e-05,\n",
            "        3.0816e-05, 3.0816e-05, 3.0577e-05, 3.0577e-05, 3.0577e-05, 3.0577e-05,\n",
            "        3.1769e-05, 3.0816e-05, 3.1948e-05, 3.1054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.1769e-05, 3.1590e-05, 3.1054e-05, 3.0577e-05, 3.1769e-05, 3.1590e-05,\n",
            "        3.0816e-05, 3.0577e-05, 3.0577e-05, 3.0577e-05, 3.0577e-05, 3.0577e-05,\n",
            "        3.1769e-05, 3.0816e-05, 3.0816e-05, 3.1054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.1187e-05, 3.8981e-05, 3.8266e-05, 4.3392e-05, 4.3988e-05, 4.0174e-05,\n",
            "        4.2617e-05, 4.9293e-05, 4.2975e-05, 4.1842e-05, 4.1008e-05, 5.2691e-05,\n",
            "        4.8757e-05, 5.3883e-05, 4.1842e-05, 5.8174e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([4.1008e-05, 4.6134e-05, 3.8028e-05, 4.7565e-05, 4.6551e-05, 4.6551e-05,\n",
            "        4.9531e-05, 5.2452e-05, 4.1664e-05, 4.8161e-05, 3.9518e-05, 4.9174e-05,\n",
            "        4.2379e-05, 4.7863e-05, 3.9399e-05, 6.1035e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.2737e-05, 3.8505e-05, 3.8981e-05, 4.7684e-05, 4.8935e-05, 3.8862e-05,\n",
            "        4.5180e-05, 5.3048e-05, 4.4525e-05, 4.9710e-05, 4.1306e-05, 4.0770e-05,\n",
            "        4.4525e-05, 4.2617e-05, 4.0293e-05, 4.4942e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.5465e-05, 3.7432e-05, 3.6895e-05, 4.1544e-05, 4.4405e-05, 3.7193e-05,\n",
            "        3.8862e-05, 3.9637e-05, 3.9697e-05, 3.7134e-05, 3.8624e-05, 3.8922e-05,\n",
            "        3.6418e-05, 3.6776e-05, 3.6955e-05, 3.5167e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.6478e-05, 3.9160e-05, 3.9399e-05, 4.2021e-05, 3.9041e-05, 3.8445e-05,\n",
            "        3.9041e-05, 3.9876e-05, 3.9697e-05, 3.8862e-05, 3.8683e-05, 4.0591e-05,\n",
            "        3.7372e-05, 3.8743e-05, 3.8266e-05, 3.5048e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.6478e-05, 3.8445e-05, 3.9399e-05, 3.9399e-05, 3.9339e-05, 3.8445e-05,\n",
            "        3.9339e-05, 4.0233e-05, 3.8981e-05, 3.9101e-05, 3.8683e-05, 4.1127e-05,\n",
            "        3.7491e-05, 3.9041e-05, 3.8445e-05, 3.8028e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.8445e-05, 3.8445e-05, 3.9876e-05, 3.7789e-05, 3.9399e-05, 3.8445e-05,\n",
            "        3.8564e-05, 3.9399e-05, 3.9101e-05, 3.9101e-05, 3.7432e-05, 4.1187e-05,\n",
            "        3.7611e-05, 3.9399e-05, 3.8445e-05, 3.8445e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.8445e-05, 3.8445e-05, 3.8445e-05, 3.7789e-05, 3.9399e-05, 3.8445e-05,\n",
            "        3.7789e-05, 3.9399e-05, 3.9160e-05, 3.9101e-05, 3.7432e-05, 3.9399e-05,\n",
            "        3.7789e-05, 3.9399e-05, 3.8445e-05, 3.8922e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.8445e-05, 3.8445e-05, 3.8445e-05, 3.7789e-05, 3.9399e-05, 3.8445e-05,\n",
            "        3.7789e-05, 3.9399e-05, 3.9101e-05, 3.9101e-05, 3.7432e-05, 3.9399e-05,\n",
            "        3.7789e-05, 3.9399e-05, 3.8445e-05, 3.9160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.8445e-05, 3.8445e-05, 3.8445e-05, 3.7789e-05, 3.9399e-05, 3.8445e-05,\n",
            "        3.7789e-05, 3.9399e-05, 3.8505e-05, 3.9101e-05, 3.7432e-05, 3.7789e-05,\n",
            "        3.7789e-05, 3.9399e-05, 3.8445e-05, 3.9399e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.8445e-05, 3.8445e-05, 3.8445e-05, 3.7789e-05, 3.9399e-05, 3.8445e-05,\n",
            "        3.7789e-05, 3.9399e-05, 3.8564e-05, 3.9101e-05, 3.7432e-05, 3.7789e-05,\n",
            "        3.7789e-05, 3.9399e-05, 3.8445e-05, 3.9399e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.5942e-05, 3.8445e-05, 3.5942e-05, 3.5882e-05, 3.5882e-05, 3.8445e-05,\n",
            "        3.5882e-05, 3.7491e-05, 3.7134e-05, 3.7253e-05, 3.5942e-05, 3.5882e-05,\n",
            "        3.5882e-05, 3.7491e-05, 3.8445e-05, 3.7491e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.5942e-05, 3.7074e-05, 3.5942e-05, 3.5882e-05, 3.5882e-05, 3.7074e-05,\n",
            "        3.5882e-05, 3.5882e-05, 3.5763e-05, 3.7253e-05, 3.5942e-05, 3.5882e-05,\n",
            "        3.5882e-05, 3.5882e-05, 3.5942e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.5942e-05, 3.7074e-05, 3.5942e-05, 3.5882e-05, 3.5703e-05, 3.7074e-05,\n",
            "        3.5882e-05, 3.5882e-05, 3.5763e-05, 3.7491e-05, 3.5942e-05, 3.5882e-05,\n",
            "        3.5703e-05, 3.5882e-05, 3.5942e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.5942e-05, 3.7074e-05, 3.5167e-05, 3.5882e-05, 3.5703e-05, 3.7074e-05,\n",
            "        3.5882e-05, 3.5882e-05, 3.5882e-05, 3.4630e-05, 3.5942e-05, 3.4630e-05,\n",
            "        3.5703e-05, 3.5703e-05, 3.5942e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.5763e-05, 3.7074e-05, 3.5167e-05, 3.5882e-05, 3.5703e-05, 3.7074e-05,\n",
            "        3.5882e-05, 3.5882e-05, 3.5942e-05, 3.4630e-05, 3.5763e-05, 3.4630e-05,\n",
            "        3.5703e-05, 3.5703e-05, 3.5942e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.5763e-05, 3.7074e-05, 3.5167e-05, 3.5703e-05, 3.4392e-05, 3.4988e-05,\n",
            "        3.5882e-05, 3.5882e-05, 3.4630e-05, 3.4630e-05, 3.4988e-05, 3.4630e-05,\n",
            "        3.5703e-05, 3.4392e-05, 3.5763e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.4988e-05, 3.6895e-05, 3.5167e-05, 3.5703e-05, 3.4392e-05, 3.4988e-05,\n",
            "        3.5882e-05, 3.4630e-05, 3.4451e-05, 3.4630e-05, 3.4988e-05, 3.4630e-05,\n",
            "        3.5703e-05, 3.4392e-05, 3.5763e-05, 3.5882e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.4988e-05, 3.5763e-05, 3.4988e-05, 3.5703e-05, 3.4392e-05, 3.4988e-05,\n",
            "        3.5882e-05, 3.4630e-05, 3.4392e-05, 3.4630e-05, 3.4988e-05, 3.4630e-05,\n",
            "        3.5703e-05, 3.4392e-05, 3.4988e-05, 3.4630e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.4988e-05, 3.5763e-05, 3.4988e-05, 3.4392e-05, 3.4392e-05, 3.4988e-05,\n",
            "        3.5882e-05, 3.4630e-05, 3.4451e-05, 3.4630e-05, 3.4988e-05, 3.4630e-05,\n",
            "        3.5703e-05, 3.4392e-05, 3.4988e-05, 3.4630e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.6464e-05, 2.4021e-05, 3.0458e-05, 2.6166e-05, 2.4140e-05, 2.6226e-05,\n",
            "        2.7955e-05, 2.8253e-05, 2.7061e-05, 2.0087e-05, 2.6941e-05, 2.4140e-05,\n",
            "        2.3425e-05, 3.4153e-05, 2.1517e-05, 2.5153e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.2246e-05, 2.7537e-05, 3.3855e-05, 2.7835e-05, 3.0100e-05, 2.2650e-05,\n",
            "        2.5570e-05, 3.2961e-05, 2.5213e-05, 2.0206e-05, 2.2769e-05, 1.9908e-05,\n",
            "        2.3305e-05, 2.8431e-05, 2.4080e-05, 2.6286e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.4438e-05, 3.1948e-05, 4.1485e-05, 2.6941e-05, 2.9683e-05, 2.7418e-05,\n",
            "        2.7299e-05, 3.3498e-05, 2.5094e-05, 2.4140e-05, 2.5630e-05, 2.2233e-05,\n",
            "        2.4498e-05, 3.2961e-05, 2.1338e-05, 2.3305e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.1994e-05, 3.2723e-05, 4.1842e-05, 2.5272e-05, 2.8431e-05, 2.4199e-05,\n",
            "        2.5749e-05, 2.5511e-05, 2.1696e-05, 2.1458e-05, 2.2233e-05, 2.1517e-05,\n",
            "        2.2948e-05, 2.3246e-05, 2.3186e-05, 2.3842e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.0087e-05, 2.6166e-05, 3.1531e-05, 2.6107e-05, 2.2173e-05, 2.2352e-05,\n",
            "        2.5094e-05, 2.2829e-05, 2.0683e-05, 2.2352e-05, 2.1040e-05, 1.9252e-05,\n",
            "        2.2411e-05, 2.3484e-05, 2.2054e-05, 2.2173e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.0266e-05, 2.2709e-05, 2.7180e-05, 2.6405e-05, 2.2411e-05, 2.1398e-05,\n",
            "        2.5332e-05, 2.3007e-05, 2.1815e-05, 2.2531e-05, 2.1279e-05, 2.0444e-05,\n",
            "        2.2829e-05, 2.3782e-05, 2.2411e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.2531e-05, 2.2709e-05, 2.6286e-05, 2.1100e-05, 2.2531e-05, 2.1458e-05,\n",
            "        2.3305e-05, 2.3067e-05, 2.1935e-05, 2.2650e-05, 2.1398e-05, 2.0385e-05,\n",
            "        2.2829e-05, 2.1994e-05, 2.2590e-05, 2.2531e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.2650e-05, 2.2829e-05, 2.4080e-05, 2.1636e-05, 2.2471e-05, 2.1517e-05,\n",
            "        2.3305e-05, 2.3067e-05, 2.0564e-05, 2.2769e-05, 2.1517e-05, 2.0444e-05,\n",
            "        2.1577e-05, 2.2113e-05, 2.1756e-05, 2.2471e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.2531e-05, 2.2054e-05, 2.3365e-05, 2.1040e-05, 2.1756e-05, 2.1517e-05,\n",
            "        2.3484e-05, 2.1696e-05, 2.0683e-05, 1.9729e-05, 1.9550e-05, 2.0564e-05,\n",
            "        2.0325e-05, 2.2233e-05, 2.1875e-05, 2.1636e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.1696e-05, 2.2113e-05, 2.3365e-05, 2.1160e-05, 2.1696e-05, 2.1577e-05,\n",
            "        2.3484e-05, 2.1696e-05, 2.0683e-05, 1.9848e-05, 1.9670e-05, 2.0564e-05,\n",
            "        2.0504e-05, 2.2471e-05, 2.0921e-05, 2.0623e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.1756e-05, 2.0862e-05, 2.3484e-05, 2.1279e-05, 2.1756e-05, 2.1815e-05,\n",
            "        2.2829e-05, 2.1696e-05, 2.0742e-05, 2.0921e-05, 1.9848e-05, 2.0623e-05,\n",
            "        1.9848e-05, 2.2471e-05, 1.9968e-05, 2.0623e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.0921e-05, 2.1160e-05, 2.3365e-05, 2.1338e-05, 2.1815e-05, 2.1756e-05,\n",
            "        2.1756e-05, 2.1756e-05, 2.0742e-05, 1.9729e-05, 1.9789e-05, 1.9729e-05,\n",
            "        1.9848e-05, 2.2650e-05, 1.9848e-05, 2.0683e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.9610e-05, 1.9908e-05, 2.3484e-05, 2.1398e-05, 1.9610e-05, 1.9550e-05,\n",
            "        2.1875e-05, 2.0564e-05, 1.9550e-05, 1.9729e-05, 1.8656e-05, 1.9729e-05,\n",
            "        1.8775e-05, 2.2650e-05, 1.8775e-05, 1.9550e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.9491e-05, 2.0027e-05, 2.1398e-05, 2.0683e-05, 1.9610e-05, 1.9550e-05,\n",
            "        2.1040e-05, 1.9491e-05, 1.9610e-05, 1.9729e-05, 1.8597e-05, 1.8597e-05,\n",
            "        1.8775e-05, 2.2650e-05, 1.8775e-05, 1.9431e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.9670e-05, 2.0027e-05, 2.1398e-05, 2.0921e-05, 1.9789e-05, 1.9550e-05,\n",
            "        2.0981e-05, 1.9610e-05, 1.8716e-05, 1.8597e-05, 1.8597e-05, 1.8597e-05,\n",
            "        1.8775e-05, 2.2709e-05, 1.8775e-05, 1.8597e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.9610e-05, 1.8656e-05, 2.0266e-05, 2.1040e-05, 1.8775e-05, 1.9550e-05,\n",
            "        1.7583e-05, 1.9670e-05, 1.8775e-05, 1.8597e-05, 1.8597e-05, 1.8597e-05,\n",
            "        1.8775e-05, 1.9193e-05, 1.8775e-05, 1.8597e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.9610e-05, 1.8656e-05, 1.7583e-05, 1.7464e-05, 1.8775e-05, 1.9550e-05,\n",
            "        1.7583e-05, 1.8775e-05, 1.8775e-05, 1.8597e-05, 1.8597e-05, 1.8597e-05,\n",
            "        1.7583e-05, 1.9193e-05, 1.8775e-05, 1.8597e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.9610e-05, 1.7405e-05, 1.7583e-05, 1.7703e-05, 1.8597e-05, 1.9550e-05,\n",
            "        1.7524e-05, 1.8775e-05, 1.8597e-05, 1.8597e-05, 1.8597e-05, 1.8597e-05,\n",
            "        1.7583e-05, 1.9252e-05, 1.8775e-05, 1.8597e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.8597e-05, 1.6391e-05, 1.7583e-05, 1.7703e-05, 1.8597e-05, 1.8418e-05,\n",
            "        1.7524e-05, 1.8775e-05, 1.8597e-05, 1.8597e-05, 1.8597e-05, 1.7762e-05,\n",
            "        1.7405e-05, 1.8299e-05, 1.8597e-05, 1.8597e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.7524e-05, 1.6391e-05, 1.7583e-05, 1.7703e-05, 1.8597e-05, 1.8418e-05,\n",
            "        1.7524e-05, 1.8775e-05, 1.7524e-05, 1.7524e-05, 1.7524e-05, 1.6689e-05,\n",
            "        1.7405e-05, 1.8299e-05, 1.7524e-05, 1.8597e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0007, 0.0006, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
            "        0.0008, 0.0006, 0.0007, 0.0006, 0.0007, 0.0007, 0.0007],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0007, 0.0006, 0.0007, 0.0007, 0.0007, 0.0007, 0.0006, 0.0007, 0.0007,\n",
            "        0.0007, 0.0007, 0.0007, 0.0006, 0.0007, 0.0008, 0.0007],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
            "        0.0007, 0.0007, 0.0007, 0.0006, 0.0007, 0.0007, 0.0007],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
            "        0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0007, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([2.5690e-05, 2.7776e-05, 2.2113e-05, 1.7166e-05, 2.7895e-05, 1.7345e-05,\n",
            "        1.4305e-05, 1.8239e-05, 1.8537e-05, 2.1517e-05, 2.5451e-05, 2.4736e-05,\n",
            "        1.7285e-05, 1.9908e-05, 2.4736e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.7776e-05, 2.5213e-05, 2.5749e-05, 1.3590e-05, 2.5690e-05, 1.5020e-05,\n",
            "        1.6391e-05, 1.7345e-05, 1.7643e-05, 2.1815e-05, 2.3425e-05, 2.3901e-05,\n",
            "        1.9550e-05, 1.6212e-05, 2.4855e-05, 1.7524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.1994e-05, 2.4021e-05, 2.3663e-05, 1.5676e-05, 1.9908e-05, 1.8001e-05,\n",
            "        1.9252e-05, 1.8835e-05, 1.7643e-05, 2.3246e-05, 2.2292e-05, 2.2888e-05,\n",
            "        1.9491e-05, 1.6451e-05, 2.4736e-05, 1.8477e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.3067e-05, 1.9968e-05, 2.5630e-05, 1.8060e-05, 1.9073e-05, 1.9550e-05,\n",
            "        2.1935e-05, 1.8775e-05, 2.0325e-05, 2.1935e-05, 1.9729e-05, 2.3663e-05,\n",
            "        1.8418e-05, 1.9014e-05, 1.8001e-05, 2.1935e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.8716e-05, 1.8775e-05, 2.5272e-05, 1.7226e-05, 2.0027e-05, 1.7583e-05,\n",
            "        1.6510e-05, 1.7583e-05, 1.8120e-05, 1.7226e-05, 1.8775e-05, 1.9908e-05,\n",
            "        1.6093e-05, 2.0504e-05, 1.7285e-05, 1.6093e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.7285e-05, 1.7345e-05, 1.7464e-05, 1.6391e-05, 1.8716e-05, 1.6928e-05,\n",
            "        1.6749e-05, 1.6928e-05, 1.7583e-05, 1.7285e-05, 1.7703e-05, 1.8656e-05,\n",
            "        1.6391e-05, 1.7762e-05, 1.7643e-05, 1.6332e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.7345e-05, 1.7405e-05, 1.7405e-05, 1.6510e-05, 1.7762e-05, 1.7107e-05,\n",
            "        1.6987e-05, 1.7047e-05, 1.6570e-05, 1.5676e-05, 1.7643e-05, 1.8716e-05,\n",
            "        1.6510e-05, 1.7822e-05, 1.7822e-05, 1.6570e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.7405e-05, 1.7464e-05, 1.7524e-05, 1.6689e-05, 1.7822e-05, 1.7107e-05,\n",
            "        1.5438e-05, 1.7166e-05, 1.6630e-05, 1.5199e-05, 1.8120e-05, 1.8775e-05,\n",
            "        1.6630e-05, 1.7881e-05, 1.7762e-05, 1.6630e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.6332e-05, 1.7464e-05, 1.7524e-05, 1.7047e-05, 1.7881e-05, 1.7285e-05,\n",
            "        1.5616e-05, 1.7166e-05, 1.6570e-05, 1.5318e-05, 1.8120e-05, 1.8299e-05,\n",
            "        1.6689e-05, 1.8001e-05, 1.7047e-05, 1.6749e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.6332e-05, 1.7643e-05, 1.7643e-05, 1.7107e-05, 1.7881e-05, 1.6510e-05,\n",
            "        1.5616e-05, 1.7285e-05, 1.6570e-05, 1.5378e-05, 1.7047e-05, 1.8418e-05,\n",
            "        1.6570e-05, 1.8060e-05, 1.7047e-05, 1.7285e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.6570e-05, 1.7703e-05, 1.7703e-05, 1.6451e-05, 1.7941e-05, 1.6510e-05,\n",
            "        1.6391e-05, 1.6510e-05, 1.7226e-05, 1.6153e-05, 1.6928e-05, 1.8418e-05,\n",
            "        1.6510e-05, 1.8179e-05, 1.7226e-05, 1.6630e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.6749e-05, 1.6809e-05, 1.7762e-05, 1.5020e-05, 1.8060e-05, 1.4305e-05,\n",
            "        1.5616e-05, 1.5855e-05, 1.7226e-05, 1.6212e-05, 1.7107e-05, 1.8418e-05,\n",
            "        1.5676e-05, 1.7226e-05, 1.7285e-05, 1.5080e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.6868e-05, 1.6928e-05, 1.7762e-05, 1.5020e-05, 1.7226e-05, 1.4305e-05,\n",
            "        1.5616e-05, 1.5020e-05, 1.7226e-05, 1.5438e-05, 1.7047e-05, 1.7941e-05,\n",
            "        1.5080e-05, 1.7226e-05, 1.7285e-05, 1.5080e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7047e-05, 1.7047e-05, 1.5914e-05, 1.5020e-05, 1.7226e-05, 1.4305e-05,\n",
            "        1.4842e-05, 1.5020e-05, 1.4663e-05, 1.5676e-05, 1.7226e-05, 1.8120e-05,\n",
            "        1.5080e-05, 1.6749e-05, 1.7226e-05, 1.4961e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.7166e-05, 1.7166e-05, 1.5914e-05, 1.5020e-05, 1.7285e-05, 1.4305e-05,\n",
            "        1.4842e-05, 1.4901e-05, 1.4663e-05, 1.5676e-05, 1.3947e-05, 1.5378e-05,\n",
            "        1.4305e-05, 1.3530e-05, 1.4663e-05, 1.4246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7405e-05, 1.7166e-05, 1.5914e-05, 1.5020e-05, 1.4663e-05, 1.4305e-05,\n",
            "        1.4305e-05, 1.4901e-05, 1.4663e-05, 1.5676e-05, 1.4007e-05, 1.5318e-05,\n",
            "        1.4246e-05, 1.3530e-05, 1.4663e-05, 1.4246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.4663e-05, 1.3888e-05, 1.3769e-05, 1.5020e-05, 1.4663e-05, 1.4305e-05,\n",
            "        1.4305e-05, 1.4246e-05, 1.4663e-05, 1.5676e-05, 1.4007e-05, 1.4663e-05,\n",
            "        1.4246e-05, 1.3471e-05, 1.4544e-05, 1.4246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.4663e-05, 1.3888e-05, 1.3888e-05, 1.4305e-05, 1.4663e-05, 1.4246e-05,\n",
            "        1.4305e-05, 1.4246e-05, 1.4007e-05, 1.5676e-05, 1.4007e-05, 1.4663e-05,\n",
            "        1.4246e-05, 1.3471e-05, 1.4544e-05, 1.4246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.4007e-05, 1.3888e-05, 1.3828e-05, 1.4305e-05, 1.4544e-05, 1.4246e-05,\n",
            "        1.4305e-05, 1.4246e-05, 1.3888e-05, 1.4961e-05, 1.4007e-05, 1.4663e-05,\n",
            "        1.4246e-05, 1.3471e-05, 1.4544e-05, 1.4246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.4007e-05, 1.3888e-05, 1.3888e-05, 1.3530e-05, 1.4544e-05, 1.4246e-05,\n",
            "        1.4305e-05, 1.3471e-05, 1.3888e-05, 1.4961e-05, 1.3888e-05, 1.4007e-05,\n",
            "        1.4246e-05, 1.3471e-05, 1.4544e-05, 1.4246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([1.0312e-04, 1.1188e-04, 1.0234e-04, 9.9719e-05, 1.0681e-04, 1.0210e-04,\n",
            "        1.0324e-04, 1.0335e-04, 1.2100e-04, 1.0073e-04, 1.0675e-04, 1.0312e-04,\n",
            "        1.0896e-04, 1.0234e-04, 1.0401e-04, 1.0169e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([1.0049e-04, 1.0049e-04, 1.0049e-04, 9.9182e-05, 1.0049e-04, 1.0049e-04,\n",
            "        1.0049e-04, 1.0049e-04, 1.0049e-04, 9.9182e-05, 1.0163e-04, 1.0049e-04,\n",
            "        1.0014e-04, 1.0163e-04, 1.0049e-04, 9.8884e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.0049e-04, 1.0049e-04, 1.0049e-04, 1.0014e-04, 1.0049e-04, 1.0049e-04,\n",
            "        1.0049e-04, 1.0049e-04, 1.0049e-04, 9.9182e-05, 1.0163e-04, 1.0049e-04,\n",
            "        1.0014e-04, 1.0163e-04, 1.0049e-04, 1.0049e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.0049e-04, 1.0049e-04, 1.0049e-04, 1.0014e-04, 1.0049e-04, 1.0049e-04,\n",
            "        1.0049e-04, 1.0049e-04, 1.0014e-04, 9.9182e-05, 1.0163e-04, 1.0049e-04,\n",
            "        1.0014e-04, 1.0049e-04, 1.0049e-04, 1.0049e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([1.0049e-04, 1.0049e-04, 9.8586e-05, 9.8586e-05, 1.0049e-04, 1.0049e-04,\n",
            "        1.0049e-04, 1.0049e-04, 1.0049e-04, 9.8586e-05, 1.0049e-04, 1.0049e-04,\n",
            "        1.0049e-04, 1.0049e-04, 1.0049e-04, 1.0049e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([9.8586e-05, 1.0014e-04, 9.8228e-05, 9.8586e-05, 1.0049e-04, 9.8586e-05,\n",
            "        9.8586e-05, 9.8586e-05, 1.0049e-04, 9.8228e-05, 9.8586e-05, 9.8586e-05,\n",
            "        1.0049e-04, 9.8586e-05, 9.8586e-05, 9.8586e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([9.8586e-05, 9.8228e-05, 9.8228e-05, 9.8586e-05, 9.8586e-05, 9.8586e-05,\n",
            "        9.8586e-05, 9.8586e-05, 9.8586e-05, 9.8228e-05, 9.8586e-05, 9.8586e-05,\n",
            "        9.8586e-05, 9.8586e-05, 9.8586e-05, 9.8586e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([9.8586e-05, 9.8228e-05, 9.8228e-05, 9.8586e-05, 9.8586e-05, 9.8586e-05,\n",
            "        9.8228e-05, 9.8228e-05, 9.8586e-05, 9.8228e-05, 9.8586e-05, 9.8228e-05,\n",
            "        9.8586e-05, 9.8586e-05, 9.8586e-05, 9.8586e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([9.8586e-05, 9.8228e-05, 9.6798e-05, 9.8586e-05, 9.8586e-05, 9.8586e-05,\n",
            "        9.8228e-05, 9.8228e-05, 9.8586e-05, 9.8228e-05, 9.8586e-05, 9.8228e-05,\n",
            "        9.8586e-05, 9.8586e-05, 9.8586e-05, 9.8586e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([9.8586e-05, 9.8228e-05, 9.6798e-05, 9.8586e-05, 9.8586e-05, 9.8586e-05,\n",
            "        9.8228e-05, 9.8228e-05, 9.8586e-05, 9.8228e-05, 9.8586e-05, 9.8228e-05,\n",
            "        9.8586e-05, 9.8228e-05, 9.8228e-05, 9.8586e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([9.8228e-05, 9.8228e-05, 9.6798e-05, 9.8586e-05, 9.8586e-05, 9.8228e-05,\n",
            "        9.8228e-05, 9.8228e-05, 9.8586e-05, 9.8228e-05, 9.8586e-05, 9.8228e-05,\n",
            "        9.8586e-05, 9.8228e-05, 9.8228e-05, 9.8586e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0009, 0.0010, 0.0009, 0.0010, 0.0010, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0010, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0009, 0.0009, 0.0009, 0.0011, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0011, 0.0009, 0.0009, 0.0011, 0.0009, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0009,\n",
            "        0.0011, 0.0010, 0.0010, 0.0011, 0.0009, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0010, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([4.0591e-05, 4.0948e-05, 4.5955e-05, 5.2392e-05, 5.5134e-05, 5.0545e-05,\n",
            "        5.4121e-05, 5.0128e-05, 4.2856e-05, 5.1975e-05, 4.2737e-05, 4.5240e-05,\n",
            "        4.4763e-05, 4.8280e-05, 6.5565e-05, 5.4598e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.5942e-05, 4.8399e-05, 4.3333e-05, 5.5313e-05, 5.6803e-05, 5.9009e-05,\n",
            "        4.9710e-05, 5.6922e-05, 5.0247e-05, 4.0889e-05, 4.7743e-05, 4.8816e-05,\n",
            "        5.1796e-05, 3.8564e-05, 6.6698e-05, 4.6492e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.1008e-05, 4.7028e-05, 5.2392e-05, 4.8816e-05, 4.6670e-05, 5.5254e-05,\n",
            "        5.1022e-05, 5.6982e-05, 4.5002e-05, 4.7326e-05, 4.1127e-05, 4.6849e-05,\n",
            "        4.3452e-05, 4.2677e-05, 6.4313e-05, 4.2200e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.6551e-05, 4.8220e-05, 4.7922e-05, 4.2140e-05, 4.8041e-05, 5.4479e-05,\n",
            "        4.7565e-05, 5.8830e-05, 5.0008e-05, 4.6611e-05, 4.7147e-05, 4.9174e-05,\n",
            "        4.5240e-05, 4.6432e-05, 5.0128e-05, 4.5419e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([4.0293e-05, 4.1008e-05, 4.0829e-05, 4.1366e-05, 4.0710e-05, 4.0233e-05,\n",
            "        4.0710e-05, 4.0710e-05, 3.7909e-05, 4.2021e-05, 3.9339e-05, 4.3869e-05,\n",
            "        3.9518e-05, 4.0531e-05, 4.1902e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.0531e-05, 3.9518e-05, 4.1246e-05, 4.1604e-05, 4.0948e-05, 4.0352e-05,\n",
            "        4.1008e-05, 4.0889e-05, 3.8803e-05, 4.0948e-05, 3.9339e-05, 4.3094e-05,\n",
            "        3.9518e-05, 4.0531e-05, 4.2081e-05, 3.9577e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([4.0531e-05, 4.0531e-05, 4.1544e-05, 4.1962e-05, 4.1306e-05, 4.1604e-05,\n",
            "        4.1246e-05, 4.0889e-05, 3.8862e-05, 4.1306e-05, 3.9339e-05, 4.3273e-05,\n",
            "        4.0531e-05, 4.0531e-05, 4.2140e-05, 4.0174e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([4.0531e-05, 4.0531e-05, 4.0174e-05, 4.2200e-05, 4.1425e-05, 4.1783e-05,\n",
            "        4.1366e-05, 4.0889e-05, 3.8862e-05, 4.1842e-05, 4.0293e-05, 4.3273e-05,\n",
            "        4.0531e-05, 4.0531e-05, 4.2081e-05, 4.0531e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([4.0531e-05, 4.0531e-05, 4.0531e-05, 4.2200e-05, 4.1664e-05, 4.1842e-05,\n",
            "        4.1604e-05, 4.0948e-05, 3.8862e-05, 4.2260e-05, 4.0293e-05, 4.3273e-05,\n",
            "        4.0531e-05, 4.0293e-05, 4.0650e-05, 4.0531e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([4.0531e-05, 4.0650e-05, 4.0531e-05, 4.2200e-05, 4.1902e-05, 4.2140e-05,\n",
            "        3.9816e-05, 4.0829e-05, 3.8862e-05, 4.2200e-05, 4.0293e-05, 4.2200e-05,\n",
            "        4.0531e-05, 4.0293e-05, 4.0650e-05, 4.0531e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([4.0531e-05, 3.9518e-05, 4.0531e-05, 4.2200e-05, 4.2081e-05, 4.0531e-05,\n",
            "        4.0174e-05, 4.0948e-05, 3.8862e-05, 4.1842e-05, 4.0293e-05, 4.0650e-05,\n",
            "        4.0531e-05, 4.0352e-05, 4.0710e-05, 4.0531e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([4.0531e-05, 3.9458e-05, 3.8862e-05, 4.2200e-05, 4.2200e-05, 4.0531e-05,\n",
            "        4.0531e-05, 4.1127e-05, 3.8862e-05, 4.1842e-05, 4.0293e-05, 3.8862e-05,\n",
            "        4.0531e-05, 4.0412e-05, 4.0829e-05, 4.0531e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.9220e-05, 3.9518e-05, 3.7193e-05, 3.8505e-05, 4.0174e-05, 3.8505e-05,\n",
            "        3.8505e-05, 3.9041e-05, 3.7193e-05, 3.9935e-05, 4.0293e-05, 3.7193e-05,\n",
            "        4.0352e-05, 4.0412e-05, 4.0770e-05, 3.8266e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.8147e-05, 3.8087e-05, 3.7193e-05, 3.8505e-05, 3.8505e-05, 3.8505e-05,\n",
            "        3.6895e-05, 3.7491e-05, 3.7193e-05, 3.8266e-05, 3.9101e-05, 3.7193e-05,\n",
            "        3.8981e-05, 3.8922e-05, 3.6895e-05, 3.8266e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.8147e-05, 3.8147e-05, 3.7193e-05, 3.8266e-05, 3.8505e-05, 3.8505e-05,\n",
            "        3.6895e-05, 3.7909e-05, 3.7193e-05, 3.7193e-05, 3.8147e-05, 3.7193e-05,\n",
            "        3.7909e-05, 3.9101e-05, 3.5644e-05, 3.8266e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.7968e-05, 3.8147e-05, 3.7193e-05, 3.8266e-05, 3.8505e-05, 3.7193e-05,\n",
            "        3.6895e-05, 3.6776e-05, 3.7193e-05, 3.7193e-05, 3.8147e-05, 3.7193e-05,\n",
            "        3.7849e-05, 3.8028e-05, 3.4332e-05, 3.6895e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.7968e-05, 3.8147e-05, 3.7193e-05, 3.6895e-05, 3.8266e-05, 3.7193e-05,\n",
            "        3.6895e-05, 3.7074e-05, 3.7193e-05, 3.7193e-05, 3.8147e-05, 3.7193e-05,\n",
            "        3.6895e-05, 3.8028e-05, 3.4511e-05, 3.6895e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.7968e-05, 3.7968e-05, 3.7193e-05, 3.6895e-05, 3.6895e-05, 3.7193e-05,\n",
            "        3.6895e-05, 3.7074e-05, 3.7193e-05, 3.7193e-05, 3.8147e-05, 3.6895e-05,\n",
            "        3.7014e-05, 3.6836e-05, 3.4869e-05, 3.6895e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.7968e-05, 3.7968e-05, 3.7193e-05, 3.6895e-05, 3.6895e-05, 3.6895e-05,\n",
            "        3.6895e-05, 3.7074e-05, 3.6895e-05, 3.7193e-05, 3.8147e-05, 3.6895e-05,\n",
            "        3.6955e-05, 3.6836e-05, 3.5286e-05, 3.6895e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.6955e-05, 3.7968e-05, 3.6895e-05, 3.6895e-05, 3.6895e-05, 3.6895e-05,\n",
            "        3.6895e-05, 3.7074e-05, 3.6895e-05, 3.7193e-05, 3.8147e-05, 3.6895e-05,\n",
            "        3.6955e-05, 3.6836e-05, 3.5763e-05, 3.6895e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.5736e-05, 1.5557e-05, 1.9848e-05, 2.1398e-05, 2.5868e-05, 2.1100e-05,\n",
            "        2.9027e-05, 1.6689e-05, 1.6391e-05, 2.0683e-05, 1.6212e-05, 1.8477e-05,\n",
            "        1.9014e-05, 1.7047e-05, 2.6166e-05, 1.9193e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.9193e-05, 1.2040e-05, 1.7166e-05, 2.2590e-05, 2.7895e-05, 2.0623e-05,\n",
            "        2.6405e-05, 1.8358e-05, 1.8060e-05, 1.6451e-05, 1.5855e-05, 1.4663e-05,\n",
            "        2.0206e-05, 1.6034e-05, 1.5736e-05, 2.0802e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.0087e-05, 1.3649e-05, 1.5497e-05, 1.6212e-05, 2.3186e-05, 2.2709e-05,\n",
            "        1.5140e-05, 1.9610e-05, 1.5259e-05, 1.8716e-05, 1.4007e-05, 1.5616e-05,\n",
            "        1.7822e-05, 1.7643e-05, 1.7941e-05, 1.9550e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.7107e-05, 1.7107e-05, 1.5974e-05, 1.6928e-05, 1.8060e-05, 2.3603e-05,\n",
            "        1.5795e-05, 2.0623e-05, 1.6153e-05, 2.3425e-05, 1.6272e-05, 1.6689e-05,\n",
            "        1.7226e-05, 1.7405e-05, 1.9789e-05, 2.1219e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.6868e-05, 1.5676e-05, 1.6212e-05, 1.7226e-05, 1.9848e-05, 1.8418e-05,\n",
            "        1.7047e-05, 1.6332e-05, 1.7881e-05, 2.0385e-05, 1.4782e-05, 1.5318e-05,\n",
            "        1.8179e-05, 1.6332e-05, 2.0206e-05, 1.9133e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.7226e-05, 1.4722e-05, 1.6510e-05, 2.0206e-05, 2.0146e-05, 1.8656e-05,\n",
            "        1.7285e-05, 1.6034e-05, 1.8179e-05, 2.0623e-05, 1.5676e-05, 1.4484e-05,\n",
            "        1.8477e-05, 1.6451e-05, 1.9372e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.7166e-05, 1.4842e-05, 1.6212e-05, 1.7464e-05, 1.8537e-05, 1.6451e-05,\n",
            "        1.4961e-05, 1.5020e-05, 1.8299e-05, 1.8775e-05, 1.5736e-05, 1.4126e-05,\n",
            "        1.8179e-05, 1.5914e-05, 1.7226e-05, 1.8895e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.5616e-05, 1.4067e-05, 1.6212e-05, 1.5438e-05, 1.6391e-05, 1.5438e-05,\n",
            "        1.4544e-05, 1.5080e-05, 1.6630e-05, 1.6928e-05, 1.4484e-05, 1.4126e-05,\n",
            "        1.5020e-05, 1.5914e-05, 1.7166e-05, 1.6391e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.4544e-05, 1.4126e-05, 1.5080e-05, 1.5497e-05, 1.5318e-05, 1.5318e-05,\n",
            "        1.4544e-05, 1.5080e-05, 1.6689e-05, 1.4782e-05, 1.4544e-05, 1.4067e-05,\n",
            "        1.5080e-05, 1.6034e-05, 1.5974e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.4544e-05, 1.4305e-05, 1.5199e-05, 1.5438e-05, 1.5378e-05, 1.5318e-05,\n",
            "        1.4544e-05, 1.5080e-05, 1.6630e-05, 1.5497e-05, 1.4544e-05, 1.3649e-05,\n",
            "        1.5080e-05, 1.6034e-05, 1.5140e-05, 1.5378e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.4424e-05, 1.4246e-05, 1.5199e-05, 1.5378e-05, 1.5378e-05, 1.5378e-05,\n",
            "        1.4544e-05, 1.5140e-05, 1.5378e-05, 1.4544e-05, 1.4544e-05, 1.3769e-05,\n",
            "        1.5378e-05, 1.5914e-05, 1.5140e-05, 1.5378e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.4424e-05, 1.4305e-05, 1.4365e-05, 1.5438e-05, 1.5378e-05, 1.5318e-05,\n",
            "        1.4544e-05, 1.5259e-05, 1.4305e-05, 1.4544e-05, 1.3769e-05, 1.3828e-05,\n",
            "        1.5438e-05, 1.6153e-05, 1.5199e-05, 1.5438e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.4484e-05, 1.3471e-05, 1.3471e-05, 1.4603e-05, 1.5378e-05, 1.5318e-05,\n",
            "        1.4544e-05, 1.3232e-05, 1.4424e-05, 1.4603e-05, 1.2934e-05, 1.2994e-05,\n",
            "        1.5438e-05, 1.4365e-05, 1.4603e-05, 1.5438e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.4544e-05, 1.3530e-05, 1.3530e-05, 1.4007e-05, 1.5497e-05, 1.5438e-05,\n",
            "        1.4544e-05, 1.3232e-05, 1.4365e-05, 1.4544e-05, 1.2994e-05, 1.3053e-05,\n",
            "        1.3351e-05, 1.3292e-05, 1.4544e-05, 1.4842e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.3888e-05, 1.3530e-05, 1.3590e-05, 1.3947e-05, 1.5438e-05, 1.4603e-05,\n",
            "        1.4544e-05, 1.3351e-05, 1.4424e-05, 1.4603e-05, 1.2934e-05, 1.3053e-05,\n",
            "        1.3471e-05, 1.3411e-05, 1.4603e-05, 1.4901e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.3947e-05, 1.1086e-05, 1.3649e-05, 1.3888e-05, 1.4544e-05, 1.4544e-05,\n",
            "        1.4544e-05, 1.2398e-05, 1.4305e-05, 1.4722e-05, 1.3113e-05, 1.3292e-05,\n",
            "        1.3351e-05, 1.3590e-05, 1.3888e-05, 1.4901e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.1802e-05, 1.1086e-05, 1.2636e-05, 1.1921e-05, 1.4603e-05, 1.1861e-05,\n",
            "        1.4603e-05, 1.1683e-05, 1.2398e-05, 1.2040e-05, 1.3113e-05, 1.3351e-05,\n",
            "        1.2517e-05, 1.2636e-05, 1.1921e-05, 1.1981e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.1861e-05, 1.1086e-05, 1.2636e-05, 1.1921e-05, 1.2517e-05, 1.1921e-05,\n",
            "        1.2398e-05, 1.1683e-05, 1.2457e-05, 1.1861e-05, 1.2279e-05, 1.3351e-05,\n",
            "        1.2696e-05, 1.2636e-05, 1.1981e-05, 1.1921e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.1861e-05, 1.1086e-05, 1.2636e-05, 1.1861e-05, 1.2517e-05, 1.1921e-05,\n",
            "        1.1861e-05, 1.1683e-05, 1.2338e-05, 1.2040e-05, 1.2398e-05, 1.2457e-05,\n",
            "        1.1683e-05, 1.2636e-05, 1.1861e-05, 1.1981e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.1861e-05, 1.1086e-05, 1.1683e-05, 1.1921e-05, 1.2577e-05, 1.1742e-05,\n",
            "        1.1802e-05, 1.1563e-05, 1.2398e-05, 1.1921e-05, 1.2398e-05, 1.1802e-05,\n",
            "        1.1086e-05, 1.2636e-05, 1.1921e-05, 1.1802e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([8.1241e-05, 6.3539e-05, 6.1154e-05, 7.2062e-05, 7.6771e-05, 6.4611e-05,\n",
            "        6.7770e-05, 5.9068e-05, 6.3419e-05, 6.8724e-05, 5.6684e-05, 6.8605e-05,\n",
            "        8.3208e-05, 7.0095e-05, 8.1539e-05, 6.2346e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([8.9407e-05, 6.4433e-05, 5.7936e-05, 6.5506e-05, 6.5207e-05, 6.3956e-05,\n",
            "        7.7248e-05, 5.8472e-05, 6.8128e-05, 6.1214e-05, 6.1989e-05, 6.4671e-05,\n",
            "        8.6129e-05, 6.6519e-05, 7.9155e-05, 6.4790e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([7.4625e-05, 6.5446e-05, 6.1870e-05, 7.0632e-05, 5.9605e-05, 6.3837e-05,\n",
            "        7.9036e-05, 6.6340e-05, 6.6161e-05, 6.3777e-05, 6.8307e-05, 6.0141e-05,\n",
            "        8.5592e-05, 7.0095e-05, 7.8380e-05, 6.8963e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([7.3135e-05, 7.0453e-05, 6.6817e-05, 7.5519e-05, 6.7294e-05, 6.6102e-05,\n",
            "        6.8963e-05, 7.0214e-05, 6.8128e-05, 6.5267e-05, 7.1228e-05, 6.7115e-05,\n",
            "        8.2314e-05, 7.0632e-05, 7.4446e-05, 6.5804e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([6.0618e-05, 6.2466e-05, 6.0022e-05, 6.3241e-05, 6.4135e-05, 6.1810e-05,\n",
            "        6.2108e-05, 6.2346e-05, 6.2466e-05, 6.0081e-05, 6.3837e-05, 6.0260e-05,\n",
            "        6.2287e-05, 6.1572e-05, 6.7234e-05, 6.2287e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([5.8472e-05, 6.2287e-05, 6.0022e-05, 6.3241e-05, 6.4135e-05, 6.1810e-05,\n",
            "        6.0022e-05, 6.2346e-05, 6.2466e-05, 6.1333e-05, 6.2227e-05, 6.2168e-05,\n",
            "        6.0022e-05, 6.1572e-05, 5.9903e-05, 6.2287e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([5.8889e-05, 6.0022e-05, 6.0022e-05, 6.0797e-05, 6.1750e-05, 6.1750e-05,\n",
            "        6.0022e-05, 5.9724e-05, 6.0201e-05, 5.9068e-05, 6.0022e-05, 5.9843e-05,\n",
            "        6.0022e-05, 5.9843e-05, 5.9903e-05, 6.0022e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([5.8711e-05, 6.0022e-05, 6.0022e-05, 6.0797e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 5.9724e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 5.9068e-05, 6.0141e-05, 6.0022e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([5.8711e-05, 6.0022e-05, 6.0022e-05, 6.0797e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 6.0797e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 5.9068e-05, 6.0141e-05, 6.0022e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([5.8889e-05, 6.0022e-05, 6.0022e-05, 6.0797e-05, 6.0022e-05, 6.0797e-05,\n",
            "        6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 5.9068e-05, 6.0141e-05, 6.0022e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([5.8770e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0797e-05,\n",
            "        6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 6.0022e-05, 6.0141e-05, 6.0022e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([5.8949e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0797e-05,\n",
            "        6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 6.0022e-05, 6.0141e-05, 6.0022e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([5.6446e-05, 6.0022e-05, 5.8293e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 5.8293e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 6.0022e-05, 5.6446e-05, 6.0022e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([5.6446e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05,\n",
            "        5.8293e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05,\n",
            "        6.0022e-05, 5.8293e-05, 5.6446e-05, 5.8055e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([5.6446e-05, 5.8055e-05, 5.7101e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05,\n",
            "        5.8293e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05,\n",
            "        5.8293e-05, 5.8293e-05, 5.4479e-05, 5.8055e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([5.6446e-05, 5.8055e-05, 5.7101e-05, 5.8293e-05, 5.8293e-05, 5.8293e-05,\n",
            "        5.8293e-05, 5.8293e-05, 5.8293e-05, 5.8055e-05, 5.8293e-05, 5.7101e-05,\n",
            "        5.8293e-05, 5.8293e-05, 5.4479e-05, 5.8055e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([5.6446e-05, 5.8055e-05, 5.7101e-05, 5.8293e-05, 5.8293e-05, 5.7101e-05,\n",
            "        5.8293e-05, 5.7101e-05, 5.8293e-05, 5.8055e-05, 5.7101e-05, 5.7101e-05,\n",
            "        5.8293e-05, 5.8293e-05, 5.4479e-05, 5.8055e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([5.6446e-05, 5.6863e-05, 5.6863e-05, 5.8293e-05, 5.8293e-05, 5.7101e-05,\n",
            "        5.8293e-05, 5.7101e-05, 5.8293e-05, 5.8055e-05, 5.7101e-05, 5.6863e-05,\n",
            "        5.8293e-05, 5.7101e-05, 5.4479e-05, 5.8055e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([5.4479e-05, 5.6863e-05, 5.6863e-05, 5.8293e-05, 5.8293e-05, 5.7101e-05,\n",
            "        5.8293e-05, 5.6267e-05, 5.8293e-05, 5.8055e-05, 5.6863e-05, 5.6863e-05,\n",
            "        5.8293e-05, 5.7101e-05, 5.4479e-05, 5.8055e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([5.4538e-05, 5.6863e-05, 5.6863e-05, 5.8293e-05, 5.7101e-05, 5.7101e-05,\n",
            "        5.8293e-05, 5.6267e-05, 5.8293e-05, 5.6863e-05, 5.6863e-05, 5.6863e-05,\n",
            "        5.8293e-05, 5.7101e-05, 5.4479e-05, 5.6863e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([8.4519e-05, 9.7811e-05, 8.5473e-05, 6.8903e-05, 7.4029e-05, 7.2956e-05,\n",
            "        6.5804e-05, 8.3685e-05, 6.6340e-05, 7.5400e-05, 7.7546e-05, 8.9467e-05,\n",
            "        9.4891e-05, 1.0484e-04, 8.6844e-05, 6.9320e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([8.5473e-05, 8.2731e-05, 7.2658e-05, 6.5982e-05, 8.2076e-05, 7.4685e-05,\n",
            "        6.2644e-05, 8.3745e-05, 6.6876e-05, 6.9976e-05, 8.5115e-05, 6.1274e-05,\n",
            "        8.1301e-05, 1.1837e-04, 9.1493e-05, 6.1929e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([7.6234e-05, 8.0764e-05, 7.0751e-05, 7.3373e-05, 7.8559e-05, 7.9989e-05,\n",
            "        7.0333e-05, 8.4877e-05, 7.3254e-05, 7.0691e-05, 7.1108e-05, 6.9678e-05,\n",
            "        9.0599e-05, 1.0210e-04, 8.9884e-05, 6.9439e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([6.4850e-05, 6.5863e-05, 6.4790e-05, 6.4969e-05, 6.8724e-05, 6.8307e-05,\n",
            "        6.5565e-05, 6.8128e-05, 6.5863e-05, 6.5267e-05, 6.5207e-05, 6.6757e-05,\n",
            "        6.8128e-05, 6.5684e-05, 7.4565e-05, 6.5625e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([6.8069e-05, 6.5446e-05, 6.6221e-05, 6.6459e-05, 7.0095e-05, 7.1228e-05,\n",
            "        6.5446e-05, 6.8128e-05, 6.7234e-05, 6.6280e-05, 6.6280e-05, 6.5446e-05,\n",
            "        6.8128e-05, 6.6102e-05, 6.8128e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([6.8069e-05, 6.4731e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05, 6.7234e-05,\n",
            "        6.5446e-05, 6.8128e-05, 6.7234e-05, 6.6280e-05, 6.6280e-05, 6.5446e-05,\n",
            "        6.6280e-05, 6.6996e-05, 6.8128e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05, 6.7234e-05,\n",
            "        6.5446e-05, 6.6280e-05, 6.7234e-05, 6.6280e-05, 6.6280e-05, 6.5446e-05,\n",
            "        6.6280e-05, 6.4611e-05, 6.8128e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.6280e-05, 6.7234e-05, 6.6280e-05, 6.6280e-05, 6.5446e-05,\n",
            "        6.6280e-05, 6.4611e-05, 6.7234e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.6280e-05, 6.7234e-05, 6.6280e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.4611e-05, 6.7234e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.6280e-05, 6.7234e-05, 6.6280e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.4611e-05, 6.7234e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.4611e-05, 6.7234e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.4611e-05, 6.7234e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6042e-05, 6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.4611e-05, 6.7234e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05, 6.6280e-05,\n",
            "        6.6042e-05, 6.6280e-05, 6.6280e-05, 6.6280e-05, 6.6042e-05, 6.6280e-05,\n",
            "        6.6280e-05, 6.2108e-05, 6.6280e-05, 6.6280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([6.6280e-05, 6.6280e-05, 6.4731e-05, 6.4731e-05, 6.4731e-05, 6.4731e-05,\n",
            "        6.4552e-05, 6.6280e-05, 6.4731e-05, 6.4731e-05, 6.4552e-05, 6.4731e-05,\n",
            "        6.6280e-05, 6.2108e-05, 6.6280e-05, 6.4731e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([6.4731e-05, 6.4731e-05, 6.4731e-05, 6.3598e-05, 6.4731e-05, 6.4731e-05,\n",
            "        6.4552e-05, 6.4731e-05, 6.3598e-05, 6.3598e-05, 6.4552e-05, 6.4731e-05,\n",
            "        6.4731e-05, 6.2108e-05, 6.4731e-05, 6.4731e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([6.4731e-05, 6.4731e-05, 6.4731e-05, 6.3598e-05, 6.4731e-05, 6.4731e-05,\n",
            "        6.4552e-05, 6.4731e-05, 6.3598e-05, 6.3598e-05, 6.4552e-05, 6.4552e-05,\n",
            "        6.4731e-05, 6.2108e-05, 6.4731e-05, 6.4731e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([6.4731e-05, 6.4552e-05, 6.3419e-05, 6.3598e-05, 6.4731e-05, 6.4731e-05,\n",
            "        6.4552e-05, 6.4731e-05, 6.3598e-05, 6.3419e-05, 6.4552e-05, 6.4552e-05,\n",
            "        6.4731e-05, 6.2108e-05, 6.4731e-05, 6.3598e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([6.4731e-05, 6.4552e-05, 6.3419e-05, 6.3598e-05, 6.4731e-05, 6.4731e-05,\n",
            "        6.4731e-05, 6.4731e-05, 6.3598e-05, 6.3419e-05, 6.4552e-05, 6.4552e-05,\n",
            "        6.4731e-05, 6.2108e-05, 6.4731e-05, 6.3419e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([6.4552e-05, 6.3419e-05, 6.3419e-05, 6.3598e-05, 6.4731e-05, 6.4731e-05,\n",
            "        6.3598e-05, 6.4731e-05, 6.3598e-05, 6.3419e-05, 6.4552e-05, 6.4552e-05,\n",
            "        6.4552e-05, 5.9962e-05, 6.4731e-05, 6.3419e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0006, 0.0006, 0.0005, 0.0006, 0.0005, 0.0005, 0.0006, 0.0006, 0.0005,\n",
            "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0006, 0.0006, 0.0006, 0.0006, 0.0005, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006, 0.0006, 0.0005,\n",
            "        0.0005, 0.0006, 0.0005, 0.0005, 0.0006, 0.0005, 0.0006],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([5.7042e-05, 3.8743e-05, 4.8697e-05, 5.1975e-05, 4.4882e-05, 6.4194e-05,\n",
            "        4.0591e-05, 4.1604e-05, 4.7326e-05, 4.4048e-05, 4.3988e-05, 4.1902e-05,\n",
            "        4.8876e-05, 4.1187e-05, 4.2498e-05, 4.6730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.0724e-05, 4.3392e-05, 4.6313e-05, 5.7995e-05, 4.6253e-05, 7.0751e-05,\n",
            "        3.5167e-05, 4.5180e-05, 5.5194e-05, 4.3929e-05, 4.3750e-05, 4.2975e-05,\n",
            "        5.2691e-05, 4.4346e-05, 3.9935e-05, 4.3452e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([5.6863e-05, 5.1141e-05, 4.1127e-05, 4.4525e-05, 5.3465e-05, 7.3731e-05,\n",
            "        3.9876e-05, 4.8518e-05, 4.3392e-05, 4.3511e-05, 4.5300e-05, 4.4882e-05,\n",
            "        4.5300e-05, 5.0604e-05, 4.1962e-05, 5.1320e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([5.0783e-05, 4.5598e-05, 4.4048e-05, 4.7266e-05, 5.1379e-05, 4.5776e-05,\n",
            "        4.3690e-05, 4.6730e-05, 4.5240e-05, 4.5776e-05, 4.8578e-05, 4.5598e-05,\n",
            "        4.8220e-05, 4.6849e-05, 4.3511e-05, 5.4419e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([4.2558e-05, 4.0472e-05, 3.7611e-05, 3.9160e-05, 3.9756e-05, 3.8028e-05,\n",
            "        3.8266e-05, 4.0531e-05, 4.0531e-05, 3.9220e-05, 4.1246e-05, 3.8445e-05,\n",
            "        4.1127e-05, 3.8683e-05, 3.6597e-05, 4.5955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.9577e-05, 3.9220e-05, 3.7670e-05, 3.8564e-05, 3.9697e-05, 3.9339e-05,\n",
            "        3.7730e-05, 3.9756e-05, 3.8683e-05, 3.9637e-05, 3.9637e-05, 4.0531e-05,\n",
            "        4.1425e-05, 3.8743e-05, 3.8803e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.9518e-05, 3.9399e-05, 3.7611e-05, 3.9697e-05, 3.9697e-05, 3.9399e-05,\n",
            "        3.7193e-05, 4.0054e-05, 3.8803e-05, 3.9816e-05, 3.9816e-05, 4.0531e-05,\n",
            "        3.9577e-05, 3.8803e-05, 3.8743e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.9399e-05, 3.9935e-05, 3.7670e-05, 3.9816e-05, 3.8445e-05, 3.9458e-05,\n",
            "        3.8922e-05, 3.8505e-05, 3.8147e-05, 3.9816e-05, 3.9876e-05, 3.9101e-05,\n",
            "        3.9697e-05, 3.8862e-05, 3.8862e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.9399e-05, 3.9995e-05, 3.9041e-05, 3.9995e-05, 3.8624e-05, 3.9458e-05,\n",
            "        3.9101e-05, 3.8624e-05, 3.8207e-05, 3.8087e-05, 3.9995e-05, 3.9756e-05,\n",
            "        3.9637e-05, 3.8922e-05, 3.8981e-05, 3.9399e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.9458e-05, 3.9995e-05, 3.9399e-05, 3.9995e-05, 3.8683e-05, 3.9458e-05,\n",
            "        3.9101e-05, 3.8505e-05, 3.8326e-05, 3.8266e-05, 4.0114e-05, 3.9756e-05,\n",
            "        3.9756e-05, 3.9220e-05, 3.8922e-05, 3.9577e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.9339e-05, 3.9995e-05, 3.9697e-05, 3.8266e-05, 3.8981e-05, 3.9458e-05,\n",
            "        3.9101e-05, 3.8743e-05, 3.8564e-05, 3.8505e-05, 4.0233e-05, 3.9756e-05,\n",
            "        3.9816e-05, 3.9637e-05, 3.9041e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.7730e-05, 4.0174e-05, 4.0114e-05, 3.8385e-05, 3.7551e-05, 3.9399e-05,\n",
            "        3.9101e-05, 3.8683e-05, 3.8743e-05, 3.8683e-05, 4.0472e-05, 3.7968e-05,\n",
            "        3.8087e-05, 3.9876e-05, 3.7909e-05, 3.9518e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.5524e-05, 3.8147e-05, 3.6418e-05, 3.6657e-05, 3.5524e-05, 3.9399e-05,\n",
            "        3.6538e-05, 3.6895e-05, 3.6776e-05, 3.6776e-05, 3.6776e-05, 3.7968e-05,\n",
            "        3.6180e-05, 3.8385e-05, 3.6418e-05, 3.4153e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.5644e-05, 3.6657e-05, 3.6716e-05, 3.6776e-05, 3.5942e-05, 3.6895e-05,\n",
            "        3.6538e-05, 3.7074e-05, 3.6776e-05, 3.6776e-05, 3.6776e-05, 3.6538e-05,\n",
            "        3.6359e-05, 3.8445e-05, 3.5405e-05, 3.4332e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.4273e-05, 3.6776e-05, 3.6538e-05, 3.5524e-05, 3.6001e-05, 3.5107e-05,\n",
            "        3.6538e-05, 3.5524e-05, 3.6776e-05, 3.6776e-05, 3.6776e-05, 3.6538e-05,\n",
            "        3.5465e-05, 3.8207e-05, 3.5405e-05, 3.4571e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.3200e-05, 3.6538e-05, 3.5286e-05, 3.5524e-05, 3.4988e-05, 3.5226e-05,\n",
            "        3.6538e-05, 3.4213e-05, 3.6776e-05, 3.6776e-05, 3.6776e-05, 3.6538e-05,\n",
            "        3.5584e-05, 3.6538e-05, 3.5226e-05, 3.4571e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.3379e-05, 3.5286e-05, 3.5286e-05, 3.5524e-05, 3.5286e-05, 3.5286e-05,\n",
            "        3.5584e-05, 3.4213e-05, 3.6776e-05, 3.6776e-05, 3.5524e-05, 3.6538e-05,\n",
            "        3.5584e-05, 3.6538e-05, 3.5465e-05, 3.3557e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.3796e-05, 3.3915e-05, 3.5286e-05, 3.5524e-05, 3.5286e-05, 3.3915e-05,\n",
            "        3.5584e-05, 3.4213e-05, 3.6776e-05, 3.5524e-05, 3.5286e-05, 3.6538e-05,\n",
            "        3.5584e-05, 3.6538e-05, 3.5405e-05, 3.3915e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.4034e-05, 3.3915e-05, 3.5286e-05, 3.5524e-05, 3.5286e-05, 3.4094e-05,\n",
            "        3.5405e-05, 3.4213e-05, 3.6776e-05, 3.5524e-05, 3.3915e-05, 3.6538e-05,\n",
            "        3.5584e-05, 3.6538e-05, 3.5405e-05, 3.3915e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.3915e-05, 3.3915e-05, 3.5286e-05, 3.5286e-05, 3.5286e-05, 3.4273e-05,\n",
            "        3.5405e-05, 3.4213e-05, 3.5584e-05, 3.5286e-05, 3.3915e-05, 3.6538e-05,\n",
            "        3.5584e-05, 3.6538e-05, 3.5405e-05, 3.3915e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.1855e-04, 1.2231e-04, 1.2267e-04, 1.1599e-04, 1.2958e-04, 1.1730e-04,\n",
            "        9.5963e-05, 1.1700e-04, 1.1289e-04, 1.1814e-04, 1.2922e-04, 1.1593e-04,\n",
            "        1.1522e-04, 9.8705e-05, 1.2189e-04, 1.4210e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([9.5248e-05, 1.1986e-04, 1.1843e-04, 1.1718e-04, 1.3387e-04, 1.2803e-04,\n",
            "        9.7215e-05, 1.1355e-04, 1.1152e-04, 1.1468e-04, 1.2100e-04, 1.3351e-04,\n",
            "        1.2946e-04, 1.0622e-04, 1.2946e-04, 1.4615e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.0169e-04, 1.2684e-04, 1.1069e-04, 1.1599e-04, 1.1021e-04, 1.0824e-04,\n",
            "        9.2685e-05, 1.2189e-04, 1.0216e-04, 9.6321e-05, 1.1289e-04, 1.3125e-04,\n",
            "        9.7811e-05, 1.0914e-04, 1.1367e-04, 1.3876e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.0586e-04, 1.0240e-04, 1.0753e-04, 1.0854e-04, 1.0431e-04, 1.0532e-04,\n",
            "        1.0121e-04, 1.1128e-04, 9.9599e-05, 1.0264e-04, 1.1826e-04, 1.1396e-04,\n",
            "        1.0657e-04, 1.1307e-04, 1.0848e-04, 1.0675e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([9.3937e-05, 9.3758e-05, 9.1493e-05, 9.3222e-05, 9.3400e-05, 9.5844e-05,\n",
            "        9.2208e-05, 9.3222e-05, 9.2208e-05, 9.0778e-05, 9.2924e-05, 1.0234e-04,\n",
            "        9.3937e-05, 9.2626e-05, 9.3222e-05, 9.2506e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([9.1374e-05, 9.3758e-05, 9.3937e-05, 9.3222e-05, 9.4533e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.2208e-05, 9.0718e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3937e-05, 9.2626e-05, 9.3222e-05, 9.0003e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([9.3222e-05, 9.3222e-05, 9.0480e-05, 9.3222e-05, 9.4533e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.2208e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3937e-05, 9.2626e-05, 9.3222e-05, 9.0122e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([9.3222e-05, 9.3222e-05, 9.0480e-05, 9.3222e-05, 9.4533e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.2208e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.5844e-05, 9.2626e-05, 9.3222e-05, 9.0659e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([9.3222e-05, 9.3222e-05, 9.0480e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.2208e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.2626e-05, 9.3222e-05, 9.1374e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([9.3222e-05, 9.3222e-05, 9.0480e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.4533e-05, 9.3222e-05, 9.2268e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([9.3222e-05, 9.3222e-05, 9.0480e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.4533e-05, 9.3222e-05, 9.3222e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([9.3222e-05, 9.3222e-05, 9.0480e-05, 9.2924e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([9.3222e-05, 9.3222e-05, 8.6606e-05, 9.2924e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.3222e-05, 9.3222e-05, 9.3222e-05, 9.3222e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([9.0361e-05, 9.3222e-05, 8.6606e-05, 9.2924e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.0361e-05, 9.3222e-05, 9.0361e-05, 9.0361e-05, 9.3222e-05, 9.3222e-05,\n",
            "        9.0361e-05, 9.0361e-05, 9.2924e-05, 9.3222e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([9.0361e-05, 9.0003e-05, 8.6606e-05, 9.0003e-05, 9.0361e-05, 9.0003e-05,\n",
            "        9.0003e-05, 9.0361e-05, 9.0361e-05, 9.0361e-05, 9.0361e-05, 9.0361e-05,\n",
            "        9.0361e-05, 9.0361e-05, 9.0003e-05, 9.0361e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([9.0003e-05, 9.0003e-05, 8.6606e-05, 9.0003e-05, 9.0361e-05, 9.0003e-05,\n",
            "        9.0003e-05, 9.0361e-05, 9.0361e-05, 9.0361e-05, 9.0361e-05, 9.0361e-05,\n",
            "        9.0361e-05, 9.0361e-05, 9.0003e-05, 9.0361e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([9.0003e-05, 9.0003e-05, 8.6606e-05, 9.0003e-05, 9.0361e-05, 9.0003e-05,\n",
            "        8.8394e-05, 8.8692e-05, 9.0003e-05, 9.0003e-05, 9.0003e-05, 9.0361e-05,\n",
            "        9.0361e-05, 9.0361e-05, 9.0003e-05, 9.0361e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([9.0003e-05, 9.0003e-05, 8.6069e-05, 9.0003e-05, 9.0361e-05, 9.0003e-05,\n",
            "        8.8394e-05, 8.8692e-05, 9.0003e-05, 9.0003e-05, 9.0003e-05, 9.0361e-05,\n",
            "        9.0361e-05, 9.0361e-05, 9.0003e-05, 9.0361e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([9.0003e-05, 9.0003e-05, 8.2970e-05, 8.8394e-05, 9.0361e-05, 9.0003e-05,\n",
            "        8.8394e-05, 8.8394e-05, 9.0003e-05, 8.8394e-05, 9.0003e-05, 9.0361e-05,\n",
            "        9.0361e-05, 8.8692e-05, 9.0003e-05, 9.0361e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([9.0003e-05, 9.0003e-05, 8.2970e-05, 8.8394e-05, 9.0361e-05, 9.0003e-05,\n",
            "        8.8394e-05, 8.8394e-05, 9.0003e-05, 8.8394e-05, 9.0003e-05, 9.0361e-05,\n",
            "        8.8692e-05, 8.8692e-05, 9.0003e-05, 9.0361e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([6.4075e-05, 5.5671e-05, 7.6592e-05, 1.0586e-04, 8.2016e-05, 5.7101e-05,\n",
            "        7.5877e-05, 8.3923e-05, 8.9765e-05, 7.4387e-05, 7.3016e-05, 7.4565e-05,\n",
            "        6.6519e-05, 8.1539e-05, 9.1553e-05, 7.6413e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([6.8545e-05, 6.4015e-05, 6.5804e-05, 8.3923e-05, 8.2254e-05, 6.1631e-05,\n",
            "        5.7638e-05, 7.1883e-05, 8.9467e-05, 7.9513e-05, 8.6606e-05, 6.3121e-05,\n",
            "        7.2181e-05, 7.3850e-05, 5.7280e-05, 8.4460e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([6.5029e-05, 5.5432e-05, 5.6028e-05, 5.9962e-05, 6.5446e-05, 5.6684e-05,\n",
            "        5.6088e-05, 6.9797e-05, 6.3181e-05, 5.3823e-05, 6.0797e-05, 6.2346e-05,\n",
            "        6.0856e-05, 6.4790e-05, 5.5790e-05, 5.1916e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([5.7638e-05, 5.5492e-05, 5.6446e-05, 6.1154e-05, 5.7459e-05, 5.7518e-05,\n",
            "        5.5790e-05, 6.4492e-05, 5.7280e-05, 5.5432e-05, 5.8353e-05, 5.9366e-05,\n",
            "        5.6088e-05, 6.0260e-05, 5.5254e-05, 5.5432e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([5.7817e-05, 5.7280e-05, 5.6744e-05, 5.6207e-05, 5.9187e-05, 5.7518e-05,\n",
            "        5.6028e-05, 5.5671e-05, 5.9247e-05, 5.6863e-05, 5.7995e-05, 5.8055e-05,\n",
            "        5.6207e-05, 5.6803e-05, 5.6267e-05, 5.6326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([5.7817e-05, 5.7280e-05, 5.7459e-05, 5.6684e-05, 5.9783e-05, 5.7518e-05,\n",
            "        5.5194e-05, 5.5790e-05, 5.9307e-05, 5.6982e-05, 5.8293e-05, 5.8055e-05,\n",
            "        5.6446e-05, 5.7042e-05, 5.6684e-05, 5.6624e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([5.9068e-05, 5.7280e-05, 5.7518e-05, 5.6624e-05, 6.0201e-05, 5.7518e-05,\n",
            "        5.5194e-05, 5.6088e-05, 5.9724e-05, 5.7280e-05, 5.8711e-05, 5.6267e-05,\n",
            "        5.6446e-05, 5.7280e-05, 5.9307e-05, 5.7161e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([5.7518e-05, 5.7280e-05, 5.7518e-05, 5.7042e-05, 5.7638e-05, 5.7518e-05,\n",
            "        5.5194e-05, 5.6207e-05, 5.7399e-05, 5.6446e-05, 5.7518e-05, 5.6267e-05,\n",
            "        5.7518e-05, 5.7518e-05, 5.9307e-05, 5.7518e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([5.7518e-05, 5.7280e-05, 5.7518e-05, 5.7101e-05, 5.7936e-05, 5.7518e-05,\n",
            "        5.6267e-05, 5.6267e-05, 5.7340e-05, 5.6446e-05, 5.7757e-05, 5.7518e-05,\n",
            "        5.7518e-05, 5.7518e-05, 5.8472e-05, 5.7817e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([5.7518e-05, 5.7280e-05, 5.7518e-05, 5.7042e-05, 5.8293e-05, 5.7518e-05,\n",
            "        5.6267e-05, 5.6624e-05, 5.7578e-05, 5.6446e-05, 5.8115e-05, 5.7518e-05,\n",
            "        5.7518e-05, 5.7518e-05, 5.8472e-05, 5.8293e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([5.7518e-05, 5.7280e-05, 5.7518e-05, 5.4002e-05, 5.5373e-05, 5.7518e-05,\n",
            "        5.5730e-05, 5.4598e-05, 5.5015e-05, 5.7518e-05, 5.5373e-05, 5.5730e-05,\n",
            "        5.7518e-05, 5.7518e-05, 5.8472e-05, 5.5373e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([5.5730e-05, 5.5492e-05, 5.5730e-05, 5.4121e-05, 5.5373e-05, 5.5730e-05,\n",
            "        5.5730e-05, 5.5194e-05, 5.5373e-05, 5.5730e-05, 5.5373e-05, 5.5730e-05,\n",
            "        5.5730e-05, 5.5730e-05, 5.6624e-05, 5.5373e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([5.5730e-05, 5.5730e-05, 5.5730e-05, 5.4240e-05, 5.5373e-05, 5.5730e-05,\n",
            "        5.5730e-05, 5.5730e-05, 5.5373e-05, 5.5730e-05, 5.5373e-05, 5.5730e-05,\n",
            "        5.5730e-05, 5.5730e-05, 5.6624e-05, 5.5373e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([5.5730e-05, 5.5730e-05, 5.5730e-05, 5.4479e-05, 5.5373e-05, 5.4359e-05,\n",
            "        5.5730e-05, 5.5730e-05, 5.5373e-05, 5.5730e-05, 5.3227e-05, 5.5730e-05,\n",
            "        5.5730e-05, 5.5730e-05, 5.6624e-05, 5.5373e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([5.5730e-05, 5.5730e-05, 5.5730e-05, 5.4777e-05, 5.3227e-05, 5.4359e-05,\n",
            "        5.4359e-05, 5.5730e-05, 5.5373e-05, 5.5730e-05, 5.3227e-05, 5.5730e-05,\n",
            "        5.5730e-05, 5.5492e-05, 5.4359e-05, 5.5373e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([5.5730e-05, 5.3227e-05, 5.5492e-05, 5.2810e-05, 5.3227e-05, 5.4359e-05,\n",
            "        5.4359e-05, 5.5730e-05, 5.3227e-05, 5.5730e-05, 5.3227e-05, 5.4359e-05,\n",
            "        5.5730e-05, 5.5492e-05, 5.4359e-05, 5.3227e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([5.4061e-05, 5.3227e-05, 5.5492e-05, 5.3227e-05, 5.3227e-05, 5.4359e-05,\n",
            "        5.4359e-05, 5.5730e-05, 5.3227e-05, 5.5492e-05, 5.1320e-05, 5.4359e-05,\n",
            "        5.5492e-05, 5.4061e-05, 5.4359e-05, 5.3227e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([5.4061e-05, 5.3227e-05, 5.5492e-05, 5.3227e-05, 5.1320e-05, 5.4359e-05,\n",
            "        5.4359e-05, 5.4061e-05, 5.3227e-05, 5.4061e-05, 5.1320e-05, 5.4359e-05,\n",
            "        5.4061e-05, 5.4061e-05, 5.4359e-05, 5.3227e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([5.4061e-05, 5.3227e-05, 5.5492e-05, 5.3227e-05, 5.1320e-05, 5.4061e-05,\n",
            "        5.4061e-05, 5.4061e-05, 5.3227e-05, 5.4061e-05, 5.1320e-05, 5.4359e-05,\n",
            "        5.4061e-05, 5.4061e-05, 5.4359e-05, 5.3227e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([5.4061e-05, 5.3227e-05, 5.4061e-05, 5.3227e-05, 5.1320e-05, 5.4061e-05,\n",
            "        5.4061e-05, 5.4061e-05, 5.3227e-05, 5.4061e-05, 5.1320e-05, 5.4359e-05,\n",
            "        5.4061e-05, 5.4061e-05, 5.3227e-05, 5.3227e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0005, 0.0005, 0.0005, 0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006, 0.0005,\n",
            "        0.0005, 0.0006, 0.0005, 0.0006, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0005, 0.0005, 0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006, 0.0005,\n",
            "        0.0005, 0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([2.8431e-05, 2.3603e-05, 2.8431e-05, 2.4199e-05, 2.6047e-05, 2.8014e-05,\n",
            "        2.6882e-05, 3.3140e-05, 1.9670e-05, 2.1815e-05, 2.0504e-05, 2.0623e-05,\n",
            "        1.7345e-05, 1.6391e-05, 2.0504e-05, 3.0756e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.8074e-05, 2.3484e-05, 2.4676e-05, 2.3603e-05, 2.6107e-05, 2.3484e-05,\n",
            "        2.3246e-05, 3.2127e-05, 1.9670e-05, 2.5690e-05, 1.8954e-05, 2.2888e-05,\n",
            "        2.0385e-05, 1.7226e-05, 2.0623e-05, 2.6226e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.4213e-05, 2.5451e-05, 2.3246e-05, 2.7955e-05, 2.6584e-05, 2.3901e-05,\n",
            "        2.3901e-05, 3.2127e-05, 2.2292e-05, 2.0206e-05, 1.9133e-05, 2.4974e-05,\n",
            "        2.4259e-05, 1.9789e-05, 1.9431e-05, 2.3782e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.2471e-05, 2.2888e-05, 2.2829e-05, 2.7001e-05, 2.7418e-05, 2.2709e-05,\n",
            "        2.4974e-05, 2.5332e-05, 2.2471e-05, 2.2471e-05, 2.3544e-05, 2.2769e-05,\n",
            "        2.2471e-05, 1.9014e-05, 2.0504e-05, 2.1100e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.2233e-05, 2.1935e-05, 2.2590e-05, 2.2709e-05, 2.0385e-05, 2.1815e-05,\n",
            "        2.1577e-05, 2.3663e-05, 1.9670e-05, 2.2054e-05, 1.8895e-05, 1.9550e-05,\n",
            "        2.0802e-05, 1.8775e-05, 1.7881e-05, 2.0504e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.2411e-05, 2.2173e-05, 2.2829e-05, 2.2948e-05, 2.0504e-05, 2.2054e-05,\n",
            "        2.1756e-05, 2.2054e-05, 2.0027e-05, 2.2471e-05, 1.9252e-05, 1.9729e-05,\n",
            "        1.9848e-05, 1.9133e-05, 1.8179e-05, 1.9193e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.0921e-05, 2.0683e-05, 2.1279e-05, 2.1398e-05, 1.9193e-05, 2.0683e-05,\n",
            "        2.0981e-05, 2.1935e-05, 2.0027e-05, 2.1040e-05, 1.9252e-05, 1.9968e-05,\n",
            "        2.0087e-05, 1.9431e-05, 1.8358e-05, 1.9252e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.1040e-05, 2.0802e-05, 2.0981e-05, 2.1398e-05, 1.9193e-05, 2.0683e-05,\n",
            "        2.0981e-05, 2.0564e-05, 2.0087e-05, 2.0921e-05, 1.9372e-05, 2.0087e-05,\n",
            "        2.0146e-05, 1.9550e-05, 1.8358e-05, 1.9312e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.0981e-05, 2.0802e-05, 2.0981e-05, 2.1040e-05, 2.0087e-05, 1.9968e-05,\n",
            "        2.1040e-05, 2.0564e-05, 2.0027e-05, 2.1100e-05, 1.9372e-05, 2.0146e-05,\n",
            "        2.0206e-05, 1.9848e-05, 1.8358e-05, 1.9431e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.1100e-05, 1.8835e-05, 2.0921e-05, 2.0981e-05, 2.0146e-05, 2.0981e-05,\n",
            "        2.1160e-05, 1.9372e-05, 2.0027e-05, 2.1040e-05, 1.9372e-05, 1.9312e-05,\n",
            "        2.0325e-05, 2.0146e-05, 1.8358e-05, 1.9431e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.1040e-05, 1.8954e-05, 2.1100e-05, 2.1100e-05, 2.0206e-05, 2.0981e-05,\n",
            "        2.0266e-05, 1.9431e-05, 2.0027e-05, 2.1100e-05, 1.8418e-05, 1.9431e-05,\n",
            "        2.0206e-05, 2.0206e-05, 1.8358e-05, 1.9550e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.1160e-05, 1.9073e-05, 2.1040e-05, 2.0981e-05, 2.0206e-05, 2.0981e-05,\n",
            "        2.0325e-05, 1.8477e-05, 1.8358e-05, 2.1040e-05, 1.8418e-05, 1.9312e-05,\n",
            "        1.9372e-05, 1.9372e-05, 1.8358e-05, 1.9550e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.1160e-05, 1.9252e-05, 2.1160e-05, 2.0266e-05, 2.0266e-05, 2.1160e-05,\n",
            "        2.0444e-05, 1.8477e-05, 1.7345e-05, 2.0325e-05, 1.7464e-05, 1.8418e-05,\n",
            "        1.8358e-05, 1.7464e-05, 1.7345e-05, 1.9729e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.1279e-05, 1.9372e-05, 2.1160e-05, 2.0325e-05, 2.0325e-05, 2.0385e-05,\n",
            "        1.9550e-05, 1.8716e-05, 1.7345e-05, 1.9431e-05, 1.7464e-05, 1.8418e-05,\n",
            "        1.8358e-05, 1.7464e-05, 1.7345e-05, 1.9848e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.0385e-05, 1.9491e-05, 2.0385e-05, 2.0385e-05, 1.7107e-05, 2.0504e-05,\n",
            "        1.9610e-05, 1.8775e-05, 1.7464e-05, 1.9431e-05, 1.7464e-05, 1.8239e-05,\n",
            "        1.7464e-05, 1.7345e-05, 1.7345e-05, 1.9908e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7166e-05, 1.9610e-05, 1.7047e-05, 1.7226e-05, 1.7166e-05, 1.6332e-05,\n",
            "        1.6510e-05, 1.8895e-05, 1.7464e-05, 1.6332e-05, 1.7464e-05, 1.8239e-05,\n",
            "        1.7464e-05, 1.7345e-05, 1.7464e-05, 1.9968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.7166e-05, 1.6510e-05, 1.6510e-05, 1.6510e-05, 1.7166e-05, 1.6510e-05,\n",
            "        1.6332e-05, 1.8835e-05, 1.7464e-05, 1.6332e-05, 1.7464e-05, 1.8239e-05,\n",
            "        1.7345e-05, 1.7345e-05, 1.7464e-05, 2.0027e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.7166e-05, 1.6510e-05, 1.6332e-05, 1.6510e-05, 1.7166e-05, 1.6332e-05,\n",
            "        1.6332e-05, 1.6272e-05, 1.7464e-05, 1.6332e-05, 1.7464e-05, 1.7345e-05,\n",
            "        1.7345e-05, 1.7345e-05, 1.7464e-05, 1.6332e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.6510e-05, 1.6510e-05, 1.6332e-05, 1.6510e-05, 1.6510e-05, 1.6332e-05,\n",
            "        1.6332e-05, 1.6332e-05, 1.7464e-05, 1.6332e-05, 1.7464e-05, 1.6809e-05,\n",
            "        1.7345e-05, 1.7345e-05, 1.7464e-05, 1.6391e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.6510e-05, 1.6332e-05, 1.6332e-05, 1.6332e-05, 1.6510e-05, 1.6332e-05,\n",
            "        1.6332e-05, 1.6332e-05, 1.7464e-05, 1.6332e-05, 1.7464e-05, 1.6809e-05,\n",
            "        1.7345e-05, 1.7345e-05, 1.6510e-05, 1.6332e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([6.5029e-05, 6.5446e-05, 6.1810e-05, 4.3869e-05, 5.8949e-05, 5.7161e-05,\n",
            "        6.4552e-05, 6.7234e-05, 5.0008e-05, 5.8174e-05, 6.6161e-05, 6.7115e-05,\n",
            "        7.0810e-05, 5.8830e-05, 6.5684e-05, 5.6744e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([6.2764e-05, 5.3406e-05, 5.0426e-05, 4.5896e-05, 5.1558e-05, 5.3167e-05,\n",
            "        6.0022e-05, 6.3360e-05, 5.3644e-05, 5.9426e-05, 6.8963e-05, 5.6565e-05,\n",
            "        5.9009e-05, 5.2631e-05, 6.1572e-05, 5.6863e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([6.0260e-05, 5.3823e-05, 4.8637e-05, 5.0187e-05, 5.6982e-05, 5.2989e-05,\n",
            "        6.0916e-05, 5.7518e-05, 5.3823e-05, 5.9664e-05, 5.2452e-05, 5.4181e-05,\n",
            "        5.4598e-05, 5.4896e-05, 6.1810e-05, 5.9128e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.5896e-05, 5.0068e-05, 4.6372e-05, 4.8041e-05, 4.6492e-05, 4.7147e-05,\n",
            "        4.7088e-05, 5.1260e-05, 5.1022e-05, 4.6670e-05, 4.6372e-05, 5.0008e-05,\n",
            "        5.0485e-05, 5.1498e-05, 4.8637e-05, 4.8280e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([4.6074e-05, 4.6790e-05, 4.7147e-05, 4.7147e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.7088e-05, 4.5955e-05, 4.7266e-05, 4.6849e-05, 4.6730e-05, 5.0724e-05,\n",
            "        5.0843e-05, 4.7982e-05, 4.9949e-05, 4.8518e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.6253e-05, 4.6670e-05, 4.7147e-05, 4.7147e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.7863e-05, 4.6015e-05, 4.8995e-05, 4.6849e-05, 4.6730e-05, 4.7088e-05,\n",
            "        4.6074e-05, 4.7982e-05, 4.7326e-05, 4.7326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([4.6492e-05, 4.6670e-05, 4.6134e-05, 4.7147e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.6730e-05, 4.5955e-05, 4.7922e-05, 4.6849e-05, 4.6730e-05, 4.7326e-05,\n",
            "        4.6074e-05, 4.6849e-05, 4.7326e-05, 4.7326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([4.6849e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.6730e-05, 4.5836e-05, 4.7326e-05, 4.6849e-05, 4.6730e-05, 4.7326e-05,\n",
            "        4.6134e-05, 4.6849e-05, 4.7326e-05, 4.7326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.6730e-05, 4.5836e-05, 4.7326e-05, 4.6849e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.6253e-05, 4.6849e-05, 4.7326e-05, 4.7326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.7326e-05, 4.5836e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.6313e-05, 4.6849e-05, 4.7326e-05, 4.7326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.7326e-05, 4.5776e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.6432e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.7326e-05, 4.5955e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.6790e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([4.7326e-05, 4.7326e-05, 4.8935e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.7326e-05, 4.4346e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.7088e-05, 4.7326e-05, 4.7326e-05, 4.7326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([4.7326e-05, 4.7326e-05, 4.6372e-05, 4.6372e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.7326e-05, 4.4465e-05, 4.6372e-05, 4.7147e-05, 4.7326e-05, 4.7326e-05,\n",
            "        4.6372e-05, 4.7326e-05, 4.7147e-05, 4.7326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([4.6372e-05, 4.6372e-05, 4.6372e-05, 4.6372e-05, 4.6372e-05, 4.6194e-05,\n",
            "        4.6372e-05, 4.4525e-05, 4.6372e-05, 4.6194e-05, 4.6372e-05, 4.6372e-05,\n",
            "        4.6372e-05, 4.6372e-05, 4.6194e-05, 4.6372e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([4.6372e-05, 4.6372e-05, 4.6372e-05, 4.6372e-05, 4.6372e-05, 4.6194e-05,\n",
            "        4.6372e-05, 4.4823e-05, 4.6372e-05, 4.6194e-05, 4.6372e-05, 4.6372e-05,\n",
            "        4.6372e-05, 4.6372e-05, 4.6194e-05, 4.6372e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([4.6372e-05, 4.6372e-05, 4.6372e-05, 4.5657e-05, 4.6372e-05, 4.6194e-05,\n",
            "        4.6372e-05, 4.5121e-05, 4.6372e-05, 4.6194e-05, 4.6194e-05, 4.6372e-05,\n",
            "        4.6372e-05, 4.6372e-05, 4.6194e-05, 4.6372e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([4.6372e-05, 4.6372e-05, 4.6372e-05, 4.5657e-05, 4.6194e-05, 4.6194e-05,\n",
            "        4.6372e-05, 4.5538e-05, 4.6372e-05, 4.6194e-05, 4.6194e-05, 4.6372e-05,\n",
            "        4.6372e-05, 4.6372e-05, 4.6194e-05, 4.6372e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([4.6372e-05, 4.6194e-05, 4.5657e-05, 4.5657e-05, 4.6194e-05, 4.6194e-05,\n",
            "        4.6372e-05, 4.6074e-05, 4.6372e-05, 4.6194e-05, 4.6194e-05, 4.6372e-05,\n",
            "        4.6372e-05, 4.6372e-05, 4.6194e-05, 4.6372e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([4.6372e-05, 4.5478e-05, 4.5478e-05, 4.5657e-05, 4.6194e-05, 4.6194e-05,\n",
            "        4.6372e-05, 4.6194e-05, 4.5657e-05, 4.6194e-05, 4.6194e-05, 4.6372e-05,\n",
            "        4.6372e-05, 4.6372e-05, 4.6194e-05, 4.6194e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.6524e-05, 2.5153e-05, 3.6240e-05, 2.9445e-05, 1.9193e-05, 2.2173e-05,\n",
            "        2.6882e-05, 2.2531e-05, 3.3498e-05, 2.1100e-05, 2.2292e-05, 2.6286e-05,\n",
            "        2.2769e-05, 3.6299e-05, 2.1517e-05, 2.7239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.2292e-05, 2.3782e-05, 2.9027e-05, 3.2365e-05, 2.0027e-05, 2.3842e-05,\n",
            "        2.5153e-05, 2.5392e-05, 3.5465e-05, 2.3544e-05, 1.8358e-05, 2.0444e-05,\n",
            "        2.3425e-05, 3.3379e-05, 2.6584e-05, 3.4034e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.6345e-05, 2.8253e-05, 3.4571e-05, 3.3557e-05, 2.1696e-05, 2.8074e-05,\n",
            "        2.6941e-05, 2.3425e-05, 2.8491e-05, 2.2650e-05, 2.2829e-05, 2.4498e-05,\n",
            "        2.7180e-05, 3.0160e-05, 2.3305e-05, 2.7537e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.4855e-05, 2.3484e-05, 2.7776e-05, 2.5451e-05, 2.2948e-05, 2.5094e-05,\n",
            "        2.2531e-05, 2.4319e-05, 2.4676e-05, 2.3842e-05, 2.4855e-05, 2.3305e-05,\n",
            "        2.2829e-05, 2.9802e-05, 2.6703e-05, 2.5332e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.5511e-05, 2.1577e-05, 2.7537e-05, 2.7657e-05, 2.1338e-05, 2.2531e-05,\n",
            "        2.5749e-05, 2.2173e-05, 2.5988e-05, 2.1100e-05, 2.5451e-05, 2.4796e-05,\n",
            "        2.3246e-05, 2.5630e-05, 2.2829e-05, 2.5809e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.3663e-05, 2.1815e-05, 2.5511e-05, 2.3007e-05, 2.3186e-05, 2.2590e-05,\n",
            "        2.6166e-05, 2.3723e-05, 2.5451e-05, 2.1398e-05, 2.6047e-05, 2.5868e-05,\n",
            "        2.3603e-05, 2.3842e-05, 2.3186e-05, 2.6226e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.3603e-05, 2.0623e-05, 2.2948e-05, 2.3007e-05, 2.1875e-05, 2.1875e-05,\n",
            "        2.1875e-05, 2.3723e-05, 2.5570e-05, 2.1398e-05, 2.4021e-05, 2.3127e-05,\n",
            "        2.1815e-05, 2.3723e-05, 2.0623e-05, 2.4080e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.2173e-05, 2.0683e-05, 2.2829e-05, 2.3246e-05, 2.0802e-05, 2.1756e-05,\n",
            "        2.0921e-05, 2.2769e-05, 2.4557e-05, 2.1398e-05, 2.2054e-05, 2.3186e-05,\n",
            "        2.1875e-05, 2.3782e-05, 1.9789e-05, 2.4140e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.2292e-05, 2.1815e-05, 2.2888e-05, 2.3246e-05, 1.9968e-05, 2.1756e-05,\n",
            "        2.0921e-05, 2.1636e-05, 2.4498e-05, 1.9848e-05, 2.1935e-05, 2.3127e-05,\n",
            "        2.0921e-05, 2.2948e-05, 2.1040e-05, 2.3127e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.0981e-05, 2.1040e-05, 2.2888e-05, 2.2471e-05, 2.0027e-05, 2.1875e-05,\n",
            "        2.2054e-05, 2.0504e-05, 2.4617e-05, 1.9848e-05, 2.2054e-05, 2.3127e-05,\n",
            "        2.2054e-05, 2.2948e-05, 2.1040e-05, 2.3186e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.1040e-05, 2.1160e-05, 2.2113e-05, 2.2650e-05, 1.9968e-05, 2.1994e-05,\n",
            "        2.1994e-05, 2.0564e-05, 2.4617e-05, 1.9968e-05, 2.1100e-05, 2.2054e-05,\n",
            "        2.2054e-05, 2.2948e-05, 2.1160e-05, 2.2113e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.1160e-05, 2.1219e-05, 2.2113e-05, 2.2769e-05, 1.8775e-05, 2.1935e-05,\n",
            "        2.2054e-05, 1.8656e-05, 2.1577e-05, 1.8716e-05, 2.1100e-05, 2.1160e-05,\n",
            "        2.1994e-05, 2.2948e-05, 1.8597e-05, 2.2233e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.1338e-05, 1.9908e-05, 2.2113e-05, 2.2948e-05, 1.8895e-05, 1.8597e-05,\n",
            "        2.2113e-05, 1.8775e-05, 2.0444e-05, 1.8716e-05, 2.1219e-05, 2.1100e-05,\n",
            "        2.2054e-05, 2.2948e-05, 1.8537e-05, 2.2113e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.1636e-05, 1.9908e-05, 2.2173e-05, 2.2888e-05, 1.8835e-05, 1.8597e-05,\n",
            "        2.2173e-05, 1.8775e-05, 2.0564e-05, 1.8835e-05, 2.1040e-05, 2.1160e-05,\n",
            "        2.2054e-05, 2.1636e-05, 1.8656e-05, 2.2233e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.1040e-05, 1.8895e-05, 2.2233e-05, 2.2888e-05, 1.8895e-05, 1.8597e-05,\n",
            "        1.7405e-05, 1.8835e-05, 2.0623e-05, 1.8597e-05, 1.7524e-05, 1.7464e-05,\n",
            "        1.7405e-05, 2.1756e-05, 1.8716e-05, 2.2054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7405e-05, 1.8775e-05, 2.1100e-05, 1.8299e-05, 1.8775e-05, 1.8716e-05,\n",
            "        1.7464e-05, 1.8835e-05, 2.0683e-05, 1.8775e-05, 1.7524e-05, 1.7524e-05,\n",
            "        1.7464e-05, 2.1756e-05, 1.8895e-05, 1.8299e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.7345e-05, 1.8775e-05, 1.7881e-05, 1.8179e-05, 1.8775e-05, 1.8775e-05,\n",
            "        1.7524e-05, 1.8895e-05, 1.7405e-05, 1.8716e-05, 1.7643e-05, 1.7643e-05,\n",
            "        1.7405e-05, 1.8299e-05, 1.8775e-05, 1.8418e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.7464e-05, 1.7762e-05, 1.7881e-05, 1.8179e-05, 1.8597e-05, 1.8716e-05,\n",
            "        1.7524e-05, 1.8775e-05, 1.7464e-05, 1.8716e-05, 1.7524e-05, 1.7583e-05,\n",
            "        1.7345e-05, 1.7166e-05, 1.7762e-05, 1.8418e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.7464e-05, 1.6689e-05, 1.8001e-05, 1.8239e-05, 1.7464e-05, 1.8775e-05,\n",
            "        1.7464e-05, 1.8775e-05, 1.7524e-05, 1.8656e-05, 1.7703e-05, 1.7703e-05,\n",
            "        1.7345e-05, 1.7226e-05, 1.7762e-05, 1.8477e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.7524e-05, 1.6570e-05, 1.7941e-05, 1.8299e-05, 1.7464e-05, 1.7703e-05,\n",
            "        1.7524e-05, 1.7583e-05, 1.7524e-05, 1.7464e-05, 1.7524e-05, 1.7762e-05,\n",
            "        1.7524e-05, 1.7345e-05, 1.6689e-05, 1.8418e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.0518e-05, 2.7120e-05, 3.4869e-05, 2.5153e-05, 2.7359e-05, 2.5570e-05,\n",
            "        2.5868e-05, 2.3425e-05, 3.1471e-05, 2.5332e-05, 2.7239e-05, 2.6226e-05,\n",
            "        3.2246e-05, 4.1068e-05, 2.2888e-05, 1.9908e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.4630e-05, 2.5988e-05, 3.1888e-05, 2.6584e-05, 2.4319e-05, 2.7001e-05,\n",
            "        2.9087e-05, 2.4080e-05, 3.8147e-05, 3.0279e-05, 2.4617e-05, 2.5868e-05,\n",
            "        2.2590e-05, 3.7074e-05, 2.4498e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.0650e-05, 2.6762e-05, 3.5703e-05, 2.2411e-05, 3.0100e-05, 2.9862e-05,\n",
            "        2.8849e-05, 2.5570e-05, 2.3365e-05, 2.5153e-05, 2.5809e-05, 2.7776e-05,\n",
            "        2.4557e-05, 3.0637e-05, 3.0637e-05, 2.1279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.2948e-05, 2.2352e-05, 2.3961e-05, 2.1577e-05, 2.3127e-05, 2.7239e-05,\n",
            "        2.2054e-05, 2.1875e-05, 2.1577e-05, 2.1160e-05, 2.0862e-05, 2.3484e-05,\n",
            "        2.1219e-05, 2.8789e-05, 2.7061e-05, 1.9312e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.3305e-05, 2.3067e-05, 2.1756e-05, 2.3305e-05, 2.2054e-05, 2.1756e-05,\n",
            "        2.1815e-05, 2.2292e-05, 2.1935e-05, 2.2531e-05, 2.1040e-05, 2.1756e-05,\n",
            "        2.0742e-05, 2.1636e-05, 2.2411e-05, 2.1279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.3603e-05, 2.3425e-05, 2.1994e-05, 2.2888e-05, 2.2292e-05, 2.2054e-05,\n",
            "        2.2113e-05, 2.2769e-05, 2.2233e-05, 2.2888e-05, 2.1338e-05, 2.2113e-05,\n",
            "        2.0862e-05, 2.1756e-05, 2.2590e-05, 2.1398e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.3842e-05, 2.3603e-05, 2.1994e-05, 2.2888e-05, 2.2531e-05, 2.2233e-05,\n",
            "        2.2233e-05, 2.2888e-05, 2.1279e-05, 2.3186e-05, 2.1517e-05, 2.2173e-05,\n",
            "        2.2054e-05, 2.1994e-05, 2.0742e-05, 2.1338e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.2292e-05, 2.2352e-05, 2.2113e-05, 2.3007e-05, 2.1577e-05, 2.2292e-05,\n",
            "        2.2292e-05, 2.1398e-05, 2.1458e-05, 2.3305e-05, 2.1577e-05, 2.2352e-05,\n",
            "        2.2233e-05, 2.2113e-05, 2.1040e-05, 2.1636e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.2531e-05, 2.2590e-05, 2.2173e-05, 2.3365e-05, 2.1517e-05, 2.2411e-05,\n",
            "        2.2411e-05, 2.1577e-05, 2.1517e-05, 2.2769e-05, 2.1756e-05, 2.2411e-05,\n",
            "        2.2411e-05, 2.2113e-05, 2.1160e-05, 2.1756e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.2769e-05, 2.2709e-05, 2.2352e-05, 2.3365e-05, 2.1577e-05, 2.1636e-05,\n",
            "        2.2590e-05, 2.1696e-05, 2.1696e-05, 2.2769e-05, 2.2769e-05, 2.2590e-05,\n",
            "        2.1696e-05, 2.2292e-05, 2.0504e-05, 2.1100e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.2769e-05, 2.2769e-05, 2.2411e-05, 2.2769e-05, 2.0444e-05, 2.1815e-05,\n",
            "        2.2709e-05, 2.1756e-05, 2.1756e-05, 2.2769e-05, 2.2769e-05, 2.1636e-05,\n",
            "        2.0862e-05, 2.2292e-05, 2.0623e-05, 2.1398e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.2769e-05, 2.1636e-05, 2.2590e-05, 2.1636e-05, 2.0444e-05, 2.0564e-05,\n",
            "        2.2769e-05, 2.0564e-05, 2.1756e-05, 2.1636e-05, 2.0683e-05, 2.0385e-05,\n",
            "        2.0862e-05, 2.2411e-05, 1.9729e-05, 1.9729e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.0623e-05, 2.1636e-05, 2.1517e-05, 2.0564e-05, 1.9610e-05, 2.0564e-05,\n",
            "        2.1636e-05, 2.0564e-05, 2.0564e-05, 2.0683e-05, 2.0683e-05, 2.0385e-05,\n",
            "        1.9789e-05, 2.0444e-05, 1.9848e-05, 1.9789e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.0623e-05, 1.9789e-05, 2.1636e-05, 2.0564e-05, 1.9550e-05, 2.0564e-05,\n",
            "        2.0564e-05, 1.9789e-05, 2.0564e-05, 2.0683e-05, 2.0683e-05, 2.0385e-05,\n",
            "        1.9014e-05, 2.0564e-05, 1.9014e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.9491e-05, 1.9789e-05, 2.1636e-05, 1.9789e-05, 1.9610e-05, 1.9848e-05,\n",
            "        2.0564e-05, 1.9789e-05, 2.0564e-05, 2.0683e-05, 1.9789e-05, 2.0385e-05,\n",
            "        1.9014e-05, 2.0564e-05, 1.9014e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.9491e-05, 1.9789e-05, 1.9789e-05, 1.9789e-05, 1.9729e-05, 1.9848e-05,\n",
            "        2.0385e-05, 1.9789e-05, 1.9789e-05, 2.0683e-05, 1.9789e-05, 2.0564e-05,\n",
            "        1.9014e-05, 2.0564e-05, 1.9014e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.9491e-05, 1.9789e-05, 1.9610e-05, 1.9789e-05, 1.9789e-05, 1.9848e-05,\n",
            "        1.9610e-05, 1.9610e-05, 1.9789e-05, 2.0683e-05, 1.9610e-05, 2.0564e-05,\n",
            "        1.8895e-05, 2.0564e-05, 1.9014e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.8239e-05, 1.9789e-05, 1.9610e-05, 1.9789e-05, 1.8716e-05, 1.9848e-05,\n",
            "        1.9610e-05, 1.9610e-05, 1.9789e-05, 1.9789e-05, 1.8895e-05, 1.9789e-05,\n",
            "        1.8895e-05, 2.0564e-05, 1.9014e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.8239e-05, 1.9789e-05, 1.8895e-05, 1.9789e-05, 1.8716e-05, 1.9848e-05,\n",
            "        1.9610e-05, 1.9610e-05, 1.9789e-05, 1.9789e-05, 1.8895e-05, 1.8716e-05,\n",
            "        1.8895e-05, 2.0564e-05, 1.9014e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.8299e-05, 1.9789e-05, 1.8895e-05, 1.9610e-05, 1.8716e-05, 1.9848e-05,\n",
            "        1.9610e-05, 1.9610e-05, 1.9789e-05, 1.9789e-05, 1.8895e-05, 1.8716e-05,\n",
            "        1.8895e-05, 2.0385e-05, 1.8895e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.4928e-05, 4.4167e-05, 5.7220e-05, 4.1783e-05, 4.2737e-05, 3.6180e-05,\n",
            "        3.0041e-05, 4.9829e-05, 5.2273e-05, 4.3929e-05, 5.5134e-05, 5.3287e-05,\n",
            "        3.9518e-05, 4.1187e-05, 5.2094e-05, 5.0485e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.3677e-05, 4.3809e-05, 5.9545e-05, 3.8922e-05, 4.2498e-05, 3.6776e-05,\n",
            "        3.2663e-05, 4.2379e-05, 4.2260e-05, 3.7014e-05, 5.1320e-05, 4.4942e-05,\n",
            "        3.7968e-05, 4.1485e-05, 4.1008e-05, 4.0591e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.4153e-05, 3.8326e-05, 5.5373e-05, 3.4750e-05, 3.9577e-05, 4.2498e-05,\n",
            "        3.6538e-05, 4.2915e-05, 3.9279e-05, 3.6299e-05, 4.6909e-05, 4.2677e-05,\n",
            "        3.3975e-05, 4.3035e-05, 3.4869e-05, 3.8207e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.3319e-05, 3.1650e-05, 3.4571e-05, 3.1412e-05, 3.2187e-05, 3.3200e-05,\n",
            "        3.1650e-05, 3.3319e-05, 3.5226e-05, 3.4451e-05, 3.4809e-05, 3.8922e-05,\n",
            "        3.3021e-05, 3.3379e-05, 3.2425e-05, 3.4094e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.3498e-05, 3.1650e-05, 3.2961e-05, 3.4928e-05, 3.3319e-05, 3.3259e-05,\n",
            "        3.2604e-05, 3.4153e-05, 3.5644e-05, 3.5107e-05, 3.2067e-05, 3.3677e-05,\n",
            "        3.4332e-05, 3.3438e-05, 3.2961e-05, 3.5584e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.3498e-05, 3.2723e-05, 3.4750e-05, 3.5048e-05, 3.3498e-05, 3.3498e-05,\n",
            "        3.3319e-05, 3.4750e-05, 3.3081e-05, 3.3736e-05, 3.2127e-05, 3.4034e-05,\n",
            "        3.3796e-05, 3.3259e-05, 3.1769e-05, 3.5763e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.3498e-05, 3.2961e-05, 3.5048e-05, 3.4094e-05, 3.3498e-05, 3.3498e-05,\n",
            "        3.3319e-05, 3.5107e-05, 3.4392e-05, 3.3915e-05, 3.2604e-05, 3.4451e-05,\n",
            "        3.4094e-05, 3.3259e-05, 3.4094e-05, 3.4392e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.3498e-05, 3.3081e-05, 3.3617e-05, 3.4273e-05, 3.3498e-05, 3.3498e-05,\n",
            "        3.3498e-05, 3.4392e-05, 3.4392e-05, 3.4153e-05, 3.2663e-05, 3.4392e-05,\n",
            "        3.4094e-05, 3.3259e-05, 3.4094e-05, 3.4392e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.3498e-05, 3.3081e-05, 3.3975e-05, 3.4392e-05, 3.3498e-05, 3.3498e-05,\n",
            "        3.3319e-05, 3.4392e-05, 3.4392e-05, 3.4213e-05, 3.3021e-05, 3.4392e-05,\n",
            "        3.3498e-05, 3.3259e-05, 3.4273e-05, 3.4392e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.3498e-05, 3.3081e-05, 3.4332e-05, 3.3021e-05, 3.3498e-05, 3.3498e-05,\n",
            "        3.3319e-05, 3.4392e-05, 3.4392e-05, 3.4392e-05, 3.3021e-05, 3.4392e-05,\n",
            "        3.3498e-05, 3.3259e-05, 3.4392e-05, 3.4392e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.3498e-05, 3.3259e-05, 3.4392e-05, 3.1412e-05, 3.3498e-05, 3.3498e-05,\n",
            "        3.3319e-05, 3.4392e-05, 3.4392e-05, 3.3021e-05, 3.3021e-05, 3.4392e-05,\n",
            "        3.3498e-05, 3.3259e-05, 3.4392e-05, 3.4392e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.2306e-05, 3.0398e-05, 3.2663e-05, 3.1412e-05, 3.3498e-05, 3.2485e-05,\n",
            "        3.1471e-05, 3.2663e-05, 3.2663e-05, 3.1412e-05, 3.1412e-05, 3.2663e-05,\n",
            "        3.2485e-05, 3.2663e-05, 3.1412e-05, 3.2663e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.2306e-05, 3.0398e-05, 3.2663e-05, 3.1412e-05, 3.2485e-05, 3.2485e-05,\n",
            "        3.1471e-05, 3.2663e-05, 3.2663e-05, 3.1412e-05, 3.1412e-05, 3.2663e-05,\n",
            "        3.1650e-05, 3.2663e-05, 3.1412e-05, 3.2663e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.2306e-05, 3.0398e-05, 3.1412e-05, 3.1412e-05, 3.2485e-05, 3.2485e-05,\n",
            "        3.1471e-05, 3.2663e-05, 3.2663e-05, 3.1412e-05, 3.1412e-05, 3.2663e-05,\n",
            "        3.1650e-05, 3.1233e-05, 3.1412e-05, 3.2663e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.1471e-05, 3.0398e-05, 3.1412e-05, 3.1412e-05, 3.2485e-05, 3.2306e-05,\n",
            "        3.1471e-05, 3.2663e-05, 3.2663e-05, 3.1412e-05, 3.1412e-05, 3.2663e-05,\n",
            "        3.1650e-05, 3.0220e-05, 3.1412e-05, 3.2663e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.1471e-05, 3.0398e-05, 3.1412e-05, 3.1412e-05, 3.2485e-05, 3.1650e-05,\n",
            "        3.1650e-05, 3.2663e-05, 3.1710e-05, 3.0398e-05, 3.1412e-05, 3.2663e-05,\n",
            "        3.1650e-05, 3.0220e-05, 3.1412e-05, 3.2663e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.1471e-05, 3.0398e-05, 3.1412e-05, 3.1412e-05, 3.2485e-05, 3.1650e-05,\n",
            "        3.1650e-05, 3.0220e-05, 3.1710e-05, 3.0398e-05, 3.1412e-05, 3.1412e-05,\n",
            "        3.1650e-05, 3.0220e-05, 3.1412e-05, 3.1412e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.0816e-05, 3.0398e-05, 3.1412e-05, 3.1412e-05, 3.1650e-05, 3.0816e-05,\n",
            "        3.1650e-05, 3.0220e-05, 3.0398e-05, 3.0398e-05, 3.1412e-05, 3.0398e-05,\n",
            "        3.1650e-05, 3.0220e-05, 3.1412e-05, 3.1412e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.0816e-05, 3.0220e-05, 3.1412e-05, 3.1412e-05, 3.1650e-05, 3.0816e-05,\n",
            "        3.1650e-05, 3.0220e-05, 3.0398e-05, 3.0398e-05, 3.1412e-05, 3.0398e-05,\n",
            "        3.0994e-05, 3.0220e-05, 3.1233e-05, 3.1412e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.0816e-05, 3.0279e-05, 3.0398e-05, 3.1412e-05, 3.0816e-05, 3.0816e-05,\n",
            "        3.1650e-05, 3.0220e-05, 3.0398e-05, 3.0220e-05, 3.0398e-05, 3.0398e-05,\n",
            "        3.0994e-05, 3.0220e-05, 3.0220e-05, 3.1412e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.7285e-05, 1.7345e-05, 2.3961e-05, 1.8179e-05, 1.8537e-05, 2.0146e-05,\n",
            "        1.9372e-05, 2.1636e-05, 1.8239e-05, 1.7941e-05, 2.1636e-05, 1.8775e-05,\n",
            "        1.9252e-05, 1.6391e-05, 1.6868e-05, 1.9908e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.8895e-05, 1.5438e-05, 1.9014e-05, 1.4126e-05, 2.0981e-05, 1.7583e-05,\n",
            "        1.9372e-05, 2.2292e-05, 1.8358e-05, 2.1040e-05, 1.7464e-05, 1.8179e-05,\n",
            "        2.3603e-05, 1.6868e-05, 1.4365e-05, 2.1636e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.5438e-05, 1.5855e-05, 2.2411e-05, 1.5795e-05, 1.8597e-05, 1.4663e-05,\n",
            "        1.6212e-05, 2.0385e-05, 1.9431e-05, 1.7881e-05, 2.0385e-05, 2.0325e-05,\n",
            "        2.3305e-05, 1.9431e-05, 1.6391e-05, 2.1636e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.8060e-05, 1.6630e-05, 2.3067e-05, 1.7762e-05, 2.0683e-05, 1.7464e-05,\n",
            "        1.8001e-05, 2.2113e-05, 2.1875e-05, 1.9372e-05, 1.8060e-05, 1.6391e-05,\n",
            "        2.0266e-05, 2.0266e-05, 1.7285e-05, 2.0087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.6153e-05, 1.5378e-05, 1.9073e-05, 1.6272e-05, 1.9431e-05, 1.7524e-05,\n",
            "        1.8418e-05, 1.8001e-05, 1.7941e-05, 1.9848e-05, 1.7881e-05, 1.6689e-05,\n",
            "        1.9848e-05, 1.7524e-05, 1.8716e-05, 1.9789e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.6451e-05, 1.5616e-05, 1.7703e-05, 1.5497e-05, 1.8418e-05, 1.8656e-05,\n",
            "        1.7345e-05, 1.6928e-05, 1.8179e-05, 2.0206e-05, 1.6570e-05, 1.5736e-05,\n",
            "        1.8358e-05, 1.5974e-05, 1.8597e-05, 1.8477e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.6451e-05, 1.5676e-05, 1.7285e-05, 1.5438e-05, 1.6689e-05, 1.6630e-05,\n",
            "        1.7405e-05, 1.6272e-05, 1.6153e-05, 1.8537e-05, 1.6928e-05, 1.6510e-05,\n",
            "        1.7285e-05, 1.6093e-05, 1.8716e-05, 1.7107e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.5736e-05, 1.5795e-05, 1.7226e-05, 1.5557e-05, 1.6630e-05, 1.5855e-05,\n",
            "        1.6928e-05, 1.6332e-05, 1.5557e-05, 1.5795e-05, 1.5855e-05, 1.5855e-05,\n",
            "        1.5199e-05, 1.4544e-05, 1.6570e-05, 1.7107e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.5080e-05, 1.5795e-05, 1.6749e-05, 1.5616e-05, 1.6630e-05, 1.5914e-05,\n",
            "        1.7047e-05, 1.6332e-05, 1.5557e-05, 1.5736e-05, 1.5259e-05, 1.5914e-05,\n",
            "        1.5199e-05, 1.4663e-05, 1.6689e-05, 1.7107e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.5020e-05, 1.5795e-05, 1.6689e-05, 1.5080e-05, 1.6630e-05, 1.5974e-05,\n",
            "        1.6332e-05, 1.6391e-05, 1.5736e-05, 1.4842e-05, 1.4842e-05, 1.5974e-05,\n",
            "        1.5259e-05, 1.4603e-05, 1.6630e-05, 1.6570e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.5259e-05, 1.5736e-05, 1.6689e-05, 1.5199e-05, 1.6689e-05, 1.5974e-05,\n",
            "        1.6510e-05, 1.5676e-05, 1.4961e-05, 1.4901e-05, 1.4901e-05, 1.5974e-05,\n",
            "        1.6212e-05, 1.4246e-05, 1.6689e-05, 1.6689e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.5199e-05, 1.4961e-05, 1.6689e-05, 1.4424e-05, 1.6689e-05, 1.5974e-05,\n",
            "        1.5855e-05, 1.5676e-05, 1.4901e-05, 1.4961e-05, 1.4901e-05, 1.5974e-05,\n",
            "        1.6391e-05, 1.4246e-05, 1.5199e-05, 1.5676e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.3590e-05, 1.3530e-05, 1.5736e-05, 1.3590e-05, 1.6749e-05, 1.6093e-05,\n",
            "        1.4424e-05, 1.5020e-05, 1.5020e-05, 1.5020e-05, 1.4961e-05, 1.6093e-05,\n",
            "        1.5795e-05, 1.3769e-05, 1.5259e-05, 1.5676e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.3590e-05, 1.3530e-05, 1.5795e-05, 1.3590e-05, 1.6034e-05, 1.5855e-05,\n",
            "        1.4424e-05, 1.5020e-05, 1.5140e-05, 1.5080e-05, 1.4305e-05, 1.5140e-05,\n",
            "        1.4901e-05, 1.2875e-05, 1.5259e-05, 1.5736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.3530e-05, 1.3530e-05, 1.4961e-05, 1.3590e-05, 1.5974e-05, 1.2696e-05,\n",
            "        1.4544e-05, 1.5259e-05, 1.5199e-05, 1.5020e-05, 1.4424e-05, 1.2696e-05,\n",
            "        1.4961e-05, 1.2875e-05, 1.2696e-05, 1.5736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.3530e-05, 1.3590e-05, 1.5080e-05, 1.3590e-05, 1.2815e-05, 1.2696e-05,\n",
            "        1.4544e-05, 1.2636e-05, 1.2875e-05, 1.2875e-05, 1.4484e-05, 1.2696e-05,\n",
            "        1.4961e-05, 1.2875e-05, 1.2696e-05, 1.4901e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.3530e-05, 1.3590e-05, 1.2815e-05, 1.3590e-05, 1.2696e-05, 1.2696e-05,\n",
            "        1.4544e-05, 1.2636e-05, 1.2815e-05, 1.2755e-05, 1.2338e-05, 1.2696e-05,\n",
            "        1.2875e-05, 1.2875e-05, 1.2696e-05, 1.2755e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.3530e-05, 1.3590e-05, 1.2815e-05, 1.3530e-05, 1.2696e-05, 1.2696e-05,\n",
            "        1.3709e-05, 1.2696e-05, 1.2815e-05, 1.2815e-05, 1.2338e-05, 1.2696e-05,\n",
            "        1.2696e-05, 1.1563e-05, 1.2696e-05, 1.2755e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.3530e-05, 1.2815e-05, 1.2815e-05, 1.2696e-05, 1.2696e-05, 1.2696e-05,\n",
            "        1.2815e-05, 1.2755e-05, 1.2338e-05, 1.2338e-05, 1.2398e-05, 1.2696e-05,\n",
            "        1.2755e-05, 1.1563e-05, 1.2696e-05, 1.2875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.2696e-05, 1.2815e-05, 1.2696e-05, 1.2696e-05, 1.2696e-05, 1.2696e-05,\n",
            "        1.2815e-05, 1.2815e-05, 1.2338e-05, 1.2457e-05, 1.2219e-05, 1.2696e-05,\n",
            "        1.2040e-05, 1.1623e-05, 1.2696e-05, 1.2815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([5.2273e-05, 4.3452e-05, 3.4332e-05, 3.3379e-05, 4.5896e-05, 4.2140e-05,\n",
            "        3.6895e-05, 4.1783e-05, 3.5644e-05, 4.0531e-05, 3.7670e-05, 3.6478e-05,\n",
            "        5.6565e-05, 3.7491e-05, 3.9041e-05, 4.8757e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.9247e-05, 4.0650e-05, 3.6776e-05, 3.1114e-05, 4.0472e-05, 3.8087e-05,\n",
            "        3.3200e-05, 4.4763e-05, 4.2200e-05, 4.5300e-05, 3.4750e-05, 3.6120e-05,\n",
            "        5.0843e-05, 3.5167e-05, 3.7730e-05, 4.5061e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.1008e-05, 4.1246e-05, 4.1902e-05, 3.4213e-05, 4.4525e-05, 3.6538e-05,\n",
            "        3.4332e-05, 4.2677e-05, 4.1664e-05, 4.1544e-05, 3.8981e-05, 3.4511e-05,\n",
            "        3.1590e-05, 3.4928e-05, 4.1306e-05, 5.1916e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.4153e-05, 4.0352e-05, 3.4690e-05, 3.2067e-05, 3.9279e-05, 3.4690e-05,\n",
            "        3.2663e-05, 3.5524e-05, 3.3081e-05, 3.3259e-05, 3.5167e-05, 3.3915e-05,\n",
            "        3.0696e-05, 3.2485e-05, 3.6538e-05, 3.4153e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.4750e-05, 4.0829e-05, 3.4690e-05, 3.2842e-05, 3.3498e-05, 3.3677e-05,\n",
            "        3.3438e-05, 3.5703e-05, 3.3855e-05, 3.3975e-05, 3.5346e-05, 3.5882e-05,\n",
            "        3.2067e-05, 3.2067e-05, 3.3379e-05, 3.5644e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.5346e-05, 3.6299e-05, 3.4869e-05, 3.2842e-05, 3.3915e-05, 3.3915e-05,\n",
            "        3.3677e-05, 3.6001e-05, 3.4094e-05, 3.4034e-05, 3.5703e-05, 3.6299e-05,\n",
            "        3.4273e-05, 3.2067e-05, 3.4630e-05, 3.6180e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.5763e-05, 3.5524e-05, 3.3677e-05, 3.2842e-05, 3.4153e-05, 3.3975e-05,\n",
            "        3.3677e-05, 3.3438e-05, 3.4213e-05, 3.2723e-05, 3.5882e-05, 3.4690e-05,\n",
            "        3.4571e-05, 3.2067e-05, 3.4690e-05, 3.6299e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.3796e-05, 3.5524e-05, 3.3677e-05, 3.2842e-05, 3.4511e-05, 3.3438e-05,\n",
            "        3.3677e-05, 3.3438e-05, 3.4451e-05, 3.2783e-05, 3.5882e-05, 3.4690e-05,\n",
            "        3.4392e-05, 3.2067e-05, 3.4690e-05, 3.5524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.3796e-05, 3.5524e-05, 3.3677e-05, 3.2842e-05, 3.4392e-05, 3.3438e-05,\n",
            "        3.3677e-05, 3.3438e-05, 3.3200e-05, 3.2842e-05, 3.4690e-05, 3.4690e-05,\n",
            "        3.4690e-05, 3.2842e-05, 3.4690e-05, 3.5524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.3796e-05, 3.5524e-05, 3.3677e-05, 3.2842e-05, 3.4392e-05, 3.3438e-05,\n",
            "        3.3677e-05, 3.4690e-05, 3.3259e-05, 3.2902e-05, 3.4690e-05, 3.4690e-05,\n",
            "        3.4690e-05, 3.2842e-05, 3.4690e-05, 3.5524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.2067e-05, 3.5524e-05, 3.3677e-05, 3.2842e-05, 3.4392e-05, 3.2067e-05,\n",
            "        3.3438e-05, 3.4690e-05, 3.3259e-05, 3.2842e-05, 3.3081e-05, 3.4690e-05,\n",
            "        3.4690e-05, 3.2842e-05, 3.4690e-05, 3.5524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.2187e-05, 3.3021e-05, 3.3677e-05, 3.1590e-05, 3.4392e-05, 3.1710e-05,\n",
            "        3.3438e-05, 3.3259e-05, 3.3259e-05, 3.2842e-05, 3.1531e-05, 3.3021e-05,\n",
            "        3.3259e-05, 3.1590e-05, 3.2783e-05, 3.5524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.0875e-05, 3.3021e-05, 3.2485e-05, 3.1590e-05, 3.2783e-05, 3.0637e-05,\n",
            "        3.2306e-05, 3.1710e-05, 3.1710e-05, 3.1471e-05, 3.0458e-05, 3.1710e-05,\n",
            "        3.1710e-05, 3.1590e-05, 3.2783e-05, 3.3021e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.0875e-05, 3.2008e-05, 3.2485e-05, 3.1590e-05, 3.2783e-05, 3.0637e-05,\n",
            "        3.2306e-05, 3.0637e-05, 3.1710e-05, 3.1471e-05, 3.0458e-05, 3.0637e-05,\n",
            "        3.0637e-05, 3.1590e-05, 3.2783e-05, 3.3021e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.0875e-05, 3.2008e-05, 3.2485e-05, 3.1590e-05, 3.1710e-05, 3.0637e-05,\n",
            "        3.2306e-05, 3.0637e-05, 3.1710e-05, 3.1471e-05, 3.0458e-05, 3.0458e-05,\n",
            "        3.0637e-05, 3.1590e-05, 3.1531e-05, 3.3021e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.0875e-05, 3.2008e-05, 3.2306e-05, 3.0756e-05, 3.1710e-05, 3.0458e-05,\n",
            "        3.1471e-05, 3.0637e-05, 3.1531e-05, 3.1471e-05, 3.0458e-05, 3.0458e-05,\n",
            "        3.0637e-05, 3.1471e-05, 3.0458e-05, 3.3021e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.9445e-05, 3.0637e-05, 3.0756e-05, 3.0756e-05, 3.1710e-05, 3.0458e-05,\n",
            "        3.1471e-05, 3.0637e-05, 3.1531e-05, 3.0577e-05, 3.0458e-05, 3.0458e-05,\n",
            "        3.0637e-05, 3.1471e-05, 3.0637e-05, 3.3021e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.7597e-05, 3.0637e-05, 3.0756e-05, 3.0756e-05, 3.0637e-05, 3.0458e-05,\n",
            "        3.0756e-05, 3.0458e-05, 3.1531e-05, 3.0518e-05, 3.0458e-05, 3.0458e-05,\n",
            "        3.0637e-05, 3.1471e-05, 3.0637e-05, 3.1710e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.7776e-05, 3.0637e-05, 3.0756e-05, 3.0756e-05, 3.0637e-05, 3.0458e-05,\n",
            "        3.0756e-05, 3.0458e-05, 3.0458e-05, 3.0577e-05, 3.0458e-05, 3.0458e-05,\n",
            "        3.0637e-05, 3.1471e-05, 3.0637e-05, 3.1710e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.7835e-05, 3.0458e-05, 3.0756e-05, 3.0756e-05, 3.0637e-05, 3.0458e-05,\n",
            "        3.0756e-05, 3.0458e-05, 3.0458e-05, 3.0696e-05, 3.0458e-05, 3.0458e-05,\n",
            "        3.0637e-05, 3.1471e-05, 3.0637e-05, 3.1710e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.5703e-05, 4.5836e-05, 4.4405e-05, 4.4346e-05, 4.7565e-05, 6.1989e-05,\n",
            "        3.4153e-05, 4.4525e-05, 4.3154e-05, 4.7803e-05, 4.9949e-05, 3.7789e-05,\n",
            "        6.0678e-05, 6.3837e-05, 6.5923e-05, 5.0664e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.9399e-05, 4.6194e-05, 3.8922e-05, 5.1320e-05, 5.5313e-05, 5.2810e-05,\n",
            "        3.7253e-05, 4.1544e-05, 4.7863e-05, 5.0962e-05, 4.2319e-05, 3.5524e-05,\n",
            "        5.6386e-05, 5.2333e-05, 7.3969e-05, 5.3883e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.8385e-05, 3.8862e-05, 3.7193e-05, 3.3975e-05, 4.8280e-05, 4.7982e-05,\n",
            "        3.4928e-05, 3.6120e-05, 4.1962e-05, 3.9518e-05, 3.6061e-05, 3.3796e-05,\n",
            "        4.7326e-05, 4.4465e-05, 6.3062e-05, 4.7684e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.0889e-05, 4.5359e-05, 4.1366e-05, 3.5286e-05, 3.9041e-05, 4.1604e-05,\n",
            "        3.4332e-05, 3.6001e-05, 4.2677e-05, 3.7611e-05, 3.9399e-05, 3.6836e-05,\n",
            "        4.5180e-05, 4.0829e-05, 5.0962e-05, 4.5419e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.6299e-05, 3.7313e-05, 3.8505e-05, 3.6359e-05, 3.7968e-05, 3.9816e-05,\n",
            "        3.6538e-05, 3.6836e-05, 4.0472e-05, 3.7789e-05, 3.8445e-05, 3.7372e-05,\n",
            "        3.8743e-05, 3.8862e-05, 3.9220e-05, 3.7551e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.7551e-05, 3.7313e-05, 3.9637e-05, 3.7730e-05, 3.8326e-05, 4.0233e-05,\n",
            "        3.6538e-05, 3.8981e-05, 3.8922e-05, 3.8564e-05, 3.8922e-05, 3.7372e-05,\n",
            "        3.8981e-05, 3.9339e-05, 3.8028e-05, 3.7789e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.7551e-05, 3.7551e-05, 3.9876e-05, 3.8028e-05, 3.6836e-05, 4.0412e-05,\n",
            "        3.6538e-05, 3.8326e-05, 3.8981e-05, 3.8683e-05, 3.8981e-05, 3.7372e-05,\n",
            "        3.7372e-05, 3.9697e-05, 3.8207e-05, 3.6478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.7551e-05, 3.7551e-05, 3.9876e-05, 3.8266e-05, 3.7014e-05, 4.0710e-05,\n",
            "        3.7551e-05, 3.7611e-05, 3.8981e-05, 3.7193e-05, 3.8981e-05, 3.7313e-05,\n",
            "        3.6657e-05, 3.7611e-05, 3.6418e-05, 3.6478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.7551e-05, 3.7551e-05, 3.9876e-05, 3.8683e-05, 3.7253e-05, 3.8981e-05,\n",
            "        3.7551e-05, 3.7670e-05, 3.7253e-05, 3.7551e-05, 3.8981e-05, 3.7313e-05,\n",
            "        3.6955e-05, 3.7968e-05, 3.6418e-05, 3.6776e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.7551e-05, 3.7551e-05, 3.7849e-05, 3.8922e-05, 3.5584e-05, 3.6895e-05,\n",
            "        3.5048e-05, 3.7789e-05, 3.4034e-05, 3.5584e-05, 3.6895e-05, 3.7313e-05,\n",
            "        3.5048e-05, 3.8326e-05, 3.4571e-05, 3.4869e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.5048e-05, 3.6299e-05, 3.5405e-05, 3.6895e-05, 3.5584e-05, 3.6895e-05,\n",
            "        3.5048e-05, 3.6359e-05, 3.4034e-05, 3.5584e-05, 3.6895e-05, 3.6120e-05,\n",
            "        3.5286e-05, 3.5107e-05, 3.4750e-05, 3.5226e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.5048e-05, 3.6299e-05, 3.5405e-05, 3.6895e-05, 3.5584e-05, 3.6895e-05,\n",
            "        3.5048e-05, 3.6418e-05, 3.4034e-05, 3.5584e-05, 3.6895e-05, 3.4928e-05,\n",
            "        3.5584e-05, 3.5405e-05, 3.4988e-05, 3.5584e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.5048e-05, 3.6299e-05, 3.5405e-05, 3.5405e-05, 3.5584e-05, 3.6895e-05,\n",
            "        3.4928e-05, 3.3855e-05, 3.4034e-05, 3.4094e-05, 3.5405e-05, 3.4928e-05,\n",
            "        3.5584e-05, 3.5405e-05, 3.5226e-05, 3.5584e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.5048e-05, 3.6299e-05, 3.4034e-05, 3.5405e-05, 3.5584e-05, 3.6895e-05,\n",
            "        3.4928e-05, 3.3975e-05, 3.3855e-05, 3.4094e-05, 3.4034e-05, 3.4928e-05,\n",
            "        3.5584e-05, 3.5405e-05, 3.5584e-05, 3.5584e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.5048e-05, 3.6299e-05, 3.4034e-05, 3.4034e-05, 3.4094e-05, 3.5584e-05,\n",
            "        3.4928e-05, 3.4034e-05, 3.3855e-05, 3.4094e-05, 3.4034e-05, 3.5048e-05,\n",
            "        3.4332e-05, 3.4034e-05, 3.5584e-05, 3.5584e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.5048e-05, 3.6299e-05, 3.4034e-05, 3.4034e-05, 3.3915e-05, 3.2842e-05,\n",
            "        3.4928e-05, 3.3915e-05, 3.3855e-05, 3.4094e-05, 3.4034e-05, 3.5048e-05,\n",
            "        3.4332e-05, 3.4034e-05, 3.4094e-05, 3.4094e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.5048e-05, 3.6299e-05, 3.4034e-05, 3.4034e-05, 3.3915e-05, 3.2842e-05,\n",
            "        3.4928e-05, 3.3975e-05, 3.3855e-05, 3.4094e-05, 3.3855e-05, 3.5048e-05,\n",
            "        3.2604e-05, 3.2306e-05, 3.4094e-05, 3.4094e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.4928e-05, 3.6120e-05, 3.2306e-05, 3.4034e-05, 3.3915e-05, 3.2842e-05,\n",
            "        3.4928e-05, 3.4034e-05, 3.3855e-05, 3.2604e-05, 3.3855e-05, 3.5048e-05,\n",
            "        3.2604e-05, 3.2306e-05, 3.4094e-05, 3.4094e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.4928e-05, 3.4928e-05, 3.2306e-05, 3.4034e-05, 3.3915e-05, 3.2842e-05,\n",
            "        3.4094e-05, 3.4034e-05, 3.3855e-05, 3.0935e-05, 3.2127e-05, 3.5048e-05,\n",
            "        3.2604e-05, 3.2306e-05, 3.4094e-05, 3.4094e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.4928e-05, 3.4928e-05, 3.2306e-05, 3.2306e-05, 3.2127e-05, 3.1114e-05,\n",
            "        3.4094e-05, 3.4034e-05, 3.2127e-05, 3.0935e-05, 3.2127e-05, 3.5048e-05,\n",
            "        3.2604e-05, 3.2306e-05, 3.4094e-05, 3.2127e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0003,\n",
            "        0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002,\n",
            "        0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003,\n",
            "        0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0001, 0.0001, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([2.3723e-05, 1.9133e-05, 1.8895e-05, 1.7643e-05, 1.5318e-05, 2.8431e-05,\n",
            "        1.9073e-05, 2.3901e-05, 1.5676e-05, 2.4438e-05, 1.3888e-05, 3.2663e-05,\n",
            "        1.8597e-05, 2.4974e-05, 1.7643e-05, 2.1577e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.7359e-05, 2.1458e-05, 1.7941e-05, 1.9968e-05, 1.6034e-05, 2.3603e-05,\n",
            "        2.3425e-05, 2.3782e-05, 1.8775e-05, 1.9550e-05, 1.3888e-05, 3.4153e-05,\n",
            "        1.7643e-05, 2.3007e-05, 2.1338e-05, 2.0802e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.0325e-05, 2.1458e-05, 1.7703e-05, 2.1398e-05, 1.7822e-05, 2.4498e-05,\n",
            "        2.3246e-05, 1.7703e-05, 1.9133e-05, 2.2113e-05, 1.4842e-05, 3.2008e-05,\n",
            "        1.9073e-05, 2.2411e-05, 1.7285e-05, 2.1696e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.0921e-05, 1.9014e-05, 1.8954e-05, 2.0385e-05, 2.0087e-05, 2.5749e-05,\n",
            "        2.1517e-05, 1.9789e-05, 1.5616e-05, 1.6630e-05, 1.5974e-05, 2.9385e-05,\n",
            "        1.9372e-05, 2.5988e-05, 1.6153e-05, 1.9014e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.9848e-05, 1.6928e-05, 1.6332e-05, 1.8418e-05, 1.6928e-05, 1.9372e-05,\n",
            "        1.8418e-05, 1.7047e-05, 1.6391e-05, 1.5795e-05, 1.7822e-05, 1.7941e-05,\n",
            "        1.6391e-05, 2.1100e-05, 1.5259e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.0087e-05, 1.7166e-05, 1.6391e-05, 1.6391e-05, 1.7107e-05, 1.8358e-05,\n",
            "        1.6272e-05, 1.7107e-05, 1.6689e-05, 1.5795e-05, 1.7405e-05, 1.6510e-05,\n",
            "        1.7226e-05, 1.9372e-05, 1.5378e-05, 1.7166e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.8775e-05, 1.7166e-05, 1.6510e-05, 1.6510e-05, 1.7285e-05, 1.7166e-05,\n",
            "        1.6391e-05, 1.7226e-05, 1.6749e-05, 1.8299e-05, 1.7703e-05, 1.7762e-05,\n",
            "        1.7405e-05, 1.7822e-05, 1.5318e-05, 1.7166e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.8775e-05, 1.7226e-05, 1.6749e-05, 1.6510e-05, 1.6451e-05, 1.7107e-05,\n",
            "        1.6451e-05, 1.7285e-05, 1.6034e-05, 1.7464e-05, 1.6749e-05, 1.7643e-05,\n",
            "        1.7524e-05, 1.7822e-05, 1.6332e-05, 1.7285e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.8299e-05, 1.7285e-05, 1.6749e-05, 1.6689e-05, 1.5795e-05, 1.7166e-05,\n",
            "        1.5497e-05, 1.7345e-05, 1.5974e-05, 1.7524e-05, 1.5974e-05, 1.6451e-05,\n",
            "        1.7583e-05, 1.8001e-05, 1.6272e-05, 1.6451e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.8418e-05, 1.6451e-05, 1.6809e-05, 1.5795e-05, 1.5914e-05, 1.7405e-05,\n",
            "        1.5557e-05, 1.7464e-05, 1.5914e-05, 1.6749e-05, 1.5974e-05, 1.6510e-05,\n",
            "        1.7762e-05, 1.7941e-05, 1.6630e-05, 1.6570e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.8358e-05, 1.6630e-05, 1.6749e-05, 1.5736e-05, 1.5259e-05, 1.7464e-05,\n",
            "        1.5676e-05, 1.7405e-05, 1.5914e-05, 1.6034e-05, 1.5974e-05, 1.6630e-05,\n",
            "        1.7703e-05, 1.8001e-05, 1.6689e-05, 1.6630e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.8477e-05, 1.5736e-05, 1.5914e-05, 1.5080e-05, 1.4305e-05, 1.7643e-05,\n",
            "        1.5020e-05, 1.7524e-05, 1.4305e-05, 1.5140e-05, 1.5140e-05, 1.6689e-05,\n",
            "        1.5795e-05, 1.8060e-05, 1.5795e-05, 1.5855e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.8477e-05, 1.5140e-05, 1.5140e-05, 1.5020e-05, 1.4305e-05, 1.6749e-05,\n",
            "        1.5140e-05, 1.5974e-05, 1.4305e-05, 1.5140e-05, 1.5140e-05, 1.6868e-05,\n",
            "        1.5020e-05, 1.7226e-05, 1.5855e-05, 1.5795e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7583e-05, 1.5199e-05, 1.5140e-05, 1.5020e-05, 1.4305e-05, 1.6809e-05,\n",
            "        1.5020e-05, 1.5259e-05, 1.4305e-05, 1.4424e-05, 1.5140e-05, 1.6809e-05,\n",
            "        1.5020e-05, 1.7405e-05, 1.5914e-05, 1.5914e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.4186e-05, 1.5140e-05, 1.5140e-05, 1.4961e-05, 1.4424e-05, 1.4067e-05,\n",
            "        1.5020e-05, 1.5080e-05, 1.4305e-05, 1.4424e-05, 1.4424e-05, 1.7107e-05,\n",
            "        1.4305e-05, 1.7643e-05, 1.4424e-05, 1.5140e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.4186e-05, 1.5140e-05, 1.5140e-05, 1.5020e-05, 1.4424e-05, 1.3947e-05,\n",
            "        1.4961e-05, 1.4305e-05, 1.4305e-05, 1.4424e-05, 1.4424e-05, 1.7166e-05,\n",
            "        1.4305e-05, 1.4782e-05, 1.4424e-05, 1.5140e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.4186e-05, 1.4305e-05, 1.5020e-05, 1.4067e-05, 1.4424e-05, 1.4067e-05,\n",
            "        1.4007e-05, 1.4305e-05, 1.4424e-05, 1.4424e-05, 1.4305e-05, 1.4424e-05,\n",
            "        1.4305e-05, 1.4842e-05, 1.4424e-05, 1.5080e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.4186e-05, 1.4305e-05, 1.3530e-05, 1.4007e-05, 1.4424e-05, 1.4067e-05,\n",
            "        1.3828e-05, 1.4305e-05, 1.4424e-05, 1.4424e-05, 1.4305e-05, 1.3888e-05,\n",
            "        1.4305e-05, 1.4842e-05, 1.4424e-05, 1.4305e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.4186e-05, 1.4305e-05, 1.3530e-05, 1.4067e-05, 1.4424e-05, 1.4067e-05,\n",
            "        1.3888e-05, 1.4305e-05, 1.4424e-05, 1.4305e-05, 1.3530e-05, 1.4067e-05,\n",
            "        1.4305e-05, 1.4067e-05, 1.4424e-05, 1.4305e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.4186e-05, 1.4305e-05, 1.3530e-05, 1.4186e-05, 1.4424e-05, 1.4067e-05,\n",
            "        1.3947e-05, 1.4305e-05, 1.4424e-05, 1.4305e-05, 1.3530e-05, 1.4126e-05,\n",
            "        1.4305e-05, 1.4067e-05, 1.3590e-05, 1.3530e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.3220e-04, 8.9407e-05, 8.3864e-05, 1.1468e-04, 1.1551e-04, 1.3721e-04,\n",
            "        9.8944e-05, 8.6844e-05, 1.1730e-04, 1.1396e-04, 1.1492e-04, 1.0633e-04,\n",
            "        1.1456e-04, 1.1063e-04, 1.1951e-04, 1.3745e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.1957e-04, 9.4593e-05, 9.3579e-05, 1.1843e-04, 1.1331e-04, 1.4091e-04,\n",
            "        1.0294e-04, 9.8944e-05, 1.2207e-04, 1.2159e-04, 1.0675e-04, 9.6023e-05,\n",
            "        1.3089e-04, 1.0443e-04, 1.2469e-04, 1.0824e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([9.4116e-05, 9.7752e-05, 9.3818e-05, 9.4712e-05, 9.4414e-05, 9.5487e-05,\n",
            "        9.6262e-05, 1.0115e-04, 9.4295e-05, 9.4116e-05, 9.6381e-05, 9.6917e-05,\n",
            "        1.0061e-04, 9.5785e-05, 1.0937e-04, 9.6321e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([9.4414e-05, 9.7871e-05, 9.7811e-05, 9.6798e-05, 9.4414e-05, 9.5487e-05,\n",
            "        9.7871e-05, 1.0026e-04, 9.4414e-05, 9.4414e-05, 9.8705e-05, 1.0026e-04,\n",
            "        9.4116e-05, 9.4414e-05, 1.0091e-04, 9.6917e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([9.4414e-05, 9.4414e-05, 9.7811e-05, 9.4414e-05, 9.4414e-05, 9.4831e-05,\n",
            "        9.4473e-05, 9.6798e-05, 9.4414e-05, 9.4414e-05, 9.6679e-05, 9.7811e-05,\n",
            "        9.4116e-05, 9.4414e-05, 9.5606e-05, 9.6917e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4116e-05, 9.4831e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4473e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.5606e-05, 9.6917e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4116e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.5606e-05, 9.6798e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4116e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.5606e-05, 9.5606e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4116e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.5606e-05, 9.5606e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4116e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.4414e-05, 9.5606e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4116e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.4414e-05, 9.5606e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([9.4414e-05, 9.2030e-05, 9.2030e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05, 9.4414e-05,\n",
            "        9.4414e-05, 9.4116e-05, 9.4414e-05, 9.5606e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([9.4414e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05, 9.2030e-05, 9.4414e-05,\n",
            "        9.2030e-05, 9.2030e-05, 9.4414e-05, 9.2030e-05, 9.2030e-05, 9.2030e-05,\n",
            "        9.2030e-05, 9.1791e-05, 9.2030e-05, 9.3162e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([9.2030e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05, 9.2030e-05, 9.2030e-05,\n",
            "        9.1791e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05, 9.2030e-05, 9.2030e-05,\n",
            "        9.2030e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([9.2030e-05, 9.1791e-05, 9.2030e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05,\n",
            "        9.1791e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05, 9.2030e-05, 9.2030e-05,\n",
            "        9.2030e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([9.2030e-05, 9.1791e-05, 9.0599e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05,\n",
            "        9.1791e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05, 9.2030e-05, 9.1791e-05,\n",
            "        9.2030e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([9.2030e-05, 9.0301e-05, 9.0599e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05,\n",
            "        9.1791e-05, 9.1791e-05, 9.2030e-05, 9.1791e-05, 9.2030e-05, 9.1791e-05,\n",
            "        9.2030e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([9.2030e-05, 9.0301e-05, 9.0599e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05,\n",
            "        9.1791e-05, 9.1791e-05, 9.2030e-05, 9.1791e-05, 9.2030e-05, 9.1791e-05,\n",
            "        9.2030e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([9.1791e-05, 9.0301e-05, 9.0599e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05,\n",
            "        9.1791e-05, 9.0301e-05, 9.2030e-05, 9.0301e-05, 9.1791e-05, 9.1791e-05,\n",
            "        9.2030e-05, 9.1791e-05, 9.2030e-05, 9.2030e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.1306e-05, 4.1544e-05, 4.3690e-05, 3.8624e-05, 3.7014e-05, 5.9605e-05,\n",
            "        4.8757e-05, 3.6716e-05, 4.1485e-05, 4.9233e-05, 3.4153e-05, 3.4630e-05,\n",
            "        5.0545e-05, 5.4359e-05, 3.5763e-05, 3.7432e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([4.5180e-05, 4.3511e-05, 3.7551e-05, 3.8803e-05, 3.8147e-05, 4.7982e-05,\n",
            "        5.5611e-05, 3.6716e-05, 4.6253e-05, 4.5300e-05, 3.4153e-05, 3.0816e-05,\n",
            "        5.0128e-05, 6.2764e-05, 3.5048e-05, 3.4392e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.0591e-05, 4.1544e-05, 3.8624e-05, 4.7326e-05, 4.3452e-05, 4.9472e-05,\n",
            "        3.6597e-05, 3.6657e-05, 3.7849e-05, 4.7207e-05, 3.7014e-05, 3.2425e-05,\n",
            "        4.2140e-05, 5.1677e-05, 3.7014e-05, 4.0352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.7432e-05, 3.1590e-05, 3.0994e-05, 3.7372e-05, 3.5763e-05, 4.0412e-05,\n",
            "        3.6180e-05, 3.7551e-05, 3.7372e-05, 4.2558e-05, 3.5644e-05, 3.1233e-05,\n",
            "        3.7491e-05, 4.2319e-05, 3.2604e-05, 3.8743e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.8981e-05, 3.3259e-05, 3.0518e-05, 3.2902e-05, 3.2783e-05, 3.3379e-05,\n",
            "        3.0994e-05, 3.4809e-05, 3.4332e-05, 3.4034e-05, 3.2425e-05, 3.1650e-05,\n",
            "        3.3140e-05, 3.2961e-05, 3.2902e-05, 3.3617e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.5048e-05, 3.3379e-05, 3.0696e-05, 3.3259e-05, 3.3438e-05, 3.3557e-05,\n",
            "        3.2425e-05, 3.3438e-05, 3.3319e-05, 3.4153e-05, 3.2723e-05, 3.4809e-05,\n",
            "        3.3319e-05, 3.3081e-05, 3.3140e-05, 3.2365e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.5346e-05, 3.3557e-05, 3.0756e-05, 3.3259e-05, 3.3438e-05, 3.2187e-05,\n",
            "        3.3379e-05, 3.3438e-05, 3.3319e-05, 3.3379e-05, 3.2723e-05, 3.5286e-05,\n",
            "        3.3379e-05, 3.3379e-05, 3.2365e-05, 3.3379e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.3677e-05, 3.3498e-05, 3.0875e-05, 3.3259e-05, 3.3259e-05, 3.2127e-05,\n",
            "        3.3319e-05, 3.3379e-05, 3.1948e-05, 3.3379e-05, 3.2783e-05, 3.4928e-05,\n",
            "        3.3498e-05, 3.3259e-05, 3.1769e-05, 3.3438e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.3796e-05, 3.3677e-05, 3.0935e-05, 3.3200e-05, 3.3379e-05, 3.3200e-05,\n",
            "        3.3379e-05, 3.3557e-05, 3.1948e-05, 3.3379e-05, 3.2902e-05, 3.3200e-05,\n",
            "        3.3438e-05, 3.3259e-05, 3.1888e-05, 3.3498e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.3855e-05, 3.3736e-05, 3.2306e-05, 3.1710e-05, 3.3379e-05, 3.1590e-05,\n",
            "        3.3438e-05, 3.3677e-05, 3.1948e-05, 3.3557e-05, 3.3140e-05, 3.3259e-05,\n",
            "        3.3379e-05, 3.3379e-05, 3.1948e-05, 3.1948e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.3975e-05, 3.0696e-05, 3.2365e-05, 3.2008e-05, 3.1948e-05, 3.1590e-05,\n",
            "        3.3498e-05, 3.1948e-05, 3.1948e-05, 3.3617e-05, 3.3259e-05, 3.3259e-05,\n",
            "        3.1769e-05, 3.3438e-05, 3.2008e-05, 3.2246e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.9266e-05, 2.8610e-05, 3.0637e-05, 2.9981e-05, 3.0160e-05, 3.1710e-05,\n",
            "        3.3677e-05, 2.8729e-05, 3.0160e-05, 3.0577e-05, 3.1531e-05, 3.0160e-05,\n",
            "        3.1948e-05, 3.3438e-05, 3.0220e-05, 3.0398e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.9445e-05, 2.8789e-05, 2.8133e-05, 2.9981e-05, 3.0279e-05, 2.9624e-05,\n",
            "        2.8789e-05, 2.8968e-05, 2.9147e-05, 2.8789e-05, 3.0756e-05, 3.0160e-05,\n",
            "        3.0041e-05, 3.1888e-05, 3.0220e-05, 3.0518e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.9445e-05, 2.9027e-05, 2.8133e-05, 2.8729e-05, 2.8074e-05, 2.9802e-05,\n",
            "        2.8789e-05, 2.7657e-05, 2.8014e-05, 2.8908e-05, 3.0756e-05, 2.8133e-05,\n",
            "        2.8729e-05, 2.9981e-05, 2.8849e-05, 3.0696e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.8133e-05, 2.9087e-05, 2.8133e-05, 2.8968e-05, 2.8133e-05, 2.9802e-05,\n",
            "        2.8908e-05, 2.7895e-05, 2.8074e-05, 2.9027e-05, 2.9504e-05, 2.8133e-05,\n",
            "        2.8789e-05, 2.7478e-05, 2.8133e-05, 2.9445e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.8133e-05, 2.9206e-05, 2.8133e-05, 2.9147e-05, 2.8133e-05, 2.8491e-05,\n",
            "        2.9027e-05, 2.7955e-05, 2.8133e-05, 2.9206e-05, 2.8133e-05, 2.8133e-05,\n",
            "        2.8610e-05, 2.7597e-05, 2.8133e-05, 2.8133e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.8133e-05, 2.7955e-05, 2.8133e-05, 2.9027e-05, 2.8133e-05, 2.8551e-05,\n",
            "        2.7955e-05, 2.7955e-05, 2.8133e-05, 2.9266e-05, 2.8133e-05, 2.8133e-05,\n",
            "        2.8849e-05, 2.7657e-05, 2.8133e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.8133e-05, 2.7955e-05, 2.7955e-05, 2.9147e-05, 2.8133e-05, 2.7478e-05,\n",
            "        2.8193e-05, 2.7955e-05, 2.8133e-05, 2.9445e-05, 2.8133e-05, 2.8133e-05,\n",
            "        2.9027e-05, 2.7955e-05, 2.8133e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.8133e-05, 2.7955e-05, 2.7955e-05, 2.9206e-05, 2.8133e-05, 2.7537e-05,\n",
            "        2.7955e-05, 2.7955e-05, 2.8133e-05, 2.9206e-05, 2.8133e-05, 2.8133e-05,\n",
            "        2.7776e-05, 2.8014e-05, 2.8133e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.8133e-05, 2.7955e-05, 2.7955e-05, 2.9206e-05, 2.8133e-05, 2.7597e-05,\n",
            "        2.7955e-05, 2.7955e-05, 2.8133e-05, 2.9206e-05, 2.8133e-05, 2.8133e-05,\n",
            "        2.7955e-05, 2.8074e-05, 2.8133e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([7.7665e-05, 5.6028e-05, 6.5923e-05, 6.4075e-05, 6.6519e-05, 5.7936e-05,\n",
            "        6.2525e-05, 7.5161e-05, 5.5850e-05, 5.6803e-05, 7.4983e-05, 5.7399e-05,\n",
            "        5.3942e-05, 5.6207e-05, 5.5075e-05, 6.5088e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([7.2479e-05, 5.7995e-05, 5.9426e-05, 6.9499e-05, 6.6280e-05, 6.3896e-05,\n",
            "        5.0902e-05, 6.3539e-05, 5.1498e-05, 5.9366e-05, 5.5850e-05, 5.5313e-05,\n",
            "        5.7340e-05, 5.5492e-05, 5.2154e-05, 6.9976e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([6.5982e-05, 6.3598e-05, 6.4731e-05, 7.4804e-05, 7.5400e-05, 6.7949e-05,\n",
            "        5.4479e-05, 6.4552e-05, 5.6028e-05, 5.6326e-05, 5.8115e-05, 6.0916e-05,\n",
            "        6.2525e-05, 5.8115e-05, 5.7817e-05, 6.0022e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([6.1989e-05, 5.4419e-05, 5.9605e-05, 6.7592e-05, 6.4671e-05, 6.1095e-05,\n",
            "        5.0962e-05, 5.2869e-05, 5.1796e-05, 5.3406e-05, 5.3942e-05, 5.4240e-05,\n",
            "        5.0962e-05, 5.2214e-05, 5.2392e-05, 5.0902e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([5.6863e-05, 5.4359e-05, 5.3585e-05, 6.0201e-05, 5.4419e-05, 5.5194e-05,\n",
            "        5.5194e-05, 5.4419e-05, 5.3942e-05, 5.3406e-05, 5.6386e-05, 5.5194e-05,\n",
            "        5.1975e-05, 5.3823e-05, 5.4181e-05, 5.2750e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([5.3585e-05, 5.4359e-05, 5.3585e-05, 5.3585e-05, 5.4419e-05, 5.5194e-05,\n",
            "        5.5194e-05, 5.4359e-05, 5.3585e-05, 5.3406e-05, 5.5075e-05, 5.5194e-05,\n",
            "        5.1975e-05, 5.3823e-05, 5.2750e-05, 5.3406e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([5.3585e-05, 5.4419e-05, 5.3585e-05, 5.3585e-05, 5.4419e-05, 5.4419e-05,\n",
            "        5.4538e-05, 5.2631e-05, 5.3585e-05, 5.3406e-05, 5.5134e-05, 5.5194e-05,\n",
            "        5.1975e-05, 5.2512e-05, 5.3585e-05, 5.3585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([5.3585e-05, 5.4419e-05, 5.3585e-05, 5.3585e-05, 5.4419e-05, 5.4419e-05,\n",
            "        5.4419e-05, 5.2631e-05, 5.3585e-05, 5.3585e-05, 5.3942e-05, 5.3585e-05,\n",
            "        5.3585e-05, 5.2750e-05, 5.3585e-05, 5.3585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([5.3585e-05, 5.4419e-05, 5.3585e-05, 5.3585e-05, 5.4419e-05, 5.4419e-05,\n",
            "        5.4419e-05, 5.2631e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05,\n",
            "        5.3585e-05, 5.2750e-05, 5.3585e-05, 5.3585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([5.3585e-05, 5.4419e-05, 5.3585e-05, 5.3585e-05, 5.4419e-05, 5.4419e-05,\n",
            "        5.4419e-05, 5.2392e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05,\n",
            "        5.3585e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([5.3585e-05, 5.4419e-05, 5.3406e-05, 5.3585e-05, 5.4419e-05, 5.4419e-05,\n",
            "        5.4419e-05, 5.2333e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05,\n",
            "        5.3585e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([5.3585e-05, 5.3585e-05, 5.3406e-05, 5.3585e-05, 5.4419e-05, 5.4419e-05,\n",
            "        5.4419e-05, 5.2452e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05,\n",
            "        5.3585e-05, 5.3585e-05, 5.3585e-05, 5.3585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([5.3585e-05, 5.2035e-05, 5.3406e-05, 5.3585e-05, 5.3585e-05, 5.2810e-05,\n",
            "        5.2035e-05, 4.8518e-05, 5.2035e-05, 5.2035e-05, 5.3585e-05, 5.2035e-05,\n",
            "        5.2035e-05, 5.2035e-05, 5.2035e-05, 5.3585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([5.2035e-05, 5.2035e-05, 5.1796e-05, 5.2035e-05, 5.2035e-05, 5.2035e-05,\n",
            "        5.2035e-05, 4.8816e-05, 5.1796e-05, 5.2035e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.1796e-05, 5.2035e-05, 5.0902e-05, 5.2035e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([5.2035e-05, 5.2035e-05, 5.2035e-05, 5.2035e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.2035e-05, 4.9114e-05, 5.1796e-05, 5.0902e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.0724e-05, 5.0902e-05, 5.0902e-05, 5.2035e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([5.2035e-05, 5.2035e-05, 5.2035e-05, 5.2035e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.2035e-05, 4.9293e-05, 5.1796e-05, 5.0902e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.0724e-05, 5.0902e-05, 5.0902e-05, 5.2035e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([5.2035e-05, 5.2035e-05, 5.2035e-05, 5.1796e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.0902e-05, 4.9889e-05, 5.1796e-05, 5.0902e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.0724e-05, 5.0902e-05, 5.0902e-05, 5.2035e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([5.1796e-05, 5.0902e-05, 5.2035e-05, 5.1796e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.0902e-05, 5.0366e-05, 5.1796e-05, 5.0902e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.0724e-05, 5.0902e-05, 5.0724e-05, 5.0902e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([5.0724e-05, 5.0902e-05, 5.2035e-05, 5.1796e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.0902e-05, 5.0902e-05, 5.1796e-05, 5.0724e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.0724e-05, 5.0902e-05, 5.0724e-05, 5.0902e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([5.0724e-05, 5.0902e-05, 5.2035e-05, 5.1796e-05, 5.2035e-05, 5.0902e-05,\n",
            "        5.0902e-05, 5.0902e-05, 5.0724e-05, 5.0724e-05, 5.0724e-05, 5.0902e-05,\n",
            "        5.0724e-05, 5.0902e-05, 5.0724e-05, 5.0902e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.2386e-04, 1.0723e-04, 1.0157e-04, 1.0610e-04, 1.3125e-04, 1.2082e-04,\n",
            "        1.0794e-04, 1.1086e-04, 1.0395e-04, 1.1081e-04, 1.0955e-04, 1.2946e-04,\n",
            "        1.0300e-04, 1.1885e-04, 9.8407e-05, 1.2875e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([3.4153e-05, 2.8491e-05, 3.0339e-05, 3.2008e-05, 3.6657e-05, 3.3379e-05,\n",
            "        4.3154e-05, 2.0862e-05, 3.4571e-05, 3.0041e-05, 2.5868e-05, 3.6597e-05,\n",
            "        3.2663e-05, 3.8445e-05, 3.0279e-05, 2.5809e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.8253e-05, 2.9802e-05, 2.7776e-05, 3.2783e-05, 3.4571e-05, 3.0160e-05,\n",
            "        3.5167e-05, 2.2113e-05, 3.1352e-05, 3.3021e-05, 2.3603e-05, 4.0114e-05,\n",
            "        3.4034e-05, 3.8922e-05, 2.6345e-05, 2.8431e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.4034e-05, 3.2067e-05, 2.8312e-05, 3.8266e-05, 2.9624e-05, 3.0816e-05,\n",
            "        3.2008e-05, 2.0862e-05, 3.1471e-05, 2.3484e-05, 2.2888e-05, 3.6478e-05,\n",
            "        3.6240e-05, 3.1292e-05, 2.5690e-05, 2.6762e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.7597e-05, 3.0398e-05, 2.6882e-05, 2.8729e-05, 2.7180e-05, 2.8431e-05,\n",
            "        3.4750e-05, 2.4557e-05, 3.0398e-05, 2.4438e-05, 2.7001e-05, 3.2842e-05,\n",
            "        2.9743e-05, 2.6643e-05, 2.7537e-05, 2.7120e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.2948e-05, 2.7061e-05, 2.2829e-05, 2.7657e-05, 2.6345e-05, 2.3901e-05,\n",
            "        2.8372e-05, 2.3842e-05, 2.1636e-05, 2.1875e-05, 2.3901e-05, 2.7299e-05,\n",
            "        2.2590e-05, 2.8372e-05, 2.3901e-05, 2.3901e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.3961e-05, 2.8849e-05, 2.5272e-05, 2.5928e-05, 2.3603e-05, 2.3961e-05,\n",
            "        2.6643e-05, 2.4140e-05, 2.1696e-05, 2.2829e-05, 2.4259e-05, 2.5511e-05,\n",
            "        2.2709e-05, 2.4557e-05, 2.4080e-05, 2.4259e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.3961e-05, 2.6762e-05, 2.4259e-05, 2.5868e-05, 2.3842e-05, 2.4199e-05,\n",
            "        2.5392e-05, 2.4319e-05, 2.1875e-05, 2.2948e-05, 2.4378e-05, 2.5511e-05,\n",
            "        2.3842e-05, 2.4676e-05, 2.4319e-05, 2.4557e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.4080e-05, 2.5451e-05, 2.4319e-05, 2.5928e-05, 2.4140e-05, 2.4259e-05,\n",
            "        2.5392e-05, 2.3246e-05, 2.1994e-05, 2.3067e-05, 2.4676e-05, 2.5690e-05,\n",
            "        2.4140e-05, 2.4796e-05, 2.4498e-05, 2.3305e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.4199e-05, 2.4498e-05, 2.4498e-05, 2.5272e-05, 2.4140e-05, 2.4319e-05,\n",
            "        2.5392e-05, 2.3246e-05, 2.2113e-05, 2.3186e-05, 2.4736e-05, 2.4676e-05,\n",
            "        2.4199e-05, 2.4736e-05, 2.4676e-05, 2.3305e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.4259e-05, 2.4557e-05, 2.3365e-05, 2.5332e-05, 2.4259e-05, 2.4498e-05,\n",
            "        2.5392e-05, 2.3305e-05, 2.3305e-05, 2.3305e-05, 2.4796e-05, 2.4021e-05,\n",
            "        2.4199e-05, 2.3246e-05, 2.4736e-05, 2.3603e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.3127e-05, 2.4617e-05, 2.3484e-05, 2.5511e-05, 2.3484e-05, 2.4676e-05,\n",
            "        2.5451e-05, 2.3365e-05, 2.3365e-05, 2.3425e-05, 2.4736e-05, 2.4259e-05,\n",
            "        2.4438e-05, 2.3305e-05, 2.3603e-05, 2.3544e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.0921e-05, 2.4617e-05, 2.2292e-05, 2.5511e-05, 2.3603e-05, 2.2233e-05,\n",
            "        2.5630e-05, 2.1160e-05, 2.3425e-05, 2.3544e-05, 2.2352e-05, 2.4498e-05,\n",
            "        2.3365e-05, 2.3425e-05, 2.3544e-05, 2.2292e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.1100e-05, 2.4736e-05, 2.1398e-05, 2.5749e-05, 2.3544e-05, 2.1398e-05,\n",
            "        2.4676e-05, 2.1160e-05, 2.2233e-05, 2.2113e-05, 2.2352e-05, 2.4557e-05,\n",
            "        2.2113e-05, 2.3603e-05, 2.1398e-05, 2.2292e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.1219e-05, 2.0862e-05, 2.1398e-05, 2.5809e-05, 2.3663e-05, 2.1398e-05,\n",
            "        2.4736e-05, 2.1398e-05, 2.2292e-05, 2.2113e-05, 2.0444e-05, 2.3484e-05,\n",
            "        2.2113e-05, 2.3782e-05, 2.1398e-05, 2.2292e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.1398e-05, 2.0862e-05, 2.1398e-05, 2.1636e-05, 1.9670e-05, 2.1398e-05,\n",
            "        1.9789e-05, 2.1398e-05, 2.2292e-05, 2.1219e-05, 2.0444e-05, 2.3544e-05,\n",
            "        2.2292e-05, 2.3961e-05, 2.1398e-05, 2.2292e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.1398e-05, 2.0862e-05, 2.1398e-05, 2.1636e-05, 1.9729e-05, 2.1398e-05,\n",
            "        1.9908e-05, 2.0444e-05, 2.2292e-05, 2.1219e-05, 2.0444e-05, 1.9550e-05,\n",
            "        2.1398e-05, 1.9610e-05, 2.0444e-05, 2.2292e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.1398e-05, 2.0862e-05, 2.1398e-05, 2.0742e-05, 1.9670e-05, 2.1398e-05,\n",
            "        1.9252e-05, 2.0444e-05, 2.2292e-05, 2.1219e-05, 2.0444e-05, 1.9670e-05,\n",
            "        2.1398e-05, 1.9491e-05, 2.0444e-05, 2.2292e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.0444e-05, 2.0742e-05, 2.1398e-05, 2.0862e-05, 1.9848e-05, 2.1398e-05,\n",
            "        1.9252e-05, 2.0444e-05, 2.0444e-05, 2.1338e-05, 2.0444e-05, 1.9729e-05,\n",
            "        2.1398e-05, 1.9431e-05, 2.0444e-05, 2.2292e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.0444e-05, 2.0742e-05, 2.0444e-05, 2.0862e-05, 1.9848e-05, 2.1398e-05,\n",
            "        1.9252e-05, 2.0444e-05, 2.0444e-05, 2.0027e-05, 2.0206e-05, 1.9848e-05,\n",
            "        2.0444e-05, 1.9550e-05, 1.9252e-05, 2.0862e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.0206e-05, 2.0742e-05, 1.9252e-05, 2.0742e-05, 1.9848e-05, 2.0444e-05,\n",
            "        1.9252e-05, 1.9252e-05, 1.9252e-05, 2.0027e-05, 1.9073e-05, 2.0027e-05,\n",
            "        1.9252e-05, 1.9729e-05, 1.9252e-05, 2.0027e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0035, 0.0033, 0.0036, 0.0036, 0.0032, 0.0034, 0.0034, 0.0032, 0.0034,\n",
            "        0.0033, 0.0034, 0.0035, 0.0034, 0.0034, 0.0034, 0.0034],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0034, 0.0034, 0.0038, 0.0036, 0.0033, 0.0035, 0.0035, 0.0033, 0.0036,\n",
            "        0.0034, 0.0033, 0.0035, 0.0034, 0.0034, 0.0033, 0.0035],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0035, 0.0035, 0.0038, 0.0037, 0.0033, 0.0035, 0.0035, 0.0034, 0.0036,\n",
            "        0.0034, 0.0034, 0.0035, 0.0035, 0.0035, 0.0034, 0.0035],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0035, 0.0036, 0.0036, 0.0036, 0.0034, 0.0036, 0.0035, 0.0035, 0.0035,\n",
            "        0.0034, 0.0035, 0.0035, 0.0035, 0.0035, 0.0035, 0.0035],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0034, 0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0034, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033,\n",
            "        0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([9.0599e-05, 9.7871e-05, 1.0830e-04, 1.0860e-04, 9.5308e-05, 9.7752e-05,\n",
            "        8.2433e-05, 8.7917e-05, 9.5785e-05, 1.0461e-04, 9.1791e-05, 8.9765e-05,\n",
            "        1.2481e-04, 9.6917e-05, 1.0359e-04, 9.4831e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([8.6010e-05, 9.9957e-05, 1.1319e-04, 1.0073e-04, 9.0659e-05, 8.6069e-05,\n",
            "        9.0599e-05, 7.3552e-05, 9.2745e-05, 1.1444e-04, 8.4281e-05, 8.3089e-05,\n",
            "        1.1140e-04, 9.5308e-05, 8.8036e-05, 8.8871e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([8.5831e-05, 9.6560e-05, 1.0490e-04, 1.0288e-04, 8.8215e-05, 8.6367e-05,\n",
            "        9.3043e-05, 8.2254e-05, 9.3758e-05, 1.0157e-04, 8.6010e-05, 9.4473e-05,\n",
            "        9.9957e-05, 9.6083e-05, 9.3997e-05, 9.8407e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([8.1301e-05, 8.4519e-05, 8.3148e-05, 8.7380e-05, 8.5235e-05, 8.0764e-05,\n",
            "        8.2135e-05, 7.8857e-05, 7.9334e-05, 8.5652e-05, 8.1122e-05, 8.3804e-05,\n",
            "        8.2374e-05, 8.1539e-05, 9.3281e-05, 8.2612e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([8.2970e-05, 8.5235e-05, 8.3685e-05, 8.3685e-05, 8.4698e-05, 8.2672e-05,\n",
            "        8.3685e-05, 8.2672e-05, 8.1658e-05, 8.4698e-05, 8.3387e-05, 8.5652e-05,\n",
            "        8.3447e-05, 8.3685e-05, 9.3997e-05, 8.3685e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([8.3685e-05, 8.5235e-05, 8.3685e-05, 8.3685e-05, 8.4698e-05, 8.2672e-05,\n",
            "        8.3685e-05, 8.3685e-05, 8.1897e-05, 8.3685e-05, 8.2910e-05, 8.5652e-05,\n",
            "        8.3447e-05, 8.3685e-05, 8.5652e-05, 8.3685e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([8.3685e-05, 8.4698e-05, 8.3685e-05, 8.3685e-05, 8.4698e-05, 8.2910e-05,\n",
            "        8.3685e-05, 8.3685e-05, 8.3685e-05, 8.3685e-05, 8.2910e-05, 8.5652e-05,\n",
            "        8.3447e-05, 8.3685e-05, 8.3685e-05, 8.3685e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([8.3685e-05, 8.4698e-05, 8.3685e-05, 8.3685e-05, 8.4698e-05, 8.2910e-05,\n",
            "        8.3685e-05, 8.3685e-05, 8.3685e-05, 8.3685e-05, 8.3685e-05, 8.5652e-05,\n",
            "        8.3447e-05, 8.3685e-05, 8.3685e-05, 8.3685e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.1480e-05, 8.0585e-05,\n",
            "        8.0585e-05, 8.3685e-05, 8.3685e-05, 8.0585e-05, 8.0585e-05, 8.1480e-05,\n",
            "        8.0287e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.1480e-05, 8.0585e-05,\n",
            "        8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.1480e-05,\n",
            "        8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.1480e-05, 8.0585e-05,\n",
            "        8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0287e-05, 8.1480e-05,\n",
            "        8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05,\n",
            "        8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0287e-05, 8.1480e-05,\n",
            "        8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05,\n",
            "        8.0287e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0287e-05, 8.1480e-05,\n",
            "        8.0585e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([7.8082e-05, 7.8261e-05, 8.0287e-05, 8.0585e-05, 7.8261e-05, 7.8261e-05,\n",
            "        7.8082e-05, 7.8261e-05, 7.8261e-05, 8.0585e-05, 7.8082e-05, 7.9215e-05,\n",
            "        8.0585e-05, 7.8261e-05, 7.8261e-05, 7.8261e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([7.8082e-05, 7.8261e-05, 7.8082e-05, 7.8261e-05, 7.8261e-05, 7.8261e-05,\n",
            "        7.8082e-05, 7.8261e-05, 7.8261e-05, 7.8261e-05, 7.8082e-05, 7.8261e-05,\n",
            "        7.8261e-05, 7.8261e-05, 7.8261e-05, 7.8082e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([7.8082e-05, 7.8261e-05, 7.8082e-05, 7.8261e-05, 7.8261e-05, 7.8261e-05,\n",
            "        7.8082e-05, 7.8261e-05, 7.8261e-05, 7.8261e-05, 7.8082e-05, 7.8261e-05,\n",
            "        7.8261e-05, 7.8082e-05, 7.8261e-05, 7.8082e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([7.8082e-05, 7.6771e-05, 7.8082e-05, 7.8082e-05, 7.8261e-05, 7.8261e-05,\n",
            "        7.8082e-05, 7.8082e-05, 7.8261e-05, 7.8261e-05, 7.8082e-05, 7.8261e-05,\n",
            "        7.8261e-05, 7.8082e-05, 7.8261e-05, 7.8082e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([7.8082e-05, 7.6532e-05, 7.8082e-05, 7.8082e-05, 7.8261e-05, 7.8261e-05,\n",
            "        7.8082e-05, 7.8082e-05, 7.8261e-05, 7.8261e-05, 7.8082e-05, 7.8261e-05,\n",
            "        7.8261e-05, 7.8082e-05, 7.8261e-05, 7.8082e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([7.8082e-05, 7.6532e-05, 7.8082e-05, 7.8082e-05, 7.8261e-05, 7.8082e-05,\n",
            "        7.8082e-05, 7.8082e-05, 7.8082e-05, 7.6771e-05, 7.8082e-05, 7.8261e-05,\n",
            "        7.8261e-05, 7.8082e-05, 7.8261e-05, 7.8082e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([7.8082e-05, 7.6532e-05, 7.8261e-05, 7.8082e-05, 7.8261e-05, 7.8082e-05,\n",
            "        7.6532e-05, 7.8082e-05, 7.6532e-05, 7.6771e-05, 7.8082e-05, 7.8261e-05,\n",
            "        7.8261e-05, 7.8082e-05, 7.8261e-05, 7.8082e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([6.6936e-05, 7.0870e-05, 9.2745e-05, 6.8188e-05, 8.7500e-05, 7.5579e-05,\n",
            "        8.9884e-05, 8.8215e-05, 7.5400e-05, 5.9724e-05, 7.4029e-05, 7.8142e-05,\n",
            "        6.5088e-05, 6.6638e-05, 8.0407e-05, 6.4373e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([7.6473e-05, 7.6354e-05, 8.4579e-05, 6.6757e-05, 8.6725e-05, 7.7069e-05,\n",
            "        9.5785e-05, 8.5235e-05, 8.1360e-05, 6.7055e-05, 6.5267e-05, 7.9095e-05,\n",
            "        6.3062e-05, 6.4552e-05, 7.2002e-05, 6.2406e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([7.3612e-05, 8.1837e-05, 8.9467e-05, 6.7592e-05, 9.8944e-05, 7.2837e-05,\n",
            "        9.6440e-05, 7.8201e-05, 7.0691e-05, 6.6042e-05, 7.3135e-05, 7.2300e-05,\n",
            "        7.1585e-05, 6.9916e-05, 6.5923e-05, 7.1049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([7.2420e-05, 7.4685e-05, 7.6890e-05, 7.4029e-05, 8.4579e-05, 7.1645e-05,\n",
            "        9.4891e-05, 7.0691e-05, 6.8545e-05, 7.2300e-05, 7.1347e-05, 7.0333e-05,\n",
            "        7.3791e-05, 7.2598e-05, 6.8545e-05, 7.3016e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([6.5029e-05, 6.5267e-05, 6.0678e-05, 6.5088e-05, 6.0201e-05, 6.4850e-05,\n",
            "        7.0274e-05, 6.0201e-05, 6.4552e-05, 6.4433e-05, 6.2585e-05, 6.4850e-05,\n",
            "        6.5267e-05, 6.6042e-05, 6.2287e-05, 6.5267e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([6.2346e-05, 6.2943e-05, 6.0976e-05, 6.2525e-05, 6.0678e-05, 6.2943e-05,\n",
            "        6.1154e-05, 6.0558e-05, 6.2048e-05, 6.1810e-05, 6.0081e-05, 6.2466e-05,\n",
            "        6.2525e-05, 6.3360e-05, 6.1572e-05, 6.2525e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([6.2406e-05, 6.2883e-05, 6.1333e-05, 6.2883e-05, 6.0856e-05, 6.2883e-05,\n",
            "        6.1572e-05, 6.1095e-05, 6.3896e-05, 6.1810e-05, 6.0081e-05, 6.2883e-05,\n",
            "        6.2525e-05, 6.3360e-05, 6.2525e-05, 6.2525e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([6.2406e-05, 6.2883e-05, 6.1810e-05, 6.0260e-05, 6.1154e-05, 6.2883e-05,\n",
            "        6.1870e-05, 6.1870e-05, 6.4135e-05, 6.1810e-05, 6.0081e-05, 6.2883e-05,\n",
            "        6.2525e-05, 6.1572e-05, 6.2525e-05, 6.2525e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([6.2525e-05, 6.2883e-05, 6.2108e-05, 6.0260e-05, 6.1572e-05, 6.2883e-05,\n",
            "        6.2287e-05, 6.2346e-05, 6.4254e-05, 6.0856e-05, 6.2525e-05, 6.2883e-05,\n",
            "        6.2525e-05, 6.1572e-05, 6.2525e-05, 6.2525e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([6.2525e-05, 6.2883e-05, 6.2883e-05, 6.0260e-05, 6.2108e-05, 6.2883e-05,\n",
            "        6.2883e-05, 6.2823e-05, 6.4254e-05, 6.0856e-05, 6.2525e-05, 6.2883e-05,\n",
            "        6.2525e-05, 6.1572e-05, 6.2525e-05, 6.2525e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([6.2525e-05, 6.2883e-05, 6.2883e-05, 6.0260e-05, 6.2764e-05, 6.2883e-05,\n",
            "        6.2883e-05, 6.2883e-05, 6.1572e-05, 6.0856e-05, 6.2525e-05, 6.2883e-05,\n",
            "        6.2525e-05, 6.2525e-05, 6.2525e-05, 6.2525e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([6.2227e-05, 6.0260e-05, 6.2883e-05, 6.0260e-05, 6.2883e-05, 6.2883e-05,\n",
            "        6.2883e-05, 6.2883e-05, 6.1572e-05, 6.2525e-05, 6.2525e-05, 6.2883e-05,\n",
            "        6.2525e-05, 6.2525e-05, 6.2525e-05, 6.2525e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([6.0320e-05, 5.7578e-05, 6.0022e-05, 5.7578e-05, 6.0022e-05, 6.0022e-05,\n",
            "        6.0022e-05, 6.0022e-05, 5.7578e-05, 5.8889e-05, 6.0499e-05, 5.7578e-05,\n",
            "        6.0499e-05, 6.0499e-05, 6.0499e-05, 5.8889e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([6.0320e-05, 5.7578e-05, 5.7578e-05, 5.7578e-05, 6.0022e-05, 5.7578e-05,\n",
            "        6.0022e-05, 6.0022e-05, 5.7578e-05, 5.8889e-05, 5.8889e-05, 5.7578e-05,\n",
            "        5.8591e-05, 6.0499e-05, 6.0320e-05, 5.8889e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([6.0499e-05, 5.7578e-05, 5.7578e-05, 5.5552e-05, 6.0022e-05, 5.7578e-05,\n",
            "        6.0022e-05, 5.7578e-05, 5.7578e-05, 5.8889e-05, 5.8591e-05, 5.7578e-05,\n",
            "        5.8591e-05, 6.0499e-05, 6.0320e-05, 5.8889e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([5.8889e-05, 5.7578e-05, 5.7578e-05, 5.5552e-05, 6.0022e-05, 5.7578e-05,\n",
            "        6.0022e-05, 5.7578e-05, 5.7578e-05, 5.8889e-05, 5.8591e-05, 5.7578e-05,\n",
            "        5.8591e-05, 5.8889e-05, 6.0320e-05, 5.8889e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([5.8889e-05, 5.7578e-05, 5.5790e-05, 5.5552e-05, 6.0022e-05, 5.7578e-05,\n",
            "        5.9664e-05, 5.7578e-05, 5.5790e-05, 5.8889e-05, 5.8889e-05, 5.7578e-05,\n",
            "        5.8591e-05, 5.8889e-05, 6.0320e-05, 5.8591e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([5.8889e-05, 5.7578e-05, 5.5552e-05, 5.5552e-05, 5.7578e-05, 5.7220e-05,\n",
            "        5.7220e-05, 5.5790e-05, 5.5790e-05, 5.8889e-05, 5.8889e-05, 5.7578e-05,\n",
            "        5.8591e-05, 5.8591e-05, 6.0320e-05, 5.8591e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([5.8889e-05, 5.7220e-05, 5.5552e-05, 5.5552e-05, 5.7578e-05, 5.7220e-05,\n",
            "        5.5552e-05, 5.5790e-05, 5.5790e-05, 5.8889e-05, 5.8889e-05, 5.7578e-05,\n",
            "        5.8591e-05, 5.8591e-05, 6.0320e-05, 5.8591e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([5.8889e-05, 5.7220e-05, 5.5552e-05, 5.5492e-05, 5.5552e-05, 5.7220e-05,\n",
            "        5.5552e-05, 5.5790e-05, 5.5790e-05, 5.8591e-05, 5.8889e-05, 5.7578e-05,\n",
            "        5.8591e-05, 5.8591e-05, 5.8591e-05, 5.8591e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0005, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0004, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0005,\n",
            "        0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0005, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0004, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0005,\n",
            "        0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,\n",
            "        0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0020, 0.0020, 0.0021, 0.0021, 0.0020, 0.0020, 0.0019, 0.0020, 0.0021,\n",
            "        0.0022, 0.0021, 0.0022, 0.0021, 0.0020, 0.0022, 0.0021],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0019, 0.0020, 0.0020,\n",
            "        0.0023, 0.0020, 0.0022, 0.0021, 0.0020, 0.0021, 0.0021],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0022, 0.0021, 0.0021, 0.0022, 0.0021, 0.0021, 0.0020, 0.0021, 0.0020,\n",
            "        0.0023, 0.0021, 0.0022, 0.0021, 0.0021, 0.0022, 0.0021],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0021, 0.0021, 0.0022, 0.0021, 0.0021, 0.0022, 0.0021, 0.0021, 0.0021,\n",
            "        0.0021, 0.0021, 0.0022, 0.0021, 0.0021, 0.0022, 0.0021],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0020, 0.0020, 0.0021, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
            "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([3.4034e-05, 4.1842e-05, 5.2571e-05, 3.4690e-05, 3.1948e-05, 4.6790e-05,\n",
            "        3.2008e-05, 4.8339e-05, 4.0054e-05, 5.5015e-05, 5.7578e-05, 2.8789e-05,\n",
            "        4.9651e-05, 3.8862e-05, 4.5776e-05, 5.7459e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.5048e-05, 4.6551e-05, 5.3048e-05, 3.1054e-05, 3.3796e-05, 5.8591e-05,\n",
            "        3.4213e-05, 5.0485e-05, 4.3213e-05, 5.2333e-05, 4.8637e-05, 3.3915e-05,\n",
            "        4.8459e-05, 4.0531e-05, 5.3465e-05, 6.8784e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.7074e-05, 4.0948e-05, 6.1631e-05, 3.4809e-05, 3.6418e-05, 4.6670e-05,\n",
            "        3.2067e-05, 3.9518e-05, 4.6432e-05, 4.8041e-05, 5.8353e-05, 3.8624e-05,\n",
            "        4.6134e-05, 3.5465e-05, 3.3081e-05, 4.9591e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.9279e-05, 4.5002e-05, 4.1544e-05, 3.8743e-05, 3.3379e-05, 3.8981e-05,\n",
            "        3.7313e-05, 3.9756e-05, 4.2319e-05, 4.6968e-05, 4.3273e-05, 4.0352e-05,\n",
            "        3.8445e-05, 3.4332e-05, 3.9637e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.1769e-05, 3.7849e-05, 3.9935e-05, 3.8207e-05, 3.1769e-05, 3.7432e-05,\n",
            "        3.1948e-05, 3.8326e-05, 4.0293e-05, 3.8087e-05, 3.7730e-05, 3.5346e-05,\n",
            "        3.7074e-05, 3.4630e-05, 3.6895e-05, 3.7193e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.2187e-05, 3.2187e-05, 3.8564e-05, 3.8743e-05, 3.2246e-05, 3.5405e-05,\n",
            "        3.2306e-05, 3.8743e-05, 4.0770e-05, 3.7611e-05, 3.5644e-05, 3.5882e-05,\n",
            "        3.4332e-05, 3.5286e-05, 3.7313e-05, 3.4332e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.2127e-05, 3.2127e-05, 3.5703e-05, 3.5763e-05, 3.2127e-05, 3.5822e-05,\n",
            "        3.0398e-05, 3.5822e-05, 3.5763e-05, 3.5763e-05, 3.5822e-05, 3.5882e-05,\n",
            "        3.2485e-05, 3.3021e-05, 3.4332e-05, 3.2604e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.2008e-05, 3.1948e-05, 3.5703e-05, 3.5822e-05, 3.2187e-05, 3.5703e-05,\n",
            "        3.0577e-05, 3.5644e-05, 3.3855e-05, 3.5763e-05, 3.5703e-05, 3.4094e-05,\n",
            "        3.2663e-05, 3.3319e-05, 3.2187e-05, 3.2604e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.2008e-05, 3.2008e-05, 3.5703e-05, 3.5763e-05, 3.2187e-05, 3.5703e-05,\n",
            "        3.2008e-05, 3.5703e-05, 3.3736e-05, 3.5822e-05, 3.5703e-05, 3.2365e-05,\n",
            "        3.2783e-05, 3.3736e-05, 3.2365e-05, 3.2902e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.2127e-05, 3.2008e-05, 3.5703e-05, 3.4034e-05, 3.2246e-05, 3.5703e-05,\n",
            "        3.2067e-05, 3.5703e-05, 3.3796e-05, 3.3855e-05, 3.5703e-05, 3.0935e-05,\n",
            "        3.3021e-05, 3.3975e-05, 3.2604e-05, 3.3081e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.2127e-05, 3.2067e-05, 3.5703e-05, 3.2604e-05, 3.0875e-05, 3.5703e-05,\n",
            "        3.0756e-05, 3.5703e-05, 3.3796e-05, 3.3855e-05, 3.5703e-05, 2.9564e-05,\n",
            "        3.3140e-05, 3.3915e-05, 3.2842e-05, 3.3379e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.0816e-05, 3.0756e-05, 3.4273e-05, 3.2604e-05, 2.9504e-05, 3.5703e-05,\n",
            "        3.0994e-05, 3.4273e-05, 3.2604e-05, 3.3796e-05, 3.5644e-05, 2.9564e-05,\n",
            "        3.3498e-05, 3.4034e-05, 3.3140e-05, 3.3677e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.8968e-05, 2.8968e-05, 3.4273e-05, 3.2783e-05, 2.7716e-05, 3.4273e-05,\n",
            "        2.7537e-05, 3.4332e-05, 3.2604e-05, 3.3855e-05, 3.4273e-05, 2.7955e-05,\n",
            "        3.3796e-05, 3.3975e-05, 3.3498e-05, 3.2723e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.9087e-05, 2.8968e-05, 3.4273e-05, 3.1412e-05, 2.7835e-05, 3.4273e-05,\n",
            "        2.7597e-05, 3.4273e-05, 3.1292e-05, 3.3855e-05, 3.4273e-05, 2.7776e-05,\n",
            "        3.0816e-05, 3.4094e-05, 3.2544e-05, 3.3081e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.7716e-05, 2.8908e-05, 3.4273e-05, 3.1233e-05, 2.7835e-05, 3.4273e-05,\n",
            "        2.7716e-05, 3.4273e-05, 3.1292e-05, 3.3915e-05, 3.2604e-05, 2.7955e-05,\n",
            "        3.1412e-05, 3.4213e-05, 3.2842e-05, 3.1352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.7776e-05, 2.9027e-05, 3.4034e-05, 2.5868e-05, 2.7776e-05, 3.4392e-05,\n",
            "        2.7657e-05, 2.8551e-05, 2.5928e-05, 3.3915e-05, 3.2544e-05, 2.8014e-05,\n",
            "        3.1292e-05, 2.8491e-05, 3.2783e-05, 3.1233e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.7835e-05, 2.9147e-05, 2.8133e-05, 2.5928e-05, 2.7895e-05, 2.7061e-05,\n",
            "        2.7835e-05, 2.7061e-05, 2.5928e-05, 2.8312e-05, 2.7061e-05, 2.7955e-05,\n",
            "        2.6047e-05, 2.7239e-05, 2.7299e-05, 3.1233e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.7955e-05, 2.7835e-05, 2.6762e-05, 2.5928e-05, 2.7776e-05, 2.5868e-05,\n",
            "        2.7835e-05, 2.7239e-05, 2.5988e-05, 2.7180e-05, 2.7120e-05, 2.7955e-05,\n",
            "        2.6166e-05, 2.7418e-05, 2.7239e-05, 2.5809e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.7835e-05, 2.7835e-05, 2.6822e-05, 2.6047e-05, 2.7776e-05, 2.5928e-05,\n",
            "        2.8014e-05, 2.5988e-05, 2.6047e-05, 2.5928e-05, 2.7120e-05, 2.7955e-05,\n",
            "        2.5988e-05, 2.6166e-05, 2.7239e-05, 2.5868e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.6107e-05, 2.7895e-05, 2.5690e-05, 2.6047e-05, 2.6107e-05, 2.5749e-05,\n",
            "        2.8014e-05, 2.6047e-05, 2.6047e-05, 2.6047e-05, 2.7061e-05, 2.7955e-05,\n",
            "        2.6107e-05, 2.6286e-05, 2.7359e-05, 2.5868e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0002, 0.0001, 0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([3.4988e-05, 3.3617e-05, 2.8431e-05, 3.8028e-05, 4.3392e-05, 3.5226e-05,\n",
            "        4.4584e-05, 4.2379e-05, 3.6776e-05, 3.0398e-05, 2.7061e-05, 3.5048e-05,\n",
            "        3.7849e-05, 3.8147e-05, 4.2796e-05, 3.2425e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.3975e-05, 3.5822e-05, 2.9266e-05, 4.5955e-05, 4.5538e-05, 3.6538e-05,\n",
            "        4.3452e-05, 4.3750e-05, 4.1068e-05, 3.3200e-05, 3.1352e-05, 3.6418e-05,\n",
            "        4.1604e-05, 4.3154e-05, 4.2558e-05, 3.7432e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.8147e-05, 3.5167e-05, 3.2663e-05, 4.9591e-05, 3.7909e-05, 3.4034e-05,\n",
            "        4.0948e-05, 4.5836e-05, 4.3333e-05, 3.7253e-05, 3.4750e-05, 3.9279e-05,\n",
            "        3.5703e-05, 4.7922e-05, 4.1604e-05, 3.8385e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.6120e-05, 3.5822e-05, 3.5942e-05, 4.5955e-05, 3.8683e-05, 3.6359e-05,\n",
            "        4.0472e-05, 3.8564e-05, 4.8101e-05, 3.9935e-05, 3.5107e-05, 4.1962e-05,\n",
            "        3.9220e-05, 3.4988e-05, 3.6836e-05, 3.5346e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.1054e-05, 3.1590e-05, 3.1412e-05, 3.1114e-05, 3.2008e-05, 2.8551e-05,\n",
            "        2.9802e-05, 3.1412e-05, 3.2663e-05, 2.9445e-05, 3.0398e-05, 3.4809e-05,\n",
            "        3.1412e-05, 3.1173e-05, 3.1173e-05, 2.9802e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.1233e-05, 3.1173e-05, 2.9862e-05, 3.1054e-05, 3.2127e-05, 2.8610e-05,\n",
            "        3.1471e-05, 3.1471e-05, 3.1352e-05, 3.0458e-05, 2.8908e-05, 2.8729e-05,\n",
            "        3.1531e-05, 3.1173e-05, 3.1233e-05, 2.9862e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.1292e-05, 3.2246e-05, 2.9862e-05, 3.1054e-05, 3.2246e-05, 2.8729e-05,\n",
            "        3.1412e-05, 3.1471e-05, 3.1292e-05, 3.0398e-05, 3.0458e-05, 2.8789e-05,\n",
            "        3.1471e-05, 3.1173e-05, 3.1233e-05, 3.0100e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.1412e-05, 3.2306e-05, 2.9862e-05, 3.1233e-05, 3.1412e-05, 2.8908e-05,\n",
            "        3.1412e-05, 3.1531e-05, 3.1352e-05, 3.0458e-05, 3.0458e-05, 2.8968e-05,\n",
            "        3.1590e-05, 3.1412e-05, 3.1471e-05, 3.0220e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.1531e-05, 3.2485e-05, 2.9922e-05, 3.1233e-05, 3.1590e-05, 2.9147e-05,\n",
            "        3.1471e-05, 3.1650e-05, 3.1352e-05, 3.0577e-05, 3.0458e-05, 2.8968e-05,\n",
            "        3.1769e-05, 2.8849e-05, 3.1471e-05, 3.0398e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.1590e-05, 3.2604e-05, 2.9743e-05, 3.1054e-05, 3.1710e-05, 3.0458e-05,\n",
            "        3.1590e-05, 3.1769e-05, 2.9981e-05, 2.9504e-05, 3.0577e-05, 2.9027e-05,\n",
            "        3.0279e-05, 2.8968e-05, 3.1590e-05, 3.0458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.0279e-05, 3.1173e-05, 2.9624e-05, 3.1233e-05, 3.1829e-05, 3.0458e-05,\n",
            "        3.0100e-05, 3.1829e-05, 3.0041e-05, 2.9802e-05, 3.0577e-05, 3.0279e-05,\n",
            "        3.0339e-05, 3.0041e-05, 3.1650e-05, 3.0577e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.7537e-05, 2.8431e-05, 2.8372e-05, 2.9385e-05, 2.9922e-05, 2.9027e-05,\n",
            "        2.8372e-05, 3.1948e-05, 2.8372e-05, 2.8312e-05, 2.9504e-05, 2.8670e-05,\n",
            "        2.8729e-05, 2.8431e-05, 3.1829e-05, 2.9087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.7597e-05, 2.7776e-05, 2.8193e-05, 2.8074e-05, 2.9981e-05, 2.9087e-05,\n",
            "        2.8670e-05, 3.0220e-05, 2.7299e-05, 2.8491e-05, 2.8372e-05, 2.8849e-05,\n",
            "        2.7597e-05, 2.7239e-05, 2.8551e-05, 2.9087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.7955e-05, 2.7955e-05, 2.8312e-05, 2.6941e-05, 2.7597e-05, 2.8014e-05,\n",
            "        2.8729e-05, 2.8968e-05, 2.7537e-05, 2.8789e-05, 2.8372e-05, 2.9027e-05,\n",
            "        2.7776e-05, 2.7478e-05, 2.8849e-05, 2.9087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.7955e-05, 2.7955e-05, 2.8312e-05, 2.7061e-05, 2.7716e-05, 2.8014e-05,\n",
            "        2.7597e-05, 2.9087e-05, 2.7597e-05, 2.9027e-05, 2.8372e-05, 2.7955e-05,\n",
            "        2.7776e-05, 2.7597e-05, 2.7597e-05, 2.9087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.6822e-05, 2.7955e-05, 2.8312e-05, 2.7359e-05, 2.7955e-05, 2.8014e-05,\n",
            "        2.7776e-05, 2.9087e-05, 2.7955e-05, 2.8849e-05, 2.7657e-05, 2.7955e-05,\n",
            "        2.7776e-05, 2.7776e-05, 2.7895e-05, 2.9087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.6822e-05, 2.6822e-05, 2.7537e-05, 2.7597e-05, 2.7955e-05, 2.6822e-05,\n",
            "        2.7776e-05, 2.6822e-05, 2.7955e-05, 2.8849e-05, 2.7657e-05, 2.7955e-05,\n",
            "        2.7776e-05, 2.7955e-05, 2.7955e-05, 2.9087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.6822e-05, 2.6822e-05, 2.7597e-05, 2.7597e-05, 2.7776e-05, 2.6822e-05,\n",
            "        2.7776e-05, 2.6822e-05, 2.7955e-05, 2.7835e-05, 2.7597e-05, 2.7776e-05,\n",
            "        2.7776e-05, 2.7955e-05, 2.7955e-05, 2.9087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.6822e-05, 2.6822e-05, 2.7657e-05, 2.7895e-05, 2.7776e-05, 2.6822e-05,\n",
            "        2.7776e-05, 2.6822e-05, 2.6822e-05, 2.7835e-05, 2.7597e-05, 2.6643e-05,\n",
            "        2.7776e-05, 2.6822e-05, 2.7955e-05, 2.9087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.6822e-05, 2.6822e-05, 2.7657e-05, 2.7955e-05, 2.7776e-05, 2.6822e-05,\n",
            "        2.7776e-05, 2.6822e-05, 2.6822e-05, 2.7835e-05, 2.7657e-05, 2.6643e-05,\n",
            "        2.7776e-05, 2.6822e-05, 2.7955e-05, 2.9087e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.0233e-05, 5.1260e-05, 4.2439e-05, 4.7147e-05, 6.4135e-05, 5.3525e-05,\n",
            "        5.5075e-05, 3.9697e-05, 5.4777e-05, 4.3690e-05, 4.2379e-05, 5.1737e-05,\n",
            "        5.5432e-05, 5.2750e-05, 5.1260e-05, 3.9339e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.3379e-05, 4.8161e-05, 4.7863e-05, 4.6432e-05, 6.2943e-05, 4.7445e-05,\n",
            "        5.7399e-05, 3.3796e-05, 5.4359e-05, 4.2677e-05, 4.2498e-05, 4.2319e-05,\n",
            "        5.6803e-05, 5.1439e-05, 3.8564e-05, 3.5524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.7968e-05, 4.8459e-05, 4.5657e-05, 5.3287e-05, 5.8711e-05, 5.0843e-05,\n",
            "        4.6849e-05, 3.7968e-05, 5.3227e-05, 4.3035e-05, 4.4942e-05, 3.6836e-05,\n",
            "        4.7147e-05, 5.1081e-05, 4.2439e-05, 4.0114e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.3557e-05, 4.4525e-05, 4.1723e-05, 3.4451e-05, 5.5313e-05, 4.5836e-05,\n",
            "        3.5107e-05, 3.4630e-05, 4.9889e-05, 3.5226e-05, 3.5822e-05, 3.3438e-05,\n",
            "        4.0770e-05, 4.2200e-05, 3.8147e-05, 3.4571e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.5226e-05, 3.6299e-05, 3.6538e-05, 3.6299e-05, 3.6478e-05, 3.6597e-05,\n",
            "        3.6418e-05, 3.6061e-05, 3.8266e-05, 3.4809e-05, 3.5226e-05, 3.5942e-05,\n",
            "        3.6955e-05, 3.5763e-05, 3.4034e-05, 3.6359e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.4451e-05, 3.6776e-05, 3.6776e-05, 3.6716e-05, 3.6597e-05, 3.6955e-05,\n",
            "        3.6657e-05, 3.6180e-05, 3.8624e-05, 3.5167e-05, 3.7014e-05, 3.6120e-05,\n",
            "        3.7313e-05, 3.6061e-05, 3.4451e-05, 3.6657e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.4451e-05, 3.6776e-05, 3.6657e-05, 3.6716e-05, 3.6657e-05, 3.6955e-05,\n",
            "        3.6776e-05, 3.6359e-05, 3.8743e-05, 3.5644e-05, 3.5703e-05, 3.6299e-05,\n",
            "        3.7551e-05, 3.6240e-05, 3.4630e-05, 3.7014e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.4451e-05, 3.6776e-05, 3.6836e-05, 3.7134e-05, 3.6538e-05, 3.7074e-05,\n",
            "        3.6776e-05, 3.6716e-05, 3.6359e-05, 3.5822e-05, 3.5703e-05, 3.6538e-05,\n",
            "        3.6299e-05, 3.6299e-05, 3.4928e-05, 3.7134e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.4451e-05, 3.6716e-05, 3.6657e-05, 3.7074e-05, 3.6597e-05, 3.7193e-05,\n",
            "        3.6836e-05, 3.7074e-05, 3.6478e-05, 3.6001e-05, 3.5882e-05, 3.6955e-05,\n",
            "        3.6418e-05, 3.6418e-05, 3.5048e-05, 3.7253e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.4690e-05, 3.6776e-05, 3.6716e-05, 3.7134e-05, 3.6538e-05, 3.7134e-05,\n",
            "        3.6836e-05, 3.7432e-05, 3.6657e-05, 3.6120e-05, 3.6001e-05, 3.7193e-05,\n",
            "        3.4451e-05, 3.6418e-05, 3.5286e-05, 3.5763e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.4690e-05, 3.5226e-05, 3.5286e-05, 3.7372e-05, 3.6597e-05, 3.7193e-05,\n",
            "        3.7074e-05, 3.7789e-05, 3.6895e-05, 3.6120e-05, 3.6120e-05, 3.7372e-05,\n",
            "        3.4630e-05, 3.4511e-05, 3.3319e-05, 3.5822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.4690e-05, 3.3259e-05, 3.3259e-05, 3.5405e-05, 3.4988e-05, 3.5226e-05,\n",
            "        3.7134e-05, 3.5882e-05, 3.7074e-05, 3.4273e-05, 3.4273e-05, 3.5405e-05,\n",
            "        3.4750e-05, 3.4809e-05, 3.3379e-05, 3.4034e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.3259e-05, 3.3438e-05, 3.1948e-05, 3.5644e-05, 3.2842e-05, 3.3796e-05,\n",
            "        3.5048e-05, 3.2783e-05, 3.3498e-05, 3.4273e-05, 3.2783e-05, 3.5644e-05,\n",
            "        3.3379e-05, 3.3319e-05, 3.2008e-05, 3.2604e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.3259e-05, 3.3557e-05, 3.2127e-05, 3.5882e-05, 3.3021e-05, 3.2783e-05,\n",
            "        3.5167e-05, 3.2783e-05, 3.3796e-05, 3.4273e-05, 3.2783e-05, 3.4273e-05,\n",
            "        3.3498e-05, 3.3557e-05, 3.1710e-05, 3.2604e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.3259e-05, 3.3915e-05, 3.2425e-05, 3.4273e-05, 3.3140e-05, 3.2723e-05,\n",
            "        3.3736e-05, 3.2604e-05, 3.1888e-05, 3.4273e-05, 3.2783e-05, 3.2783e-05,\n",
            "        3.3736e-05, 3.3677e-05, 3.1829e-05, 3.2783e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.3259e-05, 3.4094e-05, 3.1114e-05, 3.2783e-05, 3.3319e-05, 3.2663e-05,\n",
            "        3.2723e-05, 3.2604e-05, 3.1888e-05, 3.4273e-05, 3.2604e-05, 3.2604e-05,\n",
            "        3.3736e-05, 3.3796e-05, 3.1948e-05, 3.2783e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.3200e-05, 3.2961e-05, 3.1352e-05, 3.1531e-05, 3.3379e-05, 3.2663e-05,\n",
            "        3.2902e-05, 3.2604e-05, 2.9922e-05, 3.4273e-05, 3.2604e-05, 3.2604e-05,\n",
            "        3.0100e-05, 3.0637e-05, 3.2067e-05, 3.2783e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.2306e-05, 3.2961e-05, 3.1352e-05, 3.1531e-05, 3.2008e-05, 3.2663e-05,\n",
            "        3.2961e-05, 3.2604e-05, 2.9922e-05, 3.2783e-05, 3.2604e-05, 3.2604e-05,\n",
            "        2.9981e-05, 3.0875e-05, 3.0458e-05, 3.2783e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.2425e-05, 3.2663e-05, 3.1352e-05, 3.1531e-05, 3.2425e-05, 3.2663e-05,\n",
            "        3.2961e-05, 3.2604e-05, 2.9922e-05, 3.2783e-05, 3.2604e-05, 3.2604e-05,\n",
            "        2.9862e-05, 3.0875e-05, 3.0756e-05, 3.2783e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.2187e-05, 3.1352e-05, 3.1352e-05, 3.1531e-05, 3.2663e-05, 3.2663e-05,\n",
            "        3.2663e-05, 3.2604e-05, 2.9922e-05, 3.2604e-05, 3.2604e-05, 3.2604e-05,\n",
            "        2.9981e-05, 3.0875e-05, 3.0875e-05, 3.1531e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([5.3465e-05, 4.4584e-05, 3.5584e-05, 4.7028e-05, 5.6148e-05, 3.9995e-05,\n",
            "        5.7280e-05, 5.2571e-05, 4.9174e-05, 5.4777e-05, 7.0393e-05, 5.3346e-05,\n",
            "        3.8922e-05, 6.2227e-05, 5.0485e-05, 5.8591e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.2810e-05, 3.7849e-05, 3.3081e-05, 3.9518e-05, 5.1200e-05, 4.1306e-05,\n",
            "        4.8637e-05, 5.1439e-05, 4.4167e-05, 5.6148e-05, 6.4790e-05, 4.1664e-05,\n",
            "        4.1187e-05, 4.9889e-05, 4.5598e-05, 4.5538e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.0948e-05, 3.8981e-05, 3.5942e-05, 4.5300e-05, 4.2737e-05, 4.4882e-05,\n",
            "        5.2333e-05, 5.1439e-05, 3.7253e-05, 4.7326e-05, 5.1916e-05, 4.3392e-05,\n",
            "        4.4286e-05, 4.6790e-05, 5.1141e-05, 4.4942e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.8981e-05, 3.7134e-05, 3.5882e-05, 4.1902e-05, 3.9041e-05, 3.6895e-05,\n",
            "        4.2498e-05, 4.6432e-05, 3.5763e-05, 3.9577e-05, 4.3988e-05, 3.9458e-05,\n",
            "        4.2498e-05, 3.6955e-05, 4.8995e-05, 4.0889e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.9518e-05, 3.8147e-05, 3.7968e-05, 3.9041e-05, 4.0770e-05, 3.9160e-05,\n",
            "        4.4167e-05, 4.1187e-05, 3.6955e-05, 3.8147e-05, 3.9816e-05, 4.0412e-05,\n",
            "        3.9160e-05, 3.7313e-05, 3.9101e-05, 3.7789e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.9577e-05, 3.9756e-05, 3.7968e-05, 4.0770e-05, 4.1008e-05, 3.9160e-05,\n",
            "        4.1068e-05, 3.9697e-05, 3.7014e-05, 3.8505e-05, 4.0174e-05, 4.0412e-05,\n",
            "        3.9160e-05, 3.9518e-05, 3.9399e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.9697e-05, 3.9756e-05, 3.9160e-05, 4.0174e-05, 3.9697e-05, 3.9160e-05,\n",
            "        3.9697e-05, 3.9697e-05, 3.9101e-05, 3.8564e-05, 4.0531e-05, 4.0412e-05,\n",
            "        3.9160e-05, 3.9995e-05, 3.9697e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.9697e-05, 3.9160e-05, 3.9160e-05, 4.0174e-05, 3.9697e-05, 3.9160e-05,\n",
            "        3.9697e-05, 3.9697e-05, 3.9160e-05, 3.9697e-05, 3.8981e-05, 4.0412e-05,\n",
            "        3.9160e-05, 3.9279e-05, 3.9697e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.9697e-05, 3.9160e-05, 3.9160e-05, 3.9160e-05, 3.9697e-05, 3.9160e-05,\n",
            "        3.9697e-05, 3.9697e-05, 3.8922e-05, 3.9697e-05, 3.9279e-05, 3.9160e-05,\n",
            "        3.9220e-05, 3.9458e-05, 3.9697e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.9697e-05, 3.9160e-05, 3.9160e-05, 3.9160e-05, 3.9697e-05, 3.9160e-05,\n",
            "        3.9697e-05, 3.9697e-05, 3.9041e-05, 3.9697e-05, 3.9577e-05, 3.9160e-05,\n",
            "        3.7968e-05, 3.9458e-05, 3.9697e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.9697e-05, 3.9160e-05, 3.9160e-05, 3.9160e-05, 3.9697e-05, 3.9160e-05,\n",
            "        3.9697e-05, 3.9697e-05, 3.8922e-05, 3.9697e-05, 3.9697e-05, 3.9160e-05,\n",
            "        3.7968e-05, 3.9577e-05, 3.9697e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.9697e-05, 3.9160e-05, 3.8266e-05, 3.9160e-05, 3.9697e-05, 3.9160e-05,\n",
            "        3.9697e-05, 3.9697e-05, 3.7730e-05, 3.9697e-05, 3.9697e-05, 3.9160e-05,\n",
            "        3.8087e-05, 3.9637e-05, 3.8028e-05, 3.6299e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.8028e-05, 3.7014e-05, 3.7014e-05, 3.7014e-05, 3.8028e-05, 3.7968e-05,\n",
            "        3.8028e-05, 3.8028e-05, 3.6418e-05, 3.8028e-05, 3.8028e-05, 3.7968e-05,\n",
            "        3.6836e-05, 3.8028e-05, 3.8028e-05, 3.6299e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.8028e-05, 3.7014e-05, 3.7014e-05, 3.7014e-05, 3.8028e-05, 3.7968e-05,\n",
            "        3.8028e-05, 3.6478e-05, 3.6359e-05, 3.8028e-05, 3.8028e-05, 3.7968e-05,\n",
            "        3.6895e-05, 3.6478e-05, 3.8028e-05, 3.6299e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.6478e-05, 3.7014e-05, 3.7014e-05, 3.7014e-05, 3.8028e-05, 3.7789e-05,\n",
            "        3.6299e-05, 3.6478e-05, 3.6597e-05, 3.8028e-05, 3.8028e-05, 3.7014e-05,\n",
            "        3.7014e-05, 3.6478e-05, 3.6836e-05, 3.6478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.6478e-05, 3.7014e-05, 3.7014e-05, 3.7014e-05, 3.6478e-05, 3.6895e-05,\n",
            "        3.6299e-05, 3.6478e-05, 3.6538e-05, 3.8028e-05, 3.8028e-05, 3.7014e-05,\n",
            "        3.7014e-05, 3.5346e-05, 3.6836e-05, 3.6478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.6299e-05, 3.7014e-05, 3.7014e-05, 3.6895e-05, 3.6478e-05, 3.6895e-05,\n",
            "        3.6299e-05, 3.5346e-05, 3.6597e-05, 3.6836e-05, 3.8028e-05, 3.7014e-05,\n",
            "        3.7014e-05, 3.5346e-05, 3.5346e-05, 3.6478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.6299e-05, 3.6895e-05, 3.6240e-05, 3.6895e-05, 3.5346e-05, 3.6895e-05,\n",
            "        3.6299e-05, 3.5346e-05, 3.6836e-05, 3.6836e-05, 3.8028e-05, 3.6240e-05,\n",
            "        3.7014e-05, 3.5346e-05, 3.5346e-05, 3.6478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.6299e-05, 3.6895e-05, 3.6061e-05, 3.6895e-05, 3.5346e-05, 3.6895e-05,\n",
            "        3.6299e-05, 3.5346e-05, 3.6955e-05, 3.5346e-05, 3.8028e-05, 3.6061e-05,\n",
            "        3.7014e-05, 3.5346e-05, 3.5346e-05, 3.6478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.6299e-05, 3.6895e-05, 3.6061e-05, 3.6895e-05, 3.5346e-05, 3.6895e-05,\n",
            "        3.6299e-05, 3.5346e-05, 3.7014e-05, 3.5346e-05, 3.6478e-05, 3.6061e-05,\n",
            "        3.7014e-05, 3.5107e-05, 3.5346e-05, 3.5346e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
            "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([5.2333e-05, 5.7995e-05, 5.8830e-05, 6.1989e-05, 6.3419e-05, 5.6803e-05,\n",
            "        6.4254e-05, 5.1022e-05, 5.3465e-05, 7.5579e-05, 5.8055e-05, 4.9651e-05,\n",
            "        5.8770e-05, 6.0022e-05, 8.2314e-05, 6.3717e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.8532e-05, 5.4240e-05, 5.0545e-05, 5.3883e-05, 5.2214e-05, 4.4703e-05,\n",
            "        7.5996e-05, 5.2691e-05, 5.7220e-05, 5.4896e-05, 5.2214e-05, 5.2750e-05,\n",
            "        6.5982e-05, 5.9605e-05, 5.5492e-05, 5.3525e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([5.1081e-05, 6.2346e-05, 5.0068e-05, 5.2094e-05, 5.6624e-05, 4.9353e-05,\n",
            "        6.0499e-05, 6.0141e-05, 6.2525e-05, 6.2704e-05, 5.6386e-05, 5.7697e-05,\n",
            "        6.9678e-05, 5.8711e-05, 5.7340e-05, 5.0545e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([5.4657e-05, 5.8234e-05, 5.2750e-05, 5.5611e-05, 5.4121e-05, 5.1975e-05,\n",
            "        5.3108e-05, 5.3942e-05, 5.7936e-05, 5.4002e-05, 5.3346e-05, 5.5313e-05,\n",
            "        5.4181e-05, 5.6982e-05, 6.0380e-05, 5.4240e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([5.0128e-05, 4.8101e-05, 4.6670e-05, 4.9353e-05, 4.9233e-05, 4.7565e-05,\n",
            "        4.7743e-05, 4.8637e-05, 4.7922e-05, 4.9472e-05, 4.9174e-05, 4.9174e-05,\n",
            "        4.8101e-05, 5.0664e-05, 4.6194e-05, 4.8220e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.8161e-05, 4.6492e-05, 4.6849e-05, 4.7445e-05, 4.7326e-05, 4.8637e-05,\n",
            "        4.6134e-05, 4.6730e-05, 4.6015e-05, 4.8101e-05, 4.8161e-05, 4.8161e-05,\n",
            "        4.6372e-05, 4.7982e-05, 4.4942e-05, 4.6372e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([4.8161e-05, 4.6670e-05, 4.5121e-05, 4.7445e-05, 4.6074e-05, 4.6730e-05,\n",
            "        4.5598e-05, 4.6730e-05, 4.6372e-05, 4.8041e-05, 4.7445e-05, 4.8161e-05,\n",
            "        4.6313e-05, 4.6730e-05, 4.4882e-05, 4.8161e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([4.6730e-05, 4.6670e-05, 4.5896e-05, 4.7445e-05, 4.6074e-05, 4.6730e-05,\n",
            "        4.6492e-05, 4.6730e-05, 4.6372e-05, 4.6372e-05, 4.7445e-05, 4.6730e-05,\n",
            "        4.6372e-05, 4.6730e-05, 4.4882e-05, 4.6730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([4.6730e-05, 4.6670e-05, 4.6730e-05, 4.6730e-05, 4.6074e-05, 4.6730e-05,\n",
            "        4.6670e-05, 4.6730e-05, 4.6313e-05, 4.6670e-05, 4.7445e-05, 4.6730e-05,\n",
            "        4.6313e-05, 4.6730e-05, 4.4882e-05, 4.6730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([4.6730e-05, 4.6670e-05, 4.6730e-05, 4.6730e-05, 4.6730e-05, 4.6730e-05,\n",
            "        4.6730e-05, 4.6730e-05, 4.6372e-05, 4.6730e-05, 4.6730e-05, 4.6730e-05,\n",
            "        4.6372e-05, 4.6730e-05, 4.4942e-05, 4.6730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([4.6730e-05, 4.6670e-05, 4.6730e-05, 4.6730e-05, 4.6730e-05, 4.6730e-05,\n",
            "        4.6730e-05, 4.6730e-05, 4.6492e-05, 4.6730e-05, 4.6730e-05, 4.6730e-05,\n",
            "        4.6372e-05, 4.6730e-05, 4.4942e-05, 4.6730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([4.6730e-05, 4.4882e-05, 4.6730e-05, 4.6730e-05, 4.6730e-05, 4.6730e-05,\n",
            "        4.6730e-05, 4.6730e-05, 4.6492e-05, 4.6730e-05, 4.6730e-05, 4.6730e-05,\n",
            "        4.6313e-05, 4.6730e-05, 4.4882e-05, 4.6730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([4.6730e-05, 4.2796e-05, 4.5478e-05, 4.6730e-05, 4.6730e-05, 4.5478e-05,\n",
            "        4.6730e-05, 4.6730e-05, 4.4525e-05, 4.6730e-05, 4.6730e-05, 4.5478e-05,\n",
            "        4.6194e-05, 4.6730e-05, 4.4882e-05, 4.6730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([4.5478e-05, 4.2796e-05, 4.5478e-05, 4.5478e-05, 4.5478e-05, 4.5478e-05,\n",
            "        4.5478e-05, 4.5240e-05, 4.4525e-05, 4.5478e-05, 4.5478e-05, 4.5478e-05,\n",
            "        4.4465e-05, 4.5478e-05, 4.2975e-05, 4.5478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([4.5478e-05, 4.2617e-05, 4.5478e-05, 4.5478e-05, 4.4405e-05, 4.5478e-05,\n",
            "        4.5478e-05, 4.5240e-05, 4.4525e-05, 4.5478e-05, 4.5478e-05, 4.5478e-05,\n",
            "        4.4584e-05, 4.5478e-05, 4.3035e-05, 4.5478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([4.5478e-05, 4.2617e-05, 4.5478e-05, 4.5478e-05, 4.4405e-05, 4.4405e-05,\n",
            "        4.5478e-05, 4.5240e-05, 4.2796e-05, 4.5478e-05, 4.5478e-05, 4.4405e-05,\n",
            "        4.4644e-05, 4.5240e-05, 4.3094e-05, 4.5478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([4.5240e-05, 4.2617e-05, 4.4227e-05, 4.5478e-05, 4.4405e-05, 4.4405e-05,\n",
            "        4.4405e-05, 4.5240e-05, 4.2796e-05, 4.5478e-05, 4.4405e-05, 4.4405e-05,\n",
            "        4.4763e-05, 4.5240e-05, 4.3094e-05, 4.5478e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([4.5240e-05, 4.2617e-05, 4.4227e-05, 4.5478e-05, 4.4405e-05, 4.4227e-05,\n",
            "        4.4405e-05, 4.5240e-05, 4.2796e-05, 4.5240e-05, 4.4405e-05, 4.4227e-05,\n",
            "        4.4286e-05, 4.5240e-05, 4.3213e-05, 4.5240e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([4.4227e-05, 4.2617e-05, 4.4227e-05, 4.5240e-05, 4.4405e-05, 4.4227e-05,\n",
            "        4.4405e-05, 4.5240e-05, 4.2796e-05, 4.5240e-05, 4.4405e-05, 4.4227e-05,\n",
            "        4.4405e-05, 4.5240e-05, 4.2319e-05, 4.5240e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([4.4227e-05, 4.2617e-05, 4.4227e-05, 4.4227e-05, 4.4405e-05, 4.4227e-05,\n",
            "        4.4405e-05, 4.5240e-05, 4.2796e-05, 4.5240e-05, 4.4405e-05, 4.4227e-05,\n",
            "        4.4405e-05, 4.4227e-05, 4.2319e-05, 4.5240e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([6.4313e-05, 5.0962e-05, 5.6565e-05, 4.1723e-05, 6.2823e-05, 4.8101e-05,\n",
            "        3.4273e-05, 4.8339e-05, 4.7266e-05, 5.9605e-05, 4.9233e-05, 5.4300e-05,\n",
            "        4.8399e-05, 6.2108e-05, 5.5909e-05, 5.2035e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([6.5804e-05, 4.7147e-05, 4.6134e-05, 4.0710e-05, 7.5459e-05, 5.2750e-05,\n",
            "        3.9518e-05, 5.0604e-05, 4.8637e-05, 5.0962e-05, 5.3823e-05, 6.0320e-05,\n",
            "        4.5776e-05, 6.8545e-05, 5.6088e-05, 6.1512e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([6.4552e-05, 5.5432e-05, 4.4525e-05, 4.6551e-05, 6.2823e-05, 4.6849e-05,\n",
            "        4.5717e-05, 5.3406e-05, 5.5611e-05, 5.8413e-05, 4.5180e-05, 5.5909e-05,\n",
            "        5.2989e-05, 5.0604e-05, 5.8234e-05, 6.2048e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([5.2452e-05, 4.7982e-05, 4.0531e-05, 4.7266e-05, 4.7147e-05, 4.8459e-05,\n",
            "        4.8161e-05, 4.6730e-05, 5.4717e-05, 4.9233e-05, 4.3392e-05, 4.3809e-05,\n",
            "        4.5419e-05, 4.3690e-05, 4.5061e-05, 5.3942e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([4.1306e-05, 4.1306e-05, 3.9876e-05, 4.1842e-05, 4.3154e-05, 4.4167e-05,\n",
            "        4.2081e-05, 4.1068e-05, 4.1902e-05, 4.2021e-05, 3.9995e-05, 3.8385e-05,\n",
            "        4.0829e-05, 3.8505e-05, 4.0412e-05, 4.2021e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.1425e-05, 4.0770e-05, 4.0412e-05, 4.2081e-05, 4.3690e-05, 4.2975e-05,\n",
            "        4.2081e-05, 4.0352e-05, 4.1962e-05, 4.2260e-05, 3.9995e-05, 4.1723e-05,\n",
            "        4.0829e-05, 3.8624e-05, 4.0114e-05, 4.2379e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([4.1902e-05, 4.1127e-05, 4.1127e-05, 4.2081e-05, 4.1962e-05, 4.3213e-05,\n",
            "        4.2081e-05, 4.0650e-05, 4.1962e-05, 4.3333e-05, 4.0174e-05, 3.8862e-05,\n",
            "        4.0829e-05, 3.8743e-05, 4.1127e-05, 4.2379e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([4.1902e-05, 4.1544e-05, 4.1366e-05, 4.2081e-05, 4.2021e-05, 4.3213e-05,\n",
            "        4.2081e-05, 4.1068e-05, 4.1962e-05, 4.3333e-05, 4.2140e-05, 3.9101e-05,\n",
            "        4.0829e-05, 4.2379e-05, 4.1246e-05, 4.2617e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([4.0293e-05, 4.1664e-05, 4.1366e-05, 4.2081e-05, 4.0352e-05, 4.3213e-05,\n",
            "        4.2081e-05, 4.2081e-05, 4.2081e-05, 4.3631e-05, 4.1842e-05, 3.9458e-05,\n",
            "        4.2081e-05, 4.2439e-05, 4.1544e-05, 4.2856e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([4.0352e-05, 4.1664e-05, 4.1366e-05, 4.2081e-05, 4.0293e-05, 4.3213e-05,\n",
            "        4.2081e-05, 4.2081e-05, 4.2081e-05, 4.3929e-05, 4.1842e-05, 3.9697e-05,\n",
            "        4.2081e-05, 4.2856e-05, 4.1664e-05, 4.2975e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([4.0472e-05, 4.1664e-05, 4.1366e-05, 4.2081e-05, 4.0531e-05, 4.3213e-05,\n",
            "        4.2081e-05, 4.2081e-05, 4.2081e-05, 4.4227e-05, 4.1842e-05, 4.1366e-05,\n",
            "        4.1842e-05, 4.0889e-05, 4.2081e-05, 4.3213e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([4.0770e-05, 4.3213e-05, 4.1366e-05, 4.2081e-05, 4.0829e-05, 4.1664e-05,\n",
            "        4.0829e-05, 4.2081e-05, 4.2140e-05, 4.3213e-05, 4.1842e-05, 4.1366e-05,\n",
            "        4.1842e-05, 4.1068e-05, 4.2081e-05, 4.3213e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.8922e-05, 3.9279e-05, 3.9279e-05, 3.9399e-05, 3.8862e-05, 3.9637e-05,\n",
            "        3.9279e-05, 4.0650e-05, 4.0531e-05, 4.0948e-05, 4.0472e-05, 3.9279e-05,\n",
            "        3.9279e-05, 3.9279e-05, 4.0650e-05, 4.0948e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.9220e-05, 3.7968e-05, 3.9279e-05, 3.9399e-05, 3.9339e-05, 3.9637e-05,\n",
            "        3.9279e-05, 4.0650e-05, 4.0412e-05, 3.7968e-05, 4.0472e-05, 3.9279e-05,\n",
            "        3.8207e-05, 3.9279e-05, 4.0650e-05, 3.9637e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.9518e-05, 3.7968e-05, 3.9279e-05, 3.9399e-05, 3.9518e-05, 3.7968e-05,\n",
            "        3.9279e-05, 4.0650e-05, 4.0531e-05, 3.7968e-05, 3.9279e-05, 3.9279e-05,\n",
            "        3.8207e-05, 3.9279e-05, 4.0650e-05, 3.9637e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.9518e-05, 3.7968e-05, 3.9279e-05, 3.9399e-05, 3.9518e-05, 3.7968e-05,\n",
            "        3.9279e-05, 4.0650e-05, 4.0650e-05, 3.7968e-05, 3.9399e-05, 3.9279e-05,\n",
            "        3.8385e-05, 3.9279e-05, 4.0650e-05, 3.9637e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.9518e-05, 3.7730e-05, 3.7968e-05, 3.9399e-05, 3.9518e-05, 3.7968e-05,\n",
            "        3.8207e-05, 4.0472e-05, 4.0770e-05, 3.7968e-05, 3.9399e-05, 3.9279e-05,\n",
            "        3.8385e-05, 3.7968e-05, 4.0472e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.8266e-05, 3.7730e-05, 3.7730e-05, 3.9279e-05, 3.8087e-05, 3.7730e-05,\n",
            "        3.8207e-05, 3.9279e-05, 4.0770e-05, 3.7968e-05, 3.9399e-05, 3.9279e-05,\n",
            "        3.8385e-05, 3.7968e-05, 4.0472e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.6716e-05, 3.7730e-05, 3.7730e-05, 3.9279e-05, 3.8087e-05, 3.7730e-05,\n",
            "        3.8207e-05, 3.8207e-05, 3.9399e-05, 3.7968e-05, 3.9399e-05, 3.9279e-05,\n",
            "        3.8385e-05, 3.7968e-05, 3.9518e-05, 3.7730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.6418e-05, 3.7730e-05, 3.7730e-05, 3.9279e-05, 3.8087e-05, 3.7730e-05,\n",
            "        3.8207e-05, 3.8207e-05, 3.7730e-05, 3.7968e-05, 3.9399e-05, 3.9041e-05,\n",
            "        3.8385e-05, 3.7968e-05, 3.8207e-05, 3.7730e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([8.6963e-05, 8.5115e-05, 6.0260e-05, 6.9916e-05, 6.4313e-05, 5.6505e-05,\n",
            "        7.9215e-05, 6.2287e-05, 6.4790e-05, 6.1452e-05, 6.7651e-05, 6.4015e-05,\n",
            "        8.2135e-05, 7.2241e-05, 7.0453e-05, 6.1631e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([8.5056e-05, 8.2433e-05, 6.4254e-05, 7.3552e-05, 6.2704e-05, 5.8472e-05,\n",
            "        6.9201e-05, 6.2823e-05, 6.8963e-05, 5.6386e-05, 6.8843e-05, 6.3837e-05,\n",
            "        6.8307e-05, 6.6280e-05, 6.0976e-05, 5.6982e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([8.4102e-05, 7.5281e-05, 5.8353e-05, 6.3658e-05, 6.9618e-05, 6.5863e-05,\n",
            "        6.8486e-05, 6.6221e-05, 6.1750e-05, 5.8889e-05, 5.8949e-05, 6.6698e-05,\n",
            "        7.4208e-05, 5.7817e-05, 6.8963e-05, 6.2227e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([6.0976e-05, 6.8009e-05, 5.3942e-05, 6.0678e-05, 6.3419e-05, 6.1214e-05,\n",
            "        6.5386e-05, 6.4135e-05, 6.5327e-05, 5.2333e-05, 5.9664e-05, 6.3479e-05,\n",
            "        6.3062e-05, 5.9783e-05, 7.0035e-05, 6.1631e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([5.3525e-05, 5.5075e-05, 5.3942e-05, 5.4419e-05, 5.6386e-05, 5.5254e-05,\n",
            "        5.9724e-05, 5.5671e-05, 5.6326e-05, 5.3346e-05, 5.5194e-05, 5.6386e-05,\n",
            "        5.3465e-05, 5.4300e-05, 5.4955e-05, 5.4955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([5.1618e-05, 5.3406e-05, 5.3942e-05, 5.4955e-05, 5.6386e-05, 5.4955e-05,\n",
            "        5.4419e-05, 5.5671e-05, 5.4955e-05, 5.4955e-05, 5.3644e-05, 5.5671e-05,\n",
            "        5.3585e-05, 5.5432e-05, 5.4955e-05, 5.4955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([5.1677e-05, 5.1796e-05, 5.3942e-05, 5.4955e-05, 5.4955e-05, 5.4955e-05,\n",
            "        5.3048e-05, 5.5671e-05, 5.4955e-05, 5.4955e-05, 5.3763e-05, 5.5671e-05,\n",
            "        5.2094e-05, 5.5432e-05, 5.4955e-05, 5.4955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([5.1737e-05, 5.1737e-05, 5.2094e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05,\n",
            "        5.3167e-05, 5.3823e-05, 5.3167e-05, 5.4955e-05, 5.2452e-05, 5.3823e-05,\n",
            "        5.2452e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([5.1856e-05, 5.1737e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05,\n",
            "        5.3167e-05, 5.3823e-05, 5.3167e-05, 5.3167e-05, 5.2810e-05, 5.3823e-05,\n",
            "        5.2691e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([5.1737e-05, 5.1737e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05,\n",
            "        5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3823e-05,\n",
            "        5.3108e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([5.1737e-05, 5.1737e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05,\n",
            "        5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05,\n",
            "        5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([5.1737e-05, 5.1737e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05,\n",
            "        5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05,\n",
            "        5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([5.1737e-05, 4.9591e-05, 5.1796e-05, 5.3167e-05, 5.3167e-05, 5.1796e-05,\n",
            "        5.3167e-05, 5.3167e-05, 5.3167e-05, 5.1558e-05, 5.3167e-05, 5.3167e-05,\n",
            "        5.3167e-05, 5.3167e-05, 5.3167e-05, 5.3167e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([4.9591e-05, 4.9591e-05, 5.1796e-05, 5.1558e-05, 5.1796e-05, 5.1796e-05,\n",
            "        5.1796e-05, 5.1796e-05, 5.1796e-05, 5.1558e-05, 5.1796e-05, 5.1796e-05,\n",
            "        5.1796e-05, 5.1796e-05, 5.1796e-05, 5.1796e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([4.9591e-05, 4.9591e-05, 5.1796e-05, 5.1558e-05, 5.1796e-05, 5.1796e-05,\n",
            "        5.1796e-05, 5.1796e-05, 5.1796e-05, 5.1558e-05, 5.1796e-05, 5.1796e-05,\n",
            "        5.1558e-05, 5.1558e-05, 5.1796e-05, 5.1796e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([4.9591e-05, 4.9591e-05, 5.1796e-05, 5.1558e-05, 5.1796e-05, 5.1796e-05,\n",
            "        5.1796e-05, 5.1796e-05, 5.1796e-05, 5.0545e-05, 5.1796e-05, 5.1796e-05,\n",
            "        5.1558e-05, 5.1558e-05, 5.1796e-05, 5.1796e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([4.9293e-05, 4.9651e-05, 5.1796e-05, 5.1558e-05, 5.1796e-05, 5.1558e-05,\n",
            "        5.1796e-05, 5.1796e-05, 5.1796e-05, 5.0545e-05, 5.1796e-05, 5.0664e-05,\n",
            "        5.1558e-05, 5.1558e-05, 5.1796e-05, 5.1796e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([4.9412e-05, 4.9829e-05, 5.1796e-05, 5.1558e-05, 5.0664e-05, 5.1558e-05,\n",
            "        5.1796e-05, 5.1796e-05, 5.1796e-05, 5.0545e-05, 5.0664e-05, 5.0664e-05,\n",
            "        5.1558e-05, 5.1558e-05, 5.1558e-05, 5.0664e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([4.9412e-05, 4.9651e-05, 5.0664e-05, 5.1558e-05, 5.0664e-05, 5.1558e-05,\n",
            "        5.1796e-05, 5.1796e-05, 5.1796e-05, 5.0545e-05, 5.0664e-05, 5.0664e-05,\n",
            "        5.1558e-05, 5.1558e-05, 5.1558e-05, 5.0664e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([4.8041e-05, 4.8220e-05, 5.0664e-05, 5.1558e-05, 5.0664e-05, 5.1558e-05,\n",
            "        5.1796e-05, 5.1796e-05, 5.1796e-05, 5.0545e-05, 5.0664e-05, 5.0664e-05,\n",
            "        5.1796e-05, 5.1558e-05, 5.1558e-05, 5.0664e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0002, 0.0002, 0.0001,\n",
            "        0.0001, 0.0001, 0.0002, 0.0001, 0.0002, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0002, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "dided\n",
            "time\n",
            "[3, 0, 0, 0, 3, 0, 0, 0, 3, 3, 3, 0, 3, 3, 3, 0, 0, 0, 0, 0, 3, 9, 3, 3, 0, 0, 0, 0, 3, 9, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 3, 0, 0, 0, 0, 0, 9, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 9, 3, 3, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 9, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 9, 9, 3, 3, 0, 0, 0, 0, 3, 9, 3, 3, 3, 9, 3, 3, 9, 9, 9, 3, 0, 0, 0, 0, 9, 9, 3, 3, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 9, 3, 3, 3, 9, 9, 9, 3, 0, 0, 0, 0, 3, 3, 3, 0, 3, 3, 3, 0, 3, 0, 0, 0, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 9, 3, 3, 3, 3, 3, 0, 9, 9, 9, 3, 3, 9, 3, 3, 3, 3, 3, 3, 9, 9, 9, 3, 3, 9, 3, 3, 9, 9, 3, 3, 0, 0, 0, 0, 3, 0, 0, 0, 9, 9, 3, 3, 3, 3, 3, 0, 9, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 0, 9, 9, 3, 3, 0, 0, 0, 0, 3, 3, 3, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 9, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 3, 3, 3, 3, 9, 3, 3, 3, 0, 0, 0, 0, 3, 3, 3, 0, 3, 3, 3, 0]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "\n",
        "env = TimeLimit(env, max_episode_steps=600)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6], [1, 256]\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # out = agent(state, zip(*out)[k:])\n",
        "            act = agent(state, k)\n",
        "            # act = zip(*out)[0].cpu()[0,:k].tolist()\n",
        "            # act = out[0].cpu()[0,:k].tolist()\n",
        "            # h0=lh0[k-1].unsqueeze(0)\n",
        "            # , lx, lz\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"dided\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9cm6KjvBrnNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d880886-980b-4c43-94c7-ba033f1d5f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor([9.8109e-05, 1.0461e-04, 9.7632e-05, 1.1832e-04, 1.0216e-04, 1.1384e-04,\n",
            "        9.9003e-05, 1.2565e-04, 1.2302e-04, 1.1230e-04, 9.7692e-05, 1.2314e-04,\n",
            "        1.0508e-04, 1.0544e-04, 8.5950e-05, 1.1492e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.0622e-04, 8.9765e-05, 7.8559e-05, 1.3161e-04, 1.0031e-04, 1.1331e-04,\n",
            "        1.0818e-04, 1.1611e-04, 8.6427e-05, 1.1539e-04, 1.0103e-04, 1.4377e-04,\n",
            "        9.8884e-05, 9.4652e-05, 9.0897e-05, 9.8407e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([9.6381e-05, 8.7917e-05, 8.8274e-05, 1.0061e-04, 9.9480e-05, 1.1754e-04,\n",
            "        1.1528e-04, 1.2243e-04, 9.2566e-05, 1.0055e-04, 9.7930e-05, 1.1325e-04,\n",
            "        8.9526e-05, 9.9301e-05, 1.0329e-04, 1.0109e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([9.0241e-05, 8.4758e-05, 8.5890e-05, 9.1076e-05, 8.5890e-05, 8.8155e-05,\n",
            "        9.3997e-05, 9.0361e-05, 8.9943e-05, 9.3818e-05, 8.7440e-05, 8.7321e-05,\n",
            "        8.7976e-05, 8.7440e-05, 9.0718e-05, 8.9884e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([8.6725e-05, 8.8096e-05, 8.8513e-05, 8.6963e-05, 8.6963e-05, 8.4996e-05,\n",
            "        8.5950e-05, 8.4698e-05, 9.2924e-05, 8.6010e-05, 8.6963e-05, 8.3864e-05,\n",
            "        8.9526e-05, 8.7440e-05, 8.8215e-05, 8.9169e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([8.6725e-05, 8.4519e-05, 8.4996e-05, 8.6963e-05, 8.6963e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.6427e-05, 8.7142e-05, 8.4996e-05, 8.6963e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.3148e-05, 8.4996e-05, 8.4996e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([8.6725e-05, 8.6010e-05, 8.4996e-05, 8.6963e-05, 8.4996e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.6427e-05, 8.4996e-05, 8.4996e-05, 8.6963e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.3148e-05, 8.4996e-05, 8.4996e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([8.3983e-05, 8.4996e-05, 8.4996e-05, 8.6963e-05, 8.4996e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.6427e-05, 8.4996e-05, 8.4996e-05, 8.5950e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.3148e-05, 8.4996e-05, 8.4996e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([8.3983e-05, 8.4996e-05, 8.4996e-05, 8.5950e-05, 8.4996e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.5950e-05, 8.4996e-05, 8.4996e-05, 8.5950e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.3148e-05, 8.4996e-05, 8.4996e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([8.3983e-05, 8.4996e-05, 8.4996e-05, 8.5950e-05, 8.4996e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.5950e-05, 8.4996e-05, 8.4996e-05, 8.5950e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([8.4519e-05, 8.4996e-05, 8.4996e-05, 8.5950e-05, 8.4996e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.5950e-05, 8.4996e-05, 8.4996e-05, 8.5950e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([8.3506e-05, 8.4996e-05, 8.4996e-05, 8.5950e-05, 8.4996e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.5950e-05, 8.4996e-05, 8.4996e-05, 8.5950e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.2552e-05, 8.4996e-05, 8.4996e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([8.2552e-05, 8.2552e-05, 8.2254e-05, 8.3506e-05, 8.2552e-05, 8.4996e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2552e-05, 8.3506e-05, 8.4996e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2552e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([8.2552e-05, 8.2552e-05, 8.2254e-05, 8.3506e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2552e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2552e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([8.2552e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2552e-05,\n",
            "        8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2254e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([8.0824e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2552e-05,\n",
            "        8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2254e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([8.0824e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2254e-05, 8.2254e-05, 8.2552e-05, 8.2254e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([8.0824e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2254e-05, 8.0585e-05, 8.0824e-05, 8.0585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([8.0824e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2254e-05, 8.0585e-05, 8.0824e-05, 8.0585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([8.0824e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2552e-05, 8.2552e-05, 8.2552e-05, 8.2254e-05, 8.2552e-05, 8.2254e-05,\n",
            "        8.2254e-05, 8.0585e-05, 8.0585e-05, 8.0585e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.1098e-04, 8.6606e-05, 1.0347e-04, 1.0836e-04, 7.9453e-05, 9.8407e-05,\n",
            "        8.8871e-05, 1.0920e-04, 1.1533e-04, 9.2208e-05, 1.1253e-04, 1.0377e-04,\n",
            "        9.6262e-05, 1.0192e-04, 7.8499e-05, 9.5308e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.2082e-04, 8.3148e-05, 1.0359e-04, 9.3997e-05, 9.0003e-05, 8.9407e-05,\n",
            "        9.4593e-05, 1.0073e-04, 1.2875e-04, 9.7990e-05, 9.5010e-05, 1.2016e-04,\n",
            "        9.3162e-05, 8.2970e-05, 7.9513e-05, 9.5963e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([9.5069e-05, 8.5056e-05, 1.1450e-04, 1.0556e-04, 9.9361e-05, 9.9063e-05,\n",
            "        9.8884e-05, 9.4354e-05, 1.0860e-04, 1.0419e-04, 1.0824e-04, 1.2118e-04,\n",
            "        8.9288e-05, 9.0539e-05, 8.8394e-05, 1.0681e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([9.6262e-05, 9.2149e-05, 9.7990e-05, 1.0538e-04, 1.0186e-04, 1.0037e-04,\n",
            "        9.3281e-05, 9.5844e-05, 9.6738e-05, 1.1086e-04, 1.0616e-04, 1.0741e-04,\n",
            "        9.3341e-05, 9.5487e-05, 9.3579e-05, 9.7215e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([8.5473e-05, 8.4400e-05, 8.7857e-05, 8.6784e-05, 8.6784e-05, 8.5652e-05,\n",
            "        8.5771e-05, 8.5354e-05, 8.5354e-05, 8.5115e-05, 8.6367e-05, 8.7857e-05,\n",
            "        8.4877e-05, 8.4400e-05, 8.5771e-05, 8.5771e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([8.2314e-05, 8.4400e-05, 8.4281e-05, 8.2314e-05, 8.6784e-05, 8.6427e-05,\n",
            "        8.5771e-05, 8.5354e-05, 8.0943e-05, 8.5115e-05, 8.0943e-05, 8.4221e-05,\n",
            "        8.5115e-05, 8.5771e-05, 8.5771e-05, 8.2314e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([8.2314e-05, 8.0943e-05, 8.4281e-05, 8.2314e-05, 8.3208e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.3208e-05, 8.0943e-05, 8.2314e-05, 8.0943e-05, 8.2970e-05,\n",
            "        8.1539e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([8.2314e-05, 8.0943e-05, 8.2314e-05, 8.2314e-05, 8.3208e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.0943e-05, 8.2970e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([8.2314e-05, 8.0943e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.0943e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2016e-05, 8.2314e-05, 8.2314e-05,\n",
            "        8.2314e-05, 8.2314e-05, 8.2314e-05, 8.2314e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([8.2314e-05, 7.9989e-05, 8.2314e-05, 8.2314e-05, 7.9989e-05, 8.2314e-05,\n",
            "        7.9989e-05, 8.2314e-05, 8.2314e-05, 8.2016e-05, 8.2314e-05, 8.2314e-05,\n",
            "        7.9989e-05, 7.9989e-05, 7.9989e-05, 8.2314e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([7.9989e-05, 7.9989e-05, 7.9989e-05, 7.9989e-05, 7.9989e-05, 7.9989e-05,\n",
            "        7.9989e-05, 7.9989e-05, 7.9989e-05, 7.9811e-05, 7.9989e-05, 7.9989e-05,\n",
            "        7.9989e-05, 7.9811e-05, 7.9989e-05, 7.9989e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([7.9989e-05, 7.9811e-05, 7.9989e-05, 7.9989e-05, 7.9989e-05, 7.9989e-05,\n",
            "        7.9989e-05, 7.9989e-05, 7.9989e-05, 7.9811e-05, 7.9989e-05, 7.9989e-05,\n",
            "        7.9989e-05, 7.9811e-05, 7.8499e-05, 7.9811e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([7.9989e-05, 7.9811e-05, 7.9989e-05, 7.9989e-05, 7.9989e-05, 7.9989e-05,\n",
            "        7.9989e-05, 7.9989e-05, 7.9989e-05, 7.9811e-05, 7.9989e-05, 7.9989e-05,\n",
            "        7.9989e-05, 7.9811e-05, 7.8499e-05, 7.9811e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([7.8321e-05, 7.9811e-05, 7.9989e-05, 7.9989e-05, 7.9989e-05, 7.9811e-05,\n",
            "        7.9811e-05, 7.9989e-05, 7.9811e-05, 7.9811e-05, 7.9989e-05, 7.9989e-05,\n",
            "        7.9989e-05, 7.9811e-05, 7.8499e-05, 7.9811e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([7.8321e-05, 7.9811e-05, 7.9989e-05, 7.9811e-05, 7.9989e-05, 7.9811e-05,\n",
            "        7.9811e-05, 7.9811e-05, 7.9811e-05, 7.9811e-05, 7.9989e-05, 7.9989e-05,\n",
            "        7.9989e-05, 7.9811e-05, 7.8499e-05, 7.9811e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([7.8321e-05, 7.8321e-05, 7.9989e-05, 7.9811e-05, 7.9989e-05, 7.9811e-05,\n",
            "        7.9811e-05, 7.9811e-05, 7.9811e-05, 7.8321e-05, 7.9989e-05, 7.9989e-05,\n",
            "        7.9989e-05, 7.8321e-05, 7.8499e-05, 7.9811e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.0794e-04, 1.1921e-04, 1.1122e-04, 1.0467e-04, 1.0705e-04, 1.1545e-04,\n",
            "        9.4056e-05, 8.9705e-05, 1.1259e-04, 9.5606e-05, 1.0812e-04, 1.0902e-04,\n",
            "        1.1814e-04, 8.3566e-05, 1.1057e-04, 1.1563e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([8.9824e-05, 1.2326e-04, 1.0502e-04, 1.0651e-04, 1.0908e-04, 1.1873e-04,\n",
            "        9.0539e-05, 9.6917e-05, 1.0604e-04, 1.0252e-04, 9.4414e-05, 1.2565e-04,\n",
            "        1.1981e-04, 8.3268e-05, 1.1081e-04, 1.0842e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([9.5010e-05, 1.1003e-04, 1.0866e-04, 1.0502e-04, 1.1051e-04, 1.1826e-04,\n",
            "        9.8705e-05, 1.0425e-04, 1.1599e-04, 1.0526e-04, 9.8825e-05, 1.2267e-04,\n",
            "        1.1426e-04, 9.4295e-05, 1.1176e-04, 1.0568e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([8.9288e-05, 9.2268e-05, 9.0539e-05, 9.1136e-05, 9.3937e-05, 1.0103e-04,\n",
            "        9.2804e-05, 9.2149e-05, 9.2447e-05, 9.3102e-05, 9.3400e-05, 9.8050e-05,\n",
            "        9.3043e-05, 9.0718e-05, 9.1434e-05, 1.0049e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([9.0837e-05, 9.2864e-05, 9.0539e-05, 9.4235e-05, 9.4235e-05, 9.3162e-05,\n",
            "        9.3162e-05, 9.2149e-05, 9.1493e-05, 9.2149e-05, 9.2149e-05, 9.4235e-05,\n",
            "        9.1493e-05, 9.1374e-05, 9.2149e-05, 9.3341e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([9.0837e-05, 9.2149e-05, 9.0539e-05, 9.4235e-05, 9.2149e-05, 9.2149e-05,\n",
            "        9.2864e-05, 9.2149e-05, 9.1493e-05, 9.2149e-05, 9.2149e-05, 9.2149e-05,\n",
            "        9.2149e-05, 9.0837e-05, 9.2149e-05, 9.2149e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([9.0837e-05, 8.9049e-05, 8.7619e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        9.0837e-05, 9.2149e-05, 8.9049e-05, 8.9049e-05, 9.2149e-05, 8.9049e-05,\n",
            "        8.9049e-05, 9.2149e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([8.7619e-05, 8.9049e-05, 8.7619e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.7619e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([8.7619e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.7619e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.7619e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([8.6486e-05, 8.9049e-05, 8.6486e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.6486e-05, 8.6248e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.8692e-05, 8.6486e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05,\n",
            "        8.6486e-05, 8.6248e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05,\n",
            "        8.6248e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05,\n",
            "        8.6486e-05, 8.6248e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05,\n",
            "        8.6248e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05,\n",
            "        8.6486e-05, 8.6248e-05, 8.6248e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05,\n",
            "        8.6248e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05,\n",
            "        8.6486e-05, 8.6248e-05, 8.6248e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05,\n",
            "        8.6248e-05, 8.6248e-05, 8.6486e-05, 8.6486e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([8.6486e-05, 8.6486e-05, 8.6486e-05, 8.4817e-05, 8.6248e-05, 8.6486e-05,\n",
            "        8.6486e-05, 8.4817e-05, 8.6248e-05, 8.6486e-05, 8.6486e-05, 8.6486e-05,\n",
            "        8.6248e-05, 8.4817e-05, 8.6486e-05, 8.6248e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([8.6486e-05, 8.6486e-05, 8.6486e-05, 8.4817e-05, 8.6248e-05, 8.6248e-05,\n",
            "        8.5056e-05, 8.4817e-05, 8.6248e-05, 8.6248e-05, 8.6248e-05, 8.6486e-05,\n",
            "        8.6248e-05, 8.4817e-05, 8.6248e-05, 8.6248e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([8.6486e-05, 8.6486e-05, 8.5056e-05, 8.4817e-05, 8.6248e-05, 8.6248e-05,\n",
            "        8.5056e-05, 8.4817e-05, 8.6248e-05, 8.6248e-05, 8.6248e-05, 8.6486e-05,\n",
            "        8.6248e-05, 8.4817e-05, 8.6248e-05, 8.6248e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([9.4056e-05, 1.1903e-04, 1.0294e-04, 1.0788e-04, 8.7023e-05, 1.3208e-04,\n",
            "        1.3125e-04, 1.1194e-04, 8.7261e-05, 1.1343e-04, 1.2755e-04, 1.1140e-04,\n",
            "        8.5413e-05, 9.9897e-05, 1.0818e-04, 1.0347e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([8.6784e-05, 1.0419e-04, 1.1551e-04, 1.0806e-04, 9.3102e-05, 1.4591e-04,\n",
            "        1.0014e-04, 1.0210e-04, 8.6486e-05, 1.2034e-04, 1.1587e-04, 9.5308e-05,\n",
            "        8.7857e-05, 1.0973e-04, 9.3162e-05, 1.1271e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([9.7573e-05, 1.1468e-04, 1.2064e-04, 1.1098e-04, 1.0145e-04, 1.1724e-04,\n",
            "        1.1069e-04, 1.1218e-04, 9.7156e-05, 1.1361e-04, 1.1021e-04, 1.0163e-04,\n",
            "        8.8930e-05, 1.1826e-04, 1.0270e-04, 9.8050e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([9.2030e-05, 1.0145e-04, 9.0003e-05, 9.1732e-05, 8.8692e-05, 9.2268e-05,\n",
            "        1.0294e-04, 9.8407e-05, 9.0420e-05, 1.0037e-04, 9.0718e-05, 1.0097e-04,\n",
            "        8.8155e-05, 9.2387e-05, 9.0778e-05, 9.1732e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([9.3162e-05, 9.2745e-05, 9.2745e-05, 9.2745e-05, 9.2387e-05, 9.2745e-05,\n",
            "        9.4354e-05, 9.0957e-05, 9.3758e-05, 1.0037e-04, 9.2745e-05, 9.2387e-05,\n",
            "        9.2626e-05, 9.2387e-05, 9.0837e-05, 9.2745e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([9.3162e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 9.2745e-05, 8.9049e-05,\n",
            "        8.8453e-05, 8.7380e-05, 9.2745e-05, 8.8573e-05, 8.9049e-05, 8.9049e-05,\n",
            "        9.0837e-05, 8.9049e-05, 9.1314e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([8.9467e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9943e-05, 8.9049e-05, 8.9049e-05, 8.8573e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.7321e-05, 8.9049e-05, 8.7619e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([8.9467e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9943e-05, 8.9049e-05, 8.9049e-05, 8.8573e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.7619e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([8.8692e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9943e-05, 8.9049e-05, 8.9049e-05, 8.9943e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([8.8692e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9943e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([8.8692e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9943e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([8.8692e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9943e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([8.6427e-05, 8.9049e-05, 8.9049e-05, 8.9049e-05, 8.6665e-05, 8.9049e-05,\n",
            "        8.9049e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05, 8.9049e-05, 8.9049e-05,\n",
            "        8.6665e-05, 8.9049e-05, 8.6665e-05, 8.8692e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([8.6427e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05,\n",
            "        8.6665e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05,\n",
            "        8.6665e-05, 8.6665e-05, 8.6427e-05, 8.6427e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([8.6665e-05, 8.6427e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05,\n",
            "        8.6665e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05,\n",
            "        8.6665e-05, 8.6665e-05, 8.6427e-05, 8.6427e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([8.6665e-05, 8.6427e-05, 8.6665e-05, 8.4877e-05, 8.6665e-05, 8.6665e-05,\n",
            "        8.6665e-05, 8.6665e-05, 8.6427e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05,\n",
            "        8.6665e-05, 8.6665e-05, 8.6427e-05, 8.6427e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([8.5175e-05, 8.6427e-05, 8.6665e-05, 8.4877e-05, 8.5175e-05, 8.6665e-05,\n",
            "        8.6665e-05, 8.6665e-05, 8.6427e-05, 8.6665e-05, 8.6665e-05, 8.6665e-05,\n",
            "        8.6665e-05, 8.6665e-05, 8.6427e-05, 8.6427e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([8.5175e-05, 8.6665e-05, 8.6665e-05, 8.4877e-05, 8.5175e-05, 8.6665e-05,\n",
            "        8.6427e-05, 8.6665e-05, 8.6427e-05, 8.6665e-05, 8.6665e-05, 8.6427e-05,\n",
            "        8.4877e-05, 8.6665e-05, 8.4877e-05, 8.6427e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([8.5175e-05, 8.6665e-05, 8.6427e-05, 8.4877e-05, 8.4877e-05, 8.6665e-05,\n",
            "        8.6427e-05, 8.6665e-05, 8.6427e-05, 8.6665e-05, 8.6665e-05, 8.6427e-05,\n",
            "        8.4877e-05, 8.6427e-05, 8.4877e-05, 8.6427e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([8.5175e-05, 8.6665e-05, 8.6427e-05, 8.4877e-05, 8.4877e-05, 8.6427e-05,\n",
            "        8.6427e-05, 8.5175e-05, 8.6427e-05, 8.6665e-05, 8.6427e-05, 8.6427e-05,\n",
            "        8.4877e-05, 8.6427e-05, 8.4877e-05, 8.6427e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "dided\n",
            "time\n",
            "[3, 3, 3, 0, 3, 3, 3, 0, 3, 3, 3, 0, 3, 3]\n",
            "0 #### train ####\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-a65a920d1034>:177: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "14 tensor([1.6689e-05, 1.9193e-05, 1.9193e-05, 2.1517e-05, 1.8239e-05, 2.3186e-05,\n",
            "        2.0444e-05, 1.9193e-05, 1.8597e-05, 1.9193e-05, 2.2411e-05, 1.8358e-05,\n",
            "        1.9670e-05, 1.9670e-05, 2.3186e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7345e-05, 1.9193e-05, 1.9193e-05, 2.0623e-05, 1.8358e-05, 2.3186e-05,\n",
            "        2.1040e-05, 1.9193e-05, 1.8597e-05, 1.9193e-05, 1.9372e-05, 1.8358e-05,\n",
            "        1.9670e-05, 1.9670e-05, 2.2650e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.8120e-05, 1.9193e-05, 1.9193e-05, 2.0623e-05, 1.8418e-05, 2.1815e-05,\n",
            "        2.1636e-05, 1.9252e-05, 1.8597e-05, 1.9193e-05, 1.9550e-05, 1.8358e-05,\n",
            "        1.9729e-05, 1.9729e-05, 2.2650e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.8239e-05, 1.9252e-05, 1.9252e-05, 2.0623e-05, 1.8954e-05, 2.1815e-05,\n",
            "        2.2531e-05, 1.9431e-05, 1.8597e-05, 1.9372e-05, 1.9789e-05, 1.7822e-05,\n",
            "        1.9908e-05, 1.8835e-05, 2.1815e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.8358e-05, 1.9312e-05, 1.8299e-05, 2.0623e-05, 1.9133e-05, 2.1815e-05,\n",
            "        2.3603e-05, 1.9312e-05, 1.8597e-05, 1.9431e-05, 2.0087e-05, 1.7822e-05,\n",
            "        1.9848e-05, 1.9014e-05, 2.5630e-05, 1.8358e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.8656e-05, 1.9491e-05, 1.8418e-05, 2.0623e-05, 1.9431e-05, 2.1815e-05,\n",
            "        2.4915e-05, 1.8418e-05, 1.8597e-05, 1.8418e-05, 1.9968e-05, 1.7881e-05,\n",
            "        2.0206e-05, 1.8001e-05, 2.5630e-05, 1.8358e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.5511e-05, 2.1279e-05, 2.2292e-05, 3.1888e-05, 2.2829e-05, 2.9325e-05,\n",
            "        2.8253e-05, 1.9789e-05, 1.8418e-05, 3.1292e-05, 2.2769e-05, 2.8908e-05,\n",
            "        3.2485e-05, 2.9087e-05, 1.8537e-05, 2.4140e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.7657e-05, 2.3484e-05, 2.6107e-05, 2.5630e-05, 2.6762e-05, 2.9743e-05,\n",
            "        3.3021e-05, 2.2948e-05, 2.1577e-05, 2.3603e-05, 1.6868e-05, 2.4617e-05,\n",
            "        2.8074e-05, 2.0385e-05, 1.7703e-05, 2.4736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.6762e-05, 2.8372e-05, 2.7657e-05, 2.1994e-05, 2.4378e-05, 2.1935e-05,\n",
            "        3.8683e-05, 2.8014e-05, 2.4319e-05, 2.2173e-05, 1.8597e-05, 2.4736e-05,\n",
            "        3.0458e-05, 2.4080e-05, 2.0087e-05, 2.3842e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.2054e-05, 2.9862e-05, 2.8372e-05, 2.2054e-05, 2.1458e-05, 2.5153e-05,\n",
            "        2.5153e-05, 2.2829e-05, 2.5332e-05, 1.9968e-05, 2.1517e-05, 2.7061e-05,\n",
            "        2.2471e-05, 2.6107e-05, 2.2471e-05, 2.3603e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.5332e-05, 2.7061e-05, 2.5451e-05, 2.4855e-05, 2.3663e-05, 2.5511e-05,\n",
            "        2.2233e-05, 2.5213e-05, 2.7359e-05, 2.1338e-05, 2.5630e-05, 3.0339e-05,\n",
            "        2.4438e-05, 2.5868e-05, 2.4974e-05, 2.5928e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.8074e-05, 3.0458e-05, 2.8074e-05, 2.8551e-05, 2.5988e-05, 2.3603e-05,\n",
            "        2.3723e-05, 2.7657e-05, 2.7895e-05, 2.2888e-05, 2.8908e-05, 3.2604e-05,\n",
            "        2.3663e-05, 2.9147e-05, 2.8074e-05, 2.8670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.9193e-05, 1.8179e-05, 1.9193e-05, 2.0981e-05, 2.4974e-05, 2.4080e-05,\n",
            "        2.5451e-05, 2.9147e-05, 2.2590e-05, 2.3007e-05, 1.8179e-05, 3.2425e-05,\n",
            "        2.3961e-05, 2.0206e-05, 1.9133e-05, 1.9848e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.9193e-05, 1.8179e-05, 1.9193e-05, 2.0683e-05, 2.4140e-05, 2.4498e-05,\n",
            "        2.7537e-05, 2.7180e-05, 2.2590e-05, 2.2292e-05, 1.8895e-05, 2.7120e-05,\n",
            "        2.3305e-05, 1.9014e-05, 1.9193e-05, 1.9193e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.9193e-05, 1.8895e-05, 1.9193e-05, 1.8716e-05, 2.2650e-05, 2.4676e-05,\n",
            "        1.7643e-05, 2.6345e-05, 1.9968e-05, 1.6570e-05, 1.8895e-05, 2.7120e-05,\n",
            "        2.3663e-05, 1.9014e-05, 1.9193e-05, 1.9193e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.9193e-05, 1.8895e-05, 1.9193e-05, 1.8716e-05, 2.2650e-05, 2.4676e-05,\n",
            "        1.8299e-05, 2.6345e-05, 1.8716e-05, 1.7107e-05, 1.8895e-05, 2.6464e-05,\n",
            "        2.3663e-05, 1.8716e-05, 1.8537e-05, 1.8477e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.8537e-05, 1.8895e-05, 1.8537e-05, 1.8716e-05, 2.2650e-05, 2.4676e-05,\n",
            "        1.8299e-05, 2.4199e-05, 1.8716e-05, 1.7881e-05, 1.8895e-05, 2.6464e-05,\n",
            "        2.3663e-05, 1.8716e-05, 1.8537e-05, 1.8477e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.8537e-05, 1.8656e-05, 1.8537e-05, 1.8716e-05, 2.2650e-05, 2.4676e-05,\n",
            "        1.8299e-05, 2.1398e-05, 1.8656e-05, 1.7881e-05, 1.8656e-05, 2.5511e-05,\n",
            "        2.3663e-05, 1.8716e-05, 1.8537e-05, 1.8477e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.8537e-05, 1.8716e-05, 1.8537e-05, 1.8358e-05, 2.0325e-05, 2.4676e-05,\n",
            "        1.8299e-05, 2.1398e-05, 1.8656e-05, 1.7822e-05, 1.8656e-05, 2.5511e-05,\n",
            "        2.3663e-05, 1.8716e-05, 1.8537e-05, 1.9491e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.8537e-05, 1.8775e-05, 1.8537e-05, 1.8537e-05, 2.0325e-05, 2.4676e-05,\n",
            "        1.8299e-05, 2.1219e-05, 1.8716e-05, 1.7822e-05, 1.7822e-05, 2.5511e-05,\n",
            "        2.3663e-05, 1.8716e-05, 1.7643e-05, 1.9431e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.0146e-05, 1.9431e-05, 2.0087e-05, 1.8656e-05, 2.0325e-05, 2.4676e-05,\n",
            "        1.8299e-05, 2.1219e-05, 1.9133e-05, 1.7226e-05, 1.8299e-05, 2.5511e-05,\n",
            "        2.3663e-05, 1.8775e-05, 1.7762e-05, 1.8716e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.9073e-05, 1.9372e-05, 2.0146e-05, 1.9073e-05, 2.0325e-05, 2.4498e-05,\n",
            "        1.8358e-05, 2.1219e-05, 1.9431e-05, 1.7285e-05, 1.8358e-05, 2.6464e-05,\n",
            "        2.3305e-05, 1.9491e-05, 1.9073e-05, 1.9014e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.8001e-05, 2.0027e-05, 1.7941e-05, 1.9610e-05, 2.0325e-05, 2.4498e-05,\n",
            "        1.8358e-05, 2.1517e-05, 1.9789e-05, 1.7524e-05, 1.8656e-05, 2.6464e-05,\n",
            "        2.3305e-05, 1.9729e-05, 1.9252e-05, 1.8179e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.8299e-05, 2.1040e-05, 1.8120e-05, 2.0266e-05, 2.0325e-05, 2.6107e-05,\n",
            "        1.7822e-05, 2.1517e-05, 1.9789e-05, 1.8001e-05, 1.9073e-05, 2.6226e-05,\n",
            "        2.3723e-05, 2.1458e-05, 1.8299e-05, 1.8716e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.8775e-05, 2.2590e-05, 1.6451e-05, 2.1160e-05, 2.0325e-05, 2.6107e-05,\n",
            "        1.8001e-05, 2.1517e-05, 2.0683e-05, 1.8656e-05, 2.0385e-05, 2.6226e-05,\n",
            "        2.3723e-05, 2.2054e-05, 1.6570e-05, 1.9252e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.6928e-05, 2.4736e-05, 1.6749e-05, 2.2292e-05, 2.0325e-05, 2.6107e-05,\n",
            "        1.8120e-05, 2.1219e-05, 2.1994e-05, 1.9789e-05, 2.0921e-05, 2.6226e-05,\n",
            "        2.3723e-05, 2.1994e-05, 1.6868e-05, 1.9968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.8775e-05, 1.9193e-05, 1.8358e-05, 1.5318e-05, 1.6570e-05, 1.9848e-05,\n",
            "        2.1219e-05, 1.5795e-05, 1.8179e-05, 1.7762e-05, 2.7776e-05, 1.7583e-05,\n",
            "        1.3471e-05, 1.1861e-05, 1.5855e-05, 1.4722e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.7524e-05, 1.8656e-05, 1.8418e-05, 1.5676e-05, 1.8358e-05, 1.6809e-05,\n",
            "        1.6868e-05, 1.7941e-05, 2.0444e-05, 1.6809e-05, 3.1710e-05, 1.3113e-05,\n",
            "        1.4722e-05, 1.4663e-05, 1.5914e-05, 1.2577e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.7047e-05, 2.0266e-05, 1.8716e-05, 1.8239e-05, 1.3351e-05, 1.3053e-05,\n",
            "        1.8954e-05, 1.8358e-05, 2.6286e-05, 1.6034e-05, 4.8161e-05, 1.7345e-05,\n",
            "        1.6570e-05, 1.2994e-05, 1.4782e-05, 1.1802e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.7464e-05, 1.5795e-05, 2.3425e-05, 1.7107e-05, 1.4603e-05, 1.4663e-05,\n",
            "        2.3186e-05, 1.5855e-05, 2.2709e-05, 1.9252e-05, 2.1994e-05, 1.8477e-05,\n",
            "        1.4603e-05, 1.4484e-05, 1.6391e-05, 1.4603e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.5378e-05, 1.8716e-05, 1.6272e-05, 1.7345e-05, 1.6272e-05, 1.6153e-05,\n",
            "        2.0027e-05, 1.7107e-05, 1.6451e-05, 1.9729e-05, 1.7285e-05, 1.8835e-05,\n",
            "        1.6212e-05, 1.7464e-05, 1.6689e-05, 1.6034e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.6451e-05, 1.8060e-05, 1.7643e-05, 1.9848e-05, 1.6928e-05, 1.8299e-05,\n",
            "        2.2054e-05, 1.6928e-05, 1.7583e-05, 1.7703e-05, 1.6570e-05, 1.6689e-05,\n",
            "        1.7822e-05, 1.9848e-05, 1.4901e-05, 1.7405e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.3067e-05, 1.4126e-05, 2.1040e-05, 1.4782e-05, 1.7524e-05, 1.7583e-05,\n",
            "        1.7881e-05, 2.3603e-05, 1.7822e-05, 1.7822e-05, 1.7345e-05, 1.7881e-05,\n",
            "        1.3411e-05, 1.5199e-05, 1.1623e-05, 1.9073e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.4319e-05, 1.4126e-05, 1.7822e-05, 1.3471e-05, 1.6630e-05, 1.4126e-05,\n",
            "        1.7881e-05, 1.8179e-05, 1.7822e-05, 1.9312e-05, 1.7345e-05, 1.7822e-05,\n",
            "        1.3411e-05, 1.5199e-05, 1.1623e-05, 1.9252e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.8299e-05, 1.4305e-05, 1.7822e-05, 1.3471e-05, 1.6153e-05, 1.4663e-05,\n",
            "        1.7822e-05, 1.8179e-05, 1.7822e-05, 1.9312e-05, 1.6510e-05, 1.9312e-05,\n",
            "        1.2994e-05, 1.5199e-05, 1.1623e-05, 1.9252e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.8358e-05, 1.3828e-05, 1.7822e-05, 1.3530e-05, 1.6510e-05, 1.4842e-05,\n",
            "        1.7822e-05, 1.8358e-05, 1.7822e-05, 1.9312e-05, 1.4186e-05, 1.9312e-05,\n",
            "        1.2994e-05, 1.4067e-05, 1.1444e-05, 1.9312e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.8358e-05, 1.3828e-05, 1.7822e-05, 1.3769e-05, 1.6689e-05, 1.5378e-05,\n",
            "        1.7822e-05, 1.8358e-05, 1.7822e-05, 1.9312e-05, 1.4186e-05, 1.9312e-05,\n",
            "        1.3173e-05, 1.4365e-05, 1.1683e-05, 1.9312e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.8358e-05, 1.3828e-05, 1.7822e-05, 1.4067e-05, 1.3888e-05, 1.6153e-05,\n",
            "        1.7822e-05, 1.8358e-05, 1.7822e-05, 1.9312e-05, 1.4246e-05, 1.9312e-05,\n",
            "        1.3351e-05, 1.4901e-05, 1.1981e-05, 1.9312e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.8358e-05, 1.9312e-05, 1.7822e-05, 1.4603e-05, 1.4067e-05, 1.7166e-05,\n",
            "        1.7822e-05, 1.7464e-05, 1.7822e-05, 1.7822e-05, 1.4246e-05, 1.9312e-05,\n",
            "        1.3828e-05, 1.5497e-05, 1.1325e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7464e-05, 1.3292e-05, 1.7822e-05, 1.5318e-05, 1.4305e-05, 1.6868e-05,\n",
            "        1.7822e-05, 1.7464e-05, 1.7822e-05, 1.7822e-05, 1.4246e-05, 1.9312e-05,\n",
            "        1.2934e-05, 1.6570e-05, 1.1683e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.7464e-05, 1.3292e-05, 1.9312e-05, 1.6034e-05, 1.4603e-05, 1.6809e-05,\n",
            "        1.7822e-05, 1.7464e-05, 1.7822e-05, 1.7881e-05, 1.4305e-05, 1.7822e-05,\n",
            "        1.3530e-05, 1.7822e-05, 1.2457e-05, 1.7881e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7464e-05, 1.3828e-05, 1.9372e-05, 1.7226e-05, 1.5080e-05, 1.6809e-05,\n",
            "        1.7226e-05, 1.7464e-05, 1.7822e-05, 1.7881e-05, 1.4305e-05, 1.7822e-05,\n",
            "        1.5020e-05, 1.9073e-05, 1.2994e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.7464e-05, 1.3292e-05, 1.9372e-05, 1.8418e-05, 1.6570e-05, 1.6809e-05,\n",
            "        1.7226e-05, 1.8358e-05, 1.7822e-05, 1.7822e-05, 1.4305e-05, 1.7822e-05,\n",
            "        1.6570e-05, 2.0385e-05, 1.4365e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.7464e-05, 1.3292e-05, 1.7464e-05, 1.9252e-05, 1.8001e-05, 1.6809e-05,\n",
            "        1.7226e-05, 1.8358e-05, 1.7226e-05, 1.7226e-05, 1.4305e-05, 1.7822e-05,\n",
            "        1.7285e-05, 2.0385e-05, 1.7583e-05, 1.7881e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.8239e-05, 1.3292e-05, 1.7464e-05, 1.9312e-05, 1.6809e-05, 1.6809e-05,\n",
            "        1.7226e-05, 1.8358e-05, 1.7226e-05, 1.7226e-05, 1.3828e-05, 1.6809e-05,\n",
            "        1.9848e-05, 2.0981e-05, 1.6809e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.8239e-05, 1.3292e-05, 1.7464e-05, 1.9729e-05, 1.6809e-05, 1.7464e-05,\n",
            "        1.7226e-05, 1.7464e-05, 1.7226e-05, 1.7226e-05, 1.3828e-05, 1.6809e-05,\n",
            "        1.9550e-05, 2.0981e-05, 1.8120e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.2829e-05, 2.5749e-05, 2.0564e-05, 2.0921e-05, 2.6822e-05, 2.0921e-05,\n",
            "        2.2769e-05, 2.0862e-05, 2.2888e-05, 1.9133e-05, 1.8775e-05, 2.2590e-05,\n",
            "        2.2352e-05, 2.7776e-05, 1.8537e-05, 1.7703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.1517e-05, 3.5703e-05, 1.7762e-05, 1.8954e-05, 2.3305e-05, 2.0504e-05,\n",
            "        2.3723e-05, 1.9968e-05, 2.4736e-05, 2.2829e-05, 1.7643e-05, 2.0087e-05,\n",
            "        2.3901e-05, 3.1233e-05, 1.6928e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.2113e-05, 2.0146e-05, 1.8060e-05, 2.1100e-05, 3.1590e-05, 2.3186e-05,\n",
            "        2.4974e-05, 2.4915e-05, 3.1769e-05, 1.7047e-05, 2.2769e-05, 2.2531e-05,\n",
            "        2.2709e-05, 3.4511e-05, 2.0683e-05, 2.3901e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.0862e-05, 2.0146e-05, 1.9908e-05, 2.5153e-05, 3.6418e-05, 2.3186e-05,\n",
            "        2.3365e-05, 2.4319e-05, 2.0742e-05, 2.0683e-05, 2.6584e-05, 2.5034e-05,\n",
            "        1.9550e-05, 2.1517e-05, 1.9908e-05, 2.8193e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.3842e-05, 2.1815e-05, 2.2471e-05, 2.2233e-05, 2.5511e-05, 2.5153e-05,\n",
            "        2.5332e-05, 2.2948e-05, 2.3603e-05, 2.0862e-05, 2.8610e-05, 2.5690e-05,\n",
            "        2.2411e-05, 2.5392e-05, 2.2411e-05, 2.3007e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.6166e-05, 2.2650e-05, 2.4259e-05, 2.4617e-05, 2.6166e-05, 2.8551e-05,\n",
            "        2.6405e-05, 2.3484e-05, 2.3901e-05, 2.2709e-05, 2.3782e-05, 2.5451e-05,\n",
            "        2.2590e-05, 2.5988e-05, 2.5511e-05, 2.3961e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.5094e-05, 2.1517e-05, 1.8895e-05, 1.7822e-05, 2.6524e-05, 1.7822e-05,\n",
            "        2.6524e-05, 2.3544e-05, 2.4498e-05, 2.1517e-05, 2.4557e-05, 1.8120e-05,\n",
            "        2.2948e-05, 2.6524e-05, 1.8120e-05, 2.4915e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.3127e-05, 2.1338e-05, 1.9848e-05, 1.7822e-05, 2.2054e-05, 1.6868e-05,\n",
            "        2.3127e-05, 2.3127e-05, 2.1577e-05, 2.0862e-05, 2.4915e-05, 1.7166e-05,\n",
            "        1.5736e-05, 2.2054e-05, 1.7166e-05, 2.4915e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.3127e-05, 2.1875e-05, 2.0921e-05, 1.7822e-05, 2.2054e-05, 1.6868e-05,\n",
            "        2.3127e-05, 2.3127e-05, 2.1577e-05, 2.1636e-05, 2.4915e-05, 1.7166e-05,\n",
            "        1.5795e-05, 2.2054e-05, 1.7166e-05, 2.4915e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.3127e-05, 2.1875e-05, 2.1100e-05, 1.7226e-05, 2.2054e-05, 1.6868e-05,\n",
            "        2.3127e-05, 2.3127e-05, 2.1577e-05, 2.1875e-05, 2.4199e-05, 1.7166e-05,\n",
            "        1.5795e-05, 2.2054e-05, 1.7166e-05, 2.1577e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.3127e-05, 2.1875e-05, 1.7822e-05, 1.7345e-05, 2.2054e-05, 1.5795e-05,\n",
            "        2.2054e-05, 2.3127e-05, 2.1577e-05, 2.1875e-05, 2.1279e-05, 1.7166e-05,\n",
            "        1.5855e-05, 2.2054e-05, 1.6928e-05, 2.1219e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.3127e-05, 2.1875e-05, 1.8239e-05, 1.7643e-05, 2.2054e-05, 1.5795e-05,\n",
            "        2.2054e-05, 2.3127e-05, 2.1219e-05, 2.1875e-05, 2.1279e-05, 1.7166e-05,\n",
            "        1.5914e-05, 2.1875e-05, 1.6928e-05, 2.1219e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.3127e-05, 2.1875e-05, 1.9372e-05, 1.8299e-05, 2.1875e-05, 1.5795e-05,\n",
            "        2.2054e-05, 2.3246e-05, 1.8656e-05, 2.1875e-05, 1.8656e-05, 1.6272e-05,\n",
            "        1.5914e-05, 2.1875e-05, 1.7107e-05, 1.8656e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.3127e-05, 2.2173e-05, 1.9729e-05, 1.7583e-05, 2.1875e-05, 1.5736e-05,\n",
            "        2.3246e-05, 2.1875e-05, 1.8656e-05, 2.1875e-05, 1.8656e-05, 1.6391e-05,\n",
            "        1.6034e-05, 2.1875e-05, 1.7166e-05, 1.8656e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.3127e-05, 2.2173e-05, 1.9073e-05, 1.8060e-05, 2.3246e-05, 1.6093e-05,\n",
            "        2.3246e-05, 2.1875e-05, 1.8656e-05, 2.1875e-05, 1.8656e-05, 1.6689e-05,\n",
            "        1.6749e-05, 2.3246e-05, 1.6570e-05, 1.8656e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.1875e-05, 2.2173e-05, 2.0206e-05, 2.0325e-05, 2.3246e-05, 1.6510e-05,\n",
            "        2.1875e-05, 2.1875e-05, 1.8656e-05, 2.1875e-05, 1.8656e-05, 1.8239e-05,\n",
            "        1.6928e-05, 2.3246e-05, 1.8060e-05, 2.0206e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.1875e-05, 2.2173e-05, 2.1756e-05, 2.1756e-05, 2.3246e-05, 1.7166e-05,\n",
            "        2.1875e-05, 2.3246e-05, 1.8716e-05, 2.2173e-05, 1.8656e-05, 1.9133e-05,\n",
            "        1.7107e-05, 2.3246e-05, 1.8954e-05, 2.0206e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.3246e-05, 2.2173e-05, 2.2888e-05, 2.3425e-05, 2.3246e-05, 1.8537e-05,\n",
            "        2.3246e-05, 2.3246e-05, 1.8716e-05, 2.2173e-05, 1.8716e-05, 2.0742e-05,\n",
            "        1.6987e-05, 2.3246e-05, 2.0206e-05, 1.8716e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.3246e-05, 2.2173e-05, 2.3127e-05, 2.0802e-05, 2.3246e-05, 1.9431e-05,\n",
            "        2.1875e-05, 2.3246e-05, 1.8716e-05, 2.2173e-05, 1.8716e-05, 2.2709e-05,\n",
            "        1.7822e-05, 2.3246e-05, 2.1815e-05, 1.8716e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.3246e-05, 2.2173e-05, 2.3127e-05, 2.1756e-05, 2.3246e-05, 2.0564e-05,\n",
            "        2.3246e-05, 2.3246e-05, 1.8716e-05, 2.2173e-05, 1.8716e-05, 2.3067e-05,\n",
            "        1.8954e-05, 2.3246e-05, 2.3901e-05, 1.8716e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.4021e-05, 1.5378e-05, 2.2650e-05, 1.9431e-05, 1.9610e-05, 1.8656e-05,\n",
            "        1.8954e-05, 2.2531e-05, 1.9252e-05, 1.6987e-05, 1.9491e-05, 2.1458e-05,\n",
            "        2.0444e-05, 1.4305e-05, 1.5140e-05, 1.5855e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.4663e-05, 1.5676e-05, 2.9206e-05, 1.4186e-05, 1.5020e-05, 1.7405e-05,\n",
            "        2.1398e-05, 1.9848e-05, 1.9431e-05, 1.8537e-05, 1.6332e-05, 2.6762e-05,\n",
            "        2.6286e-05, 1.8835e-05, 1.3590e-05, 1.8179e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.5914e-05, 2.0623e-05, 3.1054e-05, 1.7285e-05, 1.7762e-05, 1.9252e-05,\n",
            "        2.5272e-05, 2.1219e-05, 1.8716e-05, 2.3127e-05, 1.5080e-05, 1.6928e-05,\n",
            "        2.5868e-05, 1.6093e-05, 1.6987e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.7703e-05, 1.7941e-05, 1.6510e-05, 1.6928e-05, 1.8060e-05, 1.9312e-05,\n",
            "        2.2352e-05, 2.0564e-05, 1.4722e-05, 2.4021e-05, 1.8120e-05, 1.4722e-05,\n",
            "        1.7226e-05, 1.4901e-05, 1.6093e-05, 1.6570e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.7524e-05, 1.9372e-05, 1.9372e-05, 1.8597e-05, 2.0504e-05, 2.1696e-05,\n",
            "        1.9372e-05, 2.3544e-05, 1.5736e-05, 1.7703e-05, 1.7762e-05, 1.5259e-05,\n",
            "        2.0325e-05, 1.6332e-05, 2.0385e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.9014e-05, 2.0921e-05, 2.0146e-05, 1.7941e-05, 2.3484e-05, 2.3723e-05,\n",
            "        2.1040e-05, 1.9491e-05, 1.5438e-05, 2.3901e-05, 1.9193e-05, 1.7285e-05,\n",
            "        2.3305e-05, 1.8120e-05, 1.6868e-05, 2.1636e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.9372e-05, 2.1160e-05, 2.1577e-05, 1.8060e-05, 2.3484e-05, 2.3842e-05,\n",
            "        2.1219e-05, 1.9610e-05, 1.5497e-05, 1.9252e-05, 1.9610e-05, 1.9133e-05,\n",
            "        1.9610e-05, 1.7226e-05, 1.6868e-05, 2.1636e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.0742e-05, 2.3067e-05, 2.2471e-05, 1.9133e-05, 1.7881e-05, 1.9610e-05,\n",
            "        2.1219e-05, 1.9610e-05, 1.5736e-05, 1.9252e-05, 2.1219e-05, 1.5318e-05,\n",
            "        2.1219e-05, 1.4305e-05, 1.2815e-05, 1.6928e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.1338e-05, 2.3067e-05, 1.9610e-05, 2.0027e-05, 1.7107e-05, 1.9610e-05,\n",
            "        2.1219e-05, 2.1219e-05, 1.5318e-05, 2.0206e-05, 2.1219e-05, 1.5318e-05,\n",
            "        2.1219e-05, 1.5318e-05, 1.2517e-05, 1.6451e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.1696e-05, 2.3067e-05, 1.9610e-05, 2.1160e-05, 1.7107e-05, 1.9610e-05,\n",
            "        2.1219e-05, 2.1219e-05, 1.5259e-05, 2.0206e-05, 2.1219e-05, 1.5318e-05,\n",
            "        2.1219e-05, 1.5318e-05, 1.2577e-05, 1.6451e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.8895e-05, 2.2650e-05, 1.9610e-05, 2.1219e-05, 1.7107e-05, 1.9610e-05,\n",
            "        1.9789e-05, 2.1219e-05, 1.5318e-05, 2.0206e-05, 2.1219e-05, 1.5318e-05,\n",
            "        2.1219e-05, 1.5318e-05, 1.2696e-05, 1.6451e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.9848e-05, 2.2650e-05, 1.9610e-05, 2.1219e-05, 1.7107e-05, 2.1219e-05,\n",
            "        1.9789e-05, 1.9610e-05, 1.5259e-05, 2.0206e-05, 2.1219e-05, 1.5318e-05,\n",
            "        1.9610e-05, 1.5318e-05, 1.2875e-05, 1.6212e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.0206e-05, 2.2411e-05, 1.9610e-05, 2.1219e-05, 1.7047e-05, 2.1219e-05,\n",
            "        1.9312e-05, 1.9610e-05, 2.1219e-05, 2.0206e-05, 2.1219e-05, 1.5318e-05,\n",
            "        1.9610e-05, 1.4603e-05, 1.3113e-05, 1.4544e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.0206e-05, 1.9729e-05, 1.9610e-05, 2.1219e-05, 1.7047e-05, 2.1219e-05,\n",
            "        1.9312e-05, 1.9610e-05, 1.4603e-05, 2.0206e-05, 1.9252e-05, 1.4603e-05,\n",
            "        1.9610e-05, 1.4603e-05, 1.3590e-05, 1.6034e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.0206e-05, 2.0742e-05, 1.9610e-05, 2.1219e-05, 1.7226e-05, 1.9610e-05,\n",
            "        1.9312e-05, 1.9610e-05, 1.4782e-05, 1.9252e-05, 1.9312e-05, 1.4603e-05,\n",
            "        1.9610e-05, 1.4603e-05, 1.4484e-05, 1.6272e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.0206e-05, 1.9610e-05, 1.9610e-05, 2.1219e-05, 1.7703e-05, 1.9610e-05,\n",
            "        1.9252e-05, 1.9610e-05, 1.5080e-05, 1.9252e-05, 1.9312e-05, 1.4603e-05,\n",
            "        1.9610e-05, 1.4484e-05, 1.4484e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.0206e-05, 2.0802e-05, 2.1219e-05, 2.1219e-05, 1.6451e-05, 1.9610e-05,\n",
            "        2.0206e-05, 1.9014e-05, 1.5736e-05, 1.9312e-05, 1.9312e-05, 1.4603e-05,\n",
            "        1.9014e-05, 1.4484e-05, 1.5736e-05, 1.5795e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.0206e-05, 1.9610e-05, 2.0206e-05, 2.0921e-05, 1.7047e-05, 1.9610e-05,\n",
            "        2.0206e-05, 2.1398e-05, 1.6630e-05, 1.9312e-05, 1.9789e-05, 1.4722e-05,\n",
            "        1.9014e-05, 1.4544e-05, 1.7107e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.0206e-05, 1.9610e-05, 2.0206e-05, 2.1398e-05, 1.7762e-05, 1.9610e-05,\n",
            "        2.0206e-05, 2.1338e-05, 1.7881e-05, 1.9312e-05, 2.0206e-05, 1.4663e-05,\n",
            "        2.1338e-05, 1.4603e-05, 1.8179e-05, 1.7405e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.0206e-05, 1.9610e-05, 2.0027e-05, 2.0921e-05, 1.8775e-05, 1.9610e-05,\n",
            "        2.0206e-05, 1.9610e-05, 1.9729e-05, 2.0206e-05, 2.0206e-05, 1.4663e-05,\n",
            "        2.1338e-05, 1.4842e-05, 1.9491e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.3530e-05, 1.0490e-05, 1.7524e-05, 1.9252e-05, 1.1981e-05, 1.4007e-05,\n",
            "        1.5616e-05, 1.8179e-05, 1.2696e-05, 1.0073e-05, 1.5616e-05, 1.8775e-05,\n",
            "        1.4484e-05, 2.3007e-05, 1.8418e-05, 2.5153e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.0967e-05, 1.2815e-05, 1.5616e-05, 1.3709e-05, 1.3173e-05, 1.0610e-05,\n",
            "        1.7047e-05, 1.8656e-05, 1.3351e-05, 1.3232e-05, 1.4126e-05, 1.3411e-05,\n",
            "        1.3530e-05, 1.5378e-05, 2.6286e-05, 2.6405e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.3709e-05, 1.6332e-05, 1.6272e-05, 1.2517e-05, 1.2815e-05, 1.2100e-05,\n",
            "        1.8418e-05, 2.0027e-05, 1.2994e-05, 1.2994e-05, 1.3649e-05, 1.5736e-05,\n",
            "        1.2875e-05, 1.8835e-05, 3.2127e-05, 2.0921e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.6570e-05, 1.5497e-05, 1.7464e-05, 1.4961e-05, 1.4186e-05, 1.2934e-05,\n",
            "        1.5378e-05, 1.4246e-05, 1.6153e-05, 1.6570e-05, 1.4722e-05, 1.6868e-05,\n",
            "        1.3590e-05, 1.5080e-05, 2.1160e-05, 1.8895e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.7762e-05, 1.8120e-05, 1.5855e-05, 1.5497e-05, 1.6034e-05, 1.4484e-05,\n",
            "        1.7703e-05, 1.5974e-05, 1.6391e-05, 1.4961e-05, 1.6689e-05, 1.8954e-05,\n",
            "        1.6332e-05, 1.5140e-05, 2.3007e-05, 1.4842e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.7047e-05, 1.8954e-05, 1.6987e-05, 1.6749e-05, 1.7345e-05, 1.5140e-05,\n",
            "        1.6510e-05, 1.7226e-05, 1.8895e-05, 1.6987e-05, 1.6868e-05, 1.9372e-05,\n",
            "        1.6928e-05, 1.4186e-05, 2.0623e-05, 1.6987e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.7643e-05, 1.9133e-05, 1.6093e-05, 1.8954e-05, 1.7345e-05, 1.4544e-05,\n",
            "        1.5080e-05, 1.7345e-05, 1.7941e-05, 1.6093e-05, 1.7047e-05, 1.5855e-05,\n",
            "        1.6093e-05, 1.5616e-05, 1.8060e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.7822e-05, 1.2755e-05, 1.6093e-05, 1.5736e-05, 1.3947e-05, 1.5140e-05,\n",
            "        1.5736e-05, 1.6093e-05, 1.2517e-05, 1.7405e-05, 1.7464e-05, 1.6451e-05,\n",
            "        1.7405e-05, 1.2279e-05, 1.8060e-05, 1.6749e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.7822e-05, 1.2755e-05, 1.7405e-05, 1.6451e-05, 1.3828e-05, 1.7345e-05,\n",
            "        1.6451e-05, 1.6093e-05, 1.2517e-05, 1.7405e-05, 1.7822e-05, 1.6451e-05,\n",
            "        1.7405e-05, 1.2279e-05, 1.8001e-05, 1.6749e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.7822e-05, 1.3590e-05, 1.7405e-05, 1.6451e-05, 1.3828e-05, 1.7345e-05,\n",
            "        1.6451e-05, 1.6093e-05, 1.2517e-05, 1.7405e-05, 1.7822e-05, 1.6451e-05,\n",
            "        1.7405e-05, 1.2279e-05, 1.8001e-05, 1.2815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.7464e-05, 1.3649e-05, 1.7405e-05, 1.6451e-05, 1.4126e-05, 1.5974e-05,\n",
            "        1.6451e-05, 1.7405e-05, 1.4424e-05, 1.7405e-05, 1.7822e-05, 1.6451e-05,\n",
            "        1.7405e-05, 1.2279e-05, 1.8001e-05, 1.2815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.7464e-05, 1.3411e-05, 1.7405e-05, 1.6689e-05, 1.2636e-05, 1.3232e-05,\n",
            "        1.6451e-05, 1.7643e-05, 1.3888e-05, 1.7405e-05, 1.7822e-05, 1.6451e-05,\n",
            "        1.7405e-05, 1.2279e-05, 2.0146e-05, 1.2815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.7822e-05, 1.3828e-05, 1.7405e-05, 1.6689e-05, 1.3053e-05, 1.3232e-05,\n",
            "        1.6451e-05, 1.7643e-05, 1.3173e-05, 1.6093e-05, 1.7822e-05, 1.5736e-05,\n",
            "        1.7405e-05, 1.2279e-05, 1.7643e-05, 1.2815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7822e-05, 1.2636e-05, 1.7405e-05, 1.6689e-05, 1.4484e-05, 1.3232e-05,\n",
            "        1.6689e-05, 1.7643e-05, 1.3292e-05, 1.6093e-05, 1.7464e-05, 1.5736e-05,\n",
            "        1.7405e-05, 1.2279e-05, 1.7405e-05, 1.2815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.7047e-05, 1.2934e-05, 1.6093e-05, 1.5736e-05, 1.4901e-05, 1.3292e-05,\n",
            "        1.5736e-05, 1.7643e-05, 1.3709e-05, 1.6034e-05, 1.6093e-05, 1.5736e-05,\n",
            "        1.6093e-05, 1.2279e-05, 1.7047e-05, 1.2815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.5914e-05, 1.3530e-05, 1.6034e-05, 1.5736e-05, 1.4484e-05, 1.3292e-05,\n",
            "        1.5736e-05, 1.6034e-05, 1.3173e-05, 1.6034e-05, 1.7047e-05, 1.5736e-05,\n",
            "        1.6034e-05, 1.2279e-05, 1.7047e-05, 1.2279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.5914e-05, 1.4186e-05, 1.6034e-05, 1.5080e-05, 1.4782e-05, 1.2994e-05,\n",
            "        1.5736e-05, 1.6034e-05, 1.3590e-05, 1.5497e-05, 1.7047e-05, 1.5736e-05,\n",
            "        1.5736e-05, 1.2279e-05, 1.5914e-05, 1.2279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.5914e-05, 1.4067e-05, 1.6034e-05, 1.5318e-05, 1.5855e-05, 1.2994e-05,\n",
            "        1.5736e-05, 1.6034e-05, 1.4305e-05, 1.5497e-05, 1.5914e-05, 1.6510e-05,\n",
            "        1.5736e-05, 1.2279e-05, 1.5914e-05, 1.2279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.5914e-05, 1.4901e-05, 1.6034e-05, 1.5080e-05, 1.6987e-05, 1.3173e-05,\n",
            "        1.5736e-05, 1.6034e-05, 1.5140e-05, 1.5497e-05, 1.5914e-05, 1.6510e-05,\n",
            "        1.5080e-05, 1.2279e-05, 1.5914e-05, 1.7405e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.5914e-05, 1.5974e-05, 1.6034e-05, 1.5736e-05, 1.7405e-05, 1.3292e-05,\n",
            "        1.6510e-05, 1.6034e-05, 1.6034e-05, 1.6034e-05, 1.5914e-05, 1.6510e-05,\n",
            "        1.5080e-05, 1.2279e-05, 1.5914e-05, 1.2279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.9848e-05, 2.0266e-05, 2.3186e-05, 2.0444e-05, 2.2471e-05, 3.0041e-05,\n",
            "        2.2590e-05, 2.3365e-05, 2.3007e-05, 2.2233e-05, 2.3127e-05, 2.3365e-05,\n",
            "        3.0994e-05, 2.4140e-05, 2.6882e-05, 1.9312e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.6987e-05, 2.3186e-05, 2.0862e-05, 2.3603e-05, 2.0802e-05, 2.8551e-05,\n",
            "        2.0564e-05, 2.1219e-05, 1.8537e-05, 2.4974e-05, 2.7120e-05, 2.5630e-05,\n",
            "        4.1842e-05, 1.7107e-05, 2.7359e-05, 1.6868e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.8358e-05, 2.5094e-05, 2.3305e-05, 2.7716e-05, 2.3484e-05, 2.9027e-05,\n",
            "        2.4259e-05, 2.2590e-05, 2.0802e-05, 2.0087e-05, 2.3603e-05, 2.0385e-05,\n",
            "        3.1054e-05, 1.9968e-05, 2.4557e-05, 1.9133e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.2709e-05, 2.2709e-05, 2.5690e-05, 2.5392e-05, 2.1219e-05, 2.4498e-05,\n",
            "        2.1219e-05, 2.3007e-05, 2.2709e-05, 2.2471e-05, 2.0325e-05, 2.3007e-05,\n",
            "        1.9312e-05, 2.2650e-05, 2.2590e-05, 2.3007e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.5630e-05, 2.3305e-05, 2.8193e-05, 2.3723e-05, 2.4676e-05, 2.7418e-05,\n",
            "        2.3067e-05, 2.4140e-05, 2.5451e-05, 2.4676e-05, 2.2829e-05, 2.5451e-05,\n",
            "        2.0683e-05, 2.5868e-05, 2.1219e-05, 2.6762e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.3365e-05, 2.4021e-05, 2.6941e-05, 2.5630e-05, 2.6941e-05, 3.0160e-05,\n",
            "        2.3365e-05, 2.6643e-05, 2.8431e-05, 2.7597e-05, 2.4438e-05, 2.8551e-05,\n",
            "        2.1160e-05, 2.9445e-05, 2.3007e-05, 3.0458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.3663e-05, 2.4319e-05, 2.6941e-05, 2.4319e-05, 2.6941e-05, 2.6226e-05,\n",
            "        2.2888e-05, 2.6643e-05, 2.8551e-05, 2.6643e-05, 1.7285e-05, 2.6941e-05,\n",
            "        2.0087e-05, 2.9624e-05, 2.3425e-05, 3.0518e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.9491e-05, 2.3663e-05, 1.6868e-05, 2.0623e-05, 1.7762e-05, 2.0981e-05,\n",
            "        2.1636e-05, 1.7107e-05, 2.0444e-05, 1.7107e-05, 1.4901e-05, 1.7762e-05,\n",
            "        2.1636e-05, 1.7703e-05, 2.2650e-05, 2.1636e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.7524e-05, 2.3663e-05, 1.8060e-05, 2.0623e-05, 1.9133e-05, 1.9133e-05,\n",
            "        2.2650e-05, 1.7107e-05, 1.6868e-05, 1.7107e-05, 1.4901e-05, 1.6868e-05,\n",
            "        1.7583e-05, 1.6749e-05, 2.2650e-05, 1.9848e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.8418e-05, 2.3305e-05, 1.8060e-05, 2.0623e-05, 1.6868e-05, 1.9133e-05,\n",
            "        2.2650e-05, 1.7107e-05, 1.6868e-05, 1.7107e-05, 1.4961e-05, 1.6868e-05,\n",
            "        1.8060e-05, 1.6749e-05, 1.9729e-05, 1.9848e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.5676e-05, 2.3305e-05, 1.8060e-05, 2.0623e-05, 1.6868e-05, 1.9133e-05,\n",
            "        2.2650e-05, 1.7107e-05, 1.6868e-05, 1.6630e-05, 1.4961e-05, 1.6868e-05,\n",
            "        1.8954e-05, 1.8895e-05, 1.9729e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.5676e-05, 2.1398e-05, 1.7822e-05, 2.0623e-05, 1.6630e-05, 1.8775e-05,\n",
            "        2.0564e-05, 1.6630e-05, 1.6630e-05, 1.6630e-05, 1.4961e-05, 1.6630e-05,\n",
            "        1.9908e-05, 1.8895e-05, 2.0564e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.5616e-05, 1.9431e-05, 1.6630e-05, 2.0623e-05, 1.6630e-05, 1.6570e-05,\n",
            "        2.0564e-05, 1.6630e-05, 1.6630e-05, 1.6630e-05, 1.4961e-05, 1.6630e-05,\n",
            "        1.7047e-05, 1.8895e-05, 2.0564e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.6212e-05, 1.9431e-05, 1.6630e-05, 2.0623e-05, 1.6630e-05, 1.6570e-05,\n",
            "        2.0564e-05, 1.6451e-05, 1.6630e-05, 1.6451e-05, 1.4961e-05, 1.6630e-05,\n",
            "        1.8299e-05, 1.6212e-05, 2.0564e-05, 1.6689e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.6093e-05, 1.9431e-05, 1.6630e-05, 2.1100e-05, 1.6630e-05, 1.6212e-05,\n",
            "        1.8716e-05, 1.6451e-05, 1.6630e-05, 1.6451e-05, 1.4961e-05, 1.6034e-05,\n",
            "        2.0027e-05, 1.6212e-05, 2.0564e-05, 1.6749e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.6332e-05, 1.9431e-05, 1.6630e-05, 2.1100e-05, 1.6034e-05, 1.6212e-05,\n",
            "        1.8179e-05, 1.9372e-05, 1.6034e-05, 1.6451e-05, 1.4961e-05, 1.6034e-05,\n",
            "        2.0206e-05, 1.6212e-05, 1.9729e-05, 1.6749e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.6570e-05, 1.9431e-05, 1.6570e-05, 2.1100e-05, 1.6034e-05, 1.6332e-05,\n",
            "        1.8179e-05, 1.6391e-05, 1.6093e-05, 1.6451e-05, 1.5080e-05, 1.6034e-05,\n",
            "        2.0206e-05, 1.6212e-05, 1.8179e-05, 1.6809e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.6749e-05, 1.8775e-05, 1.6689e-05, 2.0802e-05, 1.6093e-05, 1.6391e-05,\n",
            "        1.8179e-05, 1.6570e-05, 1.6153e-05, 1.9431e-05, 1.5199e-05, 1.6093e-05,\n",
            "        2.0206e-05, 1.5616e-05, 1.8179e-05, 1.6868e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.6809e-05, 1.8537e-05, 1.6332e-05, 1.9968e-05, 1.6212e-05, 1.6451e-05,\n",
            "        1.7941e-05, 1.6630e-05, 1.6272e-05, 1.8716e-05, 1.5497e-05, 1.6212e-05,\n",
            "        1.9670e-05, 1.5795e-05, 1.8179e-05, 1.6987e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.6630e-05, 1.8537e-05, 1.6689e-05, 2.0802e-05, 1.6809e-05, 1.6689e-05,\n",
            "        1.7941e-05, 1.6987e-05, 1.6630e-05, 1.6272e-05, 1.6034e-05, 1.6868e-05,\n",
            "        1.9670e-05, 1.5855e-05, 1.7941e-05, 1.7226e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.9550e-05, 1.9848e-05, 1.5020e-05, 2.0981e-05, 1.1444e-05, 1.4246e-05,\n",
            "        2.3782e-05, 1.5438e-05, 2.2471e-05, 1.4126e-05, 2.3186e-05, 2.3603e-05,\n",
            "        1.7941e-05, 1.9312e-05, 1.5557e-05, 1.7047e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.8775e-05, 1.7703e-05, 1.4901e-05, 2.4676e-05, 1.1861e-05, 1.5616e-05,\n",
            "        1.7345e-05, 1.7285e-05, 2.1100e-05, 1.6153e-05, 2.3723e-05, 2.4855e-05,\n",
            "        1.6510e-05, 1.3173e-05, 1.6689e-05, 1.2577e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.6689e-05, 1.6332e-05, 1.6809e-05, 3.4034e-05, 1.3292e-05, 1.4424e-05,\n",
            "        1.7643e-05, 1.9610e-05, 2.2411e-05, 1.3769e-05, 2.3007e-05, 1.9789e-05,\n",
            "        1.8001e-05, 1.3947e-05, 1.7285e-05, 1.5616e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.6451e-05, 1.5557e-05, 1.8418e-05, 2.6107e-05, 1.6153e-05, 1.6212e-05,\n",
            "        1.4424e-05, 1.6153e-05, 1.7047e-05, 1.5199e-05, 2.5630e-05, 2.0206e-05,\n",
            "        1.6391e-05, 1.7226e-05, 1.4484e-05, 1.9073e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.7881e-05, 1.7405e-05, 2.2054e-05, 1.6212e-05, 1.6570e-05, 1.5855e-05,\n",
            "        1.5020e-05, 1.6630e-05, 1.5557e-05, 1.6928e-05, 1.6987e-05, 2.2769e-05,\n",
            "        1.8477e-05, 1.7226e-05, 1.6689e-05, 1.6987e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.7047e-05, 1.9372e-05, 1.8775e-05, 1.9073e-05, 1.8239e-05, 1.7166e-05,\n",
            "        1.6809e-05, 1.8239e-05, 1.6809e-05, 2.0981e-05, 1.8179e-05, 2.0444e-05,\n",
            "        1.6749e-05, 1.9550e-05, 1.9968e-05, 1.6153e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.7226e-05, 1.9670e-05, 1.8358e-05, 1.7524e-05, 1.8239e-05, 1.7226e-05,\n",
            "        1.8179e-05, 1.8775e-05, 1.6987e-05, 1.8060e-05, 1.8418e-05, 2.0683e-05,\n",
            "        1.6868e-05, 1.9252e-05, 2.0444e-05, 1.6689e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.8239e-05, 1.8001e-05, 1.8835e-05, 1.3530e-05, 1.2517e-05, 1.8001e-05,\n",
            "        1.2577e-05, 1.9193e-05, 1.7405e-05, 1.2517e-05, 1.8418e-05, 1.7226e-05,\n",
            "        1.7166e-05, 1.2517e-05, 1.8358e-05, 1.8477e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.8239e-05, 1.8001e-05, 1.8835e-05, 1.3769e-05, 1.2517e-05, 1.8239e-05,\n",
            "        1.2577e-05, 1.8656e-05, 1.7583e-05, 1.2517e-05, 1.8537e-05, 1.8239e-05,\n",
            "        1.7405e-05, 1.4782e-05, 1.8358e-05, 1.8001e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.8239e-05, 1.8001e-05, 1.8835e-05, 1.3769e-05, 1.2517e-05, 1.8239e-05,\n",
            "        1.2577e-05, 1.8656e-05, 1.7405e-05, 1.2457e-05, 1.8537e-05, 1.8239e-05,\n",
            "        1.7405e-05, 1.3292e-05, 1.7226e-05, 1.8001e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.8239e-05, 1.8239e-05, 1.8835e-05, 1.3530e-05, 1.3292e-05, 1.8239e-05,\n",
            "        1.2577e-05, 1.8656e-05, 1.7405e-05, 1.2457e-05, 1.8537e-05, 1.8239e-05,\n",
            "        1.7405e-05, 1.3292e-05, 1.7226e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.8239e-05, 1.8239e-05, 1.6093e-05, 1.3530e-05, 1.4246e-05, 1.8239e-05,\n",
            "        1.2577e-05, 1.7345e-05, 1.7405e-05, 1.2457e-05, 1.9729e-05, 1.8239e-05,\n",
            "        1.7405e-05, 1.3292e-05, 1.7226e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.7226e-05, 1.8239e-05, 1.6093e-05, 1.3530e-05, 1.3292e-05, 1.8239e-05,\n",
            "        1.2577e-05, 1.7345e-05, 1.7405e-05, 1.2457e-05, 1.9729e-05, 1.8239e-05,\n",
            "        1.7405e-05, 1.3292e-05, 1.8239e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7226e-05, 1.7226e-05, 1.6093e-05, 1.3530e-05, 1.2517e-05, 1.7226e-05,\n",
            "        1.2577e-05, 1.8001e-05, 1.7405e-05, 1.2696e-05, 1.8239e-05, 1.8239e-05,\n",
            "        1.7405e-05, 1.2517e-05, 1.8239e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.7226e-05, 1.7226e-05, 1.6093e-05, 1.3530e-05, 1.2636e-05, 1.8477e-05,\n",
            "        1.2577e-05, 1.5676e-05, 1.7405e-05, 1.2755e-05, 1.8239e-05, 1.7226e-05,\n",
            "        1.7405e-05, 1.2577e-05, 1.7226e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7226e-05, 1.7226e-05, 1.4126e-05, 1.3530e-05, 1.2755e-05, 1.8477e-05,\n",
            "        1.2577e-05, 1.6391e-05, 1.7405e-05, 1.3411e-05, 1.8239e-05, 1.7226e-05,\n",
            "        1.6928e-05, 1.2577e-05, 1.7226e-05, 1.7226e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.7226e-05, 1.7226e-05, 1.5676e-05, 1.3530e-05, 1.3828e-05, 1.8239e-05,\n",
            "        1.2577e-05, 1.7464e-05, 1.6928e-05, 1.3292e-05, 1.7226e-05, 1.7226e-05,\n",
            "        1.6928e-05, 1.3351e-05, 1.7226e-05, 1.7226e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.7226e-05, 1.7226e-05, 1.5736e-05, 1.3530e-05, 1.5497e-05, 1.7405e-05,\n",
            "        1.2577e-05, 1.8835e-05, 1.6928e-05, 1.2338e-05, 1.7226e-05, 1.7226e-05,\n",
            "        1.6928e-05, 1.3709e-05, 1.8239e-05, 1.7226e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.8239e-05, 1.8239e-05, 1.4126e-05, 1.3530e-05, 1.6153e-05, 1.7405e-05,\n",
            "        1.2577e-05, 2.0027e-05, 1.6928e-05, 1.2815e-05, 1.7226e-05, 1.7226e-05,\n",
            "        1.6928e-05, 1.4126e-05, 1.8239e-05, 1.8477e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.7405e-05, 1.8239e-05, 1.4126e-05, 1.3053e-05, 1.6570e-05, 1.7405e-05,\n",
            "        1.2577e-05, 2.0027e-05, 1.6928e-05, 1.1683e-05, 1.6809e-05, 1.7226e-05,\n",
            "        1.7405e-05, 1.4842e-05, 1.7226e-05, 1.8477e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.1425e-05, 2.7597e-05, 3.9756e-05, 3.7849e-05, 3.2306e-05, 3.1888e-05,\n",
            "        4.3392e-05, 2.6822e-05, 3.1352e-05, 3.4451e-05, 3.3736e-05, 3.5763e-05,\n",
            "        3.3915e-05, 2.8670e-05, 3.8981e-05, 2.9206e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.1888e-05, 2.7478e-05, 4.5002e-05, 3.4213e-05, 3.3975e-05, 3.2723e-05,\n",
            "        3.2246e-05, 2.8670e-05, 3.2604e-05, 2.8908e-05, 3.0816e-05, 2.9027e-05,\n",
            "        2.8551e-05, 2.6166e-05, 3.5286e-05, 3.2544e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.4498e-05, 3.1829e-05, 3.8207e-05, 3.1292e-05, 2.8968e-05, 3.2723e-05,\n",
            "        2.8253e-05, 2.9266e-05, 2.9027e-05, 3.1114e-05, 3.3557e-05, 3.1352e-05,\n",
            "        3.0279e-05, 2.8312e-05, 3.9697e-05, 3.2187e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.4676e-05, 3.1590e-05, 3.7849e-05, 3.3438e-05, 3.1352e-05, 3.7730e-05,\n",
            "        3.2306e-05, 3.1769e-05, 2.9683e-05, 2.9802e-05, 3.4094e-05, 3.4690e-05,\n",
            "        2.9802e-05, 2.9624e-05, 3.2663e-05, 3.2663e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.6703e-05, 3.4571e-05, 3.4869e-05, 3.4213e-05, 3.4094e-05, 3.7134e-05,\n",
            "        3.6597e-05, 3.4451e-05, 3.2842e-05, 3.3259e-05, 3.4153e-05, 3.2485e-05,\n",
            "        2.8133e-05, 3.2485e-05, 3.5942e-05, 3.5226e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.8491e-05, 3.7730e-05, 3.9279e-05, 3.7551e-05, 3.7551e-05, 4.2140e-05,\n",
            "        4.1664e-05, 3.7670e-05, 3.6716e-05, 3.0696e-05, 3.7730e-05, 3.7313e-05,\n",
            "        3.0935e-05, 3.7491e-05, 3.7551e-05, 3.5703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.0683e-05, 3.8087e-05, 2.5690e-05, 2.4736e-05, 3.7551e-05, 3.6001e-05,\n",
            "        2.3961e-05, 3.7968e-05, 3.6955e-05, 3.0100e-05, 3.7551e-05, 2.1696e-05,\n",
            "        3.1412e-05, 3.6478e-05, 2.4736e-05, 3.5346e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.0802e-05, 2.3842e-05, 2.5690e-05, 2.4736e-05, 2.4736e-05, 2.7478e-05,\n",
            "        2.2829e-05, 2.4319e-05, 2.5928e-05, 2.1458e-05, 2.3842e-05, 2.1756e-05,\n",
            "        2.3901e-05, 2.4319e-05, 2.4736e-05, 2.6107e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.1040e-05, 2.3842e-05, 2.2411e-05, 2.4736e-05, 2.3842e-05, 2.6762e-05,\n",
            "        2.1100e-05, 2.4319e-05, 2.5928e-05, 2.2054e-05, 2.3842e-05, 2.1100e-05,\n",
            "        2.5034e-05, 2.4319e-05, 2.3842e-05, 2.6464e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.1398e-05, 2.3127e-05, 2.1756e-05, 2.3842e-05, 2.3842e-05, 2.6762e-05,\n",
            "        2.1100e-05, 2.4319e-05, 2.2352e-05, 2.2948e-05, 2.3842e-05, 2.1160e-05,\n",
            "        2.4199e-05, 2.3305e-05, 2.3842e-05, 2.7120e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.1517e-05, 2.3127e-05, 2.1756e-05, 2.3842e-05, 2.3842e-05, 2.6762e-05,\n",
            "        2.1160e-05, 2.4319e-05, 2.1756e-05, 2.4080e-05, 2.3842e-05, 2.1577e-05,\n",
            "        2.3782e-05, 2.3305e-05, 2.3127e-05, 2.1636e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.2173e-05, 2.3127e-05, 2.1756e-05, 2.3842e-05, 2.3842e-05, 2.5213e-05,\n",
            "        2.1458e-05, 2.4319e-05, 2.1756e-05, 2.5451e-05, 2.3842e-05, 2.2054e-05,\n",
            "        2.5570e-05, 2.1756e-05, 2.3127e-05, 2.1756e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.0504e-05, 2.3127e-05, 2.1756e-05, 2.3127e-05, 2.3842e-05, 2.2709e-05,\n",
            "        2.1935e-05, 2.4319e-05, 2.1756e-05, 2.7299e-05, 2.3842e-05, 2.1219e-05,\n",
            "        2.5511e-05, 2.1756e-05, 2.3127e-05, 2.1875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.0802e-05, 2.2113e-05, 2.1756e-05, 2.3127e-05, 2.3007e-05, 2.2709e-05,\n",
            "        2.1517e-05, 2.2650e-05, 2.1756e-05, 2.6941e-05, 2.2113e-05, 2.1696e-05,\n",
            "        2.3007e-05, 2.1756e-05, 2.3127e-05, 2.1577e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.0802e-05, 2.2113e-05, 2.1756e-05, 2.2113e-05, 2.2113e-05, 2.2650e-05,\n",
            "        2.1458e-05, 2.2650e-05, 2.1338e-05, 2.4319e-05, 2.2113e-05, 2.0266e-05,\n",
            "        2.2352e-05, 2.1338e-05, 2.2113e-05, 2.0266e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.0802e-05, 2.2113e-05, 2.1338e-05, 2.2113e-05, 2.2113e-05, 2.2411e-05,\n",
            "        2.1458e-05, 2.2650e-05, 2.1338e-05, 2.0981e-05, 2.2113e-05, 1.9550e-05,\n",
            "        2.2352e-05, 2.1338e-05, 2.2113e-05, 2.0862e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.0802e-05, 2.2113e-05, 2.0802e-05, 2.2113e-05, 2.2113e-05, 2.2590e-05,\n",
            "        1.9491e-05, 2.2650e-05, 2.0802e-05, 2.0981e-05, 2.2113e-05, 2.0266e-05,\n",
            "        2.1338e-05, 2.0802e-05, 2.2113e-05, 2.1815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.0802e-05, 2.2113e-05, 2.0802e-05, 2.2113e-05, 2.2113e-05, 2.1636e-05,\n",
            "        2.0266e-05, 2.2650e-05, 2.0802e-05, 2.1100e-05, 2.2113e-05, 2.0504e-05,\n",
            "        2.1338e-05, 2.0802e-05, 2.2113e-05, 2.2054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.1338e-05, 2.2113e-05, 2.0802e-05, 2.2113e-05, 2.2113e-05, 2.1636e-05,\n",
            "        2.0504e-05, 2.2650e-05, 2.0802e-05, 2.1160e-05, 2.2113e-05, 2.1160e-05,\n",
            "        2.1338e-05, 2.0802e-05, 2.2113e-05, 2.2113e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.1517e-05, 2.2113e-05, 2.0802e-05, 2.1398e-05, 2.2113e-05, 2.1577e-05,\n",
            "        2.1160e-05, 2.2650e-05, 2.0802e-05, 2.1160e-05, 2.2113e-05, 2.1160e-05,\n",
            "        2.1338e-05, 2.1338e-05, 2.2113e-05, 2.1935e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.7432e-05, 2.9087e-05, 3.3557e-05, 2.4557e-05, 2.9325e-05, 2.6643e-05,\n",
            "        3.8624e-05, 2.9981e-05, 4.4823e-05, 2.8729e-05, 2.6882e-05, 3.4332e-05,\n",
            "        3.6538e-05, 3.3677e-05, 3.3498e-05, 3.5644e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.1471e-05, 3.1233e-05, 2.8968e-05, 2.5511e-05, 2.9266e-05, 2.9206e-05,\n",
            "        3.4630e-05, 3.5107e-05, 4.5776e-05, 3.1650e-05, 3.0816e-05, 3.1590e-05,\n",
            "        3.5644e-05, 2.9147e-05, 2.8610e-05, 3.0100e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.1173e-05, 3.1292e-05, 2.8968e-05, 2.7895e-05, 3.0696e-05, 3.3438e-05,\n",
            "        3.2723e-05, 3.0816e-05, 4.2975e-05, 3.4809e-05, 3.2544e-05, 3.2902e-05,\n",
            "        3.3081e-05, 3.0637e-05, 3.1352e-05, 3.2961e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.2902e-05, 3.4928e-05, 3.0577e-05, 3.0994e-05, 3.2723e-05, 3.2902e-05,\n",
            "        2.9981e-05, 3.1352e-05, 3.2961e-05, 3.1114e-05, 3.6836e-05, 3.0398e-05,\n",
            "        3.7134e-05, 3.1710e-05, 3.3259e-05, 3.5286e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.3259e-05, 3.3736e-05, 3.3855e-05, 3.3736e-05, 3.6836e-05, 3.3975e-05,\n",
            "        3.3379e-05, 3.4332e-05, 3.3677e-05, 3.3975e-05, 3.5405e-05, 3.2544e-05,\n",
            "        3.4034e-05, 3.3200e-05, 3.5942e-05, 3.4153e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.7193e-05, 3.7193e-05, 3.6836e-05, 3.6895e-05, 3.9816e-05, 3.7074e-05,\n",
            "        3.7372e-05, 3.7193e-05, 3.7789e-05, 3.7074e-05, 3.7611e-05, 3.6359e-05,\n",
            "        3.9160e-05, 3.7014e-05, 3.8743e-05, 3.7491e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.3961e-05, 2.3842e-05, 2.5332e-05, 2.3842e-05, 2.6345e-05, 2.3842e-05,\n",
            "        2.4021e-05, 2.3842e-05, 2.4736e-05, 2.3842e-05, 2.4498e-05, 2.3663e-05,\n",
            "        2.5272e-05, 2.3305e-05, 2.5809e-05, 2.3842e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.3961e-05, 2.3842e-05, 2.3842e-05, 2.3842e-05, 2.5928e-05, 2.3842e-05,\n",
            "        2.2292e-05, 2.3842e-05, 2.4736e-05, 2.3842e-05, 2.4259e-05, 2.2292e-05,\n",
            "        2.5272e-05, 2.3305e-05, 2.4498e-05, 2.3842e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.3961e-05, 2.3842e-05, 2.3842e-05, 2.3842e-05, 2.5928e-05, 2.3842e-05,\n",
            "        2.2292e-05, 2.3842e-05, 2.4736e-05, 2.3842e-05, 2.4259e-05, 2.2292e-05,\n",
            "        2.5272e-05, 2.2292e-05, 2.4498e-05, 2.3842e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.3961e-05, 2.3842e-05, 2.3842e-05, 2.3842e-05, 2.3305e-05, 2.3067e-05,\n",
            "        2.2292e-05, 2.3842e-05, 2.2292e-05, 2.3067e-05, 2.3544e-05, 2.2292e-05,\n",
            "        2.5272e-05, 2.2292e-05, 2.4259e-05, 2.3842e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.3961e-05, 2.3067e-05, 2.3067e-05, 2.3067e-05, 2.2888e-05, 2.3067e-05,\n",
            "        2.1875e-05, 2.3067e-05, 2.2292e-05, 2.3067e-05, 2.3544e-05, 2.1875e-05,\n",
            "        2.1756e-05, 2.1875e-05, 2.3544e-05, 2.3067e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.1875e-05, 2.3067e-05, 2.3067e-05, 2.3067e-05, 2.2888e-05, 2.3067e-05,\n",
            "        2.1875e-05, 2.3067e-05, 2.2292e-05, 2.3067e-05, 2.3544e-05, 2.1875e-05,\n",
            "        2.1756e-05, 2.1875e-05, 2.3544e-05, 2.3067e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.1458e-05, 2.2352e-05, 2.2352e-05, 2.2352e-05, 2.2650e-05, 2.2352e-05,\n",
            "        2.1458e-05, 2.2352e-05, 2.1458e-05, 2.2352e-05, 2.2829e-05, 2.1040e-05,\n",
            "        2.1756e-05, 2.1458e-05, 2.2829e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.1458e-05, 2.2352e-05, 2.2352e-05, 2.2352e-05, 2.2650e-05, 2.2352e-05,\n",
            "        2.1458e-05, 2.2352e-05, 2.1458e-05, 2.2352e-05, 2.2829e-05, 2.1040e-05,\n",
            "        2.1517e-05, 2.1458e-05, 2.2829e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.1458e-05, 2.2352e-05, 2.2352e-05, 2.2352e-05, 2.2650e-05, 2.2352e-05,\n",
            "        2.1458e-05, 2.2352e-05, 2.1458e-05, 2.2352e-05, 2.2829e-05, 2.1040e-05,\n",
            "        2.1458e-05, 2.1458e-05, 2.2829e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.1458e-05, 2.2352e-05, 2.2352e-05, 2.2352e-05, 2.2650e-05, 2.2352e-05,\n",
            "        2.1458e-05, 2.2352e-05, 2.1458e-05, 2.2352e-05, 2.2829e-05, 2.1458e-05,\n",
            "        2.1458e-05, 2.1040e-05, 2.2829e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.1458e-05, 2.2352e-05, 2.2352e-05, 2.2352e-05, 2.1815e-05, 2.2352e-05,\n",
            "        2.1040e-05, 2.2352e-05, 2.1458e-05, 2.2352e-05, 2.2829e-05, 2.1458e-05,\n",
            "        2.1458e-05, 2.1040e-05, 2.2829e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.1458e-05, 2.2352e-05, 2.2352e-05, 2.2352e-05, 2.1815e-05, 2.2352e-05,\n",
            "        2.1040e-05, 2.2352e-05, 2.1458e-05, 2.2352e-05, 2.2829e-05, 2.1458e-05,\n",
            "        2.1458e-05, 2.1040e-05, 2.2829e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.1458e-05, 2.2352e-05, 2.2352e-05, 2.2352e-05, 2.1815e-05, 2.2352e-05,\n",
            "        2.1040e-05, 2.2352e-05, 2.1458e-05, 2.2352e-05, 2.2829e-05, 2.1458e-05,\n",
            "        2.1458e-05, 2.1040e-05, 2.2829e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.1458e-05, 2.2352e-05, 2.2352e-05, 2.1517e-05, 2.1875e-05, 2.2054e-05,\n",
            "        2.1040e-05, 2.2352e-05, 2.1458e-05, 2.2352e-05, 2.2829e-05, 2.1458e-05,\n",
            "        2.1040e-05, 2.1458e-05, 2.2113e-05, 2.2352e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.4736e-05, 2.2888e-05, 2.1219e-05, 2.1756e-05, 2.9147e-05, 2.3901e-05,\n",
            "        3.2663e-05, 2.5570e-05, 2.1219e-05, 2.1398e-05, 2.5928e-05, 2.1517e-05,\n",
            "        2.5153e-05, 2.2113e-05, 2.4021e-05, 2.3425e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.2650e-05, 2.4378e-05, 2.0862e-05, 2.5332e-05, 2.0564e-05, 2.3544e-05,\n",
            "        2.5868e-05, 2.9027e-05, 2.0921e-05, 2.2531e-05, 2.1160e-05, 2.1338e-05,\n",
            "        1.9670e-05, 2.1994e-05, 2.5451e-05, 2.2590e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.1279e-05, 2.2769e-05, 2.7359e-05, 2.1279e-05, 2.2888e-05, 2.4676e-05,\n",
            "        2.2769e-05, 2.5809e-05, 2.3425e-05, 2.2829e-05, 2.5213e-05, 2.1517e-05,\n",
            "        2.1636e-05, 2.1994e-05, 2.4438e-05, 2.3305e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.4080e-05, 2.2054e-05, 2.2769e-05, 2.3603e-05, 2.3067e-05, 2.2054e-05,\n",
            "        2.2411e-05, 2.4259e-05, 2.6166e-05, 2.1935e-05, 2.2292e-05, 2.3305e-05,\n",
            "        2.3901e-05, 2.3246e-05, 2.2888e-05, 2.2888e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.4736e-05, 2.1935e-05, 2.4557e-05, 2.5988e-05, 2.5868e-05, 2.4557e-05,\n",
            "        2.5511e-05, 2.5988e-05, 2.6524e-05, 2.5511e-05, 2.1994e-05, 2.5392e-05,\n",
            "        2.6166e-05, 2.5332e-05, 2.5153e-05, 2.5094e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.5809e-05, 2.4021e-05, 2.4557e-05, 2.7657e-05, 2.9743e-05, 2.7657e-05,\n",
            "        2.9087e-05, 2.7835e-05, 3.0458e-05, 2.9087e-05, 2.4259e-05, 2.7955e-05,\n",
            "        2.7776e-05, 2.7895e-05, 2.8610e-05, 2.7657e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.8656e-05, 2.5511e-05, 1.9491e-05, 1.7583e-05, 2.0683e-05, 1.7643e-05,\n",
            "        1.7762e-05, 1.8001e-05, 1.9789e-05, 1.8895e-05, 1.8537e-05, 1.7583e-05,\n",
            "        1.9014e-05, 1.9014e-05, 1.8537e-05, 1.7583e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.6809e-05, 2.4199e-05, 2.0206e-05, 1.7583e-05, 1.9014e-05, 1.6749e-05,\n",
            "        1.7762e-05, 1.8001e-05, 1.8656e-05, 1.6749e-05, 1.8239e-05, 1.7583e-05,\n",
            "        1.9014e-05, 1.8001e-05, 1.8895e-05, 1.7583e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.6809e-05, 1.9610e-05, 1.8895e-05, 1.7583e-05, 1.9014e-05, 1.6749e-05,\n",
            "        1.6510e-05, 1.7583e-05, 1.8656e-05, 1.6749e-05, 1.8060e-05, 1.7583e-05,\n",
            "        1.8001e-05, 1.8001e-05, 1.8895e-05, 1.7583e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.6809e-05, 1.9610e-05, 1.6749e-05, 1.7047e-05, 1.9014e-05, 1.6749e-05,\n",
            "        1.5616e-05, 1.7047e-05, 1.6332e-05, 1.6749e-05, 1.8120e-05, 1.7047e-05,\n",
            "        1.8001e-05, 1.7405e-05, 1.8895e-05, 1.7047e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.6809e-05, 1.9610e-05, 1.6749e-05, 1.7047e-05, 1.6332e-05, 1.6391e-05,\n",
            "        1.5676e-05, 1.7047e-05, 1.8418e-05, 1.6391e-05, 1.8597e-05, 1.7047e-05,\n",
            "        1.7405e-05, 1.7405e-05, 1.6391e-05, 1.7047e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.6332e-05, 1.9610e-05, 1.6749e-05, 1.7047e-05, 1.6332e-05, 1.6391e-05,\n",
            "        1.5795e-05, 1.7047e-05, 1.8418e-05, 1.6391e-05, 1.6391e-05, 1.7047e-05,\n",
            "        1.7405e-05, 1.7405e-05, 1.6391e-05, 1.7047e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.6332e-05, 1.9610e-05, 1.6749e-05, 1.7047e-05, 1.6332e-05, 1.6391e-05,\n",
            "        1.6272e-05, 1.7047e-05, 1.8418e-05, 1.7583e-05, 1.5974e-05, 1.7047e-05,\n",
            "        1.7405e-05, 1.7405e-05, 1.6391e-05, 1.7047e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.8656e-05, 2.0146e-05, 1.6153e-05, 1.6510e-05, 1.6332e-05, 1.6153e-05,\n",
            "        1.6153e-05, 1.6510e-05, 1.6391e-05, 1.7583e-05, 1.5736e-05, 1.6510e-05,\n",
            "        1.6928e-05, 1.6928e-05, 1.6391e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.5974e-05, 2.0146e-05, 1.6153e-05, 1.6510e-05, 1.5974e-05, 1.5736e-05,\n",
            "        1.6451e-05, 1.6510e-05, 1.6153e-05, 1.6153e-05, 1.5736e-05, 1.6510e-05,\n",
            "        1.6928e-05, 1.6928e-05, 1.6153e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.5974e-05, 2.0146e-05, 1.6153e-05, 1.6510e-05, 1.5974e-05, 1.5736e-05,\n",
            "        1.4842e-05, 1.6510e-05, 1.6153e-05, 1.6153e-05, 1.6153e-05, 1.6510e-05,\n",
            "        1.6928e-05, 1.6928e-05, 1.6153e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.5974e-05, 1.9610e-05, 1.6272e-05, 1.6510e-05, 1.5974e-05, 1.5736e-05,\n",
            "        1.5497e-05, 1.6510e-05, 1.6153e-05, 1.5736e-05, 1.6272e-05, 1.6510e-05,\n",
            "        1.6928e-05, 1.6928e-05, 1.6153e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.5974e-05, 1.8895e-05, 1.6332e-05, 1.6510e-05, 1.5974e-05, 1.6153e-05,\n",
            "        1.5497e-05, 1.6510e-05, 1.6153e-05, 1.5736e-05, 1.6391e-05, 1.6510e-05,\n",
            "        1.6928e-05, 1.6928e-05, 1.6153e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.5974e-05, 1.8895e-05, 1.6570e-05, 1.6510e-05, 1.5974e-05, 1.6153e-05,\n",
            "        1.5497e-05, 1.6510e-05, 1.5736e-05, 1.5736e-05, 1.6391e-05, 1.6510e-05,\n",
            "        1.6391e-05, 1.6987e-05, 1.6153e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.5497e-05, 1.8895e-05, 1.6570e-05, 1.6510e-05, 1.5497e-05, 1.6153e-05,\n",
            "        1.5497e-05, 1.6510e-05, 1.5855e-05, 1.6272e-05, 1.6689e-05, 1.6212e-05,\n",
            "        1.6332e-05, 1.6987e-05, 1.6153e-05, 1.6510e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.8312e-05, 2.5511e-05, 2.2471e-05, 2.9802e-05, 2.6822e-05, 2.4796e-05,\n",
            "        2.0921e-05, 2.1756e-05, 2.5094e-05, 2.1458e-05, 4.1425e-05, 2.3901e-05,\n",
            "        3.0339e-05, 2.8729e-05, 2.6941e-05, 2.0027e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.7895e-05, 2.4199e-05, 2.3603e-05, 2.7597e-05, 2.0623e-05, 2.3186e-05,\n",
            "        2.2650e-05, 2.3305e-05, 2.8789e-05, 1.9312e-05, 3.6180e-05, 2.5272e-05,\n",
            "        2.8074e-05, 2.7180e-05, 3.1650e-05, 2.3067e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.9147e-05, 2.5153e-05, 2.2888e-05, 2.2352e-05, 2.0087e-05, 2.7299e-05,\n",
            "        2.4438e-05, 2.3782e-05, 2.3365e-05, 2.1219e-05, 2.5451e-05, 2.2292e-05,\n",
            "        2.3246e-05, 2.5988e-05, 2.8610e-05, 2.2411e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.3425e-05, 2.4498e-05, 2.4676e-05, 2.3842e-05, 2.2531e-05, 2.7478e-05,\n",
            "        2.3484e-05, 2.4855e-05, 2.7776e-05, 2.3782e-05, 2.1696e-05, 2.5153e-05,\n",
            "        2.5034e-05, 2.5392e-05, 2.7895e-05, 2.4736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.6047e-05, 2.6762e-05, 2.6941e-05, 2.6464e-05, 2.5570e-05, 2.6762e-05,\n",
            "        2.6762e-05, 2.7716e-05, 2.5392e-05, 2.6822e-05, 2.4080e-05, 2.9862e-05,\n",
            "        2.7895e-05, 2.7716e-05, 2.7239e-05, 2.7239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.8670e-05, 3.0041e-05, 2.9504e-05, 2.5451e-05, 2.9445e-05, 2.9981e-05,\n",
            "        3.0160e-05, 3.0458e-05, 2.8968e-05, 3.0041e-05, 2.7001e-05, 3.3200e-05,\n",
            "        3.0279e-05, 3.0577e-05, 2.9504e-05, 2.9802e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.9550e-05, 2.0027e-05, 1.8954e-05, 1.6630e-05, 1.9372e-05, 2.0027e-05,\n",
            "        1.7822e-05, 1.9252e-05, 2.1040e-05, 1.7822e-05, 2.8729e-05, 2.0027e-05,\n",
            "        1.9789e-05, 2.0146e-05, 1.8954e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.8537e-05, 1.7822e-05, 1.8954e-05, 1.6868e-05, 1.9372e-05, 2.0027e-05,\n",
            "        1.7822e-05, 1.9252e-05, 1.8716e-05, 1.7822e-05, 3.0756e-05, 2.0027e-05,\n",
            "        1.9789e-05, 1.9312e-05, 1.8954e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.8537e-05, 1.7822e-05, 1.8239e-05, 1.7107e-05, 1.7822e-05, 2.0027e-05,\n",
            "        1.7822e-05, 1.8954e-05, 1.8716e-05, 1.7822e-05, 2.3365e-05, 2.0027e-05,\n",
            "        1.8954e-05, 1.9312e-05, 1.8954e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.8656e-05, 2.0385e-05, 1.8239e-05, 1.7643e-05, 1.7464e-05, 1.7822e-05,\n",
            "        1.7822e-05, 1.8239e-05, 1.8716e-05, 1.7822e-05, 2.0087e-05, 1.9312e-05,\n",
            "        1.8239e-05, 1.9312e-05, 1.8239e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.8477e-05, 1.7464e-05, 1.8239e-05, 1.7345e-05, 1.7464e-05, 1.7822e-05,\n",
            "        1.7464e-05, 1.8239e-05, 1.8716e-05, 1.6987e-05, 2.0623e-05, 1.9312e-05,\n",
            "        1.8239e-05, 1.8597e-05, 1.8239e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.8954e-05, 1.7464e-05, 1.8239e-05, 1.7285e-05, 1.7464e-05, 1.7464e-05,\n",
            "        1.7464e-05, 1.8239e-05, 1.8716e-05, 1.6987e-05, 2.0623e-05, 1.8239e-05,\n",
            "        1.8239e-05, 1.8597e-05, 1.8239e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.7464e-05, 1.7464e-05, 1.8239e-05, 1.6034e-05, 1.7464e-05, 1.7464e-05,\n",
            "        1.7464e-05, 1.8239e-05, 1.7583e-05, 1.6987e-05, 2.0623e-05, 1.8239e-05,\n",
            "        1.8239e-05, 1.8597e-05, 1.8239e-05, 1.8239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7285e-05, 1.7285e-05, 1.7643e-05, 1.6570e-05, 1.7464e-05, 1.7285e-05,\n",
            "        1.7285e-05, 1.7643e-05, 1.7524e-05, 1.7285e-05, 2.0623e-05, 1.7643e-05,\n",
            "        1.7643e-05, 1.8120e-05, 1.7643e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.7285e-05, 1.7285e-05, 1.7643e-05, 1.6451e-05, 1.7285e-05, 1.7285e-05,\n",
            "        1.7285e-05, 1.7643e-05, 1.7524e-05, 1.7285e-05, 2.0623e-05, 1.7643e-05,\n",
            "        1.7643e-05, 1.8120e-05, 1.7643e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7285e-05, 1.7285e-05, 1.7643e-05, 1.6451e-05, 1.7285e-05, 1.7285e-05,\n",
            "        1.7285e-05, 1.7643e-05, 1.7583e-05, 1.7285e-05, 2.0623e-05, 1.7643e-05,\n",
            "        1.7643e-05, 1.8120e-05, 1.7643e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.7285e-05, 1.7285e-05, 1.7643e-05, 1.6868e-05, 1.6928e-05, 1.7285e-05,\n",
            "        1.7285e-05, 1.7643e-05, 1.7643e-05, 1.6928e-05, 2.0087e-05, 1.7643e-05,\n",
            "        1.7643e-05, 1.8120e-05, 1.7643e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.7285e-05, 1.7285e-05, 1.7643e-05, 1.6868e-05, 1.6928e-05, 1.7285e-05,\n",
            "        1.7285e-05, 1.7643e-05, 1.7881e-05, 1.6928e-05, 2.0087e-05, 1.7643e-05,\n",
            "        1.7643e-05, 1.8120e-05, 1.7643e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.7285e-05, 1.6928e-05, 1.7703e-05, 1.6868e-05, 1.6868e-05, 1.7285e-05,\n",
            "        1.7285e-05, 1.7643e-05, 1.8239e-05, 1.7285e-05, 2.0087e-05, 1.6928e-05,\n",
            "        1.6928e-05, 1.8299e-05, 1.7643e-05, 1.7643e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.7285e-05, 1.6928e-05, 1.7822e-05, 1.6868e-05, 1.6928e-05, 1.6928e-05,\n",
            "        1.6928e-05, 1.7703e-05, 1.8656e-05, 1.7405e-05, 2.0087e-05, 1.6928e-05,\n",
            "        1.6928e-05, 1.7524e-05, 1.7822e-05, 1.7822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.3067e-05, 2.1279e-05, 2.0444e-05, 1.9312e-05, 2.2948e-05, 2.4557e-05,\n",
            "        1.8895e-05, 2.4378e-05, 1.8597e-05, 2.4199e-05, 1.9491e-05, 1.6809e-05,\n",
            "        1.8775e-05, 1.8179e-05, 1.6332e-05, 2.6464e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.7226e-05, 2.3007e-05, 2.2113e-05, 1.7822e-05, 1.8954e-05, 2.3544e-05,\n",
            "        2.0564e-05, 2.0564e-05, 1.5855e-05, 2.5213e-05, 1.7583e-05, 1.5438e-05,\n",
            "        2.0683e-05, 1.6630e-05, 1.5199e-05, 3.1054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.6809e-05, 1.8895e-05, 2.1636e-05, 1.9908e-05, 1.8299e-05, 2.2352e-05,\n",
            "        2.3842e-05, 2.0683e-05, 1.7405e-05, 1.8835e-05, 1.8775e-05, 1.7881e-05,\n",
            "        2.2054e-05, 1.7762e-05, 1.7881e-05, 2.6822e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.8358e-05, 1.9252e-05, 1.9610e-05, 1.9789e-05, 2.0146e-05, 2.0862e-05,\n",
            "        2.1040e-05, 2.3007e-05, 1.7881e-05, 1.9491e-05, 1.6689e-05, 1.7226e-05,\n",
            "        2.1338e-05, 2.1338e-05, 2.0444e-05, 1.6212e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.6868e-05, 1.7166e-05, 1.9848e-05, 2.2173e-05, 1.8775e-05, 2.0087e-05,\n",
            "        2.0564e-05, 2.0206e-05, 2.0146e-05, 2.1696e-05, 1.8239e-05, 1.9014e-05,\n",
            "        1.8835e-05, 2.0742e-05, 1.8001e-05, 1.6987e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.8954e-05, 1.8537e-05, 1.8239e-05, 2.1815e-05, 2.0981e-05, 1.8299e-05,\n",
            "        2.3186e-05, 2.2709e-05, 2.2411e-05, 1.9431e-05, 2.0146e-05, 2.0206e-05,\n",
            "        2.0623e-05, 2.3186e-05, 1.9252e-05, 1.4901e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.1935e-05, 1.9372e-05, 1.8418e-05, 1.6212e-05, 1.4842e-05, 1.8358e-05,\n",
            "        1.6868e-05, 1.8537e-05, 1.4842e-05, 2.0206e-05, 2.0266e-05, 2.0266e-05,\n",
            "        1.4842e-05, 1.6868e-05, 1.5795e-05, 1.8299e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.6153e-05, 1.9372e-05, 1.8418e-05, 1.4126e-05, 1.4842e-05, 1.8418e-05,\n",
            "        1.6868e-05, 1.8597e-05, 1.4842e-05, 1.3530e-05, 2.0266e-05, 2.0266e-05,\n",
            "        1.4842e-05, 1.4603e-05, 1.6093e-05, 2.0385e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.3530e-05, 1.9372e-05, 1.8418e-05, 1.4126e-05, 1.4842e-05, 1.8418e-05,\n",
            "        1.4603e-05, 1.9491e-05, 1.4842e-05, 1.4126e-05, 1.9729e-05, 1.9610e-05,\n",
            "        1.4842e-05, 1.4603e-05, 1.6510e-05, 2.0385e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.3530e-05, 1.9372e-05, 1.8418e-05, 1.4126e-05, 1.4424e-05, 1.8418e-05,\n",
            "        1.4603e-05, 1.9491e-05, 1.4842e-05, 1.4126e-05, 1.9729e-05, 1.7941e-05,\n",
            "        1.4424e-05, 1.4603e-05, 1.6868e-05, 1.3411e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.4126e-05, 1.9491e-05, 1.8656e-05, 1.4126e-05, 1.4424e-05, 1.8656e-05,\n",
            "        1.4603e-05, 1.9491e-05, 1.4842e-05, 1.4126e-05, 1.7703e-05, 1.7703e-05,\n",
            "        1.4424e-05, 1.7047e-05, 1.7643e-05, 1.4067e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.4126e-05, 1.9491e-05, 1.8656e-05, 1.4842e-05, 1.5020e-05, 1.8656e-05,\n",
            "        1.4722e-05, 1.9491e-05, 1.4782e-05, 1.4126e-05, 1.7703e-05, 1.7703e-05,\n",
            "        1.4424e-05, 1.7047e-05, 1.8775e-05, 1.4126e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.4126e-05, 1.9491e-05, 1.8656e-05, 1.4961e-05, 1.5020e-05, 1.8656e-05,\n",
            "        1.3888e-05, 1.9491e-05, 1.4126e-05, 1.4126e-05, 1.7703e-05, 1.5736e-05,\n",
            "        1.4424e-05, 1.3828e-05, 1.7762e-05, 1.4126e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.4126e-05, 1.9491e-05, 1.8656e-05, 1.5020e-05, 1.4842e-05, 1.8656e-05,\n",
            "        1.4246e-05, 1.8597e-05, 1.4842e-05, 1.4126e-05, 1.5736e-05, 1.5736e-05,\n",
            "        1.4424e-05, 1.4246e-05, 1.9372e-05, 1.4126e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.4126e-05, 1.8597e-05, 1.8656e-05, 1.6034e-05, 1.5199e-05, 1.8656e-05,\n",
            "        1.4663e-05, 1.8597e-05, 1.5140e-05, 1.3530e-05, 1.5795e-05, 1.5736e-05,\n",
            "        1.4663e-05, 1.4603e-05, 1.5974e-05, 1.4186e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.3530e-05, 1.8597e-05, 1.8656e-05, 1.6153e-05, 1.4663e-05, 1.8656e-05,\n",
            "        1.5020e-05, 1.8597e-05, 1.5974e-05, 1.3530e-05, 1.5736e-05, 1.5736e-05,\n",
            "        1.5974e-05, 1.5080e-05, 1.5974e-05, 1.4126e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.3530e-05, 1.9848e-05, 1.8358e-05, 1.7345e-05, 1.5378e-05, 1.8656e-05,\n",
            "        1.5795e-05, 1.8597e-05, 1.6928e-05, 1.3530e-05, 1.5736e-05, 1.5736e-05,\n",
            "        1.6212e-05, 1.6332e-05, 1.5974e-05, 1.4126e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.3590e-05, 1.9848e-05, 1.8179e-05, 1.8358e-05, 1.6212e-05, 1.8835e-05,\n",
            "        1.7107e-05, 1.8597e-05, 1.8060e-05, 1.3590e-05, 1.5736e-05, 1.5736e-05,\n",
            "        1.4603e-05, 1.7643e-05, 1.5974e-05, 1.4126e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.3590e-05, 1.9491e-05, 1.8179e-05, 1.9968e-05, 1.7345e-05, 1.8835e-05,\n",
            "        1.8537e-05, 1.9491e-05, 1.9789e-05, 1.3709e-05, 1.5736e-05, 1.5736e-05,\n",
            "        1.5199e-05, 1.8537e-05, 1.5974e-05, 1.4126e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.3769e-05, 1.9491e-05, 1.8358e-05, 1.9073e-05, 1.5676e-05, 1.8358e-05,\n",
            "        1.8835e-05, 1.9491e-05, 1.9014e-05, 1.4126e-05, 1.5736e-05, 1.5736e-05,\n",
            "        1.3769e-05, 1.9372e-05, 1.5736e-05, 1.3530e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.9087e-05, 2.5868e-05, 2.7597e-05, 2.3723e-05, 3.3498e-05, 2.3305e-05,\n",
            "        2.5868e-05, 2.3186e-05, 2.6286e-05, 2.6464e-05, 2.7895e-05, 2.4676e-05,\n",
            "        2.9087e-05, 2.6047e-05, 2.5451e-05, 2.3842e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.2769e-05, 2.2352e-05, 2.3901e-05, 2.4915e-05, 2.6047e-05, 2.2173e-05,\n",
            "        1.9908e-05, 1.9491e-05, 2.1935e-05, 2.4021e-05, 3.0816e-05, 2.0087e-05,\n",
            "        2.4736e-05, 2.5570e-05, 2.1696e-05, 2.0683e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.1994e-05, 2.3425e-05, 2.6643e-05, 2.1398e-05, 2.0683e-05, 2.2709e-05,\n",
            "        2.2411e-05, 2.1815e-05, 2.5392e-05, 2.7359e-05, 2.9981e-05, 2.0623e-05,\n",
            "        2.2709e-05, 2.0444e-05, 2.4021e-05, 2.3305e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.3484e-05, 2.5332e-05, 2.4080e-05, 2.4378e-05, 2.3127e-05, 2.3603e-05,\n",
            "        2.2471e-05, 2.3127e-05, 2.2769e-05, 2.0802e-05, 2.3901e-05, 2.3305e-05,\n",
            "        2.3365e-05, 2.2531e-05, 2.1994e-05, 2.2411e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.5988e-05, 2.5928e-05, 2.6941e-05, 2.7239e-05, 2.3484e-05, 2.6882e-05,\n",
            "        2.5392e-05, 2.5451e-05, 2.3663e-05, 2.2411e-05, 2.5511e-05, 2.5511e-05,\n",
            "        2.4438e-05, 2.2829e-05, 2.4736e-05, 2.4021e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.9206e-05, 2.8133e-05, 2.9624e-05, 3.0160e-05, 2.4378e-05, 2.9802e-05,\n",
            "        2.8074e-05, 2.7955e-05, 2.5332e-05, 2.4796e-05, 2.7657e-05, 2.7955e-05,\n",
            "        2.7061e-05, 2.3901e-05, 2.5570e-05, 2.5928e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.0683e-05, 2.0206e-05, 2.2769e-05, 2.3007e-05, 1.9073e-05, 1.9073e-05,\n",
            "        2.0862e-05, 2.0564e-05, 2.3663e-05, 2.3842e-05, 1.8656e-05, 1.9133e-05,\n",
            "        1.9133e-05, 1.7166e-05, 1.8656e-05, 2.4021e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.9133e-05, 1.8537e-05, 1.8716e-05, 2.0802e-05, 1.7047e-05, 1.7762e-05,\n",
            "        1.7762e-05, 1.8954e-05, 2.3186e-05, 2.0921e-05, 1.8656e-05, 1.7762e-05,\n",
            "        1.7762e-05, 1.6153e-05, 1.8656e-05, 2.4259e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.7762e-05, 1.8537e-05, 1.8001e-05, 2.0802e-05, 1.6749e-05, 1.7762e-05,\n",
            "        1.7762e-05, 1.8537e-05, 2.3186e-05, 2.0981e-05, 1.8656e-05, 1.7762e-05,\n",
            "        1.7762e-05, 1.6153e-05, 1.8656e-05, 2.4676e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.7762e-05, 1.8656e-05, 1.8001e-05, 1.8716e-05, 1.7166e-05, 1.7285e-05,\n",
            "        1.7762e-05, 1.8537e-05, 2.0325e-05, 2.2590e-05, 1.8656e-05, 1.7762e-05,\n",
            "        1.7762e-05, 1.6212e-05, 1.8179e-05, 2.1577e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.7762e-05, 1.8179e-05, 1.8001e-05, 1.8716e-05, 1.7285e-05, 1.7285e-05,\n",
            "        1.7762e-05, 1.8001e-05, 1.9014e-05, 2.4736e-05, 1.8179e-05, 1.7762e-05,\n",
            "        1.7762e-05, 1.6212e-05, 1.8179e-05, 2.0981e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.7405e-05, 1.8179e-05, 1.8001e-05, 1.8597e-05, 1.7226e-05, 1.6868e-05,\n",
            "        1.7583e-05, 1.8179e-05, 1.9908e-05, 2.0564e-05, 1.8179e-05, 1.7583e-05,\n",
            "        1.7583e-05, 1.6272e-05, 1.8179e-05, 2.1935e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.7285e-05, 1.7941e-05, 1.7822e-05, 1.8597e-05, 1.7226e-05, 1.6809e-05,\n",
            "        1.7583e-05, 1.7941e-05, 1.9908e-05, 2.0564e-05, 1.7941e-05, 1.7583e-05,\n",
            "        1.7583e-05, 1.5974e-05, 1.8001e-05, 2.2233e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7285e-05, 1.7941e-05, 1.7822e-05, 1.8656e-05, 1.7047e-05, 1.6809e-05,\n",
            "        1.7583e-05, 1.7941e-05, 1.9908e-05, 2.1100e-05, 1.8001e-05, 1.7107e-05,\n",
            "        1.7583e-05, 1.6093e-05, 1.8179e-05, 2.3186e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.7285e-05, 1.8001e-05, 1.7822e-05, 1.8656e-05, 1.7762e-05, 1.7285e-05,\n",
            "        1.7583e-05, 1.7762e-05, 1.9908e-05, 2.1100e-05, 2.1040e-05, 1.7107e-05,\n",
            "        1.7583e-05, 1.6272e-05, 1.8299e-05, 2.3186e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7285e-05, 2.1040e-05, 1.8895e-05, 1.7166e-05, 1.7941e-05, 1.7285e-05,\n",
            "        1.7583e-05, 1.7822e-05, 1.9014e-05, 2.1756e-05, 1.8299e-05, 1.7107e-05,\n",
            "        1.7643e-05, 1.6570e-05, 2.1338e-05, 2.3186e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.7285e-05, 1.8001e-05, 1.8895e-05, 1.7345e-05, 1.8418e-05, 1.7285e-05,\n",
            "        1.7762e-05, 1.8001e-05, 1.9014e-05, 2.1756e-05, 1.8477e-05, 1.7226e-05,\n",
            "        1.7822e-05, 1.6987e-05, 1.7881e-05, 2.3186e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.6809e-05, 1.8179e-05, 1.8537e-05, 1.7524e-05, 1.9073e-05, 1.7285e-05,\n",
            "        1.7464e-05, 1.8239e-05, 1.9014e-05, 2.1100e-05, 1.8775e-05, 1.7822e-05,\n",
            "        1.7822e-05, 1.7643e-05, 1.8358e-05, 2.3186e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.6749e-05, 1.7345e-05, 1.8895e-05, 1.7881e-05, 1.9729e-05, 1.6749e-05,\n",
            "        1.7822e-05, 1.7881e-05, 1.9014e-05, 2.1100e-05, 1.9431e-05, 1.8120e-05,\n",
            "        1.8179e-05, 1.8120e-05, 1.8954e-05, 2.2233e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.6928e-05, 1.7524e-05, 1.9550e-05, 1.8239e-05, 2.0802e-05, 1.6749e-05,\n",
            "        1.8060e-05, 1.6332e-05, 1.9014e-05, 2.0564e-05, 2.0146e-05, 1.8358e-05,\n",
            "        1.8537e-05, 1.8537e-05, 1.9729e-05, 2.1994e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.9193e-05, 1.9372e-05, 2.3901e-05, 1.9789e-05, 3.3677e-05, 2.4021e-05,\n",
            "        2.5630e-05, 2.0921e-05, 2.5809e-05, 2.3246e-05, 1.7405e-05, 1.7047e-05,\n",
            "        2.0325e-05, 2.9683e-05, 2.0444e-05, 2.1219e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.1398e-05, 2.2590e-05, 2.3186e-05, 1.6332e-05, 2.4438e-05, 2.7597e-05,\n",
            "        2.7120e-05, 1.9550e-05, 2.2113e-05, 1.9848e-05, 1.7166e-05, 1.9670e-05,\n",
            "        1.8120e-05, 3.1054e-05, 2.1577e-05, 2.4796e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.0325e-05, 2.0266e-05, 2.1458e-05, 1.6928e-05, 2.2113e-05, 1.7345e-05,\n",
            "        2.6941e-05, 2.1636e-05, 2.2352e-05, 1.8477e-05, 1.9848e-05, 1.6391e-05,\n",
            "        2.1696e-05, 2.5988e-05, 2.3246e-05, 2.0146e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.2352e-05, 2.1219e-05, 2.4259e-05, 1.8537e-05, 2.3782e-05, 1.8656e-05,\n",
            "        2.2113e-05, 2.2292e-05, 2.1935e-05, 2.1160e-05, 2.1815e-05, 1.7583e-05,\n",
            "        2.0206e-05, 2.1398e-05, 2.1994e-05, 2.0862e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.4736e-05, 2.2411e-05, 2.2948e-05, 1.9491e-05, 1.9133e-05, 1.9968e-05,\n",
            "        1.8835e-05, 2.1815e-05, 1.9372e-05, 2.3842e-05, 2.3603e-05, 1.8775e-05,\n",
            "        2.1815e-05, 1.8418e-05, 2.4736e-05, 2.0862e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.7955e-05, 2.5511e-05, 2.2233e-05, 2.1517e-05, 1.9670e-05, 2.2411e-05,\n",
            "        2.0683e-05, 2.4736e-05, 2.0146e-05, 2.7478e-05, 2.3484e-05, 2.0564e-05,\n",
            "        1.9550e-05, 1.9908e-05, 2.4438e-05, 1.9431e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.3901e-05, 2.5630e-05, 2.2292e-05, 1.9491e-05, 2.0623e-05, 1.9491e-05,\n",
            "        2.0683e-05, 2.4736e-05, 1.9610e-05, 2.2292e-05, 2.3603e-05, 2.0146e-05,\n",
            "        1.9610e-05, 2.0683e-05, 2.4498e-05, 1.9372e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.6510e-05, 1.7285e-05, 1.7166e-05, 1.8299e-05, 2.1458e-05, 1.8954e-05,\n",
            "        1.5259e-05, 1.5676e-05, 1.4305e-05, 1.7166e-05, 1.5497e-05, 1.5736e-05,\n",
            "        1.3947e-05, 2.1756e-05, 1.6630e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.6093e-05, 1.7047e-05, 1.7166e-05, 1.8358e-05, 2.0087e-05, 1.8954e-05,\n",
            "        1.5259e-05, 1.5557e-05, 1.4305e-05, 1.6749e-05, 1.5080e-05, 1.6451e-05,\n",
            "        1.3947e-05, 1.9550e-05, 1.6153e-05, 1.9729e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.6093e-05, 1.6212e-05, 1.6749e-05, 1.8954e-05, 1.5318e-05, 1.8954e-05,\n",
            "        1.5259e-05, 1.5557e-05, 1.4424e-05, 1.6749e-05, 1.5080e-05, 1.7345e-05,\n",
            "        1.3888e-05, 1.9550e-05, 1.6153e-05, 1.9729e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.6093e-05, 1.6212e-05, 1.5259e-05, 1.8954e-05, 1.5140e-05, 1.9729e-05,\n",
            "        1.4305e-05, 1.5557e-05, 1.4603e-05, 1.6749e-05, 1.5080e-05, 1.6332e-05,\n",
            "        1.3947e-05, 1.7166e-05, 1.6153e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.6153e-05, 1.6212e-05, 1.5259e-05, 1.9729e-05, 1.5140e-05, 1.8954e-05,\n",
            "        1.4305e-05, 1.5557e-05, 1.3351e-05, 1.6749e-05, 1.5080e-05, 1.4782e-05,\n",
            "        1.4365e-05, 1.7345e-05, 1.5080e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.6153e-05, 1.6212e-05, 1.5259e-05, 1.9729e-05, 1.5140e-05, 1.8954e-05,\n",
            "        1.4424e-05, 1.5557e-05, 1.3411e-05, 1.5259e-05, 1.5080e-05, 1.4782e-05,\n",
            "        1.4722e-05, 1.7583e-05, 1.5080e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.4782e-05, 1.6212e-05, 1.5259e-05, 1.8954e-05, 1.3947e-05, 1.8954e-05,\n",
            "        1.4544e-05, 1.5557e-05, 1.3590e-05, 1.5259e-05, 1.5080e-05, 1.5497e-05,\n",
            "        1.4782e-05, 1.8835e-05, 1.5080e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.4305e-05, 1.6212e-05, 1.5259e-05, 1.8954e-05, 1.3947e-05, 1.8954e-05,\n",
            "        1.4722e-05, 1.5378e-05, 1.3590e-05, 1.5259e-05, 1.4901e-05, 1.5616e-05,\n",
            "        1.2994e-05, 1.8597e-05, 1.5080e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.4305e-05, 1.6570e-05, 1.4544e-05, 1.8954e-05, 1.4007e-05, 1.9729e-05,\n",
            "        1.3053e-05, 1.5378e-05, 1.3888e-05, 1.5140e-05, 1.4901e-05, 1.6093e-05,\n",
            "        1.3232e-05, 1.7285e-05, 1.4901e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.4305e-05, 1.5557e-05, 1.4544e-05, 1.8954e-05, 1.3947e-05, 1.8954e-05,\n",
            "        1.3113e-05, 1.5378e-05, 1.4365e-05, 1.5140e-05, 1.4901e-05, 1.6391e-05,\n",
            "        1.3649e-05, 1.5795e-05, 1.4901e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.4305e-05, 1.5557e-05, 1.4544e-05, 1.8954e-05, 1.4007e-05, 1.8954e-05,\n",
            "        1.3292e-05, 1.4663e-05, 1.4544e-05, 1.4544e-05, 1.4365e-05, 1.6809e-05,\n",
            "        1.4424e-05, 1.6689e-05, 1.4901e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.4305e-05, 1.5080e-05, 1.4603e-05, 1.8954e-05, 1.4901e-05, 1.8954e-05,\n",
            "        1.3590e-05, 1.4782e-05, 1.4544e-05, 1.4603e-05, 1.4365e-05, 1.7285e-05,\n",
            "        1.5020e-05, 1.7941e-05, 1.4305e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.4901e-05, 1.5497e-05, 1.4722e-05, 1.8954e-05, 1.4961e-05, 1.8954e-05,\n",
            "        1.4186e-05, 1.4782e-05, 1.4663e-05, 1.4603e-05, 1.4603e-05, 1.7941e-05,\n",
            "        1.5080e-05, 1.8537e-05, 1.4305e-05, 1.8716e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.2054e-05, 2.3425e-05, 1.9491e-05, 1.8775e-05, 2.1160e-05, 2.2233e-05,\n",
            "        2.1577e-05, 2.4796e-05, 2.5094e-05, 1.7822e-05, 2.4259e-05, 1.8835e-05,\n",
            "        2.0087e-05, 2.0087e-05, 2.2054e-05, 2.5392e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.8849e-05, 2.1219e-05, 2.5332e-05, 1.8001e-05, 2.4736e-05, 2.1994e-05,\n",
            "        2.1696e-05, 1.8895e-05, 2.9206e-05, 2.0087e-05, 2.3723e-05, 2.0385e-05,\n",
            "        2.4438e-05, 2.1219e-05, 1.9073e-05, 2.3007e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.3484e-05, 2.2888e-05, 2.4438e-05, 1.9133e-05, 2.4557e-05, 2.1935e-05,\n",
            "        2.4080e-05, 1.8418e-05, 2.0087e-05, 2.0385e-05, 2.5451e-05, 1.8179e-05,\n",
            "        2.6107e-05, 1.8358e-05, 2.2173e-05, 2.0742e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.1040e-05, 2.3425e-05, 2.7418e-05, 2.0504e-05, 2.0862e-05, 1.9729e-05,\n",
            "        2.0564e-05, 2.1219e-05, 2.1636e-05, 2.0087e-05, 2.0802e-05, 2.0623e-05,\n",
            "        2.0981e-05, 2.0921e-05, 2.2590e-05, 2.1696e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.2650e-05, 2.3603e-05, 2.4676e-05, 2.2650e-05, 2.2888e-05, 2.1756e-05,\n",
            "        2.2411e-05, 2.3782e-05, 2.0623e-05, 2.2471e-05, 1.9848e-05, 2.2411e-05,\n",
            "        2.2471e-05, 2.3067e-05, 2.4855e-05, 2.4199e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.4259e-05, 2.7299e-05, 2.6584e-05, 2.4676e-05, 2.4557e-05, 2.4498e-05,\n",
            "        2.4438e-05, 2.5630e-05, 2.1577e-05, 2.4438e-05, 2.1398e-05, 2.3186e-05,\n",
            "        2.2113e-05, 2.5392e-05, 2.5928e-05, 2.7239e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.9729e-05, 1.8775e-05, 2.0802e-05, 1.5795e-05, 1.6868e-05, 1.6809e-05,\n",
            "        1.6868e-05, 1.5974e-05, 2.1636e-05, 1.6868e-05, 1.9014e-05, 1.8537e-05,\n",
            "        1.6272e-05, 1.6510e-05, 1.9372e-05, 1.9372e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.9550e-05, 1.7703e-05, 2.0802e-05, 1.5795e-05, 1.6868e-05, 1.5736e-05,\n",
            "        1.6868e-05, 1.7941e-05, 2.1636e-05, 1.5795e-05, 1.9968e-05, 1.7524e-05,\n",
            "        1.6391e-05, 1.5795e-05, 1.7822e-05, 1.8001e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.9550e-05, 1.5557e-05, 2.0802e-05, 1.5795e-05, 1.5795e-05, 1.5736e-05,\n",
            "        1.5795e-05, 1.7941e-05, 2.1636e-05, 1.5795e-05, 1.9968e-05, 1.5557e-05,\n",
            "        1.6510e-05, 1.5795e-05, 1.5795e-05, 1.8001e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.9550e-05, 1.5557e-05, 2.0802e-05, 1.5795e-05, 1.5795e-05, 1.5736e-05,\n",
            "        1.5795e-05, 1.4961e-05, 2.1636e-05, 1.5795e-05, 1.9968e-05, 1.5557e-05,\n",
            "        1.6570e-05, 1.5795e-05, 1.6689e-05, 1.5736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.9550e-05, 1.5557e-05, 2.0802e-05, 1.5795e-05, 1.5795e-05, 1.5199e-05,\n",
            "        1.5795e-05, 1.4961e-05, 1.8775e-05, 1.5795e-05, 1.9908e-05, 1.5557e-05,\n",
            "        1.6987e-05, 1.5795e-05, 1.6689e-05, 1.5736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.9550e-05, 1.5557e-05, 2.0802e-05, 1.5795e-05, 1.5795e-05, 1.5199e-05,\n",
            "        1.5795e-05, 1.5557e-05, 1.9133e-05, 1.5795e-05, 1.9908e-05, 1.5557e-05,\n",
            "        1.7405e-05, 1.5795e-05, 1.6689e-05, 1.5736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.9550e-05, 1.5557e-05, 2.0802e-05, 1.5795e-05, 1.5795e-05, 1.5199e-05,\n",
            "        1.5795e-05, 1.5557e-05, 1.9133e-05, 1.5795e-05, 1.9908e-05, 1.5557e-05,\n",
            "        1.8060e-05, 1.5795e-05, 1.6689e-05, 1.5199e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.9550e-05, 1.5557e-05, 2.0802e-05, 1.5795e-05, 1.5795e-05, 1.5199e-05,\n",
            "        1.5795e-05, 1.5557e-05, 1.7345e-05, 1.5795e-05, 1.9908e-05, 1.5557e-05,\n",
            "        1.8716e-05, 1.5795e-05, 1.5855e-05, 1.5199e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.9550e-05, 1.4961e-05, 2.1219e-05, 1.7643e-05, 1.5795e-05, 1.5199e-05,\n",
            "        1.5497e-05, 1.5557e-05, 1.7345e-05, 1.5497e-05, 2.0444e-05, 1.5557e-05,\n",
            "        1.8716e-05, 1.5616e-05, 1.5914e-05, 1.5199e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.9550e-05, 1.4901e-05, 2.0921e-05, 1.7703e-05, 1.7643e-05, 1.5199e-05,\n",
            "        1.5557e-05, 1.5378e-05, 1.7345e-05, 1.5557e-05, 2.0444e-05, 1.4901e-05,\n",
            "        1.8716e-05, 1.7703e-05, 1.6332e-05, 1.5318e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.9610e-05, 1.4901e-05, 2.0921e-05, 1.5497e-05, 1.7643e-05, 1.5676e-05,\n",
            "        1.5676e-05, 1.5378e-05, 1.7345e-05, 1.5676e-05, 2.0444e-05, 1.4901e-05,\n",
            "        1.8716e-05, 1.7047e-05, 1.6630e-05, 1.5497e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.1458e-05, 1.4901e-05, 2.2709e-05, 1.5855e-05, 1.5676e-05, 1.5914e-05,\n",
            "        1.5736e-05, 1.4842e-05, 1.7345e-05, 1.5795e-05, 2.0444e-05, 1.4901e-05,\n",
            "        1.9133e-05, 1.6987e-05, 1.7226e-05, 1.5676e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.9610e-05, 1.4842e-05, 2.1219e-05, 1.4365e-05, 1.6034e-05, 1.6093e-05,\n",
            "        1.6093e-05, 1.4901e-05, 1.7107e-05, 1.6093e-05, 2.0444e-05, 1.5020e-05,\n",
            "        1.9133e-05, 1.4126e-05, 1.7941e-05, 1.6093e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.9610e-05, 1.5020e-05, 2.1219e-05, 1.4782e-05, 1.6391e-05, 1.6391e-05,\n",
            "        1.5199e-05, 1.5140e-05, 1.7107e-05, 1.6510e-05, 1.9908e-05, 1.5140e-05,\n",
            "        1.9133e-05, 1.4126e-05, 1.9252e-05, 1.6809e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.5855e-05, 1.1504e-05, 1.9252e-05, 1.9193e-05, 1.5974e-05, 1.5318e-05,\n",
            "        1.1027e-05, 1.6034e-05, 1.3173e-05, 1.8537e-05, 1.5140e-05, 1.3769e-05,\n",
            "        1.6928e-05, 1.6212e-05, 1.4126e-05, 1.6391e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.8060e-05, 1.2517e-05, 1.5616e-05, 1.1802e-05, 1.8001e-05, 1.8775e-05,\n",
            "        9.1791e-06, 1.5974e-05, 1.3351e-05, 2.1279e-05, 2.1458e-05, 1.7345e-05,\n",
            "        1.4722e-05, 1.8477e-05, 1.7345e-05, 1.3113e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.1563e-05, 1.1325e-05, 1.1802e-05, 1.4663e-05, 1.2755e-05, 2.0564e-05,\n",
            "        1.0669e-05, 1.4484e-05, 1.6034e-05, 2.4498e-05, 1.8716e-05, 1.3292e-05,\n",
            "        1.9550e-05, 1.4305e-05, 2.2531e-05, 1.4782e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.2755e-05, 1.3232e-05, 1.1027e-05, 1.7405e-05, 1.4424e-05, 1.7047e-05,\n",
            "        1.2755e-05, 1.7047e-05, 1.3769e-05, 2.3723e-05, 2.2411e-05, 1.5318e-05,\n",
            "        1.8597e-05, 1.3173e-05, 2.0921e-05, 1.5318e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.3888e-05, 1.4782e-05, 1.1742e-05, 2.0564e-05, 1.5557e-05, 1.2815e-05,\n",
            "        1.4424e-05, 1.3947e-05, 1.2696e-05, 1.4067e-05, 2.0802e-05, 1.7345e-05,\n",
            "        2.0206e-05, 1.4126e-05, 1.4126e-05, 1.8179e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.5199e-05, 1.6570e-05, 1.2755e-05, 1.8597e-05, 1.6868e-05, 1.4305e-05,\n",
            "        1.6212e-05, 1.5378e-05, 1.4901e-05, 1.8775e-05, 1.8716e-05, 1.8716e-05,\n",
            "        1.8179e-05, 1.5140e-05, 1.5259e-05, 1.6630e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.6153e-05, 1.0967e-05, 1.0848e-05, 1.5557e-05, 1.7583e-05, 1.4782e-05,\n",
            "        1.1563e-05, 1.5259e-05, 1.5378e-05, 1.5259e-05, 1.5557e-05, 1.5557e-05,\n",
            "        1.9193e-05, 1.5497e-05, 1.8835e-05, 1.3232e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.3411e-05, 1.0967e-05, 1.1146e-05, 1.5557e-05, 1.5557e-05, 1.6272e-05,\n",
            "        1.1981e-05, 1.6034e-05, 1.2517e-05, 1.5259e-05, 1.5557e-05, 1.5557e-05,\n",
            "        1.9968e-05, 1.5497e-05, 1.4782e-05, 1.2815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.3411e-05, 1.1206e-05, 1.1504e-05, 1.5557e-05, 1.5914e-05, 1.3709e-05,\n",
            "        1.1325e-05, 1.6034e-05, 1.2517e-05, 1.5259e-05, 1.5557e-05, 1.6749e-05,\n",
            "        2.0444e-05, 1.5497e-05, 1.5259e-05, 1.3351e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.1981e-05, 1.1206e-05, 1.1981e-05, 1.6749e-05, 1.5914e-05, 1.4067e-05,\n",
            "        1.1802e-05, 1.4424e-05, 1.2517e-05, 1.5259e-05, 1.5557e-05, 1.6749e-05,\n",
            "        1.8358e-05, 1.5914e-05, 1.5259e-05, 1.1265e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.2100e-05, 1.0312e-05, 1.1981e-05, 1.6749e-05, 1.5914e-05, 1.2517e-05,\n",
            "        1.2100e-05, 1.4424e-05, 1.2457e-05, 1.5259e-05, 1.5557e-05, 1.6749e-05,\n",
            "        1.5557e-05, 1.5914e-05, 1.4782e-05, 1.1325e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.2338e-05, 1.0312e-05, 1.2100e-05, 1.6749e-05, 1.5914e-05, 1.2875e-05,\n",
            "        1.1265e-05, 1.2636e-05, 1.2517e-05, 1.5259e-05, 1.5557e-05, 1.6749e-05,\n",
            "        1.5557e-05, 1.5497e-05, 1.5914e-05, 1.2755e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.2636e-05, 1.0312e-05, 1.2279e-05, 1.6749e-05, 1.5497e-05, 1.3232e-05,\n",
            "        1.1384e-05, 1.2636e-05, 1.1981e-05, 1.5259e-05, 1.5557e-05, 1.6749e-05,\n",
            "        1.5259e-05, 1.5497e-05, 1.5914e-05, 1.2875e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.3113e-05, 1.0312e-05, 1.2636e-05, 1.5557e-05, 1.5497e-05, 1.3769e-05,\n",
            "        1.1563e-05, 1.2636e-05, 1.1981e-05, 1.5914e-05, 1.6749e-05, 1.5557e-05,\n",
            "        1.5259e-05, 1.5497e-05, 1.5914e-05, 1.2279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.3173e-05, 1.1206e-05, 1.3113e-05, 1.5557e-05, 1.5497e-05, 1.5497e-05,\n",
            "        1.1921e-05, 1.2636e-05, 1.2100e-05, 1.5914e-05, 2.0027e-05, 1.5557e-05,\n",
            "        1.5259e-05, 1.5497e-05, 1.5914e-05, 1.2815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.4186e-05, 1.1444e-05, 1.3232e-05, 1.5557e-05, 1.5497e-05, 1.5736e-05,\n",
            "        1.2279e-05, 1.2636e-05, 1.2457e-05, 1.5914e-05, 1.6749e-05, 1.5378e-05,\n",
            "        1.5259e-05, 1.5497e-05, 1.5914e-05, 1.3411e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.4901e-05, 9.5963e-06, 1.4305e-05, 1.5557e-05, 1.5497e-05, 1.4961e-05,\n",
            "        1.3947e-05, 1.2636e-05, 1.2577e-05, 1.5914e-05, 1.6749e-05, 1.4961e-05,\n",
            "        1.5259e-05, 1.5259e-05, 1.5914e-05, 1.4186e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.4961e-05, 9.9540e-06, 1.5736e-05, 1.5378e-05, 1.5497e-05, 1.5259e-05,\n",
            "        1.5259e-05, 1.2636e-05, 1.2338e-05, 1.5914e-05, 1.6332e-05, 1.5557e-05,\n",
            "        1.5557e-05, 1.5259e-05, 1.5914e-05, 1.5438e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.4961e-05, 1.0133e-05, 1.5736e-05, 1.6689e-05, 1.5259e-05, 1.5259e-05,\n",
            "        1.6332e-05, 1.2636e-05, 1.2994e-05, 1.5914e-05, 1.5557e-05, 1.6749e-05,\n",
            "        1.5557e-05, 1.5259e-05, 1.5914e-05, 1.5497e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.4961e-05, 1.0490e-05, 1.5736e-05, 1.6809e-05, 1.5259e-05, 1.5914e-05,\n",
            "        1.5557e-05, 1.2636e-05, 1.3947e-05, 1.5497e-05, 1.5259e-05, 1.6749e-05,\n",
            "        1.5557e-05, 1.5259e-05, 1.5259e-05, 1.5497e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.2471e-05, 1.8239e-05, 2.2948e-05, 1.7405e-05, 2.3723e-05, 2.1338e-05,\n",
            "        1.5199e-05, 1.7285e-05, 2.1935e-05, 2.0385e-05, 2.3901e-05, 1.8060e-05,\n",
            "        2.2471e-05, 2.1577e-05, 1.5318e-05, 1.6928e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.3305e-05, 1.7464e-05, 1.5676e-05, 1.8418e-05, 3.2604e-05, 1.9729e-05,\n",
            "        1.7524e-05, 1.5616e-05, 1.5199e-05, 1.8120e-05, 2.3425e-05, 1.7345e-05,\n",
            "        2.6226e-05, 2.2173e-05, 1.6510e-05, 1.7226e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.5928e-05, 1.8954e-05, 1.8060e-05, 2.1636e-05, 2.9922e-05, 1.9491e-05,\n",
            "        1.7583e-05, 1.6093e-05, 1.6630e-05, 1.9968e-05, 2.0981e-05, 1.8358e-05,\n",
            "        2.3842e-05, 2.0206e-05, 1.4365e-05, 1.8597e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.2709e-05, 2.2650e-05, 1.8716e-05, 1.8001e-05, 1.4842e-05, 2.3007e-05,\n",
            "        2.2650e-05, 1.9789e-05, 1.9252e-05, 1.8656e-05, 1.8656e-05, 1.9431e-05,\n",
            "        2.0683e-05, 1.9968e-05, 1.6868e-05, 2.1279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.0921e-05, 2.2531e-05, 2.1756e-05, 1.9252e-05, 1.7166e-05, 2.1458e-05,\n",
            "        1.7941e-05, 2.1636e-05, 2.2411e-05, 2.0027e-05, 1.8418e-05, 1.9431e-05,\n",
            "        2.3425e-05, 2.2531e-05, 1.9550e-05, 2.1279e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.2829e-05, 2.5928e-05, 2.4855e-05, 2.1815e-05, 1.9729e-05, 2.3246e-05,\n",
            "        1.9431e-05, 2.4498e-05, 1.9789e-05, 2.2829e-05, 1.9550e-05, 2.2411e-05,\n",
            "        2.4438e-05, 2.4438e-05, 2.3961e-05, 2.4319e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.4901e-05, 1.7107e-05, 1.5914e-05, 1.8656e-05, 2.0921e-05, 1.4186e-05,\n",
            "        1.9550e-05, 1.8477e-05, 1.5140e-05, 1.4901e-05, 1.8895e-05, 1.4901e-05,\n",
            "        1.4782e-05, 1.4782e-05, 1.7643e-05, 1.4782e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.4901e-05, 1.4782e-05, 1.5914e-05, 1.9193e-05, 2.0921e-05, 1.4186e-05,\n",
            "        1.9789e-05, 1.8477e-05, 1.5140e-05, 1.5795e-05, 1.8895e-05, 1.4901e-05,\n",
            "        1.6451e-05, 1.6451e-05, 1.5914e-05, 1.6451e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.4901e-05, 1.4782e-05, 1.4067e-05, 1.9193e-05, 1.7881e-05, 1.4186e-05,\n",
            "        1.8358e-05, 1.5914e-05, 1.3649e-05, 1.4901e-05, 1.8895e-05, 1.4901e-05,\n",
            "        1.6451e-05, 1.6451e-05, 1.4842e-05, 1.6451e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.4901e-05, 1.4067e-05, 1.4067e-05, 1.9193e-05, 1.4186e-05, 1.4901e-05,\n",
            "        1.7405e-05, 1.5914e-05, 1.3649e-05, 1.4901e-05, 1.9193e-05, 1.4186e-05,\n",
            "        1.6451e-05, 1.6451e-05, 1.4842e-05, 1.6451e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.4901e-05, 1.3649e-05, 1.6034e-05, 1.9193e-05, 1.4186e-05, 1.4186e-05,\n",
            "        1.7405e-05, 1.4842e-05, 1.3649e-05, 1.4901e-05, 1.9193e-05, 1.3888e-05,\n",
            "        1.6451e-05, 1.4782e-05, 1.4842e-05, 1.6451e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.4603e-05, 1.3649e-05, 1.6034e-05, 1.9193e-05, 1.4186e-05, 1.4603e-05,\n",
            "        1.7405e-05, 1.4842e-05, 1.3649e-05, 1.4603e-05, 1.8895e-05, 1.3888e-05,\n",
            "        1.4424e-05, 1.4424e-05, 1.4901e-05, 1.4424e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.4603e-05, 1.3649e-05, 1.4544e-05, 1.9193e-05, 1.4186e-05, 1.3888e-05,\n",
            "        1.7405e-05, 1.4901e-05, 1.3471e-05, 1.4603e-05, 1.8895e-05, 1.3888e-05,\n",
            "        1.4424e-05, 1.4424e-05, 1.4663e-05, 1.4424e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.4603e-05, 1.3649e-05, 1.4484e-05, 1.8895e-05, 1.3411e-05, 1.3888e-05,\n",
            "        1.5497e-05, 1.4663e-05, 1.4424e-05, 1.3888e-05, 1.8895e-05, 1.3888e-05,\n",
            "        1.4424e-05, 1.3649e-05, 1.4722e-05, 1.4424e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.4901e-05, 1.3709e-05, 1.4544e-05, 1.8895e-05, 1.3411e-05, 1.4186e-05,\n",
            "        1.5497e-05, 1.5199e-05, 1.4961e-05, 1.4186e-05, 1.8895e-05, 1.4186e-05,\n",
            "        1.4544e-05, 1.3709e-05, 1.5557e-05, 1.4424e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.4246e-05, 1.4424e-05, 1.4603e-05, 1.8656e-05, 1.3411e-05, 1.4842e-05,\n",
            "        1.5497e-05, 1.5497e-05, 1.3292e-05, 1.4365e-05, 1.8895e-05, 1.4186e-05,\n",
            "        1.4484e-05, 1.3709e-05, 1.6153e-05, 1.4484e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.4424e-05, 1.4544e-05, 1.4663e-05, 1.8656e-05, 1.3411e-05, 1.4961e-05,\n",
            "        1.5497e-05, 1.5974e-05, 1.3649e-05, 1.4663e-05, 1.8895e-05, 1.4424e-05,\n",
            "        1.4603e-05, 1.3828e-05, 1.6809e-05, 1.3828e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.4901e-05, 1.3947e-05, 1.4067e-05, 1.8656e-05, 1.3411e-05, 1.4424e-05,\n",
            "        1.5497e-05, 1.6630e-05, 1.4126e-05, 1.5140e-05, 2.0444e-05, 1.4603e-05,\n",
            "        1.4663e-05, 1.3947e-05, 1.7762e-05, 1.3888e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.5259e-05, 1.4186e-05, 1.4365e-05, 1.8656e-05, 1.3411e-05, 1.4782e-05,\n",
            "        1.5497e-05, 1.7583e-05, 1.5199e-05, 1.5736e-05, 1.8835e-05, 1.4961e-05,\n",
            "        1.4901e-05, 1.4126e-05, 1.9431e-05, 1.4186e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.5914e-05, 1.4424e-05, 1.4722e-05, 1.8895e-05, 1.3411e-05, 1.5676e-05,\n",
            "        1.5557e-05, 1.8537e-05, 1.6391e-05, 1.6749e-05, 1.9193e-05, 1.5855e-05,\n",
            "        1.4424e-05, 1.4365e-05, 2.0444e-05, 1.4424e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.8716e-05, 2.1696e-05, 1.8895e-05, 1.9252e-05, 2.0742e-05, 2.0385e-05,\n",
            "        2.1994e-05, 1.6809e-05, 2.2173e-05, 1.5318e-05, 2.2590e-05, 2.4498e-05,\n",
            "        1.8001e-05, 2.6226e-05, 2.7001e-05, 2.5690e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.6868e-05, 1.9968e-05, 2.0504e-05, 1.7047e-05, 2.0742e-05, 1.6451e-05,\n",
            "        1.6570e-05, 2.1338e-05, 1.9550e-05, 1.5140e-05, 2.3663e-05, 1.7643e-05,\n",
            "        1.4484e-05, 1.8299e-05, 2.4974e-05, 2.1815e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.0385e-05, 2.3186e-05, 2.3007e-05, 1.7047e-05, 2.1160e-05, 1.8239e-05,\n",
            "        1.7345e-05, 1.7464e-05, 2.3067e-05, 1.7464e-05, 2.2113e-05, 1.9193e-05,\n",
            "        1.6153e-05, 1.6510e-05, 2.7359e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.1696e-05, 1.9789e-05, 2.1935e-05, 1.9491e-05, 2.2829e-05, 1.9491e-05,\n",
            "        1.8954e-05, 1.8954e-05, 2.9445e-05, 1.9431e-05, 2.8074e-05, 1.8895e-05,\n",
            "        1.8656e-05, 2.0504e-05, 2.1338e-05, 2.1100e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.9193e-05, 2.3067e-05, 2.3603e-05, 2.1935e-05, 2.1040e-05, 2.1756e-05,\n",
            "        2.1636e-05, 1.9372e-05, 2.2471e-05, 2.1219e-05, 2.2590e-05, 2.0981e-05,\n",
            "        2.1398e-05, 1.9073e-05, 2.3186e-05, 2.2888e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.0444e-05, 2.3127e-05, 2.6166e-05, 2.4319e-05, 2.3127e-05, 2.4557e-05,\n",
            "        2.4498e-05, 2.0742e-05, 2.5630e-05, 2.3305e-05, 2.0683e-05, 2.2531e-05,\n",
            "        2.4378e-05, 2.0683e-05, 2.4199e-05, 2.4796e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.9491e-05, 1.6689e-05, 1.7524e-05, 1.7107e-05, 1.5616e-05, 1.8299e-05,\n",
            "        1.6212e-05, 2.0802e-05, 2.2829e-05, 1.5795e-05, 1.9193e-05, 2.4199e-05,\n",
            "        1.8299e-05, 1.8120e-05, 1.9670e-05, 2.3305e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.9670e-05, 1.6689e-05, 1.6510e-05, 1.7107e-05, 1.5616e-05, 1.7107e-05,\n",
            "        1.7583e-05, 2.1458e-05, 1.9670e-05, 1.5795e-05, 1.9193e-05, 2.3603e-05,\n",
            "        1.5438e-05, 1.7107e-05, 2.0444e-05, 1.9670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.9670e-05, 1.5557e-05, 1.6510e-05, 1.6093e-05, 1.5616e-05, 1.5438e-05,\n",
            "        1.7583e-05, 2.1458e-05, 1.9670e-05, 1.5795e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.5438e-05, 1.7643e-05, 2.0444e-05, 1.9670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.9670e-05, 1.5557e-05, 1.6510e-05, 1.6093e-05, 1.5616e-05, 1.5438e-05,\n",
            "        1.7583e-05, 1.8597e-05, 1.9670e-05, 1.5616e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.5438e-05, 1.8358e-05, 2.0444e-05, 1.9670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.9670e-05, 1.5616e-05, 1.6510e-05, 1.6093e-05, 1.5616e-05, 1.5438e-05,\n",
            "        1.5318e-05, 1.8597e-05, 1.9670e-05, 1.5616e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.4901e-05, 1.8775e-05, 1.9670e-05, 1.9670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.9670e-05, 1.5080e-05, 1.6510e-05, 1.5497e-05, 1.5616e-05, 1.5438e-05,\n",
            "        1.5318e-05, 1.6868e-05, 1.9073e-05, 1.5080e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.4901e-05, 1.8775e-05, 1.9670e-05, 1.9670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.9670e-05, 1.5080e-05, 1.6510e-05, 1.5140e-05, 1.5140e-05, 1.5438e-05,\n",
            "        1.5080e-05, 1.6868e-05, 2.0623e-05, 1.5140e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.4901e-05, 1.8775e-05, 1.9670e-05, 1.9670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.9670e-05, 1.5140e-05, 1.6570e-05, 1.5378e-05, 1.5497e-05, 1.5497e-05,\n",
            "        1.5080e-05, 1.6868e-05, 1.9073e-05, 1.5438e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.4901e-05, 1.8775e-05, 1.9670e-05, 1.9670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.0087e-05, 1.5259e-05, 1.7107e-05, 1.7166e-05, 1.7345e-05, 1.5676e-05,\n",
            "        1.4961e-05, 1.6868e-05, 1.9073e-05, 1.6272e-05, 1.9670e-05, 1.9073e-05,\n",
            "        1.5020e-05, 1.8775e-05, 1.9670e-05, 1.9670e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.9610e-05, 1.5557e-05, 1.6630e-05, 1.7464e-05, 1.7643e-05, 1.5914e-05,\n",
            "        1.5140e-05, 1.6868e-05, 1.9610e-05, 1.6570e-05, 2.0683e-05, 2.0623e-05,\n",
            "        1.5378e-05, 1.8775e-05, 1.9073e-05, 2.0623e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.0683e-05, 1.6868e-05, 1.7285e-05, 1.3709e-05, 1.7047e-05, 1.6332e-05,\n",
            "        1.5318e-05, 1.6868e-05, 1.9610e-05, 1.6868e-05, 1.9610e-05, 2.0683e-05,\n",
            "        1.6212e-05, 1.9133e-05, 2.0683e-05, 2.0683e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.0683e-05, 1.5676e-05, 1.8179e-05, 1.4007e-05, 1.6749e-05, 1.6570e-05,\n",
            "        1.5557e-05, 1.6928e-05, 1.9610e-05, 1.4186e-05, 1.9610e-05, 1.9610e-05,\n",
            "        1.6570e-05, 1.9133e-05, 2.0683e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.9610e-05, 1.3590e-05, 1.9312e-05, 1.4186e-05, 1.7762e-05, 1.7464e-05,\n",
            "        1.5378e-05, 1.6928e-05, 2.0683e-05, 1.4603e-05, 2.0683e-05, 1.9610e-05,\n",
            "        1.7226e-05, 1.9133e-05, 1.9610e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.9610e-05, 1.4007e-05, 2.0862e-05, 1.4722e-05, 1.6034e-05, 1.8477e-05,\n",
            "        1.5914e-05, 1.6928e-05, 2.0623e-05, 1.5199e-05, 1.9610e-05, 1.9610e-05,\n",
            "        1.7941e-05, 1.9133e-05, 1.9610e-05, 1.9610e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.1636e-05, 2.5809e-05, 3.2961e-05, 2.1636e-05, 2.6822e-05, 1.8835e-05,\n",
            "        1.8835e-05, 1.8597e-05, 2.0206e-05, 2.3007e-05, 2.1517e-05, 1.9848e-05,\n",
            "        1.8954e-05, 2.8789e-05, 1.8001e-05, 2.4736e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.8418e-05, 2.3603e-05, 4.2379e-05, 1.9372e-05, 2.1875e-05, 2.1279e-05,\n",
            "        1.9908e-05, 2.0683e-05, 2.3782e-05, 2.5809e-05, 1.9729e-05, 1.9550e-05,\n",
            "        2.0981e-05, 3.7670e-05, 1.7583e-05, 2.5690e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.1815e-05, 1.9133e-05, 2.3663e-05, 1.9729e-05, 2.1100e-05, 1.8895e-05,\n",
            "        1.9491e-05, 2.1398e-05, 2.5511e-05, 2.8670e-05, 2.1219e-05, 2.3544e-05,\n",
            "        2.0027e-05, 3.4571e-05, 1.7703e-05, 2.1100e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.9908e-05, 1.8120e-05, 1.7047e-05, 2.1398e-05, 1.9610e-05, 1.9908e-05,\n",
            "        1.8537e-05, 2.2233e-05, 1.8835e-05, 2.0564e-05, 1.9908e-05, 1.9372e-05,\n",
            "        2.0504e-05, 3.2365e-05, 2.0087e-05, 2.1160e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.1338e-05, 1.9431e-05, 1.7822e-05, 2.3544e-05, 2.2054e-05, 2.1875e-05,\n",
            "        2.0087e-05, 2.1636e-05, 2.0266e-05, 2.0146e-05, 2.1815e-05, 2.1994e-05,\n",
            "        2.1994e-05, 1.8477e-05, 2.1815e-05, 2.1040e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.4021e-05, 2.0564e-05, 1.8477e-05, 2.4915e-05, 2.4974e-05, 2.4021e-05,\n",
            "        2.1517e-05, 2.5213e-05, 2.2233e-05, 2.1577e-05, 2.4498e-05, 2.5213e-05,\n",
            "        2.4915e-05, 1.8060e-05, 2.3961e-05, 2.6286e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.3544e-05, 1.8656e-05, 1.8835e-05, 2.4855e-05, 1.5199e-05, 2.3603e-05,\n",
            "        1.8835e-05, 2.5332e-05, 1.8597e-05, 1.8716e-05, 1.6630e-05, 2.2531e-05,\n",
            "        1.4663e-05, 1.8775e-05, 2.4021e-05, 1.8299e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.6928e-05, 1.8299e-05, 1.9133e-05, 1.6093e-05, 1.5199e-05, 1.4842e-05,\n",
            "        1.7345e-05, 1.7226e-05, 1.8656e-05, 1.8716e-05, 1.6212e-05, 1.6749e-05,\n",
            "        1.5199e-05, 2.0564e-05, 1.6570e-05, 1.8954e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.6451e-05, 1.8299e-05, 1.7941e-05, 1.6153e-05, 1.5020e-05, 1.4842e-05,\n",
            "        1.7762e-05, 1.6093e-05, 1.5974e-05, 1.8299e-05, 1.5676e-05, 1.6332e-05,\n",
            "        1.5199e-05, 2.0981e-05, 1.6093e-05, 1.6093e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.6451e-05, 1.8299e-05, 1.9491e-05, 1.6153e-05, 1.5020e-05, 1.4842e-05,\n",
            "        1.8299e-05, 1.5795e-05, 1.6272e-05, 1.9014e-05, 1.4842e-05, 1.6332e-05,\n",
            "        1.5020e-05, 1.4603e-05, 1.5259e-05, 1.5199e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.5020e-05, 1.9014e-05, 1.3411e-05, 1.6153e-05, 1.5020e-05, 1.4842e-05,\n",
            "        1.8299e-05, 1.5795e-05, 1.6570e-05, 1.9014e-05, 1.4842e-05, 1.5020e-05,\n",
            "        1.5020e-05, 1.4544e-05, 1.5259e-05, 1.5199e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.5020e-05, 1.9014e-05, 1.6570e-05, 1.6153e-05, 1.5020e-05, 1.4842e-05,\n",
            "        1.8299e-05, 1.5795e-05, 1.7226e-05, 1.8299e-05, 1.4842e-05, 1.5020e-05,\n",
            "        1.4365e-05, 1.4544e-05, 1.5259e-05, 1.5199e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.5020e-05, 1.8299e-05, 1.6570e-05, 1.6153e-05, 1.5020e-05, 1.4842e-05,\n",
            "        1.8299e-05, 1.5795e-05, 1.7822e-05, 1.8299e-05, 1.4842e-05, 1.5020e-05,\n",
            "        1.4365e-05, 1.4544e-05, 1.5259e-05, 1.5199e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.5020e-05, 1.8299e-05, 1.6570e-05, 1.6153e-05, 1.5020e-05, 1.4842e-05,\n",
            "        1.8299e-05, 1.5020e-05, 1.8299e-05, 1.8299e-05, 1.4842e-05, 1.5020e-05,\n",
            "        1.4365e-05, 1.4544e-05, 1.5259e-05, 1.5020e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.5020e-05, 1.8299e-05, 1.5378e-05, 1.5020e-05, 1.5020e-05, 1.4484e-05,\n",
            "        1.8299e-05, 1.5080e-05, 1.8299e-05, 1.8299e-05, 1.4484e-05, 1.5020e-05,\n",
            "        1.4484e-05, 1.3411e-05, 1.4961e-05, 1.5020e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.4484e-05, 1.8299e-05, 1.3411e-05, 1.6093e-05, 1.4484e-05, 1.4484e-05,\n",
            "        1.8299e-05, 1.5080e-05, 1.8299e-05, 1.8299e-05, 1.4484e-05, 1.5080e-05,\n",
            "        1.4484e-05, 1.3411e-05, 1.4961e-05, 1.5080e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.4484e-05, 1.8299e-05, 1.3530e-05, 1.6093e-05, 1.4484e-05, 1.4484e-05,\n",
            "        1.8299e-05, 1.4484e-05, 1.8299e-05, 1.8299e-05, 1.4484e-05, 1.5020e-05,\n",
            "        1.4484e-05, 1.3411e-05, 1.4961e-05, 1.5020e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.4484e-05, 1.8299e-05, 1.3471e-05, 1.5259e-05, 1.4424e-05, 1.4484e-05,\n",
            "        1.8299e-05, 1.4544e-05, 1.8299e-05, 1.8299e-05, 1.4544e-05, 1.4603e-05,\n",
            "        1.5080e-05, 1.3411e-05, 1.5020e-05, 1.5199e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.4544e-05, 1.7822e-05, 1.3530e-05, 1.5259e-05, 1.4544e-05, 1.4007e-05,\n",
            "        1.8299e-05, 1.4722e-05, 1.8299e-05, 1.7822e-05, 1.4126e-05, 1.4663e-05,\n",
            "        1.5080e-05, 1.3411e-05, 1.4484e-05, 1.5140e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.4544e-05, 1.7822e-05, 1.3590e-05, 1.5497e-05, 1.4603e-05, 1.4067e-05,\n",
            "        1.8299e-05, 1.5020e-05, 1.8299e-05, 1.7822e-05, 1.4126e-05, 1.5020e-05,\n",
            "        1.5140e-05, 1.3411e-05, 1.4544e-05, 1.4961e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.4378e-05, 1.8001e-05, 1.8477e-05, 1.8179e-05, 2.2650e-05, 1.7524e-05,\n",
            "        2.9564e-05, 1.5438e-05, 1.9789e-05, 1.6272e-05, 1.9610e-05, 2.8491e-05,\n",
            "        1.8537e-05, 2.2292e-05, 2.5451e-05, 1.7524e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.2352e-05, 1.5199e-05, 1.7822e-05, 1.6749e-05, 2.5034e-05, 1.8597e-05,\n",
            "        1.5140e-05, 1.8418e-05, 2.3782e-05, 1.6272e-05, 2.0504e-05, 3.4988e-05,\n",
            "        1.6749e-05, 2.4796e-05, 1.9252e-05, 2.0742e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.2650e-05, 1.4961e-05, 1.8418e-05, 1.7345e-05, 2.2411e-05, 1.9848e-05,\n",
            "        1.8775e-05, 2.0921e-05, 2.1219e-05, 1.7047e-05, 2.4796e-05, 2.9683e-05,\n",
            "        1.6034e-05, 2.6882e-05, 2.4498e-05, 2.0206e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.0921e-05, 1.7822e-05, 2.0504e-05, 2.0921e-05, 2.1398e-05, 2.5570e-05,\n",
            "        1.7941e-05, 2.5928e-05, 1.9252e-05, 1.9431e-05, 1.8239e-05, 3.1054e-05,\n",
            "        1.8597e-05, 1.7166e-05, 1.9073e-05, 2.4438e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.9252e-05, 2.1517e-05, 2.3663e-05, 2.5928e-05, 2.7239e-05, 1.9252e-05,\n",
            "        1.9014e-05, 2.3425e-05, 2.1577e-05, 2.0504e-05, 1.8716e-05, 1.9848e-05,\n",
            "        2.1040e-05, 1.6689e-05, 2.1219e-05, 2.8551e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.9550e-05, 2.5034e-05, 2.1338e-05, 2.4915e-05, 2.3246e-05, 1.9670e-05,\n",
            "        2.0683e-05, 2.4021e-05, 2.1994e-05, 2.3544e-05, 2.0444e-05, 1.5497e-05,\n",
            "        2.2829e-05, 1.8477e-05, 1.9014e-05, 2.5690e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.9610e-05, 2.4021e-05, 2.2173e-05, 2.3544e-05, 1.9610e-05, 1.9610e-05,\n",
            "        1.7345e-05, 2.4021e-05, 1.9610e-05, 2.1577e-05, 2.1040e-05, 2.0206e-05,\n",
            "        2.2888e-05, 1.7405e-05, 1.9372e-05, 2.4498e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.0504e-05, 1.8656e-05, 1.5795e-05, 1.4544e-05, 2.0504e-05, 2.0504e-05,\n",
            "        1.5676e-05, 1.8716e-05, 2.0504e-05, 1.7285e-05, 2.1875e-05, 2.0206e-05,\n",
            "        1.5020e-05, 1.4186e-05, 1.9431e-05, 2.1636e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.0504e-05, 1.5795e-05, 1.5795e-05, 1.5378e-05, 2.0504e-05, 2.0504e-05,\n",
            "        1.6093e-05, 1.5795e-05, 2.0504e-05, 1.7285e-05, 2.1875e-05, 1.5616e-05,\n",
            "        1.5020e-05, 1.4246e-05, 1.9431e-05, 2.2769e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.0504e-05, 1.5795e-05, 1.5795e-05, 1.5378e-05, 2.0504e-05, 2.0504e-05,\n",
            "        1.6928e-05, 1.5795e-05, 2.0504e-05, 1.7285e-05, 2.1875e-05, 1.5616e-05,\n",
            "        1.5020e-05, 1.4424e-05, 1.9431e-05, 2.2769e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.0504e-05, 1.5795e-05, 1.4901e-05, 1.7285e-05, 2.0504e-05, 2.0504e-05,\n",
            "        1.8358e-05, 1.4901e-05, 2.0504e-05, 1.5378e-05, 2.1040e-05, 1.4246e-05,\n",
            "        1.5020e-05, 1.4782e-05, 1.9431e-05, 2.2054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.0504e-05, 1.4544e-05, 1.4901e-05, 1.7285e-05, 2.0504e-05, 2.0504e-05,\n",
            "        1.4067e-05, 1.4901e-05, 2.0504e-05, 1.5438e-05, 1.9193e-05, 1.4246e-05,\n",
            "        1.5020e-05, 1.2875e-05, 1.9431e-05, 2.2054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.0921e-05, 1.4603e-05, 1.4901e-05, 1.4544e-05, 1.9610e-05, 2.0921e-05,\n",
            "        1.4365e-05, 1.4722e-05, 2.0504e-05, 1.6868e-05, 1.9193e-05, 1.4246e-05,\n",
            "        1.4186e-05, 1.3173e-05, 1.9908e-05, 2.2054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.0921e-05, 1.4722e-05, 1.4901e-05, 1.5378e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.4544e-05, 1.4842e-05, 2.0504e-05, 1.7107e-05, 1.6928e-05, 1.4246e-05,\n",
            "        1.4186e-05, 1.3411e-05, 1.9908e-05, 2.2054e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.9670e-05, 1.4782e-05, 1.5676e-05, 1.4186e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.4901e-05, 1.6391e-05, 1.9670e-05, 1.5616e-05, 1.6928e-05, 1.4246e-05,\n",
            "        1.4246e-05, 1.3649e-05, 1.9908e-05, 2.0564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.9670e-05, 1.5140e-05, 1.4603e-05, 1.4186e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.5855e-05, 1.5676e-05, 2.0921e-05, 1.5140e-05, 1.6928e-05, 1.4246e-05,\n",
            "        1.4544e-05, 1.4126e-05, 1.9908e-05, 2.0564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.9670e-05, 1.5557e-05, 1.4663e-05, 1.4246e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.6451e-05, 1.6332e-05, 2.0921e-05, 1.5497e-05, 1.6630e-05, 1.4305e-05,\n",
            "        1.5497e-05, 1.4365e-05, 1.9908e-05, 2.0564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.9670e-05, 1.6153e-05, 1.4722e-05, 1.5259e-05, 1.9670e-05, 1.9670e-05,\n",
            "        1.7285e-05, 1.7166e-05, 1.9670e-05, 1.6034e-05, 1.6630e-05, 1.4544e-05,\n",
            "        1.6034e-05, 1.5140e-05, 1.9252e-05, 2.0564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.9670e-05, 1.8954e-05, 1.5736e-05, 1.6153e-05, 2.0921e-05, 1.9670e-05,\n",
            "        1.8477e-05, 1.8358e-05, 2.0921e-05, 1.6570e-05, 1.6630e-05, 1.4782e-05,\n",
            "        1.6809e-05, 1.4484e-05, 1.9252e-05, 2.0564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.0921e-05, 2.0266e-05, 1.6809e-05, 1.6391e-05, 2.0921e-05, 2.0564e-05,\n",
            "        1.9968e-05, 1.9312e-05, 2.0921e-05, 1.7524e-05, 1.6630e-05, 1.5259e-05,\n",
            "        1.7047e-05, 1.4722e-05, 1.9372e-05, 2.0564e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "dided\n",
            "time\n",
            "[0, 3, 9, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 9, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 9, 3, 0, 3, 9, 3, 0, 3, 0, 3, 0, 0, 9, 3, 0, 0, 9, 3, 0, 3, 0, 3, 0, 3, 9, 3, 0, 0, 9, 3, 0, 3, 3, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 9, 3, 14, 0, 3, 3, 0, 3, 0, 3, 0, 3, 9, 3, 0, 3, 0, 3, 14, 13, 9, 3, 0, 3, 3, 3, 0, 0, 9, 3, 9, 3, 3, 3, 0, 3, 9, 3, 0, 3, 9, 3, 0, 3, 0, 3, 0, 3, 3, 3, 0, 3, 9, 3, 0, 0, 9, 3, 9, 3, 3, 3, 0, 3, 0, 3, 0, 3, 9, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 9, 3, 0, 3, 3, 3, 0, 0, 9, 3, 0, 0, 3, 3, 0, 14, 9, 3, 0, 14, 9, 3, 0, 3, 9, 3, 0, 3, 3, 3, 0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 9, 3, 0, 3, 9, 3, 0, 0, 9, 3, 0, 3, 3, 3, 0, 3, 0, 3, 0, 3, 3, 3, 0, 3, 3, 3, 0, 0, 9, 3, 0, 3, 3, 3, 0, 0, 9, 3, 0]\n",
            "28 #### train ####\n",
            "repr, std, cov, closs 0.017709828913211823 0.476806640625 0.0002518256660550833 0.31975990533828735\n",
            "0.02174242029239387 0.05102649661160923 1.0\n",
            "repr, std, cov, closs 0.015377203933894634 0.478271484375 0.00018415087834000587 0.3315885663032532\n",
            "0.021764162712686264 0.05174552960018793 1.0\n",
            "repr, std, cov, closs 0.018949490040540695 0.47607421875 0.00027295853942632675 0.31327593326568604\n",
            "0.021829520515076615 0.05247469474505791 1.0\n",
            "repr, std, cov, closs 0.01664983667433262 0.476318359375 0.00030669732950627804 0.6252709031105042\n",
            "0.021829520515076615 0.05273759349073999 1.0\n",
            "repr, std, cov, closs 0.01413374487310648 0.477783203125 0.00020142225548624992 0.3259289264678955\n",
            "0.021895074587012904 0.053748677203304535 1.0\n",
            "repr, std, cov, closs 0.0110960453748703 0.47705078125 0.00023682322353124619 0.32888898253440857\n",
            "0.021960825517892774 0.054397220593796744 1.0\n",
            "repr, std, cov, closs 0.014444392174482346 0.475830078125 0.0002666541840881109 0.7420922517776489\n",
            "0.021677323268939928 0.054180174597583966 1.0\n",
            "repr, std, cov, closs 0.011613136157393456 0.4755859375 0.00028267642483115196 0.7809923887252808\n",
            "0.02137610477422389 0.05321413482213171 1.0\n",
            "repr, std, cov, closs 0.010646140202879906 0.476318359375 0.00024610012769699097 0.35411337018013\n",
            "0.02137610477422389 0.053480738169998036 1.0\n",
            "repr, std, cov, closs 0.017574742436408997 0.475830078125 0.0003542075864970684 0.32830795645713806\n",
            "0.02120586291770664 0.05326734895695384 1.0\n",
            "repr, std, cov, closs 0.014934263192117214 0.474609375 0.00036456785164773464 0.32242652773857117\n",
            "0.02082775953942395 0.05205677999642012 1.0\n",
            "repr, std, cov, closs 0.013751056976616383 0.474609375 0.00033006793819367886 0.5446692109107971\n",
            "0.020517828378092438 0.05077212774363844 1.0\n",
            "repr, std, cov, closs 0.022562367841601372 0.47314453125 0.00044298009015619755 0.5152571797370911\n",
            "0.020374776359442095 0.05006662071665316 1.0\n",
            "repr, std, cov, closs 0.011901259422302246 0.4765625 0.00024248147383332253 0.5495652556419373\n",
            "0.020374776359442095 0.050468558355004633 1.0\n",
            "repr, std, cov, closs 0.014011873863637447 0.47607421875 0.0002819837536662817 0.3315894603729248\n",
            "0.020395151135801533 0.050670735601699456 1.0\n",
            "repr, std, cov, closs 0.009293495677411556 0.47607421875 0.00028404081240296364 0.32892265915870667\n",
            "0.020313774075579365 0.05062011548621325 1.0\n",
            "repr, std, cov, closs 0.016769547015428543 0.476318359375 0.000223987502977252 0.3214414417743683\n",
            "0.020395151135801533 0.05153906390398082 1.0\n",
            "0 tensor([0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0036, 0.0035, 0.0035,\n",
            "        0.0036, 0.0036, 0.0036, 0.0036, 0.0035, 0.0036, 0.0035],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0036, 0.0036, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0035, 0.0035,\n",
            "        0.0036, 0.0036, 0.0035, 0.0036, 0.0035, 0.0035, 0.0035],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0036, 0.0036, 0.0035, 0.0035, 0.0036, 0.0035, 0.0036, 0.0035, 0.0035,\n",
            "        0.0036, 0.0036, 0.0035, 0.0035, 0.0035, 0.0036, 0.0035],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024,\n",
            "        0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024,\n",
            "        0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0026, 0.0027, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026,\n",
            "        0.0026, 0.0026, 0.0024, 0.0026, 0.0026, 0.0026, 0.0026],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0026, 0.0027, 0.0026, 0.0026, 0.0027, 0.0026, 0.0027, 0.0026, 0.0026,\n",
            "        0.0026, 0.0027, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0026, 0.0027, 0.0026, 0.0026, 0.0027, 0.0026, 0.0027, 0.0026, 0.0026,\n",
            "        0.0026, 0.0027, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0026, 0.0027, 0.0026, 0.0026, 0.0027, 0.0026, 0.0027, 0.0026, 0.0026,\n",
            "        0.0026, 0.0027, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0023, 0.0024, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0023, 0.0023,\n",
            "        0.0023, 0.0024, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0023, 0.0024, 0.0023, 0.0023, 0.0024, 0.0023, 0.0024, 0.0023, 0.0023,\n",
            "        0.0023, 0.0024, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0023, 0.0022, 0.0022,\n",
            "        0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0023, 0.0022, 0.0022,\n",
            "        0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0023, 0.0022, 0.0022,\n",
            "        0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0023, 0.0022, 0.0022,\n",
            "        0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
            "        0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
            "        0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
            "        0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
            "        0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0022, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
            "        0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0016, 0.0016, 0.0017, 0.0017,\n",
            "        0.0016, 0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0016, 0.0017, 0.0016, 0.0017, 0.0017, 0.0016, 0.0016, 0.0017, 0.0016,\n",
            "        0.0016, 0.0016, 0.0017, 0.0017, 0.0017, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0016, 0.0017, 0.0016, 0.0017, 0.0017, 0.0016, 0.0016, 0.0017, 0.0016,\n",
            "        0.0016, 0.0016, 0.0017, 0.0016, 0.0017, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0016, 0.0017, 0.0017,\n",
            "        0.0016, 0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0016, 0.0016, 0.0017, 0.0017,\n",
            "        0.0016, 0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0016, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "        0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "        0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016,\n",
            "        0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0017, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "        0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0017, 0.0016,\n",
            "        0.0016, 0.0016, 0.0017, 0.0016, 0.0017, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0016, 0.0017, 0.0017, 0.0016, 0.0017, 0.0017, 0.0016, 0.0017, 0.0016,\n",
            "        0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "        0.0016, 0.0017, 0.0016, 0.0017, 0.0016, 0.0017, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0017, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017,\n",
            "        0.0016, 0.0016, 0.0016, 0.0017, 0.0017, 0.0017, 0.0017],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017,\n",
            "        0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "        0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016,\n",
            "        0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0017, 0.0016, 0.0016, 0.0017, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016,\n",
            "        0.0016, 0.0017, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0016,\n",
            "        0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0016, 0.0016, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "        0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0017, 0.0016],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0016, 0.0017, 0.0016, 0.0017, 0.0017, 0.0016, 0.0016, 0.0017, 0.0017,\n",
            "        0.0016, 0.0016, 0.0016, 0.0017, 0.0016, 0.0016, 0.0017],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006, 0.0005, 0.0005, 0.0005,\n",
            "        0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006, 0.0005, 0.0005, 0.0005,\n",
            "        0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
            "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([0.0068, 0.0069, 0.0067, 0.0068, 0.0069, 0.0069, 0.0069, 0.0068, 0.0068,\n",
            "        0.0068, 0.0070, 0.0068, 0.0070, 0.0069, 0.0068, 0.0068],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0061, 0.0063, 0.0061, 0.0062, 0.0063, 0.0064, 0.0061, 0.0061, 0.0062,\n",
            "        0.0061, 0.0062, 0.0062, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0062, 0.0062, 0.0061, 0.0062, 0.0063, 0.0064, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0062, 0.0062, 0.0062, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0062, 0.0063, 0.0061, 0.0062, 0.0064, 0.0065, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0063, 0.0064, 0.0062, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0062, 0.0063, 0.0061, 0.0062, 0.0063, 0.0065, 0.0061, 0.0062, 0.0063,\n",
            "        0.0062, 0.0063, 0.0064, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0062, 0.0063, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0064,\n",
            "        0.0062, 0.0063, 0.0062, 0.0063, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0061, 0.0063, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0063, 0.0062, 0.0063, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0061, 0.0063, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0063, 0.0062, 0.0063, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0061, 0.0063, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0063, 0.0062, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0061, 0.0063, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0061, 0.0062, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0061, 0.0062, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0061, 0.0063, 0.0061, 0.0061, 0.0062, 0.0063],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0061, 0.0063, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0063,\n",
            "        0.0062, 0.0061, 0.0062, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0063, 0.0063, 0.0061, 0.0062, 0.0063,\n",
            "        0.0062, 0.0061, 0.0062, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0061, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0061, 0.0062, 0.0061, 0.0061, 0.0062, 0.0063],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0061, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0061, 0.0062, 0.0061, 0.0061, 0.0062, 0.0063],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0061, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0061, 0.0062, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0061, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0061, 0.0063, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0061, 0.0061, 0.0061, 0.0062, 0.0061, 0.0063, 0.0061, 0.0062, 0.0062,\n",
            "        0.0062, 0.0061, 0.0063, 0.0061, 0.0061, 0.0062, 0.0062],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([5.3346e-05, 4.7624e-05, 4.9651e-05, 4.8697e-05, 4.9710e-05, 5.5671e-05,\n",
            "        5.6744e-05, 5.0485e-05, 6.0439e-05, 4.6313e-05, 6.5863e-05, 5.5611e-05,\n",
            "        5.4061e-05, 5.7578e-05, 4.8161e-05, 5.6088e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.6982e-05, 4.7743e-05, 4.9829e-05, 4.9233e-05, 5.0068e-05, 5.6624e-05,\n",
            "        5.8055e-05, 5.2094e-05, 5.9307e-05, 4.7266e-05, 6.0201e-05, 5.6982e-05,\n",
            "        5.5194e-05, 5.2750e-05, 4.6551e-05, 5.6326e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([5.1677e-05, 5.0008e-05, 5.1141e-05, 4.8697e-05, 5.2750e-05, 5.4955e-05,\n",
            "        5.3227e-05, 5.3227e-05, 5.5373e-05, 4.9412e-05, 5.8413e-05, 5.7817e-05,\n",
            "        5.1677e-05, 5.4777e-05, 4.9174e-05, 4.8339e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([5.2691e-05, 5.1558e-05, 5.1975e-05, 5.0902e-05, 5.3048e-05, 5.6207e-05,\n",
            "        5.3227e-05, 5.3883e-05, 5.4955e-05, 5.2512e-05, 5.2631e-05, 5.8413e-05,\n",
            "        5.2869e-05, 5.0426e-05, 5.4836e-05, 5.0485e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([5.2094e-05, 5.3048e-05, 5.2869e-05, 5.2273e-05, 5.2273e-05, 5.2094e-05,\n",
            "        5.3108e-05, 5.3883e-05, 5.2869e-05, 5.2094e-05, 4.8041e-05, 5.3287e-05,\n",
            "        5.3346e-05, 5.2094e-05, 5.2869e-05, 5.2273e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.3511e-05, 4.3988e-05, 4.1008e-05, 4.2260e-05, 4.2439e-05, 4.3511e-05,\n",
            "        4.2260e-05, 4.1723e-05, 4.1008e-05, 4.2260e-05, 3.9697e-05, 4.2439e-05,\n",
            "        4.4167e-05, 4.3511e-05, 4.1008e-05, 4.2260e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.8445e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.9697e-05, 4.0889e-05,\n",
            "        3.9458e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7670e-05, 3.9697e-05,\n",
            "        4.1127e-05, 4.0889e-05, 3.7909e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.8445e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.8147e-05, 4.0889e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05, 3.8266e-05, 3.9697e-05,\n",
            "        3.7849e-05, 3.9697e-05, 3.7909e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.8445e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.7909e-05, 4.0889e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8505e-05, 3.9697e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.7909e-05, 3.8445e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.9160e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8445e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8445e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.9458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8445e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8445e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8207e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8207e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8207e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8207e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.9458e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.8147e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.8147e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05,\n",
            "        3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05, 3.7909e-05,\n",
            "        3.7909e-05, 3.8147e-05, 3.7909e-05, 3.7909e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
            "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([4.2200e-05, 5.0902e-05, 4.6611e-05, 4.8459e-05, 4.8876e-05, 4.7684e-05,\n",
            "        4.8280e-05, 5.6386e-05, 5.0485e-05, 4.9174e-05, 4.9233e-05, 5.0724e-05,\n",
            "        5.0485e-05, 5.3942e-05, 5.2631e-05, 5.1558e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([4.6432e-05, 5.1677e-05, 4.8757e-05, 4.7445e-05, 4.6611e-05, 4.8578e-05,\n",
            "        4.8280e-05, 5.5611e-05, 4.7565e-05, 4.7028e-05, 5.0664e-05, 5.0008e-05,\n",
            "        4.9531e-05, 5.0306e-05, 4.9770e-05, 4.7863e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.7684e-05, 5.4479e-05, 5.0545e-05, 5.2333e-05, 4.8995e-05, 5.0068e-05,\n",
            "        4.8876e-05, 5.4896e-05, 5.0306e-05, 4.8876e-05, 5.2094e-05, 5.1141e-05,\n",
            "        5.0068e-05, 5.1498e-05, 5.0128e-05, 5.0008e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.8935e-05, 5.2333e-05, 5.1081e-05, 5.1975e-05, 5.0008e-05, 4.9949e-05,\n",
            "        5.0008e-05, 5.2333e-05, 5.0604e-05, 4.9710e-05, 5.0128e-05, 5.1260e-05,\n",
            "        5.1200e-05, 5.0068e-05, 5.0843e-05, 5.0068e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([5.0485e-05, 5.1856e-05, 5.0545e-05, 5.0247e-05, 5.0366e-05, 4.9949e-05,\n",
            "        5.0068e-05, 5.3108e-05, 5.0247e-05, 4.9949e-05, 4.9949e-05, 5.0187e-05,\n",
            "        5.0128e-05, 5.0545e-05, 5.1141e-05, 5.0128e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.9949e-05, 5.1856e-05, 4.9949e-05, 5.0128e-05, 5.0128e-05, 4.9949e-05,\n",
            "        5.0068e-05, 4.9412e-05, 5.0128e-05, 4.9949e-05, 4.9949e-05, 5.0187e-05,\n",
            "        5.0128e-05, 5.0247e-05, 4.9949e-05, 5.0128e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([4.5896e-05, 4.4942e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.5657e-05,\n",
            "        4.4584e-05, 4.5657e-05, 4.4703e-05, 4.5896e-05, 4.4465e-05, 4.4823e-05,\n",
            "        4.4703e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([4.5896e-05, 4.4942e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.5657e-05,\n",
            "        4.4465e-05, 4.5657e-05, 4.4703e-05, 4.4703e-05, 4.4465e-05, 4.4823e-05,\n",
            "        4.4703e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([4.5896e-05, 4.4942e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.5657e-05, 4.4703e-05, 4.4703e-05, 4.4465e-05, 4.4823e-05,\n",
            "        4.4703e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([4.4703e-05, 4.4942e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.5657e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4703e-05,\n",
            "        4.4703e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([4.4703e-05, 4.4942e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.5657e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4703e-05,\n",
            "        4.4703e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([4.4703e-05, 4.6551e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.5657e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([4.4703e-05, 4.4942e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.5657e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([4.4703e-05, 4.4942e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([4.4703e-05, 4.9055e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([4.4703e-05, 4.4465e-05, 4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4465e-05, 4.4763e-05, 4.4465e-05, 4.4465e-05, 4.4465e-05,\n",
            "        4.4465e-05, 4.4703e-05, 4.4465e-05, 4.4703e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.5300e-06, 3.9339e-06, 3.7551e-06, 3.3379e-06, 4.9472e-06, 3.0994e-06,\n",
            "        3.9339e-06, 2.1458e-06, 3.9935e-06, 5.1260e-06, 3.5763e-06, 5.2452e-06,\n",
            "        2.5630e-06, 3.4571e-06, 3.6359e-06, 6.4373e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.2187e-06, 4.0531e-06, 3.8743e-06, 2.4438e-06, 4.4107e-06, 3.3379e-06,\n",
            "        4.1723e-06, 2.2650e-06, 3.5167e-06, 3.2187e-06, 3.8147e-06, 3.7551e-06,\n",
            "        2.6226e-06, 3.2187e-06, 3.7551e-06, 5.3048e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.7551e-06, 3.9339e-06, 2.6226e-06, 2.4438e-06, 4.5300e-06, 2.9802e-06,\n",
            "        4.8280e-06, 2.5034e-06, 3.8147e-06, 3.8147e-06, 4.3511e-06, 4.3511e-06,\n",
            "        2.5034e-06, 3.7551e-06, 4.6492e-06, 6.5565e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.9935e-06, 3.2187e-06, 3.0398e-06, 2.7418e-06, 3.4571e-06, 2.7418e-06,\n",
            "        3.3379e-06, 2.9802e-06, 3.2783e-06, 3.4571e-06, 2.2054e-06, 3.9935e-06,\n",
            "        2.8610e-06, 3.0398e-06, 3.5763e-06, 4.7684e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.7418e-06, 2.7418e-06, 2.8610e-06, 2.8610e-06, 2.9206e-06, 2.7418e-06,\n",
            "        2.5630e-06, 3.2187e-06, 2.9206e-06, 2.7418e-06, 1.9073e-06, 2.8610e-06,\n",
            "        2.8014e-06, 2.8014e-06, 2.5630e-06, 3.6359e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.7418e-06, 2.9206e-06, 2.9802e-06, 2.5630e-06, 2.9802e-06, 2.6822e-06,\n",
            "        2.5630e-06, 3.0398e-06, 2.6822e-06, 2.6822e-06, 1.9073e-06, 3.0994e-06,\n",
            "        2.9206e-06, 3.0398e-06, 2.4438e-06, 2.9802e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.7418e-06, 2.8610e-06, 2.9802e-06, 2.4438e-06, 3.0398e-06, 2.7418e-06,\n",
            "        2.8014e-06, 3.2187e-06, 2.6822e-06, 2.7418e-06, 1.9073e-06, 3.2187e-06,\n",
            "        3.0398e-06, 3.0994e-06, 2.5630e-06, 3.0398e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.7418e-06, 2.5630e-06, 2.5630e-06, 2.4438e-06, 2.9802e-06, 2.8014e-06,\n",
            "        2.8014e-06, 2.3842e-06, 2.5034e-06, 2.9206e-06, 1.9073e-06, 3.2187e-06,\n",
            "        2.1458e-06, 2.9206e-06, 2.5034e-06, 3.0398e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.2187e-06, 2.3842e-06, 2.4438e-06, 2.6226e-06, 2.9802e-06, 2.0862e-06,\n",
            "        2.8014e-06, 2.3842e-06, 2.2650e-06, 3.2187e-06, 1.9073e-06, 3.0398e-06,\n",
            "        2.1458e-06, 2.7418e-06, 2.5034e-06, 3.0398e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.9206e-06, 2.3842e-06, 2.5630e-06, 2.2650e-06, 2.5630e-06, 2.0266e-06,\n",
            "        2.5630e-06, 2.2650e-06, 2.2650e-06, 3.2187e-06, 1.9670e-06, 2.5034e-06,\n",
            "        2.1458e-06, 2.6822e-06, 2.2650e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.8610e-06, 2.4438e-06, 2.5630e-06, 2.2650e-06, 2.5630e-06, 1.9670e-06,\n",
            "        2.4438e-06, 2.2054e-06, 2.2650e-06, 2.4438e-06, 1.9670e-06, 2.5034e-06,\n",
            "        2.1458e-06, 2.6822e-06, 2.2650e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.5034e-06, 2.3842e-06, 2.5630e-06, 2.2650e-06, 2.5630e-06, 2.0266e-06,\n",
            "        2.4438e-06, 2.0862e-06, 2.1458e-06, 2.3246e-06, 1.9670e-06, 2.5034e-06,\n",
            "        1.9073e-06, 2.5034e-06, 2.2650e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.5630e-06, 2.3246e-06, 2.6226e-06, 2.2650e-06, 2.7418e-06, 1.9670e-06,\n",
            "        2.2650e-06, 2.1458e-06, 2.1458e-06, 2.4438e-06, 2.0266e-06, 2.5034e-06,\n",
            "        1.9670e-06, 2.5034e-06, 2.2650e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.6226e-06, 2.3246e-06, 2.3246e-06, 2.2650e-06, 2.5630e-06, 2.0266e-06,\n",
            "        2.2650e-06, 2.2650e-06, 2.1458e-06, 2.4438e-06, 2.0862e-06, 2.5034e-06,\n",
            "        2.0266e-06, 2.5034e-06, 2.2650e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.6226e-06, 2.3246e-06, 2.2650e-06, 2.4438e-06, 2.5630e-06, 2.1458e-06,\n",
            "        2.2650e-06, 2.1458e-06, 2.1458e-06, 2.4438e-06, 2.1458e-06, 2.5034e-06,\n",
            "        1.9670e-06, 2.5630e-06, 2.4438e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.6226e-06, 2.3246e-06, 2.3246e-06, 2.4438e-06, 2.5630e-06, 2.0266e-06,\n",
            "        2.2650e-06, 2.1458e-06, 2.1458e-06, 2.2650e-06, 2.3842e-06, 2.5034e-06,\n",
            "        1.9670e-06, 2.5630e-06, 2.4438e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.4438e-06, 2.3246e-06, 2.3842e-06, 2.4438e-06, 2.5630e-06, 2.1458e-06,\n",
            "        2.4438e-06, 2.1458e-06, 2.0862e-06, 2.2650e-06, 2.5034e-06, 2.5034e-06,\n",
            "        2.0266e-06, 2.5630e-06, 2.4438e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.4438e-06, 2.3246e-06, 2.3246e-06, 2.2650e-06, 2.3842e-06, 2.2054e-06,\n",
            "        2.4438e-06, 2.2054e-06, 1.9670e-06, 2.2650e-06, 2.4438e-06, 2.5034e-06,\n",
            "        1.9073e-06, 2.5034e-06, 2.6226e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.4438e-06, 2.3246e-06, 2.2650e-06, 2.2650e-06, 2.3842e-06, 2.3246e-06,\n",
            "        2.4438e-06, 2.2650e-06, 1.9670e-06, 2.2650e-06, 2.4438e-06, 2.5034e-06,\n",
            "        1.9670e-06, 2.5034e-06, 2.2650e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.4438e-06, 2.3246e-06, 2.2650e-06, 2.4438e-06, 2.3842e-06, 2.3842e-06,\n",
            "        2.2650e-06, 2.2054e-06, 1.9670e-06, 2.4438e-06, 2.3842e-06, 2.5034e-06,\n",
            "        1.9670e-06, 2.5034e-06, 2.4438e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([5.9009e-06, 4.1127e-06, 4.7684e-06, 4.7684e-06, 5.5432e-06, 4.3511e-06,\n",
            "        5.7817e-06, 1.0431e-05, 5.9009e-06, 4.4107e-06, 5.2452e-06, 5.7220e-06,\n",
            "        6.0797e-06, 4.3511e-06, 5.4240e-06, 5.9605e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.4836e-06, 4.5896e-06, 3.8147e-06, 6.3181e-06, 3.9339e-06, 3.8147e-06,\n",
            "        4.0531e-06, 6.0797e-06, 3.8147e-06, 4.3511e-06, 5.3644e-06, 6.6161e-06,\n",
            "        6.7949e-06, 4.0531e-06, 3.5763e-06, 4.1127e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.8147e-06, 4.5896e-06, 3.5167e-06, 5.3048e-06, 4.1723e-06, 3.9935e-06,\n",
            "        4.0531e-06, 5.2452e-06, 3.4571e-06, 4.3511e-06, 6.0797e-06, 4.1127e-06,\n",
            "        6.6757e-06, 4.6492e-06, 4.1127e-06, 3.5763e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.0531e-06, 4.1127e-06, 3.6955e-06, 3.9935e-06, 4.8280e-06, 4.2915e-06,\n",
            "        3.8743e-06, 4.1127e-06, 3.7551e-06, 4.3511e-06, 5.8413e-06, 4.4703e-06,\n",
            "        5.3048e-06, 4.7684e-06, 4.5896e-06, 3.5167e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([4.0531e-06, 4.4107e-06, 4.1723e-06, 3.3975e-06, 3.9935e-06, 4.8280e-06,\n",
            "        4.0531e-06, 3.3379e-06, 3.9339e-06, 4.0531e-06, 4.5300e-06, 4.5300e-06,\n",
            "        5.3644e-06, 5.0664e-06, 3.6955e-06, 3.6359e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.0531e-06, 4.5896e-06, 4.3511e-06, 3.3975e-06, 4.0531e-06, 4.8876e-06,\n",
            "        3.7551e-06, 3.3975e-06, 3.9339e-06, 4.0531e-06, 3.6359e-06, 4.6492e-06,\n",
            "        3.6955e-06, 4.7088e-06, 3.7551e-06, 3.6955e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([4.0531e-06, 2.6822e-06, 2.8014e-06, 3.3975e-06, 4.2915e-06, 3.0994e-06,\n",
            "        3.0398e-06, 3.3379e-06, 3.3975e-06, 3.0994e-06, 3.6955e-06, 2.9802e-06,\n",
            "        3.6955e-06, 2.8014e-06, 3.7551e-06, 3.9339e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.5763e-06, 2.6822e-06, 2.8014e-06, 3.3975e-06, 4.2319e-06, 3.0994e-06,\n",
            "        3.0398e-06, 3.5167e-06, 3.4571e-06, 2.9802e-06, 3.3379e-06, 2.9206e-06,\n",
            "        3.6359e-06, 2.8014e-06, 3.8743e-06, 3.1590e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.7551e-06, 2.6822e-06, 2.8014e-06, 3.4571e-06, 4.2319e-06, 3.0994e-06,\n",
            "        3.0994e-06, 3.6359e-06, 3.4571e-06, 2.9802e-06, 3.3379e-06, 2.9206e-06,\n",
            "        3.6955e-06, 2.8014e-06, 3.5763e-06, 3.0398e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.6359e-06, 2.7418e-06, 2.9206e-06, 3.1590e-06, 3.6359e-06, 3.0994e-06,\n",
            "        3.0994e-06, 3.8743e-06, 3.0994e-06, 3.0398e-06, 3.3975e-06, 2.9206e-06,\n",
            "        3.3975e-06, 2.8014e-06, 3.4571e-06, 3.0994e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.5763e-06, 2.7418e-06, 2.9206e-06, 3.2187e-06, 3.6359e-06, 2.9206e-06,\n",
            "        2.9802e-06, 3.6955e-06, 2.9802e-06, 2.9802e-06, 3.1590e-06, 2.9206e-06,\n",
            "        3.3975e-06, 2.9206e-06, 3.4571e-06, 3.1590e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.5763e-06, 2.8014e-06, 2.9206e-06, 3.3975e-06, 3.5763e-06, 2.9206e-06,\n",
            "        2.9802e-06, 3.6955e-06, 2.9802e-06, 2.9802e-06, 3.1590e-06, 2.8014e-06,\n",
            "        3.6955e-06, 2.9206e-06, 3.1590e-06, 3.0994e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.1590e-06, 2.8014e-06, 2.9206e-06, 3.3975e-06, 3.1590e-06, 2.9206e-06,\n",
            "        2.9802e-06, 3.7551e-06, 2.9802e-06, 2.9802e-06, 3.3379e-06, 2.8014e-06,\n",
            "        3.3975e-06, 2.9206e-06, 3.1590e-06, 3.1590e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.1590e-06, 2.8014e-06, 2.8014e-06, 3.5763e-06, 3.1590e-06, 2.9206e-06,\n",
            "        2.9802e-06, 3.7551e-06, 2.9802e-06, 2.9802e-06, 3.3379e-06, 2.8014e-06,\n",
            "        3.3975e-06, 2.8014e-06, 3.1590e-06, 3.0994e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.1590e-06, 2.7418e-06, 2.7418e-06, 3.5763e-06, 3.1590e-06, 2.9206e-06,\n",
            "        2.9802e-06, 3.8147e-06, 2.9802e-06, 2.9802e-06, 3.3379e-06, 2.8014e-06,\n",
            "        3.3975e-06, 2.7418e-06, 3.1590e-06, 3.0994e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.1590e-06, 2.7418e-06, 2.6822e-06, 3.5763e-06, 3.1590e-06, 2.8014e-06,\n",
            "        2.9802e-06, 3.8743e-06, 2.9802e-06, 2.9802e-06, 3.2783e-06, 2.7418e-06,\n",
            "        3.6955e-06, 2.7418e-06, 3.1590e-06, 3.1590e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.1590e-06, 2.7418e-06, 3.3975e-06, 3.5763e-06, 3.1590e-06, 2.8014e-06,\n",
            "        2.9802e-06, 3.6359e-06, 2.9802e-06, 2.9802e-06, 3.3379e-06, 2.7418e-06,\n",
            "        3.1590e-06, 3.2783e-06, 3.1590e-06, 3.0398e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.1590e-06, 3.3379e-06, 2.7418e-06, 3.5763e-06, 3.1590e-06, 2.8014e-06,\n",
            "        3.0994e-06, 3.6359e-06, 2.9802e-06, 2.9802e-06, 3.3379e-06, 2.7418e-06,\n",
            "        3.1590e-06, 3.3379e-06, 3.1590e-06, 3.0398e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.1590e-06, 3.5167e-06, 2.6226e-06, 3.5763e-06, 3.2187e-06, 2.6822e-06,\n",
            "        3.0994e-06, 3.6359e-06, 2.9802e-06, 2.9802e-06, 3.3379e-06, 3.3975e-06,\n",
            "        3.2187e-06, 2.7418e-06, 3.1590e-06, 2.9802e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.1590e-06, 2.8014e-06, 2.7418e-06, 3.4571e-06, 3.2187e-06, 2.6822e-06,\n",
            "        3.0994e-06, 3.6359e-06, 2.9802e-06, 2.9802e-06, 3.2783e-06, 3.5167e-06,\n",
            "        3.2187e-06, 2.7418e-06, 3.1590e-06, 2.9802e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.3842e-06, 1.7881e-06, 7.7486e-07, 1.3709e-06, 1.4305e-06, 1.2517e-06,\n",
            "        7.1526e-07, 1.8477e-06, 1.0729e-06, 2.2054e-06, 2.8610e-06, 8.9407e-07,\n",
            "        3.9339e-06, 2.2054e-06, 2.0266e-06, 8.3447e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.4438e-06, 1.1325e-06, 1.0729e-06, 8.9407e-07, 2.0862e-06, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 7.1526e-07, 2.3842e-06, 1.9670e-06, 9.5367e-07,\n",
            "        4.7088e-06, 2.6226e-06, 1.1921e-06, 8.3447e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.2054e-06, 1.0133e-06, 1.4305e-06, 8.3447e-07, 1.5497e-06, 1.4901e-06,\n",
            "        7.1526e-07, 1.6093e-06, 7.7486e-07, 2.8014e-06, 2.2054e-06, 7.1526e-07,\n",
            "        3.9935e-06, 1.3113e-06, 1.4901e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.5034e-06, 8.9407e-07, 1.1325e-06, 8.9407e-07, 1.3113e-06, 1.4305e-06,\n",
            "        8.9407e-07, 1.0133e-06, 9.5367e-07, 3.0994e-06, 1.4901e-06, 8.9407e-07,\n",
            "        3.0398e-06, 1.4305e-06, 1.4305e-06, 9.5367e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.1325e-06, 8.3447e-07, 7.1526e-07, 1.0133e-06, 7.1526e-07, 1.3709e-06,\n",
            "        8.9407e-07, 1.0133e-06, 1.0133e-06, 1.0729e-06, 1.5497e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.0729e-06, 9.5367e-07, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.1325e-06, 8.3447e-07, 7.1526e-07, 1.0729e-06, 7.1526e-07, 1.0729e-06,\n",
            "        9.5367e-07, 9.5367e-07, 1.0133e-06, 1.1325e-06, 1.2517e-06, 1.0729e-06,\n",
            "        7.7486e-07, 7.7486e-07, 9.5367e-07, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.1921e-06, 8.3447e-07, 7.1526e-07, 1.1325e-06, 7.1526e-07, 1.0729e-06,\n",
            "        9.5367e-07, 8.3447e-07, 1.0133e-06, 1.1325e-06, 1.0133e-06, 1.1325e-06,\n",
            "        7.7486e-07, 7.7486e-07, 8.9407e-07, 1.1325e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.0729e-06, 8.3447e-07, 6.5565e-07, 1.0729e-06, 7.1526e-07, 9.5367e-07,\n",
            "        8.3447e-07, 8.3447e-07, 9.5367e-07, 1.0133e-06, 8.3447e-07, 1.0729e-06,\n",
            "        6.5565e-07, 7.7486e-07, 7.1526e-07, 9.5367e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.1325e-06, 8.3447e-07, 6.5565e-07, 8.3447e-07, 5.9605e-07, 9.5367e-07,\n",
            "        8.3447e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 1.0729e-06,\n",
            "        6.5565e-07, 7.1526e-07, 7.1526e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.0729e-06, 8.3447e-07, 6.5565e-07, 8.3447e-07, 5.9605e-07, 9.5367e-07,\n",
            "        7.7486e-07, 8.3447e-07, 8.3447e-07, 8.9407e-07, 7.7486e-07, 1.0729e-06,\n",
            "        6.5565e-07, 7.1526e-07, 7.1526e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([8.3447e-07, 7.1526e-07, 7.1526e-07, 8.3447e-07, 5.9605e-07, 9.5367e-07,\n",
            "        7.7486e-07, 9.5367e-07, 8.3447e-07, 8.3447e-07, 7.7486e-07, 9.5367e-07,\n",
            "        6.5565e-07, 7.1526e-07, 7.1526e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([8.3447e-07, 7.1526e-07, 7.1526e-07, 8.3447e-07, 5.9605e-07, 9.5367e-07,\n",
            "        7.7486e-07, 9.5367e-07, 8.3447e-07, 8.3447e-07, 7.7486e-07, 8.3447e-07,\n",
            "        6.5565e-07, 7.1526e-07, 7.1526e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([8.3447e-07, 7.1526e-07, 6.5565e-07, 9.5367e-07, 5.9605e-07, 9.5367e-07,\n",
            "        7.7486e-07, 9.5367e-07, 8.3447e-07, 8.3447e-07, 7.7486e-07, 8.9407e-07,\n",
            "        6.5565e-07, 7.1526e-07, 5.9605e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([8.3447e-07, 7.1526e-07, 6.5565e-07, 9.5367e-07, 5.9605e-07, 9.5367e-07,\n",
            "        7.7486e-07, 9.5367e-07, 8.3447e-07, 8.3447e-07, 7.7486e-07, 8.9407e-07,\n",
            "        6.5565e-07, 6.5565e-07, 5.9605e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([8.3447e-07, 7.1526e-07, 6.5565e-07, 9.5367e-07, 5.9605e-07, 8.9407e-07,\n",
            "        7.7486e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 8.9407e-07,\n",
            "        5.9605e-07, 6.5565e-07, 5.9605e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([8.3447e-07, 7.1526e-07, 6.5565e-07, 8.3447e-07, 5.9605e-07, 8.9407e-07,\n",
            "        7.1526e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 7.7486e-07, 8.9407e-07,\n",
            "        5.9605e-07, 6.5565e-07, 5.9605e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([8.3447e-07, 7.1526e-07, 6.5565e-07, 8.3447e-07, 5.9605e-07, 8.9407e-07,\n",
            "        7.1526e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 8.9407e-07,\n",
            "        5.9605e-07, 6.5565e-07, 5.9605e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([8.3447e-07, 7.1526e-07, 6.5565e-07, 8.3447e-07, 5.9605e-07, 8.9407e-07,\n",
            "        7.1526e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07,\n",
            "        5.9605e-07, 5.9605e-07, 5.9605e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([8.3447e-07, 7.1526e-07, 6.5565e-07, 8.3447e-07, 5.9605e-07, 8.9407e-07,\n",
            "        7.1526e-07, 7.7486e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 8.9407e-07,\n",
            "        5.9605e-07, 5.9605e-07, 5.9605e-07, 9.5367e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([8.3447e-07, 7.1526e-07, 7.1526e-07, 8.3447e-07, 5.9605e-07, 8.9407e-07,\n",
            "        5.9605e-07, 7.7486e-07, 8.3447e-07, 8.3447e-07, 8.3447e-07, 8.9407e-07,\n",
            "        5.9605e-07, 5.9605e-07, 5.9605e-07, 9.5367e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.0266e-06, 3.1590e-06, 5.9605e-06, 1.3113e-06, 1.7285e-06, 2.5034e-06,\n",
            "        1.4901e-06, 4.1127e-06, 1.3709e-06, 3.4571e-06, 2.3842e-06, 1.9073e-06,\n",
            "        1.6093e-06, 1.7285e-06, 2.1458e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.8610e-06, 4.5300e-06, 3.8147e-06, 8.9407e-07, 1.2517e-06, 1.5497e-06,\n",
            "        1.4305e-06, 3.8147e-06, 1.6689e-06, 3.0398e-06, 3.5167e-06, 2.0266e-06,\n",
            "        1.3113e-06, 1.9073e-06, 2.3842e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.0266e-06, 3.1590e-06, 2.8014e-06, 1.0133e-06, 1.5497e-06, 1.5497e-06,\n",
            "        1.8477e-06, 2.0862e-06, 9.5367e-07, 3.5167e-06, 1.4305e-06, 1.8477e-06,\n",
            "        1.3113e-06, 1.7285e-06, 1.4901e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.4438e-06, 2.5630e-06, 2.5630e-06, 1.1325e-06, 9.5367e-07, 1.8477e-06,\n",
            "        1.2517e-06, 1.0729e-06, 1.0133e-06, 2.6226e-06, 1.7285e-06, 1.6093e-06,\n",
            "        9.5367e-07, 1.9670e-06, 1.7881e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.5034e-06, 2.6226e-06, 1.3113e-06, 1.0133e-06, 1.0729e-06, 1.1325e-06,\n",
            "        1.2517e-06, 1.1325e-06, 1.0729e-06, 2.6822e-06, 1.2517e-06, 1.4305e-06,\n",
            "        1.0729e-06, 1.5497e-06, 1.6689e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.6226e-06, 2.3246e-06, 1.4901e-06, 1.1325e-06, 1.3113e-06, 1.1325e-06,\n",
            "        1.2517e-06, 1.1325e-06, 1.0729e-06, 2.2054e-06, 1.1325e-06, 1.3113e-06,\n",
            "        1.0729e-06, 1.6093e-06, 1.6689e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.6689e-06, 2.3246e-06, 1.7285e-06, 1.2517e-06, 1.0729e-06, 1.1325e-06,\n",
            "        1.2517e-06, 1.1325e-06, 1.0729e-06, 1.8477e-06, 1.1921e-06, 1.3113e-06,\n",
            "        1.0729e-06, 1.7285e-06, 1.7285e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.5497e-06, 1.8477e-06, 1.7285e-06, 1.3709e-06, 1.0729e-06, 1.1325e-06,\n",
            "        1.2517e-06, 1.0729e-06, 1.0133e-06, 1.7285e-06, 1.1921e-06, 1.3113e-06,\n",
            "        1.0729e-06, 1.7285e-06, 1.8477e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.4901e-06, 1.6689e-06, 1.7285e-06, 1.4901e-06, 1.0729e-06, 1.1325e-06,\n",
            "        1.3709e-06, 9.5367e-07, 1.4901e-06, 1.7285e-06, 1.2517e-06, 1.2517e-06,\n",
            "        1.0729e-06, 1.3113e-06, 1.4901e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.4901e-06, 1.4901e-06, 1.7285e-06, 1.4901e-06, 1.0133e-06, 9.5367e-07,\n",
            "        1.4305e-06, 9.5367e-07, 1.4901e-06, 1.7285e-06, 1.1325e-06, 1.2517e-06,\n",
            "        1.0133e-06, 1.3113e-06, 1.4305e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.3113e-06, 1.4901e-06, 1.7285e-06, 1.4901e-06, 1.4305e-06, 9.5367e-07,\n",
            "        1.3113e-06, 9.5367e-07, 1.0133e-06, 1.7285e-06, 1.1921e-06, 1.3709e-06,\n",
            "        1.2517e-06, 1.3709e-06, 1.4305e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.3113e-06, 1.4901e-06, 1.4901e-06, 1.3709e-06, 1.0133e-06, 9.5367e-07,\n",
            "        1.3113e-06, 9.5367e-07, 1.0133e-06, 1.4901e-06, 1.2517e-06, 1.3709e-06,\n",
            "        1.0133e-06, 1.3709e-06, 1.4305e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.3113e-06, 1.3113e-06, 1.4901e-06, 1.3709e-06, 9.5367e-07, 9.5367e-07,\n",
            "        1.3709e-06, 9.5367e-07, 1.0133e-06, 1.4901e-06, 1.2517e-06, 1.3113e-06,\n",
            "        1.3709e-06, 1.3709e-06, 1.4305e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.3113e-06, 1.3113e-06, 1.4901e-06, 1.2517e-06, 9.5367e-07, 9.5367e-07,\n",
            "        1.3113e-06, 9.5367e-07, 1.0133e-06, 1.4901e-06, 1.2517e-06, 1.3113e-06,\n",
            "        1.0729e-06, 1.3709e-06, 1.4305e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.3113e-06, 1.3113e-06, 1.4901e-06, 1.2517e-06, 1.2517e-06, 9.5367e-07,\n",
            "        1.4901e-06, 9.5367e-07, 1.0133e-06, 1.4901e-06, 1.2517e-06, 1.3113e-06,\n",
            "        1.0729e-06, 1.3709e-06, 1.4305e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.3113e-06, 1.3113e-06, 1.3113e-06, 1.2517e-06, 1.0133e-06, 9.5367e-07,\n",
            "        1.3113e-06, 8.9407e-07, 1.0133e-06, 1.4901e-06, 1.2517e-06, 1.3113e-06,\n",
            "        1.0133e-06, 1.3709e-06, 1.4305e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.3113e-06, 1.3113e-06, 1.3113e-06, 1.2517e-06, 9.5367e-07, 8.9407e-07,\n",
            "        1.3113e-06, 8.9407e-07, 1.0133e-06, 1.4901e-06, 1.3113e-06, 1.3113e-06,\n",
            "        1.0133e-06, 1.3113e-06, 1.4305e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.4901e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06, 9.5367e-07, 8.9407e-07,\n",
            "        1.3113e-06, 8.9407e-07, 1.0133e-06, 1.4901e-06, 1.4305e-06, 1.3113e-06,\n",
            "        1.0729e-06, 1.3113e-06, 1.4305e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.4901e-06, 1.3113e-06, 1.3113e-06, 1.5497e-06, 9.5367e-07, 8.9407e-07,\n",
            "        1.3113e-06, 8.9407e-07, 1.0133e-06, 1.4901e-06, 1.4305e-06, 1.3113e-06,\n",
            "        1.0729e-06, 1.3709e-06, 1.5497e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.3113e-06, 1.3113e-06, 1.3113e-06, 1.5497e-06, 1.0133e-06, 8.9407e-07,\n",
            "        1.4901e-06, 8.9407e-07, 1.0133e-06, 1.4305e-06, 1.4901e-06, 1.4901e-06,\n",
            "        1.4901e-06, 1.3709e-06, 1.5497e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.2517e-06, 2.0862e-06, 2.0862e-06, 2.9206e-06, 1.4305e-06, 2.1458e-06,\n",
            "        3.9935e-06, 1.6093e-06, 3.3975e-06, 4.2915e-06, 1.9670e-06, 1.6093e-06,\n",
            "        6.8545e-06, 2.0266e-06, 2.4438e-06, 2.9206e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.6093e-06, 1.7285e-06, 2.1458e-06, 1.7881e-06, 1.5497e-06, 1.3709e-06,\n",
            "        5.2452e-06, 1.4305e-06, 4.7088e-06, 2.0266e-06, 1.7285e-06, 2.0862e-06,\n",
            "        3.2783e-06, 2.1458e-06, 1.4305e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.6689e-06, 1.6689e-06, 1.1921e-06, 1.2517e-06, 1.9073e-06, 1.7881e-06,\n",
            "        2.3246e-06, 1.4305e-06, 3.9339e-06, 2.7418e-06, 1.7881e-06, 1.7881e-06,\n",
            "        3.3975e-06, 2.5630e-06, 1.4305e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.7881e-06, 1.4305e-06, 1.3709e-06, 1.5497e-06, 1.6689e-06, 1.7881e-06,\n",
            "        1.6093e-06, 1.4901e-06, 2.9206e-06, 2.0862e-06, 1.6093e-06, 1.7285e-06,\n",
            "        2.2054e-06, 2.2650e-06, 1.0133e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.3113e-06, 1.4901e-06, 1.4901e-06, 1.7881e-06, 1.4305e-06, 1.6093e-06,\n",
            "        1.2517e-06, 1.6093e-06, 2.4438e-06, 1.9670e-06, 1.6093e-06, 1.2517e-06,\n",
            "        1.3709e-06, 1.4305e-06, 1.0729e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.0133e-06, 1.4901e-06, 1.1325e-06, 1.9073e-06, 1.4305e-06, 1.5497e-06,\n",
            "        1.2517e-06, 1.6689e-06, 1.6689e-06, 1.4901e-06, 1.3709e-06, 1.3113e-06,\n",
            "        1.4305e-06, 1.4305e-06, 1.0729e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.0133e-06, 1.3709e-06, 1.1325e-06, 1.5497e-06, 1.4305e-06, 1.6689e-06,\n",
            "        1.2517e-06, 1.6689e-06, 1.7285e-06, 1.2517e-06, 1.4305e-06, 1.3113e-06,\n",
            "        1.6093e-06, 1.3113e-06, 1.0729e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.0133e-06, 1.4305e-06, 1.0133e-06, 1.6093e-06, 1.4305e-06, 1.6689e-06,\n",
            "        1.1921e-06, 1.5497e-06, 1.6689e-06, 1.2517e-06, 1.4901e-06, 1.2517e-06,\n",
            "        1.7881e-06, 1.4901e-06, 1.1325e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.0133e-06, 1.4305e-06, 1.0133e-06, 1.6689e-06, 1.3113e-06, 1.6093e-06,\n",
            "        1.0133e-06, 1.5497e-06, 1.7285e-06, 1.1921e-06, 1.4305e-06, 1.3113e-06,\n",
            "        1.7285e-06, 1.3709e-06, 1.1325e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.0133e-06, 1.4901e-06, 9.5367e-07, 1.5497e-06, 1.3113e-06, 1.3709e-06,\n",
            "        1.0133e-06, 1.4305e-06, 1.7285e-06, 1.1921e-06, 1.3113e-06, 1.3709e-06,\n",
            "        1.7285e-06, 1.2517e-06, 1.1325e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.0133e-06, 1.6093e-06, 9.5367e-07, 1.5497e-06, 1.3113e-06, 1.4305e-06,\n",
            "        1.0133e-06, 1.4305e-06, 1.7285e-06, 1.1921e-06, 1.2517e-06, 1.3113e-06,\n",
            "        1.7285e-06, 1.2517e-06, 1.1325e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.0133e-06, 1.6093e-06, 9.5367e-07, 1.6093e-06, 1.3709e-06, 1.4305e-06,\n",
            "        9.5367e-07, 1.6689e-06, 1.4901e-06, 1.1921e-06, 1.3113e-06, 1.3709e-06,\n",
            "        1.7285e-06, 1.2517e-06, 1.1325e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.0133e-06, 1.6093e-06, 9.5367e-07, 1.7285e-06, 1.4305e-06, 1.3709e-06,\n",
            "        9.5367e-07, 1.6689e-06, 1.4901e-06, 1.1921e-06, 1.3709e-06, 1.5497e-06,\n",
            "        1.3113e-06, 1.2517e-06, 1.1325e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.0133e-06, 1.5497e-06, 9.5367e-07, 1.7285e-06, 1.4305e-06, 1.3709e-06,\n",
            "        9.5367e-07, 1.4305e-06, 1.4901e-06, 1.0729e-06, 1.3113e-06, 1.6093e-06,\n",
            "        1.3113e-06, 1.3113e-06, 1.1325e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([9.5367e-07, 1.5497e-06, 9.5367e-07, 1.7285e-06, 1.4901e-06, 1.3709e-06,\n",
            "        9.5367e-07, 1.4305e-06, 1.4901e-06, 9.5367e-07, 1.3113e-06, 1.5497e-06,\n",
            "        1.3113e-06, 1.4305e-06, 1.1325e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([9.5367e-07, 1.3113e-06, 9.5367e-07, 1.3113e-06, 1.5497e-06, 1.3709e-06,\n",
            "        9.5367e-07, 1.4305e-06, 1.3113e-06, 9.5367e-07, 1.3113e-06, 1.4901e-06,\n",
            "        1.3113e-06, 1.4305e-06, 9.5367e-07, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([9.5367e-07, 1.3113e-06, 9.5367e-07, 1.3113e-06, 1.4305e-06, 1.3113e-06,\n",
            "        9.5367e-07, 1.4305e-06, 1.3113e-06, 9.5367e-07, 1.4901e-06, 1.5497e-06,\n",
            "        1.4901e-06, 1.4305e-06, 9.5367e-07, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([9.5367e-07, 1.3113e-06, 9.5367e-07, 1.4901e-06, 1.3709e-06, 1.3113e-06,\n",
            "        9.5367e-07, 1.4305e-06, 1.3113e-06, 9.5367e-07, 1.4901e-06, 1.6689e-06,\n",
            "        1.4901e-06, 1.4901e-06, 9.5367e-07, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([9.5367e-07, 1.3113e-06, 9.5367e-07, 1.4901e-06, 1.3709e-06, 1.3113e-06,\n",
            "        9.5367e-07, 1.4305e-06, 1.3113e-06, 9.5367e-07, 1.3113e-06, 1.6689e-06,\n",
            "        1.4901e-06, 1.5497e-06, 9.5367e-07, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([9.5367e-07, 1.3113e-06, 9.5367e-07, 1.6689e-06, 1.3709e-06, 1.3113e-06,\n",
            "        9.5367e-07, 1.3113e-06, 1.3113e-06, 9.5367e-07, 1.3113e-06, 1.7285e-06,\n",
            "        1.4901e-06, 1.6689e-06, 9.5367e-07, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.2054e-06, 2.3246e-06, 2.0862e-06, 2.9206e-06, 3.5763e-06, 2.8014e-06,\n",
            "        3.2187e-06, 1.9073e-06, 3.0994e-06, 3.8147e-06, 3.6955e-06, 1.9670e-06,\n",
            "        2.9802e-06, 3.1590e-06, 3.8743e-06, 5.0068e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.8477e-06, 2.6226e-06, 1.9670e-06, 3.0994e-06, 2.9802e-06, 1.7285e-06,\n",
            "        2.5034e-06, 1.9670e-06, 2.0862e-06, 4.7088e-06, 3.3379e-06, 1.6093e-06,\n",
            "        3.2783e-06, 4.2319e-06, 2.6822e-06, 2.8014e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.0266e-06, 1.6689e-06, 2.3246e-06, 3.7551e-06, 3.2783e-06, 2.0266e-06,\n",
            "        2.3246e-06, 2.0266e-06, 2.3246e-06, 2.2650e-06, 2.2650e-06, 1.6093e-06,\n",
            "        2.6822e-06, 2.6822e-06, 2.9802e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.2650e-06, 1.7285e-06, 2.0266e-06, 2.6226e-06, 2.8610e-06, 1.8477e-06,\n",
            "        2.2650e-06, 1.8477e-06, 1.9670e-06, 2.2650e-06, 2.2054e-06, 1.9073e-06,\n",
            "        2.6226e-06, 1.8477e-06, 2.3842e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.2650e-06, 1.8477e-06, 2.0266e-06, 2.3842e-06, 1.9670e-06, 1.6093e-06,\n",
            "        2.0862e-06, 1.8477e-06, 2.0266e-06, 2.2054e-06, 2.3246e-06, 2.0266e-06,\n",
            "        2.0266e-06, 1.4901e-06, 1.4901e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.1458e-06, 1.9670e-06, 2.2054e-06, 1.9670e-06, 1.9670e-06, 1.6093e-06,\n",
            "        2.0862e-06, 1.8477e-06, 2.0266e-06, 2.2650e-06, 2.3246e-06, 2.0266e-06,\n",
            "        2.0266e-06, 1.4901e-06, 1.4901e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.1458e-06, 1.9670e-06, 2.2650e-06, 2.0266e-06, 1.9670e-06, 1.6689e-06,\n",
            "        2.2650e-06, 1.8477e-06, 2.0266e-06, 2.3246e-06, 2.0266e-06, 1.9073e-06,\n",
            "        2.0862e-06, 1.4901e-06, 1.4901e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.8477e-06, 1.9670e-06, 1.4901e-06, 1.9670e-06, 1.9670e-06, 1.6689e-06,\n",
            "        1.9670e-06, 1.9670e-06, 2.0862e-06, 2.3246e-06, 2.0266e-06, 2.0266e-06,\n",
            "        2.0266e-06, 1.4901e-06, 1.4901e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.7881e-06, 1.8477e-06, 1.4305e-06, 1.9670e-06, 1.8477e-06, 1.8477e-06,\n",
            "        1.9670e-06, 2.0862e-06, 1.7881e-06, 2.0862e-06, 1.8477e-06, 1.5497e-06,\n",
            "        2.0266e-06, 1.4305e-06, 1.4901e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.7881e-06, 2.0862e-06, 1.4305e-06, 1.9670e-06, 1.8477e-06, 1.6689e-06,\n",
            "        1.9670e-06, 1.9670e-06, 1.9670e-06, 2.0862e-06, 1.8477e-06, 1.5497e-06,\n",
            "        2.0266e-06, 1.5497e-06, 1.4901e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.7881e-06, 2.0862e-06, 1.6093e-06, 1.9670e-06, 1.8477e-06, 1.7285e-06,\n",
            "        2.2650e-06, 1.9670e-06, 1.9670e-06, 2.0862e-06, 2.0862e-06, 1.4305e-06,\n",
            "        2.0266e-06, 1.6689e-06, 1.4901e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.7881e-06, 2.0862e-06, 1.6093e-06, 1.9670e-06, 1.8477e-06, 1.8477e-06,\n",
            "        2.2650e-06, 1.9670e-06, 1.9670e-06, 2.0862e-06, 2.0862e-06, 1.3709e-06,\n",
            "        1.7881e-06, 1.6093e-06, 1.4901e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.0266e-06, 1.9670e-06, 1.5497e-06, 1.9670e-06, 2.0266e-06, 1.9670e-06,\n",
            "        2.0862e-06, 1.9670e-06, 1.9670e-06, 2.0862e-06, 2.0862e-06, 1.3709e-06,\n",
            "        1.8477e-06, 1.6689e-06, 1.6689e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.0266e-06, 1.9670e-06, 1.3709e-06, 1.9670e-06, 2.2650e-06, 2.0862e-06,\n",
            "        2.0862e-06, 1.9670e-06, 1.7881e-06, 1.9073e-06, 2.0862e-06, 1.3709e-06,\n",
            "        1.7881e-06, 1.7881e-06, 1.6093e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.0266e-06, 1.9670e-06, 1.3709e-06, 2.2650e-06, 2.2650e-06, 2.0862e-06,\n",
            "        2.0862e-06, 1.7881e-06, 1.7881e-06, 1.9073e-06, 2.0862e-06, 1.3113e-06,\n",
            "        1.7881e-06, 1.9073e-06, 1.6689e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.0266e-06, 1.9073e-06, 1.3709e-06, 2.2650e-06, 2.0862e-06, 2.0862e-06,\n",
            "        2.0862e-06, 2.1458e-06, 1.7881e-06, 1.9073e-06, 2.0862e-06, 1.4305e-06,\n",
            "        1.8477e-06, 2.0266e-06, 1.6689e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.0266e-06, 1.7881e-06, 1.4901e-06, 1.9670e-06, 2.0862e-06, 2.0862e-06,\n",
            "        1.8477e-06, 1.8477e-06, 1.7881e-06, 1.9073e-06, 1.8477e-06, 1.4305e-06,\n",
            "        1.9073e-06, 2.0862e-06, 1.7285e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.7881e-06, 1.7881e-06, 1.4901e-06, 2.0266e-06, 1.7881e-06, 2.0862e-06,\n",
            "        1.7881e-06, 1.7881e-06, 2.0266e-06, 1.9073e-06, 1.8477e-06, 1.3709e-06,\n",
            "        1.9073e-06, 2.0862e-06, 1.8477e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.7881e-06, 1.7881e-06, 1.4305e-06, 2.0266e-06, 1.9073e-06, 2.0862e-06,\n",
            "        1.9073e-06, 1.7881e-06, 2.0266e-06, 1.9073e-06, 1.8477e-06, 1.3709e-06,\n",
            "        1.8477e-06, 2.0862e-06, 1.9670e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.7881e-06, 1.7881e-06, 1.4305e-06, 2.0862e-06, 1.8477e-06, 2.0862e-06,\n",
            "        2.0266e-06, 1.7881e-06, 2.0266e-06, 1.9073e-06, 1.9670e-06, 1.4901e-06,\n",
            "        1.8477e-06, 2.0862e-06, 2.0862e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.1590e-06, 2.8014e-06, 3.9339e-06, 2.8014e-06, 3.0398e-06, 2.4438e-06,\n",
            "        3.6359e-06, 2.9802e-06, 4.7684e-06, 2.9206e-06, 3.8147e-06, 4.2319e-06,\n",
            "        2.0862e-06, 2.8610e-06, 3.5167e-06, 3.0994e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.1590e-06, 2.2650e-06, 3.0398e-06, 2.3246e-06, 2.6226e-06, 2.5630e-06,\n",
            "        3.3975e-06, 2.2650e-06, 3.7551e-06, 2.3842e-06, 2.8610e-06, 4.9472e-06,\n",
            "        2.1458e-06, 2.8610e-06, 2.6822e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.7418e-06, 2.2650e-06, 3.2187e-06, 2.2650e-06, 2.9206e-06, 1.9670e-06,\n",
            "        2.8014e-06, 2.0266e-06, 4.8876e-06, 2.3842e-06, 2.9206e-06, 5.9009e-06,\n",
            "        2.2650e-06, 2.8014e-06, 2.6226e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.7418e-06, 2.4438e-06, 3.0398e-06, 2.0266e-06, 2.8610e-06, 2.0862e-06,\n",
            "        2.6226e-06, 2.5034e-06, 2.8014e-06, 2.8610e-06, 3.0398e-06, 2.7418e-06,\n",
            "        2.4438e-06, 2.4438e-06, 2.3246e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.7418e-06, 2.3246e-06, 3.2783e-06, 2.2650e-06, 1.9670e-06, 1.8477e-06,\n",
            "        2.6226e-06, 2.6226e-06, 2.7418e-06, 2.0862e-06, 3.2783e-06, 2.6822e-06,\n",
            "        1.8477e-06, 2.3842e-06, 2.3246e-06, 2.2650e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.9073e-06, 2.3246e-06, 3.4571e-06, 2.3246e-06, 2.0266e-06, 1.7881e-06,\n",
            "        2.5034e-06, 2.3842e-06, 2.5034e-06, 2.2054e-06, 2.9206e-06, 2.5034e-06,\n",
            "        1.7881e-06, 2.1458e-06, 2.3842e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.9073e-06, 2.0862e-06, 3.1590e-06, 2.2054e-06, 2.0266e-06, 1.7881e-06,\n",
            "        2.5034e-06, 2.3842e-06, 2.5034e-06, 2.2054e-06, 3.0398e-06, 2.5034e-06,\n",
            "        1.7881e-06, 1.8477e-06, 2.3842e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.9073e-06, 2.0862e-06, 2.7418e-06, 2.2054e-06, 1.9670e-06, 1.7881e-06,\n",
            "        2.3842e-06, 2.2650e-06, 2.5034e-06, 2.2054e-06, 3.0994e-06, 2.4438e-06,\n",
            "        1.7881e-06, 1.8477e-06, 2.2650e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.9073e-06, 2.0862e-06, 2.5034e-06, 2.2054e-06, 1.9670e-06, 1.7881e-06,\n",
            "        2.3842e-06, 2.3246e-06, 2.5034e-06, 2.2650e-06, 2.6822e-06, 2.5034e-06,\n",
            "        1.7881e-06, 1.8477e-06, 2.2650e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.9073e-06, 2.0862e-06, 2.5034e-06, 2.3246e-06, 1.9073e-06, 1.7881e-06,\n",
            "        2.3246e-06, 2.3246e-06, 2.5034e-06, 2.4438e-06, 2.6822e-06, 2.3246e-06,\n",
            "        1.8477e-06, 1.8477e-06, 2.2650e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.9073e-06, 2.2650e-06, 2.4438e-06, 2.2650e-06, 1.9073e-06, 1.7881e-06,\n",
            "        2.3246e-06, 2.3246e-06, 2.5034e-06, 2.4438e-06, 2.6226e-06, 2.3246e-06,\n",
            "        1.8477e-06, 1.8477e-06, 2.3246e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.9073e-06, 2.2054e-06, 2.2054e-06, 2.2054e-06, 1.8477e-06, 1.7285e-06,\n",
            "        2.3842e-06, 2.3842e-06, 2.5630e-06, 2.4438e-06, 2.3842e-06, 2.3246e-06,\n",
            "        1.9073e-06, 1.8477e-06, 2.2054e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.9073e-06, 2.2054e-06, 2.2054e-06, 2.5034e-06, 1.8477e-06, 1.7285e-06,\n",
            "        2.3842e-06, 2.3842e-06, 2.5630e-06, 2.4438e-06, 2.3842e-06, 2.3246e-06,\n",
            "        1.9073e-06, 1.9073e-06, 2.2054e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.9073e-06, 2.2054e-06, 2.2054e-06, 2.2054e-06, 1.8477e-06, 1.7285e-06,\n",
            "        2.2054e-06, 2.3842e-06, 2.3842e-06, 2.4438e-06, 2.3246e-06, 2.2054e-06,\n",
            "        1.7285e-06, 1.9073e-06, 2.2054e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.9073e-06, 2.2054e-06, 2.3842e-06, 2.2054e-06, 1.8477e-06, 1.7285e-06,\n",
            "        2.2650e-06, 2.4438e-06, 2.5630e-06, 2.2054e-06, 2.3246e-06, 2.0862e-06,\n",
            "        1.7285e-06, 1.9073e-06, 2.2054e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.8477e-06, 2.0862e-06, 2.3842e-06, 2.2054e-06, 1.8477e-06, 1.9073e-06,\n",
            "        2.2650e-06, 2.4438e-06, 2.0862e-06, 2.2054e-06, 2.3246e-06, 2.0862e-06,\n",
            "        1.7285e-06, 1.7285e-06, 2.0862e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.9670e-06, 2.0862e-06, 2.2054e-06, 2.0862e-06, 1.8477e-06, 1.9073e-06,\n",
            "        2.2650e-06, 2.3246e-06, 2.0862e-06, 2.2054e-06, 2.3246e-06, 2.0862e-06,\n",
            "        1.7285e-06, 1.7285e-06, 2.0862e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.8477e-06, 2.0862e-06, 2.2054e-06, 2.0862e-06, 1.8477e-06, 1.9073e-06,\n",
            "        2.3246e-06, 2.3246e-06, 2.3246e-06, 2.2650e-06, 2.4438e-06, 2.0862e-06,\n",
            "        1.7285e-06, 1.7285e-06, 2.0862e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.8477e-06, 2.2054e-06, 2.4438e-06, 2.0862e-06, 1.8477e-06, 1.9073e-06,\n",
            "        2.3246e-06, 2.3246e-06, 2.3246e-06, 2.2650e-06, 2.3246e-06, 2.0862e-06,\n",
            "        1.7285e-06, 1.7285e-06, 2.0862e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.8477e-06, 2.2054e-06, 2.4438e-06, 2.0862e-06, 1.8477e-06, 1.7285e-06,\n",
            "        2.3246e-06, 2.3246e-06, 2.0862e-06, 2.2650e-06, 2.2650e-06, 2.0862e-06,\n",
            "        1.7285e-06, 1.7285e-06, 2.3246e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.7088e-06, 4.5896e-06, 3.7551e-06, 4.3511e-06, 4.2319e-06, 5.1260e-06,\n",
            "        3.2187e-06, 4.3511e-06, 6.0797e-06, 4.7088e-06, 3.9339e-06, 3.6359e-06,\n",
            "        4.8876e-06, 4.4107e-06, 4.7088e-06, 5.3048e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([4.7088e-06, 3.6955e-06, 3.8743e-06, 4.5896e-06, 5.0068e-06, 3.8743e-06,\n",
            "        3.5167e-06, 3.5763e-06, 4.8876e-06, 3.8743e-06, 4.7088e-06, 4.1127e-06,\n",
            "        4.4107e-06, 4.5300e-06, 5.0068e-06, 3.6359e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.6955e-06, 3.7551e-06, 3.4571e-06, 4.3511e-06, 4.4107e-06, 4.2319e-06,\n",
            "        3.3379e-06, 3.4571e-06, 4.7088e-06, 4.3511e-06, 4.3511e-06, 4.1723e-06,\n",
            "        3.6955e-06, 3.2187e-06, 4.1127e-06, 3.4571e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.0531e-06, 3.6955e-06, 3.7551e-06, 4.0531e-06, 3.9935e-06, 3.2783e-06,\n",
            "        3.1590e-06, 3.2187e-06, 3.9935e-06, 3.3379e-06, 3.2187e-06, 4.1127e-06,\n",
            "        2.5034e-06, 3.3975e-06, 3.9935e-06, 4.3511e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.2783e-06, 3.5167e-06, 3.9339e-06, 3.7551e-06, 3.1590e-06, 3.1590e-06,\n",
            "        3.2187e-06, 3.3379e-06, 3.1590e-06, 3.4571e-06, 3.3379e-06, 3.3379e-06,\n",
            "        2.5034e-06, 3.3975e-06, 3.3379e-06, 3.6955e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.2187e-06, 3.6359e-06, 4.2319e-06, 3.9935e-06, 3.0398e-06, 3.0398e-06,\n",
            "        3.2783e-06, 3.3975e-06, 3.2783e-06, 3.2187e-06, 3.3379e-06, 3.2783e-06,\n",
            "        2.5034e-06, 3.0994e-06, 3.3379e-06, 3.1590e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.8014e-06, 3.6955e-06, 2.6822e-06, 4.1127e-06, 3.0398e-06, 3.0398e-06,\n",
            "        3.3975e-06, 2.5034e-06, 3.0398e-06, 3.2187e-06, 3.2187e-06, 3.2783e-06,\n",
            "        2.5034e-06, 3.0994e-06, 3.1590e-06, 3.1590e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.8014e-06, 2.9802e-06, 2.6822e-06, 2.5630e-06, 3.1590e-06, 2.9206e-06,\n",
            "        2.5630e-06, 2.5034e-06, 2.9802e-06, 2.9802e-06, 3.0994e-06, 2.8610e-06,\n",
            "        2.2054e-06, 2.8610e-06, 3.0994e-06, 3.1590e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.3842e-06, 2.8014e-06, 2.6822e-06, 2.4438e-06, 2.8610e-06, 2.9206e-06,\n",
            "        2.5630e-06, 2.5034e-06, 2.9802e-06, 2.9802e-06, 3.0994e-06, 2.8610e-06,\n",
            "        2.2054e-06, 3.0994e-06, 3.0994e-06, 2.8014e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.4438e-06, 2.8014e-06, 2.5630e-06, 2.4438e-06, 2.8610e-06, 2.9206e-06,\n",
            "        2.4438e-06, 2.4438e-06, 2.6822e-06, 2.9206e-06, 3.0994e-06, 2.8610e-06,\n",
            "        2.2054e-06, 2.8610e-06, 3.0994e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.6226e-06, 2.9206e-06, 2.5630e-06, 2.4438e-06, 2.8610e-06, 2.5630e-06,\n",
            "        2.4438e-06, 2.3246e-06, 2.6226e-06, 2.9206e-06, 2.7418e-06, 2.8610e-06,\n",
            "        2.3246e-06, 2.8610e-06, 3.1590e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.7418e-06, 2.9802e-06, 2.5630e-06, 2.4438e-06, 2.9206e-06, 2.5630e-06,\n",
            "        2.5034e-06, 2.3246e-06, 2.6226e-06, 2.9206e-06, 2.8014e-06, 2.8610e-06,\n",
            "        2.3842e-06, 2.8610e-06, 2.6822e-06, 2.9802e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.7418e-06, 2.2650e-06, 2.5630e-06, 2.4438e-06, 2.9206e-06, 2.5630e-06,\n",
            "        2.5630e-06, 2.2650e-06, 2.6226e-06, 2.9206e-06, 2.8014e-06, 2.8610e-06,\n",
            "        2.5034e-06, 2.7418e-06, 2.6822e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.7418e-06, 2.3246e-06, 2.3842e-06, 2.4438e-06, 2.6226e-06, 2.5034e-06,\n",
            "        2.6226e-06, 2.3246e-06, 2.6226e-06, 2.7418e-06, 2.8014e-06, 2.6226e-06,\n",
            "        2.6226e-06, 2.7418e-06, 2.6822e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.7418e-06, 2.4438e-06, 2.4438e-06, 2.3842e-06, 2.9802e-06, 2.5034e-06,\n",
            "        2.8610e-06, 2.3842e-06, 2.6226e-06, 2.7418e-06, 2.8014e-06, 2.6226e-06,\n",
            "        2.8014e-06, 2.7418e-06, 2.5630e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.7418e-06, 2.6226e-06, 2.4438e-06, 2.3842e-06, 2.6226e-06, 2.6226e-06,\n",
            "        2.9206e-06, 2.4438e-06, 2.6226e-06, 2.7418e-06, 2.8014e-06, 2.6226e-06,\n",
            "        2.9802e-06, 2.7418e-06, 2.6226e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.7418e-06, 2.7418e-06, 2.3842e-06, 2.2054e-06, 2.6226e-06, 2.6226e-06,\n",
            "        2.9206e-06, 2.5630e-06, 2.6226e-06, 2.7418e-06, 2.8014e-06, 2.4438e-06,\n",
            "        2.9802e-06, 2.7418e-06, 2.6226e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.7418e-06, 2.7418e-06, 2.3842e-06, 2.2650e-06, 2.6226e-06, 2.6226e-06,\n",
            "        2.7418e-06, 2.6226e-06, 2.6226e-06, 2.7418e-06, 2.8014e-06, 2.4438e-06,\n",
            "        2.9802e-06, 2.7418e-06, 2.6226e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.7418e-06, 2.7418e-06, 2.4438e-06, 2.3842e-06, 2.6226e-06, 2.6226e-06,\n",
            "        2.5034e-06, 2.7418e-06, 2.6226e-06, 2.7418e-06, 2.8014e-06, 2.4438e-06,\n",
            "        2.9206e-06, 2.7418e-06, 2.6226e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.7418e-06, 2.7418e-06, 2.3246e-06, 2.2650e-06, 2.6226e-06, 2.6226e-06,\n",
            "        2.5034e-06, 2.7418e-06, 2.6226e-06, 2.7418e-06, 2.8014e-06, 2.5630e-06,\n",
            "        2.8610e-06, 2.7418e-06, 2.6226e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.1723e-06, 2.9802e-06, 1.6093e-06, 2.3246e-06, 1.3709e-06, 4.0531e-06,\n",
            "        2.0266e-06, 2.9802e-06, 8.9407e-07, 2.0862e-06, 2.3246e-06, 1.7285e-06,\n",
            "        1.9670e-06, 1.9670e-06, 2.8610e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.5034e-06, 3.2187e-06, 1.7881e-06, 2.2650e-06, 1.6093e-06, 3.8147e-06,\n",
            "        2.8610e-06, 3.9935e-06, 9.5367e-07, 2.5034e-06, 1.6093e-06, 1.7285e-06,\n",
            "        2.2650e-06, 1.1921e-06, 3.6359e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.4438e-06, 2.1458e-06, 1.9670e-06, 1.6689e-06, 1.9670e-06, 2.7418e-06,\n",
            "        2.0862e-06, 2.3246e-06, 1.0133e-06, 2.5034e-06, 2.0862e-06, 1.6689e-06,\n",
            "        2.5034e-06, 1.5497e-06, 2.5034e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.9206e-06, 1.7881e-06, 1.7285e-06, 1.3709e-06, 1.1921e-06, 1.7881e-06,\n",
            "        2.4438e-06, 1.8477e-06, 1.3113e-06, 1.1921e-06, 2.0266e-06, 1.6689e-06,\n",
            "        1.7285e-06, 2.1458e-06, 2.7418e-06, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.9670e-06, 1.4305e-06, 1.7881e-06, 1.2517e-06, 1.3113e-06, 1.4901e-06,\n",
            "        1.6093e-06, 1.9073e-06, 1.3113e-06, 1.1921e-06, 1.5497e-06, 1.7285e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.0729e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.9670e-06, 1.4305e-06, 1.0729e-06, 1.1921e-06, 1.3113e-06, 1.5497e-06,\n",
            "        1.6093e-06, 1.9670e-06, 1.1921e-06, 1.1921e-06, 1.6093e-06, 1.5497e-06,\n",
            "        1.3113e-06, 1.4305e-06, 1.0133e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.7881e-06, 1.4901e-06, 8.9407e-07, 1.1921e-06, 1.3709e-06, 1.5497e-06,\n",
            "        1.6689e-06, 1.9073e-06, 1.1921e-06, 1.1921e-06, 1.6689e-06, 1.6689e-06,\n",
            "        1.3709e-06, 1.4305e-06, 9.5367e-07, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.6093e-06, 1.3709e-06, 8.9407e-07, 1.3113e-06, 1.4305e-06, 1.5497e-06,\n",
            "        1.4901e-06, 1.9073e-06, 1.1921e-06, 8.9407e-07, 1.3113e-06, 1.4901e-06,\n",
            "        1.3709e-06, 1.3709e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.6689e-06, 1.3709e-06, 8.9407e-07, 1.1325e-06, 1.3709e-06, 1.3709e-06,\n",
            "        1.5497e-06, 1.9073e-06, 1.2517e-06, 8.9407e-07, 1.3709e-06, 1.4901e-06,\n",
            "        1.4901e-06, 1.6093e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.6689e-06, 1.3709e-06, 8.9407e-07, 1.3113e-06, 1.3709e-06, 1.3709e-06,\n",
            "        1.4305e-06, 1.7285e-06, 1.2517e-06, 8.9407e-07, 1.6093e-06, 1.3709e-06,\n",
            "        1.4305e-06, 1.6093e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.6093e-06, 1.3709e-06, 8.9407e-07, 1.3709e-06, 1.3709e-06, 1.3709e-06,\n",
            "        1.4901e-06, 1.7285e-06, 1.2517e-06, 8.9407e-07, 1.6093e-06, 1.3709e-06,\n",
            "        1.4901e-06, 1.3709e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.6689e-06, 1.3709e-06, 8.9407e-07, 1.3709e-06, 1.3709e-06, 1.3709e-06,\n",
            "        1.4305e-06, 1.7285e-06, 1.1921e-06, 8.9407e-07, 1.3709e-06, 1.3709e-06,\n",
            "        1.7285e-06, 1.3709e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.6093e-06, 1.3709e-06, 8.9407e-07, 1.4305e-06, 1.3709e-06, 1.3709e-06,\n",
            "        1.4901e-06, 1.7285e-06, 1.1921e-06, 8.9407e-07, 1.3709e-06, 1.3709e-06,\n",
            "        1.7285e-06, 1.3709e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.6093e-06, 1.3709e-06, 9.5367e-07, 1.4305e-06, 1.4305e-06, 1.3709e-06,\n",
            "        1.5497e-06, 1.7285e-06, 1.1921e-06, 8.9407e-07, 1.3709e-06, 1.3709e-06,\n",
            "        1.7285e-06, 1.3709e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.6093e-06, 1.3709e-06, 9.5367e-07, 1.4901e-06, 1.4901e-06, 1.3709e-06,\n",
            "        1.5497e-06, 1.7285e-06, 1.1921e-06, 8.9407e-07, 1.3709e-06, 1.3709e-06,\n",
            "        1.7285e-06, 1.4305e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.6093e-06, 1.3709e-06, 1.0133e-06, 1.5497e-06, 1.4901e-06, 1.3709e-06,\n",
            "        1.6093e-06, 1.7285e-06, 1.1921e-06, 8.9407e-07, 1.4305e-06, 1.3709e-06,\n",
            "        1.7285e-06, 1.4305e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.5497e-06, 1.3709e-06, 1.0133e-06, 1.6093e-06, 1.4901e-06, 1.3709e-06,\n",
            "        1.6093e-06, 1.5497e-06, 1.1921e-06, 8.9407e-07, 1.4305e-06, 1.3709e-06,\n",
            "        1.3709e-06, 1.4305e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.5497e-06, 1.3709e-06, 1.0133e-06, 1.5497e-06, 1.6093e-06, 1.3709e-06,\n",
            "        1.6093e-06, 1.5497e-06, 1.1921e-06, 9.5367e-07, 1.4305e-06, 1.3709e-06,\n",
            "        1.3709e-06, 1.4305e-06, 8.9407e-07, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.5497e-06, 1.3709e-06, 1.0133e-06, 1.6093e-06, 1.6093e-06, 1.3709e-06,\n",
            "        1.6093e-06, 1.6093e-06, 1.1921e-06, 9.5367e-07, 1.4305e-06, 1.3709e-06,\n",
            "        1.3709e-06, 1.4305e-06, 8.9407e-07, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.5497e-06, 1.3709e-06, 1.0133e-06, 1.6093e-06, 1.6093e-06, 1.3709e-06,\n",
            "        1.6093e-06, 1.6093e-06, 1.1921e-06, 9.5367e-07, 1.4305e-06, 1.3709e-06,\n",
            "        1.3709e-06, 1.4305e-06, 8.9407e-07, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.3113e-06, 1.9670e-06, 4.1127e-06, 2.2054e-06, 3.1590e-06, 5.0664e-06,\n",
            "        2.0266e-06, 1.6093e-06, 2.8610e-06, 1.9670e-06, 4.2915e-06, 3.3975e-06,\n",
            "        2.5630e-06, 4.2319e-06, 3.9935e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.4305e-06, 1.7881e-06, 4.1127e-06, 2.1458e-06, 3.9339e-06, 5.6624e-06,\n",
            "        1.9670e-06, 1.4901e-06, 3.9935e-06, 1.6689e-06, 3.6955e-06, 1.7285e-06,\n",
            "        1.9073e-06, 3.9339e-06, 2.4438e-06, 3.8147e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.6093e-06, 1.8477e-06, 4.1127e-06, 2.1458e-06, 1.9073e-06, 4.4107e-06,\n",
            "        1.9670e-06, 1.8477e-06, 2.6822e-06, 1.7285e-06, 2.2650e-06, 1.6689e-06,\n",
            "        1.8477e-06, 2.3246e-06, 3.2783e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.4305e-06, 1.6689e-06, 4.7088e-06, 1.5497e-06, 1.4901e-06, 3.0994e-06,\n",
            "        2.0266e-06, 1.5497e-06, 3.0994e-06, 1.4305e-06, 1.7881e-06, 1.5497e-06,\n",
            "        1.8477e-06, 1.9670e-06, 2.0266e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.5497e-06, 1.6093e-06, 1.2517e-06, 1.5497e-06, 1.5497e-06, 1.9670e-06,\n",
            "        1.4901e-06, 1.6689e-06, 2.3246e-06, 1.6093e-06, 1.6093e-06, 1.3709e-06,\n",
            "        1.9670e-06, 1.8477e-06, 2.0862e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.5497e-06, 1.6689e-06, 1.2517e-06, 1.5497e-06, 1.7285e-06, 2.0266e-06,\n",
            "        1.4901e-06, 1.6689e-06, 2.1458e-06, 1.6093e-06, 1.6093e-06, 1.2517e-06,\n",
            "        1.6689e-06, 1.7881e-06, 2.2650e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.4901e-06, 1.6689e-06, 1.2517e-06, 1.4901e-06, 1.7285e-06, 1.9670e-06,\n",
            "        1.5497e-06, 1.4901e-06, 2.1458e-06, 1.5497e-06, 1.1325e-06, 1.2517e-06,\n",
            "        1.6689e-06, 1.9073e-06, 2.3842e-06, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.4901e-06, 1.6689e-06, 1.1921e-06, 1.4901e-06, 1.6689e-06, 1.7881e-06,\n",
            "        1.4901e-06, 1.4901e-06, 2.1458e-06, 1.6689e-06, 1.1325e-06, 1.2517e-06,\n",
            "        1.7285e-06, 1.6689e-06, 2.0266e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.3709e-06, 1.4901e-06, 1.1921e-06, 1.3709e-06, 1.7881e-06, 1.7881e-06,\n",
            "        1.5497e-06, 1.4901e-06, 1.7881e-06, 1.3709e-06, 1.1325e-06, 1.2517e-06,\n",
            "        1.6689e-06, 1.6093e-06, 1.7881e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.3709e-06, 1.2517e-06, 1.1921e-06, 1.3113e-06, 1.7881e-06, 1.7881e-06,\n",
            "        1.5497e-06, 1.4901e-06, 1.7881e-06, 1.3709e-06, 1.0729e-06, 1.0133e-06,\n",
            "        1.6689e-06, 1.6093e-06, 1.6093e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.3709e-06, 1.2517e-06, 1.1921e-06, 1.3113e-06, 1.7881e-06, 1.7881e-06,\n",
            "        1.5497e-06, 1.3709e-06, 1.7881e-06, 1.3709e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.6689e-06, 1.6093e-06, 1.6093e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.4901e-06, 1.3709e-06, 1.0133e-06, 1.3709e-06, 1.7881e-06, 1.7881e-06,\n",
            "        1.6689e-06, 1.3113e-06, 1.6093e-06, 1.3709e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.5497e-06, 1.6093e-06, 1.6093e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.2517e-06, 1.3709e-06, 1.0133e-06, 1.4305e-06, 1.7881e-06, 1.7881e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.6093e-06, 1.3709e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.4305e-06, 1.6093e-06, 1.6093e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.3113e-06, 1.3709e-06, 1.0133e-06, 1.4305e-06, 1.6093e-06, 1.7881e-06,\n",
            "        1.6689e-06, 1.3709e-06, 1.6093e-06, 1.3113e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.3709e-06, 1.4305e-06, 1.6093e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.3113e-06, 1.3709e-06, 1.0133e-06, 1.4305e-06, 1.6093e-06, 1.6093e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.6093e-06, 1.3113e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.4305e-06, 1.4305e-06, 1.6093e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.3709e-06, 1.3709e-06, 1.0133e-06, 1.4305e-06, 1.6093e-06, 1.6093e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.6093e-06, 1.3113e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.4305e-06, 1.4305e-06, 1.6093e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.3113e-06, 1.3113e-06, 1.0133e-06, 1.6093e-06, 1.6093e-06, 1.7285e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.6093e-06, 1.3113e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.4305e-06, 1.4305e-06, 1.6093e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.3709e-06, 1.3113e-06, 1.0133e-06, 1.6093e-06, 1.6093e-06, 1.7285e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.6093e-06, 1.3113e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.4305e-06, 1.4305e-06, 1.6093e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.3709e-06, 1.3113e-06, 1.0133e-06, 1.4305e-06, 1.7285e-06, 1.7285e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.6093e-06, 1.3113e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.4305e-06, 1.4305e-06, 1.6093e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.4305e-06, 1.3113e-06, 1.0133e-06, 1.4305e-06, 1.7285e-06, 1.7285e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.6093e-06, 1.3113e-06, 1.0133e-06, 1.0133e-06,\n",
            "        1.4305e-06, 1.4305e-06, 1.6093e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.6359e-06, 3.2783e-06, 2.2054e-06, 2.9206e-06, 2.5630e-06, 2.4438e-06,\n",
            "        3.5167e-06, 4.4107e-06, 3.3379e-06, 1.9670e-06, 2.6822e-06, 2.5034e-06,\n",
            "        2.8014e-06, 3.2187e-06, 1.8477e-06, 3.4571e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.9935e-06, 2.9206e-06, 2.0862e-06, 2.8610e-06, 2.0266e-06, 1.8477e-06,\n",
            "        2.9802e-06, 5.3644e-06, 1.7285e-06, 2.0862e-06, 2.0266e-06, 3.2783e-06,\n",
            "        2.9206e-06, 4.1723e-06, 1.9073e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.0531e-06, 2.5630e-06, 2.3842e-06, 1.7881e-06, 2.0862e-06, 2.2054e-06,\n",
            "        2.6822e-06, 5.8413e-06, 2.0266e-06, 2.2650e-06, 2.4438e-06, 2.8610e-06,\n",
            "        2.1458e-06, 1.9670e-06, 1.7881e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.1458e-06, 2.1458e-06, 1.7881e-06, 2.0266e-06, 2.5034e-06, 2.3246e-06,\n",
            "        2.9802e-06, 4.5896e-06, 2.1458e-06, 2.3842e-06, 2.6822e-06, 2.8014e-06,\n",
            "        2.4438e-06, 1.5497e-06, 1.7881e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.2054e-06, 2.3842e-06, 1.7881e-06, 1.8477e-06, 2.2054e-06, 1.6689e-06,\n",
            "        1.8477e-06, 2.8014e-06, 2.2650e-06, 1.8477e-06, 2.0266e-06, 2.8014e-06,\n",
            "        2.6226e-06, 1.3113e-06, 1.6689e-06, 2.2650e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.2650e-06, 2.3842e-06, 1.8477e-06, 1.8477e-06, 1.6689e-06, 1.6689e-06,\n",
            "        1.9670e-06, 2.3842e-06, 2.1458e-06, 1.8477e-06, 2.0266e-06, 2.3842e-06,\n",
            "        1.9073e-06, 1.3113e-06, 1.5497e-06, 2.0266e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.4438e-06, 1.9073e-06, 1.6689e-06, 1.9073e-06, 1.6689e-06, 1.6689e-06,\n",
            "        2.2650e-06, 2.4438e-06, 1.9670e-06, 1.9073e-06, 2.0266e-06, 1.9670e-06,\n",
            "        1.9670e-06, 1.3113e-06, 1.5497e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.9073e-06, 1.9073e-06, 1.6689e-06, 1.9670e-06, 1.6093e-06, 1.6689e-06,\n",
            "        1.9670e-06, 2.3842e-06, 1.9670e-06, 1.6093e-06, 1.9073e-06, 1.9670e-06,\n",
            "        1.8477e-06, 1.3113e-06, 1.5497e-06, 1.9670e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.9670e-06, 1.7881e-06, 1.6093e-06, 1.9670e-06, 1.5497e-06, 1.6689e-06,\n",
            "        1.7881e-06, 2.2054e-06, 1.8477e-06, 1.6093e-06, 1.7881e-06, 1.9073e-06,\n",
            "        1.6689e-06, 1.3113e-06, 1.4305e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.9670e-06, 1.6093e-06, 1.7881e-06, 1.9670e-06, 1.5497e-06, 1.5497e-06,\n",
            "        1.7881e-06, 2.2054e-06, 1.6689e-06, 1.5497e-06, 1.5497e-06, 1.9073e-06,\n",
            "        1.6093e-06, 1.3113e-06, 1.4305e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.9670e-06, 1.6093e-06, 1.7881e-06, 1.7881e-06, 1.5497e-06, 1.5497e-06,\n",
            "        1.7881e-06, 1.7881e-06, 1.6689e-06, 1.6093e-06, 1.5497e-06, 1.7881e-06,\n",
            "        1.6093e-06, 1.3113e-06, 1.4901e-06, 1.9670e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.7881e-06, 1.6093e-06, 1.5497e-06, 1.7881e-06, 1.5497e-06, 1.4901e-06,\n",
            "        1.7881e-06, 1.9670e-06, 1.6689e-06, 1.6093e-06, 1.5497e-06, 1.7881e-06,\n",
            "        1.6689e-06, 1.3113e-06, 1.4901e-06, 1.9670e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.7881e-06, 1.6093e-06, 1.6689e-06, 1.7881e-06, 1.5497e-06, 1.4901e-06,\n",
            "        1.7881e-06, 1.7881e-06, 1.6689e-06, 1.5497e-06, 1.6689e-06, 1.7881e-06,\n",
            "        1.6093e-06, 1.3113e-06, 1.4901e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7881e-06, 1.5497e-06, 1.8477e-06, 1.7881e-06, 1.6093e-06, 1.6093e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.6689e-06, 1.5497e-06, 1.6689e-06, 1.6689e-06,\n",
            "        1.6093e-06, 1.3709e-06, 1.4901e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.7881e-06, 1.5497e-06, 1.6689e-06, 1.7881e-06, 1.6093e-06, 1.6093e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.6689e-06, 1.4901e-06, 1.6689e-06, 1.6689e-06,\n",
            "        1.6093e-06, 1.4305e-06, 1.4901e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7881e-06, 1.6093e-06, 1.6689e-06, 1.7881e-06, 1.6093e-06, 1.6689e-06,\n",
            "        1.7881e-06, 1.9670e-06, 1.6689e-06, 1.4901e-06, 1.5497e-06, 1.6689e-06,\n",
            "        1.6689e-06, 1.4901e-06, 1.3113e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.7881e-06, 1.6093e-06, 1.6689e-06, 1.7881e-06, 1.6093e-06, 1.6689e-06,\n",
            "        1.7881e-06, 1.9670e-06, 1.7285e-06, 1.4901e-06, 1.6093e-06, 1.7285e-06,\n",
            "        1.7881e-06, 1.6093e-06, 1.5497e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.7881e-06, 1.6093e-06, 1.5497e-06, 1.7881e-06, 1.9073e-06, 1.6093e-06,\n",
            "        1.7881e-06, 1.7881e-06, 1.9073e-06, 1.9073e-06, 1.6093e-06, 1.6689e-06,\n",
            "        1.7881e-06, 1.7285e-06, 1.9073e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.7881e-06, 1.6689e-06, 1.6689e-06, 1.7881e-06, 2.2054e-06, 1.6093e-06,\n",
            "        1.7881e-06, 1.7881e-06, 1.9073e-06, 1.9073e-06, 1.6093e-06, 1.7285e-06,\n",
            "        1.7881e-06, 1.7285e-06, 1.4901e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.7881e-06, 2.2054e-06, 1.6689e-06, 1.7881e-06, 1.6689e-06, 1.7285e-06,\n",
            "        1.7881e-06, 1.7881e-06, 1.9670e-06, 1.9073e-06, 1.6689e-06, 1.9073e-06,\n",
            "        1.9670e-06, 1.7285e-06, 1.5497e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.1458e-06, 2.4438e-06, 1.4901e-06, 2.2650e-06, 1.0729e-06, 8.9407e-07,\n",
            "        2.9802e-06, 2.5034e-06, 1.3709e-06, 1.5497e-06, 1.6093e-06, 1.4901e-06,\n",
            "        2.6226e-06, 4.2319e-06, 2.5630e-06, 2.9802e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.0994e-06, 2.7418e-06, 1.1325e-06, 2.1458e-06, 7.7486e-07, 7.1526e-07,\n",
            "        3.9935e-06, 2.0862e-06, 1.0729e-06, 1.5497e-06, 1.1325e-06, 1.1921e-06,\n",
            "        4.2915e-06, 3.6955e-06, 2.6822e-06, 4.4107e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.1921e-06, 1.7881e-06, 1.5497e-06, 1.8477e-06, 1.0133e-06, 9.5367e-07,\n",
            "        2.5034e-06, 3.2783e-06, 1.1921e-06, 1.8477e-06, 1.2517e-06, 1.3113e-06,\n",
            "        1.8477e-06, 2.8014e-06, 2.9802e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.4901e-06, 2.2054e-06, 1.1921e-06, 1.5497e-06, 1.3113e-06, 1.0729e-06,\n",
            "        2.6822e-06, 2.0862e-06, 1.4305e-06, 2.5630e-06, 1.3113e-06, 1.6093e-06,\n",
            "        1.5497e-06, 1.9073e-06, 2.5034e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.3113e-06, 1.2517e-06, 1.4901e-06, 1.6689e-06, 1.6689e-06, 1.1325e-06,\n",
            "        1.6093e-06, 1.4901e-06, 1.3113e-06, 1.7285e-06, 9.5367e-07, 1.8477e-06,\n",
            "        1.1325e-06, 2.0862e-06, 1.7285e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([9.5367e-07, 1.0133e-06, 1.1325e-06, 1.4305e-06, 1.4305e-06, 1.1325e-06,\n",
            "        1.6689e-06, 1.3113e-06, 1.1325e-06, 1.2517e-06, 9.5367e-07, 1.7285e-06,\n",
            "        1.1921e-06, 2.1458e-06, 1.7285e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([8.9407e-07, 8.3447e-07, 1.1921e-06, 1.5497e-06, 1.3709e-06, 1.2517e-06,\n",
            "        1.7285e-06, 1.2517e-06, 1.1921e-06, 1.2517e-06, 8.3447e-07, 1.7881e-06,\n",
            "        1.1921e-06, 1.6689e-06, 1.4305e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([8.3447e-07, 8.3447e-07, 1.1921e-06, 1.6689e-06, 1.4305e-06, 1.3113e-06,\n",
            "        1.5497e-06, 1.2517e-06, 1.2517e-06, 1.1921e-06, 8.3447e-07, 1.8477e-06,\n",
            "        1.1325e-06, 1.4901e-06, 1.3113e-06, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([8.3447e-07, 8.3447e-07, 1.1921e-06, 1.4901e-06, 1.0729e-06, 1.2517e-06,\n",
            "        1.5497e-06, 1.1921e-06, 1.3113e-06, 1.1921e-06, 8.3447e-07, 1.2517e-06,\n",
            "        1.1325e-06, 1.3113e-06, 1.3113e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([8.3447e-07, 7.7486e-07, 1.0133e-06, 1.4901e-06, 1.1325e-06, 1.1921e-06,\n",
            "        1.5497e-06, 1.2517e-06, 1.3113e-06, 1.1921e-06, 8.3447e-07, 1.4901e-06,\n",
            "        1.1921e-06, 1.3113e-06, 1.3113e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([7.7486e-07, 7.7486e-07, 1.0729e-06, 1.4901e-06, 1.1921e-06, 1.2517e-06,\n",
            "        1.3113e-06, 1.2517e-06, 1.2517e-06, 1.1921e-06, 8.3447e-07, 1.3113e-06,\n",
            "        1.2517e-06, 1.3113e-06, 1.3113e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([7.7486e-07, 7.7486e-07, 1.0729e-06, 1.3113e-06, 1.1921e-06, 1.1921e-06,\n",
            "        1.3113e-06, 1.2517e-06, 1.1921e-06, 1.1921e-06, 8.3447e-07, 1.3113e-06,\n",
            "        1.2517e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([7.7486e-07, 7.7486e-07, 1.1325e-06, 1.3113e-06, 1.1921e-06, 1.3113e-06,\n",
            "        1.3113e-06, 1.2517e-06, 1.3113e-06, 1.1921e-06, 7.7486e-07, 1.3113e-06,\n",
            "        1.2517e-06, 1.3113e-06, 1.1921e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([7.7486e-07, 7.7486e-07, 1.0729e-06, 1.3113e-06, 1.1921e-06, 1.3113e-06,\n",
            "        1.3113e-06, 1.2517e-06, 1.3113e-06, 1.1921e-06, 7.7486e-07, 1.1921e-06,\n",
            "        1.1921e-06, 1.1921e-06, 1.1921e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([7.7486e-07, 7.7486e-07, 1.1325e-06, 1.3113e-06, 1.1921e-06, 1.3113e-06,\n",
            "        1.3113e-06, 1.3113e-06, 1.3113e-06, 1.1921e-06, 7.7486e-07, 1.1921e-06,\n",
            "        1.2517e-06, 1.1921e-06, 1.1921e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([7.7486e-07, 7.7486e-07, 1.1921e-06, 1.1921e-06, 1.1921e-06, 1.1921e-06,\n",
            "        1.3113e-06, 1.3113e-06, 1.1921e-06, 1.1921e-06, 7.7486e-07, 1.1921e-06,\n",
            "        1.2517e-06, 1.1921e-06, 1.1921e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([7.7486e-07, 7.7486e-07, 1.1921e-06, 1.1921e-06, 1.1921e-06, 1.1921e-06,\n",
            "        1.3113e-06, 1.3113e-06, 1.1921e-06, 1.3113e-06, 7.7486e-07, 1.1921e-06,\n",
            "        1.2517e-06, 1.1921e-06, 1.1921e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([7.7486e-07, 7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.1921e-06,\n",
            "        1.3113e-06, 1.3113e-06, 1.1921e-06, 1.1921e-06, 7.7486e-07, 1.1921e-06,\n",
            "        1.2517e-06, 1.1921e-06, 1.1921e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([7.7486e-07, 7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.1921e-06,\n",
            "        1.3113e-06, 1.3113e-06, 1.1921e-06, 1.1921e-06, 7.7486e-07, 1.1921e-06,\n",
            "        1.2517e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([7.7486e-07, 7.7486e-07, 1.3113e-06, 1.1921e-06, 1.1921e-06, 1.1921e-06,\n",
            "        1.1921e-06, 1.3113e-06, 1.1921e-06, 1.1921e-06, 8.3447e-07, 1.1921e-06,\n",
            "        1.2517e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.4048e-05, 4.4107e-05, 4.7803e-05, 4.4882e-05, 4.1723e-05, 4.3273e-05,\n",
            "        4.9055e-05, 4.1962e-05, 4.4167e-05, 4.7505e-05, 4.6551e-05, 4.1068e-05,\n",
            "        4.4107e-05, 4.6134e-05, 4.7743e-05, 4.4405e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([4.2796e-05, 4.4942e-05, 4.4584e-05, 4.3392e-05, 4.1246e-05, 4.3392e-05,\n",
            "        4.3094e-05, 4.1187e-05, 4.0710e-05, 4.3511e-05, 4.7326e-05, 4.1485e-05,\n",
            "        4.1604e-05, 4.7624e-05, 4.7028e-05, 4.3094e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.2379e-05, 4.3035e-05, 4.3333e-05, 4.1723e-05, 4.2260e-05, 4.3452e-05,\n",
            "        4.4167e-05, 4.2856e-05, 4.2200e-05, 4.4584e-05, 4.5359e-05, 4.2737e-05,\n",
            "        4.2498e-05, 4.7624e-05, 4.1008e-05, 4.3213e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.2140e-05, 4.2975e-05, 4.1306e-05, 4.3094e-05, 4.2558e-05, 4.3154e-05,\n",
            "        4.5359e-05, 4.2975e-05, 4.2558e-05, 4.3809e-05, 4.4465e-05, 4.2439e-05,\n",
            "        4.2379e-05, 4.6670e-05, 4.2796e-05, 4.3452e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([4.2439e-05, 4.2260e-05, 4.3094e-05, 4.3452e-05, 4.2558e-05, 4.2260e-05,\n",
            "        4.6730e-05, 4.3213e-05, 4.3154e-05, 4.2379e-05, 4.2260e-05, 4.2439e-05,\n",
            "        4.2379e-05, 4.3213e-05, 4.3213e-05, 4.3213e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([4.2260e-05, 4.2260e-05, 4.3571e-05, 4.2319e-05, 4.2558e-05, 4.2260e-05,\n",
            "        4.6611e-05, 4.3213e-05, 4.2737e-05, 4.2379e-05, 4.2260e-05, 4.2439e-05,\n",
            "        4.2379e-05, 4.3213e-05, 4.2558e-05, 4.2260e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.7968e-05, 3.7968e-05, 3.7789e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.8862e-05, 3.7968e-05, 3.8564e-05, 3.7968e-05, 3.7968e-05, 3.8087e-05,\n",
            "        3.8087e-05, 3.8743e-05, 3.7968e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.7968e-05, 3.7968e-05, 3.8147e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.8087e-05,\n",
            "        3.8087e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.7968e-05, 3.7968e-05, 3.8147e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.8087e-05,\n",
            "        3.8087e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.7968e-05, 3.7968e-05, 3.8147e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.8087e-05,\n",
            "        3.8087e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.7968e-05, 3.7968e-05, 3.8147e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.8087e-05,\n",
            "        3.8087e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.8087e-05,\n",
            "        3.8087e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.8087e-05,\n",
            "        3.8087e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.7551e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7551e-05, 3.7551e-05,\n",
            "        3.7551e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7551e-05, 3.7730e-05,\n",
            "        3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7968e-05, 3.7551e-05, 3.7551e-05,\n",
            "        3.7551e-05, 3.7968e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7730e-05,\n",
            "        3.7551e-05, 3.7551e-05, 3.7968e-05, 3.7551e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05,\n",
            "        3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7730e-05,\n",
            "        3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([3.7968e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05, 3.8087e-05,\n",
            "        3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([3.7968e-05, 3.7551e-05, 3.7968e-05, 3.7551e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.7968e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05, 3.7968e-05, 3.8087e-05,\n",
            "        3.7551e-05, 3.7968e-05, 3.7551e-05, 3.7551e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7551e-05, 3.7968e-05, 3.7968e-05,\n",
            "        3.7968e-05, 3.7551e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.8087e-05,\n",
            "        3.7968e-05, 3.7968e-05, 3.7551e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.7551e-05, 3.7968e-05, 3.7968e-05, 3.7551e-05, 3.7551e-05, 3.7551e-05,\n",
            "        3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7968e-05, 3.7730e-05,\n",
            "        3.7968e-05, 3.7551e-05, 3.7968e-05, 3.7968e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.7418e-06, 1.7881e-06, 3.5763e-06, 1.9670e-06, 3.5763e-06, 2.5034e-06,\n",
            "        5.7817e-06, 3.8743e-06, 2.6226e-06, 3.3379e-06, 3.2187e-06, 4.6492e-06,\n",
            "        2.5034e-06, 3.0994e-06, 2.5034e-06, 4.1127e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.0398e-06, 1.7285e-06, 3.5167e-06, 1.8477e-06, 4.2319e-06, 1.7285e-06,\n",
            "        3.3975e-06, 4.7088e-06, 2.5034e-06, 3.8743e-06, 3.2187e-06, 4.3511e-06,\n",
            "        3.2187e-06, 3.8147e-06, 2.4438e-06, 3.4571e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.4901e-06, 1.6689e-06, 1.7285e-06, 2.2054e-06, 4.2319e-06, 2.0266e-06,\n",
            "        3.5763e-06, 3.9339e-06, 2.3842e-06, 3.7551e-06, 3.9935e-06, 3.1590e-06,\n",
            "        2.1458e-06, 2.6822e-06, 2.1458e-06, 3.6359e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.4901e-06, 1.6689e-06, 1.9073e-06, 1.9073e-06, 3.7551e-06, 2.0862e-06,\n",
            "        2.9206e-06, 1.8477e-06, 2.2650e-06, 3.0994e-06, 2.3246e-06, 3.2783e-06,\n",
            "        2.0862e-06, 3.2187e-06, 2.2650e-06, 4.1127e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.4901e-06, 1.7285e-06, 2.1458e-06, 1.6093e-06, 3.6955e-06, 2.1458e-06,\n",
            "        1.8477e-06, 2.0266e-06, 2.0266e-06, 2.3246e-06, 2.3842e-06, 2.0266e-06,\n",
            "        2.0862e-06, 3.5167e-06, 2.0862e-06, 3.3379e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.4901e-06, 1.7881e-06, 2.1458e-06, 1.6689e-06, 2.2054e-06, 1.9670e-06,\n",
            "        1.9073e-06, 2.2054e-06, 1.9670e-06, 2.0862e-06, 2.3842e-06, 1.6689e-06,\n",
            "        2.0862e-06, 2.8014e-06, 1.7881e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.4901e-06, 1.7881e-06, 1.9073e-06, 1.6689e-06, 2.3246e-06, 1.7285e-06,\n",
            "        1.7881e-06, 2.2054e-06, 1.9670e-06, 2.1458e-06, 2.0266e-06, 1.6689e-06,\n",
            "        2.0862e-06, 2.2054e-06, 1.9073e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.4901e-06, 1.7881e-06, 1.7285e-06, 1.8477e-06, 2.3246e-06, 1.6689e-06,\n",
            "        1.9670e-06, 2.3246e-06, 1.9670e-06, 2.2054e-06, 2.0862e-06, 1.6689e-06,\n",
            "        2.0862e-06, 2.3246e-06, 1.9073e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.4901e-06, 1.7881e-06, 1.7881e-06, 1.5497e-06, 2.3842e-06, 1.6689e-06,\n",
            "        2.1458e-06, 1.8477e-06, 1.9670e-06, 2.3246e-06, 2.0862e-06, 1.6689e-06,\n",
            "        1.7881e-06, 2.2650e-06, 1.9073e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.5497e-06, 2.0862e-06, 1.7881e-06, 1.5497e-06, 2.3842e-06, 1.6689e-06,\n",
            "        2.2650e-06, 1.8477e-06, 1.9670e-06, 2.1458e-06, 2.2650e-06, 1.5497e-06,\n",
            "        1.9670e-06, 2.3842e-06, 2.2054e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.5497e-06, 2.0862e-06, 1.7285e-06, 1.5497e-06, 2.3842e-06, 1.7285e-06,\n",
            "        2.3246e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06, 2.3246e-06, 1.5497e-06,\n",
            "        2.3842e-06, 2.1458e-06, 2.2650e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.6093e-06, 2.0862e-06, 1.7881e-06, 1.6093e-06, 2.3842e-06, 1.7285e-06,\n",
            "        2.3246e-06, 1.8477e-06, 2.1458e-06, 1.8477e-06, 1.9073e-06, 1.5497e-06,\n",
            "        2.1458e-06, 2.1458e-06, 2.2650e-06, 2.0266e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.6093e-06, 1.9073e-06, 1.8477e-06, 1.6689e-06, 2.0266e-06, 1.7881e-06,\n",
            "        2.3246e-06, 1.8477e-06, 2.1458e-06, 1.8477e-06, 1.9670e-06, 1.6093e-06,\n",
            "        1.7881e-06, 1.8477e-06, 2.3246e-06, 2.0266e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.6093e-06, 2.0862e-06, 1.8477e-06, 1.7285e-06, 2.0266e-06, 1.9073e-06,\n",
            "        1.8477e-06, 2.1458e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06, 1.7285e-06,\n",
            "        1.7881e-06, 1.8477e-06, 1.8477e-06, 2.0266e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.7285e-06, 2.0266e-06, 1.8477e-06, 1.9073e-06, 2.0266e-06, 2.0266e-06,\n",
            "        1.8477e-06, 2.0266e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06,\n",
            "        1.7881e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.8477e-06, 1.7881e-06, 1.8477e-06, 1.9073e-06, 2.0266e-06, 2.1458e-06,\n",
            "        1.8477e-06, 2.0266e-06, 1.7881e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06,\n",
            "        1.7881e-06, 1.8477e-06, 2.0266e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.9073e-06, 1.7881e-06, 1.8477e-06, 1.9073e-06, 2.0266e-06, 2.1458e-06,\n",
            "        1.8477e-06, 1.8477e-06, 1.7881e-06, 1.8477e-06, 1.8477e-06, 2.0862e-06,\n",
            "        1.7881e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.0862e-06, 1.7285e-06, 1.8477e-06, 1.9670e-06, 1.8477e-06, 2.2650e-06,\n",
            "        2.0266e-06, 1.8477e-06, 1.7881e-06, 1.8477e-06, 1.8477e-06, 2.2650e-06,\n",
            "        1.7881e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.0862e-06, 1.6689e-06, 1.8477e-06, 1.6689e-06, 1.8477e-06, 2.2650e-06,\n",
            "        2.0266e-06, 1.8477e-06, 1.7881e-06, 1.8477e-06, 1.8477e-06, 2.3246e-06,\n",
            "        1.8477e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.0862e-06, 1.7285e-06, 1.8477e-06, 1.7285e-06, 1.8477e-06, 1.9073e-06,\n",
            "        2.0266e-06, 1.8477e-06, 1.7881e-06, 2.0266e-06, 1.8477e-06, 2.2650e-06,\n",
            "        1.9670e-06, 1.8477e-06, 1.8477e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.7418e-06, 3.0398e-06, 3.4571e-06, 4.1127e-06, 3.0994e-06, 2.6822e-06,\n",
            "        5.0664e-06, 3.0994e-06, 2.2054e-06, 5.0068e-06, 2.9802e-06, 2.5630e-06,\n",
            "        1.6093e-06, 2.9206e-06, 4.8876e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.8014e-06, 3.2783e-06, 2.9802e-06, 4.5896e-06, 2.5630e-06, 2.1458e-06,\n",
            "        3.2783e-06, 3.4571e-06, 2.6822e-06, 4.2319e-06, 3.8147e-06, 1.9670e-06,\n",
            "        2.0266e-06, 3.0994e-06, 6.3181e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.6822e-06, 2.5034e-06, 3.0398e-06, 3.9339e-06, 2.1458e-06, 2.3246e-06,\n",
            "        3.0994e-06, 3.1590e-06, 2.8610e-06, 2.9206e-06, 3.8147e-06, 2.0862e-06,\n",
            "        2.0862e-06, 2.6226e-06, 5.0664e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.5034e-06, 2.3246e-06, 3.1590e-06, 3.6955e-06, 2.2054e-06, 2.5630e-06,\n",
            "        2.4438e-06, 2.3246e-06, 2.8610e-06, 2.9206e-06, 2.7418e-06, 2.3842e-06,\n",
            "        2.3842e-06, 2.1458e-06, 4.1723e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.3246e-06, 2.1458e-06, 2.3842e-06, 2.8014e-06, 2.1458e-06, 2.2650e-06,\n",
            "        2.5034e-06, 2.2054e-06, 2.8610e-06, 2.5630e-06, 2.5034e-06, 1.9073e-06,\n",
            "        2.2650e-06, 2.1458e-06, 2.5034e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.2650e-06, 1.9670e-06, 2.3842e-06, 2.4438e-06, 2.0266e-06, 2.5034e-06,\n",
            "        2.5034e-06, 2.0862e-06, 2.6822e-06, 2.5034e-06, 2.5034e-06, 1.9073e-06,\n",
            "        2.3246e-06, 1.7881e-06, 2.2054e-06, 1.9670e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.2650e-06, 1.9670e-06, 2.3842e-06, 2.5034e-06, 2.1458e-06, 2.2054e-06,\n",
            "        2.0862e-06, 2.0862e-06, 2.2054e-06, 2.2054e-06, 2.2054e-06, 1.9670e-06,\n",
            "        2.2054e-06, 1.7881e-06, 2.1458e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.0862e-06, 1.6689e-06, 2.2054e-06, 2.5034e-06, 2.0266e-06, 2.2054e-06,\n",
            "        2.0862e-06, 2.0266e-06, 2.2054e-06, 2.2054e-06, 2.2054e-06, 1.6093e-06,\n",
            "        2.2650e-06, 1.9073e-06, 2.1458e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.6689e-06, 1.6689e-06, 2.2054e-06, 2.5034e-06, 2.2054e-06, 2.3842e-06,\n",
            "        1.9073e-06, 2.0266e-06, 2.2054e-06, 2.3842e-06, 2.2054e-06, 1.6689e-06,\n",
            "        2.2054e-06, 1.8477e-06, 2.1458e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.6689e-06, 1.6689e-06, 2.4438e-06, 2.2650e-06, 2.2054e-06, 2.2650e-06,\n",
            "        1.9073e-06, 2.0862e-06, 2.2054e-06, 1.9670e-06, 1.9073e-06, 1.6689e-06,\n",
            "        2.2054e-06, 1.6689e-06, 2.1458e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.6689e-06, 1.6689e-06, 2.2650e-06, 2.2650e-06, 2.2054e-06, 1.9670e-06,\n",
            "        1.9670e-06, 2.0862e-06, 2.2054e-06, 2.0266e-06, 1.9670e-06, 1.6689e-06,\n",
            "        2.2054e-06, 1.8477e-06, 2.1458e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.7285e-06, 1.6689e-06, 2.2650e-06, 2.1458e-06, 2.0266e-06, 1.9670e-06,\n",
            "        2.2054e-06, 1.9670e-06, 2.2054e-06, 1.9670e-06, 1.9670e-06, 1.6689e-06,\n",
            "        2.2054e-06, 1.9073e-06, 2.1458e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.6689e-06, 1.7285e-06, 2.2650e-06, 2.1458e-06, 2.0862e-06, 2.0266e-06,\n",
            "        1.9670e-06, 1.9670e-06, 2.2650e-06, 1.9670e-06, 1.9670e-06, 1.7285e-06,\n",
            "        2.0862e-06, 1.9670e-06, 2.1458e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7285e-06, 1.7285e-06, 2.3246e-06, 2.1458e-06, 2.1458e-06, 2.0266e-06,\n",
            "        1.9073e-06, 2.5034e-06, 2.1458e-06, 1.9670e-06, 1.9670e-06, 1.7285e-06,\n",
            "        2.1458e-06, 2.0266e-06, 2.1458e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.2650e-06, 2.2650e-06, 2.1458e-06, 2.1458e-06, 2.2054e-06, 2.0862e-06,\n",
            "        2.0862e-06, 1.9670e-06, 2.0862e-06, 1.9670e-06, 1.9670e-06, 2.2054e-06,\n",
            "        2.1458e-06, 2.2054e-06, 2.1458e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.2054e-06, 2.2650e-06, 1.9670e-06, 2.1458e-06, 2.0862e-06, 1.9670e-06,\n",
            "        1.9073e-06, 1.7881e-06, 2.1458e-06, 2.1458e-06, 1.9670e-06, 1.7881e-06,\n",
            "        2.1458e-06, 2.2650e-06, 2.1458e-06, 2.0266e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.7285e-06, 1.7881e-06, 1.9670e-06, 2.1458e-06, 2.1458e-06, 1.9073e-06,\n",
            "        2.2054e-06, 1.8477e-06, 2.0862e-06, 2.2650e-06, 1.9670e-06, 1.7285e-06,\n",
            "        2.0862e-06, 2.2650e-06, 1.9670e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.6093e-06, 1.7285e-06, 2.0266e-06, 2.2054e-06, 2.0862e-06, 1.9670e-06,\n",
            "        2.0862e-06, 1.8477e-06, 2.1458e-06, 2.2650e-06, 1.9073e-06, 1.7285e-06,\n",
            "        2.0862e-06, 2.3246e-06, 2.0266e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.6689e-06, 1.7285e-06, 2.0266e-06, 2.2054e-06, 2.0266e-06, 1.9073e-06,\n",
            "        2.0862e-06, 1.9670e-06, 2.6822e-06, 2.2650e-06, 1.9670e-06, 1.7881e-06,\n",
            "        2.0862e-06, 2.3842e-06, 2.0266e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.7285e-06, 1.7285e-06, 2.0862e-06, 2.2054e-06, 2.0266e-06, 2.0862e-06,\n",
            "        2.2650e-06, 1.8477e-06, 2.3246e-06, 2.0266e-06, 1.9670e-06, 2.3842e-06,\n",
            "        2.0862e-06, 2.5034e-06, 2.0266e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.8014e-06, 3.6359e-06, 2.5630e-06, 2.6226e-06, 3.5167e-06, 1.6093e-06,\n",
            "        3.3379e-06, 3.1590e-06, 2.8610e-06, 5.3048e-06, 2.9206e-06, 2.1458e-06,\n",
            "        2.2054e-06, 1.9670e-06, 5.3644e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.9670e-06, 2.6226e-06, 2.7418e-06, 2.6226e-06, 4.0531e-06, 1.6689e-06,\n",
            "        4.6492e-06, 2.8014e-06, 2.3842e-06, 2.6822e-06, 2.0266e-06, 1.6689e-06,\n",
            "        1.9073e-06, 2.2054e-06, 3.9935e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.2054e-06, 3.2783e-06, 2.4438e-06, 3.2187e-06, 3.9339e-06, 1.8477e-06,\n",
            "        4.0531e-06, 2.3246e-06, 2.7418e-06, 2.9802e-06, 1.7881e-06, 1.5497e-06,\n",
            "        1.8477e-06, 3.0994e-06, 3.9935e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.8477e-06, 3.6955e-06, 2.7418e-06, 1.6093e-06, 2.2650e-06, 2.1458e-06,\n",
            "        2.7418e-06, 2.7418e-06, 2.2054e-06, 2.6822e-06, 2.0862e-06, 1.7285e-06,\n",
            "        2.0862e-06, 1.8477e-06, 2.6226e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.8477e-06, 1.4901e-06, 2.2054e-06, 2.0266e-06, 2.3246e-06, 1.8477e-06,\n",
            "        2.4438e-06, 1.8477e-06, 2.4438e-06, 1.6093e-06, 1.7285e-06, 1.7285e-06,\n",
            "        1.6689e-06, 1.8477e-06, 2.7418e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.8477e-06, 1.4901e-06, 2.1458e-06, 2.1458e-06, 2.3246e-06, 1.8477e-06,\n",
            "        2.3842e-06, 2.0266e-06, 1.7285e-06, 1.6093e-06, 1.7285e-06, 1.6093e-06,\n",
            "        1.6689e-06, 1.8477e-06, 2.3842e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.7285e-06, 1.3709e-06, 2.1458e-06, 2.0862e-06, 2.0266e-06, 1.7881e-06,\n",
            "        2.2650e-06, 2.0266e-06, 1.7285e-06, 1.3113e-06, 1.8477e-06, 1.5497e-06,\n",
            "        1.7285e-06, 1.8477e-06, 2.3842e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.7285e-06, 1.3709e-06, 1.8477e-06, 2.1458e-06, 1.9073e-06, 1.7285e-06,\n",
            "        2.2650e-06, 2.0862e-06, 1.8477e-06, 1.3113e-06, 1.9073e-06, 1.6093e-06,\n",
            "        1.7285e-06, 1.8477e-06, 2.3842e-06, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.7285e-06, 1.3709e-06, 1.9073e-06, 2.1458e-06, 1.9073e-06, 1.7285e-06,\n",
            "        2.2650e-06, 2.1458e-06, 1.7881e-06, 1.3113e-06, 2.0266e-06, 1.6689e-06,\n",
            "        1.7285e-06, 1.7285e-06, 2.3842e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.7285e-06, 1.3709e-06, 2.0862e-06, 1.7285e-06, 1.9073e-06, 1.7285e-06,\n",
            "        1.9073e-06, 2.2054e-06, 1.7285e-06, 1.3113e-06, 2.1458e-06, 1.7285e-06,\n",
            "        1.7285e-06, 1.6689e-06, 2.2650e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.7285e-06, 1.3113e-06, 1.7285e-06, 1.7285e-06, 1.9073e-06, 1.7285e-06,\n",
            "        1.9670e-06, 1.7285e-06, 1.7285e-06, 1.2517e-06, 1.7285e-06, 2.0862e-06,\n",
            "        1.6689e-06, 1.6689e-06, 1.9073e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.5497e-06, 1.3113e-06, 1.7881e-06, 1.7285e-06, 1.9073e-06, 1.7285e-06,\n",
            "        1.9670e-06, 2.0266e-06, 1.7881e-06, 1.3113e-06, 1.7285e-06, 1.7881e-06,\n",
            "        1.6689e-06, 1.7285e-06, 1.9073e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.5497e-06, 1.3113e-06, 1.7881e-06, 1.7285e-06, 1.9073e-06, 1.5497e-06,\n",
            "        1.9670e-06, 2.0862e-06, 1.8477e-06, 1.3113e-06, 1.7881e-06, 1.9670e-06,\n",
            "        1.6689e-06, 1.7881e-06, 1.9670e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.6093e-06, 1.3113e-06, 1.7881e-06, 1.7285e-06, 1.9073e-06, 1.6093e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.8477e-06, 1.3113e-06, 1.7881e-06, 1.9670e-06,\n",
            "        1.7285e-06, 1.7881e-06, 1.9670e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.6093e-06, 1.3113e-06, 1.7881e-06, 1.7285e-06, 2.2650e-06, 1.6093e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.6689e-06, 1.3113e-06, 1.7881e-06, 1.7881e-06,\n",
            "        1.7285e-06, 1.7285e-06, 1.9670e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.6093e-06, 1.3113e-06, 2.0862e-06, 1.7881e-06, 1.9073e-06, 1.6093e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.7285e-06, 1.3113e-06, 1.7881e-06, 1.8477e-06,\n",
            "        1.6689e-06, 1.6689e-06, 1.9670e-06, 1.9670e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.6093e-06, 1.3113e-06, 1.7881e-06, 1.7881e-06, 1.9073e-06, 1.6093e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.7881e-06, 1.3113e-06, 1.7881e-06, 1.8477e-06,\n",
            "        1.6689e-06, 1.7285e-06, 1.9670e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.6093e-06, 1.3113e-06, 1.7881e-06, 1.7881e-06, 1.9670e-06, 1.9073e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.7285e-06, 1.3113e-06, 1.7881e-06, 1.8477e-06,\n",
            "        1.7285e-06, 1.8477e-06, 1.9670e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.0266e-06, 1.3113e-06, 1.7881e-06, 1.7881e-06, 1.9670e-06, 1.9073e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.7285e-06, 1.3113e-06, 1.7881e-06, 1.8477e-06,\n",
            "        1.6689e-06, 1.7881e-06, 1.7881e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.9073e-06, 1.3113e-06, 1.7881e-06, 2.0862e-06, 1.9670e-06, 1.8477e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.7285e-06, 1.3113e-06, 1.7881e-06, 1.8477e-06,\n",
            "        1.7285e-06, 1.7881e-06, 1.7881e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([5.3048e-06, 1.6689e-06, 4.2915e-06, 3.9935e-06, 2.8014e-06, 3.1590e-06,\n",
            "        3.2783e-06, 3.9339e-06, 2.4438e-06, 4.7684e-06, 2.9802e-06, 5.2452e-06,\n",
            "        5.3048e-06, 4.0531e-06, 4.9472e-06, 3.0398e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.5630e-06, 1.7285e-06, 5.2452e-06, 3.3975e-06, 2.8610e-06, 3.3975e-06,\n",
            "        2.8610e-06, 5.1856e-06, 2.4438e-06, 3.8743e-06, 2.8610e-06, 2.3246e-06,\n",
            "        5.1856e-06, 4.6492e-06, 5.2452e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.3842e-06, 2.0266e-06, 2.2054e-06, 2.8014e-06, 3.1590e-06, 2.8014e-06,\n",
            "        2.7418e-06, 2.7418e-06, 2.3246e-06, 3.7551e-06, 3.5167e-06, 2.4438e-06,\n",
            "        4.2319e-06, 2.2054e-06, 2.2650e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.2650e-06, 2.3246e-06, 2.6226e-06, 2.8014e-06, 3.4571e-06, 3.0398e-06,\n",
            "        2.2054e-06, 3.0398e-06, 2.2650e-06, 3.0994e-06, 3.5167e-06, 2.7418e-06,\n",
            "        2.9206e-06, 2.5034e-06, 2.0862e-06, 2.9206e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.5630e-06, 2.5630e-06, 2.5034e-06, 2.5630e-06, 2.3842e-06, 2.7418e-06,\n",
            "        2.2054e-06, 2.6822e-06, 2.3842e-06, 2.8014e-06, 2.3842e-06, 2.2054e-06,\n",
            "        2.5630e-06, 2.1458e-06, 2.2054e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.7418e-06, 2.5630e-06, 2.5034e-06, 2.8610e-06, 2.3842e-06, 2.3246e-06,\n",
            "        2.2054e-06, 2.6822e-06, 2.3246e-06, 2.8610e-06, 2.3842e-06, 2.2054e-06,\n",
            "        2.5034e-06, 2.0266e-06, 1.7881e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.2650e-06, 2.5630e-06, 2.5034e-06, 2.3246e-06, 2.1458e-06, 2.2054e-06,\n",
            "        2.2054e-06, 2.5034e-06, 2.3246e-06, 2.3246e-06, 2.2650e-06, 2.2650e-06,\n",
            "        2.2650e-06, 2.3842e-06, 1.6689e-06, 2.0266e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.3246e-06, 2.2650e-06, 2.5034e-06, 2.3246e-06, 2.2054e-06, 2.2054e-06,\n",
            "        2.1458e-06, 2.5034e-06, 2.3246e-06, 2.3246e-06, 2.1458e-06, 2.1458e-06,\n",
            "        2.2650e-06, 2.3246e-06, 1.6689e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.3246e-06, 1.8477e-06, 2.2650e-06, 2.3246e-06, 2.2054e-06, 2.2054e-06,\n",
            "        1.8477e-06, 2.3246e-06, 2.3842e-06, 2.1458e-06, 2.1458e-06, 2.1458e-06,\n",
            "        2.2054e-06, 2.3246e-06, 1.6689e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.3842e-06, 1.8477e-06, 2.2650e-06, 2.3246e-06, 2.2054e-06, 2.3246e-06,\n",
            "        1.8477e-06, 2.3246e-06, 2.2054e-06, 2.2054e-06, 2.1458e-06, 2.0862e-06,\n",
            "        2.2054e-06, 2.3246e-06, 1.7881e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.2650e-06, 1.8477e-06, 2.2650e-06, 2.3246e-06, 2.2054e-06, 2.0862e-06,\n",
            "        1.8477e-06, 2.3246e-06, 2.2054e-06, 2.2054e-06, 2.1458e-06, 2.0266e-06,\n",
            "        2.2054e-06, 2.1458e-06, 1.7881e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.2650e-06, 1.8477e-06, 2.2054e-06, 2.4438e-06, 2.2054e-06, 2.0266e-06,\n",
            "        1.8477e-06, 2.3246e-06, 2.2054e-06, 2.2650e-06, 2.0266e-06, 2.0266e-06,\n",
            "        2.2054e-06, 2.1458e-06, 1.8477e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.1458e-06, 1.6689e-06, 2.2054e-06, 2.3246e-06, 2.2054e-06, 2.0266e-06,\n",
            "        1.8477e-06, 2.3246e-06, 2.2054e-06, 2.2650e-06, 2.0266e-06, 2.1458e-06,\n",
            "        2.2054e-06, 2.0862e-06, 1.9670e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.0862e-06, 1.6093e-06, 2.2054e-06, 2.2650e-06, 2.0266e-06, 2.0266e-06,\n",
            "        1.8477e-06, 2.1458e-06, 2.2054e-06, 2.4438e-06, 2.0266e-06, 2.2650e-06,\n",
            "        2.2054e-06, 2.0862e-06, 2.0862e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.0862e-06, 1.7285e-06, 2.0862e-06, 2.0862e-06, 1.9670e-06, 2.0266e-06,\n",
            "        1.7881e-06, 2.1458e-06, 2.2054e-06, 2.1458e-06, 2.0266e-06, 2.2650e-06,\n",
            "        2.2054e-06, 2.1458e-06, 2.3842e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.0862e-06, 1.6689e-06, 2.0862e-06, 2.0862e-06, 1.9670e-06, 2.0266e-06,\n",
            "        1.7881e-06, 2.0266e-06, 2.2054e-06, 2.0862e-06, 2.0266e-06, 2.2650e-06,\n",
            "        2.0862e-06, 2.1458e-06, 2.3842e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.0266e-06, 1.7285e-06, 2.0862e-06, 2.0862e-06, 2.0266e-06, 2.0862e-06,\n",
            "        1.7881e-06, 2.0266e-06, 2.2054e-06, 2.0266e-06, 2.0266e-06, 2.0266e-06,\n",
            "        2.0862e-06, 2.1458e-06, 2.3842e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.0266e-06, 1.7881e-06, 2.3246e-06, 2.4438e-06, 2.0862e-06, 2.0862e-06,\n",
            "        1.7881e-06, 2.3246e-06, 2.2054e-06, 2.0266e-06, 2.1458e-06, 2.0266e-06,\n",
            "        2.0862e-06, 2.1458e-06, 2.3246e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.0862e-06, 1.7881e-06, 2.3842e-06, 2.4438e-06, 2.0266e-06, 2.0862e-06,\n",
            "        1.7881e-06, 2.5034e-06, 2.2054e-06, 2.2650e-06, 2.1458e-06, 1.9670e-06,\n",
            "        2.0862e-06, 2.2650e-06, 2.2054e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.3246e-06, 1.8477e-06, 2.3842e-06, 2.2054e-06, 2.0266e-06, 2.0862e-06,\n",
            "        1.7881e-06, 2.2054e-06, 2.2054e-06, 2.2650e-06, 2.1458e-06, 2.0266e-06,\n",
            "        2.0862e-06, 2.5034e-06, 2.1458e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.0994e-06, 3.1590e-06, 2.5630e-06, 2.3842e-06, 2.8014e-06, 2.9206e-06,\n",
            "        2.2054e-06, 3.2187e-06, 2.3246e-06, 2.3842e-06, 5.4836e-06, 3.2783e-06,\n",
            "        2.3246e-06, 3.0994e-06, 4.2319e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.5630e-06, 2.5630e-06, 2.1458e-06, 2.1458e-06, 2.3842e-06, 1.9073e-06,\n",
            "        2.9802e-06, 3.3379e-06, 1.7881e-06, 2.1458e-06, 5.0664e-06, 3.4571e-06,\n",
            "        1.8477e-06, 1.9670e-06, 3.1590e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.8610e-06, 2.6822e-06, 2.3246e-06, 2.2650e-06, 2.8014e-06, 2.0266e-06,\n",
            "        3.0398e-06, 2.3246e-06, 1.6689e-06, 1.7881e-06, 2.0862e-06, 4.6492e-06,\n",
            "        1.7285e-06, 2.2650e-06, 3.2783e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.4438e-06, 2.3842e-06, 2.3246e-06, 2.3842e-06, 3.2187e-06, 2.0266e-06,\n",
            "        2.0266e-06, 2.8014e-06, 1.6689e-06, 1.7285e-06, 1.7881e-06, 4.5300e-06,\n",
            "        1.9073e-06, 2.3246e-06, 2.6226e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.8477e-06, 2.0862e-06, 1.7285e-06, 1.8477e-06, 2.8610e-06, 2.0862e-06,\n",
            "        1.8477e-06, 1.6093e-06, 1.6689e-06, 2.0266e-06, 1.7285e-06, 2.4438e-06,\n",
            "        1.9670e-06, 1.8477e-06, 1.9670e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.8477e-06, 2.0862e-06, 1.7285e-06, 1.8477e-06, 2.0862e-06, 1.7881e-06,\n",
            "        1.8477e-06, 1.5497e-06, 1.6689e-06, 2.1458e-06, 1.7285e-06, 2.0862e-06,\n",
            "        1.9670e-06, 1.9073e-06, 1.9670e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.8477e-06, 2.0862e-06, 1.8477e-06, 1.7881e-06, 2.0862e-06, 1.9073e-06,\n",
            "        1.8477e-06, 1.6689e-06, 1.9073e-06, 1.3709e-06, 1.7285e-06, 2.0862e-06,\n",
            "        1.6689e-06, 1.7881e-06, 2.1458e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.8477e-06, 1.7881e-06, 1.9073e-06, 1.7881e-06, 2.0862e-06, 1.9073e-06,\n",
            "        1.8477e-06, 1.6689e-06, 1.9073e-06, 1.3709e-06, 1.4901e-06, 2.0266e-06,\n",
            "        1.9073e-06, 1.7881e-06, 1.8477e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.0862e-06, 1.7881e-06, 1.9073e-06, 1.7881e-06, 1.8477e-06, 1.9073e-06,\n",
            "        1.6689e-06, 1.4901e-06, 1.9073e-06, 1.3709e-06, 1.4901e-06, 2.0266e-06,\n",
            "        1.9073e-06, 1.7881e-06, 1.8477e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.9073e-06, 1.7881e-06, 1.9073e-06, 1.4901e-06, 1.7881e-06, 1.9073e-06,\n",
            "        1.6689e-06, 1.4901e-06, 2.0862e-06, 1.3709e-06, 1.4305e-06, 2.0862e-06,\n",
            "        1.9073e-06, 1.9073e-06, 1.8477e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.9073e-06, 1.7881e-06, 2.0862e-06, 1.4901e-06, 1.7881e-06, 1.9073e-06,\n",
            "        1.7881e-06, 1.4901e-06, 2.0862e-06, 1.3709e-06, 1.4305e-06, 2.0862e-06,\n",
            "        1.9073e-06, 1.9073e-06, 1.8477e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.9073e-06, 1.7881e-06, 2.0862e-06, 1.4901e-06, 2.2054e-06, 1.9670e-06,\n",
            "        2.0862e-06, 1.4901e-06, 1.8477e-06, 1.3709e-06, 1.4305e-06, 1.9073e-06,\n",
            "        1.9670e-06, 1.7881e-06, 2.0862e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.9073e-06, 1.7881e-06, 2.0862e-06, 1.4901e-06, 2.2054e-06, 1.9073e-06,\n",
            "        1.7881e-06, 1.4901e-06, 1.7285e-06, 1.3709e-06, 1.4305e-06, 1.9073e-06,\n",
            "        1.9670e-06, 1.7881e-06, 2.3246e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7285e-06, 2.2054e-06, 1.8477e-06, 1.4901e-06, 2.2054e-06, 2.1458e-06,\n",
            "        1.9670e-06, 1.4901e-06, 1.6689e-06, 1.3709e-06, 1.4305e-06, 1.8477e-06,\n",
            "        1.8477e-06, 1.6689e-06, 2.0862e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.8477e-06, 2.2054e-06, 1.6689e-06, 1.4901e-06, 1.8477e-06, 1.8477e-06,\n",
            "        1.8477e-06, 1.4901e-06, 1.6689e-06, 1.3709e-06, 1.4305e-06, 1.6689e-06,\n",
            "        1.9670e-06, 1.7285e-06, 1.8477e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.8477e-06, 1.9073e-06, 1.6689e-06, 1.4305e-06, 1.8477e-06, 1.8477e-06,\n",
            "        1.7285e-06, 1.4901e-06, 1.6689e-06, 1.3709e-06, 1.4305e-06, 1.6689e-06,\n",
            "        1.6689e-06, 1.7881e-06, 1.6689e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.6689e-06, 1.7285e-06, 1.6689e-06, 1.4305e-06, 1.8477e-06, 1.8477e-06,\n",
            "        1.7285e-06, 1.4901e-06, 1.6689e-06, 1.3113e-06, 1.4305e-06, 1.6689e-06,\n",
            "        1.7285e-06, 1.7881e-06, 1.6689e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.6689e-06, 1.7285e-06, 1.6689e-06, 1.6093e-06, 1.8477e-06, 1.9073e-06,\n",
            "        1.7285e-06, 1.4305e-06, 1.6689e-06, 1.3113e-06, 1.4305e-06, 1.6689e-06,\n",
            "        1.7285e-06, 1.6689e-06, 1.6689e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.6689e-06, 1.7285e-06, 1.6689e-06, 1.6093e-06, 1.8477e-06, 1.6093e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.7881e-06, 1.4305e-06, 1.4305e-06, 1.6689e-06,\n",
            "        1.7881e-06, 1.6689e-06, 1.6689e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.7285e-06, 1.7285e-06, 1.6689e-06, 1.6093e-06, 1.8477e-06, 1.9073e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.9073e-06, 1.5497e-06, 1.4305e-06, 1.6689e-06,\n",
            "        1.6689e-06, 1.7881e-06, 1.6689e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.6822e-06, 2.4438e-06, 2.3246e-06, 3.3975e-06, 4.4703e-06, 3.3975e-06,\n",
            "        5.2452e-06, 4.1723e-06, 4.6492e-06, 2.4438e-06, 3.3379e-06, 3.7551e-06,\n",
            "        2.6822e-06, 3.3379e-06, 3.6359e-06, 4.1723e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.6226e-06, 2.8014e-06, 2.1458e-06, 3.3975e-06, 3.9935e-06, 2.7418e-06,\n",
            "        6.6161e-06, 2.9206e-06, 2.5034e-06, 2.7418e-06, 3.1590e-06, 4.2319e-06,\n",
            "        3.2783e-06, 3.1590e-06, 3.0994e-06, 4.1723e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.2650e-06, 2.9802e-06, 2.2650e-06, 3.2187e-06, 2.6226e-06, 2.8014e-06,\n",
            "        5.7220e-06, 3.2783e-06, 2.6226e-06, 2.8014e-06, 2.8014e-06, 5.4836e-06,\n",
            "        2.9802e-06, 2.1458e-06, 3.2783e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.3842e-06, 2.8610e-06, 2.2650e-06, 2.4438e-06, 2.3246e-06, 2.5034e-06,\n",
            "        4.7684e-06, 3.3379e-06, 2.5034e-06, 2.6822e-06, 2.8610e-06, 5.3644e-06,\n",
            "        2.5630e-06, 2.3842e-06, 2.6822e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.5034e-06, 2.7418e-06, 2.3842e-06, 2.6822e-06, 2.3842e-06, 2.5630e-06,\n",
            "        3.1590e-06, 3.0398e-06, 2.5630e-06, 2.5034e-06, 2.5630e-06, 2.5630e-06,\n",
            "        2.3842e-06, 2.4438e-06, 2.6822e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.5034e-06, 2.8014e-06, 2.3246e-06, 2.6822e-06, 2.4438e-06, 2.3842e-06,\n",
            "        2.3842e-06, 2.3842e-06, 2.5630e-06, 2.5034e-06, 2.5034e-06, 2.5630e-06,\n",
            "        2.3842e-06, 2.5034e-06, 2.6822e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.5034e-06, 2.9206e-06, 2.4438e-06, 2.7418e-06, 2.4438e-06, 2.3842e-06,\n",
            "        2.3246e-06, 2.3246e-06, 2.5630e-06, 2.5034e-06, 2.3842e-06, 2.2650e-06,\n",
            "        2.3842e-06, 2.5034e-06, 2.6822e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.1458e-06, 1.9670e-06, 1.9073e-06, 2.8014e-06, 2.5630e-06, 2.4438e-06,\n",
            "        2.3246e-06, 2.3246e-06, 2.6226e-06, 2.3246e-06, 2.3246e-06, 2.2650e-06,\n",
            "        2.5034e-06, 2.3842e-06, 2.8014e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.7881e-06, 1.9670e-06, 1.7285e-06, 2.2650e-06, 2.4438e-06, 2.4438e-06,\n",
            "        2.3246e-06, 2.2650e-06, 2.6226e-06, 2.3246e-06, 2.0862e-06, 2.2650e-06,\n",
            "        2.4438e-06, 2.0266e-06, 2.5034e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.7881e-06, 1.8477e-06, 1.7881e-06, 2.2650e-06, 2.1458e-06, 2.4438e-06,\n",
            "        2.3246e-06, 2.2650e-06, 2.7418e-06, 2.0862e-06, 2.0862e-06, 2.2650e-06,\n",
            "        2.0862e-06, 2.0266e-06, 2.5034e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.7285e-06, 1.8477e-06, 1.7285e-06, 2.2650e-06, 2.1458e-06, 2.0862e-06,\n",
            "        2.3246e-06, 2.4438e-06, 2.7418e-06, 2.1458e-06, 2.0862e-06, 2.2650e-06,\n",
            "        2.0862e-06, 2.0266e-06, 2.2054e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.7285e-06, 1.8477e-06, 1.7285e-06, 2.2650e-06, 2.1458e-06, 2.0862e-06,\n",
            "        2.3246e-06, 2.0862e-06, 2.4438e-06, 2.1458e-06, 2.0862e-06, 2.2650e-06,\n",
            "        2.0862e-06, 2.0266e-06, 2.2650e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.7285e-06, 1.8477e-06, 1.7285e-06, 2.2054e-06, 2.0862e-06, 2.0862e-06,\n",
            "        2.3246e-06, 2.0862e-06, 2.4438e-06, 2.1458e-06, 2.0862e-06, 2.0862e-06,\n",
            "        2.2054e-06, 2.0862e-06, 2.2650e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.8477e-06, 1.7285e-06, 1.7881e-06, 2.3842e-06, 2.0862e-06, 2.0862e-06,\n",
            "        2.3246e-06, 2.0862e-06, 2.3842e-06, 2.1458e-06, 2.0862e-06, 2.1458e-06,\n",
            "        2.2054e-06, 2.0862e-06, 2.3246e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.8477e-06, 1.6689e-06, 1.5497e-06, 2.3842e-06, 2.0862e-06, 2.1458e-06,\n",
            "        2.3842e-06, 2.0862e-06, 2.3842e-06, 2.1458e-06, 2.0862e-06, 2.1458e-06,\n",
            "        2.1458e-06, 2.0266e-06, 2.3246e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.8477e-06, 1.6689e-06, 1.6093e-06, 2.4438e-06, 2.0862e-06, 2.2054e-06,\n",
            "        2.3842e-06, 2.2054e-06, 2.3842e-06, 2.0862e-06, 2.3246e-06, 2.1458e-06,\n",
            "        2.1458e-06, 2.1458e-06, 2.3246e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.9073e-06, 1.6689e-06, 1.6093e-06, 2.4438e-06, 2.2650e-06, 2.2054e-06,\n",
            "        2.3842e-06, 2.1458e-06, 2.3842e-06, 2.1458e-06, 2.3246e-06, 2.1458e-06,\n",
            "        2.1458e-06, 2.1458e-06, 2.3842e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.8477e-06, 1.6689e-06, 1.7285e-06, 2.3842e-06, 2.3246e-06, 2.2054e-06,\n",
            "        2.2054e-06, 2.1458e-06, 2.3842e-06, 2.1458e-06, 2.3246e-06, 2.0862e-06,\n",
            "        2.1458e-06, 2.1458e-06, 2.3842e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.8477e-06, 1.7881e-06, 1.7285e-06, 2.5034e-06, 2.3246e-06, 2.2650e-06,\n",
            "        2.2054e-06, 2.2054e-06, 2.3842e-06, 2.1458e-06, 2.0862e-06, 2.2054e-06,\n",
            "        2.2054e-06, 2.1458e-06, 2.2650e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.8477e-06, 1.7881e-06, 1.8477e-06, 2.5034e-06, 2.3246e-06, 2.2054e-06,\n",
            "        2.2054e-06, 2.3842e-06, 2.3842e-06, 2.1458e-06, 2.0862e-06, 2.2054e-06,\n",
            "        2.2650e-06, 2.2054e-06, 2.2650e-06, 2.1458e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.1921e-06, 1.0729e-06, 1.7285e-06, 1.6093e-06, 2.2054e-06, 1.0133e-06,\n",
            "        3.0398e-06, 2.2650e-06, 2.9206e-06, 1.7881e-06, 2.2650e-06, 1.4901e-06,\n",
            "        1.3709e-06, 2.0862e-06, 1.4901e-06, 3.0994e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.6093e-06, 1.3709e-06, 1.3113e-06, 1.1921e-06, 2.2054e-06, 1.3709e-06,\n",
            "        2.6822e-06, 1.7881e-06, 3.5167e-06, 1.3709e-06, 1.5497e-06, 1.7881e-06,\n",
            "        1.4901e-06, 3.0994e-06, 1.3113e-06, 3.8147e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.9073e-06, 1.7881e-06, 1.6689e-06, 1.5497e-06, 2.1458e-06, 8.9407e-07,\n",
            "        1.5497e-06, 1.0133e-06, 3.0994e-06, 1.3709e-06, 1.0729e-06, 1.0133e-06,\n",
            "        2.0862e-06, 1.8477e-06, 1.7881e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.5497e-06, 2.3246e-06, 1.3113e-06, 1.7285e-06, 2.7418e-06, 1.0729e-06,\n",
            "        1.1921e-06, 8.9407e-07, 2.0266e-06, 1.2517e-06, 1.1325e-06, 1.0729e-06,\n",
            "        1.7881e-06, 1.9073e-06, 1.4901e-06, 1.1921e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.6689e-06, 1.7881e-06, 1.3113e-06, 1.7285e-06, 1.6093e-06, 1.1921e-06,\n",
            "        9.5367e-07, 1.0133e-06, 2.0862e-06, 1.3113e-06, 1.1921e-06, 8.9407e-07,\n",
            "        1.1921e-06, 1.6093e-06, 1.0729e-06, 1.1921e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.7285e-06, 1.4305e-06, 1.2517e-06, 1.4305e-06, 1.7285e-06, 1.1325e-06,\n",
            "        8.3447e-07, 1.0133e-06, 1.6689e-06, 1.0133e-06, 1.3113e-06, 9.5367e-07,\n",
            "        1.3113e-06, 1.6093e-06, 1.1325e-06, 1.1325e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.1921e-06, 1.3113e-06, 1.1921e-06, 1.4305e-06, 1.3113e-06, 1.1325e-06,\n",
            "        8.3447e-07, 8.3447e-07, 1.7285e-06, 1.0133e-06, 1.4305e-06, 1.0133e-06,\n",
            "        1.3709e-06, 1.6093e-06, 1.1921e-06, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.1921e-06, 1.3113e-06, 1.3113e-06, 1.4901e-06, 1.4305e-06, 1.1325e-06,\n",
            "        8.3447e-07, 8.3447e-07, 1.7285e-06, 1.0729e-06, 1.4305e-06, 1.0133e-06,\n",
            "        1.1921e-06, 1.6093e-06, 1.2517e-06, 9.5367e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.1921e-06, 1.3113e-06, 1.3709e-06, 1.2517e-06, 1.4901e-06, 1.1325e-06,\n",
            "        8.3447e-07, 8.9407e-07, 1.4305e-06, 1.0729e-06, 1.4305e-06, 1.0133e-06,\n",
            "        1.1921e-06, 1.4305e-06, 1.3113e-06, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.1921e-06, 1.3113e-06, 1.3113e-06, 1.2517e-06, 1.4305e-06, 1.1325e-06,\n",
            "        8.3447e-07, 8.9407e-07, 1.4305e-06, 1.1325e-06, 1.4305e-06, 1.0133e-06,\n",
            "        1.1921e-06, 1.4305e-06, 1.3113e-06, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.1921e-06, 1.3113e-06, 1.4305e-06, 1.3113e-06, 1.4305e-06, 1.1325e-06,\n",
            "        8.9407e-07, 8.9407e-07, 1.4305e-06, 1.1325e-06, 1.4305e-06, 1.0133e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.3709e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.1921e-06, 1.3113e-06, 1.4305e-06, 1.3113e-06, 1.4305e-06, 1.1325e-06,\n",
            "        8.9407e-07, 8.9407e-07, 1.4305e-06, 1.2517e-06, 1.4305e-06, 1.0133e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.4305e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.4305e-06, 1.3113e-06, 1.4305e-06, 1.3113e-06, 1.4305e-06, 1.1325e-06,\n",
            "        8.9407e-07, 9.5367e-07, 1.4305e-06, 1.2517e-06, 1.3709e-06, 1.0729e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.4305e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.1921e-06, 1.3113e-06, 1.4305e-06, 1.3113e-06, 1.4901e-06, 1.1325e-06,\n",
            "        8.9407e-07, 1.0133e-06, 1.3709e-06, 1.1325e-06, 1.3709e-06, 1.1325e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.3709e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.1921e-06, 1.3113e-06, 1.4305e-06, 1.3113e-06, 1.4305e-06, 1.1921e-06,\n",
            "        8.9407e-07, 1.0133e-06, 1.3709e-06, 1.1921e-06, 1.3709e-06, 1.1325e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.3709e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.1921e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06, 1.4305e-06, 1.2517e-06,\n",
            "        8.9407e-07, 1.0133e-06, 1.3709e-06, 1.1921e-06, 1.3709e-06, 1.1325e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.3709e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.1921e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06, 1.4305e-06, 1.2517e-06,\n",
            "        9.5367e-07, 9.5367e-07, 1.1921e-06, 1.1921e-06, 1.3709e-06, 1.1921e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.3709e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.1921e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06, 1.2517e-06,\n",
            "        1.0133e-06, 1.0133e-06, 1.1921e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.1921e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.1921e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06,\n",
            "        1.0133e-06, 1.0133e-06, 1.1921e-06, 1.1921e-06, 1.1921e-06, 1.3709e-06,\n",
            "        1.1921e-06, 1.3709e-06, 1.1921e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.1921e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06, 1.3113e-06,\n",
            "        1.0133e-06, 1.0133e-06, 1.1921e-06, 1.1921e-06, 1.1921e-06, 1.3113e-06,\n",
            "        1.1921e-06, 1.1921e-06, 1.1921e-06, 7.7486e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.2915e-06, 3.7551e-06, 3.9339e-06, 3.4571e-06, 3.6359e-06, 3.6955e-06,\n",
            "        3.5167e-06, 5.0068e-06, 3.0398e-06, 5.0068e-06, 3.5763e-06, 2.9802e-06,\n",
            "        3.5167e-06, 2.9206e-06, 5.9605e-06, 6.1989e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.0994e-06, 4.2319e-06, 3.3975e-06, 2.3842e-06, 2.5034e-06, 3.3379e-06,\n",
            "        3.8147e-06, 6.2585e-06, 3.6359e-06, 3.7551e-06, 3.1590e-06, 3.4571e-06,\n",
            "        3.6955e-06, 3.3379e-06, 3.5763e-06, 3.2783e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.3379e-06, 4.0531e-06, 3.0398e-06, 2.5630e-06, 2.8014e-06, 3.2187e-06,\n",
            "        4.3511e-06, 3.7551e-06, 3.0994e-06, 3.2187e-06, 2.5034e-06, 3.4571e-06,\n",
            "        2.9206e-06, 2.8610e-06, 3.8743e-06, 2.8014e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.6226e-06, 3.5763e-06, 3.4571e-06, 2.7418e-06, 2.8014e-06, 3.3379e-06,\n",
            "        4.7088e-06, 3.9935e-06, 3.2187e-06, 3.2783e-06, 2.7418e-06, 2.8014e-06,\n",
            "        2.6226e-06, 2.6226e-06, 3.3379e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.7418e-06, 2.9206e-06, 2.8014e-06, 2.7418e-06, 3.1590e-06, 3.2783e-06,\n",
            "        3.4571e-06, 2.7418e-06, 3.0994e-06, 2.9802e-06, 2.4438e-06, 2.8610e-06,\n",
            "        2.8014e-06, 2.8014e-06, 2.5034e-06, 2.2650e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.3842e-06, 2.5630e-06, 2.8014e-06, 2.6822e-06, 2.9206e-06, 2.6822e-06,\n",
            "        3.5167e-06, 2.8014e-06, 2.8014e-06, 3.0398e-06, 2.4438e-06, 2.8014e-06,\n",
            "        2.8014e-06, 2.8014e-06, 2.6226e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.5630e-06, 2.5630e-06, 2.8014e-06, 2.7418e-06, 2.6822e-06, 2.6822e-06,\n",
            "        2.9206e-06, 2.8014e-06, 2.6226e-06, 2.7418e-06, 2.3842e-06, 2.5630e-06,\n",
            "        2.8014e-06, 2.7418e-06, 2.6822e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.5034e-06, 2.3246e-06, 2.5034e-06, 2.4438e-06, 2.8014e-06, 2.0862e-06,\n",
            "        2.9206e-06, 2.6226e-06, 2.5630e-06, 2.6822e-06, 2.2054e-06, 2.5034e-06,\n",
            "        2.7418e-06, 2.6822e-06, 2.6226e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.3246e-06, 2.2054e-06, 2.4438e-06, 2.4438e-06, 2.5034e-06, 2.0862e-06,\n",
            "        2.9206e-06, 2.6226e-06, 2.5630e-06, 2.6822e-06, 2.1458e-06, 2.3246e-06,\n",
            "        2.4438e-06, 2.8014e-06, 2.6226e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.3246e-06, 2.1458e-06, 2.4438e-06, 2.4438e-06, 2.6226e-06, 2.0862e-06,\n",
            "        2.8014e-06, 2.8014e-06, 2.6226e-06, 2.7418e-06, 2.1458e-06, 2.3246e-06,\n",
            "        2.4438e-06, 2.8014e-06, 2.5034e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.3246e-06, 2.1458e-06, 2.4438e-06, 2.4438e-06, 2.6226e-06, 2.0862e-06,\n",
            "        2.8014e-06, 2.5630e-06, 2.6226e-06, 2.7418e-06, 2.2054e-06, 2.3246e-06,\n",
            "        2.4438e-06, 2.4438e-06, 2.5034e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.0862e-06, 2.1458e-06, 2.4438e-06, 2.4438e-06, 2.6226e-06, 2.0862e-06,\n",
            "        2.8014e-06, 2.3246e-06, 2.6226e-06, 2.4438e-06, 2.2054e-06, 2.3246e-06,\n",
            "        2.4438e-06, 2.4438e-06, 2.5034e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.0862e-06, 2.2650e-06, 2.4438e-06, 2.4438e-06, 2.6226e-06, 1.9670e-06,\n",
            "        2.8014e-06, 2.3246e-06, 2.5630e-06, 2.4438e-06, 2.2054e-06, 2.4438e-06,\n",
            "        2.4438e-06, 2.4438e-06, 2.5034e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.2054e-06, 2.2650e-06, 2.4438e-06, 2.4438e-06, 2.6822e-06, 1.9670e-06,\n",
            "        2.8014e-06, 2.3246e-06, 2.5630e-06, 2.5034e-06, 2.2054e-06, 2.1458e-06,\n",
            "        2.4438e-06, 2.4438e-06, 2.4438e-06, 2.3246e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.1458e-06, 2.2650e-06, 2.4438e-06, 2.4438e-06, 2.8610e-06, 2.0862e-06,\n",
            "        2.5034e-06, 2.3246e-06, 2.5630e-06, 2.5034e-06, 2.2054e-06, 2.2650e-06,\n",
            "        2.4438e-06, 2.4438e-06, 2.4438e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.3246e-06, 2.2650e-06, 2.5034e-06, 2.4438e-06, 2.8610e-06, 2.0862e-06,\n",
            "        2.5630e-06, 2.3246e-06, 2.6822e-06, 2.5034e-06, 2.0266e-06, 2.3842e-06,\n",
            "        2.5034e-06, 2.4438e-06, 2.4438e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.3246e-06, 2.2650e-06, 2.5034e-06, 2.4438e-06, 2.6822e-06, 2.1458e-06,\n",
            "        2.5630e-06, 2.3246e-06, 2.6822e-06, 2.5034e-06, 2.0266e-06, 2.3246e-06,\n",
            "        2.5630e-06, 2.7418e-06, 2.3246e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.3246e-06, 2.2650e-06, 2.5034e-06, 2.5034e-06, 2.6822e-06, 2.1458e-06,\n",
            "        2.5630e-06, 2.3246e-06, 2.6822e-06, 2.4438e-06, 2.0266e-06, 2.3246e-06,\n",
            "        2.5630e-06, 2.5034e-06, 2.3246e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.3246e-06, 2.1458e-06, 2.5630e-06, 2.5034e-06, 2.6822e-06, 2.2650e-06,\n",
            "        2.5034e-06, 2.5630e-06, 2.6822e-06, 2.4438e-06, 1.9670e-06, 2.3246e-06,\n",
            "        2.5630e-06, 2.5034e-06, 2.5630e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.3246e-06, 2.1458e-06, 2.5630e-06, 2.4438e-06, 2.5034e-06, 2.3246e-06,\n",
            "        2.4438e-06, 2.5630e-06, 2.6822e-06, 2.4438e-06, 1.9670e-06, 2.3246e-06,\n",
            "        2.5630e-06, 2.7418e-06, 2.5630e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "1 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "2 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "3 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "4 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "5 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "6 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "7 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "8 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "9 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "10 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "11 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "12 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "13 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "14 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "15 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "16 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "17 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "18 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "19 tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
            "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "0 tensor([6.2406e-05, 6.6221e-05, 6.1810e-05, 5.3883e-05, 5.9485e-05, 6.9737e-05,\n",
            "        5.8532e-05, 6.2764e-05, 5.9843e-05, 5.5254e-05, 5.5075e-05, 6.2823e-05,\n",
            "        6.1631e-05, 5.6922e-05, 5.8293e-05, 6.5565e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.9545e-05, 6.3896e-05, 5.7518e-05, 5.6624e-05, 5.9783e-05, 6.9320e-05,\n",
            "        6.0976e-05, 6.3181e-05, 6.1035e-05, 5.6565e-05, 5.5432e-05, 5.9605e-05,\n",
            "        5.5909e-05, 6.0976e-05, 5.8472e-05, 6.4313e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([6.1095e-05, 6.6459e-05, 5.9903e-05, 5.9485e-05, 5.9783e-05, 6.8963e-05,\n",
            "        6.1333e-05, 6.4790e-05, 5.9962e-05, 5.8353e-05, 5.7042e-05, 6.1214e-05,\n",
            "        5.7995e-05, 6.1870e-05, 5.9724e-05, 6.7234e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([6.1691e-05, 6.0141e-05, 6.1333e-05, 6.0081e-05, 6.1393e-05, 6.6638e-05,\n",
            "        6.1512e-05, 6.7055e-05, 6.0320e-05, 6.0320e-05, 5.8949e-05, 6.0916e-05,\n",
            "        5.9664e-05, 6.2108e-05, 6.0260e-05, 6.8009e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([6.1631e-05, 6.0260e-05, 6.0260e-05, 6.0320e-05, 6.1393e-05, 6.1572e-05,\n",
            "        6.1572e-05, 6.1154e-05, 6.0320e-05, 6.1393e-05, 6.1274e-05, 6.0499e-05,\n",
            "        6.0320e-05, 6.1631e-05, 6.0737e-05, 6.0499e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([6.1274e-05, 6.0260e-05, 6.1274e-05, 6.0320e-05, 6.1393e-05, 6.1154e-05,\n",
            "        6.1572e-05, 6.1154e-05, 6.0320e-05, 6.1393e-05, 6.1274e-05, 6.0499e-05,\n",
            "        6.0320e-05, 6.1631e-05, 6.1393e-05, 6.3777e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([5.4121e-05, 5.6148e-05, 5.4121e-05, 5.4121e-05, 5.5432e-05, 5.5552e-05,\n",
            "        5.4538e-05, 5.5194e-05, 5.5194e-05, 5.4121e-05, 5.4121e-05, 5.5432e-05,\n",
            "        5.5194e-05, 5.5552e-05, 5.4300e-05, 5.6148e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([5.5194e-05, 5.6148e-05, 5.4121e-05, 5.4121e-05, 5.5432e-05, 5.6565e-05,\n",
            "        5.4300e-05, 5.5194e-05, 5.5194e-05, 5.4121e-05, 5.5194e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.5552e-05, 5.4300e-05, 5.4896e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([5.5194e-05, 5.4896e-05, 5.4121e-05, 5.4121e-05, 5.5432e-05, 5.6148e-05,\n",
            "        5.5432e-05, 5.4121e-05, 5.5194e-05, 5.5194e-05, 5.5194e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.5194e-05, 5.5432e-05, 5.4896e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([5.4121e-05, 5.4896e-05, 5.4121e-05, 5.4121e-05, 5.5194e-05, 5.4121e-05,\n",
            "        5.5432e-05, 5.4419e-05, 5.5194e-05, 5.5194e-05, 5.4121e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.5194e-05, 5.4121e-05, 5.4896e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([5.4419e-05, 5.4896e-05, 5.4121e-05, 5.4121e-05, 5.5194e-05, 5.4121e-05,\n",
            "        5.5432e-05, 5.4419e-05, 5.4121e-05, 5.5194e-05, 5.4121e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4896e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([5.4419e-05, 5.4896e-05, 5.4121e-05, 5.4121e-05, 5.5194e-05, 5.4121e-05,\n",
            "        5.4657e-05, 5.4121e-05, 5.4121e-05, 5.4419e-05, 5.4121e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.4419e-05, 5.4419e-05, 5.4896e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([5.4419e-05, 5.4896e-05, 5.4419e-05, 5.4419e-05, 5.4419e-05, 5.4121e-05,\n",
            "        5.4657e-05, 5.4121e-05, 5.4419e-05, 5.4419e-05, 5.4419e-05, 5.4657e-05,\n",
            "        5.4419e-05, 5.4419e-05, 5.4419e-05, 5.4896e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([5.4121e-05, 5.4896e-05, 5.4419e-05, 5.4419e-05, 5.4419e-05, 5.4121e-05,\n",
            "        5.4657e-05, 5.4121e-05, 5.4419e-05, 5.4121e-05, 5.4419e-05, 5.4657e-05,\n",
            "        5.4419e-05, 5.4121e-05, 5.4419e-05, 5.6148e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([5.4121e-05, 5.4896e-05, 5.4419e-05, 5.4419e-05, 5.4121e-05, 5.4419e-05,\n",
            "        5.4300e-05, 5.4121e-05, 5.4419e-05, 5.4121e-05, 5.4419e-05, 5.4657e-05,\n",
            "        5.4419e-05, 5.4121e-05, 5.4121e-05, 5.4896e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([5.4121e-05, 5.4896e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05,\n",
            "        5.4300e-05, 5.4419e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4896e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([5.4121e-05, 5.4419e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4419e-05,\n",
            "        5.4300e-05, 5.4121e-05, 5.4121e-05, 5.4419e-05, 5.4121e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05,\n",
            "        5.4300e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.4419e-05, 5.4121e-05, 5.4121e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4419e-05, 5.4121e-05,\n",
            "        5.4300e-05, 5.4419e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4419e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([5.4419e-05, 5.4419e-05, 5.4121e-05, 5.4121e-05, 5.4419e-05, 5.4121e-05,\n",
            "        5.4300e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4121e-05, 5.4300e-05,\n",
            "        5.4121e-05, 5.4419e-05, 5.4121e-05, 5.4419e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([7.6056e-05, 7.1108e-05, 6.7949e-05, 7.0810e-05, 7.3671e-05, 7.8142e-05,\n",
            "        7.2956e-05, 8.3685e-05, 7.8917e-05, 7.3850e-05, 8.4996e-05, 7.7188e-05,\n",
            "        6.4909e-05, 6.4850e-05, 6.6459e-05, 7.3254e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([7.0035e-05, 6.3956e-05, 6.8247e-05, 7.0512e-05, 6.8963e-05, 7.4923e-05,\n",
            "        6.6400e-05, 8.0884e-05, 7.7069e-05, 6.7353e-05, 7.8917e-05, 7.6115e-05,\n",
            "        6.5863e-05, 6.5923e-05, 6.4671e-05, 6.8069e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([7.2479e-05, 6.5684e-05, 7.0274e-05, 7.1108e-05, 7.2360e-05, 6.7592e-05,\n",
            "        6.8724e-05, 8.1718e-05, 7.8082e-05, 6.9797e-05, 7.9930e-05, 7.8917e-05,\n",
            "        6.8367e-05, 7.0453e-05, 6.6817e-05, 7.0453e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([7.2658e-05, 6.7830e-05, 7.0214e-05, 7.1406e-05, 7.3373e-05, 6.8128e-05,\n",
            "        7.0512e-05, 6.8009e-05, 7.1645e-05, 7.2539e-05, 7.9691e-05, 8.1480e-05,\n",
            "        7.0810e-05, 7.1764e-05, 6.9201e-05, 7.2300e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([7.1883e-05, 7.0512e-05, 7.2241e-05, 7.1049e-05, 7.1883e-05, 7.0393e-05,\n",
            "        7.0274e-05, 6.9857e-05, 7.1466e-05, 7.1049e-05, 7.0333e-05, 7.1049e-05,\n",
            "        7.2062e-05, 7.1883e-05, 7.0512e-05, 7.0632e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([7.1883e-05, 7.0512e-05, 7.2241e-05, 7.2062e-05, 7.1883e-05, 7.0274e-05,\n",
            "        7.1883e-05, 7.0333e-05, 7.2360e-05, 7.2300e-05, 7.0393e-05, 7.0512e-05,\n",
            "        7.2062e-05, 7.0512e-05, 7.0512e-05, 7.0632e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([7.1883e-05, 7.1883e-05, 7.2062e-05, 7.2062e-05, 7.1883e-05, 7.0274e-05,\n",
            "        7.1883e-05, 7.0691e-05, 7.2062e-05, 7.2062e-05, 7.0274e-05, 7.0512e-05,\n",
            "        7.2062e-05, 7.0512e-05, 7.0512e-05, 7.0632e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([6.5148e-05, 6.5148e-05, 6.6340e-05, 6.6340e-05, 6.5148e-05, 6.6876e-05,\n",
            "        6.5148e-05, 6.7890e-05, 6.5267e-05, 6.5327e-05, 6.6876e-05, 6.6102e-05,\n",
            "        6.6340e-05, 6.6102e-05, 6.6102e-05, 6.5327e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([6.6102e-05, 6.5148e-05, 6.6340e-05, 6.6340e-05, 6.6102e-05, 6.6876e-05,\n",
            "        6.5148e-05, 6.7115e-05, 6.5148e-05, 6.5148e-05, 6.6876e-05, 6.6102e-05,\n",
            "        6.6340e-05, 6.6102e-05, 6.6102e-05, 6.5327e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([6.6102e-05, 6.5148e-05, 6.6340e-05, 6.6102e-05, 6.6102e-05, 6.5148e-05,\n",
            "        6.5148e-05, 6.6340e-05, 6.5148e-05, 6.5148e-05, 6.6876e-05, 6.5148e-05,\n",
            "        6.6340e-05, 6.6102e-05, 6.6102e-05, 6.5327e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([6.6102e-05, 6.5148e-05, 6.6340e-05, 6.5148e-05, 6.6102e-05, 6.5148e-05,\n",
            "        6.5148e-05, 6.5327e-05, 6.5148e-05, 6.5148e-05, 6.5982e-05, 6.5148e-05,\n",
            "        6.6340e-05, 6.6102e-05, 6.6102e-05, 6.5327e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([6.7353e-05, 6.5148e-05, 6.6340e-05, 6.5148e-05, 6.6102e-05, 6.5148e-05,\n",
            "        6.5148e-05, 6.5327e-05, 6.5744e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05,\n",
            "        6.6340e-05, 6.6102e-05, 6.5148e-05, 6.5327e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([6.7353e-05, 6.5744e-05, 6.6340e-05, 6.5744e-05, 6.7353e-05, 6.5148e-05,\n",
            "        6.5744e-05, 6.5982e-05, 6.5744e-05, 6.5744e-05, 6.5148e-05, 6.5744e-05,\n",
            "        6.6340e-05, 6.6102e-05, 6.5148e-05, 6.5982e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([6.5148e-05, 6.5744e-05, 6.5982e-05, 6.5744e-05, 6.7353e-05, 6.5148e-05,\n",
            "        6.5744e-05, 6.5744e-05, 6.5148e-05, 6.5744e-05, 6.5744e-05, 6.5744e-05,\n",
            "        6.5982e-05, 6.7353e-05, 6.5744e-05, 6.5744e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([6.5148e-05, 6.5148e-05, 6.5982e-05, 6.5148e-05, 6.5744e-05, 6.5744e-05,\n",
            "        6.5148e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05, 6.5744e-05, 6.5148e-05,\n",
            "        6.5982e-05, 6.7353e-05, 6.5744e-05, 6.5148e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([6.5148e-05, 6.5148e-05, 6.5982e-05, 6.5148e-05, 6.5148e-05, 6.5744e-05,\n",
            "        6.5148e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05,\n",
            "        6.5982e-05, 6.7353e-05, 6.5148e-05, 6.5148e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([6.5148e-05, 6.5148e-05, 6.5327e-05, 6.5148e-05, 6.5148e-05, 6.5744e-05,\n",
            "        6.5148e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05,\n",
            "        6.5327e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([6.5744e-05, 6.5148e-05, 6.5327e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05,\n",
            "        6.5744e-05, 6.5148e-05, 6.5744e-05, 6.5744e-05, 6.5148e-05, 6.5744e-05,\n",
            "        6.5327e-05, 6.5148e-05, 6.5148e-05, 6.5744e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([6.5744e-05, 6.5744e-05, 6.5327e-05, 6.5744e-05, 6.5148e-05, 6.5148e-05,\n",
            "        6.5148e-05, 6.5744e-05, 6.5744e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05,\n",
            "        6.5327e-05, 6.5148e-05, 6.5744e-05, 6.5148e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([6.5148e-05, 6.5744e-05, 6.5327e-05, 6.5744e-05, 6.5148e-05, 6.5148e-05,\n",
            "        6.5148e-05, 6.5744e-05, 6.5148e-05, 6.5148e-05, 6.5744e-05, 6.5148e-05,\n",
            "        6.5327e-05, 6.5148e-05, 6.5148e-05, 6.5148e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.3975e-06, 3.3975e-06, 1.7285e-06, 2.1458e-06, 4.0531e-06, 2.4438e-06,\n",
            "        2.9206e-06, 2.3842e-06, 2.6822e-06, 3.5167e-06, 3.2783e-06, 2.5630e-06,\n",
            "        4.0531e-06, 1.5497e-06, 1.6689e-06, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.8477e-06, 3.8743e-06, 1.7881e-06, 1.5497e-06, 1.9073e-06, 1.6093e-06,\n",
            "        2.3842e-06, 2.1458e-06, 1.9073e-06, 2.3246e-06, 2.8014e-06, 3.4571e-06,\n",
            "        2.5630e-06, 1.7285e-06, 1.9073e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.9670e-06, 1.4305e-06, 1.6093e-06, 1.7881e-06, 1.9073e-06, 1.7285e-06,\n",
            "        2.0266e-06, 2.4438e-06, 2.3246e-06, 1.5497e-06, 1.9670e-06, 2.1458e-06,\n",
            "        2.0862e-06, 2.0862e-06, 2.0862e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.0862e-06, 1.5497e-06, 1.4901e-06, 1.7881e-06, 1.9670e-06, 1.7285e-06,\n",
            "        1.8477e-06, 2.3842e-06, 2.1458e-06, 1.4901e-06, 1.7881e-06, 2.3246e-06,\n",
            "        1.5497e-06, 1.9073e-06, 2.2650e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.7285e-06, 1.6689e-06, 1.4901e-06, 1.8477e-06, 1.8477e-06, 1.6689e-06,\n",
            "        1.7285e-06, 1.9073e-06, 1.9670e-06, 1.5497e-06, 1.9073e-06, 2.4438e-06,\n",
            "        1.7881e-06, 2.0266e-06, 1.6093e-06, 1.5497e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.6093e-06, 1.4305e-06, 1.6093e-06, 1.7285e-06, 1.5497e-06, 1.6689e-06,\n",
            "        1.7285e-06, 2.2650e-06, 1.7881e-06, 1.6093e-06, 1.9073e-06, 1.9073e-06,\n",
            "        1.7881e-06, 1.6689e-06, 1.7285e-06, 1.6093e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.7285e-06, 1.4305e-06, 1.7285e-06, 1.6689e-06, 1.5497e-06, 1.6689e-06,\n",
            "        1.7285e-06, 2.0266e-06, 1.7285e-06, 1.6689e-06, 1.9073e-06, 1.6689e-06,\n",
            "        1.7881e-06, 1.6689e-06, 1.7881e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.6093e-06, 1.4305e-06, 1.7285e-06, 1.6689e-06, 1.5497e-06, 1.6689e-06,\n",
            "        1.5497e-06, 1.7285e-06, 1.7881e-06, 1.6689e-06, 1.7285e-06, 1.8477e-06,\n",
            "        1.7881e-06, 1.6689e-06, 1.6093e-06, 1.6689e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.6689e-06, 1.4305e-06, 1.2517e-06, 1.6689e-06, 1.5497e-06, 1.7881e-06,\n",
            "        1.6093e-06, 1.7881e-06, 1.8477e-06, 1.6689e-06, 1.7881e-06, 1.9073e-06,\n",
            "        1.7881e-06, 1.6689e-06, 1.5497e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.6689e-06, 1.3113e-06, 1.2517e-06, 1.6689e-06, 1.5497e-06, 1.7881e-06,\n",
            "        1.4901e-06, 1.8477e-06, 1.7285e-06, 1.6689e-06, 1.7881e-06, 2.0266e-06,\n",
            "        1.6689e-06, 1.6689e-06, 1.3709e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.6689e-06, 1.3113e-06, 1.2517e-06, 1.6689e-06, 1.3709e-06, 1.6689e-06,\n",
            "        1.5497e-06, 1.9073e-06, 1.7881e-06, 1.6689e-06, 1.6689e-06, 2.0862e-06,\n",
            "        1.8477e-06, 1.6689e-06, 1.3709e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.7881e-06, 1.3113e-06, 1.2517e-06, 1.6689e-06, 1.3709e-06, 1.6689e-06,\n",
            "        1.6093e-06, 2.0266e-06, 1.7881e-06, 1.6689e-06, 1.6689e-06, 1.6689e-06,\n",
            "        1.6689e-06, 1.6689e-06, 1.3709e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.6689e-06, 1.3113e-06, 1.2517e-06, 1.6689e-06, 1.3113e-06, 1.6689e-06,\n",
            "        1.6689e-06, 2.0862e-06, 1.9670e-06, 1.6689e-06, 1.7285e-06, 1.8477e-06,\n",
            "        1.6689e-06, 1.6689e-06, 1.3709e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.6689e-06, 1.3113e-06, 1.2517e-06, 1.6689e-06, 1.3113e-06, 1.6689e-06,\n",
            "        1.6093e-06, 1.7881e-06, 2.0266e-06, 1.6689e-06, 1.7285e-06, 1.8477e-06,\n",
            "        1.6689e-06, 1.6689e-06, 1.3709e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.6689e-06, 1.3113e-06, 1.2517e-06, 1.6689e-06, 1.3113e-06, 1.6689e-06,\n",
            "        1.6093e-06, 1.9670e-06, 2.0862e-06, 1.6689e-06, 1.7881e-06, 1.8477e-06,\n",
            "        1.6689e-06, 1.6689e-06, 1.3709e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.6093e-06, 1.3113e-06, 1.2517e-06, 1.6689e-06, 1.3113e-06, 1.6689e-06,\n",
            "        1.6689e-06, 1.7881e-06, 2.0862e-06, 1.6689e-06, 1.6689e-06, 1.8477e-06,\n",
            "        1.6689e-06, 1.6689e-06, 1.3709e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.6093e-06, 1.3113e-06, 1.2517e-06, 1.6689e-06, 1.3113e-06, 1.6689e-06,\n",
            "        1.6689e-06, 1.7881e-06, 1.6689e-06, 1.6689e-06, 1.6689e-06, 1.8477e-06,\n",
            "        1.8477e-06, 1.6689e-06, 1.3709e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.7285e-06, 1.3113e-06, 1.2517e-06, 1.6689e-06, 1.3113e-06, 1.6689e-06,\n",
            "        1.8477e-06, 1.7881e-06, 1.8477e-06, 1.6689e-06, 1.7285e-06, 1.8477e-06,\n",
            "        1.7881e-06, 1.6689e-06, 1.3709e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.6093e-06, 1.3113e-06, 1.3113e-06, 1.7881e-06, 1.3709e-06, 1.6689e-06,\n",
            "        1.8477e-06, 1.7881e-06, 1.8477e-06, 1.6689e-06, 1.6093e-06, 1.6689e-06,\n",
            "        1.6689e-06, 1.7881e-06, 1.3709e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.6093e-06, 1.3113e-06, 1.3113e-06, 1.7881e-06, 1.3709e-06, 1.6689e-06,\n",
            "        1.8477e-06, 1.9670e-06, 1.8477e-06, 1.6689e-06, 1.6093e-06, 1.6689e-06,\n",
            "        1.6689e-06, 1.7881e-06, 1.4305e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.0443e-04, 1.0329e-04, 9.6560e-05, 1.0586e-04, 9.9182e-05, 9.7215e-05,\n",
            "        9.8825e-05, 1.1134e-04, 1.0735e-04, 9.9063e-05, 1.0818e-04, 1.0872e-04,\n",
            "        1.0747e-04, 1.0574e-04, 1.0157e-04, 1.0806e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([9.9063e-05, 9.9480e-05, 1.0127e-04, 1.0419e-04, 9.9421e-05, 1.0145e-04,\n",
            "        9.7454e-05, 1.0294e-04, 1.0049e-04, 1.0103e-04, 1.0890e-04, 1.0878e-04,\n",
            "        1.0347e-04, 1.0622e-04, 1.0115e-04, 1.0449e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([9.9361e-05, 9.9897e-05, 1.0037e-04, 1.0157e-04, 1.0169e-04, 1.0139e-04,\n",
            "        9.8825e-05, 1.0431e-04, 1.0157e-04, 1.0073e-04, 1.1021e-04, 1.0622e-04,\n",
            "        1.0478e-04, 1.0532e-04, 1.0163e-04, 1.0562e-04], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([9.0122e-05, 8.9765e-05, 9.0241e-05, 9.0957e-05, 9.1195e-05, 8.9467e-05,\n",
            "        8.9645e-05, 1.0020e-04, 8.9586e-05, 9.0480e-05, 9.6202e-05, 9.0837e-05,\n",
            "        9.5069e-05, 9.3222e-05, 9.1255e-05, 8.9347e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([8.9824e-05, 9.0659e-05, 8.9824e-05, 9.0182e-05, 9.1195e-05, 8.9884e-05,\n",
            "        8.9884e-05, 1.0067e-04, 8.9824e-05, 8.9884e-05, 9.1553e-05, 9.0659e-05,\n",
            "        9.5069e-05, 9.3699e-05, 8.9824e-05, 8.9288e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([8.9824e-05, 9.0241e-05, 8.9824e-05, 9.1314e-05, 8.9824e-05, 8.9884e-05,\n",
            "        8.9824e-05, 8.9943e-05, 8.9824e-05, 8.9884e-05, 9.1553e-05, 9.1553e-05,\n",
            "        8.9824e-05, 9.0539e-05, 8.9824e-05, 8.9407e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([9.0063e-05, 9.0063e-05, 9.0063e-05, 8.9645e-05, 9.0063e-05, 9.0241e-05,\n",
            "        9.0063e-05, 8.6904e-05, 9.0063e-05, 9.0241e-05, 8.9586e-05, 8.9586e-05,\n",
            "        9.0063e-05, 9.0659e-05, 9.0063e-05, 9.0659e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([8.9467e-05, 9.0063e-05, 9.0063e-05, 8.9467e-05, 9.0063e-05, 9.0241e-05,\n",
            "        9.0063e-05, 8.6844e-05, 9.0063e-05, 9.0241e-05, 8.9586e-05, 8.9586e-05,\n",
            "        9.0063e-05, 9.0659e-05, 8.9467e-05, 9.0063e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([8.9467e-05, 9.0063e-05, 8.9467e-05, 8.9467e-05, 9.0063e-05, 9.0241e-05,\n",
            "        8.9467e-05, 8.6844e-05, 8.6665e-05, 9.0241e-05, 8.9586e-05, 8.6188e-05,\n",
            "        8.6665e-05, 9.0659e-05, 8.9467e-05, 8.6188e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6665e-05, 8.6844e-05,\n",
            "        8.6188e-05, 8.6844e-05, 8.6665e-05, 8.6844e-05, 8.6308e-05, 8.6188e-05,\n",
            "        8.6188e-05, 8.7082e-05, 8.6188e-05, 8.6188e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6665e-05, 8.6308e-05,\n",
            "        8.6188e-05, 8.6308e-05, 8.6188e-05, 8.6308e-05, 8.6188e-05, 8.6188e-05,\n",
            "        8.6188e-05, 8.6784e-05, 8.6188e-05, 8.6188e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6308e-05,\n",
            "        8.6188e-05, 8.6308e-05, 8.6188e-05, 8.6308e-05, 8.6188e-05, 8.6188e-05,\n",
            "        8.6188e-05, 8.6784e-05, 8.6188e-05, 8.6188e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6308e-05,\n",
            "        8.6188e-05, 8.6188e-05, 8.6188e-05, 8.6308e-05, 8.6188e-05, 8.6188e-05,\n",
            "        8.6188e-05, 8.6784e-05, 8.6188e-05, 8.6188e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([8.5592e-05, 8.5592e-05, 8.5592e-05, 8.6188e-05, 8.6188e-05, 8.5592e-05,\n",
            "        8.5592e-05, 8.6188e-05, 8.6188e-05, 8.5711e-05, 8.6188e-05, 8.6188e-05,\n",
            "        8.6188e-05, 8.6784e-05, 8.6188e-05, 8.5592e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([8.5592e-05, 8.5592e-05, 8.5592e-05, 8.5592e-05, 8.5592e-05, 8.5592e-05,\n",
            "        8.5592e-05, 8.5592e-05, 8.5592e-05, 8.5711e-05, 8.6188e-05, 8.6188e-05,\n",
            "        8.6188e-05, 8.6784e-05, 8.5592e-05, 8.5592e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([8.5592e-05, 8.5592e-05, 8.4996e-05, 8.5592e-05, 8.5592e-05, 8.5592e-05,\n",
            "        8.5592e-05, 8.5592e-05, 8.5592e-05, 8.5711e-05, 8.6188e-05, 8.5592e-05,\n",
            "        8.5592e-05, 8.5592e-05, 8.5592e-05, 8.5592e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([8.5592e-05, 8.5592e-05, 8.4996e-05, 8.5592e-05, 8.5592e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.5592e-05, 8.5592e-05, 8.5115e-05, 8.5592e-05, 8.5592e-05,\n",
            "        8.5592e-05, 8.5592e-05, 8.4996e-05, 8.5592e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([8.4996e-05, 8.5592e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.5592e-05, 8.5592e-05, 8.5115e-05, 8.5592e-05, 8.5592e-05,\n",
            "        8.5592e-05, 8.5592e-05, 8.4996e-05, 8.5592e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([8.4996e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.5592e-05, 8.4996e-05, 8.5115e-05, 8.5592e-05, 8.4996e-05,\n",
            "        8.5592e-05, 8.5592e-05, 8.4996e-05, 8.5592e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([8.4996e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05, 8.4996e-05,\n",
            "        8.4996e-05, 8.4996e-05, 8.4996e-05, 8.5115e-05, 8.5592e-05, 8.4996e-05,\n",
            "        8.5592e-05, 8.5592e-05, 8.4996e-05, 8.5592e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.7014e-05, 3.3796e-05, 3.4034e-05, 3.6538e-05, 2.9445e-05, 3.2365e-05,\n",
            "        3.3379e-05, 3.0756e-05, 3.0756e-05, 3.3617e-05, 3.3855e-05, 3.3557e-05,\n",
            "        3.3140e-05, 3.6776e-05, 3.2127e-05, 3.2663e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.4750e-05, 2.6584e-05, 3.4571e-05, 3.3796e-05, 3.0935e-05, 2.9564e-05,\n",
            "        3.3498e-05, 3.1650e-05, 3.1054e-05, 3.3617e-05, 3.3140e-05, 3.5524e-05,\n",
            "        3.4034e-05, 3.7253e-05, 3.2604e-05, 3.4451e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([3.4332e-05, 2.7359e-05, 3.1292e-05, 3.0994e-05, 3.3557e-05, 3.0696e-05,\n",
            "        3.0696e-05, 3.1710e-05, 3.1710e-05, 3.2961e-05, 2.7597e-05, 2.7955e-05,\n",
            "        3.1173e-05, 3.4630e-05, 3.0816e-05, 3.2723e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.1769e-05, 2.8491e-05, 3.1173e-05, 3.1412e-05, 3.1233e-05, 3.1531e-05,\n",
            "        3.0935e-05, 3.1948e-05, 3.1769e-05, 3.3200e-05, 2.8074e-05, 2.6822e-05,\n",
            "        3.2723e-05, 3.4690e-05, 3.1292e-05, 3.2961e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.1710e-05, 2.9743e-05, 3.1173e-05, 3.1292e-05, 3.1590e-05, 3.1948e-05,\n",
            "        3.1710e-05, 3.1590e-05, 3.1590e-05, 3.2961e-05, 2.8551e-05, 2.5570e-05,\n",
            "        3.2783e-05, 3.4750e-05, 3.1769e-05, 3.2961e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.1471e-05, 3.1352e-05, 3.1471e-05, 3.1590e-05, 3.1590e-05, 3.1948e-05,\n",
            "        3.1590e-05, 3.1590e-05, 3.1292e-05, 3.3319e-05, 2.9087e-05, 2.5690e-05,\n",
            "        3.1292e-05, 3.2902e-05, 3.1769e-05, 3.3617e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.8312e-05, 2.9624e-05, 2.7955e-05, 2.7955e-05, 2.8133e-05, 2.7955e-05,\n",
            "        2.8849e-05, 2.8133e-05, 2.8729e-05, 2.9624e-05, 2.8074e-05, 2.5868e-05,\n",
            "        2.9445e-05, 2.9445e-05, 2.8133e-05, 2.8491e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.8312e-05, 2.9624e-05, 2.7955e-05, 2.7955e-05, 2.8133e-05, 2.7955e-05,\n",
            "        2.8729e-05, 2.8133e-05, 2.8729e-05, 2.8908e-05, 2.8372e-05, 2.6107e-05,\n",
            "        2.9445e-05, 2.8729e-05, 2.8133e-05, 2.8491e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.7955e-05, 2.9445e-05, 2.8312e-05, 2.8312e-05, 2.8133e-05, 2.7955e-05,\n",
            "        2.7955e-05, 2.8551e-05, 2.7955e-05, 2.7955e-05, 2.8729e-05, 2.6464e-05,\n",
            "        2.8312e-05, 2.8729e-05, 2.8133e-05, 3.0458e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.7955e-05, 2.9445e-05, 2.8312e-05, 2.8312e-05, 2.8551e-05, 2.7955e-05,\n",
            "        2.7955e-05, 2.9624e-05, 2.8312e-05, 2.7955e-05, 2.8729e-05, 2.6822e-05,\n",
            "        2.7955e-05, 2.8729e-05, 2.8312e-05, 2.9445e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.7955e-05, 2.7955e-05, 2.7955e-05, 2.8312e-05, 2.8551e-05, 2.7955e-05,\n",
            "        2.8312e-05, 2.9624e-05, 2.8312e-05, 2.7955e-05, 2.9445e-05, 2.7537e-05,\n",
            "        2.8312e-05, 2.8729e-05, 2.8312e-05, 2.8312e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.7955e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05, 2.8551e-05, 2.7955e-05,\n",
            "        2.8312e-05, 2.8133e-05, 2.7955e-05, 2.7955e-05, 2.9445e-05, 2.9266e-05,\n",
            "        2.7955e-05, 2.8729e-05, 2.8312e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.7955e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05, 2.8133e-05, 2.8312e-05,\n",
            "        2.8312e-05, 2.8133e-05, 2.7955e-05, 2.8312e-05, 2.7955e-05, 3.0100e-05,\n",
            "        2.7955e-05, 2.8729e-05, 2.7955e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.8312e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05, 2.8133e-05, 2.8312e-05,\n",
            "        2.7955e-05, 2.8133e-05, 2.7955e-05, 2.8312e-05, 2.7955e-05, 2.9683e-05,\n",
            "        2.7955e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.8312e-05, 2.8312e-05, 2.8312e-05, 2.7955e-05, 2.8133e-05, 2.8312e-05,\n",
            "        2.7955e-05, 2.7955e-05, 2.7955e-05, 2.8312e-05, 2.7955e-05, 2.9922e-05,\n",
            "        2.8312e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.8312e-05, 2.8312e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05, 2.8312e-05,\n",
            "        2.7955e-05, 2.7955e-05, 2.8312e-05, 2.7955e-05, 2.8312e-05, 2.9445e-05,\n",
            "        2.7955e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.7955e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05,\n",
            "        2.7955e-05, 2.7955e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05, 3.0279e-05,\n",
            "        2.8312e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.7955e-05, 2.7955e-05, 2.7955e-05, 2.8312e-05, 2.8312e-05, 2.7955e-05,\n",
            "        2.7955e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05, 3.0279e-05,\n",
            "        2.7955e-05, 2.7955e-05, 2.8312e-05, 2.8312e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.7955e-05, 2.7955e-05, 2.7955e-05, 2.8312e-05, 2.8312e-05, 2.7955e-05,\n",
            "        2.8312e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05, 2.8312e-05, 2.9564e-05,\n",
            "        2.7955e-05, 2.7955e-05, 2.8312e-05, 2.8312e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.7955e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05,\n",
            "        2.8312e-05, 2.8312e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05, 2.9564e-05,\n",
            "        2.7955e-05, 2.7955e-05, 2.7955e-05, 2.7955e-05], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.5167e-06, 2.1458e-06, 3.6955e-06, 4.8280e-06, 1.9073e-06, 3.5167e-06,\n",
            "        2.0862e-06, 1.7881e-06, 2.0862e-06, 2.3246e-06, 4.4107e-06, 1.5497e-06,\n",
            "        2.2650e-06, 3.6955e-06, 3.5167e-06, 1.8477e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.9935e-06, 2.2054e-06, 3.3975e-06, 3.3379e-06, 2.1458e-06, 3.3379e-06,\n",
            "        1.3113e-06, 1.7285e-06, 1.9670e-06, 1.8477e-06, 2.6226e-06, 1.7881e-06,\n",
            "        2.1458e-06, 2.9206e-06, 3.3975e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.0862e-06, 2.2054e-06, 4.4107e-06, 2.6822e-06, 1.5497e-06, 1.6689e-06,\n",
            "        1.5497e-06, 1.7881e-06, 1.9670e-06, 1.8477e-06, 2.5630e-06, 1.9670e-06,\n",
            "        2.2650e-06, 3.4571e-06, 3.3379e-06, 1.7285e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.0266e-06, 2.5630e-06, 2.6226e-06, 1.9670e-06, 1.5497e-06, 1.6689e-06,\n",
            "        1.7285e-06, 1.6689e-06, 1.9073e-06, 1.4305e-06, 2.8014e-06, 1.9670e-06,\n",
            "        1.8477e-06, 2.2054e-06, 2.1458e-06, 1.7881e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.0266e-06, 1.6689e-06, 2.0862e-06, 1.6093e-06, 1.7881e-06, 1.4901e-06,\n",
            "        1.9670e-06, 1.6689e-06, 1.7285e-06, 1.4305e-06, 2.6822e-06, 1.9670e-06,\n",
            "        1.6689e-06, 1.9670e-06, 2.1458e-06, 1.9073e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.6689e-06, 1.6689e-06, 2.0266e-06, 1.6689e-06, 1.6689e-06, 1.4305e-06,\n",
            "        1.9670e-06, 1.6093e-06, 1.6689e-06, 1.4305e-06, 2.2650e-06, 1.6093e-06,\n",
            "        1.6689e-06, 1.9670e-06, 1.4901e-06, 1.9670e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.8477e-06, 1.6689e-06, 2.0266e-06, 1.6093e-06, 1.6093e-06, 1.4305e-06,\n",
            "        1.9670e-06, 2.2054e-06, 1.7285e-06, 1.4901e-06, 2.0266e-06, 1.6689e-06,\n",
            "        1.7285e-06, 1.9670e-06, 1.3709e-06, 2.0862e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.9073e-06, 1.9670e-06, 2.0862e-06, 1.7881e-06, 1.6093e-06, 1.4901e-06,\n",
            "        1.9670e-06, 1.3113e-06, 1.8477e-06, 1.5497e-06, 2.0862e-06, 1.9073e-06,\n",
            "        1.7285e-06, 1.9670e-06, 1.3709e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.9073e-06, 1.9670e-06, 2.2054e-06, 1.9073e-06, 1.5497e-06, 1.4901e-06,\n",
            "        1.9670e-06, 1.3709e-06, 1.7881e-06, 1.9073e-06, 2.1458e-06, 1.7881e-06,\n",
            "        1.9670e-06, 2.0862e-06, 1.4901e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.9073e-06, 1.9670e-06, 2.3246e-06, 2.0862e-06, 1.5497e-06, 1.6689e-06,\n",
            "        1.7881e-06, 1.4305e-06, 1.8477e-06, 1.9073e-06, 2.0862e-06, 1.5497e-06,\n",
            "        1.9670e-06, 2.0862e-06, 1.4901e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.9073e-06, 1.9670e-06, 2.2054e-06, 2.1458e-06, 1.6093e-06, 1.7285e-06,\n",
            "        1.8477e-06, 1.3709e-06, 1.4901e-06, 1.9073e-06, 2.0862e-06, 1.4305e-06,\n",
            "        1.9670e-06, 1.8477e-06, 1.4901e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.9073e-06, 1.9670e-06, 2.0266e-06, 1.6689e-06, 1.5497e-06, 1.8477e-06,\n",
            "        1.8477e-06, 1.3709e-06, 1.4305e-06, 1.9073e-06, 1.9073e-06, 1.4305e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.4901e-06, 1.2517e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.7285e-06, 1.9670e-06, 2.0266e-06, 1.6689e-06, 1.6093e-06, 1.9073e-06,\n",
            "        1.8477e-06, 1.4901e-06, 1.4305e-06, 1.9073e-06, 1.9073e-06, 1.4305e-06,\n",
            "        1.9670e-06, 1.7881e-06, 1.3709e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.7285e-06, 1.9073e-06, 1.7881e-06, 1.6689e-06, 1.6689e-06, 1.9073e-06,\n",
            "        1.8477e-06, 1.4901e-06, 1.4305e-06, 1.9670e-06, 1.9073e-06, 1.4305e-06,\n",
            "        2.0862e-06, 1.9670e-06, 1.3709e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.7285e-06, 2.1458e-06, 1.7881e-06, 1.6689e-06, 1.7881e-06, 1.9073e-06,\n",
            "        1.8477e-06, 1.3709e-06, 1.4305e-06, 1.9670e-06, 1.7881e-06, 1.4305e-06,\n",
            "        2.0862e-06, 1.9670e-06, 1.3709e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.7285e-06, 2.0862e-06, 1.7881e-06, 1.6689e-06, 1.7881e-06, 1.9073e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.5497e-06, 1.9670e-06, 1.7881e-06, 1.4305e-06,\n",
            "        2.0862e-06, 1.7881e-06, 1.3709e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.7285e-06, 1.7285e-06, 1.7881e-06, 1.6689e-06, 1.7881e-06, 1.9073e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.4305e-06, 2.0266e-06, 1.7881e-06, 1.4305e-06,\n",
            "        1.7285e-06, 1.7881e-06, 1.3709e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.7285e-06, 1.9073e-06, 1.7881e-06, 1.6689e-06, 1.7881e-06, 1.9073e-06,\n",
            "        1.6689e-06, 1.4901e-06, 1.4901e-06, 2.0862e-06, 1.6689e-06, 1.3709e-06,\n",
            "        1.6093e-06, 1.8477e-06, 1.3709e-06, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.7285e-06, 1.9073e-06, 1.6689e-06, 1.7881e-06, 1.7881e-06, 1.9670e-06,\n",
            "        1.6689e-06, 1.4901e-06, 1.4901e-06, 2.0862e-06, 1.6689e-06, 1.4305e-06,\n",
            "        1.6093e-06, 1.7881e-06, 1.3709e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.7285e-06, 1.9073e-06, 1.6689e-06, 1.7881e-06, 1.8477e-06, 1.7881e-06,\n",
            "        1.6689e-06, 1.4305e-06, 1.4305e-06, 1.7881e-06, 1.6689e-06, 1.4305e-06,\n",
            "        1.6093e-06, 1.7881e-06, 1.3709e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.9073e-06, 1.4901e-06, 2.3842e-06, 1.2517e-06, 1.0133e-06, 1.6689e-06,\n",
            "        2.0266e-06, 1.4901e-06, 1.5497e-06, 1.9073e-06, 2.2054e-06, 2.5630e-06,\n",
            "        3.9339e-06, 1.1325e-06, 2.0266e-06, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.9073e-06, 9.5367e-07, 1.9073e-06, 1.3709e-06, 1.2517e-06, 9.5367e-07,\n",
            "        2.8610e-06, 2.0862e-06, 1.5497e-06, 1.9670e-06, 1.9670e-06, 2.1458e-06,\n",
            "        1.3113e-06, 1.5497e-06, 2.3246e-06, 1.2517e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.7881e-06, 9.5367e-07, 1.8477e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06,\n",
            "        2.8610e-06, 2.3842e-06, 1.1325e-06, 1.4305e-06, 1.7285e-06, 2.0862e-06,\n",
            "        1.4305e-06, 1.1921e-06, 3.1590e-06, 1.1921e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.8477e-06, 1.1325e-06, 1.9670e-06, 1.3709e-06, 1.3113e-06, 1.6093e-06,\n",
            "        3.0994e-06, 2.2650e-06, 1.3709e-06, 1.9073e-06, 1.2517e-06, 1.9670e-06,\n",
            "        8.9407e-07, 1.3113e-06, 2.3842e-06, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.7285e-06, 1.1325e-06, 1.4901e-06, 1.4305e-06, 1.0133e-06, 1.0133e-06,\n",
            "        8.9407e-07, 2.0266e-06, 1.1921e-06, 2.1458e-06, 9.5367e-07, 2.1458e-06,\n",
            "        1.0133e-06, 1.0133e-06, 1.3113e-06, 1.1921e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.6689e-06, 1.0729e-06, 1.5497e-06, 1.0729e-06, 1.0133e-06, 1.0133e-06,\n",
            "        8.9407e-07, 1.8477e-06, 1.0729e-06, 1.4305e-06, 1.0729e-06, 2.0862e-06,\n",
            "        1.0133e-06, 1.0133e-06, 1.0133e-06, 1.1325e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.2517e-06, 1.0729e-06, 1.3113e-06, 1.0729e-06, 1.0729e-06, 8.9407e-07,\n",
            "        8.3447e-07, 1.7881e-06, 1.0729e-06, 1.4901e-06, 1.0133e-06, 1.5497e-06,\n",
            "        1.0133e-06, 1.0133e-06, 8.9407e-07, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.3113e-06, 1.0729e-06, 1.3113e-06, 1.0729e-06, 1.0729e-06, 9.5367e-07,\n",
            "        7.7486e-07, 1.4901e-06, 1.0133e-06, 1.4305e-06, 1.0133e-06, 1.3113e-06,\n",
            "        1.0133e-06, 1.0133e-06, 8.9407e-07, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.3113e-06, 1.0729e-06, 1.3113e-06, 8.9407e-07, 1.0729e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.4305e-06, 1.0729e-06, 1.2517e-06, 1.0133e-06, 1.3113e-06,\n",
            "        9.5367e-07, 1.0133e-06, 8.3447e-07, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.3113e-06, 1.0729e-06, 1.3113e-06, 8.9407e-07, 1.0729e-06, 1.0133e-06,\n",
            "        7.1526e-07, 1.4305e-06, 1.0729e-06, 1.2517e-06, 1.0729e-06, 1.2517e-06,\n",
            "        8.3447e-07, 1.0133e-06, 8.3447e-07, 1.1325e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.3113e-06, 1.0729e-06, 1.3113e-06, 8.9407e-07, 1.0729e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.3709e-06, 1.1325e-06, 1.1921e-06, 1.0729e-06, 1.1921e-06,\n",
            "        8.3447e-07, 1.0133e-06, 8.9407e-07, 1.1325e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.3113e-06, 1.0133e-06, 1.1921e-06, 8.9407e-07, 1.1325e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.3709e-06, 1.1325e-06, 1.1921e-06, 1.0729e-06, 1.1921e-06,\n",
            "        7.7486e-07, 1.0133e-06, 7.7486e-07, 1.1325e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.3113e-06, 1.1325e-06, 1.1921e-06, 8.9407e-07, 1.2517e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.1325e-06, 1.1921e-06,\n",
            "        7.7486e-07, 1.0133e-06, 7.7486e-07, 1.2517e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.1921e-06, 1.1325e-06, 1.1921e-06, 9.5367e-07, 1.3113e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.3113e-06, 1.1921e-06,\n",
            "        7.7486e-07, 1.0729e-06, 7.7486e-07, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.1921e-06, 1.1325e-06, 1.1921e-06, 9.5367e-07, 1.2517e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06, 1.1921e-06,\n",
            "        7.7486e-07, 1.0729e-06, 7.7486e-07, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.1921e-06, 1.1325e-06, 1.1921e-06, 9.5367e-07, 1.2517e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06, 1.1921e-06,\n",
            "        7.7486e-07, 1.1325e-06, 7.7486e-07, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.1921e-06, 1.1921e-06, 1.1921e-06, 7.7486e-07, 1.2517e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06, 1.1921e-06,\n",
            "        7.7486e-07, 1.1325e-06, 7.7486e-07, 1.3113e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.1921e-06, 1.1921e-06, 1.1921e-06, 8.3447e-07, 1.2517e-06, 1.0133e-06,\n",
            "        7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06, 1.1921e-06,\n",
            "        7.7486e-07, 1.1325e-06, 7.7486e-07, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.1921e-06, 1.1921e-06, 1.1921e-06, 8.3447e-07, 1.1921e-06, 1.0729e-06,\n",
            "        7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06, 1.1921e-06,\n",
            "        7.7486e-07, 1.1325e-06, 7.7486e-07, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.1921e-06, 1.1921e-06, 1.1921e-06, 8.3447e-07, 1.1921e-06, 1.1325e-06,\n",
            "        7.7486e-07, 1.2517e-06, 1.1921e-06, 1.1921e-06, 1.2517e-06, 1.1921e-06,\n",
            "        7.7486e-07, 1.1921e-06, 7.7486e-07, 1.3709e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([2.0266e-06, 1.0729e-06, 1.1921e-06, 2.9802e-06, 1.1921e-06, 1.4901e-06,\n",
            "        1.0133e-06, 1.0133e-06, 1.1921e-06, 1.4305e-06, 1.6093e-06, 7.7486e-07,\n",
            "        1.5497e-06, 1.6689e-06, 1.6093e-06, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.6689e-06, 8.9407e-07, 1.6093e-06, 4.6492e-06, 7.1526e-07, 6.5565e-07,\n",
            "        6.5565e-07, 1.1325e-06, 8.3447e-07, 1.6093e-06, 1.3113e-06, 1.0729e-06,\n",
            "        1.4305e-06, 2.1458e-06, 1.6689e-06, 6.5565e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([1.6689e-06, 1.0729e-06, 1.3709e-06, 1.6093e-06, 8.3447e-07, 7.7486e-07,\n",
            "        8.9407e-07, 1.1921e-06, 1.0729e-06, 1.4305e-06, 1.3709e-06, 1.1325e-06,\n",
            "        1.7285e-06, 2.6226e-06, 1.6093e-06, 8.3447e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([1.9073e-06, 7.1526e-07, 7.7486e-07, 1.1325e-06, 8.3447e-07, 9.5367e-07,\n",
            "        8.3447e-07, 1.1921e-06, 1.2517e-06, 1.3709e-06, 1.3113e-06, 1.0133e-06,\n",
            "        9.5367e-07, 1.3709e-06, 2.0266e-06, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.3113e-06, 5.9605e-07, 6.5565e-07, 1.1325e-06, 6.5565e-07, 8.9407e-07,\n",
            "        1.0133e-06, 1.1921e-06, 7.1526e-07, 1.4305e-06, 6.5565e-07, 7.7486e-07,\n",
            "        7.7486e-07, 7.7486e-07, 1.3709e-06, 9.5367e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.2517e-06, 5.9605e-07, 6.5565e-07, 1.0133e-06, 6.5565e-07, 8.9407e-07,\n",
            "        1.0729e-06, 1.1921e-06, 7.7486e-07, 1.0729e-06, 6.5565e-07, 7.7486e-07,\n",
            "        6.5565e-07, 7.7486e-07, 9.5367e-07, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.0729e-06, 5.9605e-07, 6.5565e-07, 7.1526e-07, 7.1526e-07, 8.3447e-07,\n",
            "        1.0133e-06, 9.5367e-07, 7.7486e-07, 1.0133e-06, 6.5565e-07, 7.7486e-07,\n",
            "        6.5565e-07, 5.9605e-07, 9.5367e-07, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.0729e-06, 5.9605e-07, 7.1526e-07, 6.5565e-07, 7.1526e-07, 8.3447e-07,\n",
            "        8.3447e-07, 8.3447e-07, 8.9407e-07, 1.0133e-06, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 9.5367e-07, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([8.9407e-07, 5.9605e-07, 7.1526e-07, 6.5565e-07, 7.1526e-07, 8.9407e-07,\n",
            "        7.7486e-07, 7.1526e-07, 9.5367e-07, 1.0729e-06, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 9.5367e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([9.5367e-07, 5.9605e-07, 7.1526e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07,\n",
            "        8.3447e-07, 7.1526e-07, 7.7486e-07, 1.0729e-06, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 8.3447e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([9.5367e-07, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 7.7486e-07, 1.0729e-06, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 7.1526e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([9.5367e-07, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 8.3447e-07, 1.0133e-06, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.0729e-06, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 8.3447e-07, 9.5367e-07, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.0729e-06, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 8.3447e-07, 9.5367e-07, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.0729e-06, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 8.3447e-07, 9.5367e-07, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.0729e-06, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 8.9407e-07, 9.5367e-07, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.0729e-06, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 8.9407e-07, 8.9407e-07, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.0729e-06, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 9.5367e-07, 8.3447e-07, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.0133e-06, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 9.5367e-07, 8.3447e-07, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.0133e-06, 5.9605e-07, 7.1526e-07, 5.3644e-07, 6.5565e-07, 6.5565e-07,\n",
            "        8.3447e-07, 7.1526e-07, 9.5367e-07, 8.3447e-07, 5.3644e-07, 6.5565e-07,\n",
            "        6.5565e-07, 5.9605e-07, 6.5565e-07, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([1.6093e-06, 1.0133e-06, 2.6226e-06, 2.6822e-06, 1.7285e-06, 2.1458e-06,\n",
            "        5.9605e-07, 1.1921e-06, 1.6689e-06, 1.1325e-06, 2.3246e-06, 1.7881e-06,\n",
            "        2.0862e-06, 1.1325e-06, 1.3113e-06, 1.4305e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([1.6689e-06, 1.1921e-06, 1.3113e-06, 4.2915e-06, 8.9407e-07, 1.3709e-06,\n",
            "        7.7486e-07, 1.6093e-06, 1.4305e-06, 8.3447e-07, 1.1325e-06, 2.8014e-06,\n",
            "        2.2650e-06, 1.7881e-06, 1.0729e-06, 1.4901e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.2650e-06, 1.6689e-06, 1.4901e-06, 2.7418e-06, 1.1921e-06, 1.3709e-06,\n",
            "        1.0729e-06, 9.5367e-07, 1.1325e-06, 8.9407e-07, 1.3113e-06, 2.9802e-06,\n",
            "        2.5034e-06, 1.8477e-06, 1.2517e-06, 9.5367e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.3842e-06, 1.2517e-06, 1.0133e-06, 1.0133e-06, 1.0133e-06, 1.6093e-06,\n",
            "        1.3113e-06, 1.1325e-06, 1.2517e-06, 8.3447e-07, 7.1526e-07, 1.4901e-06,\n",
            "        1.6689e-06, 1.1921e-06, 1.5497e-06, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([1.2517e-06, 8.3447e-07, 1.0729e-06, 7.1526e-07, 1.1325e-06, 1.5497e-06,\n",
            "        1.1325e-06, 1.0133e-06, 1.1921e-06, 9.5367e-07, 7.1526e-07, 1.4901e-06,\n",
            "        1.2517e-06, 1.2517e-06, 1.8477e-06, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([1.0729e-06, 8.9407e-07, 8.9407e-07, 7.1526e-07, 1.1921e-06, 1.2517e-06,\n",
            "        1.1325e-06, 1.3709e-06, 1.1921e-06, 9.5367e-07, 7.1526e-07, 1.3709e-06,\n",
            "        1.1325e-06, 1.2517e-06, 1.5497e-06, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([1.1921e-06, 9.5367e-07, 8.9407e-07, 7.1526e-07, 1.0133e-06, 8.3447e-07,\n",
            "        1.0133e-06, 1.3113e-06, 1.2517e-06, 9.5367e-07, 7.1526e-07, 1.3709e-06,\n",
            "        1.1325e-06, 1.1921e-06, 1.4901e-06, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([1.2517e-06, 9.5367e-07, 6.5565e-07, 6.5565e-07, 1.0133e-06, 8.3447e-07,\n",
            "        1.0133e-06, 1.1325e-06, 1.2517e-06, 9.5367e-07, 7.7486e-07, 1.3113e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.3113e-06, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([1.1921e-06, 9.5367e-07, 6.5565e-07, 6.5565e-07, 9.5367e-07, 8.3447e-07,\n",
            "        1.0729e-06, 1.1325e-06, 1.2517e-06, 9.5367e-07, 8.3447e-07, 1.3113e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.2517e-06, 8.3447e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([1.3113e-06, 9.5367e-07, 6.5565e-07, 6.5565e-07, 9.5367e-07, 8.3447e-07,\n",
            "        1.0729e-06, 1.1325e-06, 1.2517e-06, 9.5367e-07, 8.3447e-07, 1.3113e-06,\n",
            "        1.0133e-06, 1.1921e-06, 1.2517e-06, 8.3447e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([1.3113e-06, 1.0133e-06, 6.5565e-07, 6.5565e-07, 1.0729e-06, 7.7486e-07,\n",
            "        1.0133e-06, 1.1325e-06, 1.3113e-06, 8.3447e-07, 8.3447e-07, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.2517e-06, 8.3447e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([1.3113e-06, 8.9407e-07, 6.5565e-07, 5.9605e-07, 1.0729e-06, 7.7486e-07,\n",
            "        8.9407e-07, 1.1325e-06, 1.3113e-06, 8.3447e-07, 7.7486e-07, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.0133e-06, 9.5367e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([1.0729e-06, 9.5367e-07, 6.5565e-07, 5.9605e-07, 1.0729e-06, 8.3447e-07,\n",
            "        8.9407e-07, 1.1325e-06, 1.3113e-06, 8.3447e-07, 7.7486e-07, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.0133e-06, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.0729e-06, 1.0133e-06, 6.5565e-07, 5.9605e-07, 1.0729e-06, 8.9407e-07,\n",
            "        8.9407e-07, 1.1325e-06, 1.3113e-06, 8.3447e-07, 7.7486e-07, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.0133e-06, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([1.0729e-06, 9.5367e-07, 6.5565e-07, 5.9605e-07, 1.0729e-06, 8.9407e-07,\n",
            "        8.9407e-07, 1.1325e-06, 1.1325e-06, 8.9407e-07, 8.3447e-07, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.0729e-06, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([1.0133e-06, 1.0729e-06, 6.5565e-07, 5.9605e-07, 1.0729e-06, 8.9407e-07,\n",
            "        8.9407e-07, 1.1325e-06, 1.1325e-06, 1.0133e-06, 8.3447e-07, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.0729e-06, 8.9407e-07], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([1.0133e-06, 1.0133e-06, 6.5565e-07, 5.9605e-07, 1.0729e-06, 8.9407e-07,\n",
            "        9.5367e-07, 1.1325e-06, 1.1325e-06, 1.0133e-06, 8.9407e-07, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.0729e-06, 1.0133e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([1.0133e-06, 1.0133e-06, 6.5565e-07, 5.9605e-07, 1.0729e-06, 8.9407e-07,\n",
            "        1.0729e-06, 1.1325e-06, 1.1325e-06, 9.5367e-07, 8.9407e-07, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.0729e-06, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([1.0133e-06, 1.0133e-06, 6.5565e-07, 5.9605e-07, 1.0729e-06, 8.9407e-07,\n",
            "        1.0729e-06, 1.1325e-06, 1.1325e-06, 1.0729e-06, 9.5367e-07, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.0729e-06, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([1.0133e-06, 1.0133e-06, 6.5565e-07, 5.9605e-07, 1.0729e-06, 8.9407e-07,\n",
            "        1.0729e-06, 1.1325e-06, 1.1325e-06, 1.0729e-06, 1.0729e-06, 1.0729e-06,\n",
            "        1.0729e-06, 1.1921e-06, 1.0729e-06, 1.0729e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.4703e-06, 6.2585e-06, 7.0930e-06, 3.8743e-06, 6.9737e-06, 5.9605e-06,\n",
            "        4.8876e-06, 3.2187e-06, 3.7551e-06, 5.0068e-06, 4.8876e-06, 5.4240e-06,\n",
            "        4.7088e-06, 4.8876e-06, 3.8743e-06, 3.6955e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([4.1723e-06, 5.7220e-06, 5.7220e-06, 4.4703e-06, 3.8147e-06, 4.4703e-06,\n",
            "        4.8280e-06, 3.6359e-06, 4.1127e-06, 6.6161e-06, 5.2452e-06, 4.5300e-06,\n",
            "        4.4107e-06, 4.1127e-06, 3.6955e-06, 3.5167e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.6492e-06, 5.8413e-06, 5.0068e-06, 3.8743e-06, 4.1723e-06, 4.4107e-06,\n",
            "        4.4703e-06, 3.6955e-06, 4.5896e-06, 3.0994e-06, 5.6624e-06, 4.5300e-06,\n",
            "        3.7551e-06, 4.5300e-06, 3.8147e-06, 3.8147e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.9935e-06, 4.4107e-06, 4.7088e-06, 4.1127e-06, 3.3379e-06, 3.6955e-06,\n",
            "        4.4107e-06, 3.8743e-06, 4.0531e-06, 3.0994e-06, 4.5896e-06, 3.9339e-06,\n",
            "        3.9935e-06, 3.8147e-06, 4.1723e-06, 3.9339e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.6359e-06, 3.5763e-06, 3.7551e-06, 4.1723e-06, 3.5167e-06, 3.6955e-06,\n",
            "        3.9935e-06, 3.9339e-06, 4.1723e-06, 3.0994e-06, 4.1723e-06, 4.2915e-06,\n",
            "        3.9935e-06, 3.9935e-06, 4.4107e-06, 4.2319e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.6359e-06, 3.6359e-06, 3.4571e-06, 4.4703e-06, 3.5763e-06, 3.6955e-06,\n",
            "        3.6955e-06, 4.2319e-06, 4.5300e-06, 2.8610e-06, 4.4703e-06, 4.4107e-06,\n",
            "        4.1723e-06, 3.6359e-06, 4.8280e-06, 4.6492e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.7551e-06, 3.6955e-06, 3.4571e-06, 4.5896e-06, 3.5167e-06, 3.8743e-06,\n",
            "        3.3379e-06, 4.5300e-06, 4.6492e-06, 2.8610e-06, 4.5896e-06, 4.5896e-06,\n",
            "        4.2915e-06, 3.6359e-06, 3.8147e-06, 4.1127e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.3975e-06, 3.4571e-06, 3.3379e-06, 2.7418e-06, 3.3379e-06, 3.2783e-06,\n",
            "        3.3379e-06, 3.6955e-06, 2.8014e-06, 2.8014e-06, 2.7418e-06, 2.7418e-06,\n",
            "        2.8014e-06, 3.0398e-06, 3.9339e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.0398e-06, 3.3975e-06, 3.3975e-06, 2.5630e-06, 3.3379e-06, 3.0994e-06,\n",
            "        2.9802e-06, 2.7418e-06, 2.8014e-06, 2.6822e-06, 2.5630e-06, 2.7418e-06,\n",
            "        2.8014e-06, 2.9206e-06, 3.9339e-06, 2.8014e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.0398e-06, 3.3379e-06, 3.3379e-06, 2.5630e-06, 3.3379e-06, 3.2187e-06,\n",
            "        2.9206e-06, 2.7418e-06, 2.5630e-06, 2.7418e-06, 2.4438e-06, 2.7418e-06,\n",
            "        2.8014e-06, 2.9206e-06, 2.3842e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.0398e-06, 3.3379e-06, 3.3379e-06, 2.4438e-06, 3.3379e-06, 3.0994e-06,\n",
            "        2.9206e-06, 2.7418e-06, 2.5630e-06, 2.6822e-06, 2.4438e-06, 2.4438e-06,\n",
            "        2.8014e-06, 2.9206e-06, 2.5034e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.0398e-06, 3.3379e-06, 3.4571e-06, 2.4438e-06, 3.3379e-06, 2.8610e-06,\n",
            "        2.9802e-06, 2.4438e-06, 2.5630e-06, 2.7418e-06, 2.4438e-06, 2.4438e-06,\n",
            "        2.5630e-06, 2.9206e-06, 2.4438e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([3.0398e-06, 3.3975e-06, 3.3379e-06, 2.4438e-06, 3.3379e-06, 2.8610e-06,\n",
            "        2.9802e-06, 2.4438e-06, 2.5630e-06, 2.6226e-06, 2.4438e-06, 2.4438e-06,\n",
            "        2.5630e-06, 3.0994e-06, 2.4438e-06, 2.2650e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([3.0398e-06, 3.0398e-06, 2.9802e-06, 2.4438e-06, 2.8014e-06, 2.8610e-06,\n",
            "        2.9802e-06, 2.4438e-06, 2.4438e-06, 2.8014e-06, 2.5034e-06, 2.4438e-06,\n",
            "        2.6226e-06, 2.9802e-06, 2.4438e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.9206e-06, 3.0398e-06, 2.9206e-06, 2.4438e-06, 2.8014e-06, 2.8610e-06,\n",
            "        2.9802e-06, 2.4438e-06, 2.5034e-06, 2.9206e-06, 2.5630e-06, 2.4438e-06,\n",
            "        2.4438e-06, 2.9802e-06, 2.4438e-06, 2.3842e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.9206e-06, 3.2187e-06, 2.9206e-06, 2.3842e-06, 2.8014e-06, 2.8610e-06,\n",
            "        2.9206e-06, 2.3842e-06, 2.4438e-06, 3.0994e-06, 2.6226e-06, 2.5630e-06,\n",
            "        2.5630e-06, 2.9802e-06, 2.5034e-06, 2.4438e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.9206e-06, 2.9802e-06, 2.7418e-06, 2.5034e-06, 2.8014e-06, 2.7418e-06,\n",
            "        3.1590e-06, 2.3842e-06, 2.5630e-06, 3.1590e-06, 2.6226e-06, 2.5630e-06,\n",
            "        2.6822e-06, 2.9802e-06, 2.5630e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.9206e-06, 2.9802e-06, 3.0994e-06, 2.5630e-06, 3.0398e-06, 2.7418e-06,\n",
            "        3.1590e-06, 2.5034e-06, 2.6822e-06, 3.0398e-06, 2.6226e-06, 2.5034e-06,\n",
            "        2.7418e-06, 2.8014e-06, 2.5034e-06, 2.6822e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.9206e-06, 2.9802e-06, 3.0994e-06, 2.6226e-06, 3.0398e-06, 2.7418e-06,\n",
            "        2.7418e-06, 2.5630e-06, 2.7418e-06, 3.0398e-06, 2.6822e-06, 2.6226e-06,\n",
            "        2.7418e-06, 2.8014e-06, 2.5630e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.9206e-06, 3.3975e-06, 2.7418e-06, 2.6822e-06, 2.8014e-06, 2.8014e-06,\n",
            "        2.7418e-06, 2.6226e-06, 2.7418e-06, 3.0398e-06, 2.9206e-06, 2.6226e-06,\n",
            "        2.9206e-06, 2.8014e-06, 2.6822e-06, 2.9802e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([4.2915e-06, 5.6624e-06, 4.5896e-06, 4.7088e-06, 4.1723e-06, 4.5300e-06,\n",
            "        4.3511e-06, 3.7551e-06, 5.3644e-06, 4.5300e-06, 2.9802e-06, 6.9737e-06,\n",
            "        4.9472e-06, 3.1590e-06, 5.5432e-06, 4.1127e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([5.3048e-06, 3.6955e-06, 4.1127e-06, 3.7551e-06, 4.1127e-06, 6.1393e-06,\n",
            "        3.9935e-06, 3.7551e-06, 6.9141e-06, 3.6955e-06, 3.5167e-06, 7.6890e-06,\n",
            "        4.2319e-06, 3.6359e-06, 5.1260e-06, 3.6955e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.1723e-06, 3.5167e-06, 4.7088e-06, 4.0531e-06, 4.7088e-06, 4.8876e-06,\n",
            "        3.9935e-06, 3.8147e-06, 3.5763e-06, 3.6359e-06, 3.8147e-06, 3.5167e-06,\n",
            "        3.8743e-06, 3.3975e-06, 5.4240e-06, 3.7551e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([4.0531e-06, 3.3379e-06, 4.1723e-06, 3.1590e-06, 4.4703e-06, 3.2187e-06,\n",
            "        3.9339e-06, 3.8147e-06, 3.4571e-06, 3.0398e-06, 3.4571e-06, 3.6955e-06,\n",
            "        4.1723e-06, 3.1590e-06, 4.5300e-06, 3.3379e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.1590e-06, 3.2783e-06, 4.2915e-06, 3.2783e-06, 3.3975e-06, 3.0994e-06,\n",
            "        3.3379e-06, 3.4571e-06, 3.0994e-06, 3.0994e-06, 3.4571e-06, 2.9802e-06,\n",
            "        3.3379e-06, 3.2783e-06, 3.3379e-06, 3.3975e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.1590e-06, 2.9206e-06, 3.4571e-06, 3.3379e-06, 3.3975e-06, 3.0994e-06,\n",
            "        3.3975e-06, 3.6955e-06, 3.0994e-06, 2.9206e-06, 3.4571e-06, 2.8610e-06,\n",
            "        3.2783e-06, 3.2783e-06, 3.3379e-06, 3.2187e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.1590e-06, 3.0994e-06, 3.0994e-06, 3.3379e-06, 3.3975e-06, 3.0994e-06,\n",
            "        3.4571e-06, 2.7418e-06, 3.0994e-06, 3.0398e-06, 3.4571e-06, 2.6226e-06,\n",
            "        2.5034e-06, 3.2783e-06, 3.0398e-06, 3.2187e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.0398e-06, 3.1590e-06, 2.8610e-06, 2.9206e-06, 2.9206e-06, 3.0994e-06,\n",
            "        2.7418e-06, 2.4438e-06, 3.0994e-06, 3.0994e-06, 3.0398e-06, 2.2650e-06,\n",
            "        2.7418e-06, 2.8610e-06, 3.2187e-06, 3.1590e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.0994e-06, 3.0398e-06, 2.8610e-06, 3.1590e-06, 2.9206e-06, 3.2187e-06,\n",
            "        2.8014e-06, 2.4438e-06, 3.1590e-06, 3.0994e-06, 3.0398e-06, 2.2650e-06,\n",
            "        2.7418e-06, 2.8610e-06, 3.1590e-06, 3.0398e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.7418e-06, 3.0398e-06, 2.8610e-06, 3.0994e-06, 2.8610e-06, 3.2187e-06,\n",
            "        2.8610e-06, 2.4438e-06, 3.1590e-06, 2.7418e-06, 2.6822e-06, 2.2650e-06,\n",
            "        2.8014e-06, 2.6822e-06, 3.1590e-06, 3.0398e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.7418e-06, 2.7418e-06, 2.8610e-06, 2.9206e-06, 2.8610e-06, 3.2187e-06,\n",
            "        2.8610e-06, 2.3842e-06, 3.1590e-06, 2.5630e-06, 2.6822e-06, 2.3246e-06,\n",
            "        2.6226e-06, 2.6226e-06, 3.1590e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.7418e-06, 2.7418e-06, 2.8610e-06, 2.9206e-06, 2.5630e-06, 2.9802e-06,\n",
            "        2.5630e-06, 2.3842e-06, 2.9802e-06, 2.5630e-06, 2.6822e-06, 2.3842e-06,\n",
            "        2.7418e-06, 2.6226e-06, 3.1590e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.7418e-06, 2.7418e-06, 3.0994e-06, 2.9802e-06, 2.5630e-06, 2.6226e-06,\n",
            "        2.5630e-06, 2.3246e-06, 2.9802e-06, 2.5034e-06, 2.5630e-06, 2.4438e-06,\n",
            "        2.5630e-06, 2.6226e-06, 3.2187e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.7418e-06, 2.7418e-06, 3.0994e-06, 2.9802e-06, 2.5630e-06, 2.6226e-06,\n",
            "        2.5630e-06, 2.3246e-06, 2.9802e-06, 2.5034e-06, 2.5630e-06, 2.5630e-06,\n",
            "        2.5630e-06, 2.6226e-06, 2.8014e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.7418e-06, 2.7418e-06, 2.8610e-06, 2.6226e-06, 2.5630e-06, 2.6822e-06,\n",
            "        2.5630e-06, 2.3842e-06, 2.6226e-06, 2.5034e-06, 2.5630e-06, 2.6822e-06,\n",
            "        2.5630e-06, 2.6226e-06, 2.8014e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.7418e-06, 2.7418e-06, 2.8610e-06, 2.6226e-06, 2.5630e-06, 2.6822e-06,\n",
            "        2.5630e-06, 2.6226e-06, 2.6226e-06, 2.5034e-06, 2.6226e-06, 2.8014e-06,\n",
            "        2.5630e-06, 2.6226e-06, 2.6226e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.7418e-06, 2.7418e-06, 2.8610e-06, 2.6226e-06, 2.5630e-06, 2.6822e-06,\n",
            "        2.7418e-06, 2.8014e-06, 2.6226e-06, 2.4438e-06, 2.6226e-06, 3.0398e-06,\n",
            "        2.5630e-06, 2.5630e-06, 2.6226e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.7418e-06, 2.7418e-06, 2.8610e-06, 2.6226e-06, 2.6226e-06, 2.6822e-06,\n",
            "        2.7418e-06, 2.8014e-06, 2.6226e-06, 2.5630e-06, 2.5630e-06, 3.0398e-06,\n",
            "        2.5630e-06, 2.5630e-06, 2.6226e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.7418e-06, 3.0398e-06, 3.1590e-06, 2.6226e-06, 2.5630e-06, 2.7418e-06,\n",
            "        2.7418e-06, 2.8014e-06, 2.6226e-06, 2.5034e-06, 2.5630e-06, 2.9206e-06,\n",
            "        2.4438e-06, 2.5630e-06, 3.0994e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.7418e-06, 2.7418e-06, 3.1590e-06, 2.6226e-06, 2.5630e-06, 2.7418e-06,\n",
            "        2.6226e-06, 2.8014e-06, 2.6226e-06, 2.5630e-06, 2.5630e-06, 3.0398e-06,\n",
            "        2.6226e-06, 2.5630e-06, 2.6226e-06, 2.6226e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([3.0994e-06, 2.9802e-06, 3.6955e-06, 4.8280e-06, 5.1856e-06, 3.6955e-06,\n",
            "        2.9802e-06, 2.8014e-06, 2.9802e-06, 3.0994e-06, 3.7551e-06, 3.9339e-06,\n",
            "        3.6359e-06, 3.8147e-06, 4.8876e-06, 4.4703e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([2.6226e-06, 3.3379e-06, 2.9802e-06, 4.1127e-06, 3.2783e-06, 2.6226e-06,\n",
            "        2.9802e-06, 2.8014e-06, 3.3975e-06, 3.5167e-06, 3.2187e-06, 3.2783e-06,\n",
            "        4.0531e-06, 3.3379e-06, 5.4836e-06, 4.1723e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([2.6822e-06, 2.9802e-06, 3.0398e-06, 5.0664e-06, 3.5763e-06, 2.3842e-06,\n",
            "        2.9802e-06, 2.8610e-06, 2.6822e-06, 3.5763e-06, 2.6822e-06, 3.1590e-06,\n",
            "        2.7418e-06, 3.0994e-06, 5.5432e-06, 3.0994e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([2.9206e-06, 2.9802e-06, 2.8014e-06, 2.5630e-06, 3.2783e-06, 2.5630e-06,\n",
            "        2.9206e-06, 3.1590e-06, 2.8610e-06, 2.8610e-06, 2.3246e-06, 2.8610e-06,\n",
            "        2.6226e-06, 3.1590e-06, 5.4836e-06, 3.3975e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([2.7418e-06, 3.0994e-06, 2.9206e-06, 2.1458e-06, 2.6226e-06, 2.6226e-06,\n",
            "        2.8610e-06, 2.6822e-06, 2.7418e-06, 2.6226e-06, 2.5034e-06, 2.8610e-06,\n",
            "        2.6822e-06, 2.8610e-06, 3.3975e-06, 2.7418e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([2.8610e-06, 2.8014e-06, 3.0398e-06, 2.2054e-06, 2.2650e-06, 2.7418e-06,\n",
            "        2.8610e-06, 2.6822e-06, 2.7418e-06, 2.6226e-06, 2.5630e-06, 2.9206e-06,\n",
            "        2.6226e-06, 3.1590e-06, 2.9206e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([2.9206e-06, 2.7418e-06, 3.0994e-06, 2.2054e-06, 2.3842e-06, 2.6226e-06,\n",
            "        2.9802e-06, 2.6822e-06, 2.6822e-06, 2.6226e-06, 2.4438e-06, 2.6226e-06,\n",
            "        2.6822e-06, 3.3975e-06, 2.5630e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([2.3246e-06, 2.3246e-06, 2.1458e-06, 2.2054e-06, 2.2650e-06, 2.5630e-06,\n",
            "        2.4438e-06, 2.0266e-06, 2.2650e-06, 2.4438e-06, 2.2054e-06, 2.6226e-06,\n",
            "        1.9670e-06, 1.9670e-06, 2.5630e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([2.2054e-06, 2.2650e-06, 2.0862e-06, 2.3246e-06, 2.3842e-06, 2.5034e-06,\n",
            "        2.3246e-06, 2.0862e-06, 2.2650e-06, 2.2054e-06, 2.2054e-06, 2.5034e-06,\n",
            "        1.9670e-06, 1.9670e-06, 2.6822e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([2.2054e-06, 1.9670e-06, 2.0862e-06, 2.5034e-06, 2.5630e-06, 2.2650e-06,\n",
            "        2.4438e-06, 2.0862e-06, 2.0862e-06, 2.2054e-06, 2.2054e-06, 2.3842e-06,\n",
            "        1.9073e-06, 1.9670e-06, 2.5630e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([2.2054e-06, 1.9670e-06, 2.0862e-06, 2.4438e-06, 2.5034e-06, 2.2650e-06,\n",
            "        2.5034e-06, 2.0862e-06, 2.0862e-06, 2.2054e-06, 2.2054e-06, 2.3842e-06,\n",
            "        1.9670e-06, 2.0862e-06, 2.5034e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([2.2650e-06, 2.0266e-06, 2.0266e-06, 2.4438e-06, 2.5034e-06, 2.2054e-06,\n",
            "        2.2650e-06, 2.0862e-06, 2.0862e-06, 2.2054e-06, 2.2054e-06, 2.3842e-06,\n",
            "        2.0266e-06, 2.0862e-06, 2.5034e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.2650e-06, 2.0862e-06, 1.9670e-06, 2.4438e-06, 2.5034e-06, 2.2054e-06,\n",
            "        2.2054e-06, 2.0862e-06, 2.0862e-06, 2.2054e-06, 2.2054e-06, 2.3842e-06,\n",
            "        2.0266e-06, 2.0862e-06, 2.5034e-06, 2.5630e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([1.9670e-06, 2.1458e-06, 2.0266e-06, 2.2054e-06, 2.2054e-06, 2.0862e-06,\n",
            "        2.2054e-06, 2.0862e-06, 2.0266e-06, 2.2054e-06, 2.2054e-06, 2.3842e-06,\n",
            "        1.9670e-06, 2.0862e-06, 2.5630e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.0862e-06, 2.1458e-06, 2.0266e-06, 2.2054e-06, 2.2054e-06, 2.0862e-06,\n",
            "        2.2054e-06, 2.0862e-06, 2.0266e-06, 2.2054e-06, 2.2054e-06, 2.2054e-06,\n",
            "        1.9670e-06, 2.0862e-06, 2.2054e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.1458e-06, 2.1458e-06, 2.0266e-06, 2.2054e-06, 2.2054e-06, 2.0862e-06,\n",
            "        2.1458e-06, 2.0266e-06, 2.0266e-06, 2.2054e-06, 2.2054e-06, 2.2054e-06,\n",
            "        1.9670e-06, 2.0266e-06, 2.2054e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.1458e-06, 2.1458e-06, 2.0862e-06, 2.2054e-06, 2.2054e-06, 2.0862e-06,\n",
            "        2.1458e-06, 2.0266e-06, 1.9073e-06, 2.2054e-06, 2.2054e-06, 2.2054e-06,\n",
            "        2.0266e-06, 2.1458e-06, 2.2054e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.1458e-06, 2.0266e-06, 2.1458e-06, 2.2054e-06, 2.2650e-06, 2.0862e-06,\n",
            "        1.9670e-06, 2.0266e-06, 1.9073e-06, 2.2054e-06, 2.2054e-06, 2.2054e-06,\n",
            "        2.0862e-06, 1.9670e-06, 2.2054e-06, 2.2054e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.1458e-06, 2.0266e-06, 2.0862e-06, 2.2650e-06, 2.2650e-06, 2.0266e-06,\n",
            "        1.9670e-06, 2.0266e-06, 1.9073e-06, 2.2054e-06, 2.2054e-06, 2.2054e-06,\n",
            "        2.0862e-06, 1.9670e-06, 2.2054e-06, 2.2650e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([2.2054e-06, 1.9073e-06, 2.1458e-06, 2.2650e-06, 2.2650e-06, 2.0862e-06,\n",
            "        1.9670e-06, 2.0266e-06, 1.9073e-06, 2.2054e-06, 2.2650e-06, 2.2054e-06,\n",
            "        2.2054e-06, 1.9670e-06, 2.2054e-06, 2.5034e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "0 tensor([5.2452e-06, 3.8743e-06, 5.3048e-06, 4.5300e-06, 6.5565e-06, 7.5102e-06,\n",
            "        3.8743e-06, 3.5167e-06, 5.0068e-06, 3.8743e-06, 6.2585e-06, 4.7088e-06,\n",
            "        4.1127e-06, 5.5432e-06, 5.0664e-06, 6.1393e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "1 tensor([3.9339e-06, 4.4703e-06, 5.4836e-06, 3.9339e-06, 5.8413e-06, 5.2452e-06,\n",
            "        4.1127e-06, 4.1127e-06, 5.4240e-06, 4.5300e-06, 5.9605e-06, 4.4107e-06,\n",
            "        3.4571e-06, 4.4703e-06, 4.9472e-06, 5.4836e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "2 tensor([4.4107e-06, 4.8876e-06, 3.8743e-06, 4.0531e-06, 4.1127e-06, 4.5896e-06,\n",
            "        3.9339e-06, 3.8743e-06, 5.8413e-06, 3.8743e-06, 4.5896e-06, 4.4107e-06,\n",
            "        4.0531e-06, 3.4571e-06, 4.0531e-06, 4.8876e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "3 tensor([3.9339e-06, 5.4836e-06, 4.1723e-06, 4.5300e-06, 3.0994e-06, 3.7551e-06,\n",
            "        3.4571e-06, 4.4107e-06, 4.6492e-06, 4.2319e-06, 3.5763e-06, 4.1723e-06,\n",
            "        4.0531e-06, 3.5167e-06, 4.0531e-06, 5.4836e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "4 tensor([3.6955e-06, 4.4107e-06, 4.4107e-06, 4.2319e-06, 2.7418e-06, 3.5763e-06,\n",
            "        3.4571e-06, 3.9339e-06, 3.5167e-06, 3.9339e-06, 3.6955e-06, 3.9339e-06,\n",
            "        3.8147e-06, 3.3975e-06, 3.5763e-06, 4.1127e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "5 tensor([3.4571e-06, 3.8147e-06, 3.8147e-06, 4.4703e-06, 2.7418e-06, 3.3975e-06,\n",
            "        3.4571e-06, 3.8743e-06, 3.9339e-06, 3.9339e-06, 3.6955e-06, 3.9935e-06,\n",
            "        3.7551e-06, 3.5167e-06, 3.6359e-06, 3.4571e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "6 tensor([3.6359e-06, 3.8147e-06, 3.0994e-06, 2.6822e-06, 2.3842e-06, 3.0994e-06,\n",
            "        3.3379e-06, 3.0398e-06, 3.3379e-06, 3.1590e-06, 3.6955e-06, 3.0994e-06,\n",
            "        3.3379e-06, 3.4571e-06, 2.8014e-06, 3.3379e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "7 tensor([3.2187e-06, 3.3379e-06, 2.7418e-06, 2.6822e-06, 2.3842e-06, 3.1590e-06,\n",
            "        2.8610e-06, 3.0398e-06, 3.4571e-06, 2.9802e-06, 3.6955e-06, 2.9802e-06,\n",
            "        3.3379e-06, 3.4571e-06, 2.8014e-06, 3.3379e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "8 tensor([3.4571e-06, 3.3379e-06, 2.7418e-06, 2.6822e-06, 2.5034e-06, 3.1590e-06,\n",
            "        2.8014e-06, 2.9802e-06, 3.2187e-06, 2.8014e-06, 3.6955e-06, 2.8610e-06,\n",
            "        2.9802e-06, 3.3379e-06, 2.8610e-06, 3.4571e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "9 tensor([3.2187e-06, 3.3379e-06, 2.7418e-06, 2.6822e-06, 2.5034e-06, 3.0398e-06,\n",
            "        2.8014e-06, 2.9802e-06, 2.8610e-06, 2.5034e-06, 3.6955e-06, 2.9206e-06,\n",
            "        2.9802e-06, 3.3379e-06, 2.5034e-06, 3.4571e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "10 tensor([3.2187e-06, 3.3379e-06, 2.8014e-06, 2.6822e-06, 2.6822e-06, 3.0994e-06,\n",
            "        2.7418e-06, 2.8610e-06, 2.8610e-06, 2.5034e-06, 3.6955e-06, 3.0398e-06,\n",
            "        2.9206e-06, 2.9802e-06, 2.4438e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "11 tensor([3.3379e-06, 2.9206e-06, 2.8014e-06, 2.6822e-06, 2.8014e-06, 2.6822e-06,\n",
            "        2.7418e-06, 2.9206e-06, 2.8610e-06, 2.5034e-06, 3.4571e-06, 2.5630e-06,\n",
            "        2.7418e-06, 2.9802e-06, 2.3842e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "12 tensor([2.8610e-06, 2.9206e-06, 2.8014e-06, 2.5630e-06, 2.9802e-06, 2.8610e-06,\n",
            "        2.7418e-06, 2.7418e-06, 2.8610e-06, 2.5630e-06, 3.3379e-06, 2.4438e-06,\n",
            "        2.7418e-06, 2.9802e-06, 2.3842e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "13 tensor([2.8610e-06, 2.9206e-06, 2.5630e-06, 2.5630e-06, 3.2187e-06, 2.9206e-06,\n",
            "        2.8014e-06, 2.8610e-06, 2.8610e-06, 2.4438e-06, 2.8610e-06, 2.6822e-06,\n",
            "        2.7418e-06, 2.9802e-06, 2.4438e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "14 tensor([2.8610e-06, 2.9206e-06, 2.5630e-06, 2.4438e-06, 3.3379e-06, 2.9206e-06,\n",
            "        2.8610e-06, 2.7418e-06, 2.8610e-06, 2.4438e-06, 2.8610e-06, 2.5630e-06,\n",
            "        2.7418e-06, 2.9802e-06, 3.4571e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "15 tensor([2.8610e-06, 2.9206e-06, 3.0994e-06, 2.4438e-06, 3.2783e-06, 2.8610e-06,\n",
            "        2.8610e-06, 2.8014e-06, 2.8610e-06, 2.6822e-06, 2.8610e-06, 2.5630e-06,\n",
            "        2.7418e-06, 2.9802e-06, 2.3842e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "16 tensor([2.8610e-06, 2.9206e-06, 2.5034e-06, 2.4438e-06, 3.2187e-06, 2.8610e-06,\n",
            "        2.8610e-06, 2.8014e-06, 3.3379e-06, 2.7418e-06, 2.8610e-06, 2.5630e-06,\n",
            "        2.9206e-06, 2.9802e-06, 2.5034e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "17 tensor([2.8610e-06, 2.9206e-06, 2.6226e-06, 2.3842e-06, 2.8610e-06, 2.8610e-06,\n",
            "        2.8610e-06, 2.8014e-06, 2.8610e-06, 2.9206e-06, 2.8610e-06, 2.6822e-06,\n",
            "        3.1590e-06, 2.9802e-06, 2.5630e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "18 tensor([2.8610e-06, 2.9206e-06, 2.8610e-06, 2.3842e-06, 2.8610e-06, 2.8610e-06,\n",
            "        2.9802e-06, 2.8014e-06, 2.8610e-06, 2.9802e-06, 3.3975e-06, 2.7418e-06,\n",
            "        3.0398e-06, 2.9802e-06, 2.6226e-06, 2.8610e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "19 tensor([3.3379e-06, 2.7418e-06, 2.9206e-06, 2.4438e-06, 2.8610e-06, 2.8610e-06,\n",
            "        2.9802e-06, 2.8610e-06, 2.8610e-06, 2.9802e-06, 3.3975e-06, 2.6226e-06,\n",
            "        2.8610e-06, 2.9802e-06, 2.8014e-06, 3.3975e-06], device='cuda:0',\n",
            "       dtype=torch.float16)\n",
            "dided\n",
            "time\n",
            "[14, 13, 13, 14, 0, 14, 13, 14, 0, 14, 14, 14, 0, 14, 14, 0, 14, 13, 13, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 9, 3, 3, 3, 0, 13, 13, 3, 0, 13, 13, 3, 0, 13, 13, 3, 13, 3, 3, 3, 3, 0, 3, 3, 9, 3, 0, 3, 0, 13, 13, 3, 0, 13, 13, 3, 0, 9, 3, 3, 0, 13, 13, 3, 3, 0, 0, 0, 0, 13, 9, 3, 9, 9, 3, 3, 0, 13, 13, 3, 9, 0, 3, 3, 3, 0, 13, 3, 9, 3, 3, 3, 0, 13, 13, 3, 3, 0, 3, 3, 0, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 9, 13, 3, 0, 14, 14, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 13, 13, 3, 0, 13, 13, 3, 0, 13, 13, 3, 9, 3, 0, 3, 3, 0, 3, 3, 3, 9, 3, 3, 9, 3, 0, 3]\n",
            "29 #### train ####\n",
            "repr, std, cov, closs 0.011213256046175957 0.476806640625 0.0002229337114840746 0.5482158660888672\n",
            "0.020558884552676994 0.052369902570015316 1.0\n",
            "repr, std, cov, closs 0.014810646884143353 0.475341796875 0.0003109998069703579 0.3132641017436981\n",
            "0.020476854192852546 0.05200477522119892 1.0\n",
            "repr, std, cov, closs 0.012358286418020725 0.47705078125 0.00020759622566401958 0.31984323263168335\n",
            "0.020558884552676994 0.052422272472585324 1.0\n",
            "repr, std, cov, closs 0.01654185727238655 0.4755859375 0.0003123392816632986 0.31983357667922974\n",
            "0.020558884552676994 0.05294886050126699 1.0\n",
            "repr, std, cov, closs 0.017968984320759773 0.477783203125 0.00021859188564121723 0.3263970613479614\n",
            "0.020641243526451105 0.05332061630591079 1.0\n",
            "repr, std, cov, closs 0.018535379320383072 0.47412109375 0.00033648591488599777 0.3216545879840851\n",
            "0.02057944343722967 0.05364134088020326 1.0\n",
            "repr, std, cov, closs 0.007422415539622307 0.476318359375 0.0002476752270013094 0.3132690489292145\n",
            "0.020313774075579365 0.052632276305851995 1.0\n",
            "repr, std, cov, closs 0.014832667075097561 0.474365234375 0.0003475653938949108 0.31327909231185913\n",
            "0.019991499866027107 0.05143614018746571 1.0\n",
            "repr, std, cov, closs 0.011189304292201996 0.474853515625 0.00033550988882780075 0.6164108514785767\n",
            "0.019792680015983517 0.050418140214789846 1.0\n",
            "repr, std, cov, closs 0.01707523502409458 0.474609375 0.00033872947096824646 0.4753488302230835\n",
            "0.01965468379247431 0.0494697083083422 1.0\n",
            "repr, std, cov, closs 0.011712525971233845 0.4755859375 0.00030614365823566914 0.31326431035995483\n",
            "0.019635048743730583 0.04892879241873329 1.0\n",
            "repr, std, cov, closs 0.011380771175026894 0.475341796875 0.00038930238224565983 0.3132624626159668\n",
            "0.019362208792372895 0.04776905367746387 1.0\n",
            "repr, std, cov, closs 0.01037326268851757 0.4755859375 0.0003496180288493633 0.6101425886154175\n",
            "0.019381571001165266 0.047388615217416184 1.0\n",
            "repr, std, cov, closs 0.018245920538902283 0.475830078125 0.00027404981665313244 0.31835052371025085\n",
            "0.019439773878263327 0.04762603265377856 1.0\n",
            "repr, std, cov, closs 0.017231350764632225 0.474609375 0.0003289517480880022 0.3248370885848999\n",
            "0.019595837472947217 0.04772133234511876 1.0\n",
            "repr, std, cov, closs 0.008886340074241161 0.476806640625 0.0002452353946864605 0.31928595900535583\n",
            "0.019517649690198174 0.047578454199578984 1.0\n",
            "repr, std, cov, closs 0.00957780983299017 0.477294921875 0.0002379859797656536 0.7748882174491882\n",
            "0.019615433310420163 0.04791250419342634 1.0\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(30):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    c_loader = make_weighted(buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    # train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# print(optim.param_groups[0][\"lr\"])\n",
        "optim.param_groups[0][\"lr\"] = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "b8zxYU9jpE8K",
        "outputId": "b08d1780-8c87-4841-891d-df809fb23319"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "256c75e8-e0c1-446e-a588-6de3dee18e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  self.hub = sentry_sdk.Hub(client)\n",
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240902_034119-3gthhome</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/3gthhome' target=\"_blank\">smooth-elevator-27</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/3gthhome' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/3gthhome</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        _,T,_ = sx.shape\n",
        "        batch = 1\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            # print(i,x.data, z.squeeze(), loss.item())\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def argm(self, sx, lr=3e3): # 3e3\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch_size, T, _ = sx.shape\n",
        "        batch = 16\n",
        "        # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "        z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "        optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "        sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "        # sx = sx.detach()\n",
        "        for i in range(20): # 10\n",
        "            # print(sx.shape,z.shape)\n",
        "            sxz = torch.cat([sx, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                cost = model(sxz)\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "            # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "            # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        # return z.detach()\n",
        "        # print(\"argm z\",z.squeeze().data)\n",
        "        # print(\"cost\",cost.squeeze())\n",
        "        idx = torch.argmax(loss)\n",
        "        # return z[idx].detach().unsqueeze(0)\n",
        "        return z[:,idx].detach()\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=1\n",
        "        T=1\n",
        "        x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "\n",
        "        lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "        # ratio = 6e0\n",
        "        lr = 1e-1 # adamw 1e-1\n",
        "        ratio = 4\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        # optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze())\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "        for i in range(50):\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = argm(x)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "            # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "            # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                x.clamp_(min=-1, max=1)\n",
        "            # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "        idx = torch.argmax(loss)\n",
        "        print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcOidvtW9KAH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Jx0k_ndHOEMe",
        "outputId": "b5fb2840-eeb5-45ae-f0e0-e2a15f8e7ccc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-05466a22e258>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# visualise(agent.sense,layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mvisualise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N2TGs69fnrZo"
      },
      "outputs": [],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "for name, param in agent.emb.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred[0].weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plot 3D"
      ],
      "metadata": {
        "id": "Jt_UlGz6Xoq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title sklearn RBF\n",
        "# https://gist.github.com/eljost/2c4e1af652ef02b2989da341c5569af7\n",
        "# from nn_plot.ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import scipy.stats as st\n",
        "\n",
        "# np.random.seed(1)\n",
        "def func(x):\n",
        "    # print(x.shape)\n",
        "    # x= np.sum(x**2, axis=-1)\n",
        "    x=np.random.rand(x.shape[0])\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "res = 50\n",
        "num_pts=15\n",
        "X=np.random.rand(num_pts,2)*res\n",
        "# Y = func(X)\n",
        "Y=np.random.rand(num_pts)\n",
        "# print(X);print(Y)\n",
        "\n",
        "lim = 1\n",
        "# lin = np.linspace(-lim, lim, res)\n",
        "lin = np.linspace(0, res, res)\n",
        "x1, x2 = np.meshgrid(lin, lin)\n",
        "xx = np.vstack((x1.flatten(), x2.flatten())).T\n",
        "\n",
        "kernel = RBF()\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gp.fit(X, Y)\n",
        "# print(\"Learned kernel\", gp.kernel_)\n",
        "y_mean, y_cov = gp.predict(xx, return_cov=True)\n",
        "\n",
        "posteriors = st.multivariate_normal.rvs(mean=y_mean, cov=y_cov, size=1)\n",
        "\n",
        "ax = plt.figure().add_subplot(projection='3d')\n",
        "Z=posteriors.reshape(-1, res)\n",
        "# ax.plot_surface(x1, x2, Z)\n",
        "ax.plot_surface(x1, x2, Z, cmap='rainbow', alpha=0.7)\n",
        "\n",
        "# ax.plot_surface(x1, x2, posteriors.reshape(-1, res))\n",
        "ax.contour(x1, x2, Z, zdir='z', offset=-1, cmap='coolwarm') # https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html#sphx-glr-gallery-mplot3d-contour3d-3-py\n",
        "# ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-0.4, 0.5))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-1, 2))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, c=zdata, cmap='Greens');\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, cmap='Greens');\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VX5IExbRriwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title chatgpt RBFKernelLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBFKernelLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        self.centres = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        dists = torch.cdist(x, self.centres, p=2) ** 2\n",
        "        return torch.exp(-dists / (2 * self.sigma ** 2))\n",
        "\n",
        "class SaddlePointNetwork(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(SaddlePointNetwork, self).__init__()\n",
        "        self.rbf_layer = RBFKernelLayer(in_features, out_features, sigma)\n",
        "        self.linear = nn.Linear(out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rbf_output = self.rbf_layer(x)\n",
        "        # Introduce a saddle point structure\n",
        "        linear_output = self.linear(rbf_output)\n",
        "        # Example saddle function: x^2 - y^2\n",
        "        saddle_output = torch.sum(linear_output[:, :1]**2 - linear_output[:, 1:]**2, dim=1, keepdim=True)\n",
        "        return saddle_output\n",
        "\n",
        "# sin(ax)sin(bx)\n",
        "# (x^2 - y^2)\n",
        "import torch\n",
        "\n",
        "def rbf_saddle(x, y, gamma=1.0, a=1.0, b=1.0):\n",
        "    # RBF-like term\n",
        "    rbf_term = torch.exp(-gamma * torch.norm(x - y, p=2)**2)\n",
        "    # Saddle point term\n",
        "    saddle_term = (a * x)**2 - (b * y)**2\n",
        "    return rbf_term + saddle_term\n",
        "\n",
        "# Example usage\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "output = rbf_saddle(x, y)\n",
        "print(output)\n",
        "\n",
        "# Compute gradients\n",
        "output.backward()\n",
        "print(x.grad, y.grad)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "SusX7gpzxFNL",
        "outputId": "9f14a9da-e188-49ba-f5f5-70192ff33134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2.6321], grad_fn=<AddBackward0>)\n",
            "tensor([2.7358]) tensor([-4.7358])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plot 3d\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "y = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "X, Y = torch.meshgrid(x, y)\n",
        "Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rTmCo7pm0NxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title shape NN\n",
        "num_pts=1\n",
        "\n",
        "# X=torch.rand(num_pts,4)*2-1\n",
        "# X=torch.cat([torch.tensor([0,0]).unsqueeze(0),torch.rand(num_pts,2)*2-1], dim=-1)\n",
        "X=torch.cat([torch.zeros(1,1),torch.rand(num_pts,2)*2-1,torch.zeros(1,1)], dim=-1)\n",
        "Y=torch.rand(num_pts)\n",
        "print(X,Y)\n",
        "optim = torch.optim.SGD(model.parameters(), 1e-1)\n",
        "\n",
        "# model.train()\n",
        "pred = model(X)\n",
        "# print(Y.shape,pred.shape)\n",
        "# loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "loss = F.mse_loss(Y, pred.squeeze(-1))\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fWZaQTDFg1",
        "outputId": "4c5ced88-54f1-436e-89f9-66f1c8396373",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000, -0.7231,  0.3792,  0.0000]]) tensor([0.3362])\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plot NN\n",
        "\n",
        "xx = torch.linspace(-1, 1, 100)\n",
        "yy = torch.linspace(-1, 1, 100)\n",
        "X, Y = torch.meshgrid(xx, yy) # [100,100]\n",
        "xy = torch.cat([X.unsqueeze(-1), torch.zeros(X.shape+(2,)), Y.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "with torch.no_grad(): Z = model(xy).squeeze(-1)\n",
        "# print(Z)\n",
        "# print(Z.shape)\n",
        "\n",
        "# Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "boDd__PE2sGy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test optim saddle same time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def f(x, y):\n",
        "    return x ** 2 - y ** 2 + x * y\n",
        "# (x-y)(x+y)+xy\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "dim_x, dim_z = 3, 1\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# xx = torch.empty((1, T, dim_x))\n",
        "# torch.nn.init.xavier_uniform_(xx)\n",
        "# x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "\n",
        "# tensor([[0.6478, 0.0531, 0.0861]]) tensor([[-1.,  1.]]) 0.2974517047405243\n",
        "# tensor([-0.9419, -1.0000,  0.4416, -1.0000,  1.0000,  0.2963])\n",
        "\n",
        "# x = nn.Parameter(torch.tensor([[0.6478, 0.0531, 0.0861]]))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "z = nn.Parameter(torch.empty((batch, T, dim_z)))\n",
        "# z = torch.empty((1, T, 1))\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# z = nn.Parameter(z.repeat(batch,1,1))\n",
        "# z = nn.Parameter(torch.tensor([[-1.,  1.]]))\n",
        "# optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "# optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.95), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "# .95,1e-1,3e-1\n",
        "# .99,\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50): # num epochs\n",
        "    # loss = f(x,z)\n",
        "    # loss = f(x.sum(-1),z)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step(); optim_z.step()\n",
        "    optim_x.zero_grad(); optim_z.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x.squeeze()[0].data, z[0].squeeze().data, loss[0].squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "        z.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "# idx = torch.argmax(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n",
        "\n"
      ],
      "metadata": {
        "id": "qW6BYoXsX57o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6dc0e5f-6def-42fd-f829-e227ec94833b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor([-0.1000, -0.1000,  0.1000]) tensor(0.4161) -0.0008160677389241755\n",
            "1 tensor([-0.1999, -0.1999,  0.1999]) tensor(0.8145) -0.000763896678108722\n",
            "2 tensor([-0.2997, -0.2997,  0.2997]) tensor(1.2112) -0.0007120760856196284\n",
            "3 tensor([-0.3994, -0.3994,  0.3994]) tensor(1.3960) -0.0007130901212804019\n",
            "4 tensor([-0.4989, -0.4990,  0.4990]) tensor(1.3960) -0.0007601650431752205\n",
            "5 tensor([-0.5885, -0.5989,  0.5981]) tensor(1.3965) -0.0008037001243792474\n",
            "6 tensor([-0.6703, -0.6989,  0.6969]) tensor(1.3969) -0.0008425545529462397\n",
            "7 tensor([-0.7460, -0.7991,  0.7955]) tensor(1.3972) -0.0008812308078631759\n",
            "8 tensor([-0.8166, -0.8994,  0.8938]) tensor(1.3974) -0.000919759797398001\n",
            "9 tensor([-0.8943, -1.0004,  0.9918]) tensor(1.4013) -0.0009680598159320652\n",
            "10 tensor([-0.9786, -1.1017,  1.0897]) tensor(1.4044) -0.0010272095678374171\n",
            "11 tensor([-1.0680, -1.1023,  1.0977]) tensor(1.4068) -0.0010494425659999251\n",
            "12 tensor([-1.0936, -1.1028,  1.0976]) tensor(1.4087) -0.0010546413250267506\n",
            "13 tensor([-1.0968, -1.1032,  1.0976]) tensor(1.4102) -0.0010546413250267506\n",
            "14 tensor([-1.0994, -1.1035,  1.0975]) tensor(1.4113) -0.0010546413250267506\n",
            "15 tensor([-1.1015, -1.1037,  1.0975]) tensor(1.4123) -0.0010546413250267506\n",
            "16 tensor([-1.1032, -1.1039,  1.0975]) tensor(1.4130) -0.0010546413250267506\n",
            "17 tensor([-1.1045, -1.1040,  1.0975]) tensor(1.4135) -0.0010546413250267506\n",
            "18 tensor([-1.1057, -1.1041,  1.0975]) tensor(1.4139) -0.0010546413250267506\n",
            "19 tensor([-1.1066, -1.1041,  1.0975]) tensor(1.4142) -0.0010546413250267506\n",
            "20 tensor([-1.1073, -1.1042,  1.0975]) tensor(1.4144) -0.0010546413250267506\n",
            "21 tensor([-1.1079, -1.1042,  1.0975]) tensor(1.4145) -0.0010546413250267506\n",
            "22 tensor([-1.1084, -1.1042,  1.0975]) tensor(1.4146) -0.0010546413250267506\n",
            "23 tensor([-1.1088, -1.1042,  1.0975]) tensor(1.4146) -0.0010546413250267506\n",
            "24 tensor([-1.1091, -1.1042,  1.0975]) tensor(1.4145) -0.0010546413250267506\n",
            "25 tensor([-1.1093, -1.1041,  1.0975]) tensor(1.4144) -0.0010546413250267506\n",
            "26 tensor([-1.1094, -1.1041,  1.0976]) tensor(1.4143) -0.0010546413250267506\n",
            "27 tensor([-1.1096, -1.1040,  1.0976]) tensor(1.4141) -0.0010546413250267506\n",
            "28 tensor([-1.1096, -1.1040,  1.0976]) tensor(1.4140) -0.0010546413250267506\n",
            "29 tensor([-1.1097, -1.1039,  1.0976]) tensor(1.4138) -0.0010546413250267506\n",
            "30 tensor([-1.1097, -1.1039,  1.0976]) tensor(1.4135) -0.0010546413250267506\n",
            "31 tensor([-1.1096, -1.1038,  1.0976]) tensor(1.4133) -0.0010546413250267506\n",
            "32 tensor([-1.1096, -1.1037,  1.0977]) tensor(1.4131) -0.0010546413250267506\n",
            "33 tensor([-1.1095, -1.1037,  1.0977]) tensor(1.4128) -0.0010546413250267506\n",
            "34 tensor([-1.1094, -1.1036,  1.0977]) tensor(1.4126) -0.0010546413250267506\n",
            "35 tensor([-1.1093, -1.1035,  1.0977]) tensor(1.4123) -0.0010546413250267506\n",
            "36 tensor([-1.1092, -1.1034,  1.0978]) tensor(1.4121) -0.0010546413250267506\n",
            "37 tensor([-1.1091, -1.1034,  1.0978]) tensor(1.4118) -0.0010546413250267506\n",
            "38 tensor([-1.1090, -1.1033,  1.0978]) tensor(1.4116) -0.0010546413250267506\n",
            "39 tensor([-1.1089, -1.1032,  1.0978]) tensor(1.4113) -0.0010546413250267506\n",
            "40 tensor([-1.1087, -1.1032,  1.0978]) tensor(1.4111) -0.0010546413250267506\n",
            "41 tensor([-1.1086, -1.1031,  1.0979]) tensor(1.4108) -0.0010546413250267506\n",
            "42 tensor([-1.1085, -1.1030,  1.0979]) tensor(1.4106) -0.0010546413250267506\n",
            "43 tensor([-1.1083, -1.1030,  1.0979]) tensor(1.4103) -0.0010546413250267506\n",
            "44 tensor([-1.1082, -1.1029,  1.0979]) tensor(1.4101) -0.0010546413250267506\n",
            "45 tensor([-1.1080, -1.1028,  1.0979]) tensor(1.4098) -0.0010546413250267506\n",
            "46 tensor([-1.1079, -1.1027,  1.0979]) tensor(1.4096) -0.0010546413250267506\n",
            "47 tensor([-1.1077, -1.1027,  1.0980]) tensor(1.4094) -0.0010546413250267506\n",
            "48 tensor([-1.1076, -1.1026,  1.0980]) tensor(1.4091) -0.0010546413250267506\n",
            "49 tensor([-1.1075, -1.1026,  1.0980]) tensor(1.4089) -0.0010546413250267506\n",
            "tensor([-0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011,\n",
            "        -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011],\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "tensor([[-1., -1.,  1.]]) tensor([[1.]]) -0.0010546413250267506\n",
            "tensor([[-1., -1.,  1.]]) tensor([[1.]]) -0.0010546413250267506\n",
            "tensor([[-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test optim saddle argm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def argm(sx, lr=3e3): # 3e3\n",
        "    # batch=sx.size(dim=0)\n",
        "    batch_size, T, _ = sx.shape\n",
        "    batch = 16\n",
        "    # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "    # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "    z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "    torch.nn.init.xavier_uniform_(z)\n",
        "    # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "    # sx = sx.detach()\n",
        "    for i in range(20): # 10\n",
        "        # print(sx.shape,z.shape)\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            cost = model(sxz)\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "        # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    # return z.detach()\n",
        "    idx = torch.argmax(cost.squeeze(), dim=1)\n",
        "    return z[torch.arange(z.shape[0]),idx].detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(xx.clone())\n",
        "x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50):\n",
        "    z = argm(x)\n",
        "    # print(x.shape,z.shape)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "# print(\"z\",z)\n",
        "# print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n"
      ],
      "metadata": {
        "id": "GJdFpDr2wIMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff06bd6b-2bec-4392-c749-77cd3f1688ca",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3976, -1.0000, -1.0000]]) tensor([[-1.,  1.]]) 0.29717573523521423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = torch.randn(16, 16)\n",
        "# print((b==torch.max(b)).nonzero())\n",
        "x = torch.randn(10, 3)\n",
        "idx = torch.randint(3,(10,))\n",
        "# print(x[:,idx].shape)\n",
        "print(x[torch.arange(x.shape[0]),idx].shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjIJP6RlEv2",
        "outputId": "447fdefd-452b-437d-c228-1847492b36f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wUhKd009Qvk3"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}