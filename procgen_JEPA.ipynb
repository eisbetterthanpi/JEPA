{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "456bd130-97d0-46e2-a049-0b4edd8e9f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "e96d3a76-6961-4793-8f80-c6efe25322e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model=256, drop=0.5):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Dropout(p=drop),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Bos81kQf1dwh"
      },
      "outputs": [],
      "source": [
        "# @title transfer_sd store_sd load_sd\n",
        "\n",
        "def transfer_sd(tgt_sd, src_sd): #\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            # print(wht_name, tgt_wht.shape, src_wht.shape)\n",
        "            if tgt_wht.shape==src_wht.shape:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "                continue\n",
        "            if tgt_wht.shape[0] != src_wht.shape[0]: continue # output dim diff\n",
        "            if len(tgt_wht.shape)==2: tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "    return tgt_sd\n",
        "\n",
        "def store_sd(all_sd, new_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in new_sd.keys():\n",
        "            if not wht_name in all_sd.keys():\n",
        "                # print(wht_name, new_sd[wht_name].shape)\n",
        "                all_sd[wht_name] = (new_sd[wht_name],)\n",
        "                continue\n",
        "            all_tpl, new_wht = all_sd[wht_name], new_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                print(wht_name, all_wht.shape, new_wht.shape)\n",
        "                if all_wht.shape==new_wht.shape:\n",
        "                    all_wht = new_wht\n",
        "                    break\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: continue # diff output shape\n",
        "                if len(all_wht.shape)==2: all_wht[:, :new_wht.shape[1]] = new_wht[:, :all_wht.shape[1]]\n",
        "                break\n",
        "            if len(all_wht.shape)>=2 and len(all_wht.shape)>=2:\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: all_tpl = all_tpl + (new_wht,) # wht not in all_wht\n",
        "    return all_sd\n",
        "\n",
        "def load_sd(tgt_sd, all_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in all_sd.keys(): continue\n",
        "            tgt_wht, all_tpl = tgt_sd[wht_name], all_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                # try: print(wht_name, tgt_wht.shape, all_wht.shape)\n",
        "                # except: print(wht_name, tgt_wht, all_wht)\n",
        "                if tgt_wht.shape==all_wht.shape:\n",
        "                    tgt_wht.copy_(all_wht)\n",
        "                    break\n",
        "                if tgt_wht.shape[0] != all_wht.shape[0]: continue # output dim diff\n",
        "                if len(tgt_wht.shape)==2: tgt_wht[:, :all_wht.shape[1]].copy_(all_wht[:, :tgt_wht.shape[1]])\n",
        "                break\n",
        "    return tgt_sd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# modelsd = torch.load('agent.pkl', map_location=device).values()\n",
        "# tgt_sd = transfer_sd(agent.state_dict(), modelsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = {}\n",
        "# all_sd = store_sd(all_sd, agent1.state_dict())\n",
        "# print(all_sd.keys())\n",
        "# checkpoint = {'model': all_sd}\n",
        "# torch.save(checkpoint, 'all_sd.pkl')\n",
        "\n",
        "# agent3 = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "# agent3.tcost = tcost3\n",
        "# tgt_sd = load_sd(agent3.state_dict(), all_sd)\n",
        "# agent3.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "# for x,y in zip(agent1.state_dict().values(), agent3.state_dict().values()):\n",
        "#     print((x==y).all())\n",
        "\n",
        "# print(agent1.jepa.enc.cnn[1].num_batches_tracked)\n",
        "# jepa.enc.cnn.0.weight\n",
        "# print(agent1.jepa.enc.cnn[0].weight.shape)\n",
        "# print(agent1.jepa.enc.cnn[0].weight[0][0])\n",
        "# print(agent3.jepa.enc.cnn[0].weight[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SFVbGqMDqcDR"
      },
      "outputs": [],
      "source": [
        "# @title rename_sd\n",
        "def rename_sd(agent_sd):\n",
        "    sd_={}\n",
        "    convert={}\n",
        "    na_=''\n",
        "    for wht_name, wht in agent_sd.items():\n",
        "        o=wht_name.split('.')\n",
        "        # print(\"####\", wht_name)\n",
        "        name=wht_name\n",
        "        for i in range(len(o)):\n",
        "            c = o[i]\n",
        "            if c.isnumeric():\n",
        "                na, me = '.'.join(o[:i]), '.'.join(o[i+1:])\n",
        "                c=int(c)\n",
        "                if na!=na_: # param name diff\n",
        "                    j=0 # reset num\n",
        "                    c_=c # track wht_name num\n",
        "                    na_=na # track param name\n",
        "                elif c_<c: # same param name, diff num\n",
        "                    j+=1\n",
        "                    c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "        # print(name)\n",
        "        sd_[name] = wht\n",
        "        convert[name] = wht_name\n",
        "    return sd_, convert\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# # modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, _ = rename_sd(modelsd)\n",
        "\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "riBHnAAkkzrd"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim me\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# def transfer_optim(tgt_sd, src_sd, tgt_optim, src_optim): #\n",
        "def transfer_optim(tgt_sd, src_sd, tgt_optim_sd, src_optim_sd): #\n",
        "    non_lst = ['running_mean', 'running_var', 'num_batches_tracked', 'num_batches_tracked', 'loss_fn']\n",
        "    tgt_lst, src_lst = [], []\n",
        "    for i, (k,v) in enumerate(tgt_sd.items()):\n",
        "        # print(i, k, v.shape, any(s in k for s in non_lst))\n",
        "        if not any(s in k for s in non_lst): tgt_lst.append(k)\n",
        "    for i, (k,v) in enumerate(src_sd.items()):\n",
        "        if not any(s in k for s in non_lst): src_lst.append(k)\n",
        "\n",
        "    # tgt_optim_st, src_optim_st = tgt_optim.state_dict()['state'], src_optim.state_dict()['state']\n",
        "    tgt_optim_st, src_optim_st = tgt_optim_sd['state'], src_optim_sd['state']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, wht_name in enumerate(tgt_lst):\n",
        "            if not wht_name in src_lst: continue\n",
        "            tgt_wht, src_wht = tgt_optim_st[tgt_lst.index(wht_name)], src_optim_st[src_lst.index(wht_name)]\n",
        "            # print(wht_name, tgt_wht, src_wht)\n",
        "            tgt_shp, src_shp = tgt_wht['exp_avg'].shape, src_wht['exp_avg'].shape\n",
        "            if tgt_shp==src_shp:\n",
        "                tgt_wht = src_wht\n",
        "                continue\n",
        "            if tgt_shp[0] != src_shp[0]: continue # output dim diff\n",
        "            if len(tgt_shp)==2:\n",
        "                tgt_wht['step'] = src_wht['step']\n",
        "                tgt_wht['exp_avg'][:, :src_shp[1]] = src_wht['exp_avg'][:, :tgt_shp[1]]\n",
        "                tgt_wht['exp_avg_sq'][:, :src_shp[1]] = src_wht['exp_avg_sq'][:, :tgt_shp[1]]\n",
        "    # return tgt_optim.state_dict()\n",
        "    return tgt_optim_sd\n",
        "\n",
        "# model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "# model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "# source_optimizer = optim.AdamW(model_src.parameters())\n",
        "# target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "# dummy_input = torch.randn(3, 10)\n",
        "# dummy_target = torch.randn(3, 5)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "# output = model_src(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# source_optimizer.step()\n",
        "\n",
        "# dummy_input = torch.randn(3, 20)\n",
        "# output = model_tgt(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# target_optimizer.step()\n",
        "\n",
        "\n",
        "# print(source_optimizer.state_dict())\n",
        "# print(target_optimizer.state_dict())\n",
        "\n",
        "# optimsd = transfer_optim(model_tgt.state_dict(), model_src.state_dict(), target_optimizer, source_optimizer)\n",
        "# target_optimizer.load_state_dict(optimsd)\n",
        "# print(target_optimizer.state_dict())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfjFbveH64Io",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title TCost\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TCost(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=256): # in_dim=(1+self.jepa.pred.num_layers)*d_model\n",
        "        super().__init__()\n",
        "        self.tc = torch.tensor([-1., 0.], device=device).unsqueeze(-1) # unsqueeze(0).T\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            nn.Dropout(p=0.),\n",
        "            nn.Linear(in_dim, 2, bias=False), nn.Softmax(dim=-1),\n",
        "            # nn.Linear(in_dim, d_model), nn.ReLU(),\n",
        "            # nn.Dropout(p=0.5),\n",
        "            # nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, 2), nn.Softmax(),\n",
        "            )\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def update_loss_weight(self, train_data):\n",
        "        a = len(buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "        # self.data = [step for episode in buffer for step in episode]\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tcost(x)@self.tc\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        out = self.tcost(x)\n",
        "        # print(\"ctost loss; x,out,y\",x, out, y)\n",
        "        # print(\"tcost loss; out\",out.min().item(),out.max().item(), out)\n",
        "        y = torch.where(y < -0.5, 0, 1)\n",
        "        # print(\"ctost loss\", out.shape, y.shape)\n",
        "        return self.loss_fn(out, y)\n",
        "\n",
        "\n",
        "# tcost=TCost(1024)\n",
        "# x=torch.rand(256,1024)\n",
        "# import time\n",
        "# start = time.time()\n",
        "# out=tcost(x)\n",
        "# # out=F.gumbel_softmax(out)\n",
        "# print(time.time()-start)\n",
        "# # nn.AdaptiveLogSoftmaxWithLoss(in_features=2, n_classes=2, cutoffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "6d5ddd81-11ac-46a4-d0dd-6e536a564d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-5d08c402e0cb>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v, drop=0.2):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=drop)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)# + self.z_coeff * torch.norm(z)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "# torch.norm(z, dim=-1)\n",
        "# -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "# in RL, distribution of action, if certainty is high, entropy is low\n",
        "\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "bc13a9ea-1aef-4196-8a4d-5dd2e1f8248b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-87103bcf1e3b>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=8, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=20. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=20. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=0.01 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.closs_coeff=100.\n",
        "        self.zloss_coeff=20.\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        self.lx, self.lz = torch.empty((0,dim_a),device=device), torch.empty((0,dim_z),device=device) # [T,dim_az]\n",
        "        self.sx = self.jepa.enc(torch.zeros((1, 3,64,64)))\n",
        "        self.la = torch.empty(0,device=device)\n",
        "\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        if len(self.la)>1 or laction!=None:\n",
        "            self.update_h0(lstate, laction)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                self.sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(self.sx, T=8, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.la, self.lx, self.lz = lact, lx, lz\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx - torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    self.la = torch.cat([torch.tensor(laction, device=device), self.la[len(laction):]], dim=-1)\n",
        "                la = self.emb(self.la[:seq_len])\n",
        "\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5) # torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        optim_z = torch.optim.SGD([lz], lr=1e1) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e0 ; 3e-2 1e-1\n",
        "        lsx, la = lsx.detach(), la.detach() # [T, d_model], [T, dim_a]\n",
        "        # print(\"update_h0 lz\", lz.data)\n",
        "        self.jepa.pred.train()\n",
        "        for i in range(1): # 1?\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0.detach()) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                loss = F.mse_loss(out_, out.squeeze(0))\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            # print(\"update_h0 loss, lz\",i,loss.item(), lz.data)\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1].unsqueeze(0)\n",
        "        # print(\"update_h0\", self.lx.data)\n",
        "        # print(self.la.shape, self.lx.shape, self.lz.shape, self.la[seq_len:].shape, self.lx[seq_len:].shape, self.lz[seq_len:].shape)\n",
        "        self.la, self.lx, self.lz = self.la[seq_len:], self.lx[seq_len:], self.lz[seq_len:] # [T, dim_a], [T, dim_z]\n",
        "        return h0\n",
        "\n",
        "    def argm_s(self, sx, x, h0, lr=3e3): # 3e3\n",
        "        T, _ = x.shape\n",
        "        batch = 64 # 16\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        optim_z = torch.optim.SGD([z], lr=1e4, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        with torch.no_grad():\n",
        "            z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_z]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        x = x.detach().repeat(batch,1,1) # [batch, seq_len, dim_a]\n",
        "        # print(\"argm\", z[0].squeeze())\n",
        "        for i in range(2): # 5\n",
        "            # print(\"argm_search\", sx.shape, x.shape, z.shape, h0.shape) # [1, 256], [batch_size, T, 3], [batch_size, T, 8], [1, 1, 256]\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"argm z loss\", z[0].squeeze().data, loss[0].squeeze().data)\n",
        "            # print(\"c\", c)\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return z[idx]#.unsqueeze(0)\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        x = nn.Parameter(torch.empty((T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        optim_x = torch.optim.SGD([x], lr=1e3) # 1e-1,1e-0,1e4 ; 1e2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        with torch.no_grad(): x[:self.lx.shape[0]] = self.lx[:T] # [seq_len, dim_az]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        # print(\"search\",x.squeeze().data)\n",
        "        for i in range(2): # 5\n",
        "            dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = self.argm_s(sx, x_,h0)\n",
        "            # print(\"search\", sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [T, dim_a], [T, dim_z], [1, 1, 256]\n",
        "            loss, lsx, lh0, c = self.rnn_pred(sx, x_.unsqueeze(0), z.unsqueeze(0), h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "            # print(i, \"search loss\", x.squeeze().data, loss.item(), z.squeeze().data)\n",
        "            # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "            # print(\"search c\", [f'{cc.item():g}' for cc in c.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [T]\n",
        "        # print(lact.shape, lh0.shape, x.shape, z.shape) # [1]) torch.Size([1, 1, 1, 256]) torch.Size([1, 3]) torch.Size([1, 8])\n",
        "        return lact, lh0, x.data, z # [T], [T, num_layers, batch, d_model], [T, dim_a], [T, dim_z]\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        # lsx = torch.empty((batch, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            # print(sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        # syh0 = torch.cat([lsx.flatten(1),lh0.permute(2,0,1,3).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "        syh0 = torch.cat([lsx[:,1:], lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,T,d_model], [T,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "        tcost = -self.tcost(syh0).unflatten(0, (batch, seq_len)).squeeze(-1)\n",
        "        c = (tcost + icost)*gamma**torch.arange(seq_len, device=device)\n",
        "        return c.sum(), lsx, lh0, c\n",
        "        # return c.sum(), lh0\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd):\n",
        "        self.tcost.eval()\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "        # torch.nn.init.xavier_uniform_(lz)\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "            lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "                for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                    syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                    out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                    lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                    lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "                repr_loss = F.mse_loss(lsy, lsy_)\n",
        "                syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "                # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "                cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        return lz.detach()\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state_ = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state_).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "            # sx=sy_\n",
        "            state, action, reward = Sar # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "            state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "            for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "                # lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "                lh0 = torch.zeros((rwd.shape[1],)+h0.shape, device=device)\n",
        "                lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    lsy = self.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "                    # print(\"st\",st[0][-1])\n",
        "                    # print(\"st\",st.max(),st.min())\n",
        "                    print(\"lsy\",lsy.max().item(),lsy.min().item())\n",
        "                    # print(\"lsy\",lsy[0])\n",
        "                    la = self.emb(act) # [batch_size, bptt, dim_a]\n",
        "                    # out = lsy - torch.cat([sy_, lsy[:,:-1]], dim=1)\n",
        "                    lz = self.argm(lsy, sy_, h0, la, rwd) # [batch_size, bptt, d_model],\n",
        "                    # lz = torch.zeros((lsy.shape[0], lsy.shape[1], self.dim_z), device=device)\n",
        "\n",
        "                    # for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                    #     syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                    #     out_, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    #     sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                    #     lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                    #     lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "                    # repr_loss = F.mse_loss(lsy, lsy_) # [batch_size, bptt, d_model]\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "                    repr_loss = torch.zeros(1)\n",
        "                    # std_loss, cov_loss = torch.zeros(1),torch.zeros(1)\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = 0\n",
        "\n",
        "                    # print(lsy_.requires_grad, lh0.requires_grad)\n",
        "                    syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                    # syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                    # print(\"syh0, rwd\",syh0.shape,rwd.shape)\n",
        "                    # print(\"syh0, rwd.flatten()\",syh0.shape, rwd.flatten().shape)\n",
        "                    # print(\"syh0\",syh0[0][:10])\n",
        "                    # for name, param in agent.tcost.named_parameters():\n",
        "                    #     print(\"param.data\",param.data.max(),param.data.min())\n",
        "                    #     print(\"agent.tcost\",param.data)\n",
        "                    clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    # pred = self.tcost(syh0).squeeze(-1).cpu().unflatten(0, rwd.shape) # [batch_size, bptt]\n",
        "                    pred = self.tcost(syh0).squeeze(-1).unflatten(0, rwd.shape) # [batch_size, bptt]\n",
        "                    print(\"pred\",pred[0])\n",
        "                    print(\"rwd\",rwd[0])\n",
        "                    mask = torch.where(abs(rwd- pred)>0.5,1,0).bool()\n",
        "                    # # print(\"rwd, pred, clossl\", rwd[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(st[0].cpu(), nrow=10))\n",
        "                    # # try: imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "                loss = jloss + closs\n",
        "\n",
        "                # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "                # z_norm = torch.norm(z)\n",
        "                # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                print(\"clossl, wrong\", clossl.item(), mask.sum())\n",
        "                # print(\"repr, std, cov, clossl, wrong\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), mask.sum())\n",
        "                # print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                scaler.scale(loss).backward()\n",
        "                # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "                optim.zero_grad()\n",
        "                sy_, h0 = sy_.detach(), h0.detach()\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "# # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "# ema_model = torch.optim.swa_utils.AveragedModel(model, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n",
        "# for epoch in range(300):\n",
        "#       for input, target in loader:\n",
        "#           optimizer.zero_grad()\n",
        "#           loss_fn(model(input), target).backward()\n",
        "#           optimizer.step()\n",
        "#           ema_model.update_parameters(model)\n",
        "# # Update bn statistics for the ema_model at the end\n",
        "# torch.optim.swa_utils.update_bn(loader, ema_model)\n",
        "# # Use ema_model to make predictions on test data\n",
        "# preds = ema_model(test_input)\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.999)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "# !pip show torch triton\n",
        "# # !pip install --upgrade torch\n",
        "# !pip install --upgrade triton\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "# print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "# print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "# print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.tcost._parameters['weight'].shape)\n"
      ],
      "metadata": {
        "id": "FwgWasBjZ04u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(agent.jepa.enc.parameters().values()[0].requires_grad)\n",
        "# for name, param in agent.tcost.named_parameters():\n",
        "# # # for name, param in agent.named_parameters():\n",
        "# #     # print(name, param.requires_grad)\n",
        "#     print(name, param)\n",
        "\n",
        "for name, param in agent.tcost.named_parameters(): print(param.data)\n",
        "\n",
        "# print(agent.tcost.1.weight.data)\n",
        "\n",
        "# print(agent.tcost.named_parameters()['tcost.1.weight'])\n",
        "\n",
        "# print(vars(agent.jepa.exp.named_parameters()['exp.1.weight']))"
      ],
      "metadata": {
        "id": "49RERFWFMgA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "UEH1P802JkHU",
        "outputId": "c767bf3f-aa2c-43d8-e758-ad8e2b6fa14d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'state' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2eab810ffe5c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# sx = agent.jepa.enc(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# dim_a, dim_z = 3, 8\n",
        "# batch, T = 4,6\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_a),device=device))\n",
        "# torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "# dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# x = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "# z = nn.Parameter(torch.zeros((batch, T, dim_z),device=device))\n",
        "# torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# state = torch.zeros((1, 3,64,64))\n",
        "# # state = torch.rand((1, 3,64,64), device=device)\n",
        "# sx = agent.jepa.enc(state)\n",
        "\n",
        "act = agent([state], k=4)\n",
        "# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\n",
        "# loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "# print(loss,c)\n",
        "# print(lact, lh0, lx, lz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "f69906ff-bf77-448a-fc6b-ec3cff8edc74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH\n",
            "From (redirected): https://drive.google.com/uc?id=1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH&confirm=t&uuid=267c400a-6403-495f-9130-a7993fe833f5\n",
            "To: /content/agentoptim.pkl\n",
            "100% 28.1M/28.1M [00:01<00:00, 24.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ\n",
            "From (redirected): https://drive.google.com/uc?id=1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ&confirm=t&uuid=4cf6f6da-1859-468e-9048-203f7badaa79\n",
            "To: /content/buffergo.pkl\n",
            "100% 1.80G/1.80G [00:34<00:00, 51.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1 gru3 tcost1\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2 gru3 tcost1\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4 gru1 tcost1 drop\n",
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 1UDgNtFsWGAhvqR9lwA0QbMLhUtmip4ne -O agentoptim.pkl # M1 agentoptimgru3tcost1\n",
        "# !gdown 1-0oc6yucS5JXLHX1zqbYe3NTVMuhP_5r -O agentoptim.pkl # A2 agentoptim25251c25z3\n",
        "# !gdown 1U1CuCU1FugkrzPXsvTPpIX-wzWz6szl2 -O agentoptim.pkl # T4 agentoptimargm\n",
        "# !gdown 1CWZAtiEwSnglClJbq2LJTYlKhPN10gfo -O agentoptim.pkl # S3 agentoptimargm\n",
        "# !gdown 1XAbr6l1pCmcUCKR6kYlQ_dSDsOBqRg_j -O agentoptim.pkl # B2 argm2search2\n",
        "# !gdown 1UkQuf-IC2LYErSapkF6rZM1dv3svGI5P -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1-4sNf6mINCiD5YsBdQvCrlyqzzfS64si -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1MV9Qj_53Vu6wpe7nOFn47M5vDj7F7-gv -O agentoptim.pkl # S3 agentoptimargm2\n",
        "# !gdown 1--1Vl3337zugQng-j1qbptFY8EvhZA-T -O agentoptim.pkl # T4 agentoptimargm3 online\n",
        "# !gdown 1XHFBVPSH4T4FpUOBKN8X20xDQLNmL7go -O agentoptim.pkl # M1 agentoptimargm4\n",
        "!gdown 1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH -O agentoptim.pkl # B2 agentoptimargm4\n",
        "\n",
        "# !gdown 1sCW9uvcdCJkCH5HQDdISLws5rMvmkmFR -O all_sd.pkl # M1 all_sd\n",
        "\n",
        "import pickle\n",
        "# !gdown 1j9hOq8_752duPB0PMYUJqabNvYoGLysX -O buffer512down.pkl # S\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "# with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB -O buffergo.pkl # S3\n",
        "# !gdown 1egXy0t_kn0M0oL6sbwixoVr7bqMfcB8j -O buffergo.pkl # T4\n",
        "!gdown 1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ -O buffergo.pkl # B2\n",
        "with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "b4fdeae7-0b93-4fad-ef09-d24e9a705d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-a75177a6932e>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "# with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptimargm4.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# modelsd = transfer_sd(agent.state_dict(), modelsd)\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "# # optimsd = transfer_optim(agent.state_dict(), modelsd, optim.state_dict(), optimsd)\n",
        "optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = torch.load(folder+'all_sd.pkl', map_location=device)\n",
        "# # all_sd = torch.load('all_sd.pkl', map_location=device)\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in all_sd.items())\n",
        "# allsd = {}\n",
        "# for (k, v) in all_sd.items():\n",
        "#     try: allsd[convert[k]] = v\n",
        "#     except Exception as e: print('dict err', e)\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# tgt_sd = load_sd(agent.state_dict(), allsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# for i, (k,v) in enumerate(modelsd.items()):\n",
        "# for i, (k,v) in enumerate(agent.state_dict().items()):\n",
        "#     print(i,k,v.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# checkpoint = {'model': agentsd, 'optimizer': optim.state_dict(),}\n",
        "checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "torch.save(checkpoint, folder+'agentoptimargm4.pkl')\n",
        "# torch.save(checkpoint, 'agentoptim.pkl')\n",
        "\n",
        "# all_sd = {}\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# all_sd = store_sd(all_sd, agentsd)\n",
        "# # torch.save(all_sd, 'all_sd.pkl')\n",
        "# torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 100):] for episode in cleaned]\n",
        "        random.shuffle(cleaned)\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "\n",
        "    # def pop_unif(self, buffer_, n=3):\n",
        "    #     buffer_.pop(random.randrange(len(buffer_)))\n",
        "    #     return buffer_\n",
        "\n",
        "# while len(train_data.data)>10000:\n",
        "#     buffer.pop(random.randrange(len(buffer)))\n",
        "#     train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True)\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # # [3,T,batch]\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "1e3fpbtNOiz1",
        "outputId": "ad4cd2d2-cc43-429f-8e85-2553a7fedb7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.,  0., -1., -1., -1.,  0.,  0., -1., -1.,  0.])\n",
            "tensor([ 0., -1., -1., -1., -1., -1., -1.,  0.,  0., -1.])\n",
            "tensor([-1.,  0., -1., -1.,  0., -1.,  0., -1., -1.,  0.])\n",
            "tensor([ 0., -1., -1., -1., -1., -1.,  0., -1., -1., -1.])\n",
            "tensor([-4.9740e-01, -5.0000e-01, -5.0000e-01, -4.7296e-01, -5.0000e-01,\n",
            "        -2.4656e-01, -1.0812e-04, -5.0000e-01, -5.0000e-01, -5.0000e-01])\n",
            "tensor([-0.2531, -0.5000, -0.3849, -0.2154, -0.5000, -0.5000, -0.5000, -0.0015,\n",
            "        -0.3754, -0.0238])\n",
            "tensor([-4.7640e-01, -9.2827e-08, -9.5014e-03, -1.9512e-02, -5.0000e-01,\n",
            "        -8.8051e-07, -4.9137e-01, -5.0000e-01, -1.6556e-01, -1.4720e-04])\n",
            "tensor([-0.3327, -0.5000, -0.5000, -0.0696, -0.5000, -0.5000, -0.0008, -0.4938,\n",
            "        -0.5000, -0.5000])\n",
            "tensor(0.8314, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor(0.3157)\n",
            "tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1])\n",
            "tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0])\n",
            "reward, pred tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]) tensor([-4.9740e-01, -4.7296e-01, -3.8495e-01, -2.1545e-01, -2.3823e-02,\n",
            "        -4.7640e-01, -9.5014e-03, -1.9512e-02, -8.8051e-07, -1.6556e-01,\n",
            "        -6.9621e-02, -4.9385e-01])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "ctrain_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    ctrain_data=list(zip(state,reward))\n",
        "    ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# images, labels = images.to(device), labels.to(device)\n",
        "batch=40\n",
        "images, labels = images[:batch], labels[:batch]\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range(len(labels)//10):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# # try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch, agent.d_model), device=device)\n",
        "    # h0 = torch.empty((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device)\n",
        "    # torch.nn.init.xavier_normal_(h0)\n",
        "    sy = agent.jepa.enc(images.to(device)) # [batch_size, d_model]\n",
        "    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "    pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "\n",
        "    # print(pred)\n",
        "    for x in range(len(pred)//10):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(agent.tcost.loss(syh0, labels.to(device)).squeeze(-1))\n",
        "print(F.mse_loss(labels, pred))\n",
        "\n",
        "# torch.where(abs(labels- pred)>0.5,1,0)\n",
        "for x in range(len(pred)//10):\n",
        "    print(torch.where(abs(labels- pred)>0.5,1,0)[10*x:10*x+10])\n",
        "\n",
        "mask = torch.where(abs(labels- pred)>0.5,1,0).bool()\n",
        "print(\"reward, pred\", labels[mask].data, pred[mask].data)\n",
        "try: imshow(torchvision.utils.make_grid(images[mask], nrow=10))\n",
        "except ZeroDivisionError: pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for b, Sar in enumerate(train_loader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "    state, action, reward = Sar\n",
        "    # print(state.shape, action.shape, reward.shape) # [64, 50, 3, 64, 64]) torch.Size([64, 50]) torch.Size([64, 50]\n",
        "    # if b>3:break\n",
        "    bptt=25\n",
        "    for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "        # for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "        imshow(torchvision.utils.make_grid(st[0].cpu(), nrow=10))\n",
        "        print(rwd[0])\n",
        "\n",
        "# op = iter(train_loader)\n",
        "# Sar = next(op)\n",
        "# state, action, reward = Sar\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(state[0].cpu(), nrow=10))\n",
        "# print(reward[0])\n"
      ],
      "metadata": {
        "id": "Pc3mm6al-KCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "s = torch.arange(20).reshape(2,10,1) # [batch,seq,d_model]\n",
        "# print(s)\n",
        "for la in torch.split(s, 7, dim=1): # [batch,bptt,d_model]\n",
        "    # print(la)\n",
        "    print(la.shape)\n",
        "    for a in la.permute(1,0,2): # [batch,d_model]\n",
        "        print(a)\n"
      ],
      "metadata": {
        "id": "mL-RrKsu_o9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viimAIpYSJq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70264497-acd0-4475-e627-1be37d7ec960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5161, 0.4839],\n",
            "        [0.4553, 0.5447],\n",
            "        [0.6737, 0.3263],\n",
            "        [0.4455, 0.5545],\n",
            "        [0.2712, 0.7288],\n",
            "        [0.6653, 0.3347],\n",
            "        [0.5739, 0.4261],\n",
            "        [0.3988, 0.6012],\n",
            "        [0.5869, 0.4131],\n",
            "        [0.5607, 0.4393]])\n",
            "tensor(0.5933)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "labels = torch.tensor([0])\n",
        "# pred = torch.tensor([[.999,.001]])\n",
        "pred = torch.tensor([[1.,0.]])\n",
        "pred = torch.tensor([[5.,-5.]])\n",
        "# pred = torch.tensor([[.5,.5]])\n",
        "# pred = torch.tensor([[1/a, 1/(1-a)]])\n",
        "# pred = torch.tensor([[1/(1-a), 1/a]])\n",
        "# print(F.mse_loss(labels, pred))\n",
        "\n",
        "pred = torch.rand(10,2)\n",
        "pred = nn.Softmax(dim=-1)(pred)\n",
        "print(pred)\n",
        "labels = torch.where(torch.rand(10)>0.5,1,0)\n",
        "\n",
        "\n",
        "a=10\n",
        "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)]))\n",
        "print(loss_fn(pred, labels))\n",
        "\n",
        "# print((pred@torch.log(pred).T).sum())\n",
        "# print(pred,torch.log(pred).T)\n",
        "\n",
        "# 0.6931\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# optim = torch.optim.SGD(agent.parameters(), 1e-1, momentum=0.9, dampening=0, weight_decay=0)\n",
        "# print(optim.param_groups[0][\"lr\"])\n",
        "# print(optim)\n",
        "optim.param_groups[0][\"lr\"] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OksdjCeJYpYh",
        "outputId": "25a80894-a975-4195-cf07-8b4d8dea845b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n",
            "<ipython-input-14-87103bcf1e3b>:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsy 2.62890625 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-87103bcf1e3b>:215: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.7057946920394897 tensor(21, device='cuda:0')\n",
            "lsy 4.140625 0.0\n",
            "pred tensor([-0.3794, -0.2063, -0.1946, -0.0493, -0.1259, -0.0566, -0.2690, -0.2429,\n",
            "        -0.4114, -0.3547, -0.4802, -0.4761, -0.5000, -0.4995, -0.4983, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.680679202079773 tensor(19, device='cuda:0')\n",
            "lsy 2.48828125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4978, -0.5000, -0.5000, -0.5000, -0.4985,\n",
            "        -0.5000, -0.4985, -0.5000, -0.5000, -0.5000, -0.5000, -0.4988, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4983,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6522976160049438 tensor(12, device='cuda:0')\n",
            "lsy 2.63671875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4983, -0.5000, -0.4966, -0.5000, -0.4990,\n",
            "        -0.5000, -0.5000, -0.4939, -0.5000, -0.4983, -0.4954, -0.4998, -0.4995,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.4995, -0.4976,\n",
            "        -0.4995], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6681057214736938 tensor(18, device='cuda:0')\n",
            "lsy 2.640625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.4934,\n",
            "        -0.4988, -0.5000, -0.4951, -0.5000, -0.4849, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4983, -0.5000, -0.5000, -0.4998, -0.4929, -0.5000,\n",
            "        -0.4939], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6782017350196838 tensor(16, device='cuda:0')\n",
            "lsy 2.408203125 0.0\n",
            "pred tensor([-5.0000e-01, -4.9146e-01, -5.0000e-01, -5.0000e-01, -4.9463e-01,\n",
            "        -4.9805e-01, -4.6533e-01, -4.5190e-01, -4.8071e-01, -4.5337e-01,\n",
            "        -4.7852e-01, -3.9331e-01, -3.3350e-01, -4.0796e-01, -4.2725e-01,\n",
            "        -2.6050e-01, -5.9605e-08, -6.5565e-07, -4.1723e-07, -1.3411e-05,\n",
            "         0.0000e+00, -5.9605e-08, -2.9802e-07, -4.1723e-07, -5.9605e-08],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6895274519920349 tensor(14, device='cuda:0')\n",
            "lsy 2.595703125 0.0\n",
            "pred tensor([-5.0068e-06, -4.1723e-07, -1.5974e-05, -1.1921e-07, -2.3842e-07,\n",
            "        -1.7881e-07, -5.3644e-07, -2.3663e-05, -5.9605e-08, -2.8849e-05,\n",
            "         0.0000e+00, -7.7486e-07,  0.0000e+00, -1.8835e-05, -1.1742e-05,\n",
            "        -9.3520e-05, -3.3975e-06, -5.0843e-05,  0.0000e+00, -1.0133e-06,\n",
            "        -6.2048e-05, -2.3663e-05, -3.5763e-07, -2.4974e-05,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6574261784553528 tensor(12, device='cuda:0')\n",
            "lsy 2.59765625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4995, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4895, -0.3828, -0.3391, -0.3867, -0.4912, -0.5000,\n",
            "        -0.3682, -0.4905, -0.4927, -0.3997, -0.4548, -0.4319, -0.4150, -0.2079,\n",
            "        -0.4438], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6808643937110901 tensor(15, device='cuda:0')\n",
            "lsy 2.466796875 0.0\n",
            "pred tensor([-3.7793e-01, -2.4072e-01, -2.8735e-01, -4.3945e-02, -1.0565e-01,\n",
            "        -1.6108e-03, -4.3365e-02, -2.5903e-01, -6.6772e-02, -8.5205e-02,\n",
            "        -1.4185e-01, -6.8545e-06, -1.4710e-04, -4.6110e-04, -2.1148e-04,\n",
            "        -1.0061e-03, -4.2319e-05, -9.0003e-06, -7.4565e-05, -2.3592e-04,\n",
            "        -7.9041e-03, -1.9006e-01, -1.0208e-02, -6.9031e-02, -1.7102e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6507138013839722 tensor(10, device='cuda:0')\n",
            "lsy 2.541015625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4937, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4983, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6725542545318604 tensor(8, device='cuda:0')\n",
            "lsy 2.853515625 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9756e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9683e-01, -5.0000e-01,\n",
            "        -2.1482e-04, -6.6948e-04, -1.3931e-02, -1.6327e-03, -3.7956e-03,\n",
            "        -1.4389e-02, -1.1545e-04, -7.2899e-03, -1.8311e-01, -1.0077e-01,\n",
            "        -1.7114e-01, -1.5173e-01, -1.8274e-01, -1.3770e-01, -2.8656e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.7030583620071411 tensor(23, device='cuda:0')\n",
            "lsy 2.466796875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4968, -0.4988, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6594790816307068 tensor(12, device='cuda:0')\n",
            "lsy 2.212890625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.4990, -0.5000,\n",
            "        -0.4976, -0.4983, -0.4995, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6702623963356018 tensor(11, device='cuda:0')\n",
            "17\n",
            "lsy 2.240234375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4915, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4990, -0.5000, -0.5000, -0.5000, -0.5000, -0.1069,\n",
            "        -0.0657, -0.3748, -0.0324, -0.0673, -0.1086, -0.2537, -0.1854, -0.0240,\n",
            "        -0.0958], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6740644574165344 tensor(14, device='cuda:0')\n",
            "lsy 3.09765625 0.0\n",
            "pred tensor([-0.0139, -0.0017, -0.0078, -0.0779, -0.0027, -0.0329, -0.0039, -0.0283,\n",
            "        -0.0278, -0.0039, -0.0289, -0.0134, -0.0391, -0.0105, -0.0015, -0.0063,\n",
            "        -0.1245, -0.0649, -0.0066, -0.0833, -0.1647, -0.1390, -0.0042, -0.1342,\n",
            "        -0.1962], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6441243290901184 tensor(10, device='cuda:0')\n",
            "lsy 2.375 0.0\n",
            "pred tensor([-0.3254, -0.4529, -0.1987, -0.4617, -0.4443, -0.4790, -0.4932, -0.3203,\n",
            "        -0.5000, -0.5000, -0.2988, -0.5000, -0.5000, -0.4995, -0.4976, -0.5000,\n",
            "        -0.5000, -0.4905, -0.4556, -0.4766, -0.4709, -0.4861, -0.5000, -0.4441,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6588488817214966 tensor(15, device='cuda:0')\n",
            "lsy 2.84765625 0.0\n",
            "pred tensor([-5.0000e-01, -4.9585e-01, -1.4186e-05, -1.7881e-04, -4.2305e-03,\n",
            "        -1.8969e-03, -1.0986e-02, -1.1325e-06, -1.1665e-02, -2.3096e-01,\n",
            "        -3.4771e-03, -9.7942e-04, -7.5340e-03, -2.4170e-02, -1.3718e-02,\n",
            "        -4.0603e-04, -1.2123e-02, -2.1572e-03, -3.7289e-03, -4.6134e-05,\n",
            "        -2.9816e-02, -6.1569e-03, -1.9922e-01, -5.4121e-04, -3.3474e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6289600729942322 tensor(14, device='cuda:0')\n",
            "lsy 2.443359375 0.0\n",
            "pred tensor([-5.0000e-01, -1.4893e-01, -4.6338e-01, -4.9683e-01, -3.6499e-01,\n",
            "        -4.9170e-01, -3.4155e-01, -5.0000e-01, -4.9829e-01, -4.9756e-01,\n",
            "        -4.9707e-01, -5.0000e-01, -4.9341e-01, -4.9634e-01, -1.8494e-01,\n",
            "        -5.1856e-05, -4.1485e-05, -1.2100e-05, -1.5533e-04, -2.5821e-04,\n",
            "        -2.1744e-03, -3.2187e-04, -2.1744e-03, -1.7524e-05, -1.3687e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.678569495677948 tensor(18, device='cuda:0')\n",
            "lsy 2.93359375 0.0\n",
            "pred tensor([-1.9073e-06, -2.7039e-02, -1.3684e-01, -1.0443e-03, -2.9411e-03,\n",
            "        -1.6284e-04, -4.7088e-06, -5.2429e-02, -3.7289e-03, -3.2471e-02,\n",
            "        -3.3894e-03, -4.0474e-03, -3.0441e-02, -3.5114e-03, -2.8137e-02,\n",
            "        -1.7212e-01, -1.0908e-05, -4.8950e-02, -6.4888e-03, -4.5746e-02,\n",
            "        -5.4703e-03, -4.0576e-01, -1.8018e-01, -5.3253e-02, -8.5815e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6850036978721619 tensor(18, device='cuda:0')\n",
            "lsy 3.494140625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4990,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4968, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4956, -0.0558, -0.0216, -0.1437, -0.1870, -0.4272,\n",
            "        -0.4946], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6533316969871521 tensor(15, device='cuda:0')\n",
            "lsy 3.53125 0.0\n",
            "pred tensor([-4.9463e-01, -5.0000e-01, -5.0000e-01, -4.9512e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.8389e-01, -4.8633e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.7461e-01, -5.0000e-01, -1.1867e-04, -1.6775e-03, -4.3511e-06,\n",
            "        -2.5034e-06,  0.0000e+00,  0.0000e+00, -2.2650e-05, -1.8387e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6656424403190613 tensor(14, device='cuda:0')\n",
            "lsy 3.275390625 0.0\n",
            "pred tensor([-1.8477e-06, -9.6130e-03, -6.6414e-03, -8.4305e-03, -3.2306e-05,\n",
            "        -1.4901e-05, -4.4703e-06, -3.7498e-03, -1.9547e-02, -3.9339e-06,\n",
            "        -5.6922e-05, -3.1638e-04, -6.5565e-07, -2.0752e-03, -2.2125e-03,\n",
            "        -4.9162e-04, -1.0967e-05, -4.0627e-03, -3.2306e-05, -2.1219e-05,\n",
            "        -2.9945e-04, -1.7271e-03, -2.7588e-01, -1.0208e-02, -9.3918e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6768747568130493 tensor(16, device='cuda:0')\n",
            "lsy 3.76171875 0.0\n",
            "pred tensor([-0.1185, -0.0157, -0.0154, -0.1526, -0.0796, -0.3921, -0.4226, -0.3450,\n",
            "        -0.4829, -0.4912, -0.3831, -0.4446, -0.4578, -0.4441, -0.4473, -0.4907,\n",
            "        -0.5000, -0.4832, -0.5000, -0.4995, -0.4653, -0.4937, -0.5000, -0.3320,\n",
            "        -0.4651], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6493576169013977 tensor(12, device='cuda:0')\n",
            "lsy 3.828125 0.0\n",
            "pred tensor([-8.3506e-05, -5.6934e-04,  0.0000e+00, -3.4511e-05, -5.5194e-05,\n",
            "        -2.3317e-04, -7.4646e-02, -1.0490e-05, -3.3245e-03, -1.0114e-01,\n",
            "        -6.5269e-03, -1.3709e-04, -1.2225e-01, -1.4148e-01, -9.3079e-02,\n",
            "        -6.7871e-02, -8.4290e-02, -9.5093e-02, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6518637537956238 tensor(15, device='cuda:0')\n",
            "lsy 3.29296875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.4851, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4993, -0.4995, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4985, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6867010593414307 tensor(19, device='cuda:0')\n",
            "lsy 3.00390625 0.0\n",
            "pred tensor([-0.2095, -0.0643, -0.3311, -0.4150, -0.1731, -0.4651, -0.4783, -0.3718,\n",
            "        -0.4875, -0.4504, -0.4937, -0.4600, -0.3809, -0.4866, -0.3643, -0.3293,\n",
            "        -0.2971, -0.4050, -0.4631, -0.4771, -0.4590, -0.4995, -0.4761, -0.4329,\n",
            "        -0.3413], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6660124063491821 tensor(17, device='cuda:0')\n",
            "lsy 2.833984375 0.0\n",
            "pred tensor([-0.1638, -0.1534, -0.0292, -0.1766, -0.1481, -0.1232, -0.0182, -0.2467,\n",
            "        -0.4155, -0.1907, -0.1641, -0.3247, -0.3711, -0.4241, -0.2264, -0.3296,\n",
            "        -0.3718, -0.4729, -0.4929, -0.2415, -0.1110, -0.4248, -0.4724, -0.5000,\n",
            "        -0.4614], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6644376516342163 tensor(16, device='cuda:0')\n",
            "lsy 3.77734375 0.0\n",
            "pred tensor([-0.0024, -0.4932, -0.5000, -0.4890, -0.4978, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4983, -0.5000, -0.5000, -0.4480, -0.4919, -0.4712,\n",
            "        -0.5000, -0.4949, -0.4370, -0.4846, -0.5000, -0.4670, -0.4736, -0.4961,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6361138224601746 tensor(14, device='cuda:0')\n",
            "lsy 3.3359375 0.0\n",
            "pred tensor([-0.4988, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4988], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6607603430747986 tensor(14, device='cuda:0')\n",
            "lsy 2.552734375 0.0\n",
            "pred tensor([-0.1316, -0.0166, -0.0862, -0.0059, -0.1418, -0.2074, -0.2251, -0.2366,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4983, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6762859225273132 tensor(16, device='cuda:0')\n",
            "lsy 2.55078125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6570271849632263 tensor(11, device='cuda:0')\n",
            "lsy 2.681640625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4993, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.1056,\n",
            "        -0.0107, -0.3977, -0.0766, -0.4421, -0.0856, -0.4990, -0.0692, -0.1422,\n",
            "        -0.2971], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.691490650177002 tensor(17, device='cuda:0')\n",
            "lsy 2.140625 0.0\n",
            "pred tensor([-0.3694, -0.0769, -0.2274, -0.2313, -0.4209, -0.2080, -0.4810, -0.2627,\n",
            "        -0.3264, -0.4900, -0.1947, -0.4546, -0.4465, -0.4626, -0.3994, -0.2581,\n",
            "        -0.1088, -0.2886, -0.1958, -0.4739, -0.4783, -0.2261, -0.2197, -0.4429,\n",
            "        -0.1643], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6525279879570007 tensor(7, device='cuda:0')\n",
            "lsy 2.3359375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4480, -0.4663, -0.4670, -0.1404, -0.2908, -0.4951,\n",
            "        -0.4299, -0.4204, -0.3779, -0.4939, -0.2930, -0.3599, -0.0714, -0.3865,\n",
            "        -0.3269, -0.2271, -0.3748, -0.4656, -0.3169, -0.2257, -0.2756, -0.4573,\n",
            "        -0.4868], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.686545193195343 tensor(12, device='cuda:0')\n",
            "lsy 2.37109375 0.0\n",
            "pred tensor([-0.4893, -0.4963, -0.4695, -0.5000, -0.4858, -0.5000, -0.4944, -0.5000,\n",
            "        -0.5000, -0.4934, -0.4609, -0.5000, -0.5000, -0.4451, -0.5000, -0.4905,\n",
            "        -0.4927, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4978, -0.4998,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6736049056053162 tensor(9, device='cuda:0')\n",
            "lsy 2.20703125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4880, -0.4988, -0.4912, -0.5000, -0.4995,\n",
            "        -0.5000, -0.4995, -0.4995, -0.5000, -0.5000, -0.4998, -0.4961, -0.5000,\n",
            "        -0.5000, -0.4971, -0.4988, -0.4963, -0.4976, -0.5000, -0.4963, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6757597327232361 tensor(9, device='cuda:0')\n",
            "lsy 2.564453125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6709461808204651 tensor(7, device='cuda:0')\n",
            "lsy 2.4296875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4954, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6703471541404724 tensor(10, device='cuda:0')\n",
            "lsy 3.044921875 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -1.7345e-05, -7.1704e-05, -2.9802e-06,\n",
            "        -5.0068e-06, -3.6180e-05, -1.2934e-04, -3.6478e-05, -4.2975e-05,\n",
            "        -9.5367e-07, -2.9802e-07, -4.4584e-04, -4.1723e-06,  0.0000e+00,\n",
            "        -1.2636e-05,  0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.7054554224014282 tensor(8, device='cuda:0')\n",
            "lsy 3.689453125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4993, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6764976978302002 tensor(9, device='cuda:0')\n",
            "lsy 3.169921875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.673015832901001 tensor(10, device='cuda:0')\n",
            "lsy 3.51953125 0.0\n",
            "pred tensor([-1.3113e-06, -6.4754e-04, -1.2817e-01, -3.3245e-03, -1.1511e-01,\n",
            "        -4.2572e-03, -4.0932e-03, -8.2092e-02, -7.4530e-04, -3.4912e-02,\n",
            "        -1.4305e-06, -9.3262e-02, -1.0099e-03, -1.9872e-04, -6.7949e-06,\n",
            "        -1.3399e-04,  0.0000e+00, -2.7537e-05, -4.0283e-02, -6.3896e-03,\n",
            "        -5.3596e-04, -3.4937e-01, -2.2546e-01, -4.8022e-01, -4.9487e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6468691229820251 tensor(10, device='cuda:0')\n",
            "lsy 3.244140625 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.1187e-05, -4.9934e-03,  0.0000e+00,\n",
            "        -1.8096e-04, -4.7088e-06, -3.4118e-04, -1.6606e-04, -1.7881e-07,\n",
            "        -1.5430e-03, -8.9359e-04, -5.0316e-03, -1.7405e-04, -1.1772e-04,\n",
            "        -2.3880e-03, -1.3113e-06, -2.1315e-04, -9.1791e-06, -3.1910e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6339476704597473 tensor(6, device='cuda:0')\n",
            "lsy 3.216796875 0.0\n",
            "pred tensor([-1.4661e-01, -3.5763e-04, -1.3208e-01, -7.5500e-02, -1.2335e-01,\n",
            "        -8.0444e-02, -1.0612e-02, -2.0312e-01, -6.4240e-03, -2.5101e-02,\n",
            "        -2.0618e-01, -1.7334e-01, -8.5510e-02, -4.8145e-01, -2.9883e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6920042037963867 tensor(13, device='cuda:0')\n",
            "lsy 3.06640625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4875, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4941, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6378814578056335 tensor(12, device='cuda:0')\n",
            "lsy 2.939453125 0.0\n",
            "pred tensor([-5.0000e-01, -4.9902e-01, -1.3494e-04, -3.4161e-03, -2.5959e-03,\n",
            "        -1.1921e-07, -2.1225e-02, -1.7881e-06, -3.1686e-04, -1.6647e-02,\n",
            "        -3.8743e-06, -2.3468e-02, -3.5346e-05, -2.1458e-05, -1.5974e-05,\n",
            "        -5.1260e-06, -8.5754e-03, -8.0338e-03, -2.4796e-05, -7.3314e-06,\n",
            "        -1.1456e-04, -2.0266e-06, -2.9251e-02, -5.3787e-04, -6.3515e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6367990970611572 tensor(8, device='cuda:0')\n",
            "lsy 2.94140625 0.0\n",
            "pred tensor([-0.3264, -0.0046, -0.0410, -0.2262, -0.0341, -0.0308, -0.0023, -0.3562,\n",
            "        -0.0194, -0.0999, -0.0896, -0.4937, -0.4695, -0.4695, -0.4937, -0.4629,\n",
            "        -0.4941, -0.3721, -0.3774, -0.4050, -0.4734, -0.3911, -0.5000, -0.4656,\n",
            "        -0.4951], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6487703323364258 tensor(10, device='cuda:0')\n",
            "lsy 2.8828125 0.0\n",
            "pred tensor([-0.0456, -0.0006, -0.2228, -0.4666, -0.4873, -0.5000, -0.5000, -0.4954,\n",
            "        -0.5000, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6572644114494324 tensor(11, device='cuda:0')\n",
            "lsy 2.708984375 0.0\n",
            "pred tensor([-5.0000e-01, -6.7825e-03, -6.9141e-06, -2.8431e-05, -1.3275e-02,\n",
            "        -3.2187e-05, -1.3113e-06, -4.3988e-05, -2.7451e-02, -2.0444e-05,\n",
            "        -1.8909e-01, -1.0691e-03, -2.1191e-03, -1.2827e-04, -2.3317e-04,\n",
            "        -4.6295e-02, -3.1525e-02, -1.4410e-03, -7.8535e-04, -4.7028e-05,\n",
            "        -8.7023e-06, -5.8563e-02, -3.6621e-04, -9.8953e-03, -1.7776e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6944648027420044 tensor(14, device='cuda:0')\n",
            "lsy 2.744140625 0.0\n",
            "pred tensor([-3.0460e-03, -3.5524e-05, -7.1526e-07, -1.1921e-06, -1.7262e-04,\n",
            "        -5.0068e-05, -2.1338e-05,  0.0000e+00, -2.9206e-05, -3.4690e-05,\n",
            "        -8.3447e-07, -3.9458e-05, -8.5235e-06, -1.5533e-04, -1.1921e-06,\n",
            "         0.0000e+00, -4.7684e-07, -8.2850e-05, -5.8937e-04, -1.7524e-05,\n",
            "        -8.4162e-05, -5.9009e-06, -9.4771e-06, -4.1175e-04, -1.3709e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6788200736045837 tensor(10, device='cuda:0')\n",
            "lsy 3.09765625 0.0\n",
            "pred tensor([ 0.0000e+00, -2.9802e-07, -1.1969e-01, -6.0380e-05, -5.9605e-08,\n",
            "        -9.4147e-03, -8.9407e-07, -6.8426e-05, -3.7789e-05, -4.3511e-06,\n",
            "        -1.4997e-04, -7.8678e-06, -3.5167e-06, -4.2796e-04, -1.2338e-04,\n",
            "        -1.2934e-05, -3.6359e-06, -5.8413e-06, -4.1318e-04, -7.8082e-06,\n",
            "        -8.3447e-07, -2.3723e-05, -2.1279e-05, -4.5300e-06, -1.6224e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6623865365982056 tensor(13, device='cuda:0')\n",
            "18\n",
            "lsy 3.09375 0.0\n",
            "pred tensor([ 0.0000e+00, -1.7881e-07, -5.9605e-08, -1.6534e-04, -3.9935e-05,\n",
            "        -1.1921e-07, -1.8477e-06, -3.3054e-03, -2.0844e-02,  0.0000e+00,\n",
            "        -2.8610e-06, -3.5763e-07, -1.1921e-07, -1.1027e-05, -1.9417e-03,\n",
            "        -6.9160e-03, -6.1393e-06, -7.5698e-06, -5.9605e-08, -2.0266e-06,\n",
            "        -7.7486e-07, -3.5763e-07, -1.2192e-02, -1.1921e-07, -4.2725e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6903032064437866 tensor(15, device='cuda:0')\n",
            "lsy 3.53125 0.0\n",
            "pred tensor([-4.9114e-05, -5.9605e-08, -5.8413e-06, -5.2452e-05, -8.1205e-04,\n",
            "        -2.0444e-05, -2.5768e-03, -3.6716e-03, -1.6384e-03, -1.5438e-05,\n",
            "        -2.0742e-04, -1.8001e-05, -1.4889e-04, -1.2934e-05, -5.9605e-07,\n",
            "        -1.6689e-06, -7.7486e-07, -1.6584e-03, -1.6212e-05, -2.5868e-05,\n",
            "        -2.2650e-06, -3.9339e-06, -6.0158e-03, -5.0735e-04, -3.3021e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6817324161529541 tensor(18, device='cuda:0')\n",
            "lsy 3.06640625 0.0\n",
            "pred tensor([-0.4924, -0.4961, -0.4995, -0.4990, -0.4995, -0.4946, -0.4646, -0.4949,\n",
            "        -0.5000, -0.4985, -0.4509, -0.4963, -0.4893, -0.4966, -0.3660, -0.4434,\n",
            "        -0.4946, -0.4038, -0.4978, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6911592483520508 tensor(15, device='cuda:0')\n",
            "lsy 3.10546875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.4995, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6608917117118835 tensor(16, device='cuda:0')\n",
            "lsy 2.892578125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4971, -0.4998, -0.5000, -0.4937, -0.4993, -0.5000,\n",
            "        -0.5000, -0.4998, -0.5000, -0.4978, -0.0052, -0.0199, -0.0232, -0.0022,\n",
            "        -0.0163, -0.0020, -0.2961, -0.3442, -0.4294, -0.4636, -0.5000, -0.4644,\n",
            "        -0.4998], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6748647689819336 tensor(19, device='cuda:0')\n",
            "lsy 2.828125 0.0\n",
            "pred tensor([-4.9902e-01, -5.0000e-01, -4.9316e-01, -5.0000e-01, -4.9829e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -9.1736e-02, -3.0632e-03, -1.8600e-02, -2.4512e-01, -1.4889e-04,\n",
            "        -1.9214e-01, -7.6437e-04, -1.3292e-05, -2.3687e-04, -1.0687e-01,\n",
            "         0.0000e+00, -9.6893e-03, -1.5613e-01, -4.3335e-02, -1.8477e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6596556305885315 tensor(12, device='cuda:0')\n",
            "lsy 3.04296875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4988, -0.4988, -0.4993, -0.5000, -0.4998, -0.5000,\n",
            "        -0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4995, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.620934784412384 tensor(9, device='cuda:0')\n",
            "lsy 3.083984375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.4998, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4966, -0.5000,\n",
            "        -0.5000, -0.4995, -0.5000, -0.5000, -0.5000, -0.4998, -0.5000, -0.4998,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6750951409339905 tensor(14, device='cuda:0')\n",
            "lsy 2.8203125 0.0\n",
            "pred tensor([-0.4983, -0.4941, -0.5000, -0.5000, -0.4985, -0.5000, -0.5000, -0.4944,\n",
            "        -0.4995, -0.5000, -0.4998, -0.4985, -0.4978, -0.4993, -0.4995, -0.4954,\n",
            "        -0.4988, -0.4978, -0.4976, -0.4966, -0.4998, -0.4944, -0.4846, -0.4995,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6597894430160522 tensor(15, device='cuda:0')\n",
            "lsy 3.388671875 0.0\n",
            "pred tensor([-0.4919, -0.4966, -0.4998, -0.4995, -0.4966, -0.4993, -0.4900, -0.4995,\n",
            "        -0.4995, -0.4983, -0.4988, -0.4998, -0.5000, -0.4993, -0.5000, -0.4983,\n",
            "        -0.5000, -0.5000, -0.4998, -0.5000, -0.4990, -0.4995, -0.5000, -0.5000,\n",
            "        -0.4985], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.671087920665741 tensor(15, device='cuda:0')\n",
            "lsy 2.296875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4961,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6507887840270996 tensor(13, device='cuda:0')\n",
            "lsy 2.490234375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -3.8354e-01, -2.0349e-01,\n",
            "        -3.7956e-03, -1.1384e-05, -3.0563e-02, -1.1688e-02, -1.2215e-02,\n",
            "        -4.8103e-03, -2.4890e-01, -4.4775e-04, -1.1200e-02, -1.5308e-01,\n",
            "        -1.5163e-03, -2.2449e-01, -2.6855e-01, -1.1115e-01, -3.8110e-01,\n",
            "        -6.4941e-02, -1.4990e-01, -7.5195e-02, -5.9601e-02, -6.2805e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6575256586074829 tensor(10, device='cuda:0')\n",
            "lsy 2.326171875 0.0\n",
            "pred tensor([-4.9927e-01, -5.0000e-01, -1.0449e-01, -3.9139e-03, -2.9926e-03,\n",
            "        -5.3215e-03, -6.3019e-03, -6.5804e-05, -4.9448e-04, -4.3854e-02,\n",
            "        -2.4365e-01, -3.8395e-03, -5.6915e-02, -2.8030e-02, -1.1719e-01,\n",
            "        -2.6657e-02, -1.1377e-03, -1.4111e-01, -3.8696e-01, -3.6304e-01,\n",
            "        -5.0000e-01, -4.7803e-01, -4.7461e-01, -4.9609e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6774079203605652 tensor(15, device='cuda:0')\n",
            "lsy 2.228515625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4993, -0.5000, -0.5000, -0.5000, -0.4966, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4541, -0.5000, -0.5000, -0.5000, -0.5000, -0.4966, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6591141223907471 tensor(18, device='cuda:0')\n",
            "lsy 2.435546875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.5000, -0.4988,\n",
            "        -0.5000, -0.4915, -0.4988, -0.4858, -0.4692, -0.4788, -0.4888, -0.2939,\n",
            "        -0.2295, -0.1567, -0.1808, -0.4568, -0.2316, -0.0955, -0.0096, -0.0580,\n",
            "        -0.1810], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.7102733254432678 tensor(21, device='cuda:0')\n",
            "lsy 2.388671875 0.0\n",
            "pred tensor([-0.2257, -0.0474, -0.1935, -0.2319, -0.2292, -0.0665, -0.2220, -0.4180,\n",
            "        -0.2869, -0.3762, -0.4458, -0.4790, -0.2338, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4971, -0.5000, -0.4966, -0.4575, -0.4995, -0.4990, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.669988751411438 tensor(11, device='cuda:0')\n",
            "lsy 2.111328125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4995, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4968, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6615473628044128 tensor(14, device='cuda:0')\n",
            "lsy 2.3828125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4851, -0.4939, -0.5000, -0.4998, -0.4995, -0.4958, -0.4995, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.648629903793335 tensor(11, device='cuda:0')\n",
            "lsy 2.4765625 0.0\n",
            "pred tensor([-2.1660e-04, -5.5432e-06, -2.0921e-05, -2.5034e-06, -1.6570e-05,\n",
            "        -1.9073e-05, -8.0185e-03, -7.7486e-07, -1.0948e-03, -6.5565e-07,\n",
            "        -2.8610e-06, -2.3849e-02, -2.5511e-04, -1.5088e-01, -7.3910e-06,\n",
            "        -8.5144e-03, -1.3574e-01, -2.9037e-02, -6.6040e-02, -1.5173e-01,\n",
            "        -4.7681e-01, -3.3472e-01, -3.6743e-01, -3.6841e-01, -4.8022e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6631597280502319 tensor(13, device='cuda:0')\n",
            "lsy 2.01953125 0.0\n",
            "pred tensor([-0.4973, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4902, -0.5000, -0.4998, -0.5000, -0.4924, -0.4944, -0.3789,\n",
            "        -0.3118, -0.3940, -0.2457, -0.0307, -0.0430, -0.0565, -0.0779, -0.1637,\n",
            "        -0.1627], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.655325174331665 tensor(16, device='cuda:0')\n",
            "lsy 2.166015625 0.0\n",
            "pred tensor([-0.5000, -0.2898, -0.0743, -0.3025, -0.3643, -0.4487, -0.4922, -0.4475,\n",
            "        -0.4573, -0.4189, -0.3879, -0.2766, -0.3464, -0.3386, -0.0156, -0.0920,\n",
            "        -0.2147, -0.1069, -0.0557, -0.1013, -0.3418, -0.2622, -0.1927, -0.4856,\n",
            "        -0.4688], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6753202676773071 tensor(12, device='cuda:0')\n",
            "lsy 2.880859375 0.0\n",
            "pred tensor([-0.4890, -0.4924, -0.4941, -0.4951, -0.5000, -0.4976, -0.4956, -0.5000,\n",
            "        -0.4966, -0.4780, -0.4973, -0.4600, -0.4753, -0.4326, -0.0241, -0.2676,\n",
            "        -0.0139, -0.0227, -0.1079, -0.0079, -0.0240, -0.0008, -0.0041, -0.1488,\n",
            "        -0.0018], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6553747057914734 tensor(16, device='cuda:0')\n",
            "lsy 2.59375 0.0\n",
            "pred tensor([-0.2842, -0.4924, -0.4600, -0.4980, -0.5000, -0.5000, -0.4761, -0.4985,\n",
            "        -0.4758, -0.4607, -0.4751, -0.4819, -0.4653, -0.4258, -0.3928, -0.4778,\n",
            "        -0.4861, -0.4729, -0.4585, -0.4871, -0.4812, -0.4795, -0.4614, -0.4536,\n",
            "        -0.4829], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6993694305419922 tensor(22, device='cuda:0')\n",
            "lsy 3.140625 0.0\n",
            "pred tensor([-0.4670, -0.4509, -0.4788, -0.4880, -0.4324, -0.4956, -0.4844, -0.4832,\n",
            "        -0.4319, -0.4866, -0.4976, -0.4746, -0.4648, -0.4377, -0.4492, -0.4966,\n",
            "        -0.4658, -0.4785, -0.4607, -0.4634, -0.4834, -0.4739, -0.4722, -0.4792,\n",
            "        -0.4768], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.7002106308937073 tensor(21, device='cuda:0')\n",
            "lsy 3.13671875 0.0\n",
            "pred tensor([-7.9298e-04, -1.1093e-02, -1.4544e-05, -1.2112e-03, -2.6054e-03,\n",
            "        -3.9749e-03, -4.8492e-02, -2.5073e-01, -4.2285e-01, -2.2278e-01,\n",
            "        -8.7708e-02, -5.4901e-02, -3.4241e-02, -3.8892e-01, -4.1235e-01,\n",
            "        -2.4829e-01, -4.1431e-01, -3.9160e-01, -4.9585e-01, -5.0000e-01,\n",
            "        -4.9707e-01, -4.9463e-01, -4.9878e-01, -4.9878e-01, -4.5215e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6583277583122253 tensor(21, device='cuda:0')\n",
            "lsy 3.31640625 0.0\n",
            "pred tensor([-0.4912, -0.4927, -0.4912, -0.4868, -0.4360, -0.4980, -0.4958, -0.4402,\n",
            "        -0.1990, -0.1326, -0.4556, -0.3242, -0.4875, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4993], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6589056253433228 tensor(16, device='cuda:0')\n",
            "lsy 3.1953125 0.0\n",
            "pred tensor([-0.4539, -0.4973, -0.4932, -0.4856, -0.4470, -0.4866, -0.4534, -0.4478,\n",
            "        -0.4944, -0.4990, -0.4988, -0.4673, -0.4946, -0.5000, -0.5000, -0.4995,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4863, -0.4612, -0.4805, -0.4966, -0.5000,\n",
            "        -0.4858], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.622847318649292 tensor(15, device='cuda:0')\n",
            "lsy 3.3125 0.0\n",
            "pred tensor([-4.9829e-01, -4.9243e-01, -5.0000e-01, -4.8535e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.7607e-01, -4.8999e-01,\n",
            "        -4.9854e-01, -5.0000e-01, -5.0000e-01, -4.9927e-01, -4.9927e-01,\n",
            "        -4.9951e-01, -4.8975e-01, -4.7778e-01, -6.3372e-04, -1.7071e-03,\n",
            "        -7.3528e-04, -1.4198e-04, -7.0740e-02, -1.3602e-04, -5.4932e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6518174409866333 tensor(23, device='cuda:0')\n",
            "lsy 3.283203125 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9658e-01, -3.6694e-01, -4.6802e-01, -2.2266e-01, -1.8835e-01,\n",
            "        -2.7100e-01, -2.1692e-01, -1.9543e-01, -3.8403e-01, -1.6150e-01,\n",
            "        -2.7734e-01, -4.6631e-01, -4.5801e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -9.6798e-04, -2.3973e-04, -9.1791e-06, -4.5598e-05, -1.1325e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6494258046150208 tensor(17, device='cuda:0')\n",
            "lsy 3.404296875 0.0\n",
            "pred tensor([-5.6624e-06, -2.5208e-02, -8.9943e-05, -4.7028e-02, -4.9365e-01,\n",
            "        -4.9731e-01, -5.0000e-01, -5.0000e-01, -4.9951e-01, -4.9902e-01,\n",
            "        -4.9927e-01, -5.0000e-01, -4.9341e-01, -5.0000e-01, -4.8218e-01,\n",
            "        -5.0000e-01, -4.9878e-01, -5.0000e-01, -4.9634e-01, -5.0000e-01,\n",
            "        -4.5532e-01, -4.9854e-01, -4.5239e-01, -4.7339e-01, -4.9902e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6648935675621033 tensor(22, device='cuda:0')\n",
            "lsy 2.984375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.4863, -0.5000,\n",
            "        -0.4814, -0.4998, -0.4990, -0.5000, -0.4985, -0.4988, -0.4990, -0.4888,\n",
            "        -0.5000, -0.4846, -0.4956, -0.4971, -0.5000, -0.5000, -0.4983, -0.4824,\n",
            "        -0.4998], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6793333292007446 tensor(23, device='cuda:0')\n",
            "lsy 2.794921875 0.0\n",
            "pred tensor([-0.4934, -0.4998, -0.5000, -0.4995, -0.4917, -0.4971, -0.4995, -0.4963,\n",
            "        -0.4619, -0.4998, -0.4973, -0.4873, -0.4995, -0.4880, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4929, -0.5000, -0.4658, -0.4888, -0.4521, -0.4736, -0.4517,\n",
            "        -0.0796], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.7038191556930542 tensor(26, device='cuda:0')\n",
            "lsy 3.474609375 0.0\n",
            "pred tensor([-0.0103, -0.1061, -0.0035, -0.1110, -0.0862, -0.4778, -0.1754, -0.1080,\n",
            "        -0.0219, -0.0158, -0.0280, -0.5000, -0.4878, -0.4858, -0.4329, -0.4824,\n",
            "        -0.4695, -0.4429, -0.4624, -0.4673, -0.3267, -0.4963, -0.4812, -0.4512,\n",
            "        -0.2917], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6640573143959045 tensor(20, device='cuda:0')\n",
            "lsy 2.146484375 0.0\n",
            "pred tensor([-0.4966, -0.3967, -0.4553, -0.5000, -0.4597, -0.4905, -0.3535, -0.4399,\n",
            "        -0.3882, -0.4587, -0.3975, -0.4895, -0.3809, -0.4956, -0.1943, -0.4646,\n",
            "        -0.4668, -0.2922, -0.4421, -0.3677, -0.3997, -0.4272, -0.2246, -0.4265,\n",
            "        -0.4685], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6681208610534668 tensor(24, device='cuda:0')\n",
            "lsy 1.3232421875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4980, -0.5000, -0.5000, -0.5000, -0.4705, -0.5000,\n",
            "        -0.4963, -0.4666, -0.4966, -0.5000, -0.5000, -0.5000, -0.5000, -0.4988,\n",
            "        -0.5000, -0.5000, -0.4741, -0.5000, -0.5000, -0.4932, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.7176738381385803 tensor(20, device='cuda:0')\n",
            "lsy 1.1015625 0.0\n",
            "pred tensor([-0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4932, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4958, -0.5000, -0.5000, -0.4019, -0.4778, -0.4231,\n",
            "        -0.3655], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6893807053565979 tensor(17, device='cuda:0')\n",
            "lsy 1.33984375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.4988, -0.4993, -0.4954, -0.5000,\n",
            "        -0.5000, -0.4802, -0.5000, -0.5000, -0.5000, -0.4998, -0.5000, -0.4927,\n",
            "        -0.4993, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4888, -0.4932,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6833227872848511 tensor(19, device='cuda:0')\n",
            "lsy 2.15234375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4971, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4702,\n",
            "        -0.4360, -0.4583, -0.2644, -0.1110, -0.2194, -0.1389, -0.4507, -0.4724,\n",
            "        -0.1575], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.670478105545044 tensor(15, device='cuda:0')\n",
            "19\n",
            "lsy 2.025390625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4915, -0.5000,\n",
            "        -0.4985, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4988, -0.5000, -0.4939, -0.4949, -0.4973, -0.4995, -0.4983,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6550478339195251 tensor(10, device='cuda:0')\n",
            "lsy 2.580078125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4998, -0.4993, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4978, -0.5000, -0.5000, -0.5000, -0.4985, -0.5000, -0.4990, -0.5000,\n",
            "        -0.4985, -0.4985, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6622657775878906 tensor(14, device='cuda:0')\n",
            "lsy 2.748046875 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -6.3629e-03, -3.7050e-04, -6.5002e-02,\n",
            "        -6.9857e-04, -9.3162e-05, -2.0859e-02, -3.6743e-02, -2.0587e-04,\n",
            "        -3.2776e-02, -9.4299e-03, -6.4993e-04, -7.9193e-03, -1.1467e-02,\n",
            "        -1.7548e-03, -9.6858e-05, -2.9802e-07, -2.8312e-05, -1.8120e-05,\n",
            "        -3.6430e-03, -5.2869e-05, -2.8610e-05, -4.5815e-03, -6.4373e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6678819060325623 tensor(15, device='cuda:0')\n",
            "lsy 2.890625 0.0\n",
            "pred tensor([-2.8061e-02, -2.2471e-05, -5.5194e-05, -3.9935e-05, -2.8014e-06,\n",
            "        -1.1325e-06, -8.9340e-03, -1.1377e-03, -5.5265e-04, -1.0862e-03,\n",
            "        -6.8092e-03, -1.2009e-02, -6.3229e-04, -2.9802e-07, -4.2496e-03,\n",
            "        -5.1856e-06, -6.7592e-05, -1.7881e-07,  0.0000e+00, -2.2233e-05,\n",
            "        -2.0337e-04, -1.0151e-04, -4.0710e-05, -1.4007e-05, -1.3709e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6617612838745117 tensor(13, device='cuda:0')\n",
            "lsy 2.75 0.0\n",
            "pred tensor([ 0.0000e+00, -5.9605e-08,  0.0000e+00, -5.7817e-06,  0.0000e+00,\n",
            "        -4.5300e-06, -6.9201e-05, -1.7130e-04, -4.1008e-04, -2.1648e-03,\n",
            "        -3.5346e-05, -2.7924e-02, -3.9506e-04, -7.7844e-05, -7.4341e-02,\n",
            "        -1.3769e-04, -1.0231e-02, -4.7302e-02, -1.0208e-02, -2.1027e-02,\n",
            "        -6.8245e-03, -1.0211e-01, -5.6648e-03, -3.4119e-02, -3.7659e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6450195908546448 tensor(13, device='cuda:0')\n",
            "lsy 2.873046875 0.0\n",
            "pred tensor([-0.0784, -0.2140, -0.0656, -0.1771, -0.0042, -0.0353, -0.0741, -0.0486,\n",
            "        -0.1041, -0.1252, -0.2844, -0.3110, -0.3835, -0.4136, -0.3623, -0.5000,\n",
            "        -0.4939, -0.4849, -0.4973, -0.4900, -0.4780, -0.4773, -0.4775, -0.4854,\n",
            "        -0.4966], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6765856146812439 tensor(16, device='cuda:0')\n",
            "lsy 2.814453125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4983, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.7099170684814453 tensor(17, device='cuda:0')\n",
            "lsy 2.5859375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9951e-01,\n",
            "        -5.0000e-01, -2.1315e-04, -2.9862e-05, -1.1325e-06, -5.3644e-07,\n",
            "        -1.3113e-06, -2.5749e-05, -3.1586e-02, -1.1093e-02, -1.0431e-04,\n",
            "        -2.3621e-02, -2.3842e-07, -3.9291e-03, -3.2568e-04, -9.4593e-05,\n",
            "        -4.4703e-05, -4.4703e-06, -1.7643e-05, -1.6451e-05, -6.1393e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6738346815109253 tensor(16, device='cuda:0')\n",
            "lsy 2.73046875 0.0\n",
            "pred tensor([-1.6489e-03, -2.6932e-02, -2.7612e-01, -1.1915e-04, -1.9153e-01,\n",
            "        -2.2461e-02, -8.0032e-03, -1.1421e-02, -1.6434e-02, -2.3819e-02,\n",
            "        -3.3813e-01, -1.0089e-01, -9.4938e-04, -5.9113e-02, -1.3245e-02,\n",
            "        -2.1631e-01, -1.7166e-02, -2.6054e-03, -4.4586e-02, -8.3069e-02,\n",
            "        -1.1673e-03, -1.6006e-02, -1.4008e-02, -2.2266e-01, -5.9753e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6927317380905151 tensor(21, device='cuda:0')\n",
            "lsy 2.728515625 0.0\n",
            "pred tensor([-0.0541, -0.0848, -0.0099, -0.3291, -0.0406, -0.1135, -0.0106, -0.0150,\n",
            "        -0.0105, -0.0955, -0.0198, -0.1576, -0.0005, -0.0497, -0.2448, -0.0628,\n",
            "        -0.3899, -0.0220, -0.0671, -0.2520, -0.4160, -0.2283, -0.0780, -0.3660,\n",
            "        -0.3491], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6348454356193542 tensor(11, device='cuda:0')\n",
            "lsy 2.240234375 0.0\n",
            "pred tensor([-3.2449e-04, -2.2590e-05, -6.0463e-04, -6.5565e-06, -1.7881e-07,\n",
            "        -5.1260e-06, -5.4240e-06, -3.7575e-03, -1.5192e-03, -5.2109e-03,\n",
            "        -4.5532e-02, -5.3644e-07, -1.4091e-04, -5.8365e-04, -1.0270e-04,\n",
            "        -1.7202e-04, -7.1168e-05, -1.1482e-02, -1.4317e-04, -5.0247e-05,\n",
            "        -5.8532e-05, -7.5623e-02, -1.2329e-01, -1.0841e-02, -6.6162e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.7033155560493469 tensor(14, device='cuda:0')\n",
            "lsy 2.59375 0.0\n",
            "pred tensor([-3.4424e-02, -9.5844e-04, -1.1330e-02, -1.6760e-01, -3.7231e-02,\n",
            "        -6.0852e-02, -1.0382e-01, -2.4586e-03, -6.8787e-02, -2.0027e-03,\n",
            "        -3.2187e-06, -7.8491e-02, -3.3021e-04, -1.7443e-03, -4.7684e-07,\n",
            "        -1.1620e-02, -1.0841e-02, -2.2720e-02, -2.9321e-01, -6.7688e-02,\n",
            "        -2.0935e-01, -8.1787e-02, -5.3024e-03, -4.3018e-01, -5.2704e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.735146701335907 tensor(21, device='cuda:0')\n",
            "lsy 2.376953125 0.0\n",
            "pred tensor([-0.3135, -0.1842, -0.4873, -0.4197, -0.5000, -0.5000, -0.4985, -0.5000,\n",
            "        -0.4995, -0.5000, -0.4983, -0.4727, -0.5000, -0.4995, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4900, -0.5000, -0.4995, -0.4985, -0.4937, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6453928351402283 tensor(12, device='cuda:0')\n",
            "lsy 2.259765625 0.0\n",
            "pred tensor([-0.4924, -0.5000, -0.5000, -0.0754, -0.2010, -0.1539, -0.0747, -0.3506,\n",
            "        -0.3386, -0.1406, -0.2189, -0.0460, -0.0931, -0.0106, -0.2598, -0.2009,\n",
            "        -0.3286, -0.2832, -0.4448, -0.2986, -0.3372, -0.4299, -0.4978, -0.4626,\n",
            "        -0.3635], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6908255219459534 tensor(15, device='cuda:0')\n",
            "lsy 2.279296875 0.0\n",
            "pred tensor([-0.4995, -0.4998, -0.5000, -0.5000, -0.5000, -0.4958, -0.5000, -0.5000,\n",
            "        -0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6595058441162109 tensor(13, device='cuda:0')\n",
            "lsy 2.0625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6382508873939514 tensor(7, device='cuda:0')\n",
            "lsy 2.203125 0.0\n",
            "pred tensor([-0.2437, -0.0708, -0.1304, -0.0846, -0.1208, -0.0330, -0.3867, -0.2634,\n",
            "        -0.4753, -0.4307, -0.4819, -0.4932, -0.4077, -0.4189, -0.3977, -0.4946,\n",
            "        -0.4583, -0.4805, -0.4902, -0.4985, -0.4949, -0.4973, -0.5000, -0.4958,\n",
            "        -0.4973], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6669067740440369 tensor(8, device='cuda:0')\n",
            "lsy 2.337890625 0.0\n",
            "pred tensor([-0.5000, -0.4956, -0.5000, -0.4971, -0.5000, -0.5000, -0.5000, -0.4949,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4951, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4900, -0.5000, -0.5000, -0.5000, -0.4917, -0.0024, -0.0332, -0.0817,\n",
            "        -0.0057], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6483309864997864 tensor(12, device='cuda:0')\n",
            "lsy 2.40625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.0374, -0.1323, -0.0308, -0.0014, -0.2206,\n",
            "        -0.3501, -0.4746, -0.4929, -0.0531, -0.2135, -0.1373, -0.4963, -0.0608,\n",
            "        -0.2382], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6459742784500122 tensor(15, device='cuda:0')\n",
            "lsy 2.775390625 0.0\n",
            "pred tensor([-0.0876, -0.4277, -0.3379, -0.4934, -0.2327, -0.4756, -0.3801, -0.1110,\n",
            "        -0.2993, -0.0554, -0.3269, -0.4961, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6708385944366455 tensor(14, device='cuda:0')\n",
            "lsy 2.466796875 0.0\n",
            "pred tensor([-3.0371e-01, -9.3689e-02, -2.3596e-01, -1.0889e-01, -5.1928e-04,\n",
            "        -7.1838e-02, -3.1052e-02, -9.3765e-03, -3.8892e-01, -6.8426e-05,\n",
            "        -6.9847e-03, -8.8644e-04, -1.9913e-03, -1.5430e-03, -1.7075e-02,\n",
            "        -1.2207e-03, -7.2060e-03, -3.9703e-02, -6.9499e-05, -4.6730e-03,\n",
            "        -1.2693e-03, -3.2842e-05, -1.8188e-02, -1.2207e-01, -1.2238e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6661669015884399 tensor(19, device='cuda:0')\n",
            "lsy 2.541015625 0.0\n",
            "pred tensor([-1.7853e-02, -1.2054e-02, -1.8775e-05, -1.1740e-03, -3.6438e-02,\n",
            "        -3.7567e-02, -2.7374e-02, -1.2505e-02, -1.3752e-03, -1.5497e-06,\n",
            "        -1.6998e-02, -5.6601e-04, -1.1444e-02, -5.3215e-03, -3.5400e-01,\n",
            "        -1.5906e-01, -4.9902e-01, -4.9829e-01, -5.0000e-01, -4.9927e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.9951e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6493763327598572 tensor(12, device='cuda:0')\n",
            "lsy 2.458984375 0.0\n",
            "pred tensor([-0.5000, -0.4993, -0.4741, -0.4353, -0.5000, -0.5000, -0.5000, -0.4043,\n",
            "        -0.4766, -0.4993, -0.4976, -0.4995, -0.4878, -0.4551, -0.5000, -0.4883,\n",
            "        -0.4927, -0.4617, -0.4905, -0.5000, -0.4968, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6718999743461609 tensor(23, device='cuda:0')\n",
            "lsy 3.1875 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9927e-01, -5.0000e-01,\n",
            "        -4.9951e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.9585e-01, -2.7173e-01, -2.4658e-02,\n",
            "        -7.0000e-03, -1.2032e-02, -6.4240e-03, -1.7639e-02, -2.0435e-01,\n",
            "        -3.1982e-01, -2.3621e-01, -1.0797e-01, -6.2048e-05,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6908647418022156 tensor(22, device='cuda:0')\n",
            "lsy 3.2734375 0.0\n",
            "pred tensor([-2.1231e-04, -9.7990e-05, -1.1921e-07, -5.4550e-04, -1.1414e-04,\n",
            "        -2.3687e-04, -1.1104e-04, -5.3644e-07, -5.6624e-06,  0.0000e+00,\n",
            "        -2.2113e-05, -8.6129e-05, -2.5034e-06, -2.2736e-02, -2.7418e-05,\n",
            "        -2.6226e-06, -2.8610e-06, -1.9226e-03, -4.0364e-04, -9.3579e-06,\n",
            "        -1.1730e-04, -3.2568e-04, -1.4191e-03, -3.0994e-06, -1.2207e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6499223709106445 tensor(16, device='cuda:0')\n",
            "lsy 3.6640625 0.0\n",
            "pred tensor([-0.4956, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4963, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4973, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6544392704963684 tensor(14, device='cuda:0')\n",
            "lsy 3.51171875 0.0\n",
            "pred tensor([-0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998,\n",
            "        -0.4214, -0.4932, -0.5000, -0.4111, -0.4941, -0.4695, -0.4321, -0.4990,\n",
            "        -0.4868, -0.4609, -0.5000, -0.4932, -0.5000, -0.5000, -0.4998, -0.5000,\n",
            "        -0.4978], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6602248549461365 tensor(15, device='cuda:0')\n",
            "lsy 3.884765625 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -4.8676e-02, -1.4353e-03,\n",
            "        -2.9114e-02, -7.2539e-05, -3.4881e-04, -6.8893e-03, -1.5221e-02,\n",
            "        -3.0947e-04, -3.7506e-02, -7.3528e-04, -6.9641e-02, -1.9181e-04,\n",
            "        -2.2423e-04, -6.0158e-03, -2.7108e-04, -4.3893e-04, -2.2964e-03,\n",
            "        -3.5278e-02, -8.4152e-03, -2.8735e-01, -1.3867e-01, -1.6443e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6728558540344238 tensor(13, device='cuda:0')\n",
            "lsy 3.845703125 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.9951e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -2.9883e-01, -1.7703e-05, -9.2888e-04,\n",
            "        -6.6681e-03, -2.8107e-02, -3.5065e-02, -7.2327e-02, -3.2158e-03,\n",
            "        -4.1842e-05, -3.7408e-04, -8.7585e-03, -4.9472e-05, -5.0812e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6564472913742065 tensor(10, device='cuda:0')\n",
            "lsy 3.521484375 0.0\n",
            "pred tensor([-1.7395e-01, -6.1560e-04, -2.1469e-02, -1.2016e-03, -3.9756e-05,\n",
            "        -1.2428e-02, -2.2392e-03, -6.5247e-02, -8.2254e-06, -5.9471e-03,\n",
            "        -3.0577e-05, -2.0504e-04, -5.9986e-04, -7.7486e-07, -4.3564e-03,\n",
            "        -1.9875e-03, -7.9918e-04, -4.3559e-04, -2.0618e-03, -3.1996e-04,\n",
            "        -1.2052e-04, -4.1842e-05, -3.3319e-05, -1.7607e-04, -9.4482e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6741299629211426 tensor(10, device='cuda:0')\n",
            "lsy 3.185546875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4990, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4958, -0.4937, -0.4609, -0.5000, -0.4045, -0.4954, -0.4819,\n",
            "        -0.4194], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6399523019790649 tensor(8, device='cuda:0')\n",
            "lsy 3.494140625 0.0\n",
            "pred tensor([-0.4646, -0.4932, -0.5000, -0.4583, -0.5000, -0.4204, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6894941926002502 tensor(15, device='cuda:0')\n",
            "lsy 2.572265625 0.0\n",
            "pred tensor([-0.4465, -0.5000, -0.4958, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4033], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6499068140983582 tensor(10, device='cuda:0')\n",
            "lsy 2.5859375 0.0\n",
            "pred tensor([-4.9194e-01, -4.9780e-01, -5.0000e-01, -4.7656e-01, -4.9634e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.9605e-08, -4.0221e-04, -1.9028e-02, -3.5214e-04, -8.9340e-03,\n",
            "        -1.0986e-03, -5.5850e-05, -7.6294e-06, -1.1867e-04, -2.7905e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6564193367958069 tensor(7, device='cuda:0')\n",
            "lsy 3.671875 0.0\n",
            "pred tensor([-0.4922, -0.4221, -0.4746, -0.4563, -0.4629, -0.0968, -0.0831, -0.3792,\n",
            "        -0.3943, -0.1150, -0.1147, -0.1996, -0.4697, -0.3667, -0.2444, -0.4868,\n",
            "        -0.4448, -0.0281, -0.4297, -0.4338, -0.4756, -0.0263, -0.4192, -0.0270,\n",
            "        -0.0086], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6696879863739014 tensor(10, device='cuda:0')\n",
            "lsy 3.162109375 0.0\n",
            "pred tensor([-2.2430e-02, -1.4557e-02, -3.2178e-01, -4.4751e-01, -5.0000e-01,\n",
            "        -3.9673e-01, -4.8828e-01, -3.9014e-01, -4.0527e-01, -5.0000e-01,\n",
            "        -4.2432e-01, -4.3726e-01, -3.2227e-01, -4.6234e-02, -1.8872e-01,\n",
            "        -2.4307e-02, -5.8716e-02, -1.9562e-04, -1.0144e-01, -3.4695e-03,\n",
            "        -6.5231e-04, -6.0380e-05, -5.3048e-06, -8.5815e-02, -5.0426e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6834905743598938 tensor(13, device='cuda:0')\n",
            "lsy 3.525390625 0.0\n",
            "pred tensor([-8.3069e-02, -2.6226e-06, -3.0065e-04, -3.8743e-06, -8.0729e-04,\n",
            "        -5.3787e-04,  0.0000e+00, -1.8096e-04, -1.7881e-07, -2.3594e-03,\n",
            "        -5.4836e-06, -2.6166e-05, -9.5367e-06, -5.6624e-06, -2.2709e-05,\n",
            "        -2.9778e-04, -3.5763e-07, -1.1921e-07, -9.6035e-04, -5.6791e-04,\n",
            "        -2.2049e-03, -5.5611e-05, -7.7486e-07, -5.7638e-05, -1.6199e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6709716320037842 tensor(10, device='cuda:0')\n",
            "lsy 3.94921875 0.0\n",
            "pred tensor([-0.3623, -0.4993, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4995, -0.5000, -0.4998, -0.4666, -0.5000, -0.5000, -0.4961,\n",
            "        -0.4954, -0.5000, -0.4907, -0.5000, -0.5000, -0.5000, -0.4360, -0.4988,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.654157280921936 tensor(11, device='cuda:0')\n",
            "20\n",
            "lsy 3.56640625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4990], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6461690068244934 tensor(8, device='cuda:0')\n",
            "lsy 2.73828125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4917, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4905,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6799723505973816 tensor(12, device='cuda:0')\n",
            "lsy 3.2421875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.3215, -0.0023, -0.0331, -0.0454, -0.3877,\n",
            "        -0.0269, -0.1174, -0.4629, -0.0017, -0.4111, -0.0095, -0.4126, -0.0341,\n",
            "        -0.2014], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6569324135780334 tensor(12, device='cuda:0')\n",
            "lsy 2.56640625 0.0\n",
            "pred tensor([-6.6528e-02, -2.8412e-02, -4.3286e-01, -8.1897e-05, -1.4061e-02,\n",
            "        -8.4412e-02, -4.3774e-01, -8.3984e-02, -2.6440e-01, -1.5710e-01,\n",
            "        -4.4067e-02, -5.0201e-02, -1.7432e-01, -7.2571e-02, -1.4839e-03,\n",
            "        -2.5131e-02, -6.2764e-05, -1.1921e-07, -3.1242e-03, -8.4961e-02,\n",
            "        -1.9897e-01, -1.5112e-01, -2.6077e-02, -1.9519e-01, -1.2268e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6548817753791809 tensor(8, device='cuda:0')\n",
            "lsy 2.52734375 0.0\n",
            "pred tensor([-0.4966, -0.4958, -0.4819, -0.3931, -0.3013, -0.4937, -0.3826, -0.0835,\n",
            "        -0.2874, -0.1381, -0.2847, -0.1204, -0.3606, -0.4863, -0.3977, -0.0643,\n",
            "        -0.3022, -0.1140, -0.3770, -0.4580, -0.4565, -0.0697, -0.4829, -0.4834,\n",
            "        -0.4290], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6757709383964539 tensor(17, device='cuda:0')\n",
            "lsy 2.732421875 0.0\n",
            "pred tensor([-0.5000, -0.3914, -0.4038, -0.4922, -0.3838, -0.3621, -0.5000, -0.4958,\n",
            "        -0.4905, -0.4744, -0.4917, -0.4241, -0.4561, -0.4814, -0.0204, -0.3564,\n",
            "        -0.4849, -0.4182, -0.4395, -0.4985, -0.5000, -0.4990, -0.4978, -0.4946,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6425524353981018 tensor(9, device='cuda:0')\n",
            "lsy 2.435546875 0.0\n",
            "pred tensor([-0.4993, -0.4500, -0.4922, -0.2891, -0.4883, -0.4871, -0.0328, -0.1792,\n",
            "        -0.2637, -0.0793, -0.3318, -0.2039, -0.4187, -0.4417, -0.4253, -0.4800,\n",
            "        -0.4087, -0.3115, -0.3945, -0.4399, -0.3748, -0.2974, -0.2021, -0.0539,\n",
            "        -0.1473], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6448861360549927 tensor(11, device='cuda:0')\n",
            "lsy 2.373046875 0.0\n",
            "pred tensor([-1.4038e-01, -3.9722e-01, -1.9495e-01, -2.0911e-01, -3.7354e-01,\n",
            "        -3.4082e-01, -3.4985e-01, -3.6011e-01, -4.2749e-01, -3.4302e-01,\n",
            "        -3.4393e-02, -2.0203e-02, -1.5640e-02, -8.5926e-04, -1.1921e-07,\n",
            "        -7.6890e-06, -1.9073e-06, -8.9407e-07,  0.0000e+00,  0.0000e+00,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6334936022758484 tensor(10, device='cuda:0')\n",
            "lsy 2.81640625 0.0\n",
            "pred tensor([-8.6427e-06, -9.2387e-06, -5.3048e-05, -6.4941e-02, -1.1803e-02,\n",
            "        -3.4204e-01, -5.9052e-02, -8.2825e-02, -1.2488e-01, -4.6814e-02,\n",
            "        -4.7705e-01, -4.4214e-01, -3.6035e-01, -5.0000e-01, -3.3960e-01,\n",
            "        -3.9160e-01, -3.8623e-01, -1.1127e-01, -2.0300e-01, -4.3872e-01,\n",
            "        -6.2561e-02, -4.2993e-01, -1.9995e-01, -2.1448e-01, -4.5239e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6844514608383179 tensor(18, device='cuda:0')\n",
            "lsy 2.34375 0.0\n",
            "pred tensor([-3.0371e-01, -2.2656e-01, -2.9449e-02, -2.2961e-01, -1.5955e-01,\n",
            "        -2.3413e-01, -3.2878e-04, -4.2847e-02, -4.9067e-04, -1.0809e-01,\n",
            "        -2.8198e-02, -6.1455e-03, -3.0458e-05, -2.8418e-01, -2.3055e-04,\n",
            "        -2.3193e-03, -3.5980e-02, -3.2275e-01, -5.0000e-01, -2.1887e-01,\n",
            "        -1.9727e-01, -4.2407e-01, -2.6538e-01, -4.1333e-01, -4.0918e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6795104146003723 tensor(15, device='cuda:0')\n",
            "lsy 3.5625 0.0\n",
            "pred tensor([-0.4976, -0.5000, -0.5000, -0.5000, -0.5000, -0.4946, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4873, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6972572803497314 tensor(15, device='cuda:0')\n",
            "lsy 2.677734375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.0806, -0.2295,\n",
            "        -0.1899, -0.4326, -0.3821, -0.4194, -0.4736, -0.4983, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4907, -0.4978, -0.5000, -0.5000, -0.4434, -0.4846, -0.5000,\n",
            "        -0.4956], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.675112783908844 tensor(16, device='cuda:0')\n",
            "lsy 2.48046875 0.0\n",
            "pred tensor([-1.0204e-03, -5.9652e-04, -7.2327e-02, -3.3783e-02, -7.9274e-06,\n",
            "        -1.2337e-02, -3.2783e-06, -1.5473e-04, -3.3975e-06, -4.9543e-04,\n",
            "        -1.4830e-04, -6.5804e-05, -7.6065e-03, -3.7079e-02, -6.7444e-03,\n",
            "        -1.9180e-02, -8.7967e-03, -4.2297e-02, -2.9541e-02, -4.1565e-02,\n",
            "        -1.0834e-03, -1.9995e-01, -1.7578e-02, -3.8910e-04, -2.6627e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6593008637428284 tensor(15, device='cuda:0')\n",
            "lsy 2.3671875 0.0\n",
            "pred tensor([-1.0052e-01, -4.6825e-04, -2.3987e-02, -2.6703e-02, -1.1757e-02,\n",
            "        -2.7734e-01, -1.1091e-03, -1.5974e-03, -9.4360e-02, -1.0902e-02,\n",
            "        -2.1332e-02, -4.2822e-01, -1.3351e-05, -2.2960e-04, -8.4656e-02,\n",
            "        -4.1161e-03, -1.0735e-02, -4.5959e-02, -2.8564e-01, -2.5220e-01,\n",
            "        -4.7827e-01, -4.1504e-02, -3.2959e-01, -1.4539e-01, -2.6831e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.644358217716217 tensor(7, device='cuda:0')\n",
            "lsy 2.455078125 0.0\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3709e-05,  0.0000e+00,\n",
            "         0.0000e+00, -5.9605e-08, -3.0398e-06, -2.1935e-05, -5.5611e-05,\n",
            "         0.0000e+00,  0.0000e+00, -2.4199e-05, -4.9472e-06, -2.7966e-04,\n",
            "        -2.3317e-04, -1.7881e-06, -1.4111e-01, -4.9243e-01, -4.6118e-01,\n",
            "        -4.9829e-01, -5.0000e-01, -4.8047e-01, -5.0000e-01, -4.5532e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6984391212463379 tensor(14, device='cuda:0')\n",
            "lsy 2.56640625 0.0\n",
            "pred tensor([-4.9829e-01, -4.5996e-01, -4.7876e-01, -5.0000e-01, -4.8071e-01,\n",
            "        -4.9756e-01, -4.9976e-01, -5.0000e-01, -4.9854e-01, -4.9194e-01,\n",
            "        -5.0000e-01, -4.3115e-01, -4.8901e-01, -4.5657e-04, -4.7455e-03,\n",
            "        -1.0133e-06, -2.9182e-04, -1.1921e-07, -1.1981e-05,  0.0000e+00,\n",
            "        -2.2697e-04, -5.7817e-05, -9.5367e-07, -7.6437e-04, -6.1560e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6632002592086792 tensor(13, device='cuda:0')\n",
            "lsy 2.5859375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4319, -0.3284, -0.2101, -0.5000, -0.4714, -0.4751, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6522601246833801 tensor(10, device='cuda:0')\n",
            "lsy 2.380859375 0.0\n",
            "pred tensor([-0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4917, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4966, -0.5000, -0.4822, -0.4985, -0.5000, -0.5000, -0.4612, -0.5000,\n",
            "        -0.4834], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6397364139556885 tensor(9, device='cuda:0')\n",
            "lsy 2.27734375 0.0\n",
            "pred tensor([-2.3139e-04, -7.0286e-04, -2.6894e-03, -3.7018e-02, -3.1281e-02,\n",
            "        -3.0472e-02, -4.3799e-01, -4.9438e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.6777e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9438e-01, -4.8926e-01, -5.0000e-01, -4.8560e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6665570139884949 tensor(16, device='cuda:0')\n",
            "lsy 2.7421875 0.0\n",
            "pred tensor([-4.9634e-01, -5.0000e-01, -4.9316e-01, -5.0000e-01, -4.9976e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.9854e-01, -4.8218e-01, -3.7140e-02,\n",
            "        -2.9926e-03, -3.0398e-06, -6.2561e-02, -1.4473e-02, -4.0770e-04,\n",
            "        -1.7614e-03, -4.4128e-02, -3.4943e-02, -5.4893e-03, -1.1162e-02,\n",
            "        -1.3965e-01, -1.1084e-01, -1.6321e-01, -1.0391e-02, -1.3757e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6339079737663269 tensor(6, device='cuda:0')\n",
            "lsy 2.7578125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4971,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4995, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6649935841560364 tensor(6, device='cuda:0')\n",
            "lsy 2.732421875 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9902e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -3.9458e-05, -8.0943e-05, -3.9520e-02, -5.2869e-05, -1.5533e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6476292610168457 tensor(10, device='cuda:0')\n",
            "lsy 2.337890625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4995,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.640451967716217 tensor(6, device='cuda:0')\n",
            "lsy 2.43359375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4995, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4915, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6352632641792297 tensor(6, device='cuda:0')\n",
            "lsy 3.005859375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4937, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6741563081741333 tensor(11, device='cuda:0')\n",
            "lsy 3.564453125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4995, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.662632942199707 tensor(12, device='cuda:0')\n",
            "lsy 3.185546875 0.0\n",
            "pred tensor([-0.5000, -0.4978, -0.5000, -0.4985, -0.5000, -0.5000, -0.4895, -0.4695,\n",
            "        -0.4905, -0.5000, -0.5000, -0.5000, -0.4980, -0.4895, -0.4475, -0.4744,\n",
            "        -0.4358, -0.3494, -0.4807, -0.5000, -0.4504, -0.4932, -0.4041, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6698932647705078 tensor(17, device='cuda:0')\n",
            "lsy 3.306640625 0.0\n",
            "pred tensor([-0.4934, -0.5000, -0.5000, -0.4993, -0.4988, -0.4653, -0.4688, -0.5000,\n",
            "        -0.4446, -0.4922, -0.5000, -0.4929, -0.5000, -0.4998, -0.4851, -0.5000,\n",
            "        -0.4893, -0.4966, -0.5000, -0.4797, -0.4529, -0.3423, -0.1615, -0.2050,\n",
            "        -0.0073], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6839776039123535 tensor(17, device='cuda:0')\n",
            "lsy 3.33984375 0.0\n",
            "pred tensor([-7.7095e-03, -2.5368e-03, -1.3718e-02, -1.5778e-02, -1.3304e-03,\n",
            "        -2.1875e-05, -4.6272e-03, -1.3351e-03, -1.0473e-04, -5.1270e-02,\n",
            "        -5.9187e-05, -1.2207e-03, -3.1185e-03, -7.3059e-02, -5.7190e-02,\n",
            "        -5.3864e-02, -1.2337e-02, -1.4954e-02, -2.5955e-02, -2.6337e-02,\n",
            "        -1.2323e-01, -3.0127e-01, -4.0601e-01, -4.7241e-01, -4.8413e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6789624094963074 tensor(14, device='cuda:0')\n",
            "lsy 3.23828125 0.0\n",
            "pred tensor([-0.4651, -0.4836, -0.4993, -0.5000, -0.5000, -0.4797, -0.5000, -0.4812,\n",
            "        -0.4763, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4976, -0.4951, -0.4800, -0.4783, -0.4880, -0.4927, -0.4998,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6753636002540588 tensor(12, device='cuda:0')\n",
            "lsy 3.21875 0.0\n",
            "pred tensor([-4.0054e-04, -7.1526e-07, -3.6955e-06, -2.3842e-07, -1.3113e-05,\n",
            "        -4.1723e-07, -2.1458e-05, -2.4247e-04, -2.0676e-03, -4.7088e-06,\n",
            "        -2.4295e-04, -6.7592e-05, -5.1928e-04, -2.3956e-02, -2.4750e-02,\n",
            "        -1.8359e-01, -2.9395e-01, -4.0234e-01, -4.2749e-01, -4.5972e-01,\n",
            "        -4.5776e-01, -5.0000e-01, -4.5581e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6731900572776794 tensor(15, device='cuda:0')\n",
            "lsy 3.71875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4890, -0.5000, -0.4961, -0.4822, -0.5000, -0.4927,\n",
            "        -0.5000, -0.4946, -0.4980, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4988], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6692208647727966 tensor(17, device='cuda:0')\n",
            "lsy 2.759765625 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -4.9951e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.4861e-03, -4.4107e-06, -2.7523e-03, -1.6475e-04, -2.3842e-07,\n",
            "        -1.8966e-04, -1.0073e-04, -1.2338e-04, -9.5367e-07, -6.0201e-06,\n",
            "        -4.7684e-06, -8.8501e-03, -1.2779e-04, -1.5473e-04, -1.6136e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6807703971862793 tensor(14, device='cuda:0')\n",
            "lsy 2.50390625 0.0\n",
            "pred tensor([-5.6122e-02, -1.2887e-04, -1.8177e-03, -9.2468e-02, -6.3110e-02,\n",
            "        -7.3195e-04, -3.5913e-01, -3.7207e-01, -1.6138e-01, -4.2432e-01,\n",
            "        -3.7280e-01, -4.5264e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.7019023895263672 tensor(18, device='cuda:0')\n",
            "lsy 2.333984375 0.0\n",
            "pred tensor([-0.4260, -0.4109, -0.4241, -0.2529, -0.1410, -0.0854, -0.1382, -0.2101,\n",
            "        -0.2440, -0.0072, -0.4121, -0.0803, -0.3201, -0.3333, -0.2917, -0.1609,\n",
            "        -0.4092, -0.4709, -0.4150, -0.2676, -0.4329, -0.3943, -0.3948, -0.4666,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6738715171813965 tensor(8, device='cuda:0')\n",
            "lsy 2.0546875 0.0\n",
            "pred tensor([-0.4656, -0.4377, -0.4685, -0.5000, -0.5000, -0.5000, -0.5000, -0.2815,\n",
            "        -0.3750, -0.3542, -0.1306, -0.3757, -0.0501, -0.3796, -0.4541, -0.4966,\n",
            "        -0.4358, -0.3438, -0.0920, -0.3330, -0.4871, -0.3003, -0.2170, -0.3250,\n",
            "        -0.3708], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6618984937667847 tensor(9, device='cuda:0')\n",
            "lsy 1.7451171875 0.0\n",
            "pred tensor([-0.4990, -0.4690, -0.4893, -0.4949, -0.4871, -0.4646, -0.4172, -0.4500,\n",
            "        -0.4661, -0.3193, -0.3599, -0.4683, -0.4219, -0.4929, -0.4670, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4836, -0.5000, -0.4641,\n",
            "        -0.4478], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.7032842636108398 tensor(30, device='cuda:0')\n",
            "lsy 2.88671875 0.0\n",
            "pred tensor([-4.2389e-02, -4.6936e-02, -8.1604e-02, -3.2928e-02, -2.0081e-02,\n",
            "        -3.5114e-03, -4.3755e-03, -6.3002e-05, -1.3983e-04, -1.2994e-05,\n",
            "        -2.1423e-02, -4.6825e-04, -9.9277e-04, -1.0920e-03, -1.9431e-05,\n",
            "        -9.4593e-05, -1.1787e-03, -4.7684e-07, -3.2593e-02, -8.1711e-03,\n",
            "        -4.3154e-05, -2.0580e-03, -5.0068e-06, -9.0027e-04, -1.5125e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.677169680595398 tensor(8, device='cuda:0')\n",
            "21\n",
            "lsy 2.18359375 0.0\n",
            "pred tensor([-0.5000, -0.4839, -0.4912, -0.5000, -0.4890, -0.4993, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4985, -0.5000, -0.5000, -0.4978,\n",
            "        -0.5000, -0.4995, -0.5000, -0.4927, -0.5000, -0.4995, -0.4998, -0.5000,\n",
            "        -0.4917], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.675234317779541 tensor(16, device='cuda:0')\n",
            "lsy 2.681640625 0.0\n",
            "pred tensor([-4.9438e-01, -5.0000e-01, -5.0000e-01, -4.1723e-07, -1.8811e-04,\n",
            "        -3.5763e-06, -1.7227e-02, -5.0385e-02, -3.8505e-05, -6.8140e-04,\n",
            "        -9.8133e-04, -1.3618e-03, -2.4338e-03, -1.3447e-04, -1.2052e-04,\n",
            "        -4.3907e-03, -5.1239e-02, -3.5214e-04, -2.9373e-02, -2.6894e-03,\n",
            "        -1.0231e-02, -1.8539e-02, -1.0437e-01, -2.6343e-01, -2.6636e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6743852496147156 tensor(13, device='cuda:0')\n",
            "lsy 3.1484375 0.0\n",
            "pred tensor([-0.0167, -0.0539, -0.2417, -0.4592, -0.4988, -0.4976, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4956, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6457032561302185 tensor(11, device='cuda:0')\n",
            "lsy 3.669921875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4897,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4893, -0.5000, -0.5000, -0.4963, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4919, -0.4041, -0.5000, -0.2578, -0.4697, -0.4575,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6650193929672241 tensor(15, device='cuda:0')\n",
            "lsy 3.400390625 0.0\n",
            "pred tensor([-1.4661e-01, -4.6606e-01, -3.1006e-01, -4.4971e-01, -3.8330e-01,\n",
            "        -1.9189e-01, -3.0005e-01, -2.9468e-01, -4.8535e-01, -3.9868e-01,\n",
            "        -1.4722e-01, -1.3574e-01, -2.5879e-01, -1.3977e-01, -2.8711e-01,\n",
            "        -1.6861e-02, -4.3297e-04, -9.3765e-03, -5.2214e-04, -1.0881e-03,\n",
            "        -1.1265e-05, -3.5763e-07, -2.8193e-05, -3.2349e-03, -1.8382e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6874960064888 tensor(12, device='cuda:0')\n",
            "lsy 4.359375 0.0\n",
            "pred tensor([-0.0010, -0.0046, -0.0195, -0.3572, -0.2864, -0.4998, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4958, -0.4990, -0.4863, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4890, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6503729224205017 tensor(10, device='cuda:0')\n",
            "lsy 3.859375 0.0\n",
            "pred tensor([-3.6192e-04, -1.7881e-06,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
            "        -2.3842e-07, -1.7881e-07, -3.4153e-05, -1.8668e-04, -1.8525e-04,\n",
            "        -1.7881e-07,  0.0000e+00, -2.4855e-05, -3.9053e-04, -4.1723e-07,\n",
            "        -3.3200e-05,  0.0000e+00, -5.6028e-04, -5.5194e-05, -3.3021e-04,\n",
            "        -5.9605e-08, -8.3804e-05, -1.7881e-07, -4.1723e-07, -1.7262e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6512887477874756 tensor(10, device='cuda:0')\n",
            "lsy 3.216796875 0.0\n",
            "pred tensor([ 0.0000e+00, -1.7891e-03, -5.6419e-03, -5.9605e-08,  0.0000e+00,\n",
            "        -4.5419e-05, -2.1219e-05, -2.5272e-05, -1.3602e-04, -4.7684e-07,\n",
            "        -2.8431e-05, -1.6987e-05, -8.4152e-03, -3.2640e-04, -4.7684e-07,\n",
            "        -1.7881e-07, -2.0421e-04, -7.0930e-06, -2.3663e-05, -4.6182e-04,\n",
            "        -4.7266e-01, -1.5662e-01, -1.0468e-02, -8.8440e-02, -2.3401e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6748505234718323 tensor(13, device='cuda:0')\n",
            "lsy 3.392578125 0.0\n",
            "pred tensor([-1.1921e-07, -6.3515e-04, -2.1716e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.9976e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6834295988082886 tensor(12, device='cuda:0')\n",
            "lsy 3.267578125 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.6110e-04, -3.5167e-06, -7.4768e-03, -2.1534e-03,\n",
            "        -1.6937e-03, -5.4121e-04, -8.9228e-05, -9.0408e-04, -9.3520e-05,\n",
            "        -2.1827e-04, -7.9498e-03, -2.9282e-02, -2.1076e-03, -2.3468e-02,\n",
            "        -2.0752e-03, -1.8892e-03, -5.0735e-04, -1.6391e-05, -2.6047e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6720867156982422 tensor(9, device='cuda:0')\n",
            "lsy 3.576171875 0.0\n",
            "pred tensor([-8.3466e-03, -6.1226e-03, -1.3113e-06, -1.0252e-05, -4.1318e-04,\n",
            "        -7.7095e-03, -2.8074e-05,  0.0000e+00,  0.0000e+00, -2.4140e-05,\n",
            "         0.0000e+00, -4.5896e-06,  0.0000e+00, -2.9802e-07,  0.0000e+00,\n",
            "        -4.7088e-06, -4.7684e-07, -1.7202e-04, -1.0729e-06, -1.3983e-04,\n",
            "        -5.9605e-07, -1.8177e-03, -1.6534e-04, -1.9455e-02, -5.8990e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.670656681060791 tensor(11, device='cuda:0')\n",
            "lsy 3.37109375 0.0\n",
            "pred tensor([-1.4954e-02, -5.1689e-03, -2.3560e-02, -7.2266e-02, -2.9565e-01,\n",
            "        -7.1045e-02, -2.5903e-01, -8.5592e-04, -1.6861e-02, -6.2904e-03,\n",
            "        -9.6607e-04, -1.6394e-01, -5.9013e-03, -2.1011e-02, -1.0185e-03,\n",
            "        -8.8871e-05, -1.0031e-04, -1.5497e-06, -6.1989e-06, -6.7353e-06,\n",
            "        -2.8634e-04, -1.7881e-07,  0.0000e+00, -6.3133e-03, -1.0729e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.677725076675415 tensor(13, device='cuda:0')\n",
            "lsy 3.373046875 0.0\n",
            "pred tensor([-8.6136e-03, -1.2527e-02, -7.3586e-03, -3.2406e-03, -4.3831e-03,\n",
            "        -4.3640e-02, -9.9957e-05, -2.9697e-03, -7.6294e-05, -2.0386e-02,\n",
            "        -4.2725e-02, -2.3376e-02, -2.1460e-01, -1.3965e-01, -8.5632e-02,\n",
            "        -1.4600e-01, -5.0000e-01, -2.6392e-01, -3.8086e-01, -4.2822e-01,\n",
            "        -4.9854e-01, -1.3151e-03, -1.0729e-06,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6643109321594238 tensor(13, device='cuda:0')\n",
            "lsy 3.16796875 0.0\n",
            "pred tensor([-5.3644e-07, -3.8981e-05, -6.2523e-03, -1.6708e-03, -6.7592e-05,\n",
            "        -1.4257e-04, -5.3048e-06, -2.9240e-03, -2.4887e-02, -1.1993e-01,\n",
            "        -1.3304e-03, -1.0468e-01, -7.6965e-02, -2.6050e-01, -4.2090e-01,\n",
            "        -3.6224e-02, -6.2805e-02, -1.2195e-01, -1.6699e-01, -9.0561e-03,\n",
            "        -1.1421e-02, -5.9080e-04, -4.5746e-02, -2.0660e-02, -1.2573e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6537669897079468 tensor(5, device='cuda:0')\n",
            "lsy 3.65234375 0.0\n",
            "pred tensor([-1.1871e-02, -5.0426e-05, -6.1798e-04, -1.4603e-05, -1.4896e-03,\n",
            "        -9.4299e-03, -3.4690e-05, -3.2654e-02, -2.7145e-02, -7.5439e-02,\n",
            "        -1.2012e-01, -4.6411e-01, -3.9502e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6465058922767639 tensor(6, device='cuda:0')\n",
            "lsy 2.802734375 0.0\n",
            "pred tensor([-0.4993, -0.5000, -0.5000, -0.5000, -0.5000, -0.4961, -0.5000, -0.4998,\n",
            "        -0.5000, -0.4983, -0.5000, -0.4995, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4990, -0.5000, -0.5000, -0.5000, -0.4954, -0.5000, -0.4998,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6531080603599548 tensor(11, device='cuda:0')\n",
            "lsy 3.080078125 0.0\n",
            "pred tensor([-0.5000, -0.4807, -0.5000, -0.5000, -0.5000, -0.4993, -0.5000, -0.5000,\n",
            "        -0.4895, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6841111779212952 tensor(19, device='cuda:0')\n",
            "lsy 2.830078125 0.0\n",
            "pred tensor([-0.4968, -0.4998, -0.4792, -0.4919, -0.5000, -0.5000, -0.4954, -0.5000,\n",
            "        -0.4910, -0.4927, -0.5000, -0.4875, -0.4507, -0.4150, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6829845309257507 tensor(15, device='cuda:0')\n",
            "lsy 2.857421875 0.0\n",
            "pred tensor([-0.4419, -0.3679, -0.4429, -0.3508, -0.2827, -0.3728, -0.4717, -0.4651,\n",
            "        -0.4524, -0.4602, -0.4270, -0.4460, -0.3726, -0.3811, -0.2742, -0.1643,\n",
            "        -0.2484, -0.3801, -0.3494, -0.3428, -0.4360, -0.4746, -0.4373, -0.3464,\n",
            "        -0.3958], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6789006590843201 tensor(14, device='cuda:0')\n",
            "lsy 3.595703125 0.0\n",
            "pred tensor([-0.4597, -0.3804, -0.4023, -0.4883, -0.4561, -0.4404, -0.4180, -0.4556,\n",
            "        -0.4097, -0.3845, -0.4932, -0.4221, -0.4763, -0.4624, -0.4622, -0.5000,\n",
            "        -0.4836, -0.4258, -0.4822, -0.3997, -0.4644, -0.4390, -0.4702, -0.4170,\n",
            "        -0.4592], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6777731776237488 tensor(12, device='cuda:0')\n",
            "lsy 2.900390625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4773, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.0413, -0.2617, -0.0292, -0.0415,\n",
            "        -0.0112], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6640363931655884 tensor(6, device='cuda:0')\n",
            "lsy 2.689453125 0.0\n",
            "pred tensor([-0.1858, -0.0345, -0.0217, -0.1493, -0.0513, -0.0290, -0.2113, -0.4351,\n",
            "        -0.0770, -0.4832, -0.4568, -0.4172, -0.4836, -0.2524, -0.3264, -0.4570,\n",
            "        -0.3794, -0.4685, -0.4451, -0.4082, -0.2067, -0.3152, -0.3840, -0.3640,\n",
            "        -0.1979], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6542545557022095 tensor(10, device='cuda:0')\n",
            "lsy 4.1640625 0.0\n",
            "pred tensor([ 0.0000e+00, -4.8876e-06,  0.0000e+00, -5.7161e-05, -2.3842e-05,\n",
            "        -1.0133e-06, -1.0848e-04, -8.9493e-03, -1.0973e-04, -4.7073e-03,\n",
            "        -5.6274e-02, -2.8809e-02, -1.4830e-04, -1.0614e-01, -4.3652e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.8047e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.666994571685791 tensor(11, device='cuda:0')\n",
            "lsy 3.470703125 0.0\n",
            "pred tensor([-0.5000, -0.4993, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6658743619918823 tensor(15, device='cuda:0')\n",
            "lsy 3.361328125 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.8975e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "         0.0000e+00, -3.5248e-02, -4.8126e-02, -7.9918e-04, -1.4150e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6384654641151428 tensor(8, device='cuda:0')\n",
            "lsy 3.83203125 0.0\n",
            "pred tensor([-3.8803e-05, -1.9968e-05, -4.8218e-03, -2.9802e-07, -3.8635e-02,\n",
            "        -2.6047e-05, -1.9908e-05, -1.8906e-02, -3.2234e-03,  0.0000e+00,\n",
            "        -1.1139e-02, -1.1635e-04, -3.5477e-04, -1.8477e-06, -1.8177e-03,\n",
            "        -1.3222e-02, -7.0000e-03, -6.1810e-05, -1.9073e-06, -4.4703e-05,\n",
            "        -3.1525e-02, -2.0676e-03, -2.1076e-04, -2.1572e-03, -3.7140e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.7040940523147583 tensor(13, device='cuda:0')\n",
            "lsy 3.37890625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4846, -0.5000, -0.5000, -0.4983, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.676582932472229 tensor(13, device='cuda:0')\n",
            "lsy 3.2890625 0.0\n",
            "pred tensor([-5.0000e-01,  0.0000e+00, -3.8743e-06, -2.3901e-05,  0.0000e+00,\n",
            "        -6.0129e-04, -7.8064e-02, -5.8655e-02, -1.3222e-02, -7.4387e-04,\n",
            "        -1.6689e-06, -4.2175e-02, -5.4836e-06, -1.3069e-02, -2.2650e-06,\n",
            "        -1.7285e-06, -2.3246e-06, -9.3520e-05, -3.2306e-04, -5.1308e-03,\n",
            "        -2.6631e-04, -6.3133e-04, -4.9114e-05, -7.9775e-04, -2.8662e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6632258296012878 tensor(14, device='cuda:0')\n",
            "lsy 3.23828125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.0016, -0.0006, -0.0007, -0.0127,\n",
            "        -0.0137, -0.1759, -0.2866, -0.4270, -0.1026, -0.4675, -0.5000, -0.5000,\n",
            "        -0.4978], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6450020670890808 tensor(14, device='cuda:0')\n",
            "lsy 3.005859375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9658e-01,\n",
            "        -4.7998e-01, -4.9902e-01, -4.0649e-01, -4.0430e-01, -4.4409e-01,\n",
            "        -8.0261e-02, -1.6418e-02, -9.6558e-02, -3.3760e-03, -3.3386e-02,\n",
            "        -4.4703e-06, -1.2871e-02, -5.5771e-03, -2.3136e-03, -3.1708e-02,\n",
            "        -8.5876e-02, -1.0150e-01, -2.3328e-01, -1.5930e-01, -4.6338e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6564121842384338 tensor(14, device='cuda:0')\n",
            "lsy 3.94140625 0.0\n",
            "pred tensor([-2.2519e-04, -1.3661e-04, -2.9816e-02, -2.9480e-02, -1.1871e-02,\n",
            "        -2.6270e-01, -1.8481e-01, -2.6962e-02, -4.7461e-01, -3.8477e-01,\n",
            "        -4.7754e-01, -3.0908e-01, -1.9556e-01, -1.1444e-01, -3.5742e-01,\n",
            "        -8.9844e-02, -2.9077e-01, -3.2227e-02, -3.4937e-01, -1.0242e-01,\n",
            "        -7.3318e-03, -4.9622e-02, -3.2910e-01, -1.1908e-01, -2.2815e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6939049363136292 tensor(18, device='cuda:0')\n",
            "lsy 4.05859375 0.0\n",
            "pred tensor([-0.0016, -0.0093, -0.1207, -0.0157, -0.1606, -0.0247, -0.4895, -0.4949,\n",
            "        -0.5000, -0.4983, -0.4751, -0.4902, -0.5000, -0.4946, -0.4885, -0.5000,\n",
            "        -0.4995, -0.4995, -0.4976, -0.5000, -0.5000, -0.4885, -0.5000, -0.4885,\n",
            "        -0.4900], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.7074192762374878 tensor(16, device='cuda:0')\n",
            "lsy 3.599609375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -1.1206e-03, -4.1723e-06, -6.9847e-03, -1.6034e-05, -1.0490e-02,\n",
            "        -1.6650e-01, -7.2098e-04, -5.1331e-02, -8.2764e-02, -2.9640e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6683068871498108 tensor(10, device='cuda:0')\n",
            "lsy 3.216796875 0.0\n",
            "pred tensor([-2.4475e-02, -1.6223e-01, -5.4810e-02, -1.3405e-02, -1.3664e-02,\n",
            "        -9.1016e-05, -4.0283e-02, -8.1682e-04, -5.2582e-02, -2.5537e-01,\n",
            "        -4.3994e-01, -5.3497e-02, -1.8811e-01, -1.4758e-01, -2.7124e-01,\n",
            "        -1.7676e-01, -7.9036e-05, -3.1274e-01, -1.9623e-02, -4.8208e-04,\n",
            "        -1.1492e-03, -8.0729e-04, -5.1308e-04, -6.5231e-04, -1.0473e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6445711255073547 tensor(9, device='cuda:0')\n",
            "lsy 3.30859375 0.0\n",
            "pred tensor([-3.1769e-02, -2.3127e-05, -9.1457e-04, -7.6294e-06, -2.2583e-02,\n",
            "        -1.0908e-05, -3.1018e-04, -4.5898e-02, -6.1226e-03, -2.6831e-01,\n",
            "        -4.6558e-01, -4.6143e-01, -4.8340e-01, -5.0000e-01, -4.7412e-01,\n",
            "        -4.8022e-01, -4.9976e-01, -5.0000e-01, -4.4165e-01, -4.0527e-01,\n",
            "        -3.3154e-01, -3.7720e-01, -7.7248e-03, -2.9541e-02, -9.5062e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6639406085014343 tensor(12, device='cuda:0')\n",
            "lsy 3.666015625 0.0\n",
            "pred tensor([-4.2572e-02, -1.6895e-01, -5.4504e-02, -1.6003e-01, -4.3018e-01,\n",
            "        -1.4746e-01, -8.9407e-07, -2.8253e-04,  0.0000e+00, -4.0364e-04,\n",
            "        -7.0286e-04, -3.6316e-02, -7.4863e-05, -3.7909e-05, -1.8656e-05,\n",
            "         0.0000e+00, -2.5606e-04, -1.1848e-02, -5.1689e-03, -8.9844e-02,\n",
            "        -1.0729e-06, -4.3726e-04, -4.7374e-04, -1.3709e-04, -8.1253e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6714397668838501 tensor(11, device='cuda:0')\n",
            "lsy 3.466796875 0.0\n",
            "pred tensor([-0.5000, -0.4980, -0.5000, -0.4951, -0.5000, -0.4939, -0.4714, -0.4822,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4929, -0.5000, -0.5000, -0.4937,\n",
            "        -0.5000, -0.4897, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4937], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6537036895751953 tensor(15, device='cuda:0')\n",
            "lsy 3.365234375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4724, -0.5000,\n",
            "        -0.4949, -0.4917, -0.5000, -0.5000, -0.4946, -0.4998, -0.5000, -0.4998,\n",
            "        -0.4893, -0.4993, -0.4885, -0.5000, -0.5000, -0.5000, -0.5000, -0.4995,\n",
            "        -0.4885], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6792028546333313 tensor(9, device='cuda:0')\n",
            "22\n",
            "lsy 3.171875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4973, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.660606324672699 tensor(14, device='cuda:0')\n",
            "lsy 2.744140625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4954, -0.4590,\n",
            "        -0.4961, -0.4072, -0.4492, -0.3962, -0.5000, -0.4143, -0.4033, -0.3530,\n",
            "        -0.4329], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6781849265098572 tensor(9, device='cuda:0')\n",
            "lsy 2.806640625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4990, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4978,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6459762454032898 tensor(14, device='cuda:0')\n",
            "lsy 3.0703125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4958, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.647034227848053 tensor(11, device='cuda:0')\n",
            "lsy 2.865234375 0.0\n",
            "pred tensor([-0.4644, -0.4670, -0.4075, -0.4644, -0.4604, -0.5000, -0.5000, -0.4958,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4958, -0.5000, -0.4785, -0.5000, -0.4915, -0.4644, -0.3665, -0.4224,\n",
            "        -0.4041], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.64395672082901 tensor(19, device='cuda:0')\n",
            "lsy 3.16015625 0.0\n",
            "pred tensor([-4.7314e-01, -1.5015e-01, -2.8394e-01, -3.9404e-01, -3.1323e-01,\n",
            "        -4.8193e-01, -3.4790e-01, -3.7476e-01, -2.7368e-01, -4.3677e-01,\n",
            "        -5.0426e-05, -1.2522e-03, -1.5669e-03, -4.5815e-03, -3.2978e-03,\n",
            "        -3.6263e-04, -1.6475e-04, -8.6927e-04, -3.9520e-03, -1.7932e-01,\n",
            "        -2.8229e-03, -1.4503e-02, -1.1212e-01, -2.3059e-01, -8.9600e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6774541735649109 tensor(17, device='cuda:0')\n",
            "lsy 3.615234375 0.0\n",
            "pred tensor([-0.0262, -0.1550, -0.3943, -0.5000, -0.5000, -0.4856, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4951, -0.5000, -0.5000, -0.4971, -0.4753, -0.5000, -0.4785,\n",
            "        -0.5000, -0.4963, -0.5000, -0.4963, -0.4651, -0.5000, -0.4548, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6571908593177795 tensor(13, device='cuda:0')\n",
            "lsy 4.09765625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998,\n",
            "        -0.4849, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4973, -0.5000,\n",
            "        -0.4775], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6723241209983826 tensor(12, device='cuda:0')\n",
            "lsy 3.380859375 0.0\n",
            "pred tensor([-0.4890, -0.4727, -0.4495, -0.4966, -0.4980, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4966, -0.4998, -0.4934, -0.4993, -0.4939, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4873, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6492409110069275 tensor(12, device='cuda:0')\n",
            "lsy 3.3359375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9780e-01, -5.0000e-01, -5.0000e-01, -2.0248e-02, -3.5214e-04,\n",
            "        -2.1496e-03, -2.7817e-02, -1.0490e-02, -3.6297e-03, -1.0094e-02,\n",
            "        -1.8115e-01, -3.7727e-03, -7.0810e-04, -9.5062e-03, -5.0888e-03,\n",
            "        -2.7878e-02, -7.8735e-03, -3.5458e-03, -3.9053e-04, -1.9989e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6631672382354736 tensor(12, device='cuda:0')\n",
            "lsy 3.556640625 0.0\n",
            "pred tensor([-0.0301, -0.0075, -0.1182, -0.1237, -0.2744, -0.0497, -0.4878, -0.4668,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4705, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6548777222633362 tensor(13, device='cuda:0')\n",
            "lsy 3.15234375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4978, -0.4993, -0.4868, -0.4668, -0.4885, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4883, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4995, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6405213475227356 tensor(18, device='cuda:0')\n",
            "lsy 3.84765625 0.0\n",
            "pred tensor([-0.4966, -0.4473, -0.4666, -0.4976, -0.5000, -0.4858, -0.4810, -0.4912,\n",
            "        -0.4771, -0.4971, -0.4888, -0.5000, -0.5000, -0.5000, -0.4829, -0.5000,\n",
            "        -0.4727, -0.5000, -0.5000, -0.4971, -0.4946, -0.5000, -0.4961, -0.4980,\n",
            "        -0.4697], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6728823184967041 tensor(14, device='cuda:0')\n",
            "lsy 3.53515625 0.0\n",
            "pred tensor([-4.3970e-01, -4.7852e-01, -4.6899e-01, -4.4263e-01, -1.1277e-04,\n",
            "        -2.8729e-05, -5.5850e-05, -4.2319e-05,  0.0000e+00, -2.4533e-04,\n",
            "         0.0000e+00, -6.5565e-07,  0.0000e+00, -4.1723e-07,  0.0000e+00,\n",
            "         0.0000e+00, -8.3447e-07, -4.7088e-06, -4.7565e-04, -3.6499e-02,\n",
            "        -3.8700e-03, -4.3365e-02, -2.8540e-01, -9.8572e-02, -1.7517e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6561505198478699 tensor(14, device='cuda:0')\n",
            "lsy 3.533203125 0.0\n",
            "pred tensor([-5.0000e-01,  0.0000e+00, -1.4305e-05, -3.5763e-07,  0.0000e+00,\n",
            "        -1.7822e-05, -4.7684e-07, -9.9087e-04, -7.9274e-06, -1.1673e-03,\n",
            "        -1.0548e-03, -1.7202e-04, -8.3862e-02, -4.8047e-01, -4.9854e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9438e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.655938446521759 tensor(20, device='cuda:0')\n",
            "lsy 3.658203125 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.1008e-05, -3.7265e-04, -2.8687e-02, -6.0089e-02, -2.4768e-01,\n",
            "        -3.0029e-01, -4.1901e-02, -9.7275e-03, -2.0374e-01, -2.6270e-01,\n",
            "        -1.1545e-04, -6.3232e-02, -3.6530e-02, -1.8875e-02, -2.9926e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.640470027923584 tensor(16, device='cuda:0')\n",
            "lsy 4.15625 0.0\n",
            "pred tensor([-0.2671, -0.4424, -0.0265, -0.3643, -0.4500, -0.3838, -0.4084, -0.3315,\n",
            "        -0.4285, -0.1882, -0.4248, -0.4023, -0.2346, -0.2423, -0.1108, -0.1115,\n",
            "        -0.1946, -0.1699, -0.3723, -0.2301, -0.4343, -0.4033, -0.4058, -0.4412,\n",
            "        -0.0505], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6482090353965759 tensor(16, device='cuda:0')\n",
            "lsy 4.48046875 0.0\n",
            "pred tensor([-4.3408e-01, -9.4177e-02, -3.9844e-01, -2.5366e-01, -1.8909e-01,\n",
            "        -3.1982e-01, -2.9053e-01, -1.7920e-01, -1.7126e-01, -2.2290e-01,\n",
            "        -5.0000e-01, -4.8853e-01, -4.2139e-01, -4.5679e-01, -4.9585e-01,\n",
            "        -5.0000e-01, -4.6045e-01, -5.0000e-01, -5.0000e-01,  0.0000e+00,\n",
            "        -4.1306e-05,  0.0000e+00, -2.3842e-07,  0.0000e+00, -5.9605e-08],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6749985218048096 tensor(21, device='cuda:0')\n",
            "lsy 4.8125 0.0\n",
            "pred tensor([-0.4868, -0.3357, -0.4651, -0.2942, -0.4639, -0.4160, -0.3982, -0.4626,\n",
            "        -0.4497, -0.2253, -0.4194, -0.3555, -0.3879, -0.2996, -0.2915, -0.4746,\n",
            "        -0.4138, -0.3372, -0.4312, -0.4617, -0.3608, -0.0624, -0.2708, -0.2451,\n",
            "        -0.3760], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6740819215774536 tensor(17, device='cuda:0')\n",
            "lsy 5.140625 0.0\n",
            "pred tensor([-3.5938e-01, -4.7876e-01, -4.9878e-01, -4.9194e-01, -4.2796e-05,\n",
            "        -2.1482e-04, -1.7881e-07,  0.0000e+00, -1.1921e-07,  0.0000e+00,\n",
            "        -1.8418e-05, -2.3842e-07, -5.1546e-04, -4.2114e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.7388e-01, -4.8779e-01, -4.9487e-01, -4.9585e-01,\n",
            "        -4.8633e-01, -4.8901e-01, -4.4995e-01, -5.0000e-01, -4.5532e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6732552647590637 tensor(22, device='cuda:0')\n",
            "lsy 4.91796875 0.0\n",
            "pred tensor([-0.4988, -0.5000, -0.5000, -0.4824, -0.3562, -0.1078, -0.1825, -0.2360,\n",
            "        -0.2927, -0.4988, -0.4851, -0.5000, -0.4727, -0.5000, -0.4976, -0.3967,\n",
            "        -0.4976, -0.4990, -0.4653, -0.5000, -0.4341, -0.3696, -0.4902, -0.2368,\n",
            "        -0.1541], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6408570408821106 tensor(9, device='cuda:0')\n",
            "lsy 5.359375 0.0\n",
            "pred tensor([-0.4150, -0.4338, -0.2617, -0.4946, -0.4900, -0.4971, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4980, -0.4961, -0.4597, -0.5000, -0.4956, -0.4949, -0.5000,\n",
            "        -0.4531, -0.4810, -0.4644, -0.1793, -0.4956, -0.4727, -0.0318, -0.3750,\n",
            "        -0.1371], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6700170040130615 tensor(15, device='cuda:0')\n",
            "lsy 3.53125 0.0\n",
            "pred tensor([-3.6359e-06, -3.5763e-07, -1.0185e-03, -1.9491e-04, -5.9605e-08,\n",
            "        -2.3842e-06, -1.5845e-01, -1.3374e-02, -3.2642e-01, -3.9771e-01,\n",
            "        -2.3687e-04, -1.2726e-02, -3.6430e-03, -8.5999e-02, -5.1737e-04,\n",
            "        -1.2934e-04, -5.4240e-06, -4.2175e-02, -1.6665e-04, -1.1688e-02,\n",
            "        -3.2787e-03, -2.7800e-04, -6.6340e-05, -8.2159e-04, -4.9487e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6824057698249817 tensor(17, device='cuda:0')\n",
            "lsy 4.78515625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4985, -0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6669189929962158 tensor(13, device='cuda:0')\n",
            "lsy 4.36328125 0.0\n",
            "pred tensor([-0.5000, -0.4927, -0.4961, -0.4978, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4985, -0.5000, -0.5000, -0.5000, -0.3518, -0.4995, -0.4963, -0.4556,\n",
            "        -0.4836, -0.5000, -0.4915, -0.4902, -0.5000, -0.4922, -0.4709, -0.3740,\n",
            "        -0.4316], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6615458130836487 tensor(16, device='cuda:0')\n",
            "lsy 4.890625 0.0\n",
            "pred tensor([-4.7461e-01, -4.8755e-01, -4.9316e-01, -4.8340e-01, -4.9072e-01,\n",
            "        -4.7852e-01, -4.9829e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.6539e-03, -9.0003e-06, -2.3468e-02,\n",
            "        -6.2275e-04, -3.2949e-04, -1.6647e-02, -9.3445e-02, -1.9678e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6829908490180969 tensor(10, device='cuda:0')\n",
            "lsy 4.359375 0.0\n",
            "pred tensor([-0.3806, -0.3059, -0.4141, -0.4138, -0.1345, -0.0436, -0.0858, -0.2084,\n",
            "        -0.2153, -0.2759, -0.1990, -0.3379, -0.2766, -0.1153, -0.2378, -0.1865,\n",
            "        -0.2100, -0.3992, -0.2059, -0.3767, -0.0370, -0.1263, -0.1577, -0.2423,\n",
            "        -0.1392], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6737580895423889 tensor(19, device='cuda:0')\n",
            "lsy 3.16796875 0.0\n",
            "pred tensor([-0.3325, -0.3801, -0.3213, -0.2747, -0.3540, -0.3896, -0.4717, -0.3281,\n",
            "        -0.4514, -0.4209, -0.4604, -0.5000, -0.4539, -0.4578, -0.4197, -0.4570,\n",
            "        -0.3809, -0.5000, -0.4729, -0.5000, -0.4570, -0.4900, -0.4661, -0.4697,\n",
            "        -0.4519], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.651626706123352 tensor(10, device='cuda:0')\n",
            "lsy 3.275390625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.0922, -0.1807, -0.3943, -0.4993,\n",
            "        -0.4968, -0.5000, -0.4897, -0.5000, -0.3518, -0.5000, -0.4646, -0.5000,\n",
            "        -0.4939, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6730320453643799 tensor(13, device='cuda:0')\n",
            "lsy 2.65234375 0.0\n",
            "pred tensor([-0.5000, -0.4988, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.4829,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4993, -0.5000, -0.5000, -0.5000, -0.4988,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4932, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4995], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6468058228492737 tensor(12, device='cuda:0')\n",
            "lsy 2.65234375 0.0\n",
            "pred tensor([-5.0000e-01, -4.9390e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -1.6069e-03, -5.1856e-06, -4.2999e-02, -2.2054e-06,\n",
            "        -2.0027e-03, -1.9092e-01, -1.0657e-01, -8.7261e-04, -1.6193e-03,\n",
            "        -1.7271e-03, -1.4365e-05, -8.6486e-05, -2.4872e-03, -3.3379e-06,\n",
            "        -5.1689e-03, -2.5153e-05, -2.3842e-07, -3.7408e-04, -6.2408e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6706271171569824 tensor(17, device='cuda:0')\n",
            "lsy 2.623046875 0.0\n",
            "pred tensor([-7.7546e-05, -2.5034e-06, -1.4424e-05, -1.4544e-05,  0.0000e+00,\n",
            "        -4.2892e-04, -1.5724e-04, -1.1711e-02, -3.8319e-03, -4.2114e-02,\n",
            "        -3.0398e-06, -3.3966e-02, -1.3816e-04, -6.3400e-03, -4.1008e-04,\n",
            "        -5.6190e-03, -1.0260e-01, -1.7590e-01, -4.7913e-02, -3.9478e-01,\n",
            "        -3.0731e-02, -6.0760e-02, -3.8452e-02, -9.6985e-02, -1.7471e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6264030933380127 tensor(10, device='cuda:0')\n",
            "lsy 3.517578125 0.0\n",
            "pred tensor([-8.1863e-03, -5.3787e-04, -5.9605e-08, -1.8668e-04, -3.9749e-03,\n",
            "        -1.6475e-04,  0.0000e+00, -1.1325e-06,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -5.9605e-08, -3.5305e-03,  0.0000e+00, -1.9717e-04,\n",
            "        -8.7595e-04, -5.9605e-08, -7.8430e-03,  0.0000e+00, -5.6076e-03,\n",
            "        -4.1723e-07, -1.8152e-01, -7.2479e-03, -1.9608e-03, -6.2790e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6530267596244812 tensor(15, device='cuda:0')\n",
            "lsy 2.796875 0.0\n",
            "pred tensor([-0.1088, -0.1827, -0.4092, -0.1967, -0.0201, -0.2949, -0.2208, -0.2222,\n",
            "        -0.2700, -0.4897, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4976,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4993, -0.5000, -0.4961, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6610643267631531 tensor(13, device='cuda:0')\n",
            "lsy 2.70703125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4995, -0.4968, -0.5000, -0.4988,\n",
            "        -0.4978, -0.4995, -0.4353, -0.4897, -0.4404, -0.4856, -0.4910, -0.4666,\n",
            "        -0.4856], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6758890748023987 tensor(21, device='cuda:0')\n",
            "lsy 3.70703125 0.0\n",
            "pred tensor([-0.4946, -0.4526, -0.5000, -0.5000, -0.4888, -0.4187, -0.4893, -0.5000,\n",
            "        -0.4951, -0.4980, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6599883437156677 tensor(14, device='cuda:0')\n",
            "lsy 3.53125 0.0\n",
            "pred tensor([-1.7166e-05, -5.3644e-07, -1.1593e-04, -2.4343e-04, -1.5459e-03,\n",
            "        -9.3269e-04, -9.5367e-07, -4.1504e-03, -4.4441e-03, -1.4603e-05,\n",
            "        -4.6349e-03, -5.0507e-03, -1.1725e-01, -3.2812e-01, -1.1823e-01,\n",
            "        -3.7866e-01, -3.2104e-01, -2.6099e-01, -4.5557e-01, -1.3971e-03,\n",
            "        -2.1148e-04, -4.4507e-01, -1.0754e-01, -5.1994e-03, -4.3018e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6519797444343567 tensor(18, device='cuda:0')\n",
            "lsy 4.12890625 0.0\n",
            "pred tensor([-3.8892e-01, -2.9028e-01, -2.9236e-02, -6.9885e-02, -9.5749e-03,\n",
            "        -4.0503e-01, -2.7734e-01, -3.7671e-01, -3.6072e-02, -2.1729e-01,\n",
            "        -4.3152e-02, -3.2690e-01, -3.2617e-01, -2.3529e-02, -9.9365e-02,\n",
            "        -4.7743e-05, -1.7969e-01, -7.6355e-02, -5.2795e-03, -5.8212e-03,\n",
            "        -1.6138e-01, -2.4152e-04, -9.2316e-03, -2.3224e-02, -7.6151e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6552627682685852 tensor(13, device='cuda:0')\n",
            "23\n",
            "lsy 2.96484375 0.0\n",
            "pred tensor([-1.2054e-01, -1.5459e-03, -1.7297e-01, -2.4094e-02, -1.7464e-04,\n",
            "        -4.4861e-03, -1.8784e-02, -4.0531e-06, -2.1619e-01, -9.5215e-03,\n",
            "        -1.0729e-05, -2.8968e-04, -3.1590e-05, -1.8542e-01, -7.8535e-04,\n",
            "        -6.4135e-04, -3.0689e-03, -8.9188e-03, -2.6264e-03, -2.4536e-01,\n",
            "        -1.0841e-02, -9.4116e-02, -2.4023e-01, -3.1647e-02, -2.6050e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6897114515304565 tensor(20, device='cuda:0')\n",
            "lsy 3.013671875 0.0\n",
            "pred tensor([-0.3308, -0.5000, -0.0265, -0.5000, -0.1664, -0.1877, -0.4338, -0.3589,\n",
            "        -0.2111, -0.4861, -0.5000, -0.4863, -0.4719, -0.4109, -0.5000, -0.5000,\n",
            "        -0.4709, -0.0928, -0.4744, -0.3213, -0.5000, -0.3137, -0.4248, -0.3284,\n",
            "        -0.3853], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6542250514030457 tensor(14, device='cuda:0')\n",
            "lsy 3.58203125 0.0\n",
            "pred tensor([-5.7526e-03, -9.2163e-03, -7.0984e-02, -9.6863e-02, -3.1250e-01,\n",
            "        -4.7119e-01, -4.8608e-01, -5.0000e-01, -4.9731e-01, -4.6558e-01,\n",
            "        -5.0000e-01, -4.5483e-01, -2.8662e-01, -4.0845e-01, -4.4312e-01,\n",
            "         0.0000e+00, -1.4553e-03,  0.0000e+00, -2.1517e-05, -1.3709e-06,\n",
            "        -2.3842e-07,  0.0000e+00, -2.9206e-06, -5.8413e-06,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6458554863929749 tensor(16, device='cuda:0')\n",
            "lsy 2.91796875 0.0\n",
            "pred tensor([-1.7881e-07, -7.5459e-05, -6.9141e-06,  0.0000e+00, -3.0541e-04,\n",
            "        -5.9605e-08, -1.2517e-06, -8.6288e-03, -3.2723e-05, -6.9427e-03,\n",
            "         0.0000e+00, -1.7881e-07, -2.2054e-06, -1.0353e-04, -8.9884e-04,\n",
            "        -7.9041e-02, -1.9722e-03, -6.3992e-04, -2.9816e-02, -1.0262e-03,\n",
            "        -1.1665e-02, -3.0041e-03, -4.8590e-04, -2.8412e-02, -5.4646e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6440061926841736 tensor(9, device='cuda:0')\n",
            "lsy 2.80078125 0.0\n",
            "pred tensor([-4.1229e-02, -2.3687e-04, -2.3315e-02, -8.5754e-03, -2.0752e-03,\n",
            "        -1.1021e-04, -4.9934e-03, -1.6193e-03, -1.0376e-01, -9.8755e-02,\n",
            "        -2.4815e-03, -9.0561e-03, -8.1062e-06, -1.2573e-02, -1.0974e-01,\n",
            "        -3.7018e-02, -4.4287e-01, -6.5498e-03, -3.2187e-04, -2.8763e-02,\n",
            "        -3.9520e-03, -1.9493e-03, -4.6730e-03, -8.3191e-02, -5.2738e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6482813358306885 tensor(11, device='cuda:0')\n",
            "lsy 2.759765625 0.0\n",
            "pred tensor([-1.6571e-02, -9.3384e-03, -2.9540e-04, -1.5295e-01, -4.0527e-02,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9219e-01,\n",
            "        -5.0000e-01, -4.9976e-01, -4.2822e-01, -4.9927e-01, -4.0430e-01,\n",
            "        -4.9609e-01, -4.8682e-01, -4.8853e-01, -4.5386e-01, -4.4189e-01,\n",
            "        -4.5020e-01, -4.6167e-01, -4.9902e-01, -4.0869e-01, -4.3359e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6584110856056213 tensor(15, device='cuda:0')\n",
            "lsy 2.375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4944,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4998, -0.5000, -0.5000, -0.4973, -0.5000, -0.4941, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6590425968170166 tensor(14, device='cuda:0')\n",
            "lsy 2.51953125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.4976, -0.1984, -0.1909,\n",
            "        -0.4341, -0.0962, -0.4128, -0.0479, -0.2559, -0.4805, -0.2744, -0.4824,\n",
            "        -0.3645, -0.2961, -0.4651, -0.4321, -0.3091, -0.2378, -0.4907, -0.4915,\n",
            "        -0.0452], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6668163537979126 tensor(14, device='cuda:0')\n",
            "lsy 2.501953125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4988, -0.5000, -0.4958, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4995, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4995, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.3342], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6747207641601562 tensor(13, device='cuda:0')\n",
            "lsy 1.9150390625 0.0\n",
            "pred tensor([-0.1912, -0.3784, -0.1451, -0.1572, -0.3596, -0.0839, -0.3899, -0.0055,\n",
            "        -0.5000, -0.3064, -0.4309, -0.2089, -0.4937, -0.2067, -0.3372, -0.4114,\n",
            "        -0.2656, -0.4978, -0.4805, -0.4260, -0.4807, -0.4990, -0.4851, -0.4922,\n",
            "        -0.4136], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.660729169845581 tensor(8, device='cuda:0')\n",
            "lsy 1.9462890625 0.0\n",
            "pred tensor([-4.5288e-01, -3.7427e-01, -4.8706e-01, -3.6646e-01, -3.8599e-01,\n",
            "        -4.3018e-01, -4.9438e-01, -4.8511e-01, -4.7241e-01, -4.8145e-01,\n",
            "        -3.8696e-01, -4.8657e-01, -2.7919e-04, -2.2697e-03, -2.2424e-01,\n",
            "        -3.6285e-02, -2.2546e-01, -4.8999e-01, -4.8291e-01, -4.8389e-01,\n",
            "        -3.9917e-01, -5.0000e-01, -4.7290e-01, -3.7549e-01, -4.8877e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6812992095947266 tensor(21, device='cuda:0')\n",
            "lsy 2.80078125 0.0\n",
            "pred tensor([-4.9707e-01, -3.9429e-01, -5.0000e-01, -4.6240e-01, -3.0615e-01,\n",
            "        -3.7646e-01, -4.8901e-01, -2.6929e-01, -4.8389e-01, -4.0503e-01,\n",
            "        -4.7607e-01, -4.8755e-01, -4.9585e-01, -4.3579e-01, -5.0000e-01,\n",
            "        -4.8706e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -6.2764e-05, -7.6008e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6556723117828369 tensor(14, device='cuda:0')\n",
            "lsy 3.029296875 0.0\n",
            "pred tensor([-0.4778, -0.4690, -0.4761, -0.4731, -0.4695, -0.4919, -0.4893, -0.4663,\n",
            "        -0.4707, -0.4978, -0.4514, -0.4893, -0.4766, -0.4568, -0.4534, -0.4902,\n",
            "        -0.4846, -0.4963, -0.4077, -0.2729, -0.1603, -0.3967, -0.3284, -0.3726,\n",
            "        -0.3770], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6706352233886719 tensor(22, device='cuda:0')\n",
            "lsy 2.966796875 0.0\n",
            "pred tensor([-0.4509, -0.1401, -0.3494, -0.4285, -0.4314, -0.4409, -0.2142, -0.3057,\n",
            "        -0.4375, -0.2886, -0.2050, -0.4534, -0.4270, -0.2157, -0.3582, -0.4587,\n",
            "        -0.2888, -0.3325, -0.4502, -0.2808, -0.3052, -0.4124, -0.2993, -0.3186,\n",
            "        -0.3401], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6423215866088867 tensor(19, device='cuda:0')\n",
            "lsy 2.9921875 0.0\n",
            "pred tensor([-0.4685, -0.4814, -0.4963, -0.4844, -0.4805, -0.4429, -0.4653, -0.4944,\n",
            "        -0.2634, -0.4453, -0.5000, -0.4702, -0.4893, -0.4973, -0.3848, -0.4990,\n",
            "        -0.4268, -0.4998, -0.4182, -0.4810, -0.4885, -0.0743, -0.1917, -0.1189,\n",
            "        -0.1381], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6930546760559082 tensor(24, device='cuda:0')\n",
            "lsy 3.15234375 0.0\n",
            "pred tensor([-2.0923e-01, -5.3894e-02, -3.1686e-04, -1.8509e-02, -2.4918e-02,\n",
            "        -1.4467e-03, -1.2683e-01, -1.4392e-01, -1.0556e-04, -8.3130e-02,\n",
            "        -2.3157e-01, -4.9067e-04, -1.2476e-01, -1.4782e-03, -1.2219e-01,\n",
            "        -3.6564e-03, -3.8159e-01, -4.5395e-03, -2.3689e-03, -4.1723e-04,\n",
            "        -3.3512e-03, -1.0547e-01, -9.2468e-03, -6.8245e-03, -1.3428e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6623782515525818 tensor(20, device='cuda:0')\n",
            "lsy 3.525390625 0.0\n",
            "pred tensor([-0.4912, -0.4805, -0.5000, -0.4883, -0.5000, -0.4990, -0.4810, -0.4753,\n",
            "        -0.4971, -0.4717, -0.5000, -0.4819, -0.4839, -0.5000, -0.4990, -0.4985,\n",
            "        -0.4868, -0.4919, -0.4822, -0.5000, -0.4922, -0.4883, -0.4683, -0.4565,\n",
            "        -0.4902], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6455588340759277 tensor(15, device='cuda:0')\n",
            "lsy 2.89453125 0.0\n",
            "pred tensor([-0.4963, -0.4395, -0.4915, -0.4951, -0.4912, -0.4526, -0.5000, -0.4998,\n",
            "        -0.4746, -0.4998, -0.4985, -0.4888, -0.4995, -0.5000, -0.4924, -0.4768,\n",
            "        -0.4983, -0.4717, -0.4761, -0.4734, -0.4937, -0.5000, -0.4978, -0.5000,\n",
            "        -0.4751], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6891608238220215 tensor(24, device='cuda:0')\n",
            "lsy 3.64453125 0.0\n",
            "pred tensor([-0.0831, -0.0388, -0.4993, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4995, -0.4990, -0.4915, -0.4922, -0.4929, -0.4966, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4880, -0.4937,\n",
            "        -0.2686], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6265407204627991 tensor(17, device='cuda:0')\n",
            "lsy 3.46484375 0.0\n",
            "pred tensor([-0.3577, -0.4026, -0.2280, -0.4001, -0.0168, -0.2113, -0.4509, -0.3777,\n",
            "        -0.4641, -0.2097, -0.4963, -0.4302, -0.5000, -0.4980, -0.5000, -0.5000,\n",
            "        -0.4629, -0.4834, -0.4785, -0.5000, -0.5000, -0.5000, -0.5000, -0.4988,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6666795611381531 tensor(25, device='cuda:0')\n",
            "lsy 3.919921875 0.0\n",
            "pred tensor([-0.2086, -0.0379, -0.0296, -0.1777, -0.2449, -0.2905, -0.4150, -0.4971,\n",
            "        -0.4255, -0.4600, -0.3799, -0.4346, -0.2676, -0.4746, -0.2734, -0.4348,\n",
            "        -0.4980, -0.5000, -0.4482, -0.4441, -0.4724, -0.4961, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6676565408706665 tensor(24, device='cuda:0')\n",
            "lsy 3.927734375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4871, -0.4609, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4990, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4958, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6294203400611877 tensor(15, device='cuda:0')\n",
            "lsy 5.27734375 0.0\n",
            "pred tensor([-5.0000e-01, -3.8281e-01, -5.0000e-01, -4.4897e-01, -4.8608e-01,\n",
            "        -4.6484e-01, -4.8413e-01,  0.0000e+00, -4.8280e-06,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00, -1.4651e-04,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.1921e-06,  0.0000e+00, -1.1921e-07,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6818891167640686 tensor(22, device='cuda:0')\n",
            "lsy 5.91015625 0.0\n",
            "pred tensor([ 0.0000e+00, -2.4438e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.9670e-06,  0.0000e+00,  0.0000e+00, -4.7684e-07,  0.0000e+00,\n",
            "        -1.6034e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.2517e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.3530e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6266196966171265 tensor(10, device='cuda:0')\n",
            "lsy 3.8515625 0.0\n",
            "pred tensor([-0.0045, -0.0421, -0.0650, -0.1219, -0.0135, -0.1696, -0.0949, -0.3757,\n",
            "        -0.3418, -0.3992, -0.4802, -0.4092, -0.4819, -0.4924, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4951,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6677043437957764 tensor(16, device='cuda:0')\n",
            "lsy 3.794921875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4849, -0.4829, -0.4150,\n",
            "        -0.3787, -0.4834, -0.4172, -0.4871, -0.4626, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4973], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6228745579719543 tensor(10, device='cuda:0')\n",
            "lsy 3.71484375 0.0\n",
            "pred tensor([-0.0014, -0.0007, -0.0193, -0.0110, -0.1935, -0.0145, -0.1306, -0.0074,\n",
            "        -0.4343, -0.3684, -0.0563, -0.3481, -0.4727, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998,\n",
            "        -0.4529], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6646261215209961 tensor(17, device='cuda:0')\n",
            "lsy 3.298828125 0.0\n",
            "pred tensor([-0.4949, -0.4912, -0.4670, -0.4465, -0.4583, -0.4697, -0.4963, -0.5000,\n",
            "        -0.4800, -0.4946, -0.4985, -0.4526, -0.4917, -0.5000, -0.4680, -0.4683,\n",
            "        -0.4771, -0.5000, -0.4990, -0.4475, -0.4375, -0.4509, -0.4844, -0.4280,\n",
            "        -0.3967], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6639514565467834 tensor(14, device='cuda:0')\n",
            "lsy 2.998046875 0.0\n",
            "pred tensor([-1.8530e-03, -2.7776e-05, -5.8327e-03, -2.3842e-07, -1.2924e-02,\n",
            "        -5.8413e-06, -1.4901e-06, -1.6093e-06, -7.5102e-04, -2.5063e-03,\n",
            "        -7.1526e-07, -3.5644e-05, -4.1723e-07, -9.2163e-03, -1.9272e-02,\n",
            "        -3.4070e-04, -6.1035e-02, -2.5451e-05, -3.8147e-04, -1.3113e-06,\n",
            "        -3.4666e-04, -1.3113e-06, -6.1226e-03, -2.4796e-05, -3.0708e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6315310001373291 tensor(7, device='cuda:0')\n",
            "lsy 3.2734375 0.0\n",
            "pred tensor([-6.5756e-04, -2.8458e-03, -6.8245e-03, -1.5076e-02, -3.9087e-01,\n",
            "        -4.9658e-01, -5.0000e-01, -4.9805e-01, -4.5117e-01, -3.5669e-01,\n",
            "        -1.8823e-01, -2.6123e-01, -1.5869e-01, -2.9469e-03, -2.5986e-02,\n",
            "        -4.0936e-04, -1.3947e-05, -7.7486e-07, -3.2043e-03, -5.4216e-04,\n",
            "        -3.5596e-01, -4.1284e-01, -4.6655e-01, -4.5410e-01, -4.6997e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6922969818115234 tensor(13, device='cuda:0')\n",
            "lsy 3.509765625 0.0\n",
            "pred tensor([-5.2109e-03, -2.3484e-05, -2.9343e-02, -1.0556e-04, -3.1471e-05,\n",
            "        -6.5660e-04, -1.0269e-02, -2.5964e-04, -9.5749e-03, -1.9226e-03,\n",
            "        -7.0286e-04, -1.1688e-01, -2.9388e-02, -1.8673e-03, -5.4359e-05,\n",
            "        -1.3330e-01, -7.2937e-02, -3.5498e-01, -1.5784e-01, -8.9294e-02,\n",
            "        -1.1426e-01, -2.5244e-01, -3.1860e-01, -1.9873e-01, -3.5010e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6760500073432922 tensor(13, device='cuda:0')\n",
            "lsy 3.728515625 0.0\n",
            "pred tensor([-0.0964, -0.2145, -0.4622, -0.4277, -0.5000, -0.5000, -0.4990, -0.4246,\n",
            "        -0.4343, -0.4736, -0.4871, -0.4700, -0.4421, -0.4688, -0.4578, -0.4690,\n",
            "        -0.4001, -0.4734, -0.3108, -0.4648, -0.4824, -0.4980, -0.4700, -0.4900,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6444089412689209 tensor(13, device='cuda:0')\n",
            "lsy 3.25 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4873, -0.5000, -0.4951,\n",
            "        -0.5000, -0.4685, -0.4883, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6717551350593567 tensor(17, device='cuda:0')\n",
            "lsy 3.412109375 0.0\n",
            "pred tensor([-0.5000, -0.4983, -0.5000, -0.5000, -0.5000, -0.5000, -0.4990, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.2766, -0.4468, -0.4734,\n",
            "        -0.5000, -0.4568, -0.4319, -0.4373, -0.4937, -0.4692, -0.4546, -0.3948,\n",
            "        -0.4995], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.657538652420044 tensor(18, device='cuda:0')\n",
            "lsy 3.015625 0.0\n",
            "pred tensor([-0.4910, -0.4880, -0.4873, -0.5000, -0.5000, -0.4849, -0.3911, -0.4749,\n",
            "        -0.4624, -0.4417, -0.4949, -0.4910, -0.4980, -0.5000, -0.4780, -0.4985,\n",
            "        -0.5000, -0.4744, -0.5000, -0.4988, -0.4995, -0.5000, -0.4927, -0.5000,\n",
            "        -0.4944], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6482110619544983 tensor(12, device='cuda:0')\n",
            "lsy 3.294921875 0.0\n",
            "pred tensor([-0.5000, -0.4985, -0.5000, -0.5000, -0.4998, -0.5000, -0.4973, -0.4983,\n",
            "        -0.4783, -0.4722, -0.4651, -0.5000, -0.4309, -0.5000, -0.4578, -0.5000,\n",
            "        -0.4941, -0.5000, -0.4868, -0.4919, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4976], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6713107824325562 tensor(21, device='cuda:0')\n",
            "lsy 4.515625 0.0\n",
            "pred tensor([-0.5000, -0.4956, -0.4995, -0.5000, -0.5000, -0.5000, -0.4619, -0.4358,\n",
            "        -0.3994, -0.5000, -0.5000, -0.4880, -0.4392, -0.4932, -0.4524, -0.4673,\n",
            "        -0.4990, -0.4500, -0.4851, -0.4846, -0.4836, -0.4614, -0.4749, -0.4990,\n",
            "        -0.4890], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6747878789901733 tensor(14, device='cuda:0')\n",
            "lsy 3.45703125 0.0\n",
            "pred tensor([-0.4380, -0.4167, -0.4255, -0.4944, -0.4719, -0.4485, -0.5000, -0.4736,\n",
            "        -0.4807, -0.4897, -0.4895, -0.4512, -0.4658, -0.4873, -0.5000, -0.4893,\n",
            "        -0.4961, -0.4441, -0.5000, -0.4868, -0.5000, -0.4756, -0.4893, -0.4624,\n",
            "        -0.3018], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6859018206596375 tensor(17, device='cuda:0')\n",
            "24\n",
            "lsy 3.87109375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -9.4771e-06,  0.0000e+00, -2.1458e-06, -1.6689e-06, -1.4424e-04,\n",
            "        -8.3804e-05, -3.6548e-01, -1.3721e-01, -3.6816e-01, -4.7290e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9951e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6482606530189514 tensor(14, device='cuda:0')\n",
            "lsy 3.791015625 0.0\n",
            "pred tensor([-0.4622, -0.4846, -0.5000, -0.4912, -0.4893, -0.4502, -0.4822, -0.4954,\n",
            "        -0.3953, -0.4922, -0.2411, -0.4724, -0.4939, -0.0556, -0.0478, -0.1248,\n",
            "        -0.3804, -0.2510, -0.2487, -0.2352, -0.0092, -0.0846, -0.0502, -0.3699,\n",
            "        -0.0920], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6693753004074097 tensor(17, device='cuda:0')\n",
            "lsy 3.3125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4910, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6493967175483704 tensor(13, device='cuda:0')\n",
            "lsy 4.7421875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4961, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4905, -0.4944,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4976], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6441701650619507 tensor(11, device='cuda:0')\n",
            "lsy 3.208984375 0.0\n",
            "pred tensor([-1.2004e-04,  0.0000e+00, -2.7418e-06, -2.3842e-07,  0.0000e+00,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7881e-07,\n",
            "        -4.5300e-06, -4.2319e-06,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
            "        -1.0133e-06, -1.6689e-06,  0.0000e+00, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9609e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6353121995925903 tensor(13, device='cuda:0')\n",
            "lsy 4.48828125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4976, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6878058314323425 tensor(17, device='cuda:0')\n",
            "lsy 4.40234375 0.0\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -3.7193e-04, -1.3113e-06, -8.4817e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6379740834236145 tensor(10, device='cuda:0')\n",
            "lsy 3.8671875 0.0\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
            "        -4.0352e-05,  0.0000e+00, -8.1658e-06, -1.3292e-05, -5.9605e-08,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.1921e-07, -4.0531e-06,  0.0000e+00, -3.1710e-05, -8.4043e-06,\n",
            "        -5.9605e-08, -1.2871e-02, -8.1658e-06, -1.7285e-06, -7.8869e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6837874054908752 tensor(23, device='cuda:0')\n",
            "lsy 3.853515625 0.0\n",
            "pred tensor([-2.7418e-06, -7.4244e-04, -1.2243e-04, -3.5429e-04, -1.2238e-02,\n",
            "        -5.6267e-04, -3.9291e-03, -3.8544e-02, -2.2693e-01, -3.2730e-03,\n",
            "        -1.0841e-02, -9.8133e-04, -4.5532e-02, -4.1199e-02, -9.0637e-02,\n",
            "        -2.6270e-01, -4.0161e-01, -4.8926e-01, -9.2590e-02, -4.7192e-01,\n",
            "        -4.5581e-01, -4.9731e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6628872156143188 tensor(15, device='cuda:0')\n",
            "lsy 4.3359375 0.0\n",
            "pred tensor([-0.4949, -0.5000, -0.5000, -0.4990, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4949, -0.5000, -0.5000, -0.5000, -0.5000, -0.4990, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6559228897094727 tensor(17, device='cuda:0')\n",
            "lsy 3.787109375 0.0\n",
            "pred tensor([-2.2650e-06, -9.6035e-04, -7.7629e-04, -3.5763e-05, -1.1505e-02,\n",
            "        -5.7068e-03, -1.8921e-02, -7.8308e-02, -1.8616e-01, -4.3701e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -3.5763e-06, -7.5459e-05, -5.5850e-05, -4.4167e-05,\n",
            "        -4.2677e-05, -1.1861e-05, -1.1104e-04, -1.7941e-05, -8.7619e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.670132040977478 tensor(11, device='cuda:0')\n",
            "lsy 3.716796875 0.0\n",
            "pred tensor([-2.5511e-04, -7.5245e-04, -2.4967e-03, -2.4826e-02, -2.4445e-02,\n",
            "        -3.3264e-02, -1.5808e-02, -5.3139e-03, -1.1823e-01, -3.3643e-01,\n",
            "        -2.0203e-01, -4.7388e-01, -1.2140e-01, -1.1743e-01, -2.1790e-01,\n",
            "        -5.7281e-02, -4.6802e-01, -1.6861e-02, -1.8481e-01, -4.9591e-02,\n",
            "        -1.8127e-02, -2.9926e-03, -4.3213e-02, -6.1432e-02, -6.4148e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6621733903884888 tensor(14, device='cuda:0')\n",
            "lsy 3.658203125 0.0\n",
            "pred tensor([-9.6619e-02, -4.0009e-02, -1.7664e-01, -2.5537e-01, -1.8762e-01,\n",
            "        -3.9458e-05, -1.6248e-01, -7.8735e-02, -1.3623e-01, -4.3411e-03,\n",
            "        -9.7534e-02, -3.5889e-01, -2.1252e-01, -4.0845e-01, -4.6265e-01,\n",
            "        -3.1641e-01, -5.0000e-01, -4.2798e-01, -5.0000e-01, -4.9390e-01,\n",
            "        -4.8950e-01, -5.0000e-01, -5.0000e-01, -4.7803e-01, -4.9780e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.670376718044281 tensor(15, device='cuda:0')\n",
            "lsy 3.224609375 0.0\n",
            "pred tensor([-0.5000, -0.4985, -0.4976, -0.4966, -0.4561, -0.4080, -0.1136, -0.0039,\n",
            "        -0.4375, -0.2766, -0.1433, -0.0144, -0.0249, -0.4866, -0.2876, -0.4609,\n",
            "        -0.4854, -0.0530, -0.2925, -0.2910, -0.0495, -0.0025, -0.0045, -0.3059,\n",
            "        -0.4644], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6215654015541077 tensor(7, device='cuda:0')\n",
            "lsy 3.11328125 0.0\n",
            "pred tensor([-2.0264e-01, -4.8584e-02, -4.7646e-03, -9.4175e-06, -1.1035e-01,\n",
            "        -4.3823e-02, -1.6037e-02, -3.8910e-02, -7.0068e-02, -1.0551e-02,\n",
            "        -2.9834e-01, -1.4685e-01, -4.1553e-01, -4.4580e-01, -3.9404e-01,\n",
            "        -4.5068e-01, -4.8755e-01, -5.0000e-01, -6.7383e-02, -6.3515e-03,\n",
            "        -4.7565e-05, -3.6011e-01, -8.3148e-05, -3.7122e-04, -3.6621e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6552513837814331 tensor(12, device='cuda:0')\n",
            "lsy 3.7421875 0.0\n",
            "pred tensor([-2.3895e-02, -1.8811e-04, -1.1223e-02, -1.0166e-03, -1.2755e-05,\n",
            "        -1.2696e-05, -2.3413e-04, -5.1618e-05, -1.4997e-04, -5.4199e-02,\n",
            "        -2.8595e-02, -1.8597e-04, -2.0790e-03, -1.5221e-02, -2.2873e-02,\n",
            "        -1.7166e-02, -2.0065e-02, -4.7922e-05, -6.0141e-05, -5.2035e-05,\n",
            "        -4.2358e-02, -7.5388e-04, -8.5510e-02, -4.2798e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6477578282356262 tensor(10, device='cuda:0')\n",
            "lsy 3.19921875 0.0\n",
            "pred tensor([-2.9395e-01, -2.9834e-01, -2.9614e-01, -4.7656e-01, -4.9512e-01,\n",
            "        -5.0000e-01, -2.0264e-01, -4.9854e-01, -4.6875e-01, -1.3351e-05,\n",
            "        -3.3838e-01, -1.5527e-01, -1.7444e-01, -1.4539e-01, -2.0422e-01,\n",
            "        -4.2358e-01, -4.8975e-01, -4.3335e-01, -4.5410e-01, -3.7451e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.7534e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6907021403312683 tensor(19, device='cuda:0')\n",
            "lsy 3.283203125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4966, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.69558185338974 tensor(17, device='cuda:0')\n",
            "lsy 3.494140625 0.0\n",
            "pred tensor([-2.2485e-01, -1.7883e-01, -1.0391e-02, -2.9739e-02, -7.5439e-02,\n",
            "        -3.3188e-03, -1.2947e-02,  0.0000e+00,  0.0000e+00, -4.8943e-03,\n",
            "        -2.9087e-05, -9.9277e-04, -6.6161e-06, -1.9089e-02, -1.7822e-01,\n",
            "        -1.0931e-01, -4.9585e-01, -4.7241e-01, -4.3970e-01, -4.7021e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.8462e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6871256232261658 tensor(19, device='cuda:0')\n",
            "lsy 3.05078125 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9707e-01, -2.4963e-02,\n",
            "        -2.2829e-05, -6.7949e-06, -5.0018e-02, -6.2523e-03, -2.1252e-01,\n",
            "        -1.1861e-05, -7.2327e-03, -5.0140e-02, -1.5974e-05, -3.0823e-03,\n",
            "        -1.0010e-02, -2.1324e-03, -9.4175e-04, -1.2045e-03, -1.4229e-02,\n",
            "        -4.8876e-05, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9780e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.659116804599762 tensor(17, device='cuda:0')\n",
            "lsy 3.236328125 0.0\n",
            "pred tensor([-5.0000e-01, -4.9463e-01, -4.8584e-01, -4.8755e-01, -2.2717e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.8267e-01, -4.7241e-01, -4.5166e-01, -4.3701e-01, -1.7607e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6825108528137207 tensor(16, device='cuda:0')\n",
            "lsy 2.345703125 0.0\n",
            "pred tensor([-2.0096e-02, -2.8551e-05, -2.3071e-02,  0.0000e+00, -2.9469e-04,\n",
            "        -3.2978e-03, -4.5300e-04, -6.0701e-04, -8.7786e-04, -3.5950e-02,\n",
            "        -2.5223e-02, -1.6553e-01, -1.1917e-02, -1.9031e-01, -1.5674e-01,\n",
            "        -5.9605e-07, -1.3664e-02, -3.2990e-02, -3.2568e-01, -4.3457e-01,\n",
            "        -4.8413e-01, -4.7461e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6463155150413513 tensor(14, device='cuda:0')\n",
            "lsy 3.0546875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4739, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4880], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6601884365081787 tensor(8, device='cuda:0')\n",
            "lsy 4.390625 0.0\n",
            "pred tensor([-0.4973, -0.4993, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4995, -0.4724, -0.4993, -0.3818, -0.5000, -0.4968,\n",
            "        -0.4558, -0.4988, -0.2446, -0.4121, -0.4878, -0.4963, -0.4910, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6478800177574158 tensor(11, device='cuda:0')\n",
            "lsy 3.7890625 0.0\n",
            "pred tensor([-3.6890e-01, -4.1870e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9951e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -2.3842e-07, -2.7156e-04, -1.7881e-07, -6.6757e-06, -1.0729e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6476379632949829 tensor(8, device='cuda:0')\n",
            "lsy 3.51953125 0.0\n",
            "pred tensor([-3.3379e-06, -1.6541e-02, -4.0186e-01, -3.2861e-01, -4.5044e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.632434606552124 tensor(8, device='cuda:0')\n",
            "lsy 3.94921875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4922,\n",
            "        -0.4619], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6569262742996216 tensor(10, device='cuda:0')\n",
            "lsy 3.873046875 0.0\n",
            "pred tensor([-0.5000, -0.4863, -0.4958, -0.4954, -0.4841, -0.4995, -0.4932, -0.5000,\n",
            "        -0.5000, -0.4504, -0.4163, -0.4561, -0.4814, -0.4043, -0.5000, -0.2876,\n",
            "        -0.4968, -0.4900, -0.5000, -0.4277, -0.4470, -0.4849, -0.4944, -0.4722,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6243957877159119 tensor(6, device='cuda:0')\n",
            "lsy 3.900390625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4988, -0.5000, -0.4915, -0.5000, -0.5000,\n",
            "        -0.4968, -0.4282, -0.4526, -0.5000, -0.4983, -0.5000, -0.4956, -0.5000,\n",
            "        -0.4128, -0.4956, -0.4951, -0.4976, -0.4993, -0.3821, -0.4939, -0.4890,\n",
            "        -0.4895], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6513134837150574 tensor(14, device='cuda:0')\n",
            "lsy 4.01171875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4883, -0.4690, -0.4980, -0.4548, -0.3821,\n",
            "        -0.4839, -0.4380, -0.4915, -0.3608, -0.4236, -0.2125, -0.4832, -0.1973,\n",
            "        -0.4927, -0.4114, -0.4172, -0.3042, -0.3628, -0.0849, -0.3291, -0.4409,\n",
            "        -0.3350], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6486417055130005 tensor(9, device='cuda:0')\n",
            "lsy 4.99609375 0.0\n",
            "pred tensor([-0.4170, -0.4343, -0.2681, -0.3403, -0.3704, -0.3525, -0.0768, -0.2352,\n",
            "        -0.1746, -0.1693, -0.2151, -0.0443, -0.0151, -0.3196, -0.0634, -0.0373,\n",
            "        -0.1125, -0.1300, -0.3020, -0.3013, -0.2162, -0.4084, -0.3669, -0.4563,\n",
            "        -0.4932], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.7019424438476562 tensor(16, device='cuda:0')\n",
            "lsy 4.4609375 0.0\n",
            "pred tensor([-0.4968, -0.4807, -0.4680, -0.4822, -0.4937, -0.4346, -0.0542, -0.4036,\n",
            "        -0.4297, -0.1234, -0.4844, -0.4517, -0.4797, -0.3772, -0.1423, -0.0488,\n",
            "        -0.0601, -0.1334, -0.0586, -0.3333, -0.0051, -0.0164, -0.0132, -0.0034,\n",
            "        -0.0858], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.645507276058197 tensor(13, device='cuda:0')\n",
            "lsy 4.84375 0.0\n",
            "pred tensor([-0.5000, -0.4939, -0.5000, -0.4690, -0.5000, -0.5000, -0.4954, -0.4731,\n",
            "        -0.4836, -0.4985, -0.4807, -0.4875, -0.4893, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4990, -0.4941, -0.4978,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6364851593971252 tensor(9, device='cuda:0')\n",
            "lsy 4.515625 0.0\n",
            "pred tensor([-0.5000, -0.4480, -0.4495, -0.4741, -0.4541, -0.5000, -0.4941, -0.4404,\n",
            "        -0.5000, -0.5000, -0.4941, -0.5000, -0.4983, -0.4448, -0.4895, -0.4141,\n",
            "        -0.4958, -0.4895, -0.4658, -0.5000, -0.4375, -0.4812, -0.5000, -0.5000,\n",
            "        -0.4985], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6532774567604065 tensor(16, device='cuda:0')\n",
            "lsy 5.8828125 0.0\n",
            "pred tensor([-0.4421, -0.0284, -0.3269, -0.5000, -0.5000, -0.4514, -0.4915, -0.4988,\n",
            "        -0.4946, -0.3879, -0.3711, -0.4580, -0.5000, -0.4771, -0.3403, -0.4038,\n",
            "        -0.4780, -0.4971, -0.4944, -0.5000, -0.4302, -0.4514, -0.4966, -0.4949,\n",
            "        -0.4912], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6615685224533081 tensor(21, device='cuda:0')\n",
            "lsy 5.61328125 0.0\n",
            "pred tensor([-0.4277, -0.4832, -0.4983, -0.4338, -0.4968, -0.4241, -0.4805, -0.5000,\n",
            "        -0.4663, -0.3979, -0.4097, -0.5000, -0.4897, -0.4890, -0.4895, -0.4988,\n",
            "        -0.5000, -0.5000, -0.4998, -0.4941, -0.4785, -0.5000, -0.5000, -0.4961,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.674098014831543 tensor(16, device='cuda:0')\n",
            "lsy 5.5390625 0.0\n",
            "pred tensor([-0.1589, -0.0503, -0.3193, -0.4548, -0.3096, -0.4485, -0.1204, -0.4905,\n",
            "        -0.4082, -0.0040, -0.5000, -0.0518, -0.2969, -0.1323, -0.3533, -0.0815,\n",
            "        -0.2173, -0.4333, -0.3240, -0.3865, -0.4827, -0.4546, -0.5000, -0.4905,\n",
            "        -0.4597], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6676055192947388 tensor(15, device='cuda:0')\n",
            "lsy 6.55859375 0.0\n",
            "pred tensor([-4.4165e-01, -4.9658e-01, -5.0000e-01, -4.9536e-01, -4.6118e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -2.4438e-06, -1.0133e-05,\n",
            "        -2.4438e-06, -2.6321e-03, -8.8978e-04, -1.1545e-04, -1.1711e-02,\n",
            "        -4.3154e-05, -8.6451e-04, -9.8896e-04, -1.4305e-05, -2.6352e-02,\n",
            "        -7.4506e-06, -4.5300e-06, -9.3579e-06, -2.1332e-02, -2.7588e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6624718904495239 tensor(16, device='cuda:0')\n",
            "25\n",
            "lsy 4.8828125 0.0\n",
            "pred tensor([-6.0158e-03, -3.2187e-05, -2.8300e-04, -1.0242e-03, -2.9421e-04,\n",
            "        -7.5102e-06, -1.4324e-03, -8.4043e-06, -1.1921e-07, -1.0338e-03,\n",
            "        -1.6689e-06,  0.0000e+00, -4.4174e-03, -3.2067e-05, -2.8610e-06,\n",
            "        -5.9009e-06, -3.6955e-06, -3.2101e-03, -5.9652e-04, -1.4305e-06,\n",
            "        -3.9404e-01, -4.8267e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6514320373535156 tensor(14, device='cuda:0')\n",
            "lsy 5.296875 0.0\n",
            "pred tensor([-0.5000, -0.4978, -0.5000, -0.4983, -0.5000, -0.5000, -0.5000, -0.4844,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4844, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6772478222846985 tensor(18, device='cuda:0')\n",
            "lsy 5.7890625 0.0\n",
            "pred tensor([-0.5000, -0.4968, -0.4634, -0.4771, -0.4429, -0.4961, -0.4429, -0.4199,\n",
            "        -0.4661, -0.2942, -0.3823, -0.4177, -0.3669, -0.3933, -0.2163, -0.4937,\n",
            "        -0.3984, -0.3777, -0.3279, -0.4363, -0.3162, -0.3923, -0.4128, -0.4961,\n",
            "        -0.3796], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6539593935012817 tensor(13, device='cuda:0')\n",
            "lsy 5.92578125 0.0\n",
            "pred tensor([-0.3682, -0.4827, -0.4719, -0.4814, -0.4316, -0.4548, -0.3279, -0.4299,\n",
            "        -0.3828, -0.5000, -0.3711, -0.4456, -0.4800, -0.3928, -0.4541, -0.4102,\n",
            "        -0.4512, -0.4294, -0.4182, -0.3782, -0.4807, -0.4797, -0.4705, -0.3591,\n",
            "        -0.3779], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6498963236808777 tensor(12, device='cuda:0')\n",
            "lsy 5.1328125 0.0\n",
            "pred tensor([-7.9870e-06, -2.0676e-03, -5.1856e-05, -2.2087e-03, -8.4305e-03,\n",
            "        -7.8535e-04, -7.8003e-02, -3.7720e-02, -1.0583e-01, -3.5449e-01,\n",
            "        -4.4385e-01, -1.4502e-01, -3.6987e-01, -3.9575e-01, -3.7744e-01,\n",
            "        -4.4629e-01, -1.5823e-02, -9.1125e-02, -8.1100e-03, -8.4915e-03,\n",
            "        -4.6921e-03, -7.2241e-04, -5.0888e-03, -1.8097e-02, -3.4302e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6367142796516418 tensor(14, device='cuda:0')\n",
            "lsy 5.51171875 0.0\n",
            "pred tensor([-0.0048, -0.0010, -0.0080, -0.1753, -0.1593, -0.0391, -0.0758, -0.3369,\n",
            "        -0.1990, -0.1952, -0.0019, -0.1998, -0.0985, -0.1482, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6608253717422485 tensor(17, device='cuda:0')\n",
            "lsy 6.02734375 0.0\n",
            "pred tensor([-0.0236, -0.3188, -0.5000, -0.3748, -0.1982, -0.3311, -0.0072, -0.1705,\n",
            "        -0.0022, -0.1638, -0.0594, -0.3066, -0.4697, -0.3770, -0.4158, -0.4678,\n",
            "        -0.5000, -0.4534, -0.4956, -0.3513, -0.5000, -0.4817, -0.4973, -0.4971,\n",
            "        -0.4670], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6742870211601257 tensor(17, device='cuda:0')\n",
            "lsy 5.54296875 0.0\n",
            "pred tensor([-0.4941, -0.4795, -0.4683, -0.4331, -0.4937, -0.4990, -0.4158, -0.4941,\n",
            "        -0.4934, -0.3474, -0.5000, -0.4297, -0.4998, -0.4875, -0.4788, -0.4966,\n",
            "        -0.4412, -0.4038, -0.3423, -0.3491, -0.4006, -0.2961, -0.4917, -0.4866,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.7160883545875549 tensor(21, device='cuda:0')\n",
            "lsy 5.9140625 0.0\n",
            "pred tensor([-4.4458e-01, -3.6255e-02, -1.2793e-01, -4.3359e-01, -2.6416e-01,\n",
            "        -2.4548e-01, -9.1248e-03, -2.3098e-03, -3.1686e-04, -4.9472e-06,\n",
            "        -3.4189e-04, -3.6068e-03, -4.1723e-07, -2.6226e-04, -9.5654e-04,\n",
            "        -1.1139e-02, -1.9045e-03, -2.1350e-01, -4.0293e-04, -1.9089e-02,\n",
            "        -8.6129e-05, -1.5454e-01, -3.3436e-03, -5.0116e-04, -4.0741e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6630060076713562 tensor(19, device='cuda:0')\n",
            "lsy 5.32421875 0.0\n",
            "pred tensor([-1.3723e-03, -1.8005e-02, -4.1931e-02, -4.3396e-02, -9.4175e-04,\n",
            "        -6.6161e-06,  0.0000e+00, -2.0862e-06,  0.0000e+00, -1.1867e-04,\n",
            "        -5.9013e-03, -2.4199e-05, -3.4523e-04, -2.3823e-03, -1.4114e-02,\n",
            "        -2.3651e-03, -6.2927e-02, -1.7807e-02, -6.1157e-02, -2.5375e-02,\n",
            "        -2.3376e-02, -4.5738e-03, -4.2462e-04, -7.1669e-04, -5.5504e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6537918448448181 tensor(14, device='cuda:0')\n",
            "lsy 3.935546875 0.0\n",
            "pred tensor([-0.4895, -0.5000, -0.4939, -0.4822, -0.4954, -0.4998, -0.5000, -0.5000,\n",
            "        -0.4849, -0.4658, -0.4978, -0.5000, -0.5000, -0.5000, -0.4973, -0.4817,\n",
            "        -0.5000, -0.5000, -0.4656, -0.4998, -0.5000, -0.4739, -0.4751, -0.4963,\n",
            "        -0.4829], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6711462140083313 tensor(21, device='cuda:0')\n",
            "lsy 4.7421875 0.0\n",
            "pred tensor([-0.4922, -0.4790, -0.5000, -0.5000, -0.5000, -0.5000, -0.4966, -0.5000,\n",
            "        -0.4949, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4607, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.670680046081543 tensor(15, device='cuda:0')\n",
            "lsy 3.4296875 0.0\n",
            "pred tensor([-4.1602e-01, -5.0000e-01, -4.1553e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.9976e-01, -5.0000e-01, -5.0000e-01,\n",
            "         0.0000e+00, -2.4152e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -5.9605e-08, -5.9605e-08, -4.3511e-06, -5.1439e-05,  0.0000e+00,\n",
            "        -9.3579e-06, -1.1921e-07, -1.7881e-06, -1.0170e-02, -1.1921e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6541878581047058 tensor(14, device='cuda:0')\n",
            "lsy 4.07421875 0.0\n",
            "pred tensor([-8.9407e-07, -6.0856e-05, -6.0201e-06,  0.0000e+00, -2.9802e-07,\n",
            "        -2.3842e-07, -2.9802e-07,  0.0000e+00, -3.9935e-06, -7.9036e-05,\n",
            "        -5.9605e-08, -1.3351e-02, -9.3872e-02, -2.5122e-01, -4.9292e-01,\n",
            "        -4.8413e-01, -4.7803e-01, -4.4263e-01, -4.4849e-01, -4.7949e-01,\n",
            "        -4.4946e-01, -3.4814e-01, -2.0923e-01, -1.0033e-02, -6.0141e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.639523983001709 tensor(7, device='cuda:0')\n",
            "lsy 2.865234375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4895, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4976, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4736], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6531869769096375 tensor(10, device='cuda:0')\n",
            "lsy 3.421875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4961, -0.5000, -0.5000, -0.5000, -0.4973, -0.4797,\n",
            "        -0.4937, -0.4985, -0.5000, -0.4998, -0.4971, -0.5000, -0.5000, -0.4971,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4976, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6470481157302856 tensor(7, device='cuda:0')\n",
            "lsy 3.302734375 0.0\n",
            "pred tensor([-0.4995, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4758, -0.4993, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6511884927749634 tensor(12, device='cuda:0')\n",
            "lsy 3.6328125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.3528, -0.3506, -0.1577, -0.3667, -0.4778, -0.3547, -0.2634,\n",
            "        -0.3894, -0.3259, -0.2673, -0.4407, -0.4502, -0.4517, -0.3147, -0.3645,\n",
            "        -0.4385], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6812068819999695 tensor(15, device='cuda:0')\n",
            "lsy 3.408203125 0.0\n",
            "pred tensor([-0.4990, -0.5000, -0.5000, -0.4995, -0.5000, -0.5000, -0.5000, -0.4375,\n",
            "        -0.5000, -0.4939, -0.4841, -0.4590, -0.4854, -0.4451, -0.3901, -0.4380,\n",
            "        -0.4417, -0.4314, -0.4841, -0.4661, -0.4790, -0.4858, -0.4849, -0.4893,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6546368598937988 tensor(12, device='cuda:0')\n",
            "lsy 3.39453125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4995, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4934, -0.5000, -0.5000, -0.5000, -0.4983, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4985, -0.5000, -0.5000, -0.5000, -0.4988, -0.4880,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.67145174741745 tensor(13, device='cuda:0')\n",
            "lsy 3.947265625 0.0\n",
            "pred tensor([-3.1590e-06, -2.1286e-03, -7.3891e-03, -2.0654e-01, -2.9321e-01,\n",
            "        -2.2485e-01, -4.9512e-01, -4.9951e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9390e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6453864574432373 tensor(10, device='cuda:0')\n",
            "lsy 3.8203125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4976, -0.5000, -0.4807, -0.5000, -0.5000, -0.4846, -0.5000, -0.4807,\n",
            "        -0.5000, -0.5000, -0.4897, -0.4917, -0.5000, -0.4990, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.632769763469696 tensor(12, device='cuda:0')\n",
            "lsy 3.57421875 0.0\n",
            "pred tensor([-1.1921e-07, -3.0398e-06,  0.0000e+00, -5.5432e-05, -1.9848e-05,\n",
            "        -1.1921e-07, -6.2048e-05, -3.5763e-07, -5.9605e-08, -3.6192e-04,\n",
            "        -1.3113e-06, -2.3246e-06, -1.5335e-03, -5.1439e-05, -1.0729e-05,\n",
            "        -1.5497e-06, -5.9605e-08, -2.5034e-06,  0.0000e+00,  0.0000e+00,\n",
            "        -9.1629e-03, -6.6414e-03, -8.1543e-02, -1.2749e-02,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6583364009857178 tensor(12, device='cuda:0')\n",
            "lsy 3.578125 0.0\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00, -2.6011e-04, -4.5896e-06,  0.0000e+00,\n",
            "        -3.5763e-07,  0.0000e+00, -5.9605e-08, -5.9605e-08, -2.8801e-04,\n",
            "         0.0000e+00,  0.0000e+00, -5.3644e-07, -2.8610e-06, -1.1921e-06,\n",
            "        -9.2387e-06,  0.0000e+00, -3.0339e-05, -2.5868e-05,  0.0000e+00,\n",
            "        -2.9802e-07, -3.9279e-05,  0.0000e+00, -1.2195e-04,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.680211067199707 tensor(17, device='cuda:0')\n",
            "lsy 3.607421875 0.0\n",
            "pred tensor([-5.9605e-08, -1.6689e-06, -7.9691e-05, -1.0014e-05, -3.8147e-06,\n",
            "         0.0000e+00, -3.2187e-06, -1.1086e-05, -7.7248e-05, -1.0729e-06,\n",
            "         0.0000e+00, -6.5565e-07, -2.6822e-05, -8.1897e-05, -3.5524e-05,\n",
            "        -2.3102e-02, -1.9372e-05, -4.6563e-04, -2.0862e-06, -1.1921e-07,\n",
            "        -2.2278e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6745377779006958 tensor(18, device='cuda:0')\n",
            "lsy 4.25 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.2830, -0.1782, -0.3647, -0.1274, -0.3523,\n",
            "        -0.2009, -0.0790, -0.1155, -0.3652, -0.1622, -0.0393, -0.2820, -0.1835,\n",
            "        -0.2637, -0.2207, -0.1571, -0.4146, -0.3030, -0.3296, -0.4331, -0.4138,\n",
            "        -0.4746], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6622576713562012 tensor(12, device='cuda:0')\n",
            "lsy 4.23828125 0.0\n",
            "pred tensor([-0.3728, -0.4124, -0.4502, -0.4233, -0.5000, -0.4539, -0.5000, -0.5000,\n",
            "        -0.4944, -0.5000, -0.5000, -0.4902, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4973, -0.5000,\n",
            "        -0.4756], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6363109350204468 tensor(11, device='cuda:0')\n",
            "lsy 5.84765625 0.0\n",
            "pred tensor([-0.1088, -0.1265, -0.3376, -0.5000, -0.4407, -0.4194, -0.4380, -0.5000,\n",
            "        -0.4561, -0.4812, -0.4990, -0.4878, -0.5000, -0.4939, -0.5000, -0.4976,\n",
            "        -0.4524, -0.3201, -0.1813, -0.0506, -0.3906, -0.2756, -0.1327, -0.4854,\n",
            "        -0.4937], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6668651700019836 tensor(18, device='cuda:0')\n",
            "lsy 3.771484375 0.0\n",
            "pred tensor([-0.1525, -0.0671, -0.0786, -0.0844, -0.0714, -0.1132, -0.4060, -0.0743,\n",
            "        -0.2656, -0.0536, -0.1340, -0.1144, -0.1248, -0.0240, -0.0853, -0.0586,\n",
            "        -0.3003, -0.1169, -0.0126, -0.0354, -0.2307, -0.0970, -0.3760, -0.0087,\n",
            "        -0.0263], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6839415431022644 tensor(16, device='cuda:0')\n",
            "lsy 4.125 0.0\n",
            "pred tensor([-0.1359, -0.1459, -0.0630, -0.2235, -0.1570, -0.3740, -0.1660, -0.1359,\n",
            "        -0.1257, -0.2462, -0.0894, -0.1714, -0.1081, -0.0503, -0.1378, -0.0525,\n",
            "        -0.1566, -0.0093, -0.0170, -0.0635, -0.2115, -0.0648, -0.4895, -0.3555,\n",
            "        -0.3398], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6789921522140503 tensor(13, device='cuda:0')\n",
            "lsy 3.669921875 0.0\n",
            "pred tensor([-4.9121e-01, -1.4551e-01, -1.7881e-07, -1.7881e-07, -1.3924e-04,\n",
            "        -9.4593e-05, -8.5831e-05, -6.5565e-07,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.4305e-06, -5.0664e-05, -8.9407e-07, -2.2423e-04,\n",
            "        -1.0133e-06, -2.9802e-07, -9.1839e-04, -1.8537e-05, -2.2392e-03,\n",
            "        -4.6600e-02, -9.9277e-04, -1.4282e-01, -4.9854e-01, -4.7363e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6449484825134277 tensor(13, device='cuda:0')\n",
            "lsy 3.5859375 0.0\n",
            "pred tensor([-4.9219e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9854e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -1.6108e-03, -3.2568e-04, -1.9670e-06, -4.2915e-06,\n",
            "        -5.9357e-03, -6.8092e-03, -1.8835e-05, -1.3447e-04, -2.0742e-04,\n",
            "        -1.1826e-02, -2.1954e-03, -1.6289e-03, -2.6077e-02, -1.8845e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6417744159698486 tensor(15, device='cuda:0')\n",
            "lsy 4.80859375 0.0\n",
            "pred tensor([-0.4551, -0.4819, -0.5000, -0.4963, -0.4998, -0.4888, -0.4617, -0.4746,\n",
            "        -0.4749, -0.4956, -0.5000, -0.4822, -0.4502, -0.4524, -0.4207, -0.4648,\n",
            "        -0.4482, -0.4053, -0.3079, -0.4211, -0.3962, -0.4966, -0.4275, -0.4734,\n",
            "        -0.3926], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6573994755744934 tensor(19, device='cuda:0')\n",
            "lsy 4.3515625 0.0\n",
            "pred tensor([-0.4885, -0.4636, -0.5000, -0.4956, -0.4419, -0.5000, -0.4792, -0.4688,\n",
            "        -0.4805, -0.4807, -0.4187, -0.4558, -0.4893, -0.4951, -0.3621, -0.4548,\n",
            "        -0.4849, -0.4033, -0.4749, -0.4504, -0.4939, -0.4673, -0.4209, -0.4761,\n",
            "        -0.4661], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6373550295829773 tensor(12, device='cuda:0')\n",
            "lsy 4.80078125 0.0\n",
            "pred tensor([-5.0000e-01, -4.9866e-02, -9.1732e-05, -1.4175e-02, -1.2789e-03,\n",
            "        -3.3997e-02, -5.6171e-04, -1.3196e-01, -7.2205e-02, -1.8652e-01,\n",
            "        -4.2969e-02, -1.2077e-02, -2.6050e-01, -2.7145e-02, -8.7585e-02,\n",
            "        -5.1880e-02, -2.0966e-02, -3.1274e-01, -1.6003e-01, -1.1243e-01,\n",
            "        -3.8037e-01, -1.2299e-01, -1.0391e-02, -5.4216e-04, -5.2452e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6542153358459473 tensor(12, device='cuda:0')\n",
            "lsy 4.42578125 0.0\n",
            "pred tensor([-5.6534e-03, -1.3471e-05, -1.7953e-04, -5.6028e-06, -6.7353e-06,\n",
            "        -6.8545e-06, -5.9605e-08, -1.3741e-02, -1.3170e-03, -1.2770e-03,\n",
            "        -8.3130e-02, -1.9312e-01, -1.6159e-02, -3.4760e-02, -2.5586e-01,\n",
            "        -1.0490e-02, -1.0968e-01, -1.7517e-01, -3.2642e-01, -3.2104e-02,\n",
            "        -1.8567e-01, -3.5132e-01, -2.8271e-01, -4.4775e-01, -4.8242e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6665850281715393 tensor(20, device='cuda:0')\n",
            "lsy 4.44140625 0.0\n",
            "pred tensor([-5.9605e-08, -5.4359e-05, -1.2894e-03, -5.9605e-08, -5.9605e-08,\n",
            "         0.0000e+00, -1.1921e-07, -5.3644e-07,  0.0000e+00, -1.1921e-07,\n",
            "        -5.9605e-07, -1.0133e-06, -2.3842e-07,  0.0000e+00, -1.2815e-05,\n",
            "        -5.9605e-08, -2.1458e-06, -8.3447e-07,  0.0000e+00,  0.0000e+00,\n",
            "        -1.3828e-03, -1.8775e-05,  0.0000e+00, -4.1723e-07,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6479926705360413 tensor(17, device='cuda:0')\n",
            "lsy 4.51953125 0.0\n",
            "pred tensor([-3.0994e-03, -4.2796e-05, -1.8054e-01, -1.8811e-04, -7.7705e-03,\n",
            "        -5.7587e-02, -1.7981e-01, -6.3293e-02, -6.6711e-02, -6.4774e-03,\n",
            "        -2.2163e-03, -3.5644e-05, -2.0020e-02, -1.8024e-04, -3.8743e-04,\n",
            "        -5.7182e-03, -7.3303e-02, -2.4438e-06, -6.0272e-03, -2.0203e-01,\n",
            "        -3.2806e-02, -4.7386e-05, -6.6833e-02, -1.2146e-02, -9.8511e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6511838436126709 tensor(16, device='cuda:0')\n",
            "26\n",
            "lsy 4.95703125 0.0\n",
            "pred tensor([-0.5000, -0.4810, -0.4973, -0.5000, -0.5000, -0.5000, -0.4482, -0.4778,\n",
            "        -0.5000, -0.4866, -0.5000, -0.4119, -0.2346, -0.2725, -0.0823, -0.0033,\n",
            "        -0.1175, -0.0688, -0.0329, -0.3157, -0.4094, -0.4934, -0.4812, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6289492845535278 tensor(15, device='cuda:0')\n",
            "lsy 4.45703125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.4954, -0.4985, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4895, -0.4985, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.1356, -0.0809, -0.0327, -0.0209, -0.0145,\n",
            "        -0.0285], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.645899772644043 tensor(21, device='cuda:0')\n",
            "lsy 4.56640625 0.0\n",
            "pred tensor([-0.4946, -0.4749, -0.5000, -0.4609, -0.4958, -0.4954, -0.4985, -0.4827,\n",
            "        -0.4778, -0.5000, -0.4121, -0.4968, -0.4167, -0.3435, -0.4929, -0.4199,\n",
            "        -0.4275, -0.5000, -0.5000, -0.4885, -0.4761, -0.4590, -0.5000, -0.4851,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6794315576553345 tensor(22, device='cuda:0')\n",
            "lsy 4.359375 0.0\n",
            "pred tensor([-0.4810, -0.5000, -0.5000, -0.4944, -0.5000, -0.4785, -0.4966, -0.4827,\n",
            "        -0.5000, -0.4834, -0.4998, -0.4985, -0.5000, -0.5000, -0.4609, -0.5000,\n",
            "        -0.4871, -0.5000, -0.4922, -0.4470, -0.4358, -0.4995, -0.3306, -0.4719,\n",
            "        -0.4524], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6680128574371338 tensor(19, device='cuda:0')\n",
            "lsy 5.21484375 0.0\n",
            "pred tensor([-0.5000, -0.4780, -0.5000, -0.4958, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4812, -0.4597, -0.4358, -0.3149, -0.4067,\n",
            "        -0.4119, -0.4741, -0.4365, -0.4983, -0.4849, -0.4763, -0.4539, -0.4939,\n",
            "        -0.4983], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.664510190486908 tensor(20, device='cuda:0')\n",
            "lsy 4.1796875 0.0\n",
            "pred tensor([-0.4897, -0.5000, -0.5000, -0.4873, -0.4705, -0.4172, -0.4680, -0.4570,\n",
            "        -0.4788, -0.4624, -0.4634, -0.4822, -0.3481, -0.4236, -0.2444, -0.4294,\n",
            "        -0.4453, -0.5000, -0.4316, -0.4961, -0.4819, -0.5000, -0.4543, -0.4783,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.655290961265564 tensor(16, device='cuda:0')\n",
            "lsy 3.72265625 0.0\n",
            "pred tensor([-1.3696e-01, -9.3689e-02, -9.7473e-02, -2.8687e-01, -2.0935e-01,\n",
            "        -3.7915e-01, -4.9243e-01, -4.6069e-01, -3.0151e-01, -4.6729e-01,\n",
            "        -2.8247e-01, -1.1469e-01, -2.1277e-01, -6.6042e-05, -5.4474e-03,\n",
            "        -1.4915e-02, -1.7881e-07, -2.0862e-06, -3.3975e-06, -2.3914e-01,\n",
            "        -5.3465e-05, -8.3303e-04, -1.0192e-05, -7.0534e-03, -5.4657e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6391265392303467 tensor(17, device='cuda:0')\n",
            "lsy 4.46484375 0.0\n",
            "pred tensor([-1.6093e-06, -4.7302e-02, -2.2087e-03, -3.9291e-03, -1.1070e-02,\n",
            "        -1.3000e-01, -3.4888e-01, -3.7427e-01, -3.0908e-01, -3.8623e-01,\n",
            "        -3.6084e-01, -4.5459e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9609e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6563224196434021 tensor(12, device='cuda:0')\n",
            "lsy 4.26953125 0.0\n",
            "pred tensor([-4.6948e-01, -4.9634e-01, -5.0000e-01, -5.0000e-01, -4.8267e-01,\n",
            "        -4.2480e-01, -4.9365e-01, -4.8047e-01, -3.7842e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.8462e-01, -5.0000e-01, -5.0000e-01, -4.9707e-01,\n",
            "        -5.0000e-01, -4.6460e-01, -5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
            "        -2.0862e-06, -4.0531e-06, -1.9159e-03, -6.6280e-04, -7.1526e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6408742070198059 tensor(13, device='cuda:0')\n",
            "lsy 5.59765625 0.0\n",
            "pred tensor([-2.6166e-05, -5.6267e-05, -2.0087e-05, -3.0994e-06,  0.0000e+00,\n",
            "         0.0000e+00, -5.9605e-08, -1.1921e-07, -3.2783e-06, -4.4084e-04,\n",
            "        -1.2934e-04,  0.0000e+00, -1.7881e-07,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -2.3842e-07,  0.0000e+00, -3.7551e-06, -2.9802e-07,\n",
            "        -1.3494e-04, -1.6678e-02, -4.9463e-01, -5.0000e-01, -4.9463e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6548084616661072 tensor(15, device='cuda:0')\n",
            "lsy 3.939453125 0.0\n",
            "pred tensor([-4.8608e-01, -4.9829e-01, -4.9048e-01, -4.9390e-01, -4.6338e-01,\n",
            "        -4.0454e-01, -3.2812e-01, -4.0112e-01, -4.8096e-01, -3.2690e-01,\n",
            "        -3.6084e-01, -2.9346e-01, -4.7461e-01, -5.0000e-01, -1.0126e-01,\n",
            "        -1.4612e-01,  0.0000e+00,  0.0000e+00, -3.7074e-05,  0.0000e+00,\n",
            "        -3.5906e-04,  0.0000e+00, -4.7684e-07, -4.3726e-04, -8.2254e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6650030016899109 tensor(15, device='cuda:0')\n",
            "lsy 3.560546875 0.0\n",
            "pred tensor([-1.3113e-06, -7.0930e-06,  0.0000e+00, -7.1526e-07, -1.2052e-04,\n",
            "        -1.0986e-02, -5.1331e-02, -3.9404e-01, -3.8477e-01, -4.6191e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -1.3514e-03, -4.8697e-05, -1.2517e-06, -9.0599e-06, -6.8665e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6285312175750732 tensor(11, device='cuda:0')\n",
            "lsy 3.375 0.0\n",
            "pred tensor([-8.5632e-02, -1.3220e-01, -3.3984e-01, -5.0049e-02, -2.2534e-01,\n",
            "        -3.1689e-01, -9.6680e-02, -2.2363e-01, -6.7932e-02, -2.4817e-01,\n",
            "        -3.5648e-03, -2.8244e-02, -1.3023e-02, -4.3030e-02, -2.5562e-01,\n",
            "        -1.7053e-01, -4.5593e-02, -3.3356e-02,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -1.9646e-04, -4.5052e-03, -3.4619e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6783767938613892 tensor(19, device='cuda:0')\n",
            "lsy 3.9140625 0.0\n",
            "pred tensor([-0.4614, -0.4895, -0.4707, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.4495, -0.4980, -0.4497,\n",
            "        -0.4839, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4939, -0.4875,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6950621008872986 tensor(17, device='cuda:0')\n",
            "lsy 3.822265625 0.0\n",
            "pred tensor([-5.0000e-01, -4.9707e-01, -3.5449e-01, -3.9062e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.9829e-01, -1.0419e-01, -2.0715e-01, -1.8143e-02,\n",
            "        -2.9984e-03,  0.0000e+00, -9.2363e-04,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.6475e-04, -7.0618e-02, -1.0967e-03, -5.8556e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6267908811569214 tensor(9, device='cuda:0')\n",
            "lsy 4.07421875 0.0\n",
            "pred tensor([-2.0862e-06, -6.2622e-02, -1.4648e-01, -4.1211e-01, -4.9683e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.8047e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.9854e-01, -4.7705e-01, -4.9146e-01, -4.4873e-01,\n",
            "        -4.7266e-01, -4.8877e-01, -4.7925e-01, -3.6499e-01, -4.6411e-01,\n",
            "        -4.9951e-01, -5.0000e-01, -4.6680e-01, -4.4653e-01, -4.2236e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6684508323669434 tensor(13, device='cuda:0')\n",
            "lsy 3.91796875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4795, -0.4939, -0.5000,  0.0000,\n",
            "         0.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6289806365966797 tensor(12, device='cuda:0')\n",
            "lsy 5.48046875 0.0\n",
            "pred tensor([-3.5524e-05, -5.2869e-05, -2.8496e-03, -3.1471e-03, -1.6312e-02,\n",
            "        -2.4951e-01, -6.2439e-02, -1.9446e-01, -2.3087e-02, -1.4246e-05,\n",
            "        -9.1124e-04, -1.1473e-03, -8.1897e-05, -5.9605e-08, -2.9206e-05,\n",
            "         0.0000e+00, -4.0603e-04, -5.9605e-06, -1.4296e-03,  0.0000e+00,\n",
            "        -9.9540e-06, -6.4125e-03, -3.4738e-04, -4.6005e-03, -3.2043e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6637027859687805 tensor(13, device='cuda:0')\n",
            "lsy 4.234375 0.0\n",
            "pred tensor([-0.4954, -0.4990, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4976, -0.4978, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4988, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6931198835372925 tensor(19, device='cuda:0')\n",
            "lsy 4.17578125 0.0\n",
            "pred tensor([-5.0000e-01, -4.8120e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9976e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.0068e-06,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6635239720344543 tensor(14, device='cuda:0')\n",
            "lsy 4.54296875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.2297, -0.0862, -0.1443,\n",
            "        -0.4622, -0.1824, -0.4412, -0.4702, -0.4919, -0.5000, -0.5000, -0.4919,\n",
            "        -0.5000, -0.4939, -0.4817, -0.5000, -0.4929, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6519323587417603 tensor(13, device='cuda:0')\n",
            "lsy 3.853515625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4812, -0.4819, -0.4475, -0.4778, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4773, -0.5000, -0.4656, -0.5000, -0.4844,\n",
            "        -0.4546, -0.4714, -0.0048, -0.1082, -0.2527, -0.2076, -0.3411, -0.4688,\n",
            "        -0.4902], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6523966193199158 tensor(16, device='cuda:0')\n",
            "lsy 6.0703125 0.0\n",
            "pred tensor([-0.4558, -0.4880, -0.4927, -0.4897, -0.4565, -0.4683, -0.4919, -0.4937,\n",
            "        -0.4795, -0.4917, -0.5000, -0.5000, -0.1481, -0.0858, -0.2573, -0.1598,\n",
            "        -0.1659, -0.2656, -0.0739, -0.0290, -0.0611, -0.0911, -0.1337, -0.1151,\n",
            "        -0.0105], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6346642971038818 tensor(9, device='cuda:0')\n",
            "lsy 5.2890625 0.0\n",
            "pred tensor([-0.0297, -0.0366, -0.0233, -0.0742, -0.1177, -0.0867, -0.1142, -0.1566,\n",
            "        -0.0977, -0.3389, -0.1212, -0.3523, -0.1134, -0.0746, -0.0642, -0.1372,\n",
            "        -0.0265, -0.1326, -0.0083, -0.1860, -0.1775, -0.0568, -0.0123, -0.1868,\n",
            "        -0.2283], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6270068287849426 tensor(16, device='cuda:0')\n",
            "lsy 4.28515625 0.0\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -5.9605e-08, -2.3842e-07, -1.0014e-05,  0.0000e+00,\n",
            "        -4.2498e-05, -3.2597e-03,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
            "         0.0000e+00, -5.9605e-08,  0.0000e+00, -2.5451e-05,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6348974704742432 tensor(14, device='cuda:0')\n",
            "lsy 4.171875 0.0\n",
            "pred tensor([-5.3644e-07, -1.1921e-07, -1.1921e-07, -4.6362e-01, -1.1093e-02,\n",
            "        -1.3367e-01, -5.4443e-02, -4.4604e-01, -2.6270e-01, -4.7876e-01,\n",
            "        -2.9077e-01, -3.0542e-01, -3.6963e-01, -4.7388e-01, -5.0000e-01,\n",
            "        -4.9561e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9756e-01, -4.9731e-01, -3.2788e-01, -4.9072e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6588466763496399 tensor(24, device='cuda:0')\n",
            "lsy 4.0390625 0.0\n",
            "pred tensor([-0.4731, -0.4412, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4973,\n",
            "        -0.5000, -0.5000, -0.0898, -0.0343, -0.1381, -0.0614, -0.1434, -0.0285,\n",
            "        -0.2281, -0.1448, -0.0701, -0.0593, -0.1815, -0.0167, -0.0529, -0.0483,\n",
            "        -0.0718], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.682869553565979 tensor(20, device='cuda:0')\n",
            "lsy 3.98828125 0.0\n",
            "pred tensor([-0.1296, -0.0076, -0.0927, -0.0613, -0.1123, -0.2240, -0.0241, -0.0688,\n",
            "        -0.1422, -0.0360, -0.2133, -0.1373, -0.0695, -0.0213, -0.0094, -0.0671,\n",
            "        -0.0364, -0.1041, -0.0284, -0.1797, -0.1877, -0.1389, -0.1555, -0.1611,\n",
            "        -0.0192], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6825985312461853 tensor(22, device='cuda:0')\n",
            "lsy 4.81640625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.2000, -0.1213, -0.0460, -0.0195, -0.0007,\n",
            "        -0.1169, -0.1848, -0.4402, -0.4424, -0.4854, -0.4939, -0.4871, -0.5000,\n",
            "        -0.5000, -0.4995, -0.4829, -0.4792, -0.4897, -0.4534, -0.4353, -0.4568,\n",
            "        -0.4851], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6449688076972961 tensor(17, device='cuda:0')\n",
            "lsy 4.50390625 0.0\n",
            "pred tensor([-0.5000, -0.4934, -0.4788, -0.5000, -0.4839, -0.4985, -0.5000, -0.4968,\n",
            "        -0.4949, -0.5000, -0.5000, -0.5000, -0.4983, -0.4983, -0.4832, -0.4973,\n",
            "        -0.5000, -0.5000, -0.4890, -0.4873, -0.4990, -0.4736, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6748012900352478 tensor(21, device='cuda:0')\n",
            "lsy 3.8125 0.0\n",
            "pred tensor([-0.5000, -0.4587, -0.4937, -0.5000, -0.4666, -0.4690, -0.3938, -0.4927,\n",
            "        -0.5000, -0.4939, -0.4875, -0.4739, -0.4883, -0.4932, -0.4919, -0.4717,\n",
            "        -0.4900, -0.5000, -0.4937, -0.5000, -0.4929, -0.4968, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6742010712623596 tensor(16, device='cuda:0')\n",
            "lsy 3.638671875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4927, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6472002863883972 tensor(11, device='cuda:0')\n",
            "lsy 4.17578125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4707, -0.5000, -0.5000, -0.4985, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4729, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4802,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4827, -0.4792, -0.4998, -0.3079, -0.4297,\n",
            "        -0.4304], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6661259531974792 tensor(12, device='cuda:0')\n",
            "lsy 3.408203125 0.0\n",
            "pred tensor([-0.4851, -0.1940, -0.2483, -0.0313, -0.0906, -0.4431, -0.4316, -0.5000,\n",
            "        -0.4578, -0.4617, -0.4902, -0.4985, -0.4800, -0.4829, -0.5000, -0.4993,\n",
            "        -0.5000, -0.4380, -0.5000, -0.4775, -0.4834, -0.4907, -0.5000, -0.3643,\n",
            "        -0.3308], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6586589813232422 tensor(3, device='cuda:0')\n",
            "lsy 3.8984375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.2322, -0.1958, -0.1163,\n",
            "        -0.4102, -0.0661, -0.1770, -0.0977, -0.0865, -0.0420, -0.0073, -0.0832,\n",
            "        -0.0384], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6860936284065247 tensor(19, device='cuda:0')\n",
            "lsy 3.48046875 0.0\n",
            "pred tensor([-3.2990e-02, -1.4392e-01, -8.0994e-02, -1.2825e-02, -2.6569e-03,\n",
            "        -1.0818e-02, -1.7881e-06, -1.2970e-03, -4.5825e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.9683e-01, -5.0000e-01, -4.6021e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6388536095619202 tensor(12, device='cuda:0')\n",
            "lsy 4.02734375 0.0\n",
            "pred tensor([-0.4553, -0.2015, -0.2629, -0.4133, -0.2087, -0.1935, -0.3245, -0.1204,\n",
            "        -0.4834, -0.3530, -0.4189, -0.4653, -0.5000, -0.4995, -0.5000, -0.4829,\n",
            "        -0.4768, -0.0194, -0.0827, -0.0065, -0.3918, -0.2969, -0.2903, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6613039970397949 tensor(12, device='cuda:0')\n",
            "lsy 4.234375 0.0\n",
            "pred tensor([-3.4424e-01, -4.1064e-01, -4.3262e-01, -3.1274e-01, -4.5337e-01,\n",
            "        -4.1284e-01, -8.3780e-04, -5.0316e-03,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7881e-07,\n",
            "         0.0000e+00, -5.6624e-06, -8.7023e-06, -3.2471e-02, -3.6895e-05,\n",
            "        -4.9353e-04, -9.2447e-05, -1.0691e-03, -4.0344e-02, -9.9365e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6671589016914368 tensor(8, device='cuda:0')\n",
            "27\n",
            "lsy 3.529296875 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -7.2241e-05, -3.4738e-04,\n",
            "        -3.6407e-04, -4.1723e-07, -1.2815e-05, -1.1921e-05, -2.0802e-05,\n",
            "        -2.0862e-06, -3.3331e-04, -5.3358e-04, -2.8725e-03, -1.0729e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6538611650466919 tensor(10, device='cuda:0')\n",
            "lsy 4.56640625 0.0\n",
            "pred tensor([-4.3392e-04, -2.4140e-05, -5.1856e-06, -8.4162e-05, -1.4901e-05,\n",
            "        -1.5676e-05, -3.3331e-04, -1.2517e-06, -4.0552e-01, -4.9170e-01,\n",
            "        -5.0000e-01, -4.9365e-01, -4.8193e-01, -5.0000e-01, -4.9097e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9609e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -3.4851e-02, -2.2471e-04, -7.9224e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.639198362827301 tensor(4, device='cuda:0')\n",
            "lsy 4.4921875 0.0\n",
            "pred tensor([-3.7378e-01, -2.9077e-01, -2.2119e-01, -4.0112e-01, -4.6460e-01,\n",
            "        -4.0601e-01, -3.9795e-01, -2.6221e-01, -6.5735e-02, -1.1017e-01,\n",
            "        -2.9272e-01, -8.9294e-02, -1.1621e-01, -5.7648e-02, -5.1788e-02,\n",
            "        -2.0844e-02, -8.5068e-04, -5.0735e-04, -4.8780e-04, -2.2564e-03,\n",
            "        -9.6500e-05, -4.8113e-04, -1.2617e-03, -5.7755e-03, -7.4341e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6692447662353516 tensor(13, device='cuda:0')\n",
            "lsy 5.5703125 0.0\n",
            "pred tensor([-8.1711e-03, -4.0063e-01, -1.2825e-02, -4.0588e-02, -4.9170e-01,\n",
            "        -1.0974e-01, -3.4504e-03, -2.5269e-01, -8.0444e-02, -4.8584e-01,\n",
            "        -4.9561e-01, -4.9561e-01, -4.6680e-01, -4.8779e-01, -4.3311e-01,\n",
            "        -4.4775e-01, -4.3701e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.5215e-01, -4.8975e-01, -5.0000e-01,  0.0000e+00, -1.6344e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6086142063140869 tensor(7, device='cuda:0')\n",
            "lsy 4.16015625 0.0\n",
            "pred tensor([-1.0567e-03, -1.5116e-04, -5.6505e-05, -6.5517e-04, -4.7684e-07,\n",
            "        -8.5592e-04, -1.1053e-03, -4.4703e-05, -1.3456e-03, -1.8167e-04,\n",
            "        -1.1574e-02, -1.7748e-03, -1.3245e-02,  0.0000e+00, -1.1225e-03,\n",
            "        -6.7711e-04, -9.7752e-06, -7.9989e-05, -4.7743e-05, -4.0841e-04,\n",
            "        -2.7523e-03, -2.5146e-01, -2.7295e-01, -4.6509e-02, -1.7383e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.64016193151474 tensor(6, device='cuda:0')\n",
            "lsy 5.1171875 0.0\n",
            "pred tensor([-0.3228, -0.2610, -0.1997, -0.3928, -0.4683, -0.5000, -0.5000, -0.4338,\n",
            "        -0.4966, -0.5000, -0.4980, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6356626749038696 tensor(9, device='cuda:0')\n",
            "lsy 4.79296875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4990, -0.4771, -0.4958, -0.5000, -0.5000, -0.4946, -0.5000, -0.5000,\n",
            "        -0.4905], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6557510495185852 tensor(9, device='cuda:0')\n",
            "lsy 4.80859375 0.0\n",
            "pred tensor([-0.4980, -0.5000, -0.5000, -0.4648, -0.4915, -0.5000, -0.4846, -0.5000,\n",
            "        -0.4688, -0.4888, -0.5000, -0.5000, -0.5000, -0.4841, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6784219145774841 tensor(14, device='cuda:0')\n",
            "lsy 4.74609375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -4.5508e-01, -4.9927e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.9683e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.0223e-03, -9.6083e-05, -7.1168e-05, -5.8770e-05,\n",
            "        -3.1796e-03, -2.2926e-03, -2.2614e-02, -2.3003e-03, -1.8966e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6794515252113342 tensor(16, device='cuda:0')\n",
            "lsy 4.73046875 0.0\n",
            "pred tensor([-2.7800e-04,  0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00, -1.7881e-07, -2.3842e-07,\n",
            "        -2.9802e-06, -1.7881e-07, -8.7142e-05, -1.0223e-03, -2.4438e-06,\n",
            "         0.0000e+00, -1.3709e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6664420366287231 tensor(11, device='cuda:0')\n",
            "lsy 5.01953125 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.9487e-01, -4.5654e-01, -5.0000e-01, -4.6680e-01,\n",
            "        -4.9976e-01, -4.2432e-01, -4.3384e-01, -5.0000e-01, -4.7900e-01,\n",
            "        -4.1357e-01, -4.8120e-01, -4.7217e-01, -4.5459e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -7.5102e-06, -6.7062e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.639131486415863 tensor(8, device='cuda:0')\n",
            "lsy 5.03125 0.0\n",
            "pred tensor([-3.6359e-05, -3.1829e-05, -1.1235e-04, -5.0545e-04, -1.0779e-01,\n",
            "        -1.0083e-01, -1.9729e-05, -8.8811e-06, -3.4690e-05, -9.0027e-03,\n",
            "        -4.2648e-03, -1.1803e-02, -4.3555e-01, -3.2776e-02, -1.3695e-03,\n",
            "        -2.2469e-03, -4.2648e-03, -2.6367e-01, -3.4241e-02, -1.5497e-06,\n",
            "        -1.3590e-05, -9.3384e-03, -7.7486e-04, -6.5002e-02, -1.0394e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6581099033355713 tensor(11, device='cuda:0')\n",
            "lsy 4.54296875 0.0\n",
            "pred tensor([-2.3723e-05, -2.8014e-06, -1.1921e-06, -1.4549e-02, -1.9550e-05,\n",
            "        -1.6504e-01, -5.0000e-01, -4.9414e-01, -5.0000e-01, -4.8462e-01,\n",
            "        -4.9902e-01, -4.8315e-01, -4.5898e-01, -4.7119e-01, -4.7168e-01,\n",
            "        -4.3774e-01, -5.0000e-01, -4.9487e-01, -4.6704e-01, -4.8804e-01,\n",
            "        -4.7656e-01, -4.6851e-01, -4.9463e-01, -4.3628e-01, -3.6987e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6733434200286865 tensor(9, device='cuda:0')\n",
            "lsy 4.55078125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4946, -0.4871, -0.5000, -0.4902, -0.4978, -0.4800,\n",
            "        -0.4531, -0.4976, -0.4963, -0.5000, -0.5000, -0.5000, -0.4863, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4963, -0.5000, -0.5000, -0.4790, -0.4868, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6380555033683777 tensor(8, device='cuda:0')\n",
            "lsy 4.12890625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6883627772331238 tensor(12, device='cuda:0')\n",
            "lsy 4.42578125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6714664697647095 tensor(9, device='cuda:0')\n",
            "lsy 4.40625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6531340479850769 tensor(8, device='cuda:0')\n",
            "lsy 5.0625 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,  0.0000e+00,\n",
            "        -3.4809e-05,  0.0000e+00, -1.4651e-04, -1.2338e-05,  0.0000e+00,\n",
            "         0.0000e+00, -2.2602e-04, -7.1526e-07, -1.9109e-04, -2.2829e-05,\n",
            "        -2.9325e-05, -8.3780e-04, -6.0201e-06, -4.1723e-07, -1.0133e-06,\n",
            "         0.0000e+00, -1.7881e-07, -8.9228e-05, -3.1616e-02, -1.0292e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.660868763923645 tensor(12, device='cuda:0')\n",
            "lsy 3.83203125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4985, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4990, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6669728755950928 tensor(13, device='cuda:0')\n",
            "lsy 4.07421875 0.0\n",
            "pred tensor([-0.5000, -0.4995, -0.5000, -0.4958, -0.4539, -0.4919, -0.4958, -0.0865,\n",
            "        -0.4985, -0.2319, -0.4775, -0.4834, -0.4971, -0.3938, -0.4045, -0.4963,\n",
            "        -0.4551, -0.1514, -0.0272, -0.3416, -0.1178, -0.3391, -0.4407, -0.4504,\n",
            "        -0.4177], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6511579751968384 tensor(6, device='cuda:0')\n",
            "lsy 3.671875 0.0\n",
            "pred tensor([-0.3127, -0.4763, -0.3325, -0.2676, -0.1635, -0.4863, -0.4910, -0.3674,\n",
            "        -0.1536, -0.4634, -0.4619, -0.4688, -0.5000, -0.5000, -0.4170, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6485181450843811 tensor(6, device='cuda:0')\n",
            "lsy 3.734375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.0363, -0.4648, -0.2267, -0.4326, -0.3413, -0.1934,\n",
            "        -0.3887, -0.4031, -0.5000, -0.4807, -0.4785, -0.4851, -0.4585, -0.4985,\n",
            "        -0.5000, -0.4951, -0.4890, -0.5000, -0.4993, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6627009510993958 tensor(12, device='cuda:0')\n",
            "lsy 4.73046875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.5000, -0.5000,\n",
            "        -0.4983, -0.5000, -0.5000, -0.4768, -0.4812, -0.5000, -0.4995, -0.5000,\n",
            "        -0.4849], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6473724246025085 tensor(7, device='cuda:0')\n",
            "lsy 3.529296875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4983, -0.5000, -0.4580, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4958, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4998, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6655088067054749 tensor(8, device='cuda:0')\n",
            "lsy 4.28125 0.0\n",
            "pred tensor([-1.5903e-04, -1.7181e-02, -1.3530e-05, -1.7285e-05, -9.7229e-02,\n",
            "        -9.6500e-05, -1.3794e-02, -2.7002e-01, -1.0550e-05, -3.9795e-01,\n",
            "        -3.0346e-03, -1.5906e-01, -4.5776e-02, -9.1248e-02, -4.6417e-02,\n",
            "        -1.0490e-02, -8.0994e-02, -3.3008e-01, -2.7124e-01, -3.7720e-02,\n",
            "        -9.1309e-02, -6.3515e-03, -4.9731e-01, -5.3520e-03, -2.9206e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.646230161190033 tensor(9, device='cuda:0')\n",
            "lsy 3.568359375 0.0\n",
            "pred tensor([-0.3667, -0.0039, -0.1835, -0.0047, -0.0069, -0.0154, -0.0007, -0.1417,\n",
            "        -0.4998, -0.4614, -0.2384, -0.4226, -0.0319, -0.0042, -0.0007, -0.0054,\n",
            "        -0.2781, -0.1343, -0.2676, -0.4922, -0.4844, -0.3884, -0.4985, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6408666968345642 tensor(8, device='cuda:0')\n",
            "lsy 3.61328125 0.0\n",
            "pred tensor([-9.5654e-04, -3.7909e-05, -2.8107e-02, -1.2317e-01, -7.8857e-02,\n",
            "        -5.9692e-02, -2.3148e-02, -6.0364e-02, -3.0460e-03, -4.2462e-04,\n",
            "        -1.3123e-03, -5.1022e-04, -7.0286e-04, -4.8161e-05, -1.6224e-04,\n",
            "        -5.3644e-07, -3.2783e-06, -3.8743e-06,  0.0000e+00, -7.1526e-07,\n",
            "        -4.7684e-07, -5.7526e-03, -2.5757e-01, -2.0532e-01, -3.9355e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6377861499786377 tensor(6, device='cuda:0')\n",
            "lsy 3.822265625 0.0\n",
            "pred tensor([-0.0790, -0.1394, -0.4495, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6525955200195312 tensor(14, device='cuda:0')\n",
            "lsy 3.8828125 0.0\n",
            "pred tensor([-0.3638, -0.0064, -0.4861, -0.2161, -0.4436, -0.3076, -0.4771, -0.4941,\n",
            "        -0.4998, -0.4558, -0.3630, -0.1290, -0.3062, -0.2988, -0.4617, -0.2925,\n",
            "        -0.2168, -0.0293, -0.0410, -0.0084, -0.0048, -0.0257, -0.1108, -0.1058,\n",
            "        -0.4861], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6124768853187561 tensor(5, device='cuda:0')\n",
            "lsy 6.99609375 0.0\n",
            "pred tensor([-0.1981, -0.4741, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6526740193367004 tensor(14, device='cuda:0')\n",
            "lsy 4.9140625 0.0\n",
            "pred tensor([ 0.0000e+00, -5.9605e-08, -2.5725e-04, -5.9605e-08,  0.0000e+00,\n",
            "        -4.4107e-06,  0.0000e+00, -3.5763e-07, -4.4525e-05,  0.0000e+00,\n",
            "         0.0000e+00, -9.5367e-07,  0.0000e+00,  0.0000e+00, -1.5497e-06,\n",
            "        -3.2187e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "        -1.7881e-07, -6.8128e-05, -6.4969e-06, -1.4830e-04, -7.3969e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6369761228561401 tensor(13, device='cuda:0')\n",
            "lsy 4.91796875 0.0\n",
            "pred tensor([-0.0166, -0.4290, -0.4988, -0.5000, -0.4993, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6363360285758972 tensor(10, device='cuda:0')\n",
            "lsy 5.73046875 0.0\n",
            "pred tensor([-5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -1.2004e-04, -2.2693e-01, -4.3433e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6384714841842651 tensor(10, device='cuda:0')\n",
            "lsy 5.6875 0.0\n",
            "pred tensor([-0.4648, -0.5000, -0.5000, -0.5000, -0.4998, -0.4954, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4844, -0.5000, -0.5000, -0.5000, -0.4971, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4980, -0.5000, -0.5000, -0.4971, -0.4856, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6612753868103027 tensor(19, device='cuda:0')\n",
            "lsy 5.34765625 0.0\n",
            "pred tensor([-0.4624, -0.5000, -0.4978, -0.5000, -0.5000, -0.5000, -0.0033, -0.0005,\n",
            "        -0.0720, -0.0156, -0.1442, -0.0095, -0.3320, -0.4231, -0.0183, -0.0835,\n",
            "        -0.2786, -0.0575, -0.0526, -0.1113, -0.2832, -0.0029, -0.1465, -0.3792,\n",
            "        -0.2018], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6923070549964905 tensor(18, device='cuda:0')\n",
            "lsy 5.79296875 0.0\n",
            "pred tensor([-0.4675, -0.4976, -0.4971, -0.5000, -0.5000, -0.5000, -0.4985, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4990,\n",
            "        -0.4998, -0.4968, -0.5000, -0.5000, -0.5000, -0.4985, -0.4211, -0.4927,\n",
            "        -0.4954], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.687916100025177 tensor(17, device='cuda:0')\n",
            "lsy 4.125 0.0\n",
            "pred tensor([-7.5378e-02, -2.4487e-01, -1.6937e-02, -2.2507e-02, -3.0200e-01,\n",
            "        -1.8298e-01, -1.4313e-02, -3.4790e-01, -1.7456e-01, -1.3464e-01,\n",
            "        -2.4567e-02, -1.4651e-04, -5.5432e-06, -2.3842e-07, -8.3447e-07,\n",
            "         0.0000e+00, -5.9605e-08, -5.6171e-04,  0.0000e+00, -2.5570e-05,\n",
            "        -2.9802e-07, -5.3644e-07, -5.1117e-04, -4.8071e-01, -2.6417e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6770747900009155 tensor(13, device='cuda:0')\n",
            "lsy 5.11328125 0.0\n",
            "pred tensor([-6.0272e-03, -3.5553e-02, -4.5395e-03, -9.9182e-03,  0.0000e+00,\n",
            "        -9.2983e-06,  0.0000e+00, -1.1921e-06, -3.2306e-05,  0.0000e+00,\n",
            "        -6.3181e-06, -2.0742e-04, -7.7486e-07, -4.5896e-06, -1.7773e-01,\n",
            "        -4.9854e-01, -4.7510e-01, -3.2690e-01, -1.1035e-01, -6.7322e-02,\n",
            "        -7.2594e-03, -3.5691e-04, -1.6663e-01, -6.3515e-03, -1.6284e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6646655797958374 tensor(7, device='cuda:0')\n",
            "28\n",
            "lsy 5.58984375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4375, -0.4644, -0.1870,\n",
            "        -0.3682, -0.4692, -0.4924, -0.4934, -0.4709, -0.4802, -0.4993, -0.4773,\n",
            "        -0.4934, -0.4939, -0.4814, -0.4802, -0.5000, -0.3660, -0.4739, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6792612075805664 tensor(15, device='cuda:0')\n",
            "lsy 5.21875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6508772373199463 tensor(6, device='cuda:0')\n",
            "lsy 4.0 0.0\n",
            "pred tensor([-4.3225e-04, -1.0413e-01, -3.5791e-01, -2.4829e-01, -8.4045e-02,\n",
            "        -4.6289e-01, -4.3896e-01, -2.9492e-01, -7.3910e-06, -1.5616e-05,\n",
            "        -4.3333e-05, -1.5354e-04, -7.0267e-03, -2.4962e-04, -4.6372e-04,\n",
            "        -1.1921e-07, -1.3769e-05, -1.2123e-02, -5.3048e-06, -1.6541e-02,\n",
            "        -2.8400e-03, -1.8854e-03, -5.3711e-02, -4.7241e-02, -5.8098e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.649284839630127 tensor(5, device='cuda:0')\n",
            "lsy 4.26953125 0.0\n",
            "pred tensor([-8.3447e-04, -9.2089e-05, -3.5962e-01, -6.0616e-03, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -8.0466e-06, -3.9756e-05, -3.0935e-05,  0.0000e+00,\n",
            "         0.0000e+00, -1.9372e-05,  0.0000e+00,  0.0000e+00, -1.5318e-05,\n",
            "        -5.9605e-08, -3.0994e-03, -4.2139e-01, -7.3486e-02, -9.2850e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6421692371368408 tensor(4, device='cuda:0')\n",
            "lsy 3.708984375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9854e-01, -5.0000e-01, -3.3379e-06,  0.0000e+00, -1.5497e-06,\n",
            "        -6.7353e-06, -4.3058e-04, -1.9336e-04,  0.0000e+00, -1.7881e-07,\n",
            "        -1.1921e-07, -1.1921e-07, -4.7684e-07, -2.3224e-02,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6485065221786499 tensor(3, device='cuda:0')\n",
            "lsy 4.51171875 0.0\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00, -9.5844e-04,  0.0000e+00,  0.0000e+00,\n",
            "        -1.6093e-06, -5.7220e-06,  0.0000e+00, -5.6922e-05,  0.0000e+00,\n",
            "         0.0000e+00, -6.9046e-03,  0.0000e+00, -1.7881e-06,  0.0000e+00,\n",
            "        -2.8014e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3709e-06,\n",
            "        -1.4091e-04, -5.9605e-08, -2.1076e-04, -5.9605e-07,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6746141910552979 tensor(8, device='cuda:0')\n",
            "lsy 4.3515625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4968, -0.5000,\n",
            "        -0.4980, -0.5000, -0.4995, -0.5000, -0.5000, -0.5000, -0.4932, -0.5000,\n",
            "        -0.4958, -0.5000, -0.5000, -0.4976, -0.5000, -0.5000, -0.5000, -0.4941,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6452589631080627 tensor(5, device='cuda:0')\n",
            "lsy 4.1796875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.4968, -0.4993,\n",
            "        -0.4827, -0.4651, -0.4399, -0.4868, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.4626, -0.4878, -0.3455, -0.4602, -0.4600, -0.4011,\n",
            "        -0.4922], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6573876738548279 tensor(7, device='cuda:0')\n",
            "lsy 4.109375 0.0\n",
            "pred tensor([-6.7749e-02, -7.9989e-05, -1.8387e-03, -1.8295e-02, -1.3245e-01,\n",
            "        -4.8492e-02, -5.4283e-03, -5.2032e-02, -3.0184e-04, -1.0204e-03,\n",
            "        -8.1100e-03, -7.4585e-02, -3.3594e-01, -2.3956e-02, -1.7212e-02,\n",
            "        -1.9739e-01, -4.5197e-02, -1.4997e-04, -5.2338e-02, -4.4167e-05,\n",
            "        -2.3758e-02, -1.3914e-03, -7.7486e-07, -1.1539e-03, -3.0640e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.684171736240387 tensor(9, device='cuda:0')\n",
            "lsy 4.6015625 0.0\n",
            "pred tensor([-4.4556e-02, -3.1372e-01, -4.2432e-01, -2.2351e-01, -1.9470e-01,\n",
            "        -3.8477e-01, -2.3865e-02, -1.8811e-04, -1.3588e-02, -2.0374e-01,\n",
            "        -1.0278e-01, -7.2083e-02, -4.9194e-01, -1.7365e-02, -1.2810e-02,\n",
            "        -1.4343e-01, -2.1643e-01, -2.7075e-01, -2.1698e-02, -4.7900e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6617589592933655 tensor(9, device='cuda:0')\n",
            "lsy 4.65234375 0.0\n",
            "pred tensor([-2.0218e-02, -1.5114e-02, -9.1248e-02, -4.7656e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -4.9658e-01, -5.0000e-01, -5.0000e-01, -3.9917e-01,\n",
            "        -2.6831e-01, -2.8687e-01, -3.1592e-01, -2.1033e-01, -2.6657e-02,\n",
            "        -1.5533e-04, -6.0158e-03, -4.2358e-01, -4.9707e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6630672216415405 tensor(9, device='cuda:0')\n",
            "lsy 4.65625 0.0\n",
            "pred tensor([-3.7445e-02, -1.1921e-07, -4.2391e-04, -4.1580e-04, -1.0073e-04,\n",
            "        -1.5857e-01, -8.7786e-04, -1.0443e-03, -3.3855e-05, -4.0771e-02,\n",
            "        -3.9339e-06, -5.7936e-04, -1.7414e-03, -5.0316e-03, -4.1089e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.624608039855957 tensor(6, device='cuda:0')\n",
            "lsy 3.962890625 0.0\n",
            "pred tensor([-0.4919, -0.5000, -0.5000, -0.4814, -0.4968, -0.4971, -0.5000, -0.4827,\n",
            "        -0.4885, -0.4771, -0.4927, -0.4949, -0.4949, -0.4661, -0.5000, -0.4924,\n",
            "        -0.4946, -0.5000, -0.4863, -0.5000, -0.4998, -0.4944, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6668165922164917 tensor(14, device='cuda:0')\n",
            "lsy 5.62890625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.4998, -0.5000, -0.4983, -0.4990, -0.4963,\n",
            "        -0.5000, -0.5000, -0.4956, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.4866, -0.5000, -0.4736, -0.2820,\n",
            "        -0.0858], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6562033891677856 tensor(8, device='cuda:0')\n",
            "lsy 5.1484375 0.0\n",
            "pred tensor([-4.8999e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -2.2327e-01, -3.7988e-01,\n",
            "        -4.9536e-01, -4.6509e-01, -4.8022e-01, -4.8364e-01, -4.5605e-01,\n",
            "        -5.0000e-01, -2.0789e-01, -1.8387e-02, -3.9978e-03, -7.5317e-02,\n",
            "        -6.9153e-02, -1.8425e-03, -1.5140e-05, -3.7575e-03, -2.8906e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.637101948261261 tensor(6, device='cuda:0')\n",
            "lsy 5.03125 0.0\n",
            "pred tensor([-0.2432, -0.5000, -0.3828, -0.4973, -0.4968, -0.4985, -0.4944, -0.4607,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4739, -0.5000, -0.4875, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4968,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6540323495864868 tensor(7, device='cuda:0')\n",
            "lsy 6.53125 0.0\n",
            "pred tensor([-2.7905e-01, -2.7905e-01, -2.6343e-01, -4.8804e-01, -2.1399e-01,\n",
            "        -2.9248e-01, -8.7402e-02, -4.4800e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.6411e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -3.5763e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6596128940582275 tensor(9, device='cuda:0')\n",
            "lsy 5.28515625 0.0\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4438e-06, -9.8944e-06,\n",
            "         0.0000e+00, -3.5763e-07, -1.1606e-03, -1.3113e-06, -3.4809e-04,\n",
            "        -6.3133e-03, -1.4648e-01, -4.1895e-01, -4.7632e-01, -4.9976e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9756e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6624481081962585 tensor(12, device='cuda:0')\n",
            "lsy 3.724609375 0.0\n",
            "pred tensor([-6.3002e-05, -3.4027e-03,  0.0000e+00, -4.2076e-03, -9.3162e-05,\n",
            "        -1.3456e-03, -3.6955e-06, -2.9802e-07, -1.0389e-04, -3.0935e-05,\n",
            "        -4.1080e-04, -1.0967e-05, -7.2539e-05,  0.0000e+00, -1.0556e-04,\n",
            "        -3.1185e-03, -4.7684e-07, -3.5167e-06, -6.1333e-05, -3.1796e-03,\n",
            "        -1.2934e-04, -2.3842e-07, -3.3894e-03, -1.3723e-03, -2.1326e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6446046233177185 tensor(9, device='cuda:0')\n",
            "lsy 6.5078125 0.0\n",
            "pred tensor([-0.0320, -0.0217, -0.2937, -0.0283, -0.4211, -0.4670, -0.4229, -0.3901,\n",
            "        -0.2686, -0.0167, -0.5000, -0.4702, -0.4531, -0.4785, -0.4529, -0.4685,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4854, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6028587222099304 tensor(2, device='cuda:0')\n",
            "lsy 5.7734375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.4956, -0.4939, -0.5000, -0.5000, -0.4954, -0.4971, -0.4944,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6535139679908752 tensor(10, device='cuda:0')\n",
            "lsy 5.5859375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.4993, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4607, -0.4033, -0.3845,\n",
            "        -0.0260], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6559544801712036 tensor(9, device='cuda:0')\n",
            "lsy 5.44140625 0.0\n",
            "pred tensor([-8.5235e-06, -9.4175e-06,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00, -9.5367e-07,  0.0000e+00,\n",
            "        -2.0203e-02, -2.1713e-02, -3.9825e-02, -1.8723e-02, -1.6650e-01,\n",
            "        -4.6045e-01, -4.1162e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9927e-01, -5.0000e-01, -4.9805e-01, -5.0000e-01, -4.7314e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6490310430526733 tensor(12, device='cuda:0')\n",
            "lsy 5.39453125 0.0\n",
            "pred tensor([-5.0000e-01, -4.8462e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -5.1403e-04, -2.2233e-05, -2.9802e-07,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -5.3644e-07,  0.0000e+00, -2.3842e-07,  0.0000e+00, -1.4937e-04,\n",
            "         0.0000e+00, -3.3498e-05, -3.0935e-05,  0.0000e+00, -9.9182e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6994054913520813 tensor(15, device='cuda:0')\n",
            "lsy 4.5234375 0.0\n",
            "pred tensor([-4.8657e-01, -5.0000e-01, -4.8169e-01, -5.0000e-01,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -3.7551e-06,  0.0000e+00,  0.0000e+00, -5.9605e-08, -8.3447e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6655269861221313 tensor(11, device='cuda:0')\n",
            "lsy 4.30078125 0.0\n",
            "pred tensor([-1.2636e-05,  0.0000e+00, -2.5392e-05, -4.8113e-04, -7.8979e-02,\n",
            "        -3.0231e-04, -3.7003e-03, -1.6680e-03, -1.2894e-02, -4.2651e-01,\n",
            "        -4.2554e-01, -4.9707e-01, -4.8486e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6751805543899536 tensor(12, device='cuda:0')\n",
            "lsy 4.71484375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6626200675964355 tensor(10, device='cuda:0')\n",
            "lsy 4.33203125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6867517232894897 tensor(11, device='cuda:0')\n",
            "lsy 5.5390625 0.0\n",
            "pred tensor([-1.7285e-06, -4.9472e-06, -5.2368e-02, -2.8896e-03, -9.5081e-04,\n",
            "        -4.9438e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -1.5497e-05, -5.6419e-03,  0.0000e+00,\n",
            "        -4.1723e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -6.1798e-03,  0.0000e+00, -5.3644e-07,  0.0000e+00, -4.6849e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6658377647399902 tensor(6, device='cuda:0')\n",
            "lsy 4.08203125 0.0\n",
            "pred tensor([-5.9605e-08,  0.0000e+00, -2.1875e-05, -2.7418e-06, -1.5497e-06,\n",
            "        -5.9605e-08, -2.3842e-07, -2.3842e-07, -9.7215e-05, -5.9605e-08,\n",
            "         0.0000e+00, -6.8665e-04, -1.0848e-04,  0.0000e+00, -2.6584e-05,\n",
            "         0.0000e+00, -1.1921e-07, -3.1137e-04, -4.7150e-02, -6.2585e-06,\n",
            "        -5.1117e-02, -1.8299e-05, -6.6147e-03, -1.4150e-04, -1.1861e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6541951298713684 tensor(6, device='cuda:0')\n",
            "lsy 4.71875 0.0\n",
            "pred tensor([-0.1709, -0.3542, -0.5000, -0.5000, -0.5000, -0.4995, -0.5000, -0.5000,\n",
            "        -0.4846, -0.4998, -0.5000, -0.5000, -0.5000, -0.4973, -0.5000, -0.5000,\n",
            "        -0.4990, -0.4880, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6698516607284546 tensor(10, device='cuda:0')\n",
            "lsy 5.0859375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4998, -0.5000,\n",
            "        -0.4695, -0.5000, -0.4956, -0.5000, -0.4988, -0.4788, -0.4905, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4988,\n",
            "        -0.4954], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6463097333908081 tensor(3, device='cuda:0')\n",
            "lsy 4.4765625 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.5170e-04, -8.4460e-05, -4.1723e-07, -5.3644e-07, -3.9935e-06,\n",
            "        -4.5919e-04, -7.6151e-04, -9.9659e-04, -3.1226e-01, -3.0100e-05,\n",
            "        -1.4901e-06, -1.3888e-05, -7.7486e-07, -3.2961e-05, -2.1458e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6469934582710266 tensor(8, device='cuda:0')\n",
            "lsy 4.375 0.0\n",
            "pred tensor([-8.2254e-06, -6.4135e-04, -3.4571e-06, -2.6276e-02, -4.7455e-02,\n",
            "        -7.3471e-03, -3.6914e-01, -7.6294e-02, -4.5068e-01, -4.7241e-01,\n",
            "        -4.8291e-01, -5.0000e-01, -4.9316e-01, -4.5288e-01, -5.0000e-01,\n",
            "        -2.6538e-01, -4.9390e-01, -5.0000e-01, -5.0000e-01, -4.9756e-01,\n",
            "        -4.9561e-01, -4.7949e-01, -4.1577e-01, -4.2432e-01, -4.6924e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6852789521217346 tensor(12, device='cuda:0')\n",
            "lsy 4.3203125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4946, -0.4998, -0.5000,\n",
            "        -0.4995, -0.4966, -0.5000, -0.5000, -0.5000, -0.4893, -0.4961, -0.4949,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4993, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6776716709136963 tensor(15, device='cuda:0')\n",
            "lsy 4.7109375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6705098152160645 tensor(10, device='cuda:0')\n",
            "lsy 4.4453125 0.0\n",
            "pred tensor([-1.3876e-04, -4.2629e-04, -3.3760e-03, -6.5565e-07, -4.4775e-04,\n",
            "        -1.8606e-03, -6.0730e-03, -6.2585e-06, -3.3402e-04, -1.1871e-01,\n",
            "        -3.9749e-03, -1.0907e-01, -1.9165e-01, -4.6143e-01, -4.0063e-01,\n",
            "        -3.4839e-01, -3.8770e-01, -2.6562e-01, -1.5112e-01, -1.2964e-01,\n",
            "        -1.7120e-02, -1.7761e-01, -2.7393e-01, -2.7466e-01, -4.2432e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6584088206291199 tensor(10, device='cuda:0')\n",
            "lsy 4.578125 0.0\n",
            "pred tensor([-5.0000e-01, -4.7754e-01, -4.4385e-01, -4.6851e-01, -4.9658e-01,\n",
            "        -4.8096e-01, -1.8787e-01, -3.1689e-01, -4.2578e-01, -4.1534e-02,\n",
            "        -4.8291e-01, -6.7871e-02, -4.1235e-01, -3.4595e-01, -4.3262e-01,\n",
            "        -3.5376e-01, -2.0683e-05, -3.1079e-01, -3.0701e-02, -4.3610e-02,\n",
            "        -1.4355e-01, -2.0398e-01, -2.6733e-01, -4.8535e-01, -4.9438e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6363718509674072 tensor(6, device='cuda:0')\n",
            "29\n",
            "lsy 4.59375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -1.7297e-01, -7.5378e-02,\n",
            "        -1.2146e-02, -2.9614e-01, -1.1194e-04, -2.1100e-05, -3.1590e-06,\n",
            "        -9.4147e-03, -2.4815e-03, -1.8716e-05, -1.7285e-06, -5.8770e-05,\n",
            "        -4.3884e-02, -2.7405e-02, -6.7825e-03, -1.3901e-02, -2.3596e-01,\n",
            "        -2.2510e-01, -3.9038e-01, -3.8843e-01, -4.8279e-02, -7.2937e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6415974497795105 tensor(8, device='cuda:0')\n",
            "lsy 4.48828125 0.0\n",
            "pred tensor([-3.1396e-01, -7.4097e-02, -1.4233e-01, -2.0422e-01, -3.8696e-01,\n",
            "        -3.5449e-01, -4.9023e-01, -4.9731e-01, -4.6826e-01, -4.3433e-01,\n",
            "        -4.7046e-01, -4.8755e-01, -4.0308e-01, -4.6362e-01, -3.4253e-01,\n",
            "        -5.8937e-04,  0.0000e+00,  0.0000e+00, -2.3842e-07,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9802e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6510119438171387 tensor(6, device='cuda:0')\n",
            "lsy 3.763671875 0.0\n",
            "pred tensor([-6.1096e-02, -7.1526e-07,  0.0000e+00, -5.9557e-04, -2.0266e-06,\n",
            "        -1.0681e-04, -5.9605e-08, -1.0133e-06, -5.9605e-08, -4.6921e-04,\n",
            "         0.0000e+00, -1.6570e-05, -5.7399e-05, -1.1921e-07,  0.0000e+00,\n",
            "        -4.6468e-04, -8.7500e-05, -8.7786e-04, -1.3494e-04, -6.1393e-06,\n",
            "        -2.8553e-03, -2.8976e-02, -8.3447e-07, -3.3894e-03, -9.8877e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6550662517547607 tensor(7, device='cuda:0')\n",
            "lsy 4.04296875 0.0\n",
            "pred tensor([-4.1723e-07, -6.0272e-03, -3.9887e-04, -2.0862e-06, -8.3447e-07,\n",
            "         0.0000e+00, -2.8431e-05, -3.0689e-03,  0.0000e+00, -2.3651e-03,\n",
            "        -2.2590e-05, -5.9605e-08,  0.0000e+00, -1.1986e-02, -2.4438e-06,\n",
            "        -1.1921e-07, -1.1255e-01, -4.5300e-06, -2.2709e-05, -2.5392e-05,\n",
            "        -9.8145e-02, -2.0504e-04, -7.6008e-04, -2.8782e-03, -1.3403e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6682016849517822 tensor(7, device='cuda:0')\n",
            "lsy 5.02734375 0.0\n",
            "pred tensor([-0.1848, -0.1610, -0.2639, -0.3320, -0.4641, -0.4993, -0.5000, -0.4993,\n",
            "        -0.4956, -0.4971, -0.5000, -0.5000, -0.4968, -0.5000, -0.4783, -0.4771,\n",
            "        -0.4978, -0.4929, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6680665016174316 tensor(9, device='cuda:0')\n",
            "lsy 3.96875 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4880, -0.4761,\n",
            "        -0.4946, -0.4939, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.4998, -0.5000, -0.1257, -0.3594, -0.0583, -0.4751, -0.4922, -0.3503,\n",
            "        -0.4980], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6456241607666016 tensor(9, device='cuda:0')\n",
            "lsy 4.61328125 0.0\n",
            "pred tensor([-0.4802, -0.4900, -0.4900, -0.3970, -0.4836, -0.4385, -0.4910, -0.4712,\n",
            "        -0.4949, -0.4873, -0.4912, -0.4695, -0.4990, -0.5000, -0.5000, -0.4980,\n",
            "        -0.5000, -0.4922, -0.5000, -0.5000, -0.5000, -0.5000, -0.4978, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6707758903503418 tensor(16, device='cuda:0')\n",
            "lsy 4.453125 0.0\n",
            "pred tensor([-0.4988, -0.5000, -0.5000, -0.5000, -0.4968, -0.4976, -0.4893, -0.4692,\n",
            "        -0.4956, -0.4871, -0.4890, -0.4973, -0.4856, -0.5000, -0.4817, -0.4626,\n",
            "        -0.4661, -0.4980, -0.4895, -0.4927, -0.4524, -0.4895, -0.4946, -0.4919,\n",
            "        -0.4976], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6452914476394653 tensor(17, device='cuda:0')\n",
            "lsy 4.72265625 0.0\n",
            "pred tensor([-3.3142e-02, -3.0884e-02, -7.7087e-02, -3.0828e-04, -7.9632e-04,\n",
            "        -1.0498e-01, -7.7095e-03, -9.0515e-02, -4.8633e-01, -4.9976e-01,\n",
            "        -3.6719e-01, -4.9634e-01, -1.9409e-01, -3.8843e-01, -4.9536e-01,\n",
            "        -5.0000e-01, -4.4482e-01, -8.3780e-04, -1.3647e-01, -2.6172e-01,\n",
            "        -2.1683e-02, -1.1584e-01, -3.1830e-02, -1.4544e-05, -8.8806e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6856995224952698 tensor(13, device='cuda:0')\n",
            "lsy 4.796875 0.0\n",
            "pred tensor([-0.0436, -0.0021, -0.0162, -0.0038, -0.2473, -0.3210, -0.4397, -0.4624,\n",
            "        -0.3865, -0.2537, -0.2140, -0.0614, -0.4995, -0.5000, -0.5000, -0.4958,\n",
            "        -0.5000, -0.5000, -0.4800, -0.5000, -0.5000, -0.4995, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6777685284614563 tensor(13, device='cuda:0')\n",
            "lsy 4.5 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.4963, -0.5000, -0.5000, -0.4951, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.4993, -0.4980, -0.5000, -0.5000, -0.4954,\n",
            "        -0.5000, -0.5000, -0.4990, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6855261325836182 tensor(17, device='cuda:0')\n",
            "lsy 4.62109375 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6442175507545471 tensor(10, device='cuda:0')\n",
            "lsy 4.703125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4951, -0.5000, -0.4956,\n",
            "        -0.4988, -0.4976, -0.4993, -0.5000, -0.5000, -0.5000, -0.5000, -0.4985,\n",
            "        -0.5000, -0.5000, -0.0077, -0.4946, -0.4285, -0.2998, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6404914259910583 tensor(13, device='cuda:0')\n",
            "lsy 4.31640625 0.0\n",
            "pred tensor([-0.4993, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6485302448272705 tensor(12, device='cuda:0')\n",
            "lsy 8.6953125 0.0\n",
            "pred tensor([-4.9561e-01, -4.9561e-01, -4.9951e-01, -4.9634e-01, -4.5386e-01,\n",
            "        -4.8047e-01, -4.9829e-01, -4.5483e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
            "         0.0000e+00,  0.0000e+00, -1.1635e-04, -1.6434e-02, -2.0428e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.657241940498352 tensor(18, device='cuda:0')\n",
            "lsy 4.1484375 0.0\n",
            "pred tensor([-6.0201e-06, -4.5013e-02, -1.7738e-04, -7.0000e-04, -1.9646e-04,\n",
            "        -9.1374e-05, -1.0986e-02, -1.2815e-05, -7.4244e-04, -1.3660e-01,\n",
            "        -1.1921e-07, -1.1921e-07, -1.5854e-02, -5.6696e-04, -5.9605e-08,\n",
            "        -4.2229e-03, -9.1248e-02, -3.1357e-03, -1.1963e-02, -8.9359e-04,\n",
            "        -8.8013e-02, -8.3447e-07, -1.4673e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6473156809806824 tensor(14, device='cuda:0')\n",
            "lsy 5.765625 0.0\n",
            "pred tensor([-4.9463e-01, -4.9756e-01, -4.9976e-01, -5.0000e-01, -3.9844e-01,\n",
            "        -1.1475e-01, -1.3354e-01, -1.5850e-03, -1.6391e-05, -1.6737e-04,\n",
            "        -1.5378e-05, -1.7881e-07, -1.4740e-02, -7.2815e-02, -4.3030e-02,\n",
            "        -3.8110e-01, -2.6685e-01, -4.7314e-01, -3.4985e-01, -3.6230e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.9927e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6465967297554016 tensor(10, device='cuda:0')\n",
            "lsy 7.15234375 0.0\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.8207e-05,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.1921e-07,  0.0000e+00, -5.1270e-02, -2.3773e-02, -1.2646e-01,\n",
            "        -4.7876e-01, -5.0000e-01, -4.9243e-01, -4.9341e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6780996918678284 tensor(17, device='cuda:0')\n",
            "lsy 7.41015625 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4980, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4976,\n",
            "        -0.4978, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.659976065158844 tensor(21, device='cuda:0')\n",
            "lsy 3.90625 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.9731e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -8.9407e-07, -1.7712e-01, -2.0523e-02,\n",
            "        -5.0018e-02, -1.2354e-01, -3.8696e-01, -4.2065e-01, -4.5581e-01,\n",
            "        -4.8975e-01, -4.9414e-01, -3.6084e-01, -1.2842e-01, -1.0760e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6397900581359863 tensor(17, device='cuda:0')\n",
            "lsy 5.71875 0.0\n",
            "pred tensor([-1.6724e-01, -2.5558e-03, -7.8064e-02, -2.3352e-01, -4.1138e-01,\n",
            "        -2.0471e-01, -2.0471e-01, -4.8267e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.8364e-01,\n",
            "        -4.9927e-01, -2.3022e-01, -1.4075e-01, -7.0862e-02, -3.1152e-01,\n",
            "        -5.6091e-02, -2.7061e-04, -5.7068e-03, -2.5406e-03, -4.1943e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6281731128692627 tensor(14, device='cuda:0')\n",
            "lsy 4.97265625 0.0\n",
            "pred tensor([-2.5464e-01, -1.6861e-02, -6.0028e-02, -2.2461e-02, -3.9160e-05,\n",
            "        -5.8502e-02, -3.9337e-02, -2.2925e-01, -4.4043e-01, -3.6157e-01,\n",
            "        -4.5386e-01, -4.5312e-01, -4.1455e-01, -4.0820e-01, -4.8926e-01,\n",
            "        -4.9341e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6577282547950745 tensor(18, device='cuda:0')\n",
            "lsy 5.99609375 0.0\n",
            "pred tensor([-0.5000, -0.4866, -0.5000, -0.5000, -0.5000, -0.4910, -0.4988, -0.4927,\n",
            "        -0.4890, -0.5000, -0.4810, -0.4971, -0.4958, -0.4119, -0.4563, -0.4382,\n",
            "        -0.4048, -0.4395, -0.2236, -0.1481, -0.0157, -0.0051, -0.0019, -0.0194,\n",
            "        -0.0306], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6574476361274719 tensor(16, device='cuda:0')\n",
            "lsy 5.9375 0.0\n",
            "pred tensor([-0.0041, -0.2542, -0.0524, -0.0400, -0.0750, -0.0473, -0.1379, -0.0105,\n",
            "        -0.0401, -0.0327, -0.0883, -0.0340, -0.0337, -0.1890, -0.2480, -0.2881,\n",
            "        -0.2825, -0.2717, -0.3669, -0.4795, -0.4153, -0.1591, -0.4258, -0.1299,\n",
            "        -0.3853], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6501257419586182 tensor(15, device='cuda:0')\n",
            "lsy 4.00390625 0.0\n",
            "pred tensor([-0.4255, -0.0138, -0.3206, -0.3274, -0.0580, -0.2507, -0.0076, -0.1896,\n",
            "        -0.2030, -0.4260, -0.2471, -0.4475, -0.4880, -0.4177, -0.4951, -0.4709,\n",
            "        -0.5000, -0.4834, -0.4705, -0.5000, -0.4856, -0.4988, -0.5000, -0.4812,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6559340953826904 tensor(15, device='cuda:0')\n",
            "lsy 3.8203125 0.0\n",
            "pred tensor([-0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000, -0.5000, -0.2993, -0.4114, -0.2859, -0.0804, -0.0495, -0.0122,\n",
            "        -0.0712], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.668018639087677 tensor(12, device='cuda:0')\n",
            "lsy 4.91796875 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.9536e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -4.9927e-01, -5.0000e-01, -5.0000e-01,  0.0000e+00,  0.0000e+00,\n",
            "        -5.4836e-06, -1.7881e-07,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "        -1.1921e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7881e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.675530731678009 tensor(17, device='cuda:0')\n",
            "lsy 4.7109375 0.0\n",
            "pred tensor([-5.2691e-05,  0.0000e+00, -4.5300e-06, -2.3842e-07,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -2.8753e-04,  0.0000e+00, -1.2009e-02,\n",
            "         0.0000e+00,  0.0000e+00, -1.2779e-04,  0.0000e+00, -1.1921e-07,\n",
            "         0.0000e+00, -2.3365e-03, -2.0313e-03, -4.9707e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6736979484558105 tensor(11, device='cuda:0')\n",
            "lsy 3.970703125 0.0\n",
            "pred tensor([-4.3384e-01, -2.2888e-01, -4.9048e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -1.9958e-01, -1.0095e-01,  0.0000e+00,\n",
            "         0.0000e+00, -3.6955e-06,  0.0000e+00, -8.1682e-04, -2.2058e-01,\n",
            "        -5.8289e-02, -2.7563e-01, -4.3481e-01, -3.3789e-01, -3.4741e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6301518082618713 tensor(7, device='cuda:0')\n",
            "lsy 3.6796875 0.0\n",
            "pred tensor([-4.3433e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -4.9854e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.4240e-06, -1.1921e-05, -1.7338e-03, -1.0490e-02, -2.5058e-04,\n",
            "        -4.3631e-05, -1.2627e-02, -7.7486e-07, -2.5630e-06, -3.6359e-06,\n",
            "        -3.7336e-04, -3.9087e-01, -1.0151e-04, -5.9605e-07, -1.3304e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6493400931358337 tensor(7, device='cuda:0')\n",
            "lsy 6.21484375 0.0\n",
            "pred tensor([-5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.8706e-01, -1.0056e-02, -3.1638e-04,\n",
            "        -7.6843e-02, -1.2279e-05, -1.1063e-04, -6.6528e-03, -1.1921e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6600598692893982 tensor(9, device='cuda:0')\n",
            "lsy 3.5078125 0.0\n",
            "pred tensor([ 0.0000e+00, -9.0003e-06,  0.0000e+00, -1.7285e-06, -5.3644e-07,\n",
            "        -4.4403e-02, -2.4533e-04, -5.0247e-05, -2.1637e-02, -4.3091e-02,\n",
            "        -1.7041e-01, -1.4435e-02, -1.2123e-02, -5.5542e-03, -1.1162e-02,\n",
            "        -7.7009e-04, -1.3227e-03, -4.6825e-04, -2.0325e-05, -7.9989e-05,\n",
            "        -2.5630e-06, -1.0473e-04,  0.0000e+00, -1.1921e-07,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6444130539894104 tensor(9, device='cuda:0')\n",
            "lsy 4.40234375 0.0\n",
            "pred tensor([-0.4060, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000, -0.4980,\n",
            "        -0.5000, -0.5000, -0.4954, -0.4016, -0.4580, -0.4431, -0.2026, -0.4790,\n",
            "        -0.4949, -0.4712, -0.4875, -0.4932, -0.4939, -0.5000, -0.5000, -0.5000,\n",
            "        -0.5000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.64005047082901 tensor(9, device='cuda:0')\n",
            "lsy 4.88671875 0.0\n",
            "pred tensor([-4.9658e-01, -5.0000e-01, -4.9023e-01, -5.0000e-01, -2.1301e-01,\n",
            "        -4.2944e-01, -4.0332e-01, -4.6704e-01, -3.2959e-01, -4.5532e-01,\n",
            "        -8.5388e-02, -1.0300e-03, -5.3072e-04,  0.0000e+00, -1.3888e-05,\n",
            "         0.0000e+00, -1.3709e-06,  0.0000e+00, -7.3814e-04, -3.0696e-05,\n",
            "        -2.0981e-02, -2.9697e-03, -2.0337e-04, -4.9622e-02, -4.1723e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.6690326929092407 tensor(8, device='cuda:0')\n",
            "lsy 3.435546875 0.0\n",
            "pred tensor([-2.2058e-01, -9.5978e-03, -4.9268e-01, -5.0000e-01, -3.9697e-01,\n",
            "        -4.7363e-01, -5.0000e-01, -4.9463e-01, -5.0000e-01, -5.0000e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.8804e-01, -5.0000e-01, -4.7974e-01,\n",
            "        -5.0000e-01, -5.0000e-01, -4.0503e-01, -3.8428e-01, -2.3853e-01,\n",
            "        -3.0640e-02, -8.2159e-04, -5.5611e-05, -5.9605e-07, -1.1921e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6295361518859863 tensor(8, device='cuda:0')\n",
            "lsy 6.52734375 0.0\n",
            "pred tensor([-2.4104e-04, -9.0301e-05, -3.7909e-05, -4.1962e-05, -2.3842e-07,\n",
            "        -2.3186e-04, -5.9605e-07, -6.5565e-07, -4.0054e-05, -7.8440e-05,\n",
            "        -3.3188e-03, -2.5131e-02, -3.8550e-01, -2.6758e-01, -1.9739e-01,\n",
            "        -2.1347e-02, -3.7289e-03, -4.0356e-01, -1.6394e-01, -2.2537e-02,\n",
            "        -2.6779e-03, -2.2293e-02, -2.9802e-07, -4.5459e-01, -8.3618e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.6855540871620178 tensor(10, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "for i in range(100):\n",
        "    print(i)\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    torch.save(checkpoint, folder+'agentoptimargm4.pkl')\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "dd2b4f22-31f1-4f12-c6f1-25ac68e565d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "# ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n",
        "\n",
        "\n",
        "# # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "# 2  5/11 8\n",
        "# 1/10 4 7/9\n",
        "# 0  3/12 6\n",
        "\n",
        "# 13 11 14\n",
        "# 10 12 9\n",
        "\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "env = TimeLimit(env, max_episode_steps=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PraFUAPB3j7v",
        "outputId": "691b40c7-6945-40d2-9226-cd21ededb4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "<ipython-input-9-99d212b47ecb>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-9-99d212b47ecb>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dided\n",
            "time\n",
            "[0, 5, 6, 12, 6, 11, 2, 11, 4, 9, 11, 5, 9, 5, 4, 11, 8]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    lstate=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6], [1, 256]\n",
        "        lstate.append(state)\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # act = agent(state, k)\n",
        "            act = agent(lstate, k=k)\n",
        "            lstate=[]\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"dided\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cm6KjvBrnNO",
        "outputId": "38db3cb7-fc99-44b0-ac57-5ceb0109f8ca",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-d80e75a9d669>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-47-d80e75a9d669>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dided\n",
            "time\n",
            "[8, 8, 4, 5, 3, 5, 8, 8, 14, 8, 8, 5, 4, 8, 8, 13, 6, 8, 10, 8, 13, 8, 9, 3, 0, 4, 14, 0, 9, 4, 5, 4, 3, 3, 8, 5, 3, 8, 3, 8, 11, 8, 0, 8, 0, 8, 4, 6, 4, 8, 2, 14, 8, 14, 8, 8, 8, 8, 8, 3, 0, 14, 14, 8, 8, 8, 3, 2, 3, 0, 0, 4, 3, 3, 11, 7, 4, 5, 5, 8, 4, 8, 0, 8, 14, 6, 2, 9, 0, 8, 0, 5, 2, 8, 4, 6, 8, 2, 0, 6, 1, 2, 4, 14, 12, 8, 14, 2, 8, 8, 5, 8, 8, 0, 7, 8, 2, 8, 3, 5, 2, 13, 3, 8, 8, 4, 13, 8, 4, 2, 5, 8, 11, 3, 4, 3, 2, 14, 9, 8, 14, 8, 3, 8, 8, 3, 3, 13, 8, 2, 11, 4, 3, 3, 8, 2, 3, 5, 14, 4, 8, 4, 8, 8, 4, 5, 8, 4, 9, 3, 13, 5, 13, 8, 3, 5, 3, 8, 14, 9, 11, 4, 5, 4, 3, 14, 2, 8, 3, 8, 9, 8, 8, 8, 0, 3, 8, 4, 5, 8, 3, 8, 5, 14, 3, 13, 8, 8, 14, 13, 5, 8, 3, 11, 8, 8, 8, 4, 5, 8, 8, 8, 4, 3, 8, 2, 8, 3, 3, 14, 13, 2, 8, 2, 8, 8, 14, 2, 0, 11, 4, 3, 11, 13, 4, 8, 4, 8, 9, 8, 3, 5, 8, 9, 9, 6, 9, 3, 11, 11, 8, 11, 5, 2, 3, 4, 3, 11, 4, 3, 8, 3, 3, 8, 8, 4, 13, 3, 8, 6, 4, 4, 8, 9, 0, 8, 13, 8, 3, 3, 8, 0, 11, 2, 8, 2, 3, 8, 8, 8, 14, 14, 2, 8, 11, 2, 2, 6, 14, 8, 2, 11, 4, 8, 14, 5, 9, 3, 8, 9, 0, 8, 14, 3, 8, 13, 8, 3, 14, 8, 6, 0, 8, 8, 3, 8, 8, 3, 5, 14, 14, 9, 4, 14, 2, 3, 8, 11, 3, 3, 8, 8, 0, 5, 4, 2, 2, 13, 4, 13, 2, 2, 2, 5, 11, 9, 3, 5, 4, 8, 0, 2, 4, 5, 8, 4, 8, 8, 9, 2, 11, 13, 11, 3, 11, 4, 4, 8, 8, 8, 8, 4, 6, 4, 8, 2, 9, 3, 8, 3, 14, 11, 2, 8, 4, 4, 8, 8, 8, 2, 2, 4, 3, 14, 0, 5, 2, 8, 0, 0, 9, 8, 8, 4, 8, 4, 0, 11, 8, 9, 5, 5, 8, 5, 8, 13, 8, 2, 8, 8, 11, 14, 8, 0, 0, 8, 4, 2, 6, 3, 2, 6, 2, 8, 8, 14, 2, 14, 2, 4, 8, 8, 4, 4, 8, 14, 3, 14, 8, 8, 8, 4, 2, 0, 3, 5, 4, 2, 8, 14, 3, 3, 9, 14, 0, 5, 8, 6, 11, 8, 8, 2, 5, 14, 3, 11, 8, 0, 2, 13, 5, 8, 8, 8, 5, 3, 8, 13, 3, 8, 11, 0, 3, 5, 8, 5, 13, 11, 8, 3, 5, 0, 8, 3, 4, 3, 4, 0, 8, 14, 3, 8, 8, 4, 4, 3, 3, 5, 0, 3, 0, 0, 8, 11, 13, 8, 9, 11, 14, 8, 2, 13, 8, 11, 8, 4, 0, 11, 8, 8, 8, 14, 3, 5, 5, 2, 8, 3, 4, 0, 11, 9, 8, 3, 14, 8, 8, 14, 3, 8, 2, 11, 8, 8, 3, 11, 14, 5, 8, 8, 3, 11, 3, 3, 2, 0, 8, 14, 8, 3, 0, 8, 3, 3, 8, 3, 0, 3, 11, 8, 2, 3, 0, 3, 8, 8, 13, 8, 14]\n",
            "dided\n",
            "time\n",
            "[8, 14, 4, 4, 4, 2, 8, 11, 8, 8, 8, 3, 2, 9, 8, 0, 2, 8, 8, 5, 8, 0, 2, 14, 3, 8, 8, 8, 8, 2, 3, 8, 8, 8, 13]\n",
            "dided\n",
            "time\n",
            "[8, 13, 8, 6, 8, 8, 11, 6, 13, 8, 8, 4, 8, 8, 2, 3, 9, 13, 8, 11, 5, 5, 3, 4, 8, 3, 8, 3, 12, 8, 14, 8, 0, 3, 4, 2, 0, 6, 4, 2, 4, 14, 4, 3, 3, 11, 11, 8, 13, 8, 8, 4, 2, 8, 8, 13, 3, 8, 5, 4, 3, 14, 9, 8, 14, 11, 4, 8, 3, 6, 8, 5, 3, 8, 5, 0, 4, 2, 9, 2, 3, 0, 8, 8, 8, 8, 0, 9, 3, 3, 8, 14, 8, 8, 13, 8, 14, 3, 3, 8, 4, 8, 9, 14, 8, 3, 3, 8, 8, 8, 9, 11, 3, 14, 8, 9, 8, 4, 8, 3, 3, 6, 8, 8, 13, 14, 0, 4, 3, 5, 13, 4, 2, 0, 8, 2, 2, 13, 14, 0, 2, 4, 8, 5, 3, 6, 8, 3, 3, 0, 3, 8, 3, 3, 8, 8, 8, 4, 14, 14, 5, 14, 8, 0, 8, 4, 8, 3, 3, 3, 8, 8, 3, 3, 2, 14, 11, 3, 13, 5, 3, 3, 11, 13, 8, 8, 8, 8, 8, 8, 8, 3, 3, 2, 11, 2, 14, 14, 5, 8, 0, 4, 0, 14, 2, 4, 14, 8, 4, 14, 13, 5, 3, 8, 8, 4, 3, 3, 0]\n",
            "dided\n",
            "time\n",
            "[3, 0, 11, 4, 10, 3, 2, 8, 6, 8, 8, 8, 4, 3, 4, 8, 8, 3, 8, 14, 8, 8, 14, 8, 9, 11, 9, 8, 0, 14, 14, 8, 8, 5, 4, 11, 0, 9, 14, 0, 3, 4]\n",
            "dided\n",
            "time\n",
            "[4, 0, 8, 3, 5, 14, 3, 8, 5, 8, 4, 3, 8, 11, 8, 4, 8, 4, 8, 3, 8, 8, 5, 14, 11, 8, 8, 8, 8, 3, 9, 4, 4, 4, 13, 14, 6, 11, 5, 9, 2, 8, 8, 8, 9, 8, 5, 14, 8, 3, 4, 2, 8, 3, 8, 0, 0, 8, 9, 0, 0, 14, 8, 5, 3, 8, 8, 14, 14, 13, 14, 8, 0, 5, 3, 4, 4, 4, 11, 8, 5, 3, 13, 8, 8, 8, 13, 6, 2, 3]\n",
            "0 #### train ####\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-d80e75a9d669>:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-47-d80e75a9d669>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "repr, std, cov, clossl, z, norm 0.050199687480926514 0.2283935546875 3.4165687561035156 0.49619200825691223 1.3102490901947021 2.627256393432617\n",
            "repr, std, cov, clossl, z, norm 0.05339137092232704 0.2294921875 3.3471217155456543 0.4295380711555481 1.2595643997192383 2.339510202407837\n",
            "repr, std, cov, clossl, z, norm 0.04236960411071777 0.2298583984375 3.330925464630127 0.42237308621406555 1.278914213180542 2.836348533630371\n",
            "repr, std, cov, clossl, z, norm 0.05475496128201485 0.2296142578125 3.3375391960144043 0.4159761965274811 1.2828510999679565 2.350651264190674\n",
            "repr, std, cov, clossl, z, norm 0.04160648211836815 0.2293701171875 3.3557305335998535 0.45039573311805725 1.4270884990692139 2.6106982231140137\n",
            "repr, std, cov, clossl, z, norm 0.0648849755525589 0.2291259765625 3.3698673248291016 0.5053079724311829 1.3031196594238281 2.460367202758789\n",
            "repr, std, cov, clossl, z, norm 0.040094297379255295 0.2308349609375 3.2985994815826416 0.43139466643333435 1.2930095195770264 2.6224358081817627\n",
            "repr, std, cov, clossl, z, norm 0.05146126076579094 0.231201171875 3.274540424346924 0.4848814308643341 1.2869971990585327 2.209646701812744\n",
            "repr, std, cov, clossl, z, norm 0.040287576615810394 0.22607421875 3.504601001739502 0.4350287914276123 1.2827478647232056 2.573362350463867\n",
            "repr, std, cov, clossl, z, norm 0.04332108423113823 0.228515625 3.4079105854034424 0.4363424777984619 1.2271088361740112 2.102628231048584\n",
            "repr, std, cov, clossl, z, norm 0.030417054891586304 0.2265625 3.5241854190826416 0.40806660056114197 1.3688805103302002 2.1776013374328613\n",
            "repr, std, cov, clossl, z, norm 0.03740246966481209 0.2271728515625 3.4766132831573486 0.3913949728012085 1.2893091440200806 2.1527504920959473\n",
            "train_data.data 20482\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 4, 2, 2, 8, 4, 8, 14, 6, 8, 3, 3, 8, 8, 8, 4, 2, 8, 8, 0, 8, 9, 2, 2, 3, 8, 3, 8, 8, 14, 8, 2, 3, 3, 13, 4, 2, 3, 3, 0, 13, 8, 8, 4, 14, 2, 2, 6, 4, 8, 8, 8, 14, 6, 14, 2, 3, 3, 8, 3, 3, 3, 9, 3, 3, 3, 3, 4, 3, 5, 11, 13, 3, 3, 3, 3, 4, 2, 8, 5, 4, 3, 5, 11, 8, 4, 9, 13, 3, 8, 2, 3, 4, 8, 0, 14, 3, 5, 2, 14, 14, 4, 9, 8, 8, 3, 8, 0, 3, 3, 0, 6]\n",
            "dided\n",
            "time\n",
            "[6, 8, 2, 8, 14, 3, 3, 8, 4, 11, 9, 8, 3, 11, 6, 8, 3, 14, 8, 14]\n",
            "dided\n",
            "time\n",
            "[14, 8, 14, 8, 8, 13, 0, 8, 8, 9, 8, 6, 4, 3, 8, 2, 3, 8, 8, 8, 8, 13, 0, 2, 8, 11, 4, 8, 8, 3, 8, 8, 9, 8, 11, 14, 8, 3, 5, 8, 0, 8, 13, 11, 3, 8, 8, 14, 8, 3, 8, 2, 14, 8, 14, 4, 3, 8, 0, 13, 14, 3, 8, 3, 4, 4, 8, 8, 8, 14, 14, 11, 14, 0, 14, 3, 3, 11, 2, 9, 3, 4, 8, 11, 14, 2, 14, 4, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 4, 0, 8, 4, 3, 9, 5, 0, 4, 0, 3, 8, 10, 6, 14, 5, 11, 4, 4, 8, 0, 14, 2, 14, 13, 8, 2, 0, 8, 8, 8, 8, 14, 3, 8, 4, 8, 5, 8, 8, 3, 4, 4, 8, 14, 4, 8, 3]\n",
            "dided\n",
            "time\n",
            "[3, 6, 0, 8, 8, 4, 3, 2, 0, 2, 5, 8, 0, 3, 5, 5, 8, 8, 5, 2, 3, 8, 0, 8, 0, 14, 3, 8, 8, 8, 4, 13, 3, 4, 3, 2, 8, 5, 5, 8, 14, 8, 8, 8, 2, 5, 2, 8, 11, 13, 8, 14, 4, 8, 14, 8, 8, 11, 8, 11, 3, 8, 5, 13, 8, 3, 2, 3, 13, 8, 5, 5, 3, 13, 9, 8, 2, 14, 0, 13, 14, 4, 3, 2, 8, 2, 4, 0, 8, 4, 2, 5, 11, 3, 14, 3, 3, 4, 14, 8, 14, 3, 14, 3, 3, 14, 7, 8, 8, 4, 3, 8, 9, 14, 8, 8, 2, 8, 8, 8, 8, 3, 5, 4, 3, 3, 3, 9, 8, 9, 3, 4, 4, 8, 0, 8, 14, 8, 8, 13, 13, 14, 13, 14]\n",
            "1 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.030239468440413475 0.233642578125 3.158824920654297 0.37288448214530945 1.2757306098937988 2.152578115463257\n",
            "repr, std, cov, clossl, z, norm 0.03477417677640915 0.2314453125 3.265927791595459 0.46659520268440247 1.2961010932922363 1.7384819984436035\n",
            "repr, std, cov, clossl, z, norm 0.03046225570142269 0.2266845703125 3.5106117725372314 0.4347151815891266 1.3484346866607666 2.3537771701812744\n",
            "repr, std, cov, clossl, z, norm 0.04478132724761963 0.229736328125 3.3402657508850098 0.4005786180496216 1.2879937887191772 2.1635429859161377\n",
            "repr, std, cov, clossl, z, norm 0.029292959719896317 0.2308349609375 3.2981433868408203 0.4605409502983093 1.2740751504898071 1.8021148443222046\n",
            "repr, std, cov, clossl, z, norm 0.03841055557131767 0.22412109375 3.6301350593566895 0.5085026025772095 1.293576717376709 2.092989683151245\n",
            "repr, std, cov, clossl, z, norm 0.02933657541871071 0.2210693359375 3.804314613342285 0.41870516538619995 1.343421220779419 1.6218377351760864\n",
            "repr, std, cov, clossl, z, norm 0.03793816640973091 0.2293701171875 3.377295970916748 0.47742098569869995 1.2621781826019287 1.9240930080413818\n",
            "repr, std, cov, clossl, z, norm 0.031285952776670456 0.2401123046875 2.877171277999878 0.3724302649497986 1.2581950426101685 1.878142237663269\n",
            "repr, std, cov, clossl, z, norm 0.03870415687561035 0.2303466796875 3.3250019550323486 0.45123210549354553 1.2791039943695068 2.3636653423309326\n",
            "repr, std, cov, clossl, z, norm 0.027417577803134918 0.2257080078125 3.551722764968872 0.35597702860832214 1.2955002784729004 2.0038914680480957\n",
            "repr, std, cov, clossl, z, norm 0.03728378191590309 0.227294921875 3.4771690368652344 0.4645523726940155 1.319761037826538 2.1804027557373047\n",
            "train_data.data 20482\n",
            "dided\n",
            "time\n",
            "[14, 13, 14, 8, 14, 8, 3, 4, 8, 9, 3, 8, 5, 8, 4, 5, 4, 0, 13, 8, 4, 11, 8, 4, 8, 4, 4, 2, 0, 8, 3, 2, 8, 3, 0, 8, 11, 14, 14, 14, 14, 4, 3, 8, 11, 6, 3, 8, 8, 2, 11, 8, 8, 13, 5, 11, 8, 3, 14, 13, 8, 5, 14, 8, 13, 5, 8, 0, 6, 2, 8, 4, 0, 2, 14, 8, 8, 6, 8, 2, 14, 8, 8, 9, 4, 3, 3, 8, 8, 8, 3, 5, 0, 8, 8, 8, 9, 8, 5, 13, 3, 11, 4, 9, 5, 8, 9, 9, 3, 3, 2, 0, 4, 8, 8, 9, 13, 14, 14, 0, 0, 8, 8, 5, 8, 6, 8, 2, 8, 3, 8, 8, 5, 3, 8, 14, 14, 9, 3, 14, 5, 4, 0, 9, 2, 2, 8, 4, 3, 13, 8, 5, 6, 11, 13, 3, 8, 9, 4, 5, 14, 8, 8, 8, 8, 4, 2, 3, 3, 8, 0, 2, 8, 2, 3, 3, 14, 14, 2, 8, 8, 8, 0, 4, 11, 3, 0, 2, 4, 8, 8, 8, 8, 2, 3, 14, 14, 8, 2, 14, 8, 4, 14, 5, 2, 5, 14, 3, 0, 0, 3, 8, 14, 8, 3, 8, 8, 8, 14, 8, 8, 8, 4, 3, 2, 8, 14, 5, 4, 0, 0, 0, 3, 2, 8, 14, 5, 4, 8, 8, 14, 0, 14, 8, 8, 3, 6, 8, 2, 2, 10, 3, 0, 2, 8, 4, 4, 5, 4, 14, 8, 8, 5, 3, 0, 3, 8, 4, 11, 2, 8, 8, 8, 8, 8, 14, 14, 4, 8, 8, 3, 8, 3, 3, 4, 5, 8, 8, 0, 5, 4, 8, 8, 6, 13, 3, 5, 8, 14, 8, 9, 8, 4, 4, 8, 14, 5, 8, 13, 8, 14, 13, 7, 8, 2, 0, 0, 4, 5, 3, 4, 3, 8, 3, 2, 5, 8, 3, 14, 8, 4, 3, 2, 4, 14, 0, 3, 0, 4, 0, 6, 8, 9, 8, 8, 8, 2, 2, 0, 8, 3, 8, 3, 3, 14, 14, 2, 3, 13, 8, 5, 8, 8, 3, 8, 9, 11, 8, 8, 3, 13, 0, 3, 8, 3, 8, 2, 13, 2, 0, 4, 0, 8, 3, 4, 3, 3, 2, 14, 8, 0, 9, 3, 5, 8, 8, 8, 14, 4, 5, 10, 2, 2, 14, 8, 2, 14, 9, 5, 8, 8, 0, 8, 8, 8, 2, 8, 11, 8, 4, 2, 5, 3, 3, 8, 8, 8, 0, 4, 3, 11, 14, 4, 3, 3, 0, 6, 5, 4, 4, 4, 6, 4, 4, 3, 14, 14, 13, 4, 2, 14, 8, 0, 5, 11, 8, 3, 11, 8, 2, 13, 9, 14, 8, 5, 3, 8, 8, 9, 9, 4, 8, 8, 9, 8, 0, 3, 14, 13, 8, 11, 11, 5, 3, 5, 6, 8, 8, 2, 11, 8, 4, 13, 4, 3, 3, 6, 14, 3, 8, 8, 2, 4, 3, 14, 8, 11, 8, 4, 8, 14, 3, 8, 4, 9, 8, 5, 3, 13, 0, 3, 11, 5, 2, 2, 9, 3, 8, 5, 3, 8, 8, 8, 5, 8, 3, 4, 8, 8, 3, 8, 0, 3, 4, 3, 9, 13, 4, 14, 8, 2, 6, 4, 8, 9, 3, 3, 2, 2, 4, 2, 0, 14, 3, 14, 0, 8, 4, 2, 8, 3, 8, 8, 8, 4, 6, 11, 5, 3, 2, 14, 3, 3, 8, 3, 3, 8, 0, 3, 14, 3, 3, 8, 4, 3, 13, 4, 2, 3, 4, 9, 11, 2, 5, 11, 14, 8, 0, 0, 3, 4, 14, 14, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 14, 8, 8, 4, 3, 4, 8, 8, 9, 8, 8, 13, 13, 8, 14, 3, 2, 8, 2, 8, 3, 3, 11]\n",
            "dided\n",
            "time\n",
            "[9, 14, 5, 8, 0, 14, 5, 4, 4, 3, 8, 8, 8, 3, 8, 0, 9, 8, 3, 8, 8, 3, 8, 8, 13, 0, 2, 8, 9, 5, 14, 8, 2, 11, 14, 8, 8, 2, 8, 8, 14, 0, 2, 8, 8, 14, 0, 5, 8, 14, 5, 4, 2, 14, 0, 11, 3, 8, 8, 6, 0, 11, 3, 9, 3, 8, 9, 2, 8, 13, 4, 11, 3, 5, 8, 8, 8, 3, 0, 8, 14, 3, 8, 2, 6, 3, 13, 2, 8]\n",
            "dided\n",
            "time\n",
            "[8, 13, 4, 14, 2, 8, 11, 3, 2, 3, 8, 4, 3, 3, 3, 0, 0, 13, 2, 13, 2, 5, 8, 3, 14, 2, 3, 8, 2, 0, 12, 8, 4, 11, 3, 2, 8, 5, 8, 4, 9, 5]\n",
            "dided\n",
            "time\n",
            "[5, 2, 11, 3, 11, 14, 3, 3, 2, 3, 6, 11, 3, 8, 8, 8, 14, 3, 14, 14, 2, 8, 8, 8, 14, 2, 14, 2, 8, 3, 0, 3, 8, 2, 13, 4, 9, 14, 8, 2, 8, 3, 8, 14, 8, 4, 8, 3, 5, 0, 14, 5, 9, 8, 14, 8, 8, 0, 11, 14, 3, 4, 2, 8, 4, 5, 5, 10, 3, 3, 11, 8, 8, 8, 8, 13, 5, 14, 9, 1, 14, 3, 8, 2, 6, 0, 8, 8, 8, 8, 5, 4, 11, 3, 11, 8, 8, 8, 3, 8, 8, 13, 0, 3, 3, 8, 8, 0, 4, 8, 9, 8, 8, 4, 5, 14, 11, 5, 14, 8, 3, 8, 9, 4, 8, 9, 11, 2, 8, 9, 8, 8, 0, 2, 0, 0, 0, 14, 5, 14, 8, 8, 14, 3, 9, 0, 3, 8, 4, 11, 14, 5, 8, 8, 9, 14, 8, 0, 13, 8, 5, 4, 2, 3, 8, 0, 3, 0, 0, 0, 8, 8, 0, 0, 3, 8, 0, 2, 8, 9, 4, 8, 4, 8, 5, 14, 8, 14, 3, 3, 8, 5, 4, 8, 13, 0, 4, 2, 4, 2, 14, 0, 8, 14, 4, 0, 2, 8, 11, 0, 6, 5, 2, 8, 2, 8, 2, 3, 2, 8, 8, 13, 8, 3, 14, 14, 4, 14, 3, 0, 0, 14, 11, 8, 3, 0, 4, 3, 2, 3, 3, 8, 3, 2, 5, 2, 8, 3, 4, 8, 2, 2, 2, 8, 5, 8, 14, 2, 3, 5, 5, 8, 8, 2, 2, 14, 3, 3, 11, 3, 3, 4, 4, 13, 8, 11, 2, 6, 3, 14, 3, 9, 11, 3, 4, 8, 8, 3, 3, 14, 8, 8, 2, 11, 8, 4, 14, 8, 6, 8, 0, 11, 3, 14, 3, 8, 3, 8, 13, 0, 8, 8, 5, 6, 14, 4, 4, 3, 8, 0, 5, 14, 14, 14, 8, 2, 5, 14, 4, 9, 8, 8, 11, 14, 8, 8, 5, 4, 8, 8, 8, 8, 2, 4, 8, 14, 6, 0, 14, 9, 3, 8, 6, 3, 8, 2, 5, 8, 8, 8, 5, 14, 8, 8, 11, 2, 8, 0, 8, 3, 2, 14, 8, 8, 11, 8, 3, 3, 8, 2, 5, 4, 4, 8, 14, 0, 8, 13, 8, 2, 3, 2, 3, 13, 8, 2, 4, 5, 14, 0, 8, 8, 5, 8, 14, 2, 3, 8, 0, 5, 2, 8, 8, 8, 8, 4, 6, 3, 0, 8, 3, 3, 3, 4, 8, 4, 14, 8, 3, 3, 11, 8, 2, 0, 8, 0, 5, 8, 8, 14, 8, 3, 4, 14, 0, 8, 8, 4, 13, 2, 8, 9, 3, 8, 5, 4, 14, 4, 8, 0, 11, 14, 13, 8, 8, 6, 7, 4, 8, 8, 8, 8, 0, 3, 2, 8, 8, 3, 4, 14, 2, 8, 3, 8, 9, 6, 6, 14, 8, 3, 8, 3, 8, 14, 14, 8, 11, 11, 4, 8, 2, 5, 2, 8, 3, 14, 13, 8, 14, 14, 14, 2, 2, 13, 3, 4, 13, 14, 3, 5, 4, 9, 8, 0, 5, 4, 8, 3, 4, 3, 4, 3, 8, 2, 14, 3, 5, 2, 4, 8, 5, 2, 2, 5, 9, 0, 6, 13, 0, 2, 4, 3, 8, 4, 4, 4, 4, 9, 8, 0, 5, 3, 2, 8, 4, 8, 8, 3, 8, 14, 14, 3, 2, 8, 5, 3, 8, 4, 8, 8, 5, 8, 8, 3, 11, 4, 4, 3, 13, 0, 4, 8, 8, 8, 2, 4, 11, 2, 2, 8, 2, 8, 11, 8, 9, 3, 11, 8, 0, 8, 0, 14, 8, 8, 14, 8, 7, 2, 3, 8, 8, 0, 14, 5, 6, 4, 8, 3, 8, 4, 3, 11, 10, 8, 0, 8, 8, 8, 8, 8, 3, 5, 14, 4, 13, 8, 2, 14, 4, 8, 8, 8, 8, 8, 3, 4, 4, 11, 2, 8, 14, 8, 4, 0, 8, 14, 8, 4, 3, 14, 3, 8, 8, 4, 3, 0, 8, 3, 14, 4, 13, 0, 0]\n",
            "2 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02485060878098011 0.2252197265625 3.570621967315674 0.41615286469459534 1.2404062747955322 2.3992795944213867\n",
            "repr, std, cov, clossl, z, norm 0.034216366708278656 0.2286376953125 3.384977340698242 0.4774250388145447 1.2245571613311768 2.2174956798553467\n",
            "repr, std, cov, clossl, z, norm 0.026624483987689018 0.229248046875 3.360844612121582 0.5352165699005127 1.3333603143692017 2.3448619842529297\n",
            "repr, std, cov, clossl, z, norm 0.03183343634009361 0.2344970703125 3.1166858673095703 0.5451011657714844 1.242904782295227 2.495314121246338\n",
            "repr, std, cov, clossl, z, norm 0.02659958228468895 0.234375 3.123640298843384 0.4627368748188019 1.2942672967910767 1.9189338684082031\n",
            "repr, std, cov, clossl, z, norm 0.03252717852592468 0.2275390625 3.4733314514160156 0.4439626634120941 1.2708508968353271 1.9212883710861206\n",
            "repr, std, cov, clossl, z, norm 0.025751007720828056 0.2264404296875 3.5179672241210938 0.4248899221420288 1.2907782793045044 2.0556976795196533\n",
            "repr, std, cov, clossl, z, norm 0.03526955470442772 0.2283935546875 3.4046435356140137 0.45062950253486633 1.300011157989502 1.7587661743164062\n",
            "repr, std, cov, clossl, z, norm 0.024946225807070732 0.2294921875 3.3561367988586426 0.4758056700229645 1.2846043109893799 2.214874505996704\n",
            "repr, std, cov, clossl, z, norm 0.03324524313211441 0.229736328125 3.3520689010620117 0.44731205701828003 1.2911038398742676 2.0093228816986084\n",
            "repr, std, cov, clossl, z, norm 0.027985719963908195 0.229736328125 3.3436312675476074 0.40503257513046265 1.2876981496810913 2.3253231048583984\n",
            "repr, std, cov, clossl, z, norm 0.03943905979394913 0.225830078125 3.5496597290039062 0.47621434926986694 1.265323281288147 2.156860589981079\n",
            "train_data.data 20466\n",
            "dided\n",
            "time\n",
            "[0, 0, 2, 3, 8, 2, 3, 3, 8, 8, 8, 4, 3, 8, 3, 13, 2, 4, 2, 0, 9, 8, 3, 3, 9, 8, 8, 14, 14, 0, 3, 2, 4, 4, 6, 8, 2, 9, 3, 8, 9, 9, 12, 14, 0, 5, 3, 8, 2, 11, 2, 8, 0, 0, 4, 14, 4, 3, 3, 4, 6, 5, 1, 9, 9, 8, 5, 14, 4, 8, 3, 8, 8, 5, 2, 8, 9, 13, 4, 8, 3, 3, 8, 8, 4, 5, 8, 8, 9, 3, 4, 3, 9, 4, 8, 0, 9, 9, 8, 3, 0, 8, 3, 14, 4, 3, 2, 3, 9, 14, 8, 8, 6, 11, 10, 12, 8, 8, 5, 8, 9, 11, 11, 13, 9, 3, 5, 5, 6, 9, 12, 4, 8, 8, 3, 0, 0, 8, 4, 4, 9, 5, 4, 2, 8, 13, 0, 5, 6, 10, 12, 12, 9, 2, 5, 14, 9, 9, 12, 12, 3, 9, 9, 14, 9, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 0, 2, 6, 4, 12, 12, 6, 11, 12, 11, 9, 4, 14, 14, 3, 9, 10, 1, 6, 12, 1, 12, 9, 3, 13, 2, 4, 3, 5, 11, 9, 8, 14, 9, 9, 8, 2]\n",
            "dided\n",
            "time\n",
            "[3, 9, 14, 12, 9, 9, 4, 14, 4, 9, 3, 8, 4, 8, 5, 8, 9, 4, 8]\n",
            "dided\n",
            "time\n",
            "[4, 8, 8, 4, 9, 3, 8, 14, 9, 8, 13, 14, 6, 12, 1, 10, 8, 8, 0, 8, 6, 11, 12, 12, 4, 14, 11, 3, 9, 8, 2, 8, 8, 3, 8, 8, 8, 8, 2, 13, 9, 3, 8, 5, 2, 3, 5, 14, 8, 8, 3, 14, 2, 4, 8, 0, 3, 9, 8, 3, 9, 14, 8, 5, 8, 13, 3, 3, 14, 9, 8, 0, 3, 8, 8, 9, 8, 11, 8, 13, 5, 6, 3, 3, 8, 8, 9, 8, 4, 9, 8, 2, 8, 8, 11, 3, 6, 12, 10, 12, 8, 8, 5, 14, 9, 9, 11, 0, 8, 3, 4, 3, 6, 0, 9, 3, 8, 2, 2, 2, 9, 9, 12, 12, 6, 9, 12, 12, 9, 4, 3, 4, 4, 5, 14, 3, 9, 2, 11, 2, 11, 8, 3, 3, 4, 4, 8, 8, 0, 8, 3, 3, 14, 3, 14, 9, 9, 3, 8, 6, 6, 11, 12, 8, 0, 8, 0, 8, 14, 8, 3, 3, 13, 9, 9, 5, 3, 3, 2, 14, 9, 14, 3, 8, 9, 9, 8, 8, 8, 3]\n",
            "dided\n",
            "time\n",
            "[4, 3, 3, 14, 9, 9, 8, 2, 4, 8, 8, 8, 4, 8, 8, 2, 2, 14, 8, 8, 3, 3, 8, 8, 9, 4, 9, 4, 9, 14, 2, 5, 9, 2, 3, 5, 6, 11, 12, 12, 8, 3, 8, 0, 9, 14, 4, 8, 8, 13, 4, 8, 0, 8, 14, 8, 6, 1, 12, 10, 11, 3, 8, 6, 0, 3, 0, 3, 6, 4, 12, 12, 4, 8, 11, 14, 13, 14, 9, 8, 9, 3, 2, 3, 6, 9, 12, 12, 2, 0, 8, 0, 4, 8, 9, 8, 6, 1, 1, 10, 9, 4, 12, 12, 9, 6, 2, 3, 3, 3, 4, 0, 0, 4, 0, 8, 8, 3, 8, 8, 4, 11, 8, 8, 0, 3, 8, 8, 9, 3, 13, 9, 6, 9, 11, 4, 9, 3, 8, 3, 8, 11, 13, 4, 9, 8, 8, 14, 14, 13, 8, 4, 0, 13, 11, 2, 4, 2, 8, 4, 3, 8, 9, 4, 9, 11, 5, 11, 2, 8, 3, 9, 9, 9, 11, 6, 3, 3, 4, 2, 8, 14, 4, 8, 3, 9, 5, 14, 4, 3, 0, 14, 8, 2, 4, 3, 9, 6, 14, 3, 4, 8, 8, 14, 4, 4, 11, 2, 9, 14, 8, 3, 9, 8, 11, 6, 9, 9, 8]\n",
            "3 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.028400344774127007 0.2269287109375 3.486931085586548 0.43777284026145935 1.3294031620025635 2.2941055297851562\n",
            "repr, std, cov, clossl, z, norm 0.03544149175286293 0.2296142578125 3.352665901184082 0.46403688192367554 1.2812387943267822 2.236327648162842\n",
            "repr, std, cov, clossl, z, norm 0.030852526426315308 0.23046875 3.2988719940185547 0.4497179687023163 1.2915910482406616 2.0718281269073486\n",
            "repr, std, cov, clossl, z, norm 0.044823843985795975 0.2337646484375 3.142090320587158 0.45316165685653687 1.2850888967514038 1.9388556480407715\n",
            "repr, std, cov, clossl, z, norm 0.030817998573184013 0.22998046875 3.32828426361084 0.4037620425224304 1.2815935611724854 2.031543254852295\n",
            "repr, std, cov, clossl, z, norm 0.04572347179055214 0.2249755859375 3.5834121704101562 0.49382033944129944 1.3481073379516602 1.7467930316925049\n",
            "repr, std, cov, clossl, z, norm 0.03056401200592518 0.2252197265625 3.571194648742676 0.5417723655700684 1.2204015254974365 1.8776277303695679\n",
            "repr, std, cov, clossl, z, norm 0.04324404522776604 0.231201171875 3.266833782196045 0.4957757592201233 1.3077532052993774 1.773760199546814\n",
            "repr, std, cov, clossl, z, norm 0.030661718919873238 0.228515625 3.413473129272461 0.439273864030838 1.335067868232727 2.3735575675964355\n",
            "repr, std, cov, clossl, z, norm 0.03359704092144966 0.23046875 3.2964115142822266 0.4832916855812073 1.292873501777649 2.089559316635132\n",
            "repr, std, cov, clossl, z, norm 0.028415238484740257 0.2281494140625 3.4093284606933594 0.48129555583000183 1.286649465560913 2.1321864128112793\n",
            "repr, std, cov, clossl, z, norm 0.033155038952827454 0.2286376953125 3.383457660675049 0.4053097069263458 1.3547136783599854 1.5568451881408691\n",
            "train_data.data 20630\n",
            "dided\n",
            "time\n",
            "[9, 9, 8, 13, 11, 8, 3, 4, 3, 0, 8, 3, 8, 3, 4, 7, 3, 3, 2, 8, 3, 2, 11, 2, 14, 8, 9, 3, 4, 6, 13, 14, 0, 3, 3, 8, 8, 2, 11, 14, 8, 8, 3, 8, 14, 14, 6, 4, 8, 0, 4, 6, 3, 3, 4, 3, 11, 5, 2, 3, 3, 4, 6, 2, 8, 8, 8, 3, 8, 11, 13, 14, 8, 3, 3, 3, 3, 3, 8, 8, 2, 13, 3, 14, 3]\n",
            "dided\n",
            "time\n",
            "[8, 8, 9, 0, 8, 8, 8, 9, 4, 2, 13, 3, 14, 14, 9, 8, 4, 2, 3, 9, 8, 11, 14, 3, 8, 11, 3, 3, 8, 11, 8, 3, 8, 2, 0, 2, 13, 2, 4, 2, 8, 14, 8, 8, 14, 8, 4, 8, 14, 3, 4, 3, 2, 8, 8, 4, 2, 3, 3, 8, 8, 8, 14, 5, 2, 8, 2, 5, 14, 2, 11, 4, 8, 8, 3, 3, 8, 2, 4, 11, 9, 4, 8, 11, 2, 14, 0, 8, 3, 5, 14, 11, 8, 9, 3, 5, 13, 2, 5, 0, 8, 4, 3, 8, 8, 14, 0, 3, 8, 14, 3, 3, 4, 3, 11, 2, 4, 14, 2, 8, 2, 11, 3, 14, 4, 4, 11, 8, 8, 9, 11, 5, 3, 8, 3, 3, 3, 5, 8, 2, 14, 8, 14, 14, 4, 14, 5, 5, 2, 8, 13, 8, 4, 9, 14, 3, 3, 14, 3, 3, 5, 4, 8, 4, 10, 0, 4, 5, 14, 4, 11, 0, 3, 3, 5, 5, 13, 14, 4, 13, 2, 3, 0, 3, 14, 8, 13, 5, 7, 3, 4, 8, 3, 0, 3, 2, 14, 3, 3, 5, 2, 14, 8, 8, 3, 2, 8, 2, 2, 14, 6, 3, 0, 5, 3, 13, 2, 3, 9, 8, 4, 3, 4, 8, 9, 3, 8, 8, 8, 0, 14, 9, 14, 0, 8, 4, 8, 4, 0, 8, 8, 8, 14, 2, 2, 5, 13, 0, 13, 14, 8, 0, 5, 11, 3, 2, 4, 9, 4, 2, 3, 8, 8, 8, 4, 3, 8, 8, 8, 14, 3, 8, 14, 14, 3, 8, 4, 6, 8, 8, 8, 6, 13, 0, 8, 3, 3, 14, 8, 14, 3, 2, 0, 8, 8, 14, 4, 8, 0, 11, 0, 14, 4, 14, 8, 9, 9, 8, 3, 8, 11, 8, 4, 3, 9, 2, 13, 3, 5, 14, 5, 5, 0, 0, 13, 5, 2, 3, 5, 4, 2, 0, 5, 3, 11, 8, 13, 0, 3, 4, 5, 13, 8, 2, 4, 3, 11, 14, 14, 9, 2, 8, 3, 5, 3, 9, 8, 14, 11, 3, 3, 6, 6, 3, 9, 5, 8, 8, 11, 8, 3, 11, 8, 0, 2, 8, 8, 14, 3, 4, 5, 8, 4, 8, 6, 0, 0, 13, 9, 8, 8, 5, 8, 8, 5, 13, 9, 8, 8, 13, 8, 8, 3, 8, 4, 14, 2, 9, 8, 14, 13, 2, 8, 0, 4, 11, 14, 14, 5, 5, 2, 0, 4, 2, 0, 13, 8, 5, 2, 3, 3, 8, 14, 9, 11, 8, 4, 5, 3, 8, 8, 3, 4, 11, 8, 2, 4, 5, 3, 11, 14, 8, 2, 2, 0, 8, 2, 6, 0, 8, 14, 14, 0, 2, 9, 4, 8, 0, 4, 4, 14, 8, 8, 8, 11, 14, 2, 10, 3, 8, 8, 14, 9, 14, 8, 8, 8, 8, 7, 2, 4, 8, 5, 8, 8, 14, 3, 8, 3, 8, 5, 0, 3, 14, 13, 3, 3, 3, 5, 11, 8, 4, 8, 8, 6, 2, 5, 8, 3, 8, 5, 13, 8]\n",
            "dided\n",
            "time\n",
            "[13, 8, 8, 9, 8, 5, 4, 4, 13, 3, 3, 3, 8, 2, 8, 5, 9, 5, 9, 0, 3, 3, 8, 11, 8, 8, 3, 3, 5, 3, 4, 3, 8, 11, 2, 8, 4, 8, 13, 8, 3, 8, 3, 3, 7, 14, 3, 3, 8, 0, 3, 8, 14, 8, 0, 8, 8, 3, 6, 3, 14, 14, 3, 3, 3, 3, 0, 4, 14, 0, 8, 14, 9, 13, 9, 8, 5, 4, 8, 0, 8]\n",
            "dided\n",
            "time\n",
            "[14, 11, 9, 14, 4, 2, 4, 3, 8, 3, 11, 9, 3, 14, 0, 5, 8, 13, 8, 0, 14, 13, 3, 8, 4, 8, 14, 9, 5, 5, 9, 5, 4, 8, 11, 8, 0, 8, 2, 8, 13, 8, 8, 8, 8, 8, 8, 8, 11, 3, 2, 3, 4, 13, 8, 8, 9, 3, 9, 14, 8, 0, 5, 8, 3, 2, 2, 14, 2, 8, 9, 8, 2, 4, 8, 8, 14, 11, 14, 14, 8, 3, 3, 4, 0, 4, 8, 8, 13, 3, 14, 8, 8, 14, 8]\n",
            "dided\n",
            "time\n",
            "[14, 8, 8, 2, 8, 13, 3, 3, 2, 8, 3, 2, 2, 8, 2, 8, 6, 2, 0, 0, 8, 3, 4, 2, 3, 8, 3, 8, 2, 3, 4, 8, 3, 3, 8, 13, 5, 4, 8, 5]\n",
            "4 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.0311942957341671 0.2257080078125 3.5400681495666504 0.3710521161556244 1.313453197479248 2.2909963130950928\n",
            "repr, std, cov, clossl, z, norm 0.034628208726644516 0.228271484375 3.4176981449127197 0.47709402441978455 1.406011939048767 1.9091883897781372\n",
            "repr, std, cov, clossl, z, norm 0.026986509561538696 0.2349853515625 3.0763564109802246 0.4736819863319397 1.3321717977523804 1.5894745588302612\n",
            "repr, std, cov, clossl, z, norm 0.02898566424846649 0.23046875 3.2977192401885986 0.44847533106803894 1.3652222156524658 1.703845500946045\n",
            "repr, std, cov, clossl, z, norm 0.023403799161314964 0.2257080078125 3.5274717807769775 0.5070616602897644 1.333919644355774 1.4394172430038452\n",
            "repr, std, cov, clossl, z, norm 0.02746584638953209 0.2275390625 3.4510254859924316 0.4651363790035248 1.3729664087295532 1.8376718759536743\n",
            "repr, std, cov, clossl, z, norm 0.031055893748998642 0.2303466796875 3.312014579772949 0.3948083221912384 1.2820547819137573 1.4970263242721558\n",
            "repr, std, cov, clossl, z, norm 0.0325605683028698 0.229248046875 3.3555150032043457 0.5294002890586853 1.302607774734497 1.6181520223617554\n",
            "repr, std, cov, clossl, z, norm 0.029630431905388832 0.223876953125 3.6282968521118164 0.46388307213783264 1.3106380701065063 1.807602047920227\n",
            "repr, std, cov, clossl, z, norm 0.039077166467905045 0.2232666015625 3.691403865814209 0.4701252579689026 1.3213098049163818 1.6997398138046265\n",
            "repr, std, cov, clossl, z, norm 0.027829106897115707 0.2257080078125 3.5676097869873047 0.4553568363189697 1.3319454193115234 1.880733609199524\n",
            "repr, std, cov, clossl, z, norm 0.0342973954975605 0.2310791015625 3.2725796699523926 0.395526260137558 1.2677297592163086 2.1700873374938965\n",
            "train_data.data 20276\n",
            "dided\n",
            "time\n",
            "[4, 8, 5, 8, 14, 4, 8, 4, 2, 8, 8, 4, 12, 3, 2, 3, 8, 5, 8, 7, 8, 8, 8, 6, 14, 11, 8, 5, 8, 14, 8, 0, 14, 8, 9, 6, 2]\n",
            "dided\n",
            "time\n",
            "[5, 9, 8, 14, 0, 11, 4, 9, 8, 3, 8, 13, 0, 8, 13, 13, 11, 3, 9, 8, 9, 8, 0, 4, 11, 9, 4, 9, 0, 2, 6, 3, 9, 4, 8, 9, 4, 5, 8, 8, 8, 2, 4, 8, 14, 8, 12, 5, 3, 3, 3, 8, 8, 14, 3, 3, 2, 4, 8, 4, 8, 8, 3, 3, 14, 2, 9, 8, 2, 8, 3, 8, 4, 2, 8, 8, 3, 4, 3, 5, 8, 0, 2, 11, 8, 8, 8, 0, 14, 8, 14, 0, 3, 14, 3, 3, 0, 0, 4, 5, 0, 8, 8, 8, 8, 4, 9, 8, 3, 3, 4, 11, 9, 4, 4, 14, 8, 13, 4, 3, 2, 4, 4, 8, 4, 6, 8, 2, 14, 8, 3, 5, 8, 14, 5, 8, 5, 8, 0, 3, 5, 8, 3, 8, 0, 2, 5, 4, 0, 8, 14, 3, 2, 9, 3, 3, 3, 13, 8, 13, 8, 14, 8, 2, 9]\n",
            "dided\n",
            "time\n",
            "[2, 8, 14, 0, 3, 8, 11, 0, 2, 2, 3, 3, 3, 2, 8, 4, 3, 3, 0, 8, 8, 8, 3, 0, 5, 8, 8, 4, 9, 14, 4, 4, 8, 8, 3, 2, 13, 0, 8, 2, 3, 4, 8, 4, 5, 8, 14, 13, 5, 8, 8, 0, 8, 0, 4, 2, 9, 2, 0, 8, 3, 8, 3, 8, 8, 8, 3, 11, 0, 3, 14, 5, 4, 2, 0, 8, 4, 2, 0, 14, 3, 14, 3, 3, 3, 2, 9, 0, 3, 14, 2, 3, 8, 0, 8, 5, 3, 2, 5, 14, 3, 5, 8, 3, 8, 0, 8, 4, 8, 8, 0, 3, 8, 8, 2, 14, 14, 2, 3, 3, 0, 14, 8, 2, 3, 3, 5, 3, 4, 2, 8, 5, 3, 0, 5, 5, 0, 11, 3, 0, 13, 0, 14, 8, 4, 0, 11, 5, 7, 14, 8, 13, 13, 9, 4, 14, 8, 14, 3, 3, 5, 8, 3, 14, 3, 5, 4, 3, 2, 8, 8, 14, 3, 2, 8, 4, 0, 3, 8, 13, 8, 14, 8, 4, 4, 8, 2, 8, 7, 13, 3, 12, 14, 3, 8, 4, 3, 0, 13, 3, 8, 14, 8, 3, 8, 3, 2, 0, 6, 8, 3, 8, 8, 14, 8, 14, 8, 3, 9, 4, 3, 8, 3, 3, 3, 8, 8, 8, 14, 14, 14, 4, 8, 8, 8, 8, 11, 3, 4, 4, 11, 8, 2, 9, 8, 4, 3, 11, 8, 4, 4, 8, 8, 8, 3, 4, 11, 3, 8, 8, 4, 8, 8, 5, 4, 10, 3, 8, 2, 14, 3, 8, 3, 8, 3, 8, 0, 5, 14, 0, 5, 9, 3, 2, 8, 3, 4, 8, 9, 14, 8, 11, 2, 8, 13, 14, 4, 0, 3, 5, 13, 0, 2, 11, 8, 8, 3, 0, 3, 2, 8, 8, 2, 8, 8, 3, 4, 3, 2, 13, 3, 8, 8, 0, 4, 13, 3, 5, 9, 6, 8, 8, 8, 8, 8, 3, 2, 13, 3, 14, 2, 14, 5, 4, 3, 4, 3, 13, 3, 14, 3, 9, 2, 3, 8, 3, 8, 8, 2, 3, 8, 11, 13, 5, 10, 8, 8, 0, 11, 8, 3, 5, 8, 3, 0, 8, 2, 8, 11, 2, 4, 4, 8, 8, 4, 3, 2, 3, 2, 9, 4, 5, 0, 4, 3, 8, 8, 8, 3, 3, 2, 2, 11, 14, 3, 2, 4, 2, 8, 3, 8, 2, 0, 9, 8, 4, 11, 11, 4, 8, 4, 8, 3, 8, 14, 8, 4, 8, 5, 8, 4, 8, 3, 13, 11, 8, 13, 5, 14, 14, 4, 4, 8, 2, 0, 8, 4, 8, 8, 8, 3, 9, 14, 0, 2, 2, 8, 3, 8, 8, 5, 8, 9, 4, 8, 4, 9, 3, 0, 8, 8, 8, 3, 8, 3, 6, 3, 8, 3, 0, 11, 9, 14, 2, 4, 14, 11, 8, 14, 2, 13, 8, 2, 0, 3, 0, 8, 3, 9, 14, 7, 4, 11, 5, 2, 2, 8, 0, 8, 5, 10, 14, 5, 8, 3, 8, 9, 3, 8, 3, 8, 8, 8, 6, 8, 8, 8, 2, 4, 3, 14, 8, 4, 5, 14, 0, 3, 3, 14, 4, 14, 8, 3, 14, 2, 8, 14, 2, 8, 3, 5, 5, 8, 2, 4, 8, 9, 14, 5, 6, 2, 9, 3, 8, 8, 5, 8, 13, 8, 8, 5, 3, 14, 3, 5, 14, 3, 3, 8, 8, 3, 2, 2, 3, 9, 4, 8, 9, 14, 8, 0, 13, 2, 8, 8, 6, 14, 3, 3, 14, 8, 8, 2, 4, 8, 11, 3, 13, 3, 8, 0, 3, 11, 8, 8, 8, 5, 8, 8, 14, 8, 4, 14, 9, 14, 3, 4, 8, 10, 3, 9, 4, 2]\n",
            "dided\n",
            "time\n",
            "[8, 3, 14, 3, 4, 0, 4, 14, 5, 14, 3, 5, 2, 14, 8, 8, 8, 8, 3, 0, 14, 0, 14, 8, 4, 9, 8, 5, 2, 3, 8, 0, 13, 8, 8, 6, 3, 8, 8, 6, 14, 6, 14, 8, 2, 5, 5, 5, 3, 0, 9, 6, 8, 8, 8, 8, 0, 0, 8, 3, 3, 9, 4, 11, 5, 11, 8, 14, 2, 8, 14, 14, 2, 3, 8, 4, 6, 9, 4, 8, 3, 3, 14]\n",
            "dided\n",
            "time\n",
            "[3, 14, 11, 4, 5, 11, 4, 8, 5, 3, 3, 8, 5, 4, 8, 4, 11, 3, 8, 3, 8, 5, 8, 3, 8, 0, 11, 6, 8, 8, 8, 8, 3, 3, 13, 11, 5, 8, 8, 4, 8, 9, 8, 4, 5, 4, 2, 13, 9, 3, 8, 6, 4, 4, 2, 5, 8, 2, 8, 8]\n",
            "5 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.025653153657913208 0.23046875 3.3082075119018555 0.4155210852622986 1.3001651763916016 2.130786657333374\n",
            "repr, std, cov, clossl, z, norm 0.027127452194690704 0.2308349609375 3.2883830070495605 0.4032193124294281 1.3106248378753662 2.2478461265563965\n",
            "repr, std, cov, clossl, z, norm 0.02475205808877945 0.227783203125 3.451923370361328 0.41745004057884216 1.3748835325241089 2.457810401916504\n",
            "repr, std, cov, clossl, z, norm 0.02816041372716427 0.2264404296875 3.5183775424957275 0.410548597574234 1.294135332107544 2.136397123336792\n",
            "repr, std, cov, clossl, z, norm 0.029010510072112083 0.226318359375 3.509275436401367 0.43223094940185547 1.30239999294281 2.283470630645752\n",
            "repr, std, cov, clossl, z, norm 0.02462456002831459 0.228515625 3.3926775455474854 0.4781641364097595 1.2572433948516846 2.2518720626831055\n",
            "repr, std, cov, clossl, z, norm 0.025441214442253113 0.2296142578125 3.3522233963012695 0.4748712182044983 1.2501469850540161 2.338423013687134\n",
            "repr, std, cov, clossl, z, norm 0.03299768269062042 0.22900390625 3.38329815864563 0.5019852519035339 1.285364031791687 2.3010568618774414\n",
            "repr, std, cov, clossl, z, norm 0.0250054020434618 0.2305908203125 3.32021427154541 0.4333431124687195 1.302261471748352 2.5980048179626465\n",
            "repr, std, cov, clossl, z, norm 0.02971210703253746 0.2296142578125 3.3685755729675293 0.5492796301841736 1.3319751024246216 1.7825355529785156\n",
            "repr, std, cov, clossl, z, norm 0.025565313175320625 0.2314453125 3.272334337234497 0.4548315703868866 1.3447856903076172 2.063746452331543\n",
            "repr, std, cov, clossl, z, norm 0.03279685229063034 0.2313232421875 3.2709717750549316 0.4525006413459778 1.2465431690216064 1.9043481349945068\n",
            "train_data.data 20459\n",
            "dided\n",
            "time\n",
            "[2, 8, 8, 4, 3, 13, 2, 8, 14, 5, 3, 9, 3, 13, 3, 4, 9, 3, 4, 4, 2, 8, 3, 8, 14, 8, 3, 2, 3, 8, 5, 5, 3, 3, 8, 9, 0, 14, 2, 0, 8, 0, 0, 4, 8, 3, 3, 9, 8, 4, 11, 5, 8, 11, 3, 11, 9, 0, 0, 3, 2, 3, 0, 5, 8, 9, 8, 3, 3, 13, 3, 4, 5, 3, 6, 11, 8, 8, 13, 8]\n",
            "dided\n",
            "time\n",
            "[8, 13, 8, 4, 8, 3, 5, 8, 3, 8, 4, 8, 5, 2, 11, 8, 2, 2, 0, 8, 3, 8, 3, 0, 3, 14, 14, 5, 2, 6, 3, 5, 8, 13, 8, 4, 2, 8, 11, 11, 8, 8, 6, 3, 2, 8, 0, 2, 0, 11, 4, 11, 3, 8, 8, 8, 8, 3, 3, 5, 6, 2, 11, 4, 4, 8, 8, 14, 9, 2, 8, 11, 2, 2, 8, 8, 5, 0, 0, 8, 5, 4, 8, 4, 4, 0, 5, 3, 8, 3, 2, 8, 2, 13, 14, 9, 9, 2, 11, 11, 3, 8, 14, 2, 13, 8, 8, 14, 11, 8, 8, 6, 14, 0, 4, 0, 8, 14, 8, 10, 3, 9, 3, 9, 8, 8, 2, 3, 6, 2, 8, 2, 8, 5, 14, 14, 14, 4, 2, 3, 8, 2, 4, 5, 0]\n",
            "dided\n",
            "time\n",
            "[8, 3, 4, 12, 9, 2, 13, 3, 8, 13, 2, 2, 5, 8, 0, 3, 2, 11, 4, 0, 8, 3, 4, 0, 14, 3, 3, 9, 5, 14, 3, 3, 8, 8, 5, 2, 13, 5, 5, 5, 14, 3, 6, 4, 8, 8, 4, 9, 8, 9, 4, 14, 2, 11, 8, 8, 14, 8, 8, 3, 8, 9, 2, 3, 3, 3, 8, 4, 9, 11, 0, 13, 8]\n",
            "dided\n",
            "time\n",
            "[13, 3, 8, 9, 8, 14, 8, 2, 8, 0, 4, 8, 3, 3, 3, 5, 7, 8, 3, 2, 11, 5, 2, 8, 4, 13, 4, 14, 0, 3, 8, 3, 10, 3, 3, 4, 3, 14, 14, 3, 8, 8, 8, 2, 8, 0, 8, 4, 8, 4, 14, 8, 3, 5, 2, 2, 11, 9, 2, 2, 3, 8, 9, 8, 5, 14, 6, 8, 5, 4, 2, 4, 14, 8, 14, 14, 4, 8]\n",
            "dided\n",
            "time\n",
            "[8, 14, 8, 4, 8, 8, 8, 2, 9, 11, 3, 13, 5, 3, 8, 6, 8, 4, 9, 2, 14, 14, 2, 11, 4, 3, 4, 2, 2, 0, 4, 4, 6, 14, 0, 8, 13, 14, 8, 3, 5, 6, 8, 8, 14, 8, 8, 0, 8, 14, 8, 8, 5, 9, 8, 2, 8, 3, 3, 4, 4, 8, 5, 13, 8, 8, 12, 13, 8, 8, 14, 4, 8, 5, 9, 0, 8, 3, 9, 8, 5, 8, 2, 8, 14, 3, 5, 3, 8, 13, 3, 3, 2, 4, 8, 4, 8, 14, 14, 3, 3, 4, 14, 8, 2, 14, 3, 3, 4, 8, 8, 9, 8, 3, 9, 0, 8, 3, 8, 3, 4, 7, 8, 0, 8, 8, 5, 8, 8, 14, 14, 5, 2, 8, 13, 13, 13, 6, 8, 4, 14, 14, 8, 8, 8, 14, 3, 5, 9, 4, 8, 5, 4, 0, 4, 3, 14, 14, 8, 4, 4, 8, 3, 4, 5, 11, 9, 3, 3, 2, 3, 8, 2, 3, 0, 2, 8, 8, 8, 2, 3, 8, 14, 2, 5, 4, 8, 8, 8, 8, 8, 8, 8, 5, 4, 4, 9, 8, 5, 4, 8, 11, 9, 5, 4, 4, 3, 8, 14, 0, 3, 8, 0, 0, 3, 2, 8, 14, 8, 13, 2, 14, 8, 5, 11, 5, 8, 8, 4, 8, 3, 4, 3, 8, 8, 3, 4, 8, 3, 4, 11, 2, 3, 11, 4, 11, 8, 11, 4, 4, 9, 3, 4, 5, 8, 2, 3, 8, 0, 3, 4, 14, 3, 14, 8, 14, 4, 14, 2, 8, 4, 5, 5, 3, 13, 8, 0, 3, 4, 2, 8, 11, 3, 14, 14, 14, 3, 2, 11, 5, 3, 2, 3, 9, 9, 9, 8, 8, 3, 4, 8, 8, 14, 8, 4, 0, 3, 13, 0, 11, 5, 3, 9, 14, 0, 8, 6, 8, 3, 14, 4, 3, 3, 8, 6, 11, 8, 14, 2, 3, 8, 14, 9, 4, 0, 14, 4, 14, 8, 4, 13, 8, 5, 8, 13, 3, 3, 3, 6, 2, 8, 3, 14, 14, 8, 3, 8, 8, 8, 2, 0, 8, 0, 3, 8, 8, 8, 4, 8, 5, 3, 14, 2, 8, 8, 0, 3, 8, 2, 8, 5, 9, 8, 9, 3, 5, 0, 8, 3, 2, 8, 9, 8, 11, 3, 0, 0, 8, 9, 4, 4, 2, 2, 5, 4, 0, 3, 2, 11, 3, 3, 3, 4, 2, 8, 3, 5, 4, 14, 8, 3, 3, 8, 3, 2, 5, 2, 14, 2, 14, 4, 8, 4, 14, 4, 8, 3, 2, 13, 8, 3, 8, 0, 3, 14, 8, 3, 3, 8, 14, 3, 14, 4, 8, 11, 2, 13, 3, 8, 5, 0, 8, 11, 5, 0, 8, 14, 14, 3, 8, 8, 5, 11, 3, 0, 8, 14, 8, 3, 8, 5, 8, 5, 2, 3, 8, 14, 8, 4, 5, 8, 6, 9, 2, 8, 8, 3, 11, 3, 2, 14, 14, 4, 3, 8, 0, 8, 14, 14, 14, 1, 3, 11, 3, 2, 11, 8, 0, 8, 8, 0, 2, 3, 3, 3, 8, 0, 11, 5, 0, 3, 6, 2, 8, 3, 8, 8, 8, 6, 9, 8, 8, 8, 14, 3, 3, 13, 3, 8, 14, 8, 13, 8, 3, 8, 8, 3, 4, 8, 0, 8, 6, 8, 3, 2, 11, 11, 3, 8, 8, 4, 8, 0, 8, 14, 2, 5, 13, 3, 14, 8, 5, 14, 2, 8, 14, 8, 14, 0, 2, 13, 3, 8, 2, 2, 13, 8, 2, 6, 8, 8, 6, 4, 13, 8, 2, 3, 11, 3, 3, 10, 2, 9, 8, 2, 8, 0, 4, 8, 14, 5, 8, 2, 4, 3, 3, 8, 4, 11, 4, 4, 11, 14, 8, 2, 3, 9, 5, 8, 3, 3, 11, 3, 8, 4, 2, 11, 6, 2, 8, 9, 14, 3, 8, 3, 3, 8, 8, 8, 9, 3, 11, 5, 8, 3, 3, 8, 4, 3, 5, 3, 8, 8, 2, 2, 14, 3, 8, 8, 8, 4, 9, 0, 14, 9, 13, 5, 3, 5, 8, 8, 11, 8, 9, 14, 8, 3, 13, 3, 3, 8, 5, 14, 3, 0, 3, 14, 3, 8, 8, 9, 8, 8, 8, 2, 3, 13, 8, 3, 0, 3, 2, 5, 11, 8, 14, 4, 5, 11, 9, 8, 3, 5, 0, 2, 3, 11, 8, 11, 4, 3, 13, 4, 2, 4, 4, 14, 3, 0, 5, 13, 8, 8, 14, 8, 8, 8, 13, 8, 8, 3, 8, 14, 9, 4, 4, 8, 0, 2, 8, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 6, 5, 8, 8, 3, 3, 14, 4, 8, 3, 14, 8, 9, 11, 11, 8, 8, 8, 5, 14, 11, 8, 3, 8, 14, 14, 2, 8, 3, 9, 3, 2, 14, 3, 8, 8, 8, 4, 11, 4, 3, 3, 8, 3, 13, 3, 11, 0, 0]\n",
            "6 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02697022818028927 0.231689453125 3.2644808292388916 0.4925228953361511 1.359649419784546 2.1612114906311035\n",
            "repr, std, cov, clossl, z, norm 0.03276905044913292 0.2276611328125 3.460308074951172 0.5408653020858765 1.2971036434173584 2.255065679550171\n",
            "repr, std, cov, clossl, z, norm 0.03154568746685982 0.220947265625 3.826835870742798 0.45222553610801697 1.27215576171875 2.63155460357666\n",
            "repr, std, cov, clossl, z, norm 0.04745533689856529 0.2276611328125 3.449700355529785 0.47329533100128174 1.2379229068756104 1.8823297023773193\n",
            "repr, std, cov, clossl, z, norm 0.03267407417297363 0.2340087890625 3.144285202026367 0.521720826625824 1.204567313194275 1.8904998302459717\n",
            "repr, std, cov, clossl, z, norm 0.046236805617809296 0.233642578125 3.1448073387145996 0.4995023310184479 1.2746907472610474 2.156045436859131\n",
            "repr, std, cov, clossl, z, norm 0.027550216764211655 0.22900390625 3.3801212310791016 0.49986574053764343 1.3610988855361938 2.3447418212890625\n",
            "repr, std, cov, clossl, z, norm 0.037958987057209015 0.225341796875 3.580761671066284 0.427501380443573 1.252273678779602 1.7984979152679443\n",
            "repr, std, cov, clossl, z, norm 0.027440419420599937 0.22705078125 3.5003225803375244 0.4202691912651062 1.320556402206421 1.937322974205017\n",
            "repr, std, cov, clossl, z, norm 0.03774913400411606 0.2281494140625 3.426877975463867 0.4869043529033661 1.2336136102676392 2.47320818901062\n",
            "repr, std, cov, clossl, z, norm 0.02843860723078251 0.230224609375 3.336027145385742 0.44254642724990845 1.2718772888183594 2.1140024662017822\n",
            "repr, std, cov, clossl, z, norm 0.03367422893643379 0.23046875 3.3074960708618164 0.43532735109329224 1.3143543004989624 2.0123331546783447\n",
            "train_data.data 20472\n",
            "dided\n",
            "time\n",
            "[0, 11, 8, 5, 8, 8, 2, 11, 0, 6, 8, 11, 2, 8, 0, 5, 2, 14, 3, 8, 3, 4, 0, 0, 5, 3, 8, 4, 13, 14, 14, 8, 3, 4, 4, 8, 9, 14, 2, 8, 8, 5, 8, 3, 5, 4, 8, 4, 8, 13, 3, 14, 0, 11, 2, 8, 3, 6, 8, 0, 8, 8, 8, 8, 8, 8, 14, 8, 0, 14, 8, 6, 4, 8, 8, 5, 8, 3, 3, 3, 11, 0, 2, 8, 14, 5, 11, 9, 2, 6, 9, 2, 8, 4, 8, 5, 6, 8, 9, 8, 11, 2, 5, 0, 8, 13, 8, 9, 9, 4, 0, 14, 6, 14, 4, 0, 2, 8, 3, 11, 8, 3, 13, 14, 3, 2, 4, 4, 3, 8, 14, 8, 8, 8, 5, 8, 8, 9, 0, 0]\n",
            "dided\n",
            "time\n",
            "[9, 0, 0, 3, 8, 8, 8, 8, 8, 5, 3, 8, 8, 8, 8, 3, 0, 3, 3, 11, 3, 6, 11, 3, 13, 8, 3, 8, 8, 8, 8, 3, 8, 9, 5, 8, 8, 2, 2, 14, 8, 4, 8, 14, 8, 3, 5, 3, 13, 8, 8, 4, 3, 4, 8, 8, 3, 8, 2, 8, 8, 0, 8, 14, 13, 5, 0, 8, 8, 5, 8, 8, 14, 8, 0, 3, 3, 11, 0, 8, 3, 2, 13, 11, 6, 8, 3, 8, 0, 14, 8, 0, 0, 8, 2, 2, 0, 8, 9, 6, 0, 8, 9, 4, 3, 3, 8, 4, 8, 3, 14, 11, 3, 2, 8, 8, 6, 3, 3, 3, 2, 11, 14, 14, 11, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 4, 0, 8, 8, 3, 2, 6, 3, 13, 8, 4, 14, 13, 3, 0, 4, 0, 11, 8, 8, 3, 8, 3, 13, 4, 13, 8, 2, 0, 5, 4, 2, 3, 8, 8, 13, 8, 4, 6, 8, 14, 8, 8, 3, 3, 9, 4, 2, 4, 8, 2, 0, 11, 9, 8, 0, 3, 2, 8, 4, 0, 8, 8, 3, 4, 8, 8, 2, 8, 9, 5, 8, 3, 4, 8, 2, 2, 9, 8, 8, 13, 8, 8, 8, 4]\n",
            "dided\n",
            "time\n",
            "[8, 8, 4, 3, 6, 2, 13, 8, 8, 14, 2, 14, 2, 3, 8, 3, 0, 8, 3, 8, 2, 2, 6, 0, 3, 0, 8, 8, 3, 3, 5, 9, 11, 3, 8, 8, 8, 9, 4, 11, 8, 9, 5, 11, 8, 8, 11, 13, 4, 0, 8, 8, 2, 3, 3, 2, 14, 3, 14, 4, 8, 4, 2, 9, 3, 8, 0, 8, 9, 9, 8, 4, 8, 8, 2, 8, 0, 9, 3, 2, 3, 0, 5, 4, 9, 8, 2, 8, 2, 8, 8, 5, 8, 8, 5, 8, 0, 8, 8, 0, 13, 6, 3, 11, 3, 14, 8, 2, 3, 5, 2, 2, 9, 8, 13, 4, 3, 3, 4, 8, 8, 8, 3, 3, 3, 3, 3, 2, 0, 11, 3, 11, 8, 3, 8, 0, 5, 3, 2, 13, 4, 6, 4, 6, 11, 3, 8, 8, 9, 5, 2, 3, 5, 14, 8, 8, 14, 2, 8, 4, 11, 8, 5, 14, 2, 3, 8, 4, 2, 2, 3, 13, 11, 5, 4, 14, 8, 3, 0, 8, 2, 3, 2, 9, 8, 8, 2, 0, 11, 4, 2, 0, 3, 8, 3, 2, 4, 8, 0, 2, 9, 0, 8, 2, 9, 9, 3, 5, 8, 11, 0, 3, 8, 4, 3, 6]\n",
            "dided\n",
            "time\n",
            "[4, 3, 6, 13, 3, 13, 8, 8, 8, 11, 3, 5, 8, 8, 0, 0, 11, 9, 0, 3, 8, 6, 3, 13, 4, 9, 14, 11, 3, 8, 6, 3, 14, 8, 14, 3, 14, 8, 3, 9, 3, 8, 13, 8, 3, 3, 8, 2, 8, 2, 3, 9, 5, 8, 14, 14, 11, 4, 2, 11, 3, 13, 14, 8, 5, 0, 5, 14, 2, 8, 3, 13, 3, 11, 8, 0, 3, 5, 8, 3, 13]\n",
            "7 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02529161609709263 0.230224609375 3.293968677520752 0.4067527949810028 1.2936476469039917 2.290499687194824\n",
            "repr, std, cov, clossl, z, norm 0.02853996679186821 0.228271484375 3.407203197479248 0.48104020953178406 1.3312346935272217 2.292938470840454\n",
            "repr, std, cov, clossl, z, norm 0.026361674070358276 0.225341796875 3.5615830421447754 0.46963992714881897 1.3557754755020142 2.665540933609009\n",
            "repr, std, cov, clossl, z, norm 0.03563272953033447 0.2293701171875 3.354449987411499 0.4139714241027832 1.2849993705749512 2.190217971801758\n",
            "repr, std, cov, clossl, z, norm 0.032700929790735245 0.2301025390625 3.3230786323547363 0.46217742562294006 1.2461278438568115 2.0056896209716797\n",
            "repr, std, cov, clossl, z, norm 0.04110687971115112 0.22607421875 3.5243024826049805 0.4124694764614105 1.3766555786132812 2.0813512802124023\n",
            "repr, std, cov, clossl, z, norm 0.03246361017227173 0.231689453125 3.268521547317505 0.44153982400894165 1.2649328708648682 2.123563766479492\n",
            "repr, std, cov, clossl, z, norm 0.042338915169239044 0.23193359375 3.2406258583068848 0.40385377407073975 1.265582799911499 2.4556877613067627\n",
            "repr, std, cov, clossl, z, norm 0.0317966602742672 0.227294921875 3.471864700317383 0.43255615234375 1.3836562633514404 2.122664451599121\n",
            "repr, std, cov, clossl, z, norm 0.043594684451818466 0.2293701171875 3.3553714752197266 0.46285712718963623 1.3038065433502197 1.827567219734192\n",
            "repr, std, cov, clossl, z, norm 0.028892196714878082 0.2254638671875 3.5413172245025635 0.44284680485725403 1.2382973432540894 2.1728177070617676\n",
            "repr, std, cov, clossl, z, norm 0.03858564794063568 0.2291259765625 3.35714054107666 0.48350459337234497 1.2579731941223145 1.9115179777145386\n",
            "train_data.data 20437\n",
            "dided\n",
            "time\n",
            "[13, 5, 3, 11, 2, 3, 3, 4, 8, 14, 8, 5, 2, 5, 3, 3, 2, 8, 8, 5, 8, 5, 5, 0, 9, 0, 0, 14, 8, 11, 5, 5, 3, 6, 2, 8, 3, 8, 3, 8, 5, 14, 8, 0, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 8, 11, 8, 0, 8, 2, 5, 8, 8, 0, 11, 3, 3, 0, 4, 0, 4, 14, 5, 8, 8, 4, 8, 8, 0, 8, 11, 9, 14, 3, 8, 0, 0, 5, 13, 3, 5, 4, 9, 3, 13]\n",
            "dided\n",
            "time\n",
            "[9, 3, 13, 8, 8, 2, 14, 4, 2, 8, 8, 4, 8, 8, 9, 8, 14, 0, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 11, 4, 14, 8, 5, 8, 3, 8, 3, 11, 8, 8, 3, 13, 11, 3, 4, 2, 3, 14, 9, 14, 5, 8, 0, 5, 14, 8, 8, 14, 8, 8, 3, 9]\n",
            "dided\n",
            "time\n",
            "[3, 9, 14, 2, 3, 8, 2, 0, 14, 8, 13, 3, 8, 9, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 13, 4, 9, 3, 8, 8, 3, 14, 13, 3, 11, 3, 0, 8, 8, 11, 2, 2, 8, 8, 0, 13, 8, 4, 8, 0, 4, 8, 0, 14, 2, 3, 2, 8, 4, 14, 3, 8, 4, 3, 8, 8, 4, 3, 8, 8, 2, 8, 14, 5, 9, 3, 11, 14, 6, 4, 8, 14, 2, 4, 8, 8, 8, 14, 2, 8, 8, 3, 2, 3, 4, 3, 13, 8, 13, 14, 8, 3, 9, 4, 3]\n",
            "8 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.027535337954759598 0.2318115234375 3.253821849822998 0.4935227334499359 1.3559530973434448 2.0962300300598145\n",
            "repr, std, cov, clossl, z, norm 0.03542451933026314 0.231689453125 3.25034499168396 0.40910160541534424 1.2663933038711548 2.156846046447754\n",
            "repr, std, cov, clossl, z, norm 0.027811046689748764 0.228271484375 3.4312307834625244 0.4875829815864563 1.3317739963531494 2.032907009124756\n",
            "repr, std, cov, clossl, z, norm 0.03236374631524086 0.228271484375 3.422123432159424 0.39517977833747864 1.3277982473373413 2.3153598308563232\n",
            "repr, std, cov, clossl, z, norm 0.024814920499920845 0.2315673828125 3.247011184692383 0.3973684012889862 1.3370606899261475 2.2721199989318848\n",
            "repr, std, cov, clossl, z, norm 0.03095058910548687 0.228759765625 3.39389705657959 0.42764031887054443 1.3120304346084595 1.8533393144607544\n",
            "repr, std, cov, clossl, z, norm 0.026144327595829964 0.228515625 3.3976991176605225 0.3933686912059784 1.2324497699737549 1.657750129699707\n",
            "repr, std, cov, clossl, z, norm 0.029161935672163963 0.226806640625 3.4892876148223877 0.45284825563430786 1.2874681949615479 2.031932830810547\n",
            "repr, std, cov, clossl, z, norm 0.027773873880505562 0.2294921875 3.3525028228759766 0.4065997302532196 1.3440889120101929 2.0134143829345703\n",
            "repr, std, cov, clossl, z, norm 0.03323023393750191 0.2320556640625 3.2193610668182373 0.3845211863517761 1.2537084817886353 1.7776256799697876\n",
            "repr, std, cov, clossl, z, norm 0.02911260724067688 0.22998046875 3.3366453647613525 0.35964760184288025 1.2630667686462402 1.452840805053711\n",
            "repr, std, cov, clossl, z, norm 0.039123669266700745 0.2298583984375 3.322871685028076 0.4407927393913269 1.2950830459594727 1.7353590726852417\n",
            "train_data.data 20367\n",
            "dided\n",
            "time\n",
            "[3, 3, 8, 9, 13, 0, 2, 14, 8, 5, 8, 4, 2, 0, 14, 4, 6, 3, 4, 8, 8, 8, 4, 4, 3, 3, 0, 3, 3, 11, 9, 2, 8]\n",
            "dided\n",
            "time\n",
            "[4, 11, 2, 10, 8, 2, 0, 2, 8, 9, 0, 8, 11, 14, 0, 8, 2, 3, 13, 8, 3, 0, 3, 9, 4, 8, 9, 3, 2, 8, 3, 4, 5, 8, 11, 4, 14, 8, 13, 13, 11, 8, 3, 8, 13, 8, 8, 5, 3, 8, 2, 9, 8, 7, 11, 3, 8]\n",
            "dided\n",
            "time\n",
            "[5, 5, 2, 14, 9, 8, 9, 13, 0, 5, 8, 14, 5, 0, 11, 2, 14, 0, 8, 11, 8, 4, 8, 5, 6, 8, 2, 3, 8, 14, 5, 0, 3, 2, 4, 8, 5, 4, 4, 3, 8, 8, 5, 8, 3, 0, 3, 4, 4, 3, 11, 8, 2, 13, 3, 8, 3, 2, 13, 4, 13, 3, 2, 8, 4, 9, 8, 3, 9, 8, 11, 3, 8, 8, 11, 3, 8, 3, 3, 5, 3, 4, 5, 8, 4, 3, 11, 2, 8, 8, 8, 5, 4, 3, 3, 5, 0, 8, 13, 8, 3, 0, 8, 14, 9, 11, 9, 4, 8, 8, 8, 4, 9, 8, 6, 14, 2, 8, 0, 9]\n",
            "dided\n",
            "time\n",
            "[8, 0, 9, 13, 3, 4, 2, 3, 3, 8, 14, 14, 3, 4, 0, 2, 4, 8, 8, 9, 14, 2, 4, 2, 11, 8, 5, 3, 14, 8, 8, 6, 9, 11, 3, 14, 8, 14, 6, 8, 5, 2, 0, 9, 13, 11, 5, 9, 11, 13, 9, 13, 2, 3, 13, 8, 8, 8, 6, 8, 13, 4, 5, 13, 8, 4, 3, 9, 2, 5, 8, 0, 5, 8, 8, 13, 8, 3]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 4, 14, 14, 5, 13, 11, 3, 14, 8, 14, 5, 5, 2, 3, 4, 3, 4, 2, 2, 5, 11, 5, 9, 8, 2, 13, 0, 0, 8, 8, 2, 9, 9, 5, 14, 3, 8, 0, 4, 8, 8, 3, 4, 11, 3, 2, 8, 0, 4, 3, 13, 4, 5, 8, 3, 2, 2, 0, 2, 14, 8, 11, 5, 8, 2, 4, 3, 3, 9, 8, 4, 8, 3, 0, 4, 4, 0, 3, 3, 4, 4]\n",
            "9 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.03285463899374008 0.225341796875 3.548342227935791 0.4537407457828522 1.2645691633224487 1.8226011991500854\n",
            "repr, std, cov, clossl, z, norm 0.05688956007361412 0.2279052734375 3.417818546295166 0.6210587024688721 1.302134394645691 1.9298256635665894\n",
            "repr, std, cov, clossl, z, norm 0.050459764897823334 0.228515625 3.4082279205322266 0.5347555875778198 1.3444961309432983 1.505632996559143\n",
            "repr, std, cov, clossl, z, norm 0.06199505180120468 0.2276611328125 3.442476749420166 0.556046724319458 1.3320510387420654 1.767141342163086\n",
            "repr, std, cov, clossl, z, norm 0.034959398210048676 0.229736328125 3.3292887210845947 0.4771645665168762 1.2901837825775146 1.8243484497070312\n",
            "repr, std, cov, clossl, z, norm 0.03577859327197075 0.2291259765625 3.373295307159424 0.5231065154075623 1.2766019105911255 1.4206011295318604\n",
            "repr, std, cov, clossl, z, norm 0.023236390203237534 0.2264404296875 3.50321626663208 0.43756356835365295 1.3103573322296143 2.142808437347412\n",
            "repr, std, cov, clossl, z, norm 0.02874409779906273 0.2242431640625 3.6166112422943115 0.40265458822250366 1.3412151336669922 1.7538363933563232\n",
            "repr, std, cov, clossl, z, norm 0.02087746560573578 0.229248046875 3.3692805767059326 0.4540559649467468 1.3028069734573364 1.6774609088897705\n",
            "repr, std, cov, clossl, z, norm 0.02320466935634613 0.234130859375 3.149522542953491 0.40547430515289307 1.3552206754684448 1.6417490243911743\n",
            "repr, std, cov, clossl, z, norm 0.018481500446796417 0.228759765625 3.4026637077331543 0.37409916520118713 1.3152979612350464 1.2702903747558594\n",
            "repr, std, cov, clossl, z, norm 0.020538538694381714 0.2281494140625 3.419999599456787 0.44452807307243347 1.4025516510009766 1.5230441093444824\n",
            "train_data.data 20438\n",
            "dided\n",
            "time\n",
            "[3, 4, 4, 3, 0, 8, 3, 14, 0, 3, 3, 2, 3, 3, 8, 4, 3, 3, 8, 9, 2, 4, 2, 14, 14, 3, 14, 2, 11, 0, 8, 13, 3, 5, 4, 8, 8, 8, 4, 13, 3, 4, 8, 8, 3, 9, 3]\n",
            "dided\n",
            "time\n",
            "[9, 3, 3, 8, 13, 3, 5, 9, 0, 13, 8, 14, 3, 2, 3, 8, 3, 4, 9, 8, 13, 9, 2, 2, 8, 4, 4, 14, 8, 8, 4, 2, 4, 8, 8, 14, 14, 8, 8, 8, 5, 8, 2, 3, 5, 8, 1, 2, 3, 0, 5, 8, 2, 13, 3, 3, 4, 3, 8, 3, 11, 8, 0, 8, 4, 3, 8, 0, 5, 2, 4, 3, 8, 2, 3, 8, 13, 2, 7, 8, 4, 14, 8, 8, 3, 5, 3, 3, 13, 13, 8, 0, 9, 8, 5, 14, 4, 8, 8, 8, 8, 5, 8, 2, 14, 8, 5, 5, 11, 8, 13, 8, 11, 8, 11, 0, 5, 4, 11]\n",
            "dided\n",
            "time\n",
            "[4, 11, 3, 3, 11, 3, 4, 8, 5, 14, 5, 3, 8, 14, 2, 2, 13, 3, 9, 8, 2, 14, 13, 4, 8, 2, 0, 3, 14, 3, 8, 3, 5, 13, 14, 13, 8, 14, 8, 2, 14, 8, 8, 8, 4, 8, 5, 2, 0, 14, 8, 13, 3, 8, 8, 2, 8, 4, 8, 8, 8, 3, 8, 8, 8, 8, 9, 8, 13, 14, 4, 9, 2, 14, 3, 5, 0, 2, 14, 4, 8, 8, 2, 8, 2, 3, 8, 9, 13, 9, 3, 9, 14, 0, 8, 5, 8, 5, 11, 2, 5, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 14, 8, 3, 2, 3, 8, 5, 4, 8, 6, 14, 13, 3, 8, 3, 5, 5, 8, 3, 14, 2, 0, 8]\n",
            "dided\n",
            "time\n",
            "[2, 6, 8, 8, 0, 9, 8, 8, 8, 9, 4, 8, 2, 3, 8, 6, 9, 5, 11, 0, 14, 8, 8, 4, 11, 8]\n",
            "10 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.019288092851638794 0.228515625 3.42250919342041 0.40786629915237427 1.277739405632019 1.8817790746688843\n",
            "repr, std, cov, clossl, z, norm 0.025751683861017227 0.229736328125 3.3578295707702637 0.39026567339897156 1.2673442363739014 1.0805763006210327\n",
            "repr, std, cov, clossl, z, norm 0.024631526321172714 0.232177734375 3.224140167236328 0.41401633620262146 1.287062168121338 1.5086588859558105\n",
            "repr, std, cov, clossl, z, norm 0.030223293229937553 0.2347412109375 3.095271110534668 0.42309099435806274 1.2339106798171997 1.9256068468093872\n",
            "repr, std, cov, clossl, z, norm 0.026220722123980522 0.230712890625 3.295884370803833 0.5028176307678223 1.2349964380264282 1.805087924003601\n",
            "repr, std, cov, clossl, z, norm 0.0392114594578743 0.2247314453125 3.580421209335327 0.41025614738464355 1.3073540925979614 1.8063111305236816\n",
            "repr, std, cov, clossl, z, norm 0.027116145938634872 0.2220458984375 3.7339706420898438 0.46508556604385376 1.2537026405334473 1.8443785905838013\n",
            "repr, std, cov, clossl, z, norm 0.037259720265865326 0.226318359375 3.505466938018799 0.49323156476020813 1.2690428495407104 2.08884859085083\n",
            "repr, std, cov, clossl, z, norm 0.03228272497653961 0.2305908203125 3.3099758625030518 0.5064743161201477 1.3354697227478027 2.0891852378845215\n",
            "repr, std, cov, clossl, z, norm 0.036937348544597626 0.2371826171875 3.0199060440063477 0.5306951403617859 1.3403384685516357 2.1062183380126953\n",
            "repr, std, cov, clossl, z, norm 0.028434786945581436 0.2337646484375 3.16462779045105 0.48779428005218506 1.2838062047958374 2.0845956802368164\n",
            "repr, std, cov, clossl, z, norm 0.03737659007310867 0.228271484375 3.4386417865753174 0.37281301617622375 1.313633680343628 2.2090351581573486\n",
            "train_data.data 20887\n",
            "dided\n",
            "time\n",
            "[8, 13, 2, 8, 0, 5, 5, 14, 8, 8, 6, 2, 14, 9, 2, 8, 8, 11, 4, 8, 14, 14, 14, 2, 8, 3, 9, 9, 8, 8, 8, 14, 3, 3, 8, 2, 8, 8, 5, 8, 0, 13, 2, 3, 3, 8, 3, 3, 4, 14, 3, 8, 14, 9, 4, 3, 6, 4, 2, 8, 6, 0, 0, 4, 0, 4, 4, 14, 8, 14, 8, 0, 11, 4, 8, 3, 4, 3, 8, 8, 5, 11]\n",
            "dided\n",
            "time\n",
            "[11, 3, 11, 2, 0, 4, 3, 0, 14, 3, 3, 8, 5, 8, 3, 3, 8, 3, 14, 11, 3, 5, 4, 8, 13, 3, 14, 14, 2, 11, 5, 3, 3, 3, 0, 13, 0, 8, 9, 2, 4, 0, 3, 8, 5, 8, 3, 8, 3, 8, 5, 14, 14, 4, 14, 5, 8, 3, 5, 3, 8, 0, 0, 3, 14, 3, 0, 11, 8, 2, 8]\n",
            "dided\n",
            "time\n",
            "[2, 8, 0, 5, 3, 8, 3, 5, 6, 13, 3, 3, 4, 13, 8, 8, 9, 8, 8, 5, 8, 8, 3, 8, 8, 8, 3, 3, 2, 14, 8, 4, 13, 3, 3, 8, 8, 8, 0, 3, 0, 8, 3, 8, 2, 2, 8, 11, 6, 13, 2, 3, 5, 4, 4, 8, 9, 13, 2, 14, 2, 11, 8]\n",
            "dided\n",
            "time\n",
            "[11, 8, 14, 4, 5, 3, 5, 9, 8, 5, 11, 13, 8, 3, 3, 3, 3, 8, 11, 3, 8, 5, 13, 8, 8, 8, 8, 11, 3, 13, 8, 11, 5, 8, 8, 8, 3, 2, 8, 8, 5, 8, 11, 8, 13, 8, 2, 0, 3, 8, 14, 8, 6, 3, 14, 8, 5, 8, 5]\n",
            "dided\n",
            "time\n",
            "[8, 5, 8, 8, 13, 5, 2, 5, 8, 2, 9, 2, 2, 8, 3, 4, 8, 3, 3, 8, 3, 11, 3, 14, 4, 5, 11, 8, 3, 8, 13, 4, 14, 0, 14, 0, 3, 3, 6, 3, 9, 5, 13, 8, 3, 3, 4, 5, 2, 8, 3, 3, 14, 8, 3, 14, 13, 0, 3, 9, 4, 6, 3, 13, 14, 3, 8, 2, 0, 11, 8, 4, 5, 8, 11, 8, 3, 5, 11, 0, 3, 0, 13, 4, 3, 5, 14, 8, 14, 13, 8, 13, 14, 8]\n",
            "11 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.025879941880702972 0.226318359375 3.5534629821777344 0.44258517026901245 1.3106061220169067 1.8573557138442993\n",
            "repr, std, cov, clossl, z, norm 0.03880420699715614 0.2293701171875 3.3777365684509277 0.45318588614463806 1.2641345262527466 1.6532984972000122\n",
            "repr, std, cov, clossl, z, norm 0.026705309748649597 0.22705078125 3.4821314811706543 0.43028679490089417 1.2184116840362549 2.088620185852051\n",
            "repr, std, cov, clossl, z, norm 0.03282112628221512 0.2257080078125 3.5484251976013184 0.41128799319267273 1.2395641803741455 2.193774938583374\n",
            "repr, std, cov, clossl, z, norm 0.02338799461722374 0.2340087890625 3.127133369445801 0.4133242964744568 1.3184369802474976 2.397143840789795\n",
            "repr, std, cov, clossl, z, norm 0.03119928576052189 0.2310791015625 3.263500213623047 0.4401763677597046 1.3142106533050537 2.065401554107666\n",
            "repr, std, cov, clossl, z, norm 0.022632351145148277 0.2291259765625 3.3516180515289307 0.42313435673713684 1.3154141902923584 2.2273969650268555\n",
            "repr, std, cov, clossl, z, norm 0.03159492090344429 0.2266845703125 3.5005102157592773 0.43147698044776917 1.3123064041137695 1.9186749458312988\n",
            "repr, std, cov, clossl, z, norm 0.033055905252695084 0.2257080078125 3.560577154159546 0.45142820477485657 1.2710825204849243 2.2102763652801514\n",
            "repr, std, cov, clossl, z, norm 0.034277692437171936 0.226318359375 3.497537612915039 0.5019078850746155 1.2720979452133179 2.123542308807373\n",
            "repr, std, cov, clossl, z, norm 0.031199177727103233 0.22705078125 3.485637664794922 0.4231764078140259 1.2453856468200684 2.3008506298065186\n",
            "repr, std, cov, clossl, z, norm 0.03920168802142143 0.23095703125 3.272953987121582 0.5411730408668518 1.2210584878921509 2.0102438926696777\n",
            "train_data.data 20358\n",
            "dided\n",
            "time\n",
            "[8, 14, 3, 8, 8, 14, 8, 9, 0, 2, 3, 9, 4, 8, 3, 3, 3, 8, 8, 8, 3, 8, 9, 8, 3, 14, 0, 0, 3, 5, 2, 8, 9, 14, 11, 2, 4, 5, 1, 4, 13, 2, 0, 8, 4, 2, 8, 8, 8, 8, 14, 3, 5, 2, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[10, 3, 8, 8, 2, 0, 3, 3, 4, 3, 5, 3, 3, 8, 14, 2, 8, 8, 3, 6, 3, 6, 5, 2, 11, 3, 3, 8, 5, 3, 5, 2, 4, 13, 9, 3, 8, 8, 8, 3, 14, 9, 8, 0, 3, 2, 3, 0, 4, 8, 5, 13, 8, 8, 8, 0, 0, 8, 2, 8, 2, 3, 11, 8, 8, 8, 8, 5, 4, 0, 2, 3]\n",
            "dided\n",
            "time\n",
            "[0, 2, 3, 13, 14, 3, 11, 0, 2, 2, 3, 11, 2, 13, 3, 3, 5, 0, 5, 8, 4, 8, 8, 14, 14, 8, 8, 3, 0, 0, 2, 3, 8, 13, 3, 5, 9, 4, 13, 8, 4, 9, 4, 3, 9, 2, 3, 4, 0, 9, 4, 14, 2, 3, 3, 5, 2, 2, 4, 2, 4, 3, 5, 14, 8, 4, 4, 2, 14, 11, 14, 8, 3, 11, 14, 3, 3, 11, 14, 8, 2, 4, 0, 3, 8, 8, 3, 7]\n",
            "dided\n",
            "time\n",
            "[8, 3, 7, 14, 11, 2, 8, 3, 13, 9, 2, 8, 8, 3, 13, 8, 9, 8, 8, 8, 8, 3, 8, 0, 9, 10, 11, 4, 0, 4, 8, 3, 8, 11, 8, 8, 8, 0, 8, 8, 3, 13, 8, 8, 2, 2, 0, 8, 3, 9, 14, 0, 8, 8, 8, 8, 2, 8, 8, 3, 8, 11, 11, 3, 8, 9, 14, 2, 3, 4, 8, 13, 0, 13, 13, 8, 5, 11, 0, 3, 3, 5, 2, 8, 4, 8, 8, 14, 8, 2, 8, 5, 4, 3]\n",
            "dided\n",
            "time\n",
            "[3, 5, 8, 8, 3, 8, 8, 13, 11, 9, 3, 8, 8, 3, 3, 13, 2, 5, 0, 4, 8, 13, 8, 5, 4, 3, 9, 0, 8, 3, 14, 2, 14, 0, 2, 3, 8, 8, 2, 14, 5, 11, 0, 14, 8, 4, 6]\n",
            "12 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.03225032612681389 0.228759765625 3.405928611755371 0.43773147463798523 1.2447561025619507 2.1359410285949707\n",
            "repr, std, cov, clossl, z, norm 0.03105231188237667 0.2281494140625 3.409276008605957 0.5922204256057739 1.2769789695739746 1.9330388307571411\n",
            "repr, std, cov, clossl, z, norm 0.029794203117489815 0.22998046875 3.311598300933838 0.35890060663223267 1.2261682748794556 1.920636773109436\n",
            "repr, std, cov, clossl, z, norm 0.02655383199453354 0.2333984375 3.1672844886779785 0.5363924503326416 1.3083597421646118 2.0056028366088867\n",
            "repr, std, cov, clossl, z, norm 0.025296581909060478 0.2291259765625 3.3878302574157715 0.4147712290287018 1.2600289583206177 2.264611005783081\n",
            "repr, std, cov, clossl, z, norm 0.02273322083055973 0.2301025390625 3.329771041870117 0.4610450565814972 1.2969169616699219 2.173818588256836\n",
            "repr, std, cov, clossl, z, norm 0.024185264483094215 0.2333984375 3.1608011722564697 0.46052029728889465 1.268160343170166 2.191113233566284\n",
            "repr, std, cov, clossl, z, norm 0.026441466063261032 0.2301025390625 3.312941074371338 0.41941896080970764 1.3000435829162598 2.1260852813720703\n",
            "repr, std, cov, clossl, z, norm 0.022723941132426262 0.2249755859375 3.5804691314697266 0.4905291199684143 1.2317447662353516 2.0518798828125\n",
            "repr, std, cov, clossl, z, norm 0.024076083675026894 0.2232666015625 3.6644861698150635 0.48506179451942444 1.265976905822754 2.2575573921203613\n",
            "repr, std, cov, clossl, z, norm 0.0208742655813694 0.2293701171875 3.3530988693237305 0.4145384728908539 1.2544831037521362 1.9956964254379272\n",
            "repr, std, cov, clossl, z, norm 0.026194920763373375 0.2291259765625 3.3827171325683594 0.4584912955760956 1.2787845134735107 2.473085641860962\n",
            "train_data.data 20411\n",
            "dided\n",
            "time\n",
            "[4, 6, 3, 3, 5, 5, 14, 0, 0, 8, 3, 11, 3, 8, 11, 5, 8, 5, 0, 6, 0, 14, 8, 8, 14, 3, 6, 8, 4, 3, 14, 8, 8, 3, 14, 8, 14, 11, 5, 3, 3, 3, 8, 4, 3, 3, 3, 0, 6, 8, 13, 3, 2, 8, 8, 3, 9, 8, 5, 2]\n",
            "dided\n",
            "time\n",
            "[8, 5, 2, 3, 4, 8, 5, 14, 5, 9, 3, 3, 8, 5, 4, 8, 8, 4, 2, 5, 13, 2, 8, 14, 11, 2, 11, 0, 13, 9, 3, 3, 8, 4, 3, 5, 5, 3, 8, 4, 2, 14, 8, 9, 14, 2, 3, 8, 0, 2, 0, 8, 8, 8, 9, 4, 11, 3, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 2, 4, 14, 8, 8, 3, 8, 3, 2, 8, 2, 8, 2, 9, 2, 11, 3, 11, 3, 14, 8, 13, 3, 2, 13, 4, 8, 8, 8, 0, 4, 0, 2, 5, 4, 3, 2, 4, 6, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 3, 0, 0, 8, 0, 8, 9, 2, 9, 8, 8, 5, 9, 8, 3, 3, 3, 14, 2, 9, 8, 4, 11, 5, 8, 3, 2, 8, 0, 8, 14, 4, 2, 4, 3, 2, 6, 5, 5, 8, 8, 5, 8, 4, 8, 2, 8]\n",
            "dided\n",
            "time\n",
            "[8, 2, 8, 2, 8, 2, 3, 11, 3, 13, 8, 9, 0, 2, 8, 0, 14, 14, 8, 0, 3, 8, 13, 0, 13, 13, 0, 3, 0, 3, 3, 11, 2, 5, 3, 3, 6, 3, 11, 8, 0, 3, 8, 8, 11, 3, 8, 3, 3, 8, 13, 13, 8, 11, 2, 3, 8, 0, 2, 8, 13, 13, 5, 8, 14, 0, 11, 2, 8, 13, 5, 8, 3, 14, 2, 4, 11, 2, 8, 8, 14, 3, 6, 9, 3, 4, 8, 8, 8, 4, 3, 8, 0, 3, 8, 8, 6, 14, 5, 3, 4, 2, 3, 2, 13, 11, 9, 11, 9, 8, 0, 8, 13, 8, 8, 0, 4, 8, 8, 9, 8, 2, 4, 9, 2, 14, 0, 5]\n",
            "13 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02187514677643776 0.228759765625 3.396859645843506 0.3839642107486725 1.2615669965744019 2.0930469036102295\n",
            "repr, std, cov, clossl, z, norm 0.031162079423666 0.2310791015625 3.272585868835449 0.4363974630832672 1.3027626276016235 1.741248369216919\n",
            "repr, std, cov, clossl, z, norm 0.021702919155359268 0.232666015625 3.219752311706543 0.46014463901519775 1.2653924226760864 2.0654659271240234\n",
            "repr, std, cov, clossl, z, norm 0.03773898631334305 0.226318359375 3.5038247108459473 0.502004086971283 1.2092418670654297 1.7271286249160767\n",
            "repr, std, cov, clossl, z, norm 0.0205391738563776 0.228271484375 3.4037842750549316 0.5209517478942871 1.4299256801605225 1.5948947668075562\n",
            "repr, std, cov, clossl, z, norm 0.03666379675269127 0.2259521484375 3.5287351608276367 0.5154902338981628 1.2781010866165161 2.0456554889678955\n",
            "repr, std, cov, clossl, z, norm 0.020000604912638664 0.2266845703125 3.52177095413208 0.5232344269752502 1.2954944372177124 1.6748274564743042\n",
            "repr, std, cov, clossl, z, norm 0.034434109926223755 0.2275390625 3.4617509841918945 0.45190560817718506 1.2726932764053345 2.144838333129883\n",
            "repr, std, cov, clossl, z, norm 0.02553306519985199 0.2266845703125 3.4875741004943848 0.44280681014060974 1.2670040130615234 2.1852550506591797\n",
            "repr, std, cov, clossl, z, norm 0.036681707948446274 0.2237548828125 3.6485509872436523 0.45783674716949463 1.276994228363037 2.2472198009490967\n",
            "repr, std, cov, clossl, z, norm 0.02001381665468216 0.2274169921875 3.4502451419830322 0.43978986144065857 1.3065952062606812 2.1573169231414795\n",
            "repr, std, cov, clossl, z, norm 0.028085479512810707 0.2269287109375 3.4677700996398926 0.47941991686820984 1.296722650527954 1.7442176342010498\n",
            "train_data.data 21118\n",
            "dided\n",
            "time\n",
            "[14, 0, 5, 3, 11, 0, 8, 8, 4, 8, 8, 5, 8, 9, 3, 2, 3, 8, 14, 8, 13, 6, 14, 3, 8, 14, 3, 8, 8, 0, 2, 8, 11, 3, 5, 4, 0, 2, 3, 3, 3, 14, 0, 8, 14, 4, 2, 0, 4, 5, 4, 8, 8, 6, 8, 6, 3, 9, 6, 8, 11, 4, 8, 3, 8, 4, 8, 8, 14, 2, 14, 3, 8, 4, 3, 4, 3, 0, 3, 2, 11, 4, 8, 3, 14, 11, 0, 4, 2, 0, 3, 2, 4, 3, 4, 2, 0, 3, 3, 2, 2, 11, 4, 2]\n",
            "dided\n",
            "time\n",
            "[11, 4, 2, 3, 3, 8, 4, 3, 3, 5, 9, 14, 13, 3, 3, 0, 8, 3, 14, 4, 9, 8, 8, 14, 3, 3, 3, 9, 8, 2, 4, 0, 8, 8, 5, 3, 5, 8, 4, 5, 11, 8, 4, 8, 5, 8, 2, 2, 13, 8, 5, 14, 14, 3, 9, 5, 11, 4, 13, 3, 4, 8, 3, 9, 5, 8, 8, 3, 5, 8, 8, 13, 8, 4, 4, 3, 0, 4, 8, 3, 0, 3, 3, 8, 8, 11, 0, 4, 3, 8, 8, 0, 11, 4, 13, 14, 5, 5, 8, 4, 11, 5, 3, 8, 3, 3, 11, 8, 8, 8, 8, 8, 3, 11, 14, 5, 4, 8, 4, 13, 0, 13, 8, 8, 5, 14, 2, 2, 6, 8, 2]\n",
            "dided\n",
            "time\n",
            "[8, 2, 8, 8, 8, 3, 3, 4, 3, 14, 8, 9, 11, 11, 5, 8, 8, 13, 8, 11, 4, 3, 9, 3, 8, 4, 8, 13, 3, 8, 0, 4, 3, 9, 8, 13, 3, 14, 3, 13, 0, 8, 9, 3, 8, 13, 3, 4, 0, 5, 4, 5, 0, 8, 8, 5, 3, 8, 8, 9, 9, 5, 0, 8, 8, 3, 0, 11, 9, 2, 14, 4, 13, 14, 8, 8, 14, 3, 9, 3, 3, 3, 3, 2, 8, 8, 4, 2, 14, 5, 0, 2, 8, 0, 4, 8, 8, 8, 5, 3, 8, 3, 9, 0, 5, 8, 8, 11, 14, 8, 13, 8, 8, 2, 4, 0, 8, 3]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 3, 3, 2, 3, 0, 13, 3, 9, 4, 9, 4, 8, 14, 8, 8, 5, 3, 6, 11, 2, 2, 4, 14, 4, 9, 3, 2, 8, 14, 3, 8, 6, 5, 14, 8, 0, 3, 5, 3, 2, 0, 6, 9, 14, 2, 5, 14, 0, 3, 2, 14, 3, 14, 8, 14, 4, 3, 9, 14, 13, 11, 8, 5, 2, 8, 5, 9, 4, 8, 3, 5, 9, 9, 3, 8, 8, 0, 8, 0, 3, 13, 8, 3, 5, 2, 8, 4, 8, 2, 8, 2, 13, 8, 8, 4, 3, 3, 5, 8, 8, 8, 4, 14, 14, 0, 4, 4, 5, 8, 3, 4, 13, 8, 3, 0, 11, 5, 3, 8, 13, 5]\n",
            "dided\n",
            "time\n",
            "[8, 13, 5, 3, 0, 8, 3, 3, 2, 4, 14, 8, 0, 3, 8, 8, 0, 3, 8, 8, 5, 4]\n",
            "14 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02437947690486908 0.231201171875 3.2809979915618896 0.4301820397377014 1.247361660003662 1.7831627130508423\n",
            "repr, std, cov, clossl, z, norm 0.03531284257769585 0.2294921875 3.354597568511963 0.39313244819641113 1.3660346269607544 1.8947935104370117\n",
            "repr, std, cov, clossl, z, norm 0.020517107099294662 0.2265625 3.499293804168701 0.3787553608417511 1.2747420072555542 1.9595407247543335\n",
            "repr, std, cov, clossl, z, norm 0.02752295508980751 0.2347412109375 3.0961108207702637 0.4459184408187866 1.3064440488815308 1.8070266246795654\n",
            "repr, std, cov, clossl, z, norm 0.02444656565785408 0.23095703125 3.275557279586792 0.4454728364944458 1.2629624605178833 1.8854553699493408\n",
            "repr, std, cov, clossl, z, norm 0.02910182811319828 0.2230224609375 3.6681504249572754 0.46763041615486145 1.2850626707077026 1.6914260387420654\n",
            "repr, std, cov, clossl, z, norm 0.022898567840456963 0.2269287109375 3.488372802734375 0.5159032344818115 1.2373052835464478 2.0106794834136963\n",
            "repr, std, cov, clossl, z, norm 0.036140237003564835 0.2239990234375 3.6511638164520264 0.47886189818382263 1.28842294216156 2.145960807800293\n",
            "repr, std, cov, clossl, z, norm 0.02291843295097351 0.22705078125 3.4806106090545654 0.5037558078765869 1.3356167078018188 2.0981903076171875\n",
            "repr, std, cov, clossl, z, norm 0.02942216955125332 0.2303466796875 3.297726631164551 0.49155572056770325 1.356189250946045 1.813692331314087\n",
            "repr, std, cov, clossl, z, norm 0.023903867229819298 0.230712890625 3.3025388717651367 0.4151118993759155 1.2687759399414062 2.151236057281494\n",
            "repr, std, cov, clossl, z, norm 0.02791311964392662 0.23046875 3.3067235946655273 0.43619030714035034 1.3231369256973267 2.125526189804077\n",
            "train_data.data 20400\n",
            "dided\n",
            "time\n",
            "[4, 2, 8, 3, 11, 8, 11, 4, 4, 8, 9, 8, 8, 6, 3, 5, 9, 14, 11, 8, 3, 8, 6, 14, 3, 2, 8, 11, 8, 9, 11, 5, 8, 8, 9, 7, 2, 3, 9, 3, 0, 3, 13, 11, 8, 3, 3, 8, 11, 9, 11, 8, 3, 2, 8, 0, 5, 8, 9, 0, 4, 4, 8, 14, 3, 14, 0, 9, 11, 11, 5, 0, 0, 0, 8, 14, 14, 4, 8, 3, 8, 3, 8, 4, 6, 4, 3, 0, 9, 14, 2, 3, 8, 2, 8, 3, 8, 8, 0, 9, 8, 0, 8, 9, 8, 14, 4, 8, 3, 8]\n",
            "dided\n",
            "time\n",
            "[8, 0, 8, 8, 3, 3, 2, 8, 2, 2, 8, 0, 4, 9, 9, 3, 14, 14, 0, 11, 5, 9, 2, 11, 2, 5, 3, 3, 13, 9, 5, 4, 8, 8, 8, 3, 13, 11, 3, 3, 4, 8, 3, 2, 3, 4, 8, 14, 11, 4, 11, 14, 11, 14, 14, 14, 0, 0, 8, 8, 2, 11, 3, 8, 14, 1, 5, 6, 0, 4, 0, 8, 5, 4, 0, 8, 8, 3, 4, 5, 3, 4, 14, 4, 3, 3, 2, 3, 2, 3, 3, 8]\n",
            "dided\n",
            "time\n",
            "[3, 3, 8, 4, 4, 8, 3, 9, 5, 6, 3, 11, 8, 14, 2, 8, 9, 14, 14, 9, 4, 13, 3, 8, 3, 8, 9, 8, 3, 11, 3, 3, 8, 0, 4, 11, 4, 14, 8, 8, 4, 3, 3, 14, 2, 2, 6, 3, 8, 8, 14, 2, 3, 8, 0, 4, 5, 0, 9, 0, 8, 14, 11, 14, 11, 14, 11, 3, 3, 2, 8, 5, 3, 0, 8, 3, 0, 3, 8, 2, 11, 0, 11, 8, 14, 2, 4, 13, 14, 8, 9, 4, 2, 8, 14, 5, 3, 8, 3, 3, 3, 8, 8, 8, 8, 6, 2, 4, 8, 5, 13, 8, 0, 3, 8, 0, 11, 3, 0, 5, 13, 8, 8, 3, 0, 3, 3, 5, 2, 5, 9, 8, 0, 8, 8, 11, 8, 3, 0, 6, 3, 0, 8, 2, 13, 3, 8, 0, 8, 3, 3, 3, 8, 4, 14, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 3, 11, 4, 8, 4, 4, 0, 3, 2, 2, 8, 2, 0, 8, 4, 14, 14, 3, 2, 14, 5, 8, 8, 13, 9, 8, 8, 2, 5, 2, 9, 8, 14, 4, 0, 3, 11, 9, 8, 8, 11, 8, 8, 3, 3, 0, 8, 8, 0, 2, 0, 5, 14]\n",
            "dided\n",
            "time\n",
            "[5, 14, 3, 14, 11, 8, 5, 8, 8, 8, 3, 5, 0, 9, 3, 11, 3, 9, 3, 3, 2, 14, 8, 8, 11, 2, 8, 3, 4, 14, 11, 8, 2, 5, 5, 3, 8, 8, 11, 4, 3, 3, 3, 11, 3, 8, 0, 4, 2, 8, 13, 13, 13, 9, 3, 9, 8, 5, 2, 3, 13, 0, 3, 8, 8, 13, 8, 2, 4, 8, 0, 9, 0, 8, 14, 5, 9, 0, 8, 4, 3, 11, 2, 4, 2, 3, 4, 5, 2, 3, 8, 0, 3, 2, 3, 2, 3, 11, 11, 5, 4, 3, 14, 11, 8, 8, 14, 14, 8, 3, 3, 8, 8, 2, 8, 0, 8, 8, 11, 14, 8, 13, 3, 14, 8, 3, 14, 8, 5, 8, 14, 3, 4, 2, 0, 3, 3, 2, 0, 13, 8, 0, 8, 8, 11, 4, 3, 0, 0, 14, 4, 8, 8, 9, 14, 9, 0, 9, 9, 4, 3, 14, 3, 9, 4, 3]\n",
            "15 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.020977236330509186 0.2298583984375 3.3232827186584473 0.36448484659194946 1.301919937133789 1.987878441810608\n",
            "repr, std, cov, clossl, z, norm 0.02747534215450287 0.232421875 3.2218880653381348 0.4181075692176819 1.2439197301864624 2.283031463623047\n",
            "repr, std, cov, clossl, z, norm 0.02402099035680294 0.23046875 3.2869863510131836 0.43465039134025574 1.286439061164856 2.1509432792663574\n",
            "repr, std, cov, clossl, z, norm 0.026652196422219276 0.228759765625 3.385120391845703 0.36790454387664795 1.371635913848877 2.078566789627075\n",
            "repr, std, cov, clossl, z, norm 0.021263260394334793 0.2269287109375 3.4835879802703857 0.48935577273368835 1.3052496910095215 2.1578361988067627\n",
            "repr, std, cov, clossl, z, norm 0.027790173888206482 0.22216796875 3.7202839851379395 0.45131513476371765 1.318672776222229 2.0589406490325928\n",
            "repr, std, cov, clossl, z, norm 0.02371656894683838 0.22705078125 3.4748783111572266 0.367085337638855 1.3010472059249878 1.9193986654281616\n",
            "repr, std, cov, clossl, z, norm 0.027812592685222626 0.2318115234375 3.264193058013916 0.4336718022823334 1.3512810468673706 1.9602322578430176\n",
            "repr, std, cov, clossl, z, norm 0.022719761356711388 0.2313232421875 3.2524633407592773 0.3619276285171509 1.2709378004074097 1.7862149477005005\n",
            "repr, std, cov, clossl, z, norm 0.039839424192905426 0.2308349609375 3.2813141345977783 0.45170801877975464 1.3562549352645874 1.7275642156600952\n",
            "repr, std, cov, clossl, z, norm 0.02965380996465683 0.2230224609375 3.7004494667053223 0.4716455340385437 1.3278156518936157 1.837412714958191\n",
            "repr, std, cov, clossl, z, norm 0.032107360661029816 0.2254638671875 3.5868406295776367 0.47098198533058167 1.3311599493026733 1.8693937063217163\n",
            "train_data.data 20758\n",
            "dided\n",
            "time\n",
            "[3, 6, 8, 3, 0, 8, 8, 8, 4, 9, 5, 8, 9, 3, 4, 3, 8, 8, 0, 11, 3, 8, 0, 3, 14, 0, 14, 3, 3, 3, 8, 8, 8, 8, 4, 4, 8, 2, 2, 8, 8, 9, 11, 14, 3, 8, 8, 4, 6, 13, 11, 14, 4, 14, 3, 4, 0, 8, 8, 2, 4, 4, 9, 8, 8, 4, 8, 14, 0, 0, 8, 8, 8, 14, 14, 6, 14, 11, 3, 0, 0, 6, 3, 3, 2, 14, 8, 3, 8, 3, 8, 8, 3, 8, 4, 3, 8, 4, 14, 14]\n",
            "dided\n",
            "time\n",
            "[4, 14, 14, 11, 3, 3, 14, 5, 3, 5, 0, 8, 9, 3, 4, 9, 0, 11, 3, 9, 14, 6, 8, 3, 8, 8, 8, 8, 5, 4, 2, 14, 2, 9, 8, 8, 14, 8, 3, 13, 11, 8, 4, 8, 8, 9, 14, 8, 8, 11, 3, 8, 14, 8, 3, 0, 0, 3, 0]\n",
            "dided\n",
            "time\n",
            "[3, 0, 4, 9, 4, 2, 13, 14, 13, 8, 4, 4, 8, 14, 5, 8, 8, 8, 8, 2, 8, 3, 14, 14, 8, 8, 8, 2, 0, 5, 2, 8, 11, 8, 3, 3, 11, 14, 3, 8, 8, 4, 2, 9, 8, 8, 4, 2, 8, 6, 0, 4, 3, 14, 8, 5, 5, 3, 3, 5, 8, 2, 8, 14, 8, 2, 3, 5, 3, 8, 8, 8, 8, 13, 4, 8, 0, 3, 3, 5, 4, 8, 9, 2, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 14, 5, 5, 8, 13, 3, 11, 0, 8, 0, 11, 14, 14, 11, 0, 0, 4, 8, 8, 0, 5, 3, 3, 5, 8, 14, 8, 8, 8, 9, 3, 3, 4, 8, 8, 3, 3, 8, 7, 9, 9, 8, 3, 8, 3, 5, 14, 8, 8, 2, 8, 14, 2, 4, 8, 4, 4, 2, 8, 3, 13, 4, 5, 8]\n",
            "dided\n",
            "time\n",
            "[4, 13, 3, 2, 3, 3, 8, 5, 3, 4, 8, 14, 8, 3, 8, 13, 5, 11, 2, 0, 8, 5, 14, 8, 8, 8, 8, 9, 0, 0, 2, 3, 8, 0, 2, 9, 3, 3, 2, 14, 3, 3, 2]\n",
            "16 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.024294691160321236 0.225341796875 3.569840669631958 0.5277284383773804 1.2646580934524536 2.042667865753174\n",
            "repr, std, cov, clossl, z, norm 0.026289023458957672 0.227783203125 3.4497556686401367 0.47850605845451355 1.3417924642562866 1.9674650430679321\n",
            "repr, std, cov, clossl, z, norm 0.022961463779211044 0.2322998046875 3.2270543575286865 0.46260130405426025 1.3597140312194824 1.7836543321609497\n",
            "repr, std, cov, clossl, z, norm 0.030287496745586395 0.233642578125 3.15311861038208 0.39152660965919495 1.3361769914627075 2.006941556930542\n",
            "repr, std, cov, clossl, z, norm 0.024870017543435097 0.2332763671875 3.1824541091918945 0.3868386149406433 1.3773373365402222 1.8010441064834595\n",
            "repr, std, cov, clossl, z, norm 0.0380508117377758 0.228759765625 3.4139461517333984 0.4021066725254059 1.259276270866394 1.4389879703521729\n",
            "repr, std, cov, clossl, z, norm 0.025079570710659027 0.227294921875 3.4711952209472656 0.42464399337768555 1.2834886312484741 1.415871262550354\n",
            "repr, std, cov, clossl, z, norm 0.040034521371126175 0.228271484375 3.4127354621887207 0.3849217891693115 1.351056694984436 1.845922589302063\n",
            "repr, std, cov, clossl, z, norm 0.027253607288002968 0.2281494140625 3.421943187713623 0.45948362350463867 1.3817365169525146 1.4622697830200195\n",
            "repr, std, cov, clossl, z, norm 0.0711519718170166 0.2283935546875 3.4075145721435547 0.423287957906723 1.1947643756866455 1.538454532623291\n",
            "repr, std, cov, clossl, z, norm 0.02456655539572239 0.231201171875 3.265091896057129 0.4486384689807892 1.2921463251113892 2.089559316635132\n",
            "repr, std, cov, clossl, z, norm 0.04561242833733559 0.2281494140625 3.431511163711548 0.491487056016922 1.2590749263763428 1.6870824098587036\n",
            "train_data.data 20906\n",
            "dided\n",
            "time\n",
            "[3, 2, 3, 2, 11, 14, 3, 8, 3, 8, 8, 14, 9, 8, 11, 8, 8, 8, 4, 8, 8, 8, 9, 3, 14, 7, 8, 3, 11, 8, 9, 11, 3, 3, 13, 11, 3, 0, 3, 6, 4, 0, 8, 6, 3, 11, 2, 4, 14, 3, 11, 8, 2, 8, 0, 3, 2, 4, 8, 3, 14, 3, 11, 5, 5, 2, 8, 0]\n",
            "dided\n",
            "time\n",
            "[2, 8, 0, 0, 4, 5, 8, 14, 4, 3, 9, 3, 0, 3, 11, 5, 8, 3, 5, 3, 3, 4, 9, 14, 8, 13, 5, 14, 2]\n",
            "dided\n",
            "time\n",
            "[0, 3, 8, 11, 4, 14, 8, 2, 0, 0, 6, 0, 11, 9, 11, 3, 9, 0, 8, 5, 13, 4, 14, 3, 8, 14, 0, 4, 11, 8, 8, 3, 13, 2, 8, 0, 3, 0, 5, 3, 3, 9, 2, 3, 9, 3, 13, 8, 0, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 8, 3, 7, 2, 8, 8, 13, 3, 3, 8, 8, 8, 0, 3, 3, 14, 8, 0, 7, 8, 14, 9, 8, 5, 11, 6, 2, 3, 3, 0, 8, 13, 8, 8, 8, 8, 8, 4, 6, 3, 8, 14, 8, 8, 2, 3, 8, 3, 11, 3, 14, 8, 4, 8, 0, 9, 8, 13, 9]\n",
            "dided\n",
            "time\n",
            "[9, 6, 8, 8, 8, 3, 8, 4, 8, 2, 0, 8, 14, 8, 3, 3, 8, 3, 14, 8, 4, 8, 2, 3, 8, 3, 0, 8, 14, 8, 2, 8, 3, 8, 14, 14, 5, 6, 8, 8, 3, 8, 0, 8, 3, 8, 5, 0, 4, 11, 8, 9, 8, 5, 8, 13, 8]\n",
            "17 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.023466838523745537 0.22900390625 3.366807222366333 0.515995442867279 1.2225959300994873 1.5330109596252441\n",
            "repr, std, cov, clossl, z, norm 0.04107869043946266 0.234130859375 3.1402230262756348 0.483138769865036 1.2821487188339233 1.973456621170044\n",
            "repr, std, cov, clossl, z, norm 0.023654531687498093 0.2340087890625 3.122159004211426 0.4752632975578308 1.2608734369277954 1.8046022653579712\n",
            "repr, std, cov, clossl, z, norm 0.03086654655635357 0.2305908203125 3.3024635314941406 0.43918848037719727 1.370351791381836 1.6662547588348389\n",
            "repr, std, cov, clossl, z, norm 0.026233749464154243 0.225830078125 3.5465087890625 0.43216514587402344 1.315356731414795 2.0865516662597656\n",
            "repr, std, cov, clossl, z, norm 0.036562371999025345 0.2269287109375 3.4650001525878906 0.5337997674942017 1.2842727899551392 1.789681315422058\n",
            "repr, std, cov, clossl, z, norm 0.02032703533768654 0.2261962890625 3.515479326248169 0.44217243790626526 1.273699164390564 1.4879751205444336\n",
            "repr, std, cov, clossl, z, norm 0.023647738620638847 0.2293701171875 3.374129295349121 0.4228993356227875 1.3182744979858398 1.9930247068405151\n",
            "repr, std, cov, clossl, z, norm 0.01877673901617527 0.228759765625 3.3915042877197266 0.40035468339920044 1.2813011407852173 2.026860237121582\n",
            "repr, std, cov, clossl, z, norm 0.025161854922771454 0.2274169921875 3.455472946166992 0.41651949286460876 1.25357985496521 1.6905580759048462\n",
            "repr, std, cov, clossl, z, norm 0.01693815551698208 0.2269287109375 3.476558208465576 0.40641582012176514 1.292720913887024 1.7340694665908813\n",
            "repr, std, cov, clossl, z, norm 0.021028095856308937 0.2281494140625 3.4382641315460205 0.4072420001029968 1.3802651166915894 1.8028185367584229\n",
            "train_data.data 20019\n",
            "dided\n",
            "time\n",
            "[3, 3, 3, 0, 2, 2, 0, 9, 5, 8, 14, 14, 8, 8, 0, 5, 2, 3, 3, 0, 2, 14, 5, 0, 2, 9]\n",
            "dided\n",
            "time\n",
            "[9, 3, 8, 8, 3, 4, 8, 9, 0, 9, 3, 4, 8, 13, 8, 0, 4, 5, 5, 3, 11, 6, 5, 14, 8, 0, 14, 0, 8, 8, 11, 9, 8, 11, 5, 13, 8, 8, 6, 0, 8, 2, 8, 5, 8, 3, 11, 14, 3, 8, 8, 3, 11, 2, 5, 8, 9, 9, 0, 8, 4, 4, 5, 5, 8, 3, 11, 8, 3, 14, 3, 8, 0, 8, 14, 8, 8, 14, 3, 8, 2, 9, 11, 8, 11, 4, 5, 0, 8, 3, 4, 2, 8, 8, 7, 14, 6, 3, 14, 3, 14, 8, 8, 0, 2, 13, 3, 3, 11, 5, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[2, 8, 8, 14, 8, 11, 8, 5, 8, 13, 11, 3, 8, 9, 8, 0, 14, 2, 9, 3, 7, 3, 0, 8, 11, 14, 3, 14, 9, 3, 3, 8, 8, 0, 8, 13, 0, 8, 4, 0, 8, 13, 8, 8, 8, 5, 4, 4, 8, 2, 0, 0, 8, 4, 9, 2, 2, 2, 3, 3, 8, 9, 0, 0, 8, 3, 3, 14, 3, 4, 14, 4, 14, 8, 9, 8, 0, 5, 9, 8, 8, 3, 14, 4, 3, 0, 9, 4, 0, 13, 8, 3, 8, 4, 2, 3, 8, 4, 3, 8, 3, 3, 3, 0, 4, 3, 8, 8, 8, 2, 0, 9, 0, 3, 2, 14, 13, 8, 8, 9, 5, 8, 8, 8, 9, 14, 4, 4, 3, 4, 3, 4]\n",
            "dided\n",
            "time\n",
            "[4, 3, 4, 8, 14, 4, 6, 8, 13, 14, 4, 8, 0, 3, 11, 5, 14, 8, 0, 8, 3, 2, 4, 14, 11, 14, 14, 5, 5, 8, 8, 8, 4, 2, 2, 8, 4, 2, 8, 13, 11, 11, 8, 4, 14, 4, 11, 3, 4, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 5, 0, 8, 0, 4, 8, 0, 11, 2, 2, 3, 8, 3, 8, 5, 5, 5, 8, 8, 3, 4, 0, 2, 3, 5, 8, 3, 8, 3, 5, 8, 3, 5, 2, 9, 13, 4, 8, 6, 8, 8, 5, 0, 6, 8, 11, 3, 8, 3, 4, 2, 9, 5, 5, 8, 8, 11, 8, 14, 6, 3, 0, 0, 3, 8, 8, 8, 8, 8, 8, 0, 2, 9, 0, 13, 3, 14, 8, 0, 2, 3, 8, 4, 0, 8, 2, 9, 8, 0, 8, 14, 8, 6, 3, 9, 8, 8, 8, 6, 8, 8, 2, 8, 7, 5, 0, 3, 13, 5]\n",
            "18 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.017246661707758904 0.2303466796875 3.328307628631592 0.4835886061191559 1.3987395763397217 1.7570000886917114\n",
            "repr, std, cov, clossl, z, norm 0.024950847029685974 0.229248046875 3.3590779304504395 0.40031901001930237 1.292405366897583 1.5847060680389404\n",
            "repr, std, cov, clossl, z, norm 0.02466520667076111 0.2249755859375 3.5738823413848877 0.41676703095436096 1.2633843421936035 1.7478103637695312\n",
            "repr, std, cov, clossl, z, norm 0.028408493846654892 0.2335205078125 3.1347603797912598 0.468270480632782 1.3021049499511719 1.9980974197387695\n",
            "repr, std, cov, clossl, z, norm 0.020517826080322266 0.232666015625 3.1898202896118164 0.42578232288360596 1.3599793910980225 2.020515203475952\n",
            "repr, std, cov, clossl, z, norm 0.027846364304423332 0.2293701171875 3.366260290145874 0.4385825991630554 1.2541158199310303 2.0198841094970703\n",
            "repr, std, cov, clossl, z, norm 0.023566145449876785 0.2283935546875 3.4151759147644043 0.4691988527774811 1.3691104650497437 2.2562737464904785\n",
            "repr, std, cov, clossl, z, norm 0.03453052043914795 0.2264404296875 3.5332231521606445 0.5341334342956543 1.2803033590316772 2.36316180229187\n",
            "repr, std, cov, clossl, z, norm 0.027334587648510933 0.228759765625 3.392683267593384 0.46608951687812805 1.3313984870910645 2.1342358589172363\n",
            "repr, std, cov, clossl, z, norm 0.02851269580423832 0.2340087890625 3.134028434753418 0.4676482677459717 1.2697921991348267 1.9373341798782349\n",
            "repr, std, cov, clossl, z, norm 0.028001166880130768 0.2344970703125 3.1253061294555664 0.4434750974178314 1.3311618566513062 2.017575263977051\n",
            "repr, std, cov, clossl, z, norm 0.03205769881606102 0.22998046875 3.3373899459838867 0.376984566450119 1.349541187286377 2.515272617340088\n",
            "train_data.data 20926\n",
            "dided\n",
            "time\n",
            "[13, 5, 8, 11, 8, 3, 5, 9, 11, 8, 6, 8, 0, 3, 9, 3, 3, 14, 14, 14, 5, 8, 4, 3, 3, 11, 2, 8, 7, 3, 8, 14, 3, 2, 14, 5, 3, 3, 14, 8, 8, 0, 8, 5, 0, 8, 11, 8, 0, 8, 3, 9, 8, 3, 0, 8, 4, 6, 8, 3, 14, 0, 0, 8, 4, 8, 8, 8, 8, 5, 9, 14, 3, 14, 3, 5, 8, 0, 8, 14, 14, 3, 8, 9, 7, 0, 2, 3, 8, 8, 4, 11, 3, 11, 13, 9, 8, 8, 8, 8, 2, 3, 4, 13, 6, 4, 8, 14, 3, 3, 3, 8, 8, 9, 8, 8, 2, 0, 4, 3, 6, 9, 8, 4, 0, 8, 8, 4, 5, 8, 8, 3, 5, 8, 2, 3, 4, 8, 4, 14, 5, 4, 3, 4, 11, 8, 8, 4, 3, 14, 14, 14, 8, 5, 0, 0, 0, 8, 0, 4, 3, 2, 4, 8, 13, 8, 13, 8, 13, 8, 8, 3, 8, 13, 9, 4, 2, 2, 3, 8, 9, 0, 13, 0, 8, 9, 8, 8, 2, 3, 11, 11, 14, 3, 4, 4, 13, 4, 14, 5, 9, 11]\n",
            "dided\n",
            "time\n",
            "[11, 8, 2, 14, 14, 8, 5, 3, 14, 11, 0, 8, 5, 3, 1, 8, 13, 4, 14, 8, 2, 8, 0, 8, 14, 4, 6, 3, 8, 5, 2, 13, 9, 3, 11, 8, 8]\n",
            "dided\n",
            "time\n",
            "[9, 4, 2, 8, 14, 0, 5, 3, 3, 5, 3, 3, 3, 4, 5, 14, 3, 8, 8, 8, 3, 8, 4, 13, 5, 9, 8, 13, 0, 13, 8, 4, 2, 3, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 5, 14, 8, 13, 8, 8, 5, 4, 8, 3, 8, 11, 0, 11, 0, 5, 4, 14, 13, 9, 9, 8, 2, 7, 4, 8, 8, 14, 8, 8, 4, 12, 14, 9, 8, 8, 11, 0, 9, 3, 2, 8, 8, 13, 8, 3, 5, 8, 9, 8, 11, 0, 5, 14, 8, 4, 0, 3, 0, 8, 3, 8, 14, 9, 9, 9, 14, 8, 13, 8, 11, 0, 8, 8, 14, 3, 11, 4, 3, 3, 2, 2, 0, 5, 3, 3, 11, 3, 3, 3, 8, 9, 3, 2, 3, 0, 4, 0, 4, 4, 8, 8, 4, 8, 6, 8, 5, 8, 13, 0, 8, 13]\n",
            "dided\n",
            "time\n",
            "[13, 3, 3, 4, 3, 8, 5, 8, 3, 3, 8, 14, 4, 0, 3, 5, 14, 8, 14, 8, 8, 4, 3, 2, 8, 13, 9, 0, 3, 2, 3, 8, 5, 8, 4, 9, 3, 3, 2, 14, 4, 0, 8, 0, 8, 3, 5, 5, 3, 8, 14, 9, 4, 0, 2, 5, 14, 13, 13, 4, 8, 8, 9, 6, 8]\n",
            "19 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.024933762848377228 0.2255859375 3.565504550933838 0.5004581212997437 1.2164186239242554 2.114684820175171\n",
            "repr, std, cov, clossl, z, norm 0.03034784458577633 0.2276611328125 3.4396204948425293 0.4123971164226532 1.2977079153060913 2.0305938720703125\n",
            "repr, std, cov, clossl, z, norm 0.023089248687028885 0.23388671875 3.1452174186706543 0.47649627923965454 1.2881978750228882 2.094391345977783\n",
            "repr, std, cov, clossl, z, norm 0.030095521360635757 0.2332763671875 3.17612886428833 0.4541093409061432 1.2339366674423218 1.609165072441101\n",
            "repr, std, cov, clossl, z, norm 0.025256173685193062 0.232177734375 3.217586040496826 0.4029996693134308 1.3611725568771362 2.3058602809906006\n",
            "repr, std, cov, clossl, z, norm 0.03661514073610306 0.2271728515625 3.469517707824707 0.47426488995552063 1.3455173969268799 1.9371910095214844\n",
            "repr, std, cov, clossl, z, norm 0.021340884268283844 0.222412109375 3.7425897121429443 0.4768086373806 1.3596042394638062 2.1334705352783203\n",
            "repr, std, cov, clossl, z, norm 0.03191789984703064 0.22705078125 3.4732842445373535 0.3830142617225647 1.2853831052780151 2.1155343055725098\n",
            "repr, std, cov, clossl, z, norm 0.024222053587436676 0.2318115234375 3.2401041984558105 0.42102402448654175 1.3167425394058228 2.068986177444458\n",
            "repr, std, cov, clossl, z, norm 0.0344453863799572 0.2314453125 3.2547049522399902 0.40722358226776123 1.4051547050476074 1.8861316442489624\n",
            "repr, std, cov, clossl, z, norm 0.02276887558400631 0.226318359375 3.5096869468688965 0.3725924789905548 1.3491655588150024 1.9576566219329834\n",
            "repr, std, cov, clossl, z, norm 0.038470882922410965 0.224609375 3.6012871265411377 0.5065472722053528 1.3450250625610352 1.9645743370056152\n",
            "train_data.data 20071\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 8, 11, 4, 8, 8, 11, 3, 8, 0, 3, 3, 14, 0, 4, 5, 3, 9, 9, 8, 8, 13, 2, 2, 3, 14, 3, 5, 8, 10, 3, 8, 9, 14, 4, 8, 8, 8, 0, 1, 13, 2, 14, 3, 14, 4, 13, 0, 9, 2, 13, 8, 14, 4]\n",
            "dided\n",
            "time\n",
            "[8, 14, 4, 0, 8, 13, 8, 14, 14, 14, 3, 5, 8, 0, 3, 0, 14, 4, 5, 14, 8, 4, 8, 3, 8, 6, 8]\n",
            "dided\n",
            "time\n",
            "[6, 8, 5, 0, 3, 11, 10, 8, 4, 3, 6, 8, 8, 9, 0, 14, 5, 8, 0, 11, 4, 3, 8, 0, 3, 14, 13, 14, 8, 0, 8, 8, 13, 3, 3, 8, 4, 8, 3, 0, 8, 11, 0, 4, 3, 5, 5, 0, 4, 0, 0, 8, 9, 3, 3, 14, 2, 3, 0, 14, 8, 3, 8, 3, 0, 8, 2, 9, 0, 5, 8, 11, 8, 5, 8, 8, 8, 14]\n",
            "dided\n",
            "time\n",
            "[14, 8, 8, 8, 2, 2, 8, 8, 4, 5, 14, 3, 8, 3, 5, 0, 2, 2, 5, 5, 5, 5, 0, 14, 14, 9, 14, 3, 4, 8, 3, 0, 8, 8, 3, 13, 8, 8, 3, 4, 9, 5, 0, 3, 2, 14, 13, 5, 14, 13, 3, 0, 11, 5, 2, 0, 3, 14, 3, 8, 8, 9, 13, 14, 3, 9, 8, 2]\n",
            "dided\n",
            "time\n",
            "[9, 8, 2, 2, 8, 8, 8, 14, 5, 4, 9, 3, 3, 8, 5, 8, 8, 3, 5, 13, 3, 13, 3, 8, 8, 0, 5, 8, 3, 0, 8, 5, 3, 5, 8, 3, 8, 9, 14, 2, 3, 8, 0, 0, 8, 8, 8, 5, 13, 13, 11, 11, 9, 8, 4, 14, 9, 3, 5, 2, 7, 13, 6, 3, 14, 2, 3, 14, 0, 8, 5, 13, 8, 4, 8, 4, 13, 8, 0, 8, 0, 4, 8, 5, 8, 8, 14, 9, 8, 4, 3, 2, 8, 13, 2, 14, 2, 11, 14, 5, 2, 8, 8, 3, 0, 3, 3]\n",
            "20 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02728155627846718 0.2232666015625 3.688267230987549 0.46608516573905945 1.2211158275604248 1.7544885873794556\n",
            "repr, std, cov, clossl, z, norm 0.03384743258357048 0.2333984375 3.173039436340332 0.5269307494163513 1.389276385307312 2.2289750576019287\n",
            "repr, std, cov, clossl, z, norm 0.023262662813067436 0.239990234375 2.8769888877868652 0.5003501772880554 1.3654299974441528 2.407559633255005\n",
            "repr, std, cov, clossl, z, norm 0.037291985005140305 0.229248046875 3.3747520446777344 0.47995778918266296 1.25258207321167 2.4777474403381348\n",
            "repr, std, cov, clossl, z, norm 0.023737534880638123 0.2218017578125 3.7849512100219727 0.41109898686408997 1.3681731224060059 2.4242453575134277\n",
            "repr, std, cov, clossl, z, norm 0.030081788077950478 0.2230224609375 3.6858887672424316 0.4313943088054657 1.3383785486221313 2.052652597427368\n",
            "repr, std, cov, clossl, z, norm 0.022475847974419594 0.22900390625 3.368110418319702 0.4138678312301636 1.3609281778335571 1.8419240713119507\n",
            "repr, std, cov, clossl, z, norm 0.035794246941804886 0.236083984375 3.026798963546753 0.4680648446083069 1.3347749710083008 2.258965015411377\n",
            "repr, std, cov, clossl, z, norm 0.023970140144228935 0.226806640625 3.4996232986450195 0.48758265376091003 1.3172321319580078 2.3088417053222656\n",
            "repr, std, cov, clossl, z, norm 0.03288242593407631 0.223876953125 3.6492974758148193 0.5425607562065125 1.3156743049621582 1.8763271570205688\n",
            "repr, std, cov, clossl, z, norm 0.02571525238454342 0.22265625 3.7068729400634766 0.4410010278224945 1.443307876586914 1.5562124252319336\n",
            "repr, std, cov, clossl, z, norm 0.03074602596461773 0.2261962890625 3.5165281295776367 0.5097915530204773 1.2995706796646118 2.2290518283843994\n",
            "train_data.data 20775\n",
            "dided\n",
            "time\n",
            "[3, 3, 9, 8, 9, 3, 0, 0, 4, 3, 11, 4, 5, 4, 14, 8, 2, 13, 9, 13, 8, 0, 8, 8, 9, 2, 8, 8, 3, 4, 8, 3, 2, 6, 8, 8, 4, 8, 8, 9, 4, 3, 14, 8, 9, 8, 3, 3, 4, 13, 8, 8, 4, 13, 0, 14, 14, 3, 4, 8, 9, 8, 14, 5, 4, 8, 3, 2, 14, 11, 3, 8, 2, 11, 4, 9, 8, 2, 3, 8, 4, 8, 8, 8, 13, 8, 14, 8, 4, 11, 10, 11, 4, 9, 11, 8, 4, 2, 8, 9, 8, 8, 2, 8, 12, 4, 8, 8, 14, 4, 14, 8, 13, 9, 3, 6, 14, 8]\n",
            "dided\n",
            "time\n",
            "[4, 9, 11, 14, 4, 4, 14, 13, 4, 8, 8, 2, 13, 3, 8, 8, 2, 14, 11, 4, 7, 3, 2, 8, 4, 3, 6, 0, 4, 2, 8, 3, 8, 8, 8, 3, 3, 6, 8, 5, 2, 8, 3, 5, 2, 3, 4, 8, 14, 5, 0, 0, 5, 0, 8, 3, 4, 5, 3, 8, 8, 3, 3, 14, 9, 4, 8, 0, 8, 4, 8, 8, 9, 0, 9, 3, 4, 2, 2, 8, 6, 9, 3, 8, 9, 3, 8, 2, 0, 8, 4, 4, 2, 3, 8, 8, 4, 11, 5, 2, 13, 14, 2, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 2, 8, 2, 9, 8, 3, 9, 2, 5, 3, 6, 4, 11, 5, 11, 9, 11, 2, 8, 8, 8, 13, 13, 9, 11, 0, 8, 4, 4, 13, 9, 2, 0, 14, 13, 8, 8, 8, 8, 4, 8, 13, 3, 9, 0, 8, 8, 11, 9, 12, 8, 4, 11, 14, 0, 4]\n",
            "dided\n",
            "time\n",
            "[8, 3, 8, 8, 0, 4, 3, 2, 9, 14, 8, 8, 4, 14, 8, 6, 4, 6, 8, 9, 9, 8, 0, 3, 9, 13, 4, 3, 14, 13, 0, 4, 9, 4, 5, 4, 8, 2, 3, 3, 9, 2, 13, 3, 2, 8, 0, 14, 9, 8, 14, 3, 3, 4, 0, 5, 3, 8, 13, 5, 0, 4, 9, 4, 4, 4, 8, 0, 5, 8, 8, 4]\n",
            "dided\n",
            "time\n",
            "[8, 8, 4, 4, 4, 0, 3, 4, 3, 3, 9, 3, 3, 3, 3, 8, 14, 3, 2, 0, 3, 11, 3, 0, 9, 0, 5, 3, 3, 0, 3, 0, 9, 0, 5, 14, 13, 8, 3, 5, 8, 13, 8, 8, 3, 8, 14, 8, 3, 8, 13, 2, 8, 0, 2, 14, 8, 9, 14, 2, 9, 8, 0, 14, 3, 9, 14, 3, 6, 2, 3, 3, 9, 5, 3, 3, 0, 3, 3, 0, 4, 9, 11, 12, 3, 8, 8, 8, 5]\n",
            "21 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.023045295849442482 0.23486328125 3.1099088191986084 0.5126065611839294 1.2998348474502563 1.7309684753417969\n",
            "repr, std, cov, clossl, z, norm 0.025246523320674896 0.23583984375 3.0608558654785156 0.4837666153907776 1.246376872062683 1.669614553451538\n",
            "repr, std, cov, clossl, z, norm 0.02153889089822769 0.237548828125 2.9864211082458496 0.42868679761886597 1.315906047821045 1.3042830228805542\n",
            "repr, std, cov, clossl, z, norm 0.02366502769291401 0.2303466796875 3.321394443511963 0.4394482970237732 1.2784290313720703 2.0489280223846436\n",
            "repr, std, cov, clossl, z, norm 0.022049764171242714 0.2235107421875 3.6534669399261475 0.4309993386268616 1.2101138830184937 1.74065363407135\n",
            "repr, std, cov, clossl, z, norm 0.027891777455806732 0.2252197265625 3.55330753326416 0.40661248564720154 1.3313133716583252 1.6604773998260498\n",
            "repr, std, cov, clossl, z, norm 0.022057494148612022 0.22607421875 3.5475869178771973 0.3654165267944336 1.3471630811691284 1.6217163801193237\n",
            "repr, std, cov, clossl, z, norm 0.03017408773303032 0.227294921875 3.4715704917907715 0.42498278617858887 1.3242844343185425 1.783899188041687\n",
            "repr, std, cov, clossl, z, norm 0.020509976893663406 0.2320556640625 3.245546340942383 0.3810482919216156 1.3411835432052612 1.5959025621414185\n",
            "repr, std, cov, clossl, z, norm 0.026557328179478645 0.23095703125 3.279473066329956 0.43807947635650635 1.2417871952056885 1.6035734415054321\n",
            "repr, std, cov, clossl, z, norm 0.022357039153575897 0.2293701171875 3.3595263957977295 0.4418973922729492 1.2719016075134277 1.8073726892471313\n",
            "repr, std, cov, clossl, z, norm 0.02832426317036152 0.2325439453125 3.2003979682922363 0.5413550138473511 1.3072071075439453 1.7887438535690308\n",
            "train_data.data 21201\n",
            "dided\n",
            "time\n",
            "[4, 8, 11, 0, 3, 3, 2, 0, 4, 8, 8, 8, 3, 6, 4, 3, 8, 4, 5, 13, 5, 13, 3, 5, 4, 8, 2, 8, 13, 2, 0, 0, 0, 14, 2, 7, 8, 4, 14, 4, 8, 3, 8, 3, 14, 0, 8, 9, 0, 13, 4, 5, 0, 13, 9, 2, 14, 8, 3, 14, 11, 14, 3]\n",
            "dided\n",
            "time\n",
            "[14, 3, 6, 2, 3, 2, 8, 5, 4, 3, 8, 13, 8, 4, 8, 2, 0, 14, 14, 2, 3, 11, 8, 3, 3, 4, 4, 3, 13, 4, 8, 8, 8, 0, 3, 9, 3, 14, 2, 8, 3, 5, 14, 13, 14, 8, 3, 8, 0, 3, 3, 4, 8, 8, 3, 3, 11, 2, 2, 2, 8, 3, 2, 8, 0, 6, 0, 0, 3, 0, 8, 3, 3, 8, 8, 8, 0, 3, 0, 8, 9, 14, 8, 3, 3, 5, 3, 8, 5, 8, 8, 5, 0, 8, 4, 11, 14, 8, 8, 14, 3, 14, 5, 3, 0, 11, 5, 5, 5, 11, 5]\n",
            "dided\n",
            "time\n",
            "[11, 5, 13, 8, 0, 8, 2, 8, 0, 8, 5, 8, 14, 8, 3, 3, 8, 13, 3, 14, 0, 14, 4, 14, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 0, 3, 3, 14, 11, 3, 14, 8, 0, 8, 3, 8, 4, 0, 5, 14, 14, 0, 3, 14, 8, 4, 14, 0, 8, 8, 2, 8, 8, 8, 0, 8, 0, 8, 3, 8, 13, 3, 5, 3, 8, 5, 0, 8, 14, 0, 3, 2, 4, 4, 11, 3, 14, 8, 13, 3, 8, 13, 8, 8, 8, 0, 8, 8, 0, 8, 5, 8, 0, 3, 8, 3, 3, 8, 0, 0, 8, 3, 0, 13, 14, 6, 8, 3, 4, 5, 4, 8, 0, 8, 9, 8, 8, 1, 14, 8, 3, 8, 8, 14, 4, 5, 2, 0, 8, 14, 8, 5, 2, 3, 2, 8, 6, 8, 14, 3, 4, 0, 0, 5, 13, 11, 2, 8, 8, 3, 13, 14, 9, 11, 4, 8, 8, 5, 0, 2, 3, 8, 5, 8, 8, 8, 3]\n",
            "dided\n",
            "time\n",
            "[8, 8, 3, 4, 5, 0, 0, 8, 2, 4, 3, 5, 3, 3, 8, 3, 3, 8, 8, 3, 13, 1, 3, 4, 8, 14, 9, 3, 11, 0, 2, 5, 13, 4, 3, 8, 8, 4, 9, 3, 3, 14, 1, 5, 0, 0, 9, 9, 5, 3, 13, 0, 14, 13, 2, 4, 8, 8, 13, 8, 4, 8, 3, 3, 8, 8, 4, 8, 3, 6, 3, 5, 3, 3, 8, 5, 8, 14, 9, 8, 8, 8, 3, 3, 0]\n",
            "22 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02372095361351967 0.2337646484375 3.158536911010742 0.4487737715244293 1.3295519351959229 1.9668608903884888\n",
            "repr, std, cov, clossl, z, norm 0.03644609451293945 0.232177734375 3.2391180992126465 0.46034348011016846 1.2835816144943237 1.731926679611206\n",
            "repr, std, cov, clossl, z, norm 0.024783451110124588 0.224609375 3.6262259483337402 0.4048144221305847 1.33124840259552 1.7884809970855713\n",
            "repr, std, cov, clossl, z, norm 0.03362453728914261 0.2265625 3.4930009841918945 0.487104594707489 1.3465477228164673 2.17256760597229\n",
            "repr, std, cov, clossl, z, norm 0.023741120472550392 0.2298583984375 3.342524528503418 0.4357489049434662 1.288879156112671 2.3531131744384766\n",
            "repr, std, cov, clossl, z, norm 0.02626609057188034 0.2247314453125 3.60383939743042 0.4742712676525116 1.3734444379806519 2.109625816345215\n",
            "repr, std, cov, clossl, z, norm 0.022400101646780968 0.2230224609375 3.6712653636932373 0.36849290132522583 1.3203048706054688 2.1400489807128906\n",
            "repr, std, cov, clossl, z, norm 0.024556845426559448 0.2275390625 3.446584463119507 0.401516318321228 1.2459361553192139 1.9040441513061523\n",
            "repr, std, cov, clossl, z, norm 0.021040014922618866 0.2305908203125 3.2880921363830566 0.42639145255088806 1.3925672769546509 2.077164888381958\n",
            "repr, std, cov, clossl, z, norm 0.024976953864097595 0.22998046875 3.315284013748169 0.4119904041290283 1.220847249031067 1.946879267692566\n",
            "repr, std, cov, clossl, z, norm 0.026847243309020996 0.2286376953125 3.389211654663086 0.5536951422691345 1.2730265855789185 2.1911585330963135\n",
            "repr, std, cov, clossl, z, norm 0.02426161803305149 0.2301025390625 3.3182902336120605 0.4066815674304962 1.2859920263290405 2.455853223800659\n",
            "train_data.data 20651\n",
            "dided\n",
            "time\n",
            "[8, 3, 8, 3, 0, 11, 8, 2, 9, 8, 5, 8, 5, 3, 11, 8, 5, 5, 8, 11, 3, 8, 3, 3, 8, 5, 2, 5, 8, 4, 8, 8, 3, 0, 8, 0, 8, 9, 4, 3, 13, 13, 3, 8, 3, 9, 13, 4, 0, 8, 0, 8, 4, 3, 3, 13, 9, 3, 8, 0, 3, 11, 9, 8, 0, 4, 5, 8, 0, 8, 8, 3, 0, 2, 5, 11, 8, 4, 4, 3, 3, 4, 14, 8, 6, 2, 0, 13, 8, 4, 3, 14, 8, 3, 11, 8, 3, 14, 8, 3, 3, 8, 3, 0, 4, 13, 13, 8, 3, 14, 8, 0, 8, 8, 8, 14, 8, 0, 3, 8, 8, 5, 4, 8, 0, 3, 5, 9, 11, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 0, 11, 0, 3, 8, 14, 0, 8, 9, 11, 0, 8, 0, 0, 2, 3, 8, 2, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 3, 14, 3, 8, 3, 8, 0, 0, 8, 4, 3, 8, 0, 3, 5, 3, 9, 8, 8, 6, 0, 8, 8, 5, 3, 8, 8, 8, 8, 14, 14, 14, 8, 3, 14, 0, 3, 13, 3, 2, 8, 14, 11, 8, 14, 14, 4, 4, 8, 3, 2, 7, 5, 2, 11, 8]\n",
            "dided\n",
            "time\n",
            "[8, 14, 0, 8, 8, 0, 13, 3, 8, 4, 13, 8, 13, 8, 3, 8, 3, 5, 13, 0, 0, 5, 2, 8, 9, 11, 5, 5, 2, 11, 3, 8, 8, 8, 8, 0, 8, 3, 8, 14, 8, 4, 3, 14, 0, 3, 2, 5, 2, 13, 8, 13, 14, 8, 8, 5, 0, 8, 3, 0, 4, 14, 11, 3, 5, 13, 3, 5, 8, 13, 8, 8, 8, 2, 3, 5, 0, 5, 3, 11, 6, 8, 3, 8, 2, 2, 4, 8, 8, 8, 0, 8, 13, 8, 9]\n",
            "dided\n",
            "time\n",
            "[8, 9, 2, 0, 4, 14, 2, 4, 8, 11, 8, 8, 5, 13, 3, 8, 13, 8, 0, 9, 4, 0, 5, 8, 8, 14, 3, 5, 8, 8, 3, 8, 0, 2, 4, 14, 8, 8, 8, 0, 14, 8, 8, 5, 6, 7, 8, 3, 3, 8, 9, 9, 4, 0, 8, 8, 8, 4, 3, 5, 3, 0, 8, 2, 8, 5, 0, 3, 0, 3, 14, 11, 4, 3, 8, 8, 0, 8, 8, 11, 8, 4, 8, 3, 3, 12, 3, 4, 8, 8, 3, 2, 8, 5, 4, 8, 4, 4, 8, 3, 8, 3, 2, 8, 3, 4, 8, 5, 3, 8, 11, 0, 0, 11, 3, 8, 8, 8, 11, 8, 0, 3, 0, 4, 0, 3, 6, 5, 8, 3, 13, 2, 8, 5, 6, 14, 1, 5, 8, 8, 3, 3, 0, 14, 14, 11, 0, 4, 11, 3]\n",
            "dided\n",
            "time\n",
            "[3, 4, 14, 0, 0, 5, 13, 3, 9, 8, 3, 3, 3, 2, 4, 8, 6, 5, 3, 3, 0, 3, 14, 5, 9, 8, 0, 0, 2, 3, 2, 2, 3, 8, 3, 0, 3, 14, 8, 8, 11, 8, 4, 13, 4, 14, 14, 3, 8, 3, 11, 0, 6, 3, 14, 5, 8, 3, 11, 6, 3, 3, 14, 3, 8, 8, 8, 4, 14, 8, 0, 3, 8, 8]\n",
            "23 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.026243416592478752 0.224365234375 3.6151230335235596 0.402402400970459 1.2675752639770508 1.8843278884887695\n",
            "repr, std, cov, clossl, z, norm 0.025536593049764633 0.23388671875 3.161813259124756 0.4784758388996124 1.3600014448165894 1.85051429271698\n",
            "repr, std, cov, clossl, z, norm 0.02613375149667263 0.232421875 3.2312934398651123 0.419685035943985 1.3177595138549805 1.6833291053771973\n",
            "repr, std, cov, clossl, z, norm 0.024857496842741966 0.2305908203125 3.2942514419555664 0.40796786546707153 1.3456127643585205 2.020928144454956\n",
            "repr, std, cov, clossl, z, norm 0.025885455310344696 0.2288818359375 3.383279323577881 0.40228739380836487 1.2058104276657104 1.4542815685272217\n",
            "repr, std, cov, clossl, z, norm 0.02898908592760563 0.2230224609375 3.688650369644165 0.4211835563182831 1.3423256874084473 1.5003700256347656\n",
            "repr, std, cov, clossl, z, norm 0.028950223699212074 0.2279052734375 3.4316306114196777 0.4584487974643707 1.2955317497253418 1.5271599292755127\n",
            "repr, std, cov, clossl, z, norm 0.03010626509785652 0.22802734375 3.433180332183838 0.5244534611701965 1.2972042560577393 1.2842460870742798\n",
            "repr, std, cov, clossl, z, norm 0.027153365314006805 0.2369384765625 2.9944472312927246 0.4685797691345215 1.4204896688461304 1.2989107370376587\n",
            "repr, std, cov, clossl, z, norm 0.03460448980331421 0.2325439453125 3.1953787803649902 0.5523991584777832 1.3945318460464478 1.8061696290969849\n",
            "repr, std, cov, clossl, z, norm 0.026112746447324753 0.225341796875 3.56815242767334 0.49178484082221985 1.3000528812408447 2.0129032135009766\n",
            "repr, std, cov, clossl, z, norm 0.028097935020923615 0.223388671875 3.6813526153564453 0.46233931183815 1.283265471458435 1.5510749816894531\n",
            "train_data.data 21391\n",
            "dided\n",
            "time\n",
            "[8, 3, 4, 3, 3, 8, 13, 14, 8, 8, 11, 3, 5, 4, 3, 8, 9, 8, 3, 3, 4, 5, 13, 8, 13, 3, 14, 3, 11, 6, 5, 3, 14, 11, 14, 14, 8, 4, 8, 3, 11, 8, 3, 8, 2, 11, 3, 0, 9, 8, 11, 2, 13, 3, 3]\n",
            "dided\n",
            "time\n",
            "[3, 3, 4, 8, 11, 3, 8, 8, 11, 14, 8, 2, 14, 5, 4, 6, 0, 8, 5, 8, 8, 8, 3, 3, 3, 14, 14, 4, 4, 5, 2, 8, 5, 8, 9, 8, 11, 0, 3, 5, 8, 3, 8, 3, 8, 8, 5, 11, 14, 4, 14, 8, 0, 8, 0, 2, 12, 8, 3, 5, 11, 8, 8, 8, 4, 8, 8, 8, 3, 5, 12, 0, 8, 14, 14, 11, 8, 3, 8, 3, 8, 0, 5, 8, 8, 14, 3, 3, 2, 3, 5, 5, 4, 9, 0, 9, 8, 11, 4, 8, 8, 14, 3, 8, 8, 11, 4, 4, 9, 0, 14, 0, 5, 0, 3, 0, 3, 8, 3]\n",
            "dided\n",
            "time\n",
            "[8, 3, 5, 11, 0, 13, 3, 6, 0, 0, 5, 8, 3, 11, 3, 8, 0, 5, 8, 8, 3, 13, 3, 8, 3, 5, 14, 0, 3, 9, 8, 11, 8, 13, 5, 0, 4, 5, 8, 3, 11, 8, 8, 8, 14, 8, 8, 8, 9, 8, 8, 2, 2, 14, 8, 4, 3, 3, 2, 5, 8, 8, 8, 3, 14, 14, 8, 0, 0, 2, 14, 14, 4, 8, 8, 8, 4, 8, 9, 11, 3, 8, 14, 2, 4, 11, 13, 8, 8, 3, 9, 4, 4]\n",
            "dided\n",
            "time\n",
            "[8, 8, 2, 8, 0, 5, 5, 2, 8, 3, 3, 2, 4, 8, 14, 13, 8, 3, 8, 3, 8, 8, 13, 3, 0, 12, 4, 3, 0, 4, 8, 14, 4, 8, 3, 3, 8, 2, 14, 8, 3, 3, 8, 4, 4, 4, 8, 8, 2, 0, 14, 8, 8, 4, 3, 13, 11, 5, 4, 9, 13, 3, 2, 14, 2, 0, 8, 8, 2, 5, 8, 14, 2, 2, 11, 8, 0, 4, 11, 5, 4, 11, 3, 4, 5, 4, 14, 2, 3, 3, 14, 14, 2, 2, 3, 13, 8, 0, 3, 2, 4, 8, 3, 2, 14, 8, 8, 8, 2]\n",
            "dided\n",
            "time\n",
            "[8, 11, 8, 4, 4, 11, 0, 5, 5, 4, 8, 8, 11, 13, 2, 3, 8, 3, 3, 8, 0, 13, 8, 8, 8, 8, 4, 3, 0, 8, 8, 9, 0, 8, 3, 5, 14, 9, 0, 13, 8, 3, 1, 6, 0, 8, 8, 9, 3, 0, 11, 3, 9, 14, 13, 3, 8, 8, 13, 3, 9, 3, 3, 3, 9, 5, 14, 8, 3, 8, 8, 0, 3, 3, 9, 4, 9, 5, 9, 2, 11, 4, 0, 4, 13, 11, 5, 9, 11, 4, 0, 4, 0, 8, 0, 6, 2, 3, 8, 5, 8, 2, 8, 8, 6, 8, 8, 8, 12, 8, 5, 5, 5, 14, 13, 4, 2, 3, 9, 8]\n",
            "24 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.024647904559969902 0.2230224609375 3.7013540267944336 0.41108518838882446 1.2157845497131348 1.618037223815918\n",
            "repr, std, cov, clossl, z, norm 0.028036437928676605 0.228271484375 3.408921241760254 0.4354279935359955 1.3527742624282837 1.803377628326416\n",
            "repr, std, cov, clossl, z, norm 0.025847002863883972 0.23046875 3.316978693008423 0.4124022126197815 1.3277186155319214 1.7546449899673462\n",
            "repr, std, cov, clossl, z, norm 0.025975791737437248 0.2315673828125 3.2379136085510254 0.4409123957157135 1.3312314748764038 1.6378402709960938\n",
            "repr, std, cov, clossl, z, norm 0.0256448183208704 0.232177734375 3.217319965362549 0.4064624011516571 1.2857848405838013 1.8995352983474731\n",
            "repr, std, cov, clossl, z, norm 0.03494228422641754 0.2298583984375 3.341538429260254 0.38790246844291687 1.2743734121322632 2.028505563735962\n",
            "repr, std, cov, clossl, z, norm 0.02635328657925129 0.2230224609375 3.700814962387085 0.4468190670013428 1.3448596000671387 2.2826340198516846\n",
            "repr, std, cov, clossl, z, norm 0.033584173768758774 0.230712890625 3.2909796237945557 0.44645991921424866 1.2903645038604736 1.9476808309555054\n",
            "repr, std, cov, clossl, z, norm 0.03052740916609764 0.23095703125 3.2785284519195557 0.5454992651939392 1.3312662839889526 2.016977071762085\n",
            "repr, std, cov, clossl, z, norm 0.03999946266412735 0.23388671875 3.126436710357666 0.4804595410823822 1.2762075662612915 2.0871541500091553\n",
            "repr, std, cov, clossl, z, norm 0.03518514707684517 0.224853515625 3.591188430786133 0.47744834423065186 1.2834596633911133 2.1123359203338623\n",
            "repr, std, cov, clossl, z, norm 0.046341657638549805 0.226318359375 3.5178070068359375 0.48355501890182495 1.279280424118042 2.2077512741088867\n",
            "train_data.data 20390\n",
            "dided\n",
            "time\n",
            "[3, 9, 8, 5, 0, 4, 9, 8, 3, 3, 2, 8, 8, 3, 8, 8, 0, 3, 4, 8, 3, 14, 2, 5, 0, 0, 8, 14, 2, 6, 3, 9, 14, 8, 0, 9, 6, 8, 3, 5, 8, 11, 3, 2, 14]\n",
            "dided\n",
            "time\n",
            "[0, 4, 14, 4, 9, 8, 8, 14, 13, 3, 14, 9, 2, 1, 8, 8, 8, 8, 14, 13, 3, 4, 8, 3, 8, 14, 11, 3, 13, 11, 0, 8, 8, 5, 8, 3, 2, 0, 3, 3, 0, 8, 0, 8, 5, 8, 8, 0, 5, 11, 4, 14, 5, 2]\n",
            "dided\n",
            "time\n",
            "[2, 3, 11, 9, 8, 5, 4, 4, 4, 8, 0, 0, 0, 12, 2, 10, 3, 8, 5, 8, 8, 14, 13, 8, 8, 2, 8, 4, 0, 13, 0, 4, 4, 3, 14, 4, 8, 2, 8, 4, 8, 14, 8, 5, 3, 4, 3, 8, 2, 8, 8, 4, 2, 0, 3, 8, 8, 14, 14, 8, 8, 8, 0, 0, 11, 9, 4, 14, 2, 3, 3, 8, 3, 8, 2, 7, 8, 8, 3, 2, 8, 14, 8, 8, 13, 7, 14, 8, 0, 8, 8, 8, 4]\n",
            "dided\n",
            "time\n",
            "[3, 3, 8, 5, 4, 13, 5, 14, 3, 5, 8, 3, 5, 11, 8, 8, 4, 14, 9, 9, 0, 13, 8, 3, 13, 4, 0]\n",
            "dided\n",
            "time\n",
            "[4, 0, 3, 2, 13, 0, 6, 3, 6, 9, 8, 14, 0, 3, 5, 2, 0, 3, 14, 8, 8, 8, 9, 8, 3, 0, 2, 8, 8, 5, 14, 6, 8, 3, 8, 3, 0, 9, 2, 8, 8, 14, 9, 3, 14, 3, 3, 8, 8, 11, 0, 4, 8, 8, 8, 4, 9, 3, 8, 8, 5, 3, 5, 0, 4, 8, 8, 0, 11, 3, 5, 4, 5, 8, 2, 3, 0, 8, 8, 9, 14, 13, 8, 3, 8, 13, 8, 8, 3, 4, 8, 8, 9, 4, 0, 14]\n",
            "25 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.03701063245534897 0.231201171875 3.258319616317749 0.47457388043403625 1.3107072114944458 2.1034998893737793\n",
            "repr, std, cov, clossl, z, norm 0.04949948191642761 0.2294921875 3.3582370281219482 0.4508177638053894 1.3784891366958618 2.0436275005340576\n",
            "repr, std, cov, clossl, z, norm 0.03761470317840576 0.2296142578125 3.336409330368042 0.38842567801475525 1.3172645568847656 1.8408739566802979\n",
            "repr, std, cov, clossl, z, norm 0.05767747759819031 0.22802734375 3.418154716491699 0.5718715190887451 1.386889934539795 2.2220218181610107\n",
            "repr, std, cov, clossl, z, norm 0.02805946208536625 0.222900390625 3.7168068885803223 0.4837803244590759 1.3507007360458374 1.5681427717208862\n",
            "repr, std, cov, clossl, z, norm 0.0341489240527153 0.228271484375 3.4089431762695312 0.46100446581840515 1.3156962394714355 1.9124231338500977\n",
            "repr, std, cov, clossl, z, norm 0.022987820208072662 0.2322998046875 3.207927703857422 0.3994975686073303 1.2899467945098877 1.882367730140686\n",
            "repr, std, cov, clossl, z, norm 0.0509551502764225 0.2381591796875 2.966928243637085 0.4171815812587738 1.3347669839859009 1.669110894203186\n",
            "repr, std, cov, clossl, z, norm 0.020398693159222603 0.236328125 3.044895887374878 0.4126386344432831 1.2615079879760742 1.7760692834854126\n",
            "repr, std, cov, clossl, z, norm 0.02934098243713379 0.2354736328125 3.090085506439209 0.4841810166835785 1.2994786500930786 1.7976096868515015\n",
            "repr, std, cov, clossl, z, norm 0.027197277173399925 0.2294921875 3.36256742477417 0.47042885422706604 1.3279019594192505 1.7772778272628784\n",
            "repr, std, cov, clossl, z, norm 0.03984888643026352 0.2215576171875 3.7678065299987793 0.47373566031455994 1.3473737239837646 1.7732572555541992\n",
            "train_data.data 20494\n",
            "dided\n",
            "time\n",
            "[4, 0, 14, 8, 14, 8, 3, 9, 8, 2, 4, 11, 8, 0, 0, 8, 3, 8, 2, 14, 8, 8, 14, 14, 4, 8, 14, 2, 13, 8, 8, 3, 3, 3, 9, 4, 5, 5, 8, 8, 8, 0, 3, 14, 8, 8, 3, 9, 2, 2, 3, 8, 3, 8, 12, 13, 8, 5, 2, 4, 8, 14, 4, 5, 5, 0, 8, 2, 0, 0, 4, 0, 13, 5, 8]\n",
            "dided\n",
            "time\n",
            "[5, 8, 8, 8, 0, 4, 2, 8, 3, 9, 8, 5, 0, 3, 8, 3, 4, 11, 8, 2, 0, 8, 8, 2, 4, 6, 4, 3, 11, 4, 14, 0, 2, 11, 3, 3, 8, 0, 4, 3, 8, 8, 9, 8]\n",
            "dided\n",
            "time\n",
            "[8, 9, 8, 8, 5, 3, 13, 0, 3, 0, 8, 8, 5, 14, 2, 0, 2, 11, 2, 8, 8, 8, 3, 8, 13, 11, 8, 5, 8, 4, 2, 5, 4, 4, 8, 11, 3, 9, 8, 2, 0, 8, 8, 8, 8, 13, 8, 0, 3, 0, 3, 0, 8, 3, 0, 5, 8, 3, 13, 8, 11, 4, 8, 13, 4, 0, 5, 13, 5, 3, 3, 0, 4, 4, 11, 14, 11, 5, 13, 0]\n",
            "dided\n",
            "time\n",
            "[5, 13, 0, 8, 8, 3, 8, 14, 4, 4, 8, 2, 3, 3, 5, 12, 14, 4, 13, 3, 13, 5, 2, 0, 6, 3, 13, 8, 3, 8, 14, 8, 8, 0, 2, 11, 0, 11, 3, 0, 3, 3, 14, 3, 14, 8, 5, 0, 3, 3, 5, 3, 4, 5, 14, 5, 4, 3, 8, 2, 3, 11, 9, 3, 0, 9, 3, 3, 8, 0, 3, 9, 8, 13, 5, 3, 11, 0, 3, 3, 14, 5, 3, 0, 8, 8, 3, 0, 13, 3, 0, 3, 11]\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 14, 8, 9, 13, 3, 8, 8, 3, 5, 5, 6, 9, 3, 14, 3, 3, 5, 13, 3, 2, 2, 0, 4, 5, 14, 8, 0, 11, 8, 2, 4, 8, 8, 3, 3, 9, 3, 14, 5, 13, 8, 5, 3, 4, 4, 11, 5, 3, 8, 3, 0, 14, 9, 14, 9, 5, 13, 3, 0, 8, 0, 8, 13, 8, 3, 3, 0, 5, 11, 11, 0, 0, 4, 7, 8, 6, 8, 5, 14, 8, 5, 2, 4, 3, 9, 8, 3, 8, 8, 8, 0, 0, 3, 8, 2, 5, 8, 3, 6, 9, 0, 8, 3, 9, 2, 3, 2, 8, 3, 11, 8, 3, 8, 8, 9, 3, 0, 9, 4, 8, 13, 8, 11, 8, 8, 8, 9, 0, 6, 6, 2, 3, 13, 8, 8, 4, 11, 3, 4, 2, 8, 0, 14, 9, 8, 2, 8, 3, 9, 0, 4, 6, 3, 3, 6, 8, 8, 3, 8, 8, 14, 0, 2, 8, 14, 8, 3, 8, 8, 4, 8, 8, 4, 7, 14, 3, 8, 4, 8, 8, 6, 3, 0]\n",
            "26 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02379721589386463 0.2225341796875 3.688401222229004 0.5267829895019531 1.3613427877426147 1.7369550466537476\n",
            "repr, std, cov, clossl, z, norm 0.026794495061039925 0.229736328125 3.3504395484924316 0.4616785943508148 1.3183425664901733 1.6978027820587158\n",
            "repr, std, cov, clossl, z, norm 0.022178998216986656 0.227294921875 3.4558608531951904 0.5152245759963989 1.2883515357971191 1.7255586385726929\n",
            "repr, std, cov, clossl, z, norm 0.02344086579978466 0.229248046875 3.360377073287964 0.5211493372917175 1.279618263244629 1.6993705034255981\n",
            "repr, std, cov, clossl, z, norm 0.019063856452703476 0.22705078125 3.4887828826904297 0.5355604887008667 1.2219146490097046 1.623456597328186\n",
            "repr, std, cov, clossl, z, norm 0.021260101348161697 0.22607421875 3.546919822692871 0.4451504051685333 1.3063459396362305 1.6334339380264282\n",
            "repr, std, cov, clossl, z, norm 0.017973577603697777 0.2344970703125 3.1132800579071045 0.42259863018989563 1.3108819723129272 1.574385404586792\n",
            "repr, std, cov, clossl, z, norm 0.018595919013023376 0.23193359375 3.2337851524353027 0.4191119968891144 1.3803056478500366 1.5161817073822021\n",
            "repr, std, cov, clossl, z, norm 0.01769280433654785 0.2332763671875 3.178919792175293 0.4383815824985504 1.3683942556381226 1.5456457138061523\n",
            "repr, std, cov, clossl, z, norm 0.019029326736927032 0.230224609375 3.319291830062866 0.3825113773345947 1.3334290981292725 1.564587116241455\n",
            "repr, std, cov, clossl, z, norm 0.017018292099237442 0.2265625 3.4934940338134766 0.45227551460266113 1.2359315156936646 2.0317182540893555\n",
            "repr, std, cov, clossl, z, norm 0.020038437098264694 0.2255859375 3.5531718730926514 0.4943931996822357 1.3553874492645264 1.5574604272842407\n",
            "train_data.data 20649\n",
            "dided\n",
            "time\n",
            "[0, 11, 13, 8, 4, 8, 2, 8, 3, 0, 3, 0, 8, 11, 0, 8, 3, 2, 8, 4, 11, 14, 14, 3, 3, 0, 8, 3, 8, 5, 11, 9, 3, 3, 2, 8, 9, 8, 8, 0, 14, 8, 3, 3, 8, 9, 3, 8, 8, 2, 8, 9, 3, 8, 3, 0, 14, 14, 14, 3, 8, 8, 8, 5, 8, 3, 8, 8, 3, 14, 8, 9, 8, 11, 8, 0, 3, 5, 11, 3, 4, 3, 8, 9, 0, 8, 8, 3, 0, 11, 11, 4, 8, 13, 13, 8, 8, 8, 0, 3, 0, 8, 3, 8, 8, 0, 4]\n",
            "dided\n",
            "time\n",
            "[0, 4, 13, 8, 13, 9, 8, 9, 6, 8, 8, 8, 5, 5, 8, 8, 8, 14, 8, 3, 0, 5, 8, 14, 14, 5, 3, 8, 11, 8, 0, 3, 9, 8, 9, 4, 0, 14, 3, 8, 4, 3, 5, 6, 3, 3, 0, 14, 4, 4, 11, 2, 4, 4, 9, 10, 3, 3, 8, 4, 4, 9, 0, 2, 9, 8, 2, 13]\n",
            "dided\n",
            "time\n",
            "[8, 2, 13, 3, 2, 14, 13, 11, 9, 4, 8, 8, 4, 9, 9, 4, 12, 4, 12, 6, 9, 0, 14, 11, 9, 6, 14, 8, 4, 6, 6, 14, 13, 14, 0, 8, 4, 4, 8, 11, 9, 8, 6, 8, 5, 8, 8, 3, 14, 8, 8, 8, 3, 8, 8, 8, 9, 8, 4, 0, 9, 4, 5, 4, 4, 14, 3, 11, 3, 5, 0, 8, 5, 8, 2, 3, 3, 3, 13, 8, 8, 3, 2, 2, 1, 3, 4, 14, 8, 5, 4, 8, 4, 4, 8, 8, 8, 8, 8, 3, 9, 4, 8, 9, 5, 11, 8, 3, 4, 14, 3, 4]\n",
            "dided\n",
            "time\n",
            "[9, 3, 4, 3, 9, 9, 4, 8, 9, 8, 5, 8, 4, 8, 14, 11, 5, 8, 4, 8, 3, 11, 8, 8, 4, 8, 4, 11, 3, 3, 8, 5, 9, 8, 6, 13, 8, 0, 3, 13, 3, 14, 8, 9, 14, 2, 8, 11, 2, 8, 3, 3, 9, 4, 8, 14, 9, 6, 8, 2, 8, 7, 4, 11, 6, 14, 14, 3, 3, 8, 4, 14, 9, 14, 0, 3, 4, 5, 9, 8, 4, 8, 5, 13, 8, 9, 4, 3, 6, 5, 2, 8, 8, 8, 9, 0, 4, 11, 14, 2, 14, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 11, 5, 0, 8, 2, 0, 0, 5, 5, 3, 9, 4, 13, 5, 9, 0, 14, 13, 9, 13, 3, 4, 9, 9, 0, 8, 3, 8, 5, 2, 3, 14, 3, 5, 9, 6, 0, 2, 5, 5, 9, 4, 9, 3, 9, 3, 4, 8, 9, 4, 9, 8, 11, 2, 8, 8, 14, 5, 13, 4, 3, 14, 9, 14, 2, 4, 8, 3, 6, 11, 8, 4, 3, 5, 5, 11, 3, 0, 4, 11, 4, 11, 13, 3, 3, 4, 9, 11, 3, 0, 9, 3, 8, 6, 8, 3, 14, 8, 8, 4, 4, 3, 4, 4, 3, 8, 3, 5, 8, 8, 8, 3, 9, 3, 8]\n",
            "27 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.01841879077255726 0.2296142578125 3.360733985900879 0.4649631679058075 1.3014483451843262 1.4700942039489746\n",
            "repr, std, cov, clossl, z, norm 0.023475833237171173 0.2279052734375 3.4356181621551514 0.45710527896881104 1.3181370496749878 1.3696155548095703\n",
            "repr, std, cov, clossl, z, norm 0.018583066761493683 0.2276611328125 3.4823856353759766 0.4592522978782654 1.4001048803329468 1.8748183250427246\n",
            "repr, std, cov, clossl, z, norm 0.02406950853765011 0.2279052734375 3.434276580810547 0.4483586549758911 1.3131276369094849 1.8840276002883911\n",
            "repr, std, cov, clossl, z, norm 0.020343512296676636 0.232421875 3.218306541442871 0.42588332295417786 1.2464425563812256 1.478369951248169\n",
            "repr, std, cov, clossl, z, norm 0.02409624680876732 0.2315673828125 3.255371570587158 0.4492309093475342 1.286125898361206 1.8212605714797974\n",
            "repr, std, cov, clossl, z, norm 0.020694930106401443 0.22705078125 3.4794044494628906 0.5341665744781494 1.2191118001937866 1.6515188217163086\n",
            "repr, std, cov, clossl, z, norm 0.024300817400217056 0.22509765625 3.5793356895446777 0.5911680459976196 1.3896896839141846 2.07415771484375\n",
            "repr, std, cov, clossl, z, norm 0.02170434407889843 0.225341796875 3.569383144378662 0.503913938999176 1.272733211517334 1.9937058687210083\n",
            "repr, std, cov, clossl, z, norm 0.027342259883880615 0.2275390625 3.4800519943237305 0.43107810616493225 1.4307961463928223 1.9183398485183716\n",
            "repr, std, cov, clossl, z, norm 0.021681467071175575 0.236572265625 3.043321132659912 0.41569647192955017 1.3643990755081177 1.7460455894470215\n",
            "repr, std, cov, clossl, z, norm 0.027275847271084785 0.2279052734375 3.4508190155029297 0.46312201023101807 1.2852996587753296 1.9193638563156128\n",
            "train_data.data 21111\n",
            "dided\n",
            "time\n",
            "[3, 9, 5, 0, 5, 3, 14, 8, 13, 5, 8, 11, 5, 8, 8, 3, 5, 0, 3, 8, 5, 8, 11, 3, 3, 3, 8, 11, 14, 5, 3, 13, 8, 8, 5, 3, 14, 9, 5, 4, 3, 2, 3, 8, 5, 5, 0, 2, 13, 8, 4, 3, 3, 5, 2, 3, 14, 3, 3, 3, 14, 14, 3, 8, 5, 3, 8, 8, 5, 3, 8, 14, 14, 3, 8, 3, 8, 8, 14, 5, 5, 8, 0, 4, 4, 5, 5, 5, 2]\n",
            "dided\n",
            "time\n",
            "[3, 0, 14, 8, 8, 2, 2, 4, 6, 0, 0, 5, 2, 8, 8, 3, 9, 8, 8, 6, 8, 14, 8, 4, 8, 8, 8, 8, 3, 3, 11, 2, 11, 6, 8, 2, 8, 8, 8, 8, 2, 14, 13, 5, 2, 3, 3, 2, 14, 4, 8, 14, 14, 5, 8, 8, 4, 0, 2, 3, 0, 3, 3, 8, 3, 0, 2, 8, 3, 8, 4, 13, 8, 2, 5, 14, 2, 13, 5, 8, 0, 4, 0, 0, 8, 3, 8, 4, 2, 0, 8, 8, 8, 8, 14, 5, 3, 3, 8, 3, 3, 14, 3, 14, 4, 3, 5, 3, 13, 3, 8, 3, 5, 3, 14, 3, 13, 14, 13, 10, 8, 3, 3, 3, 11, 5, 11, 14, 0, 11, 8, 8, 4, 4, 2, 2, 8, 14, 3, 4, 0, 5, 11, 3, 2, 13, 8, 11, 11, 9, 3, 8, 13, 13, 0, 5, 3, 5, 4, 8, 13, 13, 3, 11, 13, 11, 0, 8, 5, 3, 4, 0, 8, 8, 14, 3, 5, 11, 3, 9, 11, 0, 0, 3, 14, 11, 8, 0, 14, 8, 8, 5, 8, 8, 2, 4, 4, 3, 8, 8, 9, 8, 2, 8, 13, 8]\n",
            "dided\n",
            "time\n",
            "[8, 5, 8, 5, 3, 3, 3, 8, 3, 2, 9, 14, 14, 3, 3, 8, 0, 14, 3, 14, 8, 11, 4, 4, 13, 5, 8, 8, 8, 14, 5, 14, 4, 8, 3, 3, 11, 5, 13, 5, 9, 3, 8, 3, 3, 3, 0, 0, 0, 8, 4, 3, 2, 3, 2, 8, 5, 4, 0, 0, 3, 3, 14, 5, 8, 14, 8, 5, 8]\n",
            "dided\n",
            "time\n",
            "[5, 3, 3, 8, 3, 3, 4, 0, 3, 2, 14, 3, 8, 8, 3, 8, 8, 8, 13, 3, 5, 4, 13, 8, 6, 8, 14, 8, 6, 9, 14, 14, 5, 3, 14, 8, 13, 3, 11, 6, 8, 5, 8, 14, 0, 8, 13, 8, 0, 13, 8, 11, 0, 8, 2]\n",
            "dided\n",
            "time\n",
            "[8, 2, 14, 3, 4, 11, 8, 8, 8, 4, 2, 7, 5, 8, 13, 8, 13, 0, 8, 9, 14, 6, 14, 8, 2, 8, 0, 11, 13, 8, 0, 3, 8, 14, 3, 8, 11, 0, 3, 11, 14, 0, 5, 2, 0, 2, 11, 11, 8, 8, 5, 8, 8, 8, 4, 8, 2]\n",
            "28 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.020987696945667267 0.2266845703125 3.508538007736206 0.4265114963054657 1.3396061658859253 2.3164641857147217\n",
            "repr, std, cov, clossl, z, norm 0.024953018873929977 0.23193359375 3.237173318862915 0.4317145347595215 1.3731818199157715 2.1052756309509277\n",
            "repr, std, cov, clossl, z, norm 0.02079167775809765 0.2288818359375 3.3971800804138184 0.4212593138217926 1.26012122631073 1.9103931188583374\n",
            "repr, std, cov, clossl, z, norm 0.022995855659246445 0.2310791015625 3.2669320106506348 0.4050820767879486 1.3431206941604614 1.8580576181411743\n",
            "repr, std, cov, clossl, z, norm 0.02038145810365677 0.2294921875 3.3403725624084473 0.48781904578208923 1.2739883661270142 1.9503915309906006\n",
            "repr, std, cov, clossl, z, norm 0.024179605767130852 0.22607421875 3.5315918922424316 0.44509801268577576 1.4157648086547852 2.0605506896972656\n",
            "repr, std, cov, clossl, z, norm 0.021964630112051964 0.2220458984375 3.761442184448242 0.4633539915084839 1.3328526020050049 1.4658037424087524\n",
            "repr, std, cov, clossl, z, norm 0.027426524087786674 0.2264404296875 3.537684202194214 0.4313427209854126 1.3060301542282104 1.8141334056854248\n",
            "repr, std, cov, clossl, z, norm 0.024817006662487984 0.2303466796875 3.3631296157836914 0.4349774122238159 1.249772548675537 1.662079095840454\n",
            "repr, std, cov, clossl, z, norm 0.026666175574064255 0.2314453125 3.2828564643859863 0.470552533864975 1.2648862600326538 1.4886819124221802\n",
            "repr, std, cov, clossl, z, norm 0.020515108481049538 0.2342529296875 3.1390364170074463 0.4788002371788025 1.3686423301696777 1.3659600019454956\n",
            "repr, std, cov, clossl, z, norm 0.02667139284312725 0.2291259765625 3.384145498275757 0.38581734895706177 1.3348289728164673 1.5239684581756592\n",
            "train_data.data 21323\n",
            "dided\n",
            "time\n",
            "[5, 8, 8, 6, 11, 3, 8, 2, 4, 0, 13, 8, 3, 3, 3, 13, 11, 8, 5, 2, 2, 14, 14, 8, 9, 3, 14, 14, 3, 14, 9, 11, 2]\n",
            "dided\n",
            "time\n",
            "[8, 2, 14, 5, 8, 0, 8, 8, 8, 3, 4, 8, 5, 2, 2, 0, 3, 8, 3, 8, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 0, 8, 9, 3, 8, 8, 11, 13, 8, 9, 11, 11, 13, 3, 11, 3, 3, 13, 2, 8, 4, 2, 0, 5, 2, 4, 5, 2, 8]\n",
            "dided\n",
            "time\n",
            "[5, 2, 8, 3, 6, 13, 4, 13, 3, 8, 4, 8, 14, 13, 8, 8, 8, 8, 4, 14, 4, 4, 13, 0, 8, 8, 8, 8, 2, 14, 8, 14, 3, 9, 3, 14, 3, 9, 3, 8, 8, 4, 9, 8, 2, 3, 4, 14, 2, 14, 2, 8, 11, 8, 1, 4, 13, 5, 9, 4, 0, 4, 2, 9]\n",
            "dided\n",
            "time\n",
            "[4, 2, 9, 11, 14, 10, 8, 14, 14, 0, 3, 3, 4, 8, 3, 8, 8, 4, 2, 8, 5, 8, 3, 2, 2, 8, 8, 3, 13, 8, 14, 5, 3, 8, 3, 2, 9, 8, 4, 8, 3, 13, 8, 13, 3, 0, 0, 8, 8, 2, 5, 14, 11, 8, 5, 11, 14, 8, 3, 9, 3, 2, 8, 8, 2, 2, 8, 3, 3, 8, 6, 2, 8, 5, 8, 5, 14, 5, 5, 11, 8, 3, 9, 3, 13, 8, 4, 14, 3, 3, 3, 0, 3, 8, 0, 8, 2, 13, 8, 3]\n",
            "29 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.021770944818854332 0.2264404296875 3.5072193145751953 0.43344029784202576 1.310856819152832 1.369931697845459\n",
            "repr, std, cov, clossl, z, norm 0.027471289038658142 0.224609375 3.612905502319336 0.5068073272705078 1.1580207347869873 1.5093032121658325\n",
            "repr, std, cov, clossl, z, norm 0.0220493171364069 0.2332763671875 3.204843044281006 0.4027479588985443 1.2936636209487915 1.6034127473831177\n",
            "repr, std, cov, clossl, z, norm 0.025327539071440697 0.2340087890625 3.1614251136779785 0.40424466133117676 1.3781753778457642 1.3302680253982544\n",
            "repr, std, cov, clossl, z, norm 0.021977560594677925 0.2335205078125 3.155837059020996 0.4709432125091553 1.3158124685287476 1.625071406364441\n",
            "repr, std, cov, clossl, z, norm 0.023118607699871063 0.2313232421875 3.269291400909424 0.42773446440696716 1.256413459777832 1.2356858253479004\n",
            "repr, std, cov, clossl, z, norm 0.019976038485765457 0.2294921875 3.3560123443603516 0.4385949671268463 1.3552229404449463 1.44846510887146\n",
            "repr, std, cov, clossl, z, norm 0.03140540048480034 0.2261962890625 3.5214619636535645 0.4365968406200409 1.4420056343078613 1.183523416519165\n",
            "repr, std, cov, clossl, z, norm 0.02232544869184494 0.224853515625 3.592114210128784 0.44299647212028503 1.4148402214050293 1.4849271774291992\n",
            "repr, std, cov, clossl, z, norm 0.03191094845533371 0.224853515625 3.575329303741455 0.4512440264225006 1.342373251914978 1.5122851133346558\n",
            "repr, std, cov, clossl, z, norm 0.020772689953446388 0.22998046875 3.316129207611084 0.4315887987613678 1.3275035619735718 1.5180906057357788\n",
            "repr, std, cov, clossl, z, norm 0.031139571219682693 0.2271728515625 3.461928367614746 0.5656669735908508 1.3641165494918823 1.483857274055481\n",
            "train_data.data 20384\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(30):\n",
        "    # print(\"#### simulate ####\")\n",
        "    # buffer_=[]\n",
        "    for _ in range(5):\n",
        "        buffer = simulate(agent, buffer)\n",
        "        # buffer_ = simulate(agent, buffer_)\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # torch.save(checkpoint, folder+'agentoptim1.pkl')\n",
        "\n",
        "    # buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "    # with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "    print(\"train_data.data\",len(train_data.data))\n",
        "    while len(train_data.data)>20000: # 10000:6.9gb, 20000:5.5gb\n",
        "        buffer.pop(random.randrange(len(buffer)))\n",
        "        train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "e580aeef-d488-4777-baf1-d32bd19e1d41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "20f01aac-4ec6-4508-905c-8ff6e093c9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240925_144139-aalq779i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/aalq779i' target=\"_blank\">honest-serenity-69</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/aalq779i' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/aalq779i</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(project=\"procgen\",\n",
        "    config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        _,T,_ = sx.shape\n",
        "        batch = 1\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            # print(i,x.data, z.squeeze(), loss.item())\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mY7BITKjSKC",
        "outputId": "cbad0ffc-e8e9-4a4d-bd3c-d6d9ddaf6e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2336301\n",
            "1278976\n",
            "399360\n",
            "1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-76-e20d23bca149>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=3, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # e = d_model**-0.5\n",
        "        # self.h0 = torch.empty((self.jepa.pred.num_layers, 1, d_model), device=device).uniform_(-e, e) # [num_layers, batch, d_model]\n",
        "        # self.h0 = torch.normal(mean=0, std=e, size=(self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # torch.nn.init.xavier_uniform_(self.h0) # xavier_uniform_, kaiming_normal_\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "        state = torch.zeros((1, 3,64,64), device=device)\n",
        "        self.sx = self.jepa.enc(state)\n",
        "\n",
        "    # def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        self.update_h0(lstate, laction)\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "            # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.h0=lh0[k].unsqueeze(1) # [num_layers, 1, d_model]\n",
        "        # self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        self.lx, self.lz = lx[k:], lz[k:] # [T, dim_a], [T, dim_z]\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "\n",
        "                # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "                # print(torch.cat(lstate, dim=0).shape)\n",
        "                # lsx = self.jepa.enc(torch.stack(lstate, dim=0))#.unsqueeze(0)\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx-torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                # batch, seq_len, _ = lstate.shape\n",
        "                # seq_len, _ = lstate.shape\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    try: la = self.emb(self.la[:seq_len])\n",
        "                    except:\n",
        "                        print(\"err self.la\")\n",
        "                        # la = self.emb([0]*seq_len)\n",
        "                        la = self.emb(torch.zeros(seq_len, dtype=int, device=device))\n",
        "\n",
        "        # lz = nn.Parameter(torch.zeros((batch, seq_len, self.dim_z),device=device))\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        # optim_z = torch.optim.SGD([lz], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([lz], 1e0, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "\n",
        "        for i in range(20): # num epochs\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                # print(sxaz.shape, self.h0.shape)\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                # sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                loss = F.mse_loss(out_, out)\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            print(lz.data)\n",
        "            with torch.no_grad(): lz.clamp_(min=-1, max=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1]\n",
        "        self.la = la[k:]\n",
        "        return h0\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                # x[:,:self.lx.shape[1]], z[:,:self.lz.shape[1]] = self.lx[:,:T], self.lz[:,:T]\n",
        "                x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].repeat(batch,1,1), self.lz[:T].repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            # print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            # print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i, \"loss\", loss.squeeze().data)\n",
        "            # print(x.shape,torch.argmax(-dist,dim=-1).shape,z.shape,loss.shape) # [16, 6, 3], [16, 6], [16, 6, 1], [16, 1]\n",
        "            # print(i, torch.cat([x,torch.argmax(-dist,dim=-1),z],dim=-1).squeeze().data)\n",
        "            print(i, \"x act z\", torch.cat([x[0],torch.argmax(-dist,dim=-1)[0].unsqueeze(-1),z[0]],dim=-1).squeeze().data)\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, t, dim_a], [batch, t, dim_z]\n",
        "\n",
        "\n",
        "    def search_optimxz(self, sx, T=6, h0=None):\n",
        "        self.eval()\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 4 # 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1 ; sgd\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_uniform_(z) # xavier_normal_, xavier_uniform_\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        # optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e1, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].unsqueeze(0).repeat(batch,1,1), self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        print(\"search\", z[0].squeeze())\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        for i in range(10): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.sum().backward()\n",
        "            # optim_x.step(); optim_z.step()\n",
        "            # optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            print(i, \"search loss\", loss.squeeze().data)\n",
        "            # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "            print(i, \"search z\", z[0].squeeze().data)\n",
        "            # print(torch.argmin(dist,dim=-1).int())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        print(\"c\",torch.stack(c)[:,idx])\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        c=[]\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            # print(sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            c.append(tcost)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    # imshow(state[0].cpu())\n",
        "                    # print(\"norm\", torch.norm(sy[0]-sy_[0], dim=-1))\n",
        "                    # # if torch.norm(sy[0]-sy_[0], dim=-1) > 1:\n",
        "                    # print(i, reward[0])\n",
        "                    # print(sy)\n",
        "                    # print(sy_)\n",
        "                    # print(sy[0]-sy_[0])\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "\n",
        "                    # cost loss\n",
        "                    # syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = 100*clossl\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batc h_size, d_model]\n",
        "            # sx=sy_\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "\n",
        "                    # z = self.jepa.argm(sy_, a, sy)\n",
        "                    z = self.argm(sy, sy_, h0, a, reward)\n",
        "                    with torch.no_grad(): z.mul_(torch.rand_like(z)).mul_((torch.rand_like(z)>0.5).bool()) # dropout without scailing\n",
        "\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "\n",
        "                    # cost loss\n",
        "                    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    # syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    # print(h0.requires_grad)\n",
        "                    # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "                    # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "                    # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    # torch.norm(sy-sx, dim=-1)\n",
        "                    # sx=sy\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    norm = torch.norm(sy, dim=-1)[0].item()\n",
        "                    z_norm = torch.norm(z)\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "\n",
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cg0BI2TwY9-p"
      },
      "outputs": [],
      "source": [
        "# @title z.grad.data = -z.grad.data\n",
        "\n",
        "# self.eval()\n",
        "batch = 4 # 16\n",
        "x = nn.Parameter(torch.empty((batch, T, agent.dim_a),device=device))\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# optim_ = torch.optim.SGD([x,z], lr=1e1) # 3e3\n",
        "optim_ = torch.optim.AdamW([x,z], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "print(\"search z\", z[0].squeeze())\n",
        "print(\"search x\", x[0].squeeze())\n",
        "sx, h0 = sx.detach(), h0.detach()\n",
        "for i in range(10): # num epochs\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    z.grad.data = -z.grad.data\n",
        "    optim_.step()\n",
        "    optim_.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "    print(i, \"search loss\", loss.squeeze().data)\n",
        "    print(i, \"search z\", z[0].squeeze().data)\n",
        "    print(i, \"search x\", x[0].squeeze().data)\n",
        "dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "print(\"c\",torch.stack(c)[:,idx])\n",
        "# return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "# print(lact[idx], lh0[:,:,idx,:], x[idx], z[idx])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5VMebkQ1mJtD"
      },
      "outputs": [],
      "source": [
        "# @title argm agent.rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def argm(sx, x,h0, lr=3e3): # 3e3\n",
        "    # agent.eval()\n",
        "    # batch_size, T, _ = sx.shape\n",
        "    batch = 16 # 16\n",
        "    z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "    torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "    optim_z = torch.optim.SGD([z], lr=1e3, maximize=True) # 3e3\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.LBFGS([z], max_iter=5, lr=1)\n",
        "\n",
        "    # print(\"argm\", z[0].squeeze())\n",
        "    sx, h0 = sx.detach(), h0.detach()\n",
        "    x = x.detach().repeat(batch,1,1)\n",
        "    for i in range(5): # num epochs\n",
        "        # print(sx.shape, x.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "        loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "        loss.sum().backward()\n",
        "        optim_z.step()\n",
        "        optim_z.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "        # print(i, \"argm loss\", loss.squeeze().data)\n",
        "        # print(i, \"argm z\", z[0].squeeze().data)\n",
        "    idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "    return z[idx].unsqueeze(0)\n",
        "\n",
        "\n",
        "T=1\n",
        "xx = torch.empty((1, T, agent.dim_a))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "# print(x.shape)\n",
        "optim_x = torch.optim.SGD([x], lr=1e1) # 1e-1,1e-0,1e4 ; 1e2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "\n",
        "state = torch.zeros((1, 3,64,64))\n",
        "with torch.no_grad():\n",
        "    sx = agent.jepa.enc(state).detach()\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "print(time.time()-start)\n",
        "\n",
        "print(\"search\",x.squeeze().data)\n",
        "for i in range(20): # 5\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    z = argm(sx, x_,h0)\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [1, 1, 3], [1, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "    print(i, \"search loss\", x.squeeze().data, loss.item())\n",
        "    # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "\n",
        "# z sgd 1e3\n",
        "# 9 search loss tensor([0.0142, 0.0142, 0.0142, 0.0142])\n",
        "# 9 search z tensor([-0.3381, -0.7005, -0.5877, -0.0664, -0.1439,  0.0283,  0.0541, -0.1439])\n",
        "\n",
        "# x sgd 1e2\n",
        "# 1 tensor([0.3561, 0.3059, 0.8830]) 0.014148875139653683\n",
        "# 9 tensor([0.3560, 0.3064, 0.8828]) 2.328815611463142e-07\n",
        "\n",
        "# 1e0\n",
        "# 19 tensor([-0.5768,  0.5778,  0.5774]) 6.543130552927323e-07\n",
        "# 19 tensor([0.3570, 0.6689, 0.6521]) 2.474381801675918e-07\n",
        "# 19 tensor([0.5783, 0.5765, 0.5772]) 1.519319567933053e-07\n",
        "# 19 tensor([0.3427, 0.6795, 0.6487]) 4.220427456402831e-07\n",
        "#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gcvgdCB1h1_E"
      },
      "outputs": [],
      "source": [
        "# @title torch.optim.LBFGS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Example of a deep nonlinear model f(x)\n",
        "class DeepNonlinearModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNonlinearModel, self).__init__()\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(10, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "f = DeepNonlinearModel()\n",
        "# x = torch.randn(1, 10, requires_grad=True)\n",
        "# xx = torch.randn((1,10))\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "\n",
        "# Define loss function (mean squared error for this example)\n",
        "target = torch.tensor([[0.0]])  # Target output\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "def closure():\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    return loss\n",
        "\n",
        "optimizer = torch.optim.LBFGS([x], lr=1.0, max_iter=5)  # Limit to 2-3 iterations for speed\n",
        "start_time = time.time()\n",
        "for _ in range(2):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step(closure)  # Perform a step of optimisation\n",
        "    print(loss.item())\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "start_time = time.time()\n",
        "optimizer = torch.optim.SGD([x], lr=1e1, maximize=True) # 3e3\n",
        "for _ in range(5):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step()  # Perform a step of optimisation\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    print(loss.item())\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "# optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def argm(self, sx, lr=3e3): # 3e3\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch_size, T, _ = sx.shape\n",
        "        batch = 16\n",
        "        # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "        z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "        optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "        sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "        # sx = sx.detach()\n",
        "        for i in range(20): # 10\n",
        "            # print(sx.shape,z.shape)\n",
        "            sxz = torch.cat([sx, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                cost = model(sxz)\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "            # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "            # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        # return z.detach()\n",
        "        # print(\"argm z\",z.squeeze().data)\n",
        "        # print(\"cost\",cost.squeeze())\n",
        "        idx = torch.argmax(loss)\n",
        "        # return z[idx].detach().unsqueeze(0)\n",
        "        return z[:,idx].detach()\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=1\n",
        "        T=1\n",
        "        x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "\n",
        "        lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "        # ratio = 6e0\n",
        "        lr = 1e-1 # adamw 1e-1\n",
        "        ratio = 4\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        # optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze())\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "        for i in range(50):\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = argm(x)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "            # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "            # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                x.clamp_(min=-1, max=1)\n",
        "            # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "        idx = torch.argmax(loss)\n",
        "        print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HcOidvtW9KAH"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx0k_ndHOEMe"
      },
      "outputs": [],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "N2TGs69fnrZo",
        "outputId": "e7624553-e17a-4a9f-85a4-512720ed329a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcost.1.weight torch.Size([2, 512])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU(267, 256, batch_first=True, dropout=0.2)\n"
          ]
        }
      ],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "# for name, param in agent.emb.named_parameters():\n",
        "for name, param in agent.tcost.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjEWGq2WGi9a",
        "outputId": "649e3612-f156-496e-d8d5-fc576110e2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0015, -0.0132,  0.0280,  ...,  0.0297,  0.0289,  0.0152],\n",
            "        [ 0.0168,  0.0031, -0.0288,  ..., -0.0064, -0.0137, -0.0085]])\n"
          ]
        }
      ],
      "source": [
        "# print(vars(agent.jepa.pred.))\n",
        "# print(vars(agent.tcost.state_dict()))\n",
        "# print(agent.jepa.pred._parameters.keys())\n",
        "# print(agent.jepa.pred._parameters['weight_ih_l0'])\n",
        "# print(agent.jepa.pred._parameters['weight_hh_l2']) # weight_hh_l0, weight_hh_l2\n",
        "# print(agent.tcost.state_dict().keys())\n",
        "print(agent.tcost.state_dict()['tcost.1.weight']) # tcost.2.bias, tcost.4.bias\n",
        "# print(agent.tcost.named_parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_xnBFjXVxgz"
      },
      "outputs": [],
      "source": [
        "# @title transfer weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 10  # size of the first dimension\n",
        "a = 5   # size of the extra nodes to omit\n",
        "m = 8   # output dimension\n",
        "\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "target_layer = nn.Linear(d, m)\n",
        "# source_layer = nn.Linear(d, m)\n",
        "# target_layer = nn.Linear(d+a, m)\n",
        "\n",
        "def transfer(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt.weight[:, :src.weight.shape[1]].copy_(src.weight[:, :tgt.weight.shape[1]])\n",
        "        tgt.bias.copy_(src.bias)\n",
        "    return tgt,src\n",
        "\n",
        "target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "src_sd = source_layer.state_dict()\n",
        "tgt_sd = target_layer.state_dict()\n",
        "\n",
        "def transfersd(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt['weight'][:, :src['weight'].shape[1]].copy_(src['weight'][:, :tgt['weight'].shape[1]])\n",
        "        tgt['bias'].copy_(src['bias'])\n",
        "    return tgt\n",
        "\n",
        "tgt_sd = transfersd(tgt_sd, src_sd)\n",
        "target_layer.load_state_dict(tgt_sd)\n",
        "\n",
        "\n",
        "agent_src = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "\n",
        "# agent.tcost = TCost((1+agent.jepa.pred.num_layers)*agent.d_model) # replace tcost\n",
        "\n",
        "agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# agent.jepa.pred\n",
        "# target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(vars(agent.jepa.pred))\n",
        "# gru = agent.jepa.pred\n",
        "# gru = agent_src.jepa.pred\n",
        "# for wht_name in gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, gru._parameters[wht_name].shape)\n",
        "\n",
        "# weight_ih_l0 dim_z=3: [768, 262] , dim_z=1: [768, 260]\n",
        "# weight_hh_l0 torch.Size([768, 256])\n",
        "# bias_ih_l0 torch.Size([768])\n",
        "# bias_hh_l0 torch.Size([768])\n",
        "\n",
        "# tgt_gru = agent.jepa.pred\n",
        "# src_gru = agent_src.jepa.pred\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "\n",
        "tgt_gru[]\n",
        "def transfer_gru(tgt_gru, src_gru): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(len(tgt_gru._all_weights), len(src_gru._all_weights))):\n",
        "        # for lyr in tgt_gru._all_weights:\n",
        "            lyr = tgt_gru._all_weights[i]\n",
        "            for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "                # print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "                tgt_wht, src_wht = tgt_gru._parameters[wht_name], src_gru._parameters[wht_name]\n",
        "                if len(tgt_wht.shape)==2:\n",
        "                    tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "                elif len(tgt_wht.shape)==1:\n",
        "                    tgt_gru._parameters[wht_name] = src_wht\n",
        "    return tgt_gru\n",
        "tgt_gru = transfer_gru(tgt_gru, src_gru)\n",
        "\n",
        "# for wht_name in tgt_gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d_model=256; dim_a=3; dim_z=1; dim_v=512\n",
        "\n",
        "pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "# pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "print(pred._all_weights)\n",
        "for lyr in pred._all_weights:\n",
        "    for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "        print(wht_name, pred._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(pred.state_dict().keys())\n",
        "\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "print(tgt_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "print(src_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "\n",
        "print(tgt_gru.state_dict()['bias_ih_l0'][:10])\n",
        "print(src_gru.state_dict()['bias_ih_l0'][:10])\n",
        "tgt_gru.state_dict().keys()\n",
        "src_gru.state_dict().keys()\n",
        "\n",
        "# tgt_gru\n",
        "# src_gru\n",
        "for wht_name in tgt_gru.state_dict().keys():\n",
        "    if not wht_name in src_gru.state_dict().keys(): continue\n",
        "    print(wht_name)\n",
        "    # print(tgt_gru.state_dict()[wht_name])\n",
        "    # tgt_gru.state_dict()[wht_name].copy_(src_gru.state_dict()[wht_name])\n",
        "\n",
        "tgt_sd = tgt_gru.state_dict()\n",
        "src_sd = src_gru.state_dict()\n",
        "def transfer_sd(tgt_sd, src_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            # print(wht_name)\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            if len(tgt_wht.shape)==2:\n",
        "                tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "            elif len(tgt_wht.shape)==1:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "    return tgt_sd\n",
        "tgt_sd = transfer_sd(tgt_sd, src_sd)\n",
        "print(tgt_sd['weight_ih_l0'][0][:10])\n",
        "print(tgt_sd['bias_ih_l0'][:10])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CwApoQMMKzB",
        "outputId": "98f67f91-ef5b-406f-b852-5a93130f9e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0012018680572509766\n",
            "tensor([0.2797, 0.2218, 0.2731, 0.3268, 0.2632, 0.2914, 0.3217, 0.2845])\n"
          ]
        }
      ],
      "source": [
        "# @title test init norm\n",
        "print(agent.emb.state_dict()['weight'].norm(dim=-1))\n",
        "\n",
        "# x = torch.rand(16)\n",
        "x = torch.rand(8,16)\n",
        "# print(x)\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1.0)\n",
        "# torch.nn.init.xavier_normal_(x)\n",
        "import time\n",
        "start = time.time()\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # 0.00966, 0.000602, 0.0004\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1./x.shape[-1]**0.5)\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "print(time.time()-start)\n",
        "# std = ((Sum (xi-mean)^2)/ N)^(1/2)\n",
        "# print(x)\n",
        "# print(((x**2).sum())**(0.5))\n",
        "print(torch.norm(x, dim=-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B1yvJkX89C_o"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein\n",
        "import torch\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    # cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    # dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # cs = (x-y).cumsum(dim=-1)\n",
        "    cs = (x-y) @ torch.tril(torch.ones(x.shape[0], x.shape[0]))\n",
        "    # dist = weight * torch.abs(cs)\n",
        "    dist = weight * cs**2\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "\n",
        "def soft_wasserstein_loss(x, y, smoothing=0.1):\n",
        "    # Normalise distributions\n",
        "    x = x / x.sum()\n",
        "    y = y / y.sum()\n",
        "    # Compute the cumulative distributions (CDFs) with a small smoothing factor\n",
        "    cdf_x = torch.cumsum(x, dim=-1) + smoothing\n",
        "    cdf_y = torch.cumsum(y, dim=-1) + smoothing\n",
        "    # Compute smooth Wasserstein distance (L2 distance between CDFs)\n",
        "    distance = torch.norm(cdf_x - cdf_y, p=2)  # L2 distance instead of L1 for smoother gradients\n",
        "    return distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5], dtype=float))\n",
        "x = nn.Parameter(torch.tensor([-0.01, -0.0, -0.99], dtype=torch.float))\n",
        "y = torch.tensor([0.0, 0.0, -1.0], dtype=torch.float)\n",
        "\n",
        "# x = nn.Parameter(torch.rand(1024, dtype=float))\n",
        "# y = torch.rand(1024, dtype=float)\n",
        "# a = len(train_data.buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "a=1/45\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "print(weight)\n",
        "dist = wasserstein(x, y, weight=weight)\n",
        "print(time.time() - start)\n",
        "print(dist)  # Should output 0.7\n",
        "# dist.backward()\n",
        "\n",
        "# 0.0004496574401855469\n",
        "# 0.000331878662109375\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3nfZRhVc9Ssp"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein sinkhorn train\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# agent.eval()\n",
        "# batch_size, T, _ = sx.shape\n",
        "x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3) # 3e3\n",
        "# optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.999)) # ? 1e0 ; 3e-2 1e-1\n",
        "# optim = torch.optim.AdamW([x], 1e-0, (0.9, 0.95)) # ? 1e0 ; 3e-2 1e-1\n",
        "y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=torch.float)\n",
        "a=1/45\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "# print(weight)\n",
        "\n",
        "# loss = wasserstein(x, y, weight=weight)\n",
        "# loss = wasserstein(x, y)\n",
        "# loss = sinkhorn(x, y)\n",
        "# loss.backward()\n",
        "# print(x.grad)\n",
        "\n",
        "\n",
        "for i in range(50): # num epochs\n",
        "    loss = wasserstein(x, y, weight=weight)\n",
        "    # loss = sinkhorn(x, y)\n",
        "    # loss = sinkhorn(x, y,0.05,80)\n",
        "    loss.sum().backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(x.data, loss.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def sinkhorn(x, y, epsilon=0.05, max_iters=100):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "\n",
        "    # Compute the cost matrix: here the cost is the squared distance between indices\n",
        "    # (|i-j|^2 for each position i, j)\n",
        "    posx = torch.arange(x.shape[-1], dtype=torch.float).unsqueeze(1)\n",
        "    posy = torch.arange(y.shape[-1], dtype=torch.float).unsqueeze(0)\n",
        "    cost_matrix = (posx - posy).pow(2)  # squared distance\n",
        "\n",
        "    # Initialize the dual variables\n",
        "    u = torch.zeros_like(x)\n",
        "    v = torch.zeros_like(y)\n",
        "\n",
        "    # Sinkhorn iterations\n",
        "    K = torch.exp(-cost_matrix / epsilon)  # Kernel matrix, regularised with epsilon\n",
        "    for _ in range(max_iters):\n",
        "        u = x / (K @ (y / (K.t() @ u + 1e-8)) + 1e-8)\n",
        "        v = y / (K.t() @ (x / (K @ v + 1e-8)) + 1e-8)\n",
        "    # print(K,u.data,v.data)\n",
        "    plan = torch.diag(u) @ K @ torch.diag(v)\n",
        "    dist = torch.sum(plan * cost_matrix)\n",
        "    return dist\n",
        "\n",
        "# Example\n",
        "x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float, requires_grad=True)\n",
        "y = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float)\n",
        "# x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "# y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=float)\n",
        "\n",
        "# dist = sinkhorn(x, y)\n",
        "dist = sinkhorn(x, y, 0.05,80)\n",
        "dist.backward()  # To compute gradients with respect to x\n",
        "\n",
        "print(dist.item())\n",
        "print(x.grad)\n",
        "\n",
        "# [2.0000e+07, 3.0000e+07, 1.0000e-08]) tensor([       0.,        0., 49999996.] episodes>=80\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s1_GgDzoDyYB"
      },
      "outputs": [],
      "source": [
        "# @title torchrl.data.PrioritizedReplayBuffer\n",
        "from torchrl.data import LazyMemmapStorage, LazyTensorStorage, ListStorage\n",
        "buffer_lazytensor = ReplayBuffer(storage=LazyTensorStorage(size))\n",
        "\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "buffer_lazymemmap = ReplayBuffer(storage=LazyMemmapStorage(size), batch_size=32, sampler=SamplerWithoutReplacement())\n",
        "\n",
        "\n",
        "from torchrl.data import ListStorage, PrioritizedReplayBuffer\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "rb = PrioritizedReplayBuffer(alpha=0.7, beta=0.9, storage=ListStorage(10))\n",
        "data = range(10)\n",
        "rb.extend(data)\n",
        "# rb.extend(buffer)\n",
        "\n",
        "\n",
        "sample = rb.sample(3)\n",
        "print(sample)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_UlGz6Xoq3"
      },
      "source": [
        "## plot 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VX5IExbRriwm"
      },
      "outputs": [],
      "source": [
        "# @title sklearn RBF\n",
        "# https://gist.github.com/eljost/2c4e1af652ef02b2989da341c5569af7\n",
        "# from nn_plot.ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import scipy.stats as st\n",
        "\n",
        "# np.random.seed(1)\n",
        "def func(x):\n",
        "    # print(x.shape)\n",
        "    # x= np.sum(x**2, axis=-1)\n",
        "    x=np.random.rand(x.shape[0])\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "res = 50\n",
        "num_pts=15\n",
        "X=np.random.rand(num_pts,2)*res\n",
        "# Y = func(X)\n",
        "Y=np.random.rand(num_pts)\n",
        "# print(X);print(Y)\n",
        "\n",
        "lim = 1\n",
        "# lin = np.linspace(-lim, lim, res)\n",
        "lin = np.linspace(0, res, res)\n",
        "x1, x2 = np.meshgrid(lin, lin)\n",
        "xx = np.vstack((x1.flatten(), x2.flatten())).T\n",
        "\n",
        "kernel = RBF()\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gp.fit(X, Y)\n",
        "# print(\"Learned kernel\", gp.kernel_)\n",
        "y_mean, y_cov = gp.predict(xx, return_cov=True)\n",
        "\n",
        "posteriors = st.multivariate_normal.rvs(mean=y_mean, cov=y_cov, size=1)\n",
        "\n",
        "ax = plt.figure().add_subplot(projection='3d')\n",
        "Z=posteriors.reshape(-1, res)\n",
        "# ax.plot_surface(x1, x2, Z)\n",
        "ax.plot_surface(x1, x2, Z, cmap='rainbow', alpha=0.7)\n",
        "\n",
        "# ax.plot_surface(x1, x2, posteriors.reshape(-1, res))\n",
        "ax.contour(x1, x2, Z, zdir='z', offset=-1, cmap='coolwarm') # https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html#sphx-glr-gallery-mplot3d-contour3d-3-py\n",
        "# ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-0.4, 0.5))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-1, 2))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, c=zdata, cmap='Greens');\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, cmap='Greens');\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SusX7gpzxFNL",
        "outputId": "9f14a9da-e188-49ba-f5f5-70192ff33134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-2.6321], grad_fn=<AddBackward0>)\n",
            "tensor([2.7358]) tensor([-4.7358])\n"
          ]
        }
      ],
      "source": [
        "# @title chatgpt RBFKernelLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBFKernelLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        self.centres = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        dists = torch.cdist(x, self.centres, p=2) ** 2\n",
        "        return torch.exp(-dists / (2 * self.sigma ** 2))\n",
        "\n",
        "class SaddlePointNetwork(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(SaddlePointNetwork, self).__init__()\n",
        "        self.rbf_layer = RBFKernelLayer(in_features, out_features, sigma)\n",
        "        self.linear = nn.Linear(out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rbf_output = self.rbf_layer(x)\n",
        "        # Introduce a saddle point structure\n",
        "        linear_output = self.linear(rbf_output)\n",
        "        # Example saddle function: x^2 - y^2\n",
        "        saddle_output = torch.sum(linear_output[:, :1]**2 - linear_output[:, 1:]**2, dim=1, keepdim=True)\n",
        "        return saddle_output\n",
        "\n",
        "# sin(ax)sin(bx)\n",
        "# (x^2 - y^2)\n",
        "import torch\n",
        "\n",
        "def rbf_saddle(x, y, gamma=1.0, a=1.0, b=1.0):\n",
        "    # RBF-like term\n",
        "    rbf_term = torch.exp(-gamma * torch.norm(x - y, p=2)**2)\n",
        "    # Saddle point term\n",
        "    saddle_term = (a * x)**2 - (b * y)**2\n",
        "    return rbf_term + saddle_term\n",
        "\n",
        "# Example usage\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "output = rbf_saddle(x, y)\n",
        "print(output)\n",
        "\n",
        "# Compute gradients\n",
        "output.backward()\n",
        "print(x.grad, y.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rTmCo7pm0NxL"
      },
      "outputs": [],
      "source": [
        "# @title plot 3d\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "y = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "X, Y = torch.meshgrid(x, y)\n",
        "Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fWZaQTDFg1",
        "outputId": "4c5ced88-54f1-436e-89f9-66f1c8396373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000, -0.7231,  0.3792,  0.0000]]) tensor([0.3362])\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title shape NN\n",
        "num_pts=1\n",
        "\n",
        "# X=torch.rand(num_pts,4)*2-1\n",
        "# X=torch.cat([torch.tensor([0,0]).unsqueeze(0),torch.rand(num_pts,2)*2-1], dim=-1)\n",
        "X=torch.cat([torch.zeros(1,1),torch.rand(num_pts,2)*2-1,torch.zeros(1,1)], dim=-1)\n",
        "Y=torch.rand(num_pts)\n",
        "print(X,Y)\n",
        "optim = torch.optim.SGD(model.parameters(), 1e-1)\n",
        "\n",
        "# model.train()\n",
        "pred = model(X)\n",
        "# print(Y.shape,pred.shape)\n",
        "# loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "loss = F.mse_loss(Y, pred.squeeze(-1))\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "boDd__PE2sGy"
      },
      "outputs": [],
      "source": [
        "# @title plot NN\n",
        "\n",
        "xx = torch.linspace(-1, 1, 100)\n",
        "yy = torch.linspace(-1, 1, 100)\n",
        "X, Y = torch.meshgrid(xx, yy) # [100,100]\n",
        "xy = torch.cat([X.unsqueeze(-1), torch.zeros(X.shape+(2,)), Y.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "with torch.no_grad(): Z = model(xy).squeeze(-1)\n",
        "# print(Z)\n",
        "# print(Z.shape)\n",
        "\n",
        "# Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qW6BYoXsX57o"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle same time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def f(x, y):\n",
        "    return x ** 2 - y ** 2 + x * y\n",
        "# (x-y)(x+y)+xy\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "dim_x, dim_z = 3, 8\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "xx = torch.empty((1, T, dim_x))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "# x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "\n",
        "# tensor([[0.6478, 0.0531, 0.0861]]) tensor([[-1.,  1.]]) 0.2974517047405243\n",
        "# tensor([-0.9419, -1.0000,  0.4416, -1.0000,  1.0000,  0.2963])\n",
        "\n",
        "# x = nn.Parameter(torch.tensor([[0.6478, 0.0531, 0.0861]]))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "z = nn.Parameter(torch.empty((batch, T, dim_z)))\n",
        "# z = torch.empty((1, T, 1))\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# z = nn.Parameter(z.repeat(batch,1,1))\n",
        "# z = nn.Parameter(torch.tensor([[-1.,  1.]]))\n",
        "# optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "# optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.95), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "# .95,1e-1,3e-1\n",
        "# .99,\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(10): # num epochs\n",
        "    # loss = f(x,z)\n",
        "    # loss = f(x.sum(-1),z)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step(); optim_z.step()\n",
        "    optim_x.zero_grad(); optim_z.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x.squeeze()[0].data, z[0].squeeze().data, loss[0].squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "        z.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "# idx = torch.argmax(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GJdFpDr2wIMT"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle argm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def argm(sx, lr=3e3): # 3e3\n",
        "    # batch=sx.size(dim=0)\n",
        "    batch_size, T, _ = sx.shape\n",
        "    batch = 16\n",
        "    # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "    # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "    z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "    torch.nn.init.xavier_uniform_(z)\n",
        "    # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "    # sx = sx.detach()\n",
        "    for i in range(20): # 10\n",
        "        # print(sx.shape,z.shape)\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            cost = model(sxz)\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "        # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    # return z.detach()\n",
        "    print(cost.squeeze().data)\n",
        "    idx = torch.argmax(cost.squeeze(), dim=1)\n",
        "    return z[torch.arange(z.shape[0]),idx].detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(xx.clone())\n",
        "x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50):\n",
        "    z = argm(x)\n",
        "    # print(x.shape,z.shape)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "# print(\"z\",z)\n",
        "# print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjIJP6RlEv2",
        "outputId": "447fdefd-452b-437d-c228-1847492b36f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = torch.randn(16, 16)\n",
        "# print((b==torch.max(b)).nonzero())\n",
        "x = torch.randn(10, 3)\n",
        "idx = torch.randint(3,(10,))\n",
        "# print(x[:,idx].shape)\n",
        "print(x[torch.arange(x.shape[0]),idx].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cX71EprCMSNG"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim bad?\n",
        "\n",
        "import torch\n",
        "\n",
        "def transfer_optim(src_optim, tgt_optim, param_mapping):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    tgt_sd = tgt_optim.state_dict()\n",
        "\n",
        "    # Iterate over each parameter in the target optimizer\n",
        "    for (tgt_idx, target_param) in enumerate(tgt_optim.param_groups[0]['params']):\n",
        "        target_id = id(target_param)\n",
        "\n",
        "        # Find the corresponding source parameter using param_mapping\n",
        "        if target_id in param_mapping:\n",
        "            source_param = param_mapping[target_id]\n",
        "            source_id = id(source_param)\n",
        "\n",
        "            # If there's an existing state for the source parameter, transfer it\n",
        "            if source_id in src_sd['state']:\n",
        "                source_state = src_sd['state'][source_id]\n",
        "                target_state = {}\n",
        "\n",
        "                # Handle momentum/first and second moments (e.g., `exp_avg`, `exp_avg_sq` in Adam)\n",
        "                for key in source_state.keys():\n",
        "                    if source_state[key].shape == target_param.shape: target_state[key] = source_state[key].clone()\n",
        "                    # If size doesn't match, either copy what you can or initialise new values\n",
        "                    elif key in ['exp_avg', 'exp_avg_sq']:  # Momentums (specific to Adam-like optimizers)\n",
        "                        target_state[key] = torch.zeros_like(target_param)\n",
        "                        target_state[key][:source_param.numel()] = source_state[key].flatten()[:target_param.numel()]\n",
        "                    else: target_state[key] = torch.zeros_like(target_param) # init\n",
        "                tgt_sd['state'][target_id] = target_state\n",
        "\n",
        "    # Load the updated state dict back into the target optimizer\n",
        "    tgt_optim.load_state_dict(tgt_sd)\n",
        "    return tgt_optim\n",
        "# {'state': {0: {'step': tensor(1.), 'exp_avg': tensor, 'exp_avg_sq': tensor}, 1: }}\n",
        "\n",
        "\n",
        "\n",
        "model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "source_optimizer = optim.AdamW(model_src.parameters())\n",
        "target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "dummy_input = torch.randn(3, 10)\n",
        "dummy_target = torch.randn(3, 5)\n",
        "criterion = torch.nn.MSELoss()\n",
        "output = model_src(dummy_input)\n",
        "loss = criterion(output, dummy_target)\n",
        "loss.backward()\n",
        "source_optimizer.step()\n",
        "\n",
        "param_mapping = {id(tgt_param): src_param for src_param, tgt_param in zip(model_src.parameters(), model_tgt.parameters())}\n",
        "target_optimizer = transfer_optim(source_optimizer, target_optimizer, param_mapping)\n",
        "\n",
        "print(source_optimizer.state_dict())\n",
        "print(target_optimizer.state_dict())\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title transfer_optim bad? 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    opt_state_dict = optimizer.state_dict()\n",
        "    for group in opt_state_dict['param_groups']:\n",
        "        # For each parameter index (p in param group refers to the layer parameters)\n",
        "        for param_idx, p in enumerate(group['params']):\n",
        "            print(p,source_layer.weight)\n",
        "            if p == source_layer.weight:\n",
        "                # Find the corresponding target layer parameter (in this case, target_layer.weight)\n",
        "                target_param = target_layer.weight\n",
        "                source_state = optimizer.state[p]  # Get the state for the source parameter\n",
        "\n",
        "                # If the parameter is found in the optimizer's state dict\n",
        "                if 'exp_avg' in source_state and 'exp_avg_sq' in source_state:\n",
        "                    exp_avg = source_state['exp_avg']  # First moment (momentum)\n",
        "                    exp_avg_sq = source_state['exp_avg_sq']  # Second moment (variance)\n",
        "\n",
        "                    # Handle input dimension mismatch (copy/truncate or pad)\n",
        "                    source_in_dim = source_layer.weight.shape[1]\n",
        "                    target_in_dim = target_layer.weight.shape[1]\n",
        "\n",
        "                    # Copy optimizer state (exp_avg and exp_avg_sq) accordingly\n",
        "                    with torch.no_grad():\n",
        "                        # Copy the available part and initialize new dimensions to zero\n",
        "                        new_exp_avg = torch.zeros_like(target_param)\n",
        "                        new_exp_avg_sq = torch.zeros_like(target_param)\n",
        "                        # new_exp_avg[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        # new_exp_avg_sq[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        new_exp_avg[:, :source_in_dim] = exp_avg[:, :target_in_dim]\n",
        "                        new_exp_avg_sq[:, :source_in_dim] = exp_avg_sq[:, :target_in_dim]\n",
        "\n",
        "                    # Update the target layer's optimizer state\n",
        "                    optimizer.state[target_param] = {\n",
        "                        'exp_avg': new_exp_avg,\n",
        "                        'exp_avg_sq': new_exp_avg_sq,\n",
        "                        'step': source_state['step']  # Keep the same step count\n",
        "                    }\n",
        "\n",
        "                # Handle the bias (if it exists)\n",
        "                if hasattr(source_layer, 'bias') and hasattr(target_layer, 'bias'):\n",
        "                    source_bias = optimizer.state[source_layer.bias]\n",
        "                    target_bias = target_layer.bias\n",
        "\n",
        "                    optimizer.state[target_bias] = source_bias\n",
        "    return optimizer\n",
        "\n",
        "# Example usage:\n",
        "d = 10  # Input dimension of the source layer\n",
        "a = 5   # Extra nodes to be omitted or added in the target layer\n",
        "m = 8   # Output dimension (same for both)\n",
        "\n",
        "# Source layer (input dimension d+a)\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "\n",
        "# Target layer (input dimension d, or d+a, or arbitrary)\n",
        "target_layer = nn.Linear(d, m)\n",
        "\n",
        "# Optimizer (using AdamW in this case)\n",
        "optimizer = torch.optim.AdamW(source_layer.parameters())\n",
        "\n",
        "# Perform weight transfer (from d+a to d or vice versa) here (assumed done already)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "# Transfer optimizer states\n",
        "optimizer = transfer_optimizer_state(source_layer, target_layer, optimizer)\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    state_dict = optimizer.state_dict()\n",
        "    for old_param, new_param in zip(source_layer.parameters(), target_layer.parameters()):\n",
        "        # If old_param exists in optimizer state\n",
        "        if old_param in state_dict['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = state_dict['state'][old_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                if key in ['exp_avg', 'exp_avg_sq']:  # for Adam or AdamW momentum estimates\n",
        "                    # Handle the shape adjustment (copy, shrink, or randomly initialise the extra nodes)\n",
        "                    new_state[key] = torch.zeros_like(new_param)  # Initialise with zeros\n",
        "                    new_state[key][:old_param.shape[0]] = value[:new_param.shape[0]]  # Copy old values\n",
        "                    # else:\n",
        "                    #     new_state[key] = value.clone()  # Copy directly if shapes match\n",
        "                else:\n",
        "                    new_state[key] = value  # Copy other states directly if they exist\n",
        "\n",
        "            # Set the new parameter in optimizer state\n",
        "            state_dict['state'][new_param] = new_state\n",
        "            # Remove the old parameter from the optimizer state\n",
        "            del state_dict['state'][old_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(state_dict)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optim(src_model, tgt_model, src_optim, tgt_optim):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    for src_param, tgt_param in zip(src_model.parameters(), tgt_model.parameters()):\n",
        "        # If src_param exists in optimizer state\n",
        "        if src_param in src_sd['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = src_sd['state'][src_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                new_state[key] = torch.zeros_like(tgt_param)  # Initialise with zeros\n",
        "                new_state[key][:src_param.shape[0]] = value[:tgt_param.shape[0]]  # Copy old values\n",
        "\n",
        "            src_sd['state'][tgt_param] = new_state\n",
        "            del src_sd['state'][src_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(src_sd)\n",
        "    return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "LKUSzmYLLuRh",
        "outputId": "07ca4b89-257b-4205-c5c8-6a96474ae82a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-186620617543>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# j=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mwht_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwht_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(o)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title rename wht_name\n",
        "# wht_name='jepa.enc.cnn.0.weight'\n",
        "wht_name='jepa.pred.weight_ih_l0'\n",
        "# wht_name='emb.weight'\n",
        "# print(o.isnumeric())\n",
        "# mask = [x.isnumeric() for x in o]\n",
        "# print(o[mask])\n",
        "na_=''\n",
        "# j=0\n",
        "\n",
        "for wht_name in agent.state_dict().keys():\n",
        "    o=wht_name.split('.')\n",
        "    # print(o)\n",
        "    name=wht_name\n",
        "    print(\"####\", wht_name)\n",
        "    for i in range(len(o)):\n",
        "        c = o[i]\n",
        "        if c.isnumeric():\n",
        "            na = '.'.join(o[:i])\n",
        "            me = '.'.join(o[i+1:])\n",
        "            # print(c_,c, c_<c, )\n",
        "            c=int(c)\n",
        "            if na!=na_: # param name diff\n",
        "                j=0 # reset num\n",
        "                c_=c # track wht_name num\n",
        "                na_=na # track param name\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('1', name)\n",
        "            elif c_<c: # same param name, diff num\n",
        "                j+=1\n",
        "                c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('2', name)\n",
        "            else: # same param name, same num\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('3', name)\n",
        "    print('4', name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CACQCCaxA_Y",
        "outputId": "b5d127cd-18ce-49e5-b1e2-d883cb34125a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1746836772511624\n"
          ]
        }
      ],
      "source": [
        "# @title geomloss, Python Optimal Transport\n",
        "# !pip install geomloss[full]\n",
        "\n",
        "import torch\n",
        "from geomloss import SamplesLoss  # See also ImagesLoss, VolumesLoss\n",
        "\n",
        "# # Create some large point clouds in 3D\n",
        "# x = torch.randn(100000, 3, requires_grad=True).cuda()\n",
        "# y = torch.randn(200000, 3).cuda()\n",
        "\n",
        "# x = torch.rand(1000, 1)\n",
        "# y = torch.rand(1000, 1)\n",
        "x = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "y = torch.tensor([0, 1, 0]).float().unsqueeze(-1)\n",
        "# k=1.\n",
        "# y = torch.tensor([k, k, k]).float().unsqueeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01) # 0.05, quadratic, Wasserstein-2. low blur => closer to true Wasserstein dist but slower compute\n",
        "\n",
        "loss = loss_fn(x, y)  # By default, use constant weights = 1/number of samples\n",
        "print(loss)\n",
        "# g_x, = torch.autograd.grad(L, [x])\n",
        "\n",
        "# [0, 1, 0]: 2.4253e-12, 2.4253e-12\n",
        "# [0, 0, 0.1]: 0.1350; [0, 0, 0.5]: 0.0417; [0, 0, 1]: 0\n",
        "# k=0.: 0.1666; k=0.1: 0.1383; k=0.333: 0.1111; k=0.5: 0.1250; k=1.: 0.3333\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "# Define x and y as n-dimensional tensors representing mass distributions\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# y = torch.tensor([0, 0, 1], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# x = torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1))\n",
        "x = nn.Parameter(torch.tensor([0,1.5,0]).float().unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "\n",
        "# Create a position tensor representing the index of each element\n",
        "positions_x = torch.arange(x.shape[0], dtype=float).unsqueeze(1)\n",
        "positions_y = torch.arange(y.shape[0], dtype=float).unsqueeze(1)\n",
        "\n",
        "# Sinkhorn loss using GeomLoss\n",
        "loss_fn = SamplesLoss(\"sinkhorn\", p=1, blur=0.05)  # p=1 for Wasserstein-1\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=1, blur=0.05, scaling=0.9, debias=True)\n",
        "\n",
        "transport_cost = loss_fn(positions_x, x, positions_y, y)\n",
        "\n",
        "print(transport_cost.item())\n",
        "# 1.298424361328248\n",
        "\n",
        "transport_cost.backward()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install POT\n",
        "\n",
        "import ot\n",
        "import numpy as np\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / np.sum(x)\n",
        "    # y = y / np.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = np.abs(np.arange(n)[:, None] - np.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = np.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "x = np.array([0.2, 0.3, 0.5])\n",
        "y = np.array([0, 0, 1])\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "# distance.backward()\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / torch.sum(x)\n",
        "    # y = y / torch.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = torch.abs(torch.arange(n)[:, None] - torch.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = torch.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "# x = np.array([0.2, 0.3, 0.5])\n",
        "# y = np.array([0, 0, 1])\n",
        "x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float())#.unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float()#.unsqueeze(-1)\n",
        "\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "distance.backward()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MqBL9hljvW-5"
      },
      "outputs": [],
      "source": [
        "# @title batchify argm train\n",
        "\n",
        "def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "    self.jepa.pred.train()\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "    lsx=sx.unsqueeze(1)\n",
        "    h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "    lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "    # print(lsx.shape, la.shape, lz.shape)\n",
        "    c=[]\n",
        "    for t in range(seq_len):\n",
        "        a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "        # print(sx.shape, a.shape, z.shape)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            tcost = -self.tcost(syh0)\n",
        "        c.append(tcost)\n",
        "        lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "        lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        cost += (tcost + icost)*gamma**t\n",
        "    return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "\n",
        "def argm(self, sy, sy_, h0, a, reward, lr=3e3): # 3e3\n",
        "    self.tcost.eval()\n",
        "    batch_size = sy.shape[0] # [batch_size, d_model]\n",
        "    z = nn.Parameter(torch.zeros((batch_size, self.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(z)\n",
        "    torch.nn.init.normal_(z, mean=0., std=.3/z.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([z], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    sy, sy_ = sy.detach(), sy_.detach()\n",
        "    out = sy - sy_\n",
        "    h0, a, reward = h0.detach(), a.detach(), reward.detach()\n",
        "    for i in range(10): # 10\n",
        "        with torch.amp.autocast('cuda'):\n",
        "\n",
        "\n",
        "\n",
        "            syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "            out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            repr_loss = F.mse_loss(out, out_[:, -1, :])\n",
        "            # syh0 = torch.cat([sy.flatten(1),F.dropout(h0_, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            syh0 = torch.cat([sy.flatten(1),h0_.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "            z_loss = torch.abs(z).sum() # z_loss = torch.norm(z)\n",
        "            print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def argm(lsy, sy, h0, la, rwd):\n",
        "    # lz = agent.argm(out, h0, la, reward)\n",
        "    agent.tcost.eval()\n",
        "    batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "    lz = nn.Parameter(torch.zeros((batch_size, bptt, agent.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(lz)\n",
        "    torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "    for i in range(3): # 10\n",
        "        sy_, h0_ = sy.detach(), h0.detach()\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "        with torch.cuda.amp.autocast():\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0_ = agent.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl + agent.zloss_coeff * z_loss\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    agent.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return lz.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# closs_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01)\n",
        "bptt = 25\n",
        "for batch, Sar in enumerate(train_loader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "    state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "    sy_ = agent.jepa.enc(state).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    # sx=sy_\n",
        "    state, action, reward = Sar # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "    state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "    for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "        with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "            lsy = agent.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "            la = agent.emb(act) # [batch_size, bptt, dim_a]\n",
        "            out = lsy - torch.cat([sy_, lsy[:,:-1]], dim=1)\n",
        "            # lz = agent.argm(out, h0, la, reward)\n",
        "            lz = argm(lsy, sy_, h0, la, rwd)\n",
        "            # lz = torch.zeros((batch_size, bptt, agent.dim_z), device=device)\n",
        "\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0 = agent.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            std_loss, cov_loss = agent.jepa.v_creg(agent.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "            jloss = agent.jepa.sim_coeff * repr_loss + agent.jepa.std_coeff * std_loss + agent.jepa.cov_coeff * cov_loss\n",
        "\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            # print(\"syh0, rwd\",syh0.shape,rwd.shape)\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            # reward_ = agent.tcost(syh0)\n",
        "            # clossl = wasserstein(rwd, reward_)#.squeeze(-1)\n",
        "            closs = agent.closs_coeff * clossl\n",
        "\n",
        "            # print(h0.requires_grad)\n",
        "            # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "            # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "            # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "            # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "            # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "            loss = jloss + closs\n",
        "\n",
        "            # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "            norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "            z_norm = torch.norm(z)\n",
        "            # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "            # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "            print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            optim.zero_grad()\n",
        "            sy_, h0 = sy_.detach(), h0.detach()\n",
        "    break\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mhkK_9AQm8_q",
        "Jt_UlGz6Xoq3",
        "wUhKd009Qvk3"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}