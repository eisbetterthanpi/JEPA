{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "427b6eb6-133d-43d2-dcf8-78ea695e5c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Jx0k_ndHOEMe",
        "outputId": "b5fb2840-eeb5-45ae-f0e0-e2a15f8e7ccc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-05466a22e258>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# visualise(agent.sense,layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mvisualise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N2TGs69fnrZo"
      },
      "outputs": [],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "for name, param in agent.emb.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred[0].weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "14dffe50-9482-4473-fe53-e158e31765bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcOidvtW9KAH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "4c074259-40e0-460a-ef25-074814c0a5fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2ce37b725f67>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title sklearn RBF\n",
        "# https://gist.github.com/eljost/2c4e1af652ef02b2989da341c5569af7\n",
        "# from nn_plot.ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import scipy.stats as st\n",
        "\n",
        "# np.random.seed(1)\n",
        "def func(x):\n",
        "    # print(x.shape)\n",
        "    # x= np.sum(x**2, axis=-1)\n",
        "    x=np.random.rand(x.shape[0])\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "res = 50\n",
        "num_pts=15\n",
        "X=np.random.rand(num_pts,2)*res\n",
        "# Y = func(X)\n",
        "Y=np.random.rand(num_pts)\n",
        "# print(X);print(Y)\n",
        "\n",
        "lim = 1\n",
        "# lin = np.linspace(-lim, lim, res)\n",
        "lin = np.linspace(0, res, res)\n",
        "x1, x2 = np.meshgrid(lin, lin)\n",
        "xx = np.vstack((x1.flatten(), x2.flatten())).T\n",
        "\n",
        "kernel = RBF()\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gp.fit(X, Y)\n",
        "# print(\"Learned kernel\", gp.kernel_)\n",
        "y_mean, y_cov = gp.predict(xx, return_cov=True)\n",
        "\n",
        "posteriors = st.multivariate_normal.rvs(mean=y_mean, cov=y_cov, size=1)\n",
        "\n",
        "ax = plt.figure().add_subplot(projection='3d')\n",
        "Z=posteriors.reshape(-1, res)\n",
        "# ax.plot_surface(x1, x2, Z)\n",
        "ax.plot_surface(x1, x2, Z, cmap='rainbow', alpha=0.7)\n",
        "\n",
        "# ax.plot_surface(x1, x2, posteriors.reshape(-1, res))\n",
        "ax.contour(x1, x2, Z, zdir='z', offset=-1, cmap='coolwarm') # https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html#sphx-glr-gallery-mplot3d-contour3d-3-py\n",
        "# ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-0.4, 0.5))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-1, 2))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, c=zdata, cmap='Greens');\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, cmap='Greens');\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VX5IExbRriwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title chatgpt RBFKernelLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBFKernelLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        self.centres = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        dists = torch.cdist(x, self.centres, p=2) ** 2\n",
        "        return torch.exp(-dists / (2 * self.sigma ** 2))\n",
        "\n",
        "class SaddlePointNetwork(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(SaddlePointNetwork, self).__init__()\n",
        "        self.rbf_layer = RBFKernelLayer(in_features, out_features, sigma)\n",
        "        self.linear = nn.Linear(out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rbf_output = self.rbf_layer(x)\n",
        "        # Introduce a saddle point structure\n",
        "        linear_output = self.linear(rbf_output)\n",
        "        # Example saddle function: x^2 - y^2\n",
        "        saddle_output = torch.sum(linear_output[:, :1]**2 - linear_output[:, 1:]**2, dim=1, keepdim=True)\n",
        "        return saddle_output\n",
        "\n",
        "# sin(ax)sin(bx)\n",
        "# (x^2 - y^2)\n",
        "import torch\n",
        "\n",
        "def rbf_saddle(x, y, gamma=1.0, a=1.0, b=1.0):\n",
        "    # RBF-like term\n",
        "    rbf_term = torch.exp(-gamma * torch.norm(x - y, p=2)**2)\n",
        "    # Saddle point term\n",
        "    saddle_term = (a * x)**2 - (b * y)**2\n",
        "    return rbf_term + saddle_term\n",
        "\n",
        "# Example usage\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "output = rbf_saddle(x, y)\n",
        "print(output)\n",
        "\n",
        "# Compute gradients\n",
        "output.backward()\n",
        "print(x.grad, y.grad)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "SusX7gpzxFNL",
        "outputId": "9f14a9da-e188-49ba-f5f5-70192ff33134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2.6321], grad_fn=<AddBackward0>)\n",
            "tensor([2.7358]) tensor([-4.7358])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plot 3d\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "y = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "X, Y = torch.meshgrid(x, y)\n",
        "Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rTmCo7pm0NxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_pts=1\n",
        "\n",
        "# X=torch.rand(num_pts,4)*2-1\n",
        "# X=torch.cat([torch.tensor([0,0]).unsqueeze(0),torch.rand(num_pts,2)*2-1], dim=-1)\n",
        "X=torch.cat([torch.zeros(1,1),torch.rand(num_pts,2)*2-1,torch.zeros(1,1)], dim=-1)\n",
        "Y=torch.rand(num_pts)\n",
        "print(X,Y)\n",
        "optim = torch.optim.SGD(model.parameters(), 1e-1)\n",
        "\n",
        "# model.train()\n",
        "pred = model(X)\n",
        "# print(Y.shape,pred.shape)\n",
        "# loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "loss = F.mse_loss(Y, pred.squeeze(-1))\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fWZaQTDFg1",
        "outputId": "4c5ced88-54f1-436e-89f9-66f1c8396373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000, -0.7231,  0.3792,  0.0000]]) tensor([0.3362])\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "xx = torch.linspace(-1, 1, 100)\n",
        "yy = torch.linspace(-1, 1, 100)\n",
        "X, Y = torch.meshgrid(xx, yy) # [100,100]\n",
        "xy = torch.cat([X.unsqueeze(-1), torch.zeros(X.shape+(2,)), Y.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "with torch.no_grad(): Z = model(xy).squeeze(-1)\n",
        "# print(Z)\n",
        "# print(Z.shape)\n",
        "\n",
        "# Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "boDd__PE2sGy",
        "outputId": "eb362343-bb06-4c4f-b0bb-2ac644b48e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test optim saddle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def f(x, y):\n",
        "    return x ** 2 - y ** 2 + x * y\n",
        "\n",
        "# (x-y)(x+y)+xy\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "x = nn.Parameter(torch.empty((batch, T, 256)))\n",
        "# x = torch.empty((1, T, 10))\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(x.repeat(batch,1,1))\n",
        "# print(x.shape)\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "z = nn.Parameter(torch.empty((batch, T, 1)))\n",
        "# z = torch.empty((1, T, 1))\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# z = nn.Parameter(z.repeat(batch,1,1))\n",
        "# optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "# optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.95), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "# .95,1e-1,3e-1\n",
        "# .99,\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(20): # num epochs\n",
        "    # loss = f(x,z)\n",
        "    # loss = f(x.sum(-1),z)\n",
        "    # z = argm(x)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz).sum()\n",
        "    loss.backward()\n",
        "    optim_x.step(); optim_z.step()\n",
        "    optim_x.zero_grad(); optim_z.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=min, max=max)\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "        z.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "xz = torch.cat([x,z], dim=-1)\n",
        "loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW6BYoXsX57o",
        "outputId": "21cec63d-30a9-4acb-e0f4-430acf98461b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.1287) tensor(0.3563) -0.08120705187320709\n",
            "1 tensor(0.2276) tensor(0.7539) -0.09181972593069077\n",
            "2 tensor(0.2854) tensor(1.1476) -0.10248862206935883\n",
            "3 tensor(0.3602) tensor(1.2121) -0.11243126541376114\n",
            "4 tensor(0.3878) tensor(1.2270) -0.12009574472904205\n",
            "5 tensor(0.3858) tensor(1.2394) -0.128182053565979\n",
            "6 tensor(0.3672) tensor(1.2633) -0.13648097217082977\n",
            "7 tensor(0.3958) tensor(1.2959) -0.14386269450187683\n",
            "8 tensor(0.4459) tensor(1.3191) -0.1513235718011856\n",
            "9 tensor(0.5092) tensor(1.3365) -0.15888839960098267\n",
            "10 tensor(0.5587) tensor(1.3385) -0.16620759665966034\n",
            "11 tensor(0.5961) tensor(1.3403) -0.17150619626045227\n",
            "12 tensor(0.6211) tensor(1.3332) -0.17441897094249725\n",
            "13 tensor(0.6540) tensor(1.2494) -0.17530150711536407\n",
            "14 tensor(0.6935) tensor(1.1771) -0.17646200954914093\n",
            "15 tensor(0.7228) tensor(1.1982) -0.1773337870836258\n",
            "16 tensor(0.7679) tensor(1.2322) -0.17774531245231628\n",
            "17 tensor(0.8028) tensor(1.2458) -0.17798927426338196\n",
            "18 tensor(0.8286) tensor(1.2579) -0.17835086584091187\n",
            "19 tensor(0.8461) tensor(1.2687) -0.17864727973937988\n",
            "tensor([-0.0112, -0.0112, -0.0112, -0.0112, -0.0112, -0.0112, -0.0112, -0.0112,\n",
            "        -0.0112, -0.0112, -0.0112, -0.0112, -0.0112, -0.0112, -0.0112, -0.0112],\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "tensor([[ 0.8365, -0.9235, -1.0000, -1.0000,  1.0000,  0.2072,  0.6383, -1.0000,\n",
            "         -1.0000,  0.8461,  1.0000,  1.0000,  0.3557, -1.0000,  0.6191,  1.0000,\n",
            "         -0.9256, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
            "          1.0000, -0.7897, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
            "          1.0000, -1.0000,  0.8999, -1.0000, -1.0000,  1.0000,  1.0000,  0.4702,\n",
            "          1.0000, -1.0000, -0.9979,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n",
            "         -1.0000, -0.4370, -0.5913, -0.2169,  1.0000,  1.0000,  0.1770, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -0.4698, -1.0000, -0.9496,  1.0000,  1.0000,\n",
            "          0.7939, -1.0000,  1.0000,  1.0000,  1.0000,  0.1774, -0.6339, -1.0000,\n",
            "          1.0000, -1.0000, -1.0000, -1.0000,  0.4800, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000,  1.0000, -1.0000, -0.7884,  1.0000,  1.0000,  1.0000, -1.0000,\n",
            "          1.0000,  1.0000, -0.1540,  0.4009, -1.0000, -1.0000,  0.9757,  0.0990,\n",
            "          1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -0.4460, -1.0000,  1.0000,  1.0000, -0.9599,  1.0000, -0.6124,\n",
            "          1.0000, -0.2740, -1.0000, -0.6603, -1.0000, -1.0000, -0.3423,  1.0000,\n",
            "          0.2088, -0.8541,  1.0000,  0.1256,  1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000,  1.0000,  0.8999, -1.0000, -1.0000,  1.0000, -0.7259,\n",
            "          0.8488,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,  0.3548,\n",
            "         -1.0000, -0.7585, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000, -0.7176,\n",
            "         -1.0000,  1.0000,  1.0000, -0.9555, -1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000,  1.0000,  0.7810, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n",
            "          1.0000, -1.0000, -1.0000, -0.8327, -1.0000,  1.0000,  1.0000, -1.0000,\n",
            "         -0.6765,  1.0000,  1.0000, -0.8344,  1.0000,  1.0000,  1.0000,  0.4744,\n",
            "         -0.1834, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  0.7575,\n",
            "          0.2071, -1.0000, -1.0000, -0.4868,  1.0000, -1.0000,  1.0000,  1.0000,\n",
            "          1.0000,  1.0000,  1.0000, -0.7667,  1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000,  1.0000, -0.5873, -1.0000,  1.0000,  1.0000,  1.0000,\n",
            "          1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
            "         -0.0704,  0.5317, -1.0000, -1.0000, -0.1859, -1.0000, -1.0000,  1.0000,\n",
            "         -0.6600, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
            "          0.3355,  1.0000, -1.0000, -1.0000,  0.7728,  0.1114, -1.0000, -0.4132]]) tensor([[1.]]) -0.011200978420674801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test optim saddle argm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def argm(sx, lr=3e3): # 3e3\n",
        "    # batch=sx.size(dim=0)\n",
        "    _, T, _ = sx.shape\n",
        "    batch = 16\n",
        "    # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "    z = nn.Parameter(torch.empty((batch, T, 1)))\n",
        "    torch.nn.init.xavier_uniform_(z)\n",
        "    optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    sx = sx.detach().repeat(batch,1,1)\n",
        "    for i in range(5): # 10\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            cost = model(sxz)\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "        # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    # return z.detach()\n",
        "    idx = torch.argmax(loss)\n",
        "    return z[idx].detach().unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "batch=1\n",
        "T=1\n",
        "x = nn.Parameter(torch.empty((batch, T, 256)))\n",
        "# x = torch.empty((1, T, 10))\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(x.repeat(batch,1,1))\n",
        "# print(x.shape)\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(20):\n",
        "    z = argm(x)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz).sum()\n",
        "    loss.backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=min, max=max)\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "xz = torch.cat([x,z], dim=-1)\n",
        "loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "\n",
        "# tensor([[ 1.0000, -0.5169, -0.3977,  1.0000, -0.3707, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000]]) tensor([[-1.]]) tensor([[0.0018]],\n",
        "# tensor([[ 1.0000, -1.0000,  0.9367, -0.6767, -0.9889, -1.0000,  1.0000, -0.4944, -1.0000, -1.0000]]) tensor([[1.]]) tensor([[0.0131]],\n",
        "# tensor([[ 1.0000, -1.0000, -1.0000,  1.0000,  0.0348, -1.0000,  0.3298, -0.6863, -1.0000, -1.0000]]) tensor([[-1.]]) 0.009066864848136902\n",
        "\n"
      ],
      "metadata": {
        "id": "GJdFpDr2wIMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6123b16e-142a-4195-ed6b-f30d40da8ff5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.0665) tensor(0.2360) -0.005105329677462578\n",
            "1 tensor(0.1655) tensor(0.0914) -0.0057768854312598705\n",
            "2 tensor(0.2234) tensor(0.6963) -0.006432546768337488\n",
            "3 tensor(0.2982) tensor(-0.3470) -0.007018279284238815\n",
            "4 tensor(0.3815) tensor(0.0714) -0.007557516917586327\n",
            "5 tensor(0.4700) tensor(0.8594) -0.008033445104956627\n",
            "6 tensor(0.5595) tensor(0.4958) -0.008566698059439659\n",
            "7 tensor(0.6146) tensor(-0.3230) -0.009033972397446632\n",
            "8 tensor(0.6436) tensor(-0.3407) -0.009550676681101322\n",
            "9 tensor(0.6817) tensor(0.5070) -0.010002668015658855\n",
            "10 tensor(0.7233) tensor(0.6725) -0.010425419546663761\n",
            "11 tensor(0.7680) tensor(0.7575) -0.010741738602519035\n",
            "12 tensor(0.8238) tensor(0.8503) -0.010924096219241619\n",
            "13 tensor(0.8883) tensor(-0.0320) -0.011058242060244083\n",
            "14 tensor(0.9597) tensor(0.5321) -0.01110790017992258\n",
            "15 tensor(1.0354) tensor(-0.7496) -0.011109852232038975\n",
            "16 tensor(1.0805) tensor(0.8230) -0.011164958588778973\n",
            "17 tensor(1.0834) tensor(-0.6253) -0.011203834787011147\n",
            "18 tensor(1.0837) tensor(0.2284) -0.01120504830032587\n",
            "19 tensor(1.0841) tensor(0.3205) -0.011202049441635609\n",
            "tensor(-0.0112, grad_fn=<SqueezeBackward0>)\n",
            "tensor([[ 1.0000, -0.6943, -1.0000, -1.0000,  1.0000,  0.0165,  0.9791, -1.0000,\n",
            "         -1.0000,  0.6845,  1.0000,  1.0000,  0.3771, -1.0000,  0.6784,  1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
            "          1.0000, -0.5794, -1.0000,  1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n",
            "          1.0000, -1.0000,  0.7016, -1.0000, -0.8540,  1.0000,  1.0000,  1.0000,\n",
            "          1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000, -1.0000,\n",
            "         -1.0000, -0.2759, -0.3689, -0.5817,  1.0000,  1.0000,  0.3349, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -0.2257, -1.0000, -0.6026,  1.0000,  1.0000,\n",
            "          1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.1559, -0.9543, -1.0000,\n",
            "          1.0000, -1.0000, -1.0000, -1.0000,  0.4511, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000,  1.0000, -1.0000, -0.5894,  1.0000,  1.0000,  1.0000, -1.0000,\n",
            "          1.0000,  1.0000,  0.2530,  0.3300, -1.0000, -1.0000,  0.8874,  0.2997,\n",
            "          1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -0.1968, -1.0000,  1.0000,  1.0000, -0.6979,  1.0000, -0.6007,\n",
            "          1.0000, -0.1398, -1.0000, -0.3495, -1.0000, -1.0000, -0.2062,  1.0000,\n",
            "          0.1850, -1.0000,  1.0000,  0.3197,  1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000,  1.0000,  0.9279, -1.0000, -1.0000,  1.0000, -0.7829,\n",
            "          1.0000,  1.0000, -1.0000, -1.0000,  0.7273,  1.0000, -1.0000,  0.4032,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000,  1.0000, -1.0000,  1.0000, -0.7293,\n",
            "         -1.0000,  1.0000,  1.0000, -0.9770, -1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000,  1.0000,  0.8303, -1.0000,  1.0000,  1.0000, -1.0000,  1.0000,\n",
            "          1.0000, -1.0000, -1.0000, -0.8281, -1.0000,  1.0000,  1.0000, -1.0000,\n",
            "         -1.0000,  1.0000,  1.0000, -0.5607,  1.0000,  1.0000,  1.0000,  0.3186,\n",
            "         -0.1272, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000, -1.0000,  0.7964,\n",
            "          0.1559, -1.0000, -1.0000, -0.1394,  1.0000, -1.0000,  1.0000,  1.0000,\n",
            "          1.0000,  1.0000,  1.0000, -0.6386,  1.0000,  1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000,  1.0000, -0.5059, -1.0000,  1.0000,  1.0000,  1.0000,\n",
            "          1.0000, -1.0000,  1.0000, -1.0000, -1.0000,  1.0000,  1.0000, -1.0000,\n",
            "          0.0290,  0.7653, -1.0000, -0.9270, -0.2979, -1.0000, -1.0000,  1.0000,\n",
            "         -0.5877, -1.0000,  1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
            "          0.7000,  1.0000, -1.0000, -1.0000,  0.9036,  0.4601, -1.0000, -0.3629]]) tensor([[0.3205]]) -0.011205191724002361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# x,z = torch.tensor([[ 1.0000, -0.5169, -0.3977,  1.0000, -0.3707, -1.0000, -1.0000,  1.0000, -1.0000, -1.0000]]), torch.tensor([[1.]]) # -1 tensor([[0.0018]],\n",
        "# x,z = torch.tensor([[ 1.0000, -1.0000,  0.9367, -0.6767, -0.9889, -1.0000,  1.0000, -0.4944, -1.0000, -1.0000]]), torch.tensor([[-1.]]) # og 1 tensor([[0.0131]], -1 0.0115\n",
        "x,z = torch.tensor([[ 1.0000, -1.0000, -1.0000,  1.0000,  0.0348, -1.0000,  0.3298, -0.6863, -1.0000, -1.0000]]), torch.tensor([[1.]]) # og -1 0.009067\n",
        "\n",
        "xz = torch.cat([x,z], dim=-1)\n",
        "loss = model(xz)\n",
        "print(loss.item())\n",
        "# idx = torch.argmin(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbcSzqlRM_6u",
        "outputId": "1b6ca3ba-bdb4-4390-c316-318941dd931c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0074355751276016235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "478f87fa-0c40-4c4c-f8d9-6cb573ce232e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-11004c69714f>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "        # self.h0 = None\n",
        "        self.h0 = torch.randn((self.jepa.pred.num_layers, d_model), device=device)\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "\n",
        "    def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # 20\n",
        "        act = lact.cpu()[0,:k].tolist()\n",
        "        # act = out[0].cpu()[0,:k].tolist()\n",
        "        self.h0=lh0[k-1].unsqueeze(0)\n",
        "        self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        # return lact, lh0, lx, lz\n",
        "        return act\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        _,T,_ = sx.shape\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            # print(i,x.data, z.squeeze(), loss.item())\n",
        "            print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        # xz = torch.cat([x,z], dim=-1)\n",
        "        # loss = model(xz)\n",
        "        # print(loss.squeeze())\n",
        "        idx = torch.argmin(loss)\n",
        "        # print(x[idx],z[idx],loss[idx])\n",
        "        print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "        def argm(self.sx, lr=3e3): # 3e3\n",
        "            # batch=sx.size(dim=0)\n",
        "            _, T, _ = sx.shape\n",
        "            batch = 16\n",
        "            # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "            z = nn.Parameter(torch.empty((batch, T, 1)))\n",
        "            torch.nn.init.xavier_uniform_(z)\n",
        "            optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "            # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "            sx = sx.detach().repeat(batch,1,1)\n",
        "            for i in range(5): # 10\n",
        "                sxz = torch.cat([sx, z], dim=-1)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    cost = model(sxz)\n",
        "                cost.sum().backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "                # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "                # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "                # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "            # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "            # return z.detach()\n",
        "            idx = torch.argmax(loss)\n",
        "            return z[idx].detach().unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        lh0=h0\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                out, h0 = self.jepa.pred(sxaz, h0)\n",
        "                # print(out.shape)\n",
        "                # out = out[:, -1, :]\n",
        "                sx = sx + out\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            lh0 = torch.cat([lh0, h0], dim=0)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.randn((self.jepa.pred.num_layers, batch_size, self.d_model), device=device)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0)\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    # _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    # stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + 1000*clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "abb440fb-6cd6-4ce8-b1e2-34b3c6ecd685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB\n",
            "From (redirected): https://drive.google.com/uc?id=1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB&confirm=t&uuid=adbb6f56-b5dd-4983-addf-f9ec7f2ea71f\n",
            "To: /content/buffergo.pkl\n",
            "100% 786M/786M [00:09<00:00, 82.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3 convenc4\n",
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "import pickle\n",
        "# !gdown 19VQp7UjXqH8kJjEPABOTHDV8reg8r7Zn -O buffer512down.pkl # B\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1HRwU4u7Y6YjQWmC8xlKrDe4YGcNUWxvQ -O buffer512.pkl # B\n",
        "# with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "!gdown 1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB -O buffergo.pkl # S3\n",
        "with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "502c0788-4187-4da1-852c-ed5fa12006c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-49fecd192e7d>:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "def save(folder, name='agent.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    # agent.mem.save(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer512.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# save(folder, name='agent_jepa753333256.pth')\n",
        "# buffer = load(folder)\n",
        "# save('/content/')\n",
        "# buffer = load('/content/')\n",
        "\n",
        "# name='agent.pth'\n",
        "# print(folder+name)\n",
        "# torch.load(folder+name, map_location='o')\n",
        "# with open(folder+'buffer512down.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "# with open(folder+'buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "optim.load_state_dict(optimsd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG4Wn3c8IN4V"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = buffer[7][80][0]\n",
        "state = transform(state).unsqueeze(0).to(device)#[0]\n",
        "act = agent(state).cpu()[:1].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim1.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.normal(mean=50, std=25)\n",
        "# torch.empty(1).uniform_(from=20, to=80)\n",
        "\n",
        "print(type(buffer[6][-1][2]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtUPaTBvYRGC",
        "outputId": "7ff48918-24b1-4f10-912b-b97cf3a2571a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        buffer = self.process(buffer)\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 80):] for episode in cleaned]\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 25 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e3fpbtNOiz1",
        "outputId": "9815b9e7-5649-4795-d090-5d0e15f62ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.,  0., -1., -1., -1., -1., -1.,  0., -1.,  0.])\n",
            "tensor([-1.,  0., -1.,  0., -1.,  0.,  0., -1.,  0., -1.])\n",
            "tensor([-1.,  0., -1., -1., -1.,  0., -1., -1., -1.,  0.])\n",
            "tensor([-1., -1., -1., -1.,  0.,  0., -1., -1., -1.,  0.])\n",
            "tensor([ 0., -1.,  0.,  0., -1., -1., -1., -1.,  0.,  0.])\n",
            "tensor([ 0., -1., -1., -1., -1.,  0.,  0.,  0.,  0., -1.])\n",
            "tensor([-1.,  0.,  0.,  0.])\n",
            "tensor([-0.9897, -0.0238, -1.0627, -0.9915, -0.9915, -0.9923, -0.9923, -0.0231,\n",
            "        -0.9932, -0.0233])\n",
            "tensor([-0.9924, -0.1491, -0.9883, -0.0200, -0.9913, -0.0167, -0.0156, -0.9914,\n",
            "        -0.0313, -0.9903])\n",
            "tensor([-0.9930, -0.0256, -0.9934, -0.9924, -0.9922, -0.0245, -0.9908, -0.9924,\n",
            "        -0.9945, -0.0248])\n",
            "tensor([-0.9915, -0.9914, -0.9926, -0.9912, -0.0472, -0.0202, -0.9912, -0.9938,\n",
            "        -0.9911, -0.0540])\n",
            "tensor([-0.0205, -0.9935, -0.9977, -0.0341, -0.9936, -0.9938, -0.9905, -0.0193,\n",
            "        -0.0270, -0.0516])\n",
            "tensor([-0.0280, -0.9885, -0.0672, -0.9896, -0.9914, -0.0278, -0.0262, -0.0175,\n",
            "        -0.0220, -0.9928])\n",
            "tensor([-0.9925, -0.0261, -0.0150, -0.0173])\n",
            "tensor(0.0449)\n"
          ]
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "train_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "train_data = Datasetme(train_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    train_data=list(zip(state,reward))\n",
        "    train_data = Datasetme(train_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range((len(labels)//10)+1):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    # print(pred)\n",
        "    for x in range((len(pred)//10)+1):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(F.mse_loss(labels, pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OksdjCeJYpYh"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "for i in range(30):\n",
        "    print(i)\n",
        "    # agent.train_ae(train_loader, optim)\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "# 10 epochs 15m23s\n",
        "\n",
        "\n",
        "\n",
        "# loss 0.00027325598057359457\n",
        "# loss 0.00027538512949831784\n",
        "# loss 0.000279315427178517\n",
        "# loss 0.00028544830274768174\n",
        "# loss 0.00029633755912072957\n",
        "# loss 0.0002964686427731067\n",
        "# loss 0.00030574199627153575\n",
        "# loss 0.00031030713580548763\n",
        "# loss 0.00011697990703396499\n",
        "# loss 0.00012466282350942492\n",
        "\n",
        "# loss 0.0002805441035889089\n",
        "# loss 0.0002813159371726215\n",
        "# loss 0.00028616547933779657\n",
        "# loss 0.00029815093148499727\n",
        "# loss 0.0003055527340620756\n",
        "# loss 0.0002878434315789491\n",
        "# loss 0.0002965773455798626\n",
        "# loss 0.00030164531199261546\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "7d9ce865-612f-45b5-f95c-ddbdd1ae6eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PraFUAPB3j7v",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17537f1e-3b69-4779-894f-b4045a3828a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "tcost icost -0.206787109375 0.0\n",
            "tcost icost -0.02227783203125 0.0\n",
            "tcost icost 0.00836944580078125 0.0\n",
            "loss tensor([[-0.2148]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.1202, -0.5179, -0.3026],\n",
            "         [ 0.4726,  0.5639,  0.5914],\n",
            "         [ 0.0466, -0.2355, -0.4530],\n",
            "         [ 0.3543, -0.2557,  0.5634],\n",
            "         [-0.4586,  0.4956,  0.3525],\n",
            "         [-0.0695,  0.4390, -0.2830]]], device='cuda:0') tensor([-0.6438, -0.4521, -0.3690,  0.0911, -0.9497, -0.4836], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.21484375\n",
            "act tensor([[10, 12,  4,  7,  3, 11]], device='cuda:0')\n",
            "tcost icost -0.0027866363525390625 0.0\n",
            "tcost icost -0.016021728515625 0.0\n",
            "tcost icost -0.034454345703125 0.0\n",
            "tcost icost -0.208740234375 0.0\n",
            "tcost icost -0.02410888671875 0.0\n",
            "tcost icost 0.00737762451171875 0.0\n",
            "loss tensor([[-0.2087]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.0994, -0.5331, -0.3232],\n",
            "         [ 0.4672,  0.5658,  0.6010],\n",
            "         [ 0.0358, -0.2516, -0.4642],\n",
            "         [ 0.3625, -0.2403,  0.5711],\n",
            "         [-0.4600,  0.4938,  0.3494],\n",
            "         [-0.0693,  0.4394, -0.2825]]], device='cuda:0') tensor([-0.6151, -0.4731, -0.3507,  0.0717, -0.9456, -0.4832], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.208740234375\n",
            "act tensor([[10, 12,  4,  7,  3, 11]], device='cuda:0')\n",
            "tcost icost -0.0014715194702148438 0.0\n",
            "tcost icost -0.01056671142578125 0.0\n",
            "tcost icost -0.0296478271484375 0.0\n",
            "tcost icost -0.1953125 0.0\n",
            "tcost icost -0.0236663818359375 0.0\n",
            "tcost icost 0.00801849365234375 0.0\n",
            "loss tensor([[-0.1881]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.0754, -0.5497, -0.3462],\n",
            "         [ 0.4614,  0.5680,  0.6111],\n",
            "         [ 0.0236, -0.2697, -0.4764],\n",
            "         [ 0.3705, -0.2241,  0.5790],\n",
            "         [-0.4613,  0.4921,  0.3463],\n",
            "         [-0.0691,  0.4399, -0.2818]]], device='cuda:0') tensor([-0.5832, -0.4960, -0.3297,  0.0520, -0.9415, -0.4829], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1881103515625\n",
            "act tensor([[10, 12, 10,  7,  3, 11]], device='cuda:0')\n",
            "tcost icost 9.79304313659668e-05 0.0\n",
            "tcost icost -0.00473785400390625 0.0\n",
            "tcost icost -0.02899169921875 0.0\n",
            "tcost icost -0.19873046875 0.0\n",
            "tcost icost -0.0254974365234375 0.0\n",
            "tcost icost 0.00768280029296875 0.0\n",
            "loss tensor([[-0.1848]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.0521, -0.5647, -0.3680],\n",
            "         [ 0.4560,  0.5700,  0.6202],\n",
            "         [ 0.0121, -0.2868, -0.4868],\n",
            "         [ 0.3783, -0.2080,  0.5867],\n",
            "         [-0.4625,  0.4903,  0.3434],\n",
            "         [-0.0688,  0.4404, -0.2811]]], device='cuda:0') tensor([-0.5520, -0.5158, -0.3103,  0.0322, -0.9375, -0.4826], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.184814453125\n",
            "act tensor([[10, 12, 10,  6,  3, 11]], device='cuda:0')\n",
            "tcost icost 0.0016126632690429688 0.0\n",
            "tcost icost -0.0005707740783691406 0.0\n",
            "tcost icost -0.0235595703125 0.0\n",
            "tcost icost -0.18603515625 0.0\n",
            "tcost icost -0.0255279541015625 0.0\n",
            "tcost icost 0.0078887939453125 0.0\n",
            "loss tensor([[-0.1656]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.0269, -0.5797, -0.3907],\n",
            "         [ 0.4501,  0.5720,  0.6295],\n",
            "         [-0.0011, -0.3063, -0.4985],\n",
            "         [ 0.3856, -0.1917,  0.5943],\n",
            "         [-0.4638,  0.4886,  0.3405],\n",
            "         [-0.0686,  0.4410, -0.2803]]], device='cuda:0') tensor([-0.5187, -0.5342, -0.2880,  0.0124, -0.9335, -0.4822], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1656494140625\n",
            "act tensor([[10, 12, 10,  6,  3, 11]], device='cuda:0')\n",
            "tcost icost 0.0031833648681640625 0.0\n",
            "tcost icost 0.00337982177734375 0.0\n",
            "tcost icost -0.0175323486328125 0.0\n",
            "tcost icost -0.16845703125 0.0\n",
            "tcost icost -0.0244140625 0.0\n",
            "tcost icost 0.00839996337890625 0.0\n",
            "loss tensor([[-0.1417]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 1.4292e-04, -5.9405e-01, -4.1337e-01],\n",
            "         [ 4.4394e-01,  5.7421e-01,  6.3887e-01],\n",
            "         [-1.5488e-02, -3.2728e-01, -5.1021e-01],\n",
            "         [ 3.9277e-01, -1.7514e-01,  6.0187e-01],\n",
            "         [-4.6510e-01,  4.8680e-01,  3.3753e-01],\n",
            "         [-6.8342e-02,  4.4154e-01, -2.7943e-01]]], device='cuda:0') tensor([-0.4845, -0.5534, -0.2646, -0.0079, -0.9294, -0.4818], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1417236328125\n",
            "search tensor([[[-0.4651,  0.4868,  0.3375],\n",
            "         [-0.0683,  0.4415, -0.2794],\n",
            "         [-0.1757, -0.3745, -0.2704],\n",
            "         [-0.1096,  0.2350, -0.3838],\n",
            "         [-0.3359,  0.1848,  0.1687],\n",
            "         [ 0.0789, -0.3963,  0.1127]]], device='cuda:0') tensor([-0.9294, -0.4818, -0.2291, -0.9034,  0.4500,  0.1004], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[ 3, 11,  4, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost -0.0170440673828125 0.0\n",
            "tcost icost -0.0177459716796875 0.0\n",
            "tcost icost -0.0139617919921875 0.0\n",
            "tcost icost 0.01418304443359375 0.0\n",
            "tcost icost -0.226318359375 0.0\n",
            "tcost icost -0.028564453125 0.0\n",
            "loss tensor([[-0.1992]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.4820,  0.4688,  0.3137],\n",
            "         [-0.0608,  0.4499, -0.2728],\n",
            "         [-0.1768, -0.3754, -0.2700],\n",
            "         [-0.1140,  0.2210, -0.3965],\n",
            "         [-0.3272,  0.2006,  0.1815],\n",
            "         [ 0.0780, -0.3975,  0.1114]]], device='cuda:0') tensor([-0.9026, -0.4936, -0.2265, -0.8914,  0.4322,  0.1024], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.19921875\n",
            "act tensor([[ 3, 11,  4, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost -0.0158843994140625 0.0\n",
            "tcost icost -0.015350341796875 0.0\n",
            "tcost icost -0.01096343994140625 0.0\n",
            "tcost icost 0.01593017578125 0.0\n",
            "tcost icost -0.2181396484375 0.0\n",
            "tcost icost -0.0269012451171875 0.0\n",
            "loss tensor([[-0.1859]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.5003,  0.4479,  0.2877],\n",
            "         [-0.0526,  0.4592, -0.2651],\n",
            "         [-0.1776, -0.3763, -0.2701],\n",
            "         [-0.1186,  0.2064, -0.4092],\n",
            "         [-0.3184,  0.2162,  0.1943],\n",
            "         [ 0.0772, -0.3987,  0.1100]]], device='cuda:0') tensor([-0.8738, -0.5066, -0.2227, -0.8774,  0.4144,  0.1043], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1859130859375\n",
            "act tensor([[ 3, 11,  4, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost -0.0146484375 0.0\n",
            "tcost icost -0.012603759765625 0.0\n",
            "tcost icost -0.007678985595703125 0.0\n",
            "tcost icost 0.0197601318359375 0.0\n",
            "tcost icost -0.20947265625 0.0\n",
            "tcost icost -0.02496337890625 0.0\n",
            "loss tensor([[-0.1700]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.5190,  0.4246,  0.2604],\n",
            "         [-0.0440,  0.4689, -0.2568],\n",
            "         [-0.1786, -0.3776, -0.2706],\n",
            "         [-0.1231,  0.1916, -0.4218],\n",
            "         [-0.3095,  0.2317,  0.2073],\n",
            "         [ 0.0764, -0.3999,  0.1086]]], device='cuda:0') tensor([-0.8436, -0.5204, -0.2183, -0.8628,  0.3965,  0.1063], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1700439453125\n",
            "act tensor([[ 3, 11,  4, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost -0.01337432861328125 0.0\n",
            "tcost icost -0.00975799560546875 0.0\n",
            "tcost icost -0.004108428955078125 0.0\n",
            "tcost icost 0.023834228515625 0.0\n",
            "tcost icost -0.2008056640625 0.0\n",
            "tcost icost -0.022918701171875 0.0\n",
            "loss tensor([[-0.1533]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.5400,  0.3963,  0.2290],\n",
            "         [-0.0344,  0.4817, -0.2442],\n",
            "         [-0.1813, -0.3819, -0.2742],\n",
            "         [-0.1265,  0.1781, -0.4335],\n",
            "         [-0.3007,  0.2468,  0.2204],\n",
            "         [ 0.0755, -0.4012,  0.1072]]], device='cuda:0') tensor([-0.8092, -0.5377, -0.2099, -0.8495,  0.3787,  0.1083], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1533203125\n",
            "act tensor([[ 3, 11,  4, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost -0.01192474365234375 0.0\n",
            "tcost icost -0.006389617919921875 0.0\n",
            "tcost icost 0.0012617111206054688 0.0\n",
            "tcost icost 0.03521728515625 0.0\n",
            "tcost icost -0.1920166015625 0.0\n",
            "tcost icost -0.0203704833984375 0.0\n",
            "loss tensor([[-0.1290]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.5712,  0.3510,  0.1822],\n",
            "         [-0.0200,  0.5034, -0.2222],\n",
            "         [-0.1847, -0.3903, -0.2853],\n",
            "         [-0.1302,  0.1596, -0.4497],\n",
            "         [-0.2917,  0.2617,  0.2337],\n",
            "         [ 0.0746, -0.4024,  0.1057]]], device='cuda:0') tensor([-0.7620, -0.5692, -0.1910, -0.8229,  0.3607,  0.1103], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1290283203125\n",
            "act tensor([[ 3, 11,  4, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost -0.009979248046875 0.0\n",
            "tcost icost -0.0019702911376953125 0.0\n",
            "tcost icost 0.014007568359375 0.0\n",
            "tcost icost 0.08477783203125 0.0\n",
            "tcost icost -0.1806640625 0.0\n",
            "tcost icost -0.01617431640625 0.0\n",
            "loss tensor([[-0.0667]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.6132,  0.2861,  0.1238],\n",
            "         [-0.0024,  0.5245, -0.2078],\n",
            "         [-0.1803, -0.3900, -0.2969],\n",
            "         [-0.1391,  0.1158, -0.4874],\n",
            "         [-0.2826,  0.2766,  0.2474],\n",
            "         [ 0.0738, -0.4036,  0.1043]]], device='cuda:0') tensor([-0.7093, -0.6151, -0.1732, -0.7491,  0.3429,  0.1123], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.066650390625\n",
            "act tensor([[ 1, 11,  4, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost -0.0104522705078125 0.0\n",
            "tcost icost -0.007415771484375 0.0\n",
            "tcost icost 0.00391387939453125 0.0\n",
            "tcost icost 0.09130859375 0.0\n",
            "tcost icost -0.1748046875 0.0\n",
            "tcost icost -0.01531982421875 0.0\n",
            "loss tensor([[-0.0711]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.6598,  0.1916,  0.0432],\n",
            "         [ 0.0232,  0.5713, -0.1513],\n",
            "         [-0.1921, -0.4255, -0.3434],\n",
            "         [-0.1383,  0.0965, -0.5074],\n",
            "         [-0.2736,  0.2912,  0.2611],\n",
            "         [ 0.0729, -0.4049,  0.1028]]], device='cuda:0') tensor([-0.6266, -0.6845, -0.1156, -0.7110,  0.3249,  0.1144], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.07110595703125\n",
            "act tensor([[13, 11, 10, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost -0.00799560546875 0.0\n",
            "tcost icost -0.0035800933837890625 0.0\n",
            "tcost icost 0.013946533203125 0.0\n",
            "tcost icost 0.156494140625 0.0\n",
            "tcost icost -0.1556396484375 0.0\n",
            "tcost icost -0.01036834716796875 0.0\n",
            "loss tensor([[0.0059]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.7045,  0.0753, -0.0423],\n",
            "         [ 0.0502,  0.6146, -0.0977],\n",
            "         [-0.2012, -0.4689, -0.4013],\n",
            "         [-0.1382,  0.0629, -0.5412],\n",
            "         [-0.2643,  0.3061,  0.2758],\n",
            "         [ 0.0718, -0.4065,  0.1010]]], device='cuda:0') tensor([-0.5341, -0.7642, -0.0546, -0.6553,  0.3062,  0.1170], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.005901336669921875\n",
            "act tensor([[13,  3, 10, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost -0.004032135009765625 0.0\n",
            "tcost icost 0.003631591796875 0.0\n",
            "tcost icost 0.029937744140625 0.0\n",
            "tcost icost 0.28173828125 0.0\n",
            "tcost icost -0.12127685546875 0.0\n",
            "tcost icost -0.005840301513671875 0.0\n",
            "loss tensor([[0.1459]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.7358, -0.0513, -0.1227],\n",
            "         [ 0.0789,  0.6582, -0.0297],\n",
            "         [-0.2179, -0.5535, -0.4804],\n",
            "         [-0.1243,  0.0579, -0.5624],\n",
            "         [-0.2554,  0.3204,  0.2901],\n",
            "         [ 0.0705, -0.4083,  0.0987]]], device='cuda:0') tensor([-0.4184, -0.8407,  0.0219, -0.6578,  0.2872,  0.1197], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1458740234375\n",
            "act tensor([[13,  3, 10, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.0010051727294921875 0.0\n",
            "tcost icost 0.0106964111328125 0.0\n",
            "tcost icost 0.12384033203125 0.0\n",
            "tcost icost 0.07281494140625 0.0\n",
            "tcost icost -0.1151123046875 0.0\n",
            "tcost icost 0.0180816650390625 0.0\n",
            "loss tensor([[0.0991]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.7423, -0.0512, -0.1161],\n",
            "         [ 0.0719,  0.6222, -0.1158],\n",
            "         [-0.2245, -0.4977, -0.4690],\n",
            "         [-0.1380,  0.0015, -0.6016],\n",
            "         [-0.2420,  0.3404,  0.3116],\n",
            "         [ 0.0671, -0.4129,  0.0929]]], device='cuda:0') tensor([-0.4184, -0.7896, -0.0460, -0.5914,  0.2637,  0.1278], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.09912109375\n",
            "act tensor([[13, 11, 10, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.0010051727294921875 0.0\n",
            "tcost icost 0.0105438232421875 0.0\n",
            "tcost icost 0.09417724609375 0.0\n",
            "tcost icost 0.0289764404296875 0.0\n",
            "tcost icost -0.10760498046875 0.0\n",
            "tcost icost 0.037017822265625 0.0\n",
            "loss tensor([[0.0591]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.7459, -0.0445, -0.1071],\n",
            "         [ 0.0658,  0.5894, -0.1815],\n",
            "         [-0.2210, -0.4512, -0.4524],\n",
            "         [-0.1478, -0.0349, -0.6226],\n",
            "         [-0.2279,  0.3613,  0.3358],\n",
            "         [ 0.0628, -0.4187,  0.0856]]], device='cuda:0') tensor([-0.4190, -0.7410, -0.0906, -0.5499,  0.2383,  0.1379], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.05914306640625\n",
            "act tensor([[13, 11, 10, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.000980377197265625 0.0\n",
            "tcost icost 0.01258087158203125 0.0\n",
            "tcost icost 0.100341796875 0.0\n",
            "tcost icost 0.128173828125 0.0\n",
            "tcost icost -0.0863037109375 0.0\n",
            "tcost icost 0.032867431640625 0.0\n",
            "loss tensor([[0.1498]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.7510, -0.0363, -0.0963],\n",
            "         [ 0.0547,  0.5363, -0.2800],\n",
            "         [-0.2039, -0.3841, -0.4285],\n",
            "         [-0.1599, -0.0888, -0.6541],\n",
            "         [-0.2150,  0.3799,  0.3581],\n",
            "         [ 0.0585, -0.4241,  0.0784]]], device='cuda:0') tensor([-0.4278, -0.6817, -0.1591, -0.4885,  0.2137,  0.1473], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1497802734375\n",
            "act tensor([[13, 11, 10, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.000621795654296875 0.0\n",
            "tcost icost 0.01395416259765625 0.0\n",
            "tcost icost 0.0709228515625 0.0\n",
            "tcost icost 0.2437744140625 0.0\n",
            "tcost icost -0.0309295654296875 0.0\n",
            "tcost icost 0.026641845703125 0.0\n",
            "loss tensor([[0.2438]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.7688, -0.1425, -0.1551],\n",
            "         [ 0.0765,  0.5546, -0.2671],\n",
            "         [-0.2111, -0.4252, -0.4636],\n",
            "         [-0.1576, -0.1230, -0.6839],\n",
            "         [-0.1969,  0.4053,  0.3877],\n",
            "         [ 0.0536, -0.4302,  0.0696]]], device='cuda:0') tensor([-0.3275, -0.7206, -0.1306, -0.4669,  0.1777,  0.1570], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2437744140625\n",
            "act tensor([[13, 11, 10, 11,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.005039215087890625 0.0\n",
            "tcost icost 0.017669677734375 0.0\n",
            "tcost icost 0.12939453125 0.0\n",
            "tcost icost 0.146728515625 0.0\n",
            "tcost icost -0.00586700439453125 0.0\n",
            "tcost icost 0.07550048828125 0.0\n",
            "loss tensor([[0.2734]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.7777, -0.1310, -0.1352],\n",
            "         [ 0.0670,  0.4894, -0.3737],\n",
            "         [-0.1899, -0.3501, -0.4381],\n",
            "         [-0.1724, -0.1920, -0.7172],\n",
            "         [-0.1733,  0.4362,  0.4282],\n",
            "         [ 0.0449, -0.4407,  0.0543]]], device='cuda:0') tensor([-0.3368, -0.6477, -0.2031, -0.3898,  0.1311,  0.1739], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2734375\n",
            "act tensor([[13, 11, 10, 10,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.00457000732421875 0.0\n",
            "tcost icost 0.0205078125 0.0\n",
            "tcost icost 0.105712890625 0.0\n",
            "tcost icost 0.2423095703125 0.0\n",
            "tcost icost 0.0199432373046875 0.0\n",
            "tcost icost 0.05609130859375 0.0\n",
            "loss tensor([[0.3315]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.7860, -0.1920, -0.1622],\n",
            "         [ 0.0792,  0.4759, -0.4026],\n",
            "         [-0.1867, -0.3521, -0.4501],\n",
            "         [-0.1789, -0.2411, -0.7419],\n",
            "         [-0.1549,  0.4585,  0.4615],\n",
            "         [ 0.0380, -0.4486,  0.0424]]], device='cuda:0') tensor([-0.2743, -0.6502, -0.2137, -0.3515,  0.0895,  0.1869], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.33154296875\n",
            "act tensor([[13, 11, 10, 10,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.007694244384765625 0.0\n",
            "tcost icost 0.02264404296875 0.0\n",
            "tcost icost 0.14794921875 0.0\n",
            "tcost icost 0.21630859375 0.0\n",
            "tcost icost 0.07720947265625 0.0\n",
            "tcost icost 0.10150146484375 0.0\n",
            "loss tensor([[0.4165]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.7936, -0.2394, -0.1764],\n",
            "         [ 0.0887,  0.4443, -0.4529],\n",
            "         [-0.1766, -0.3281, -0.4466],\n",
            "         [-0.1913, -0.3063, -0.7657],\n",
            "         [-0.1318,  0.4842,  0.5044],\n",
            "         [ 0.0276, -0.4600,  0.0242]]], device='cuda:0') tensor([-0.2214, -0.6330, -0.2464, -0.2905,  0.0312,  0.2046], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.41650390625\n",
            "act tensor([[13, 11, 10, 10,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.01039886474609375 0.0\n",
            "tcost icost 0.028076171875 0.0\n",
            "tcost icost 0.173095703125 0.0\n",
            "tcost icost 0.21337890625 0.0\n",
            "tcost icost 0.1668701171875 0.0\n",
            "tcost icost 0.162353515625 0.0\n",
            "loss tensor([[0.5366]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.7975, -0.3180, -0.2089],\n",
            "         [ 0.1061,  0.4310, -0.4802],\n",
            "         [-0.1727, -0.3272, -0.4582],\n",
            "         [-0.2003, -0.3667, -0.7847],\n",
            "         [-0.1136,  0.5025,  0.5387],\n",
            "         [ 0.0186, -0.4695,  0.0070]]], device='cuda:0') tensor([-0.1297, -0.6412, -0.2583, -0.2370, -0.0320,  0.2181], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.53662109375\n",
            "act tensor([[13, 11, 10, 10,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.0148773193359375 0.0\n",
            "tcost icost 0.0404052734375 0.0\n",
            "tcost icost 0.182861328125 0.0\n",
            "tcost icost 0.17041015625 0.0\n",
            "tcost icost 0.22802734375 0.0\n",
            "tcost icost 0.218994140625 0.0\n",
            "loss tensor([[0.6025]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.8008, -0.3095, -0.2025],\n",
            "         [ 0.0973,  0.3588, -0.5673],\n",
            "         [-0.1607, -0.3202, -0.4751],\n",
            "         [-0.1991, -0.3940, -0.7958],\n",
            "         [-0.1005,  0.5105,  0.5427],\n",
            "         [ 0.0147, -0.4751, -0.0074]]], device='cuda:0') tensor([-0.1258, -0.5612, -0.2901, -0.2241, -0.0712,  0.2223], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6025390625\n",
            "act tensor([[13, 11, 10, 10,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.015045166015625 0.0\n",
            "tcost icost 0.052093505859375 0.0\n",
            "tcost icost 0.209228515625 0.0\n",
            "tcost icost 0.1907958984375 0.0\n",
            "tcost icost 0.197265625 0.0\n",
            "tcost icost 0.239990234375 0.0\n",
            "loss tensor([[0.6416]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.7868, -0.4021, -0.2666],\n",
            "         [ 0.1207,  0.3930, -0.5443],\n",
            "         [-0.1898, -0.4138, -0.5373],\n",
            "         [-0.1856, -0.3918, -0.8081],\n",
            "         [-0.0913,  0.5155,  0.5404],\n",
            "         [ 0.0102, -0.4811, -0.0234]]], device='cuda:0') tensor([-0.0020, -0.6148, -0.2289, -0.2531, -0.0966,  0.2277], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6416015625\n",
            "act tensor([[13, 11, 10, 10,  3,  4]], device='cuda:0')\n",
            "tcost icost 0.018218994140625 0.0\n",
            "tcost icost 0.0765380859375 0.0\n",
            "tcost icost 0.11224365234375 0.0\n",
            "tcost icost 0.11224365234375 0.0\n",
            "tcost icost 0.258544921875 0.0\n",
            "tcost icost 0.21337890625 0.0\n",
            "loss tensor([[0.5557]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.8092, -0.3176, -0.2037],\n",
            "         [ 0.0876,  0.2563, -0.6822],\n",
            "         [-0.1577, -0.3534, -0.5398],\n",
            "         [-0.1879, -0.4161, -0.8150],\n",
            "         [-0.0794,  0.5188,  0.5256],\n",
            "         [ 0.0163, -0.4779, -0.0253]]], device='cuda:0') tensor([-0.1099, -0.4530, -0.3100, -0.2341, -0.1173,  0.2143], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.5556640625\n",
            "search tensor([[[-0.0794,  0.5188,  0.5256],\n",
            "         [ 0.0163, -0.4779, -0.0253],\n",
            "         [-0.1867,  0.0653, -0.0998],\n",
            "         [-0.2985, -0.5115,  0.4152],\n",
            "         [-0.2775, -0.2584, -0.3747],\n",
            "         [-0.4424, -0.0407, -0.4314]]], device='cuda:0') tensor([-0.1173,  0.2143,  0.2393, -0.1069,  0.7581,  0.3946], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[3, 4, 4, 1, 4, 4]], device='cuda:0')\n",
            "tcost icost 0.123291015625 0.0\n",
            "tcost icost 0.166015625 0.0\n",
            "tcost icost 0.1748046875 0.0\n",
            "tcost icost 0.0355224609375 0.0\n",
            "tcost icost -0.032562255859375 0.0\n",
            "tcost icost 0.07818603515625 0.0\n",
            "loss tensor([[0.4651]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.1177,  0.4658,  0.3412],\n",
            "         [ 0.0922, -0.4281,  0.0241],\n",
            "         [-0.1846,  0.0754, -0.0927],\n",
            "         [-0.3149, -0.5384,  0.3764],\n",
            "         [-0.2667, -0.2301, -0.3622],\n",
            "         [-0.4425, -0.0474, -0.4356]]], device='cuda:0') tensor([ 0.1030,  0.1129,  0.2337, -0.0603,  0.7273,  0.4025], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.465087890625\n",
            "act tensor([[3, 4, 4, 1, 4, 4]], device='cuda:0')\n",
            "tcost icost 0.16357421875 0.0\n",
            "tcost icost 0.217529296875 0.0\n",
            "tcost icost 0.17578125 0.0\n",
            "tcost icost 0.01271820068359375 0.0\n",
            "tcost icost 0.07611083984375 0.0\n",
            "tcost icost 0.1317138671875 0.0\n",
            "loss tensor([[0.6387]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.0723,  0.5600,  0.5248],\n",
            "         [ 0.0011, -0.5283, -0.1282],\n",
            "         [-0.1466,  0.1625, -0.0248],\n",
            "         [-0.3321, -0.5709,  0.3256],\n",
            "         [-0.2518, -0.2050, -0.3574],\n",
            "         [-0.4399, -0.0441, -0.4368]]], device='cuda:0') tensor([-0.0340,  0.2556,  0.1539, -0.0033,  0.6944,  0.3932], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.638671875\n",
            "act tensor([[3, 4, 4, 1, 4, 4]], device='cuda:0')\n",
            "tcost icost 0.1510009765625 0.0\n",
            "tcost icost 0.2181396484375 0.0\n",
            "tcost icost 0.2047119140625 0.0\n",
            "tcost icost 0.1180419921875 0.0\n",
            "tcost icost 0.062347412109375 0.0\n",
            "tcost icost 0.0833740234375 0.0\n",
            "loss tensor([[0.6890]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.1203,  0.5277,  0.3901],\n",
            "         [ 0.0674, -0.4915, -0.1032],\n",
            "         [-0.1347,  0.1920, -0.0034],\n",
            "         [-0.3626, -0.6129,  0.2495],\n",
            "         [-0.2364, -0.1686, -0.3433],\n",
            "         [-0.4399, -0.0558, -0.4445]]], device='cuda:0') tensor([0.1282, 0.1522, 0.1239, 0.0774, 0.6510, 0.4062], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.68896484375\n",
            "act tensor([[3, 4, 4, 1, 4, 4]], device='cuda:0')\n",
            "tcost icost 0.1629638671875 0.0\n",
            "tcost icost 0.2109375 0.0\n",
            "tcost icost 0.28369140625 0.0\n",
            "tcost icost 0.118408203125 0.0\n",
            "tcost icost 0.20703125 0.0\n",
            "tcost icost 0.1700439453125 0.0\n",
            "loss tensor([[0.9053]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.0930,  0.5817,  0.5377],\n",
            "         [-0.0334, -0.5779, -0.2667],\n",
            "         [-0.0892,  0.2779,  0.0654],\n",
            "         [-0.3856, -0.6452,  0.1787],\n",
            "         [-0.2224, -0.1499, -0.3440],\n",
            "         [-0.4381, -0.0590, -0.4490]]], device='cuda:0') tensor([0.0139, 0.2718, 0.0111, 0.1350, 0.6179, 0.4048], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.9052734375\n",
            "act tensor([[ 3, 10,  3, 13,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.159423828125 0.0\n",
            "tcost icost 0.2484130859375 0.0\n",
            "tcost icost 0.2734375 0.0\n",
            "tcost icost 0.09979248046875 0.0\n",
            "tcost icost 0.1552734375 0.0\n",
            "tcost icost 0.1490478515625 0.0\n",
            "loss tensor([[0.8672]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.1373,  0.5625,  0.4512],\n",
            "         [-0.0026, -0.5669, -0.2821],\n",
            "         [-0.0683,  0.3256,  0.1146],\n",
            "         [-0.4122, -0.6803,  0.0933],\n",
            "         [-0.2007, -0.1051, -0.3280],\n",
            "         [-0.4378, -0.0712, -0.4574]]], device='cuda:0') tensor([ 0.1357,  0.2165, -0.0462,  0.2208,  0.5647,  0.4163], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8671875\n",
            "act tensor([[ 3, 10,  3, 13,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1624755859375 0.0\n",
            "tcost icost 0.1983642578125 0.0\n",
            "tcost icost 0.27294921875 0.0\n",
            "tcost icost 0.13232421875 0.0\n",
            "tcost icost 0.254150390625 0.0\n",
            "tcost icost 0.192626953125 0.0\n",
            "loss tensor([[0.9390]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.0907,  0.6128,  0.6468],\n",
            "         [-0.1723, -0.6703, -0.5023],\n",
            "         [-0.0150,  0.4209,  0.2155],\n",
            "         [-0.4277, -0.7048,  0.0191],\n",
            "         [-0.1844, -0.0898, -0.3345],\n",
            "         [-0.4345, -0.0717, -0.4619]]], device='cuda:0') tensor([-0.0308,  0.3984, -0.1896,  0.2782,  0.5317,  0.4101], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.93896484375\n",
            "act tensor([[ 3, 10,  3, 13,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1517333984375 0.0\n",
            "tcost icost 0.17822265625 0.0\n",
            "tcost icost 0.10675048828125 0.0\n",
            "tcost icost 0.275390625 0.0\n",
            "tcost icost 0.166259765625 0.0\n",
            "tcost icost 0.0750732421875 0.0\n",
            "loss tensor([[0.7524]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.1242,  0.6111,  0.5195],\n",
            "         [-0.0891, -0.6926, -0.4803],\n",
            "         [-0.0408,  0.3554,  0.0928],\n",
            "         [-0.4200, -0.7126, -0.0149],\n",
            "         [-0.1713, -0.0669, -0.3281],\n",
            "         [-0.4346, -0.0841, -0.4699]]], device='cuda:0') tensor([ 0.2038,  0.2020, -0.1058,  0.2707,  0.4978,  0.4236], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.75244140625\n",
            "act tensor([[ 3, 10,  3, 13,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1484375 0.0\n",
            "tcost icost 0.1485595703125 0.0\n",
            "tcost icost 0.229736328125 0.0\n",
            "tcost icost 0.161865234375 0.0\n",
            "tcost icost 0.27001953125 0.0\n",
            "tcost icost 0.2127685546875 0.0\n",
            "loss tensor([[0.8892]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.0703,  0.6366,  0.7154],\n",
            "         [-0.2658, -0.6725, -0.5868],\n",
            "         [ 0.0078,  0.4320,  0.1752],\n",
            "         [-0.4238, -0.7257, -0.0648],\n",
            "         [-0.1604, -0.0680, -0.3446],\n",
            "         [-0.4297, -0.0809, -0.4740]]], device='cuda:0') tensor([-0.0159,  0.4084, -0.2349,  0.2996,  0.4794,  0.4112], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.88916015625\n",
            "act tensor([[ 5, 10,  3, 13,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1583251953125 0.0\n",
            "tcost icost 0.1954345703125 0.0\n",
            "tcost icost 0.11328125 0.0\n",
            "tcost icost 0.263427734375 0.0\n",
            "tcost icost 0.1767578125 0.0\n",
            "tcost icost 0.09002685546875 0.0\n",
            "loss tensor([[0.7876]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.0855,  0.6420,  0.6276],\n",
            "         [-0.2541, -0.6468, -0.5959],\n",
            "         [-0.0322,  0.3301, -0.0118],\n",
            "         [-0.4057, -0.7250, -0.0776],\n",
            "         [-0.1467, -0.0460, -0.3395],\n",
            "         [-0.4297, -0.0960, -0.4834]]], device='cuda:0') tensor([ 0.2257,  0.1833, -0.0917,  0.2590,  0.4456,  0.4276], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.78759765625\n",
            "act tensor([[ 3, 10,  4, 13,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1422119140625 0.0\n",
            "tcost icost 0.1282958984375 0.0\n",
            "tcost icost 0.27001953125 0.0\n",
            "tcost icost 0.196044921875 0.0\n",
            "tcost icost 0.21142578125 0.0\n",
            "tcost icost 0.228271484375 0.0\n",
            "loss tensor([[0.8931]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.0372,  0.6470,  0.7589],\n",
            "         [-0.2572, -0.6857, -0.5885],\n",
            "         [ 0.0092,  0.3764,  0.0126],\n",
            "         [-0.3970, -0.7255, -0.0930],\n",
            "         [-0.1464, -0.0710, -0.3705],\n",
            "         [-0.4256, -0.0952, -0.4881]]], device='cuda:0') tensor([-0.0282,  0.3940, -0.2016,  0.2343,  0.4575,  0.4191], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.89306640625\n",
            "act tensor([[ 5, 10,  7, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.15576171875 0.0\n",
            "tcost icost 0.196044921875 0.0\n",
            "tcost icost 0.09423828125 0.0\n",
            "tcost icost 0.2261962890625 0.0\n",
            "tcost icost 0.139404296875 0.0\n",
            "tcost icost 0.0828857421875 0.0\n",
            "loss tensor([[0.7134]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.0537,  0.6451,  0.7040],\n",
            "         [-0.2349, -0.6699, -0.5999],\n",
            "         [-0.0202,  0.3030, -0.1061],\n",
            "         [-0.3885, -0.7317, -0.1309],\n",
            "         [-0.1300, -0.0438, -0.3627],\n",
            "         [-0.4255, -0.1093, -0.4968]]], device='cuda:0') tensor([ 0.1907,  0.2019, -0.1141,  0.2367,  0.4193,  0.4344], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.71337890625\n",
            "act tensor([[ 5, 10,  4, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1510009765625 0.0\n",
            "tcost icost 0.148681640625 0.0\n",
            "tcost icost 0.336181640625 0.0\n",
            "tcost icost 0.1943359375 0.0\n",
            "tcost icost 0.2138671875 0.0\n",
            "tcost icost 0.2044677734375 0.0\n",
            "loss tensor([[0.9590]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.0295,  0.6406,  0.7673],\n",
            "         [-0.2516, -0.6858, -0.5962],\n",
            "         [ 0.0187,  0.3494, -0.0841],\n",
            "         [-0.3817, -0.7360, -0.1640],\n",
            "         [-0.1215, -0.0488, -0.3813],\n",
            "         [-0.4218, -0.1108, -0.5022]]], device='cuda:0') tensor([-0.0214,  0.3865, -0.2215,  0.2322,  0.4085,  0.4294], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.958984375\n",
            "act tensor([[ 5, 10,  4, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1571044921875 0.0\n",
            "tcost icost 0.2030029296875 0.0\n",
            "tcost icost 0.0521240234375 0.0\n",
            "tcost icost 0.1309814453125 0.0\n",
            "tcost icost 0.1258544921875 0.0\n",
            "tcost icost 0.0714111328125 0.0\n",
            "loss tensor([[0.6021]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.0331,  0.6385,  0.7196],\n",
            "         [-0.2296, -0.6696, -0.6071],\n",
            "         [-0.0195,  0.2445, -0.2375],\n",
            "         [-0.3672, -0.7336, -0.1729],\n",
            "         [-0.1092, -0.0290, -0.3753],\n",
            "         [-0.4216, -0.1244, -0.5103]]], device='cuda:0') tensor([ 0.1780,  0.1979, -0.0876,  0.2063,  0.3788,  0.4442], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.60205078125\n",
            "act tensor([[ 5, 10, 11, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1541748046875 0.0\n",
            "tcost icost 0.156005859375 0.0\n",
            "tcost icost 0.265625 0.0\n",
            "tcost icost 0.1502685546875 0.0\n",
            "tcost icost 0.1954345703125 0.0\n",
            "tcost icost 0.195556640625 0.0\n",
            "loss tensor([[0.8623]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.0212,  0.6329,  0.7740],\n",
            "         [-0.2559, -0.6887, -0.5993],\n",
            "         [ 0.0349,  0.3185, -0.1969],\n",
            "         [-0.3631, -0.7390, -0.2173],\n",
            "         [-0.1073, -0.0486, -0.4034],\n",
            "         [-0.4172, -0.1207, -0.5135]]], device='cuda:0') tensor([-0.0429,  0.3903, -0.2206,  0.2240,  0.3868,  0.4322], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8623046875\n",
            "act tensor([[ 5, 10, 11, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1527099609375 0.0\n",
            "tcost icost 0.18994140625 0.0\n",
            "tcost icost 0.0423583984375 0.0\n",
            "tcost icost 0.11181640625 0.0\n",
            "tcost icost 0.125244140625 0.0\n",
            "tcost icost 0.080078125 0.0\n",
            "loss tensor([[0.5688]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.0262,  0.6267,  0.7304],\n",
            "         [-0.2342, -0.6728, -0.6103],\n",
            "         [ 0.0011,  0.2239, -0.3259],\n",
            "         [-0.3497, -0.7376, -0.2337],\n",
            "         [-0.0927, -0.0263, -0.3978],\n",
            "         [-0.4170, -0.1353, -0.5220]]], device='cuda:0') tensor([ 0.1614,  0.2083, -0.1026,  0.2089,  0.3539,  0.4480], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.56884765625\n",
            "act tensor([[ 5, 10, 11, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1583251953125 0.0\n",
            "tcost icost 0.171630859375 0.0\n",
            "tcost icost 0.30908203125 0.0\n",
            "tcost icost 0.168212890625 0.0\n",
            "tcost icost 0.195556640625 0.0\n",
            "tcost icost 0.2041015625 0.0\n",
            "loss tensor([[0.9346]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.0174,  0.6314,  0.7752],\n",
            "         [-0.2516, -0.6870, -0.6080],\n",
            "         [ 0.0536,  0.2802, -0.3127],\n",
            "         [-0.3429, -0.7405, -0.2781],\n",
            "         [-0.0899, -0.0439, -0.4250],\n",
            "         [-0.4126, -0.1333, -0.5261]]], device='cuda:0') tensor([-0.0371,  0.3676, -0.2164,  0.2250,  0.3605,  0.4378], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.9345703125\n",
            "act tensor([[ 5, 10, 11, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1539306640625 0.0\n",
            "tcost icost 0.2039794921875 0.0\n",
            "tcost icost 0.062744140625 0.0\n",
            "tcost icost 0.137451171875 0.0\n",
            "tcost icost 0.1407470703125 0.0\n",
            "tcost icost 0.09722900390625 0.0\n",
            "loss tensor([[0.6382]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.0199,  0.6246,  0.7297],\n",
            "         [-0.2251, -0.6671, -0.6235],\n",
            "         [ 0.0183,  0.1684, -0.4559],\n",
            "         [-0.3279, -0.7375, -0.2972],\n",
            "         [-0.0748, -0.0218, -0.4203],\n",
            "         [-0.4122, -0.1487, -0.5350]]], device='cuda:0') tensor([ 0.1738,  0.1689, -0.0836,  0.2048,  0.3262,  0.4541], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.63818359375\n",
            "act tensor([[ 5, 10, 11, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1552734375 0.0\n",
            "tcost icost 0.1490478515625 0.0\n",
            "tcost icost 0.2255859375 0.0\n",
            "tcost icost 0.13525390625 0.0\n",
            "tcost icost 0.1800537109375 0.0\n",
            "tcost icost 0.1900634765625 0.0\n",
            "loss tensor([[0.8013]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.0085,  0.6236,  0.7817],\n",
            "         [-0.2634, -0.6956, -0.6074],\n",
            "         [ 0.0820,  0.2484, -0.4259],\n",
            "         [-0.3211, -0.7366, -0.3319],\n",
            "         [-0.0788, -0.0549, -0.4588],\n",
            "         [-0.4070, -0.1421, -0.5379]]], device='cuda:0') tensor([-0.0711,  0.3796, -0.2268,  0.2141,  0.3528,  0.4378], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.80126953125\n",
            "act tensor([[ 5, 10, 11, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.144775390625 0.0\n",
            "tcost icost 0.1768798828125 0.0\n",
            "tcost icost 0.032257080078125 0.0\n",
            "tcost icost 0.0999755859375 0.0\n",
            "tcost icost 0.12420654296875 0.0\n",
            "tcost icost 0.08477783203125 0.0\n",
            "loss tensor([[0.5342]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.0111,  0.6122,  0.7406],\n",
            "         [-0.2438, -0.6804, -0.6172],\n",
            "         [ 0.0565,  0.1592, -0.5323],\n",
            "         [-0.3091, -0.7334, -0.3450],\n",
            "         [-0.0644, -0.0329, -0.4532],\n",
            "         [-0.4066, -0.1565, -0.5459]]], device='cuda:0') tensor([ 0.1555,  0.2006, -0.1148,  0.1989,  0.3193,  0.4534], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.5341796875\n",
            "act tensor([[ 5, 10, 11, 10,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.159912109375 0.0\n",
            "tcost icost 0.173095703125 0.0\n",
            "tcost icost 0.31494140625 0.0\n",
            "tcost icost 0.1700439453125 0.0\n",
            "tcost icost 0.1922607421875 0.0\n",
            "tcost icost 0.207275390625 0.0\n",
            "loss tensor([[0.9434]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.0054,  0.6230,  0.7822],\n",
            "         [-0.2576, -0.6938, -0.6166],\n",
            "         [ 0.1159,  0.2137, -0.5281],\n",
            "         [-0.2999, -0.7306, -0.3850],\n",
            "         [-0.0619, -0.0507, -0.4804],\n",
            "         [-0.4017, -0.1538, -0.5502]]], device='cuda:0') tensor([-0.0471,  0.3641, -0.2316,  0.2141,  0.3284,  0.4421], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.943359375\n",
            "search tensor([[[-0.0619, -0.0507, -0.4804],\n",
            "         [-0.4017, -0.1538, -0.5502],\n",
            "         [ 0.0208, -0.3390,  0.1926],\n",
            "         [-0.5255,  0.1468, -0.3828],\n",
            "         [-0.4305,  0.0618,  0.1224],\n",
            "         [ 0.3730, -0.3003, -0.2417]]], device='cuda:0') tensor([ 0.3284,  0.4421,  0.2660,  0.1196,  0.0945, -0.5341], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[4, 4, 4, 4, 1, 4]], device='cuda:0')\n",
            "tcost icost 0.130126953125 0.0\n",
            "tcost icost 0.16455078125 0.0\n",
            "tcost icost 0.2080078125 0.0\n",
            "tcost icost 0.1651611328125 0.0\n",
            "tcost icost 0.0849609375 0.0\n",
            "tcost icost 0.0134124755859375 0.0\n",
            "loss tensor([[0.6309]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.0683, -0.0797, -0.5182],\n",
            "         [-0.3936, -0.1604, -0.5689],\n",
            "         [ 0.0502, -0.3191,  0.1824],\n",
            "         [-0.5204,  0.1307, -0.4054],\n",
            "         [-0.4390,  0.0348,  0.0956],\n",
            "         [ 0.3719, -0.3014, -0.2433]]], device='cuda:0') tensor([ 0.3293,  0.3913,  0.2151,  0.1396,  0.1230, -0.5324], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.630859375\n",
            "act tensor([[11, 10,  4,  4,  1,  4]], device='cuda:0')\n",
            "tcost icost 0.10980224609375 0.0\n",
            "tcost icost 0.1590576171875 0.0\n",
            "tcost icost 0.2412109375 0.0\n",
            "tcost icost 0.1773681640625 0.0\n",
            "tcost icost 0.053192138671875 0.0\n",
            "tcost icost 0.01410675048828125 0.0\n",
            "loss tensor([[0.6206]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.0477, -0.0403, -0.5087],\n",
            "         [-0.3875, -0.2260, -0.6112],\n",
            "         [ 0.1013, -0.2714,  0.2065],\n",
            "         [-0.5175,  0.1126, -0.4254],\n",
            "         [-0.4447,  0.0165,  0.0778],\n",
            "         [ 0.3708, -0.3024, -0.2449]]], device='cuda:0') tensor([ 0.2744,  0.4186,  0.1325,  0.1601,  0.1423, -0.5310], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.62060546875\n",
            "act tensor([[11, 10,  4,  4,  1,  4]], device='cuda:0')\n",
            "tcost icost 0.1195068359375 0.0\n",
            "tcost icost 0.1668701171875 0.0\n",
            "tcost icost 0.2174072265625 0.0\n",
            "tcost icost 0.2310791015625 0.0\n",
            "tcost icost 0.10546875 0.0\n",
            "tcost icost 0.0100555419921875 0.0\n",
            "loss tensor([[0.6895]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.0396, -0.0507, -0.5386],\n",
            "         [-0.3799, -0.2373, -0.6285],\n",
            "         [ 0.1227, -0.2646,  0.1818],\n",
            "         [-0.5085,  0.1141, -0.4368],\n",
            "         [-0.4534, -0.0127,  0.0509],\n",
            "         [ 0.3698, -0.3033, -0.2463]]], device='cuda:0') tensor([ 0.2626,  0.3806,  0.0936,  0.1554,  0.1721, -0.5297], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.689453125\n",
            "act tensor([[11, 10,  4,  4,  1,  4]], device='cuda:0')\n",
            "tcost icost 0.1214599609375 0.0\n",
            "tcost icost 0.173095703125 0.0\n",
            "tcost icost 0.22802734375 0.0\n",
            "tcost icost 0.2393798828125 0.0\n",
            "tcost icost 0.12744140625 0.0\n",
            "tcost icost 0.00916290283203125 0.0\n",
            "loss tensor([[0.7256]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.0197, -0.0475, -0.5624],\n",
            "         [-0.3755, -0.2487, -0.6419],\n",
            "         [ 0.1319, -0.2740,  0.1422],\n",
            "         [-0.4975,  0.1301, -0.4391],\n",
            "         [-0.4616, -0.0433,  0.0227],\n",
            "         [ 0.3689, -0.3042, -0.2479]]], device='cuda:0') tensor([ 0.2431,  0.3496,  0.0699,  0.1307,  0.2038, -0.5288], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7255859375\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.12408447265625 0.0\n",
            "tcost icost 0.179931640625 0.0\n",
            "tcost icost 0.2296142578125 0.0\n",
            "tcost icost 0.2427978515625 0.0\n",
            "tcost icost 0.1346435546875 0.0\n",
            "tcost icost 0.00824737548828125 0.0\n",
            "loss tensor([[0.7427]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.0063, -0.0545, -0.5907],\n",
            "         [-0.3694, -0.2523, -0.6530],\n",
            "         [ 0.1367, -0.2855,  0.1022],\n",
            "         [-0.4878,  0.1412, -0.4430],\n",
            "         [-0.4672, -0.0668,  0.0008],\n",
            "         [ 0.3677, -0.3053, -0.2496]]], device='cuda:0') tensor([ 0.2405,  0.3141,  0.0529,  0.1106,  0.2268, -0.5275], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.74267578125\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.12432861328125 0.0\n",
            "tcost icost 0.1832275390625 0.0\n",
            "tcost icost 0.2442626953125 0.0\n",
            "tcost icost 0.2432861328125 0.0\n",
            "tcost icost 0.14892578125 0.0\n",
            "tcost icost 0.0085601806640625 0.0\n",
            "loss tensor([[0.7671]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.0088, -0.0579, -0.6163],\n",
            "         [-0.3638, -0.2593, -0.6650],\n",
            "         [ 0.1416, -0.2967,  0.0597],\n",
            "         [-0.4777,  0.1530, -0.4469],\n",
            "         [-0.4724, -0.0914, -0.0220],\n",
            "         [ 0.3666, -0.3063, -0.2513]]], device='cuda:0') tensor([ 0.2346,  0.2807,  0.0335,  0.0886,  0.2508, -0.5263], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.76708984375\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.12469482421875 0.0\n",
            "tcost icost 0.1851806640625 0.0\n",
            "tcost icost 0.25537109375 0.0\n",
            "tcost icost 0.2425537109375 0.0\n",
            "tcost icost 0.166748046875 0.0\n",
            "tcost icost 0.0086669921875 0.0\n",
            "loss tensor([[0.7891]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.0325, -0.0401, -0.6278],\n",
            "         [-0.3583, -0.2860, -0.6827],\n",
            "         [ 0.1562, -0.2955,  0.0288],\n",
            "         [-0.4679,  0.1593, -0.4547],\n",
            "         [-0.4772, -0.1168, -0.0456],\n",
            "         [ 0.3655, -0.3073, -0.2530]]], device='cuda:0') tensor([ 0.2061,  0.2713, -0.0009,  0.0730,  0.2758, -0.5251], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7890625\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1268310546875 0.0\n",
            "tcost icost 0.19384765625 0.0\n",
            "tcost icost 0.24072265625 0.0\n",
            "tcost icost 0.25439453125 0.0\n",
            "tcost icost 0.197509765625 0.0\n",
            "tcost icost 0.0060272216796875 0.0\n",
            "loss tensor([[0.8149]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.0470, -0.0422, -0.6513],\n",
            "         [-0.3523, -0.2944, -0.6945],\n",
            "         [ 0.1604, -0.3048, -0.0143],\n",
            "         [-0.4563,  0.1662, -0.4639],\n",
            "         [-0.4808, -0.1402, -0.0682],\n",
            "         [ 0.3642, -0.3085, -0.2550]]], device='cuda:0') tensor([ 0.2029,  0.2395, -0.0250,  0.0518,  0.2969, -0.5237], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.81494140625\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1270751953125 0.0\n",
            "tcost icost 0.1953125 0.0\n",
            "tcost icost 0.253662109375 0.0\n",
            "tcost icost 0.256103515625 0.0\n",
            "tcost icost 0.213134765625 0.0\n",
            "tcost icost 0.00562286376953125 0.0\n",
            "loss tensor([[0.8384]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.0647, -0.0344, -0.6676],\n",
            "         [-0.3450, -0.3111, -0.7091],\n",
            "         [ 0.1683, -0.3092, -0.0544],\n",
            "         [-0.4448,  0.1665, -0.4782],\n",
            "         [-0.4824, -0.1605, -0.0897],\n",
            "         [ 0.3629, -0.3097, -0.2569]]], device='cuda:0') tensor([ 0.1872,  0.2189, -0.0535,  0.0379,  0.3148, -0.5224], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.83837890625\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1279296875 0.0\n",
            "tcost icost 0.199462890625 0.0\n",
            "tcost icost 0.251953125 0.0\n",
            "tcost icost 0.263916015625 0.0\n",
            "tcost icost 0.229736328125 0.0\n",
            "tcost icost 0.0041656494140625 0.0\n",
            "loss tensor([[0.8574]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.0740, -0.0429, -0.6935],\n",
            "         [-0.3340, -0.3100, -0.7209],\n",
            "         [ 0.1646, -0.3235, -0.1026],\n",
            "         [-0.4342,  0.1642, -0.4930],\n",
            "         [-0.4816, -0.1709, -0.1032],\n",
            "         [ 0.3617, -0.3109, -0.2588]]], device='cuda:0') tensor([ 0.1943,  0.1834, -0.0640,  0.0250,  0.3209, -0.5210], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.857421875\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.127685546875 0.0\n",
            "tcost icost 0.1956787109375 0.0\n",
            "tcost icost 0.274169921875 0.0\n",
            "tcost icost 0.257568359375 0.0\n",
            "tcost icost 0.234130859375 0.0\n",
            "tcost icost 0.005352020263671875 0.0\n",
            "loss tensor([[0.8701]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.0867, -0.0375, -0.7071],\n",
            "         [-0.3247, -0.3242, -0.7347],\n",
            "         [ 0.1715, -0.3253, -0.1372],\n",
            "         [-0.4248,  0.1561, -0.5107],\n",
            "         [-0.4806, -0.1808, -0.1164],\n",
            "         [ 0.3604, -0.3121, -0.2607]]], device='cuda:0') tensor([ 0.1832,  0.1637, -0.0912,  0.0200,  0.3265, -0.5196], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8701171875\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1280517578125 0.0\n",
            "tcost icost 0.1988525390625 0.0\n",
            "tcost icost 0.2763671875 0.0\n",
            "tcost icost 0.266357421875 0.0\n",
            "tcost icost 0.2369384765625 0.0\n",
            "tcost icost 0.004604339599609375 0.0\n",
            "loss tensor([[0.8838]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.0999, -0.0313, -0.7197],\n",
            "         [-0.3153, -0.3385, -0.7480],\n",
            "         [ 0.1743, -0.3300, -0.1762],\n",
            "         [-0.4150,  0.1481, -0.5285],\n",
            "         [-0.4786, -0.1889, -0.1289],\n",
            "         [ 0.3591, -0.3133, -0.2626]]], device='cuda:0') tensor([ 0.1717,  0.1455, -0.1155,  0.0133,  0.3296, -0.5183], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8837890625\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1279296875 0.0\n",
            "tcost icost 0.2020263671875 0.0\n",
            "tcost icost 0.2763671875 0.0\n",
            "tcost icost 0.274658203125 0.0\n",
            "tcost icost 0.238525390625 0.0\n",
            "tcost icost 0.003997802734375 0.0\n",
            "loss tensor([[0.8931]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.1056, -0.0375, -0.7396],\n",
            "         [-0.3038, -0.3445, -0.7609],\n",
            "         [ 0.1707, -0.3396, -0.2217],\n",
            "         [-0.4045,  0.1372, -0.5489],\n",
            "         [-0.4757, -0.1944, -0.1399],\n",
            "         [ 0.3579, -0.3144, -0.2646]]], device='cuda:0') tensor([ 0.1796,  0.1170, -0.1330,  0.0081,  0.3288, -0.5170], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.89306640625\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1280517578125 0.0\n",
            "tcost icost 0.1942138671875 0.0\n",
            "tcost icost 0.2998046875 0.0\n",
            "tcost icost 0.26904296875 0.0\n",
            "tcost icost 0.239501953125 0.0\n",
            "tcost icost 0.004962921142578125 0.0\n",
            "loss tensor([[0.9019]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.1194, -0.0298, -0.7508],\n",
            "         [-0.2944, -0.3618, -0.7740],\n",
            "         [ 0.1691, -0.3472, -0.2665],\n",
            "         [-0.3937,  0.1304, -0.5656],\n",
            "         [-0.4731, -0.2018, -0.1521],\n",
            "         [ 0.3566, -0.3156, -0.2665]]], device='cuda:0') tensor([ 0.1690,  0.1040, -0.1554, -0.0022,  0.3310, -0.5156], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.90185546875\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1278076171875 0.0\n",
            "tcost icost 0.19775390625 0.0\n",
            "tcost icost 0.29833984375 0.0\n",
            "tcost icost 0.277099609375 0.0\n",
            "tcost icost 0.2406005859375 0.0\n",
            "tcost icost 0.0043182373046875 0.0\n",
            "loss tensor([[0.9097]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.1306, -0.0266, -0.7641],\n",
            "         [-0.2848, -0.3735, -0.7858],\n",
            "         [ 0.1614, -0.3591, -0.3156],\n",
            "         [-0.3823,  0.1258, -0.5808],\n",
            "         [-0.4700, -0.2083, -0.1639],\n",
            "         [ 0.3553, -0.3168, -0.2685]]], device='cuda:0') tensor([ 0.1673,  0.0861, -0.1714, -0.0160,  0.3308, -0.5143], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.90966796875\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1278076171875 0.0\n",
            "tcost icost 0.195068359375 0.0\n",
            "tcost icost 0.30859375 0.0\n",
            "tcost icost 0.2763671875 0.0\n",
            "tcost icost 0.2432861328125 0.0\n",
            "tcost icost 0.004627227783203125 0.0\n",
            "loss tensor([[0.9175]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.1458, -0.0127, -0.7692],\n",
            "         [-0.2775, -0.3942, -0.7970],\n",
            "         [ 0.1592, -0.3652, -0.3582],\n",
            "         [-0.3712,  0.1182, -0.5976],\n",
            "         [-0.4667, -0.2134, -0.1745],\n",
            "         [ 0.3540, -0.3179, -0.2704]]], device='cuda:0') tensor([ 0.1496,  0.0842, -0.1922, -0.0266,  0.3297, -0.5129], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.91748046875\n",
            "act tensor([[11, 10,  4,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.127197265625 0.0\n",
            "tcost icost 0.2042236328125 0.0\n",
            "tcost icost 0.2919921875 0.0\n",
            "tcost icost 0.28662109375 0.0\n",
            "tcost icost 0.240966796875 0.0\n",
            "tcost icost 0.0034618377685546875 0.0\n",
            "loss tensor([[0.9170]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.1420, -0.0333, -0.7948],\n",
            "         [-0.2631, -0.3768, -0.8079],\n",
            "         [ 0.1224, -0.3997, -0.4315],\n",
            "         [-0.3570,  0.1156, -0.6140],\n",
            "         [-0.4628, -0.2168, -0.1842],\n",
            "         [ 0.3528, -0.3191, -0.2724]]], device='cuda:0') tensor([ 0.1806,  0.0293, -0.1728, -0.0458,  0.3257, -0.5116], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.9169921875\n",
            "act tensor([[11, 10, 10,  4,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.128173828125 0.0\n",
            "tcost icost 0.1702880859375 0.0\n",
            "tcost icost 0.32470703125 0.0\n",
            "tcost icost 0.257568359375 0.0\n",
            "tcost icost 0.2176513671875 0.0\n",
            "tcost icost 0.004364013671875 0.0\n",
            "loss tensor([[0.8774]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.1657, -0.0095, -0.7954],\n",
            "         [-0.2575, -0.3910, -0.8180],\n",
            "         [ 0.0906, -0.4280, -0.4975],\n",
            "         [-0.3415,  0.1245, -0.6230],\n",
            "         [-0.4594, -0.2231, -0.1957],\n",
            "         [ 0.3515, -0.3202, -0.2743]]], device='cuda:0') tensor([ 0.1446,  0.0255, -0.1643, -0.0786,  0.3254, -0.5104], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.87744140625\n",
            "act tensor([[11, 10, 10, 11,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.126953125 0.0\n",
            "tcost icost 0.1927490234375 0.0\n",
            "tcost icost 0.3046875 0.0\n",
            "tcost icost 0.244873046875 0.0\n",
            "tcost icost 0.206298828125 0.0\n",
            "tcost icost 0.002956390380859375 0.0\n",
            "loss tensor([[0.8628]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.1714, -0.0127, -0.8089],\n",
            "         [-0.2468, -0.3893, -0.8278],\n",
            "         [ 0.0602, -0.4546, -0.5467],\n",
            "         [-0.3260,  0.1258, -0.6366],\n",
            "         [-0.4556, -0.2276, -0.2061],\n",
            "         [ 0.3502, -0.3214, -0.2762]]], device='cuda:0') tensor([ 0.1519, -0.0021, -0.1565, -0.1015,  0.3221, -0.5091], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.86279296875\n",
            "act tensor([[11, 10, 10, 11,  4,  4]], device='cuda:0')\n",
            "tcost icost 0.1273193359375 0.0\n",
            "tcost icost 0.1790771484375 0.0\n",
            "tcost icost 0.3203125 0.0\n",
            "tcost icost 0.2347412109375 0.0\n",
            "tcost icost 0.2176513671875 0.0\n",
            "tcost icost 0.005008697509765625 0.0\n",
            "loss tensor([[0.8643]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.1942,  0.0148, -0.8045],\n",
            "         [-0.2434, -0.4172, -0.8380],\n",
            "         [ 0.0486, -0.4650, -0.5823],\n",
            "         [-0.3124,  0.1232, -0.6509],\n",
            "         [-0.4516, -0.2324, -0.2170],\n",
            "         [ 0.3489, -0.3225, -0.2781]]], device='cuda:0') tensor([ 0.1137,  0.0156, -0.1706, -0.1177,  0.3193, -0.5077], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8642578125\n",
            "search tensor([[[-0.4516, -0.2324, -0.2170],\n",
            "         [ 0.3489, -0.3225, -0.2781],\n",
            "         [-0.2545, -0.1295,  0.2851],\n",
            "         [-0.1928,  0.0225, -0.1427],\n",
            "         [-0.1368, -0.1605,  0.2823],\n",
            "         [-0.5110,  0.1465,  0.2456]]], device='cuda:0') tensor([ 0.3193, -0.5077,  0.4048, -0.0644,  0.2830,  0.2830], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[4, 4, 1, 4, 1, 1]], device='cuda:0')\n",
            "tcost icost 0.2010498046875 0.0\n",
            "tcost icost 0.01358795166015625 0.0\n",
            "tcost icost -0.2313232421875 0.0\n",
            "tcost icost -0.0295257568359375 0.0\n",
            "tcost icost 0.055419921875 0.0\n",
            "tcost icost -0.07305908203125 0.0\n",
            "loss tensor([[-0.0024]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.4541, -0.1760, -0.1686],\n",
            "         [ 0.3119, -0.3570, -0.3165],\n",
            "         [-0.2241, -0.0815,  0.3198],\n",
            "         [-0.1950, -0.0038, -0.1722],\n",
            "         [-0.1364, -0.1691,  0.2683],\n",
            "         [-0.5040,  0.1612,  0.2540]]], device='cuda:0') tensor([ 0.2668, -0.4581,  0.3672, -0.0307,  0.2926,  0.2653], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.00244140625\n",
            "act tensor([[4, 4, 1, 4, 1, 1]], device='cuda:0')\n",
            "tcost icost 0.213134765625 0.0\n",
            "tcost icost 0.0245361328125 0.0\n",
            "tcost icost -0.200927734375 0.0\n",
            "tcost icost -0.0273590087890625 0.0\n",
            "tcost icost 0.08245849609375 0.0\n",
            "tcost icost -0.029327392578125 0.0\n",
            "loss tensor([[0.0894]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.4490, -0.0950, -0.1048],\n",
            "         [ 0.2607, -0.4003, -0.3649],\n",
            "         [-0.1889, -0.0244,  0.3586],\n",
            "         [-0.1971, -0.0323, -0.2043],\n",
            "         [-0.1382, -0.1819,  0.2489],\n",
            "         [-0.4968,  0.1759,  0.2622]]], device='cuda:0') tensor([ 0.1907, -0.3979,  0.3209,  0.0041,  0.3063,  0.2468], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.08935546875\n",
            "act tensor([[ 4, 10,  1,  4,  1,  1]], device='cuda:0')\n",
            "tcost icost 0.2237548828125 0.0\n",
            "tcost icost 0.04364013671875 0.0\n",
            "tcost icost -0.1859130859375 0.0\n",
            "tcost icost -0.0272369384765625 0.0\n",
            "tcost icost 0.097412109375 0.0\n",
            "tcost icost 0.005741119384765625 0.0\n",
            "loss tensor([[0.1598]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.4322,  0.0072, -0.0281],\n",
            "         [ 0.1931, -0.4520, -0.4220],\n",
            "         [-0.1528,  0.0372,  0.3996],\n",
            "         [-0.1988, -0.0629, -0.2375],\n",
            "         [-0.1398, -0.1941,  0.2297],\n",
            "         [-0.4902,  0.1891,  0.2697]]], device='cuda:0') tensor([ 0.1027, -0.3230,  0.2710,  0.0410,  0.3209,  0.2294], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1597900390625\n",
            "act tensor([[ 4, 10,  1,  4,  1,  1]], device='cuda:0')\n",
            "tcost icost 0.2220458984375 0.0\n",
            "tcost icost 0.135986328125 0.0\n",
            "tcost icost -0.12249755859375 0.0\n",
            "tcost icost -0.020660400390625 0.0\n",
            "tcost icost 0.1380615234375 0.0\n",
            "tcost icost 0.09674072265625 0.0\n",
            "loss tensor([[0.3779]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.3859,  0.1809,  0.1084],\n",
            "         [ 0.0625, -0.5395, -0.5177],\n",
            "         [-0.1088,  0.1172,  0.4468],\n",
            "         [-0.1986, -0.0982, -0.2797],\n",
            "         [-0.1480, -0.2208,  0.1897],\n",
            "         [-0.4824,  0.2040,  0.2766]]], device='cuda:0') tensor([-0.0368, -0.1994,  0.1930,  0.0777,  0.3471,  0.2081], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.3779296875\n",
            "act tensor([[ 1, 10,  3,  4,  1,  1]], device='cuda:0')\n",
            "tcost icost 0.1783447265625 0.0\n",
            "tcost icost 0.2919921875 0.0\n",
            "tcost icost -0.00939178466796875 0.0\n",
            "tcost icost -0.01544189453125 0.0\n",
            "tcost icost 0.112060546875 0.0\n",
            "tcost icost 0.1817626953125 0.0\n",
            "loss tensor([[0.6030]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.3664,  0.2475,  0.1710],\n",
            "         [-0.0704, -0.6124, -0.5875],\n",
            "         [-0.0654,  0.2106,  0.5085],\n",
            "         [-0.2027, -0.1498, -0.3278],\n",
            "         [-0.1369, -0.2207,  0.1766],\n",
            "         [-0.4785,  0.2103,  0.2784]]], device='cuda:0') tensor([-0.0658, -0.0802,  0.1115,  0.1341,  0.3482,  0.1941], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.60302734375\n",
            "act tensor([[ 3, 10,  3,  4,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.201904296875 0.0\n",
            "tcost icost 0.276611328125 0.0\n",
            "tcost icost 0.030029296875 0.0\n",
            "tcost icost -0.0017604827880859375 0.0\n",
            "tcost icost 0.178955078125 0.0\n",
            "tcost icost 0.170166015625 0.0\n",
            "loss tensor([[0.6919]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.2710,  0.4583,  0.3821],\n",
            "         [-0.2325, -0.6772, -0.5965],\n",
            "         [-0.0210,  0.3141,  0.5712],\n",
            "         [-0.2022, -0.1871, -0.3702],\n",
            "         [-0.1441, -0.2474,  0.1309],\n",
            "         [-0.4716,  0.2219,  0.2826]]], device='cuda:0') tensor([-0.2353,  0.0882, -0.0034,  0.1714,  0.3740,  0.1746], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.69189453125\n",
            "act tensor([[ 3, 10,  8,  4,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.1015625 0.0\n",
            "tcost icost 0.1629638671875 0.0\n",
            "tcost icost 0.1846923828125 0.0\n",
            "tcost icost 0.22119140625 0.0\n",
            "tcost icost 0.2205810546875 0.0\n",
            "tcost icost 0.1744384765625 0.0\n",
            "loss tensor([[0.8071]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.3846,  0.3386,  0.1894],\n",
            "         [-0.2109, -0.6666, -0.6142],\n",
            "         [-0.0187,  0.3583,  0.5990],\n",
            "         [-0.2097, -0.2609, -0.4351],\n",
            "         [-0.1162, -0.2171,  0.1506],\n",
            "         [-0.4818,  0.2006,  0.2614]]], device='cuda:0') tensor([-0.0181,  0.0229, -0.0723,  0.2225,  0.3278,  0.1949], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.80712890625\n",
            "act tensor([[ 3, 10,  8,  4,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.212158203125 0.0\n",
            "tcost icost 0.274169921875 0.0\n",
            "tcost icost 0.22314453125 0.0\n",
            "tcost icost 0.1337890625 0.0\n",
            "tcost icost 0.262939453125 0.0\n",
            "tcost icost 0.251953125 0.0\n",
            "loss tensor([[1.0586]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.2607,  0.5623,  0.4330],\n",
            "         [-0.2613, -0.6901, -0.5983],\n",
            "         [ 0.0109,  0.4341,  0.6456],\n",
            "         [-0.2032, -0.2845, -0.4692],\n",
            "         [-0.1255, -0.2444,  0.1032],\n",
            "         [-0.4858,  0.1904,  0.2486]]], device='cuda:0') tensor([-0.2209,  0.2171, -0.2265,  0.2252,  0.3497,  0.2016], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.05859375\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.10894775390625 0.0\n",
            "tcost icost 0.09808349609375 0.0\n",
            "tcost icost 0.07452392578125 0.0\n",
            "tcost icost 0.2822265625 0.0\n",
            "tcost icost 0.1475830078125 0.0\n",
            "tcost icost 0.046844482421875 0.0\n",
            "loss tensor([[0.5879]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.4090,  0.4552,  0.1289],\n",
            "         [-0.2413, -0.6736, -0.6080],\n",
            "         [ 0.0076,  0.3874,  0.5760],\n",
            "         [-0.1836, -0.2740, -0.4852],\n",
            "         [-0.1074, -0.2238,  0.1185],\n",
            "         [-0.4926,  0.1746,  0.2333]]], device='cuda:0') tensor([ 0.0930,  0.0228, -0.1137,  0.1905,  0.3172,  0.2181], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.587890625\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.2183837890625 0.0\n",
            "tcost icost 0.1761474609375 0.0\n",
            "tcost icost 0.139404296875 0.0\n",
            "tcost icost 0.06890869140625 0.0\n",
            "tcost icost 0.25 0.0\n",
            "tcost icost 0.2159423828125 0.0\n",
            "loss tensor([[0.8315]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.3057,  0.6387,  0.3682],\n",
            "         [-0.2597, -0.6937, -0.5999],\n",
            "         [ 0.0314,  0.4526,  0.6406],\n",
            "         [-0.1924, -0.3271, -0.5216],\n",
            "         [-0.1021, -0.2313,  0.0923],\n",
            "         [-0.4883,  0.1811,  0.2340]]], device='cuda:0') tensor([-0.1040,  0.2259, -0.2392,  0.2333,  0.3108,  0.2039], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.83154296875\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.18115234375 0.0\n",
            "tcost icost 0.22119140625 0.0\n",
            "tcost icost 0.1820068359375 0.0\n",
            "tcost icost 0.31982421875 0.0\n",
            "tcost icost 0.201171875 0.0\n",
            "tcost icost 0.09686279296875 0.0\n",
            "loss tensor([[0.9497]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.4307,  0.5494,  0.1032],\n",
            "         [-0.2387, -0.6797, -0.6110],\n",
            "         [ 0.0310,  0.4232,  0.5822],\n",
            "         [-0.1753, -0.3273, -0.5427],\n",
            "         [-0.0839, -0.2123,  0.1018],\n",
            "         [-0.4974,  0.1600,  0.2137]]], device='cuda:0') tensor([ 0.1841,  0.0544, -0.1619,  0.2022,  0.2779,  0.2243], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.94970703125\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.1988525390625 0.0\n",
            "tcost icost 0.11358642578125 0.0\n",
            "tcost icost 0.11395263671875 0.0\n",
            "tcost icost 0.0926513671875 0.0\n",
            "tcost icost 0.2373046875 0.0\n",
            "tcost icost 0.21533203125 0.0\n",
            "loss tensor([[0.7441]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.3127,  0.7301,  0.3561],\n",
            "         [-0.2683, -0.6978, -0.6013],\n",
            "         [ 0.0607,  0.4679,  0.6256],\n",
            "         [-0.1749, -0.3463, -0.5611],\n",
            "         [-0.1013, -0.2519,  0.0390],\n",
            "         [-0.4918,  0.1695,  0.2158]]], device='cuda:0') tensor([-0.0532,  0.2692, -0.2885,  0.2081,  0.3082,  0.2070], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.744140625\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.2071533203125 0.0\n",
            "tcost icost 0.239501953125 0.0\n",
            "tcost icost 0.1761474609375 0.0\n",
            "tcost icost 0.312255859375 0.0\n",
            "tcost icost 0.1968994140625 0.0\n",
            "tcost icost 0.10028076171875 0.0\n",
            "loss tensor([[0.9810]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.4158,  0.6700,  0.1300],\n",
            "         [-0.2513, -0.6843, -0.6096],\n",
            "         [ 0.0654,  0.4399,  0.5575],\n",
            "         [-0.1581, -0.3460, -0.5816],\n",
            "         [-0.0838, -0.2343,  0.0481],\n",
            "         [-0.5007,  0.1480,  0.1953]]], device='cuda:0') tensor([ 0.2021,  0.0908, -0.2035,  0.1751,  0.2760,  0.2276], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.98095703125\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.1954345703125 0.0\n",
            "tcost icost 0.11700439453125 0.0\n",
            "tcost icost 0.17431640625 0.0\n",
            "tcost icost 0.12158203125 0.0\n",
            "tcost icost 0.25732421875 0.0\n",
            "tcost icost 0.229736328125 0.0\n",
            "loss tensor([[0.8350]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.2941,  0.8228,  0.3995],\n",
            "         [-0.2721, -0.7025, -0.6051],\n",
            "         [ 0.0967,  0.4850,  0.6083],\n",
            "         [-0.1593, -0.3707, -0.6008],\n",
            "         [-0.0953, -0.2650, -0.0094],\n",
            "         [-0.4960,  0.1553,  0.1959]]], device='cuda:0') tensor([-0.0575,  0.3352, -0.3432,  0.1850,  0.2973,  0.2124], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8349609375\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.205322265625 0.0\n",
            "tcost icost 0.2073974609375 0.0\n",
            "tcost icost 0.0751953125 0.0\n",
            "tcost icost 0.2099609375 0.0\n",
            "tcost icost 0.167724609375 0.0\n",
            "tcost icost 0.0771484375 0.0\n",
            "loss tensor([[0.7617]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.3988,  0.7870,  0.1630],\n",
            "         [-0.2560, -0.6877, -0.6100],\n",
            "         [ 0.1036,  0.4471,  0.5149],\n",
            "         [-0.1358, -0.3485, -0.6132],\n",
            "         [-0.0750, -0.2439,  0.0071],\n",
            "         [-0.5050,  0.1327,  0.1745]]], device='cuda:0') tensor([ 0.2443,  0.1186, -0.1973,  0.1410,  0.2605,  0.2339], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.76171875\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.1861572265625 0.0\n",
            "tcost icost 0.09881591796875 0.0\n",
            "tcost icost 0.15966796875 0.0\n",
            "tcost icost 0.10791015625 0.0\n",
            "tcost icost 0.2529296875 0.0\n",
            "tcost icost 0.2159423828125 0.0\n",
            "loss tensor([[0.7764]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.2827,  0.8732,  0.3970],\n",
            "         [-0.2732, -0.7038, -0.6073],\n",
            "         [ 0.1365,  0.4903,  0.5677],\n",
            "         [-0.1395, -0.3728, -0.6303],\n",
            "         [-0.0902, -0.2767, -0.0559],\n",
            "         [-0.4996,  0.1418,  0.1756]]], device='cuda:0') tensor([-0.0112,  0.3490, -0.3307,  0.1543,  0.2853,  0.2166], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7763671875\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.212890625 0.0\n",
            "tcost icost 0.230224609375 0.0\n",
            "tcost icost 0.11328125 0.0\n",
            "tcost icost 0.257080078125 0.0\n",
            "tcost icost 0.18896484375 0.0\n",
            "tcost icost 0.1011962890625 0.0\n",
            "loss tensor([[0.8828]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.3681,  0.8487,  0.1949],\n",
            "         [-0.2570, -0.6884, -0.6125],\n",
            "         [ 0.1431,  0.4476,  0.4554],\n",
            "         [-0.1171, -0.3569, -0.6460],\n",
            "         [-0.0690, -0.2566, -0.0406],\n",
            "         [-0.5082,  0.1186,  0.1535]]], device='cuda:0') tensor([ 0.2382,  0.1174, -0.1804,  0.1172,  0.2468,  0.2378], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8828125\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.187744140625 0.0\n",
            "tcost icost 0.10247802734375 0.0\n",
            "tcost icost 0.153076171875 0.0\n",
            "tcost icost 0.08978271484375 0.0\n",
            "tcost icost 0.2509765625 0.0\n",
            "tcost icost 0.2061767578125 0.0\n",
            "loss tensor([[0.7559]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.2605,  0.8807,  0.3957],\n",
            "         [-0.2751, -0.7054, -0.6085],\n",
            "         [ 0.1791,  0.4966,  0.5232],\n",
            "         [-0.1258, -0.3879, -0.6633],\n",
            "         [-0.0835, -0.2849, -0.1010],\n",
            "         [-0.5028,  0.1278,  0.1545]]], device='cuda:0') tensor([ 0.0023,  0.3416, -0.3212,  0.1447,  0.2701,  0.2203], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.755859375\n",
            "act tensor([[ 3, 10,  8, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.21435546875 0.0\n",
            "tcost icost 0.2392578125 0.0\n",
            "tcost icost 0.1416015625 0.0\n",
            "tcost icost 0.28662109375 0.0\n",
            "tcost icost 0.20263671875 0.0\n",
            "tcost icost 0.121826171875 0.0\n",
            "loss tensor([[0.9580]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.3444,  0.8579,  0.1924],\n",
            "         [-0.2562, -0.6886, -0.6164],\n",
            "         [ 0.1799,  0.4531,  0.3993],\n",
            "         [-0.1075, -0.3823, -0.6822],\n",
            "         [-0.0609, -0.2649, -0.0875],\n",
            "         [-0.5121,  0.1016,  0.1297]]], device='cuda:0') tensor([ 0.2495,  0.1012, -0.1784,  0.1163,  0.2300,  0.2442], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.9580078125\n",
            "act tensor([[ 3, 10, 12, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.1845703125 0.0\n",
            "tcost icost 0.08612060546875 0.0\n",
            "tcost icost 0.10589599609375 0.0\n",
            "tcost icost 0.07354736328125 0.0\n",
            "tcost icost 0.22509765625 0.0\n",
            "tcost icost 0.18798828125 0.0\n",
            "loss tensor([[0.6602]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.2359,  0.8872,  0.3966],\n",
            "         [-0.2749, -0.7054, -0.6115],\n",
            "         [ 0.2148,  0.4943,  0.4586],\n",
            "         [-0.1121, -0.3979, -0.6930],\n",
            "         [-0.0851, -0.2974, -0.1507],\n",
            "         [-0.5065,  0.1123,  0.1323]]], device='cuda:0') tensor([-0.0028,  0.3162, -0.3038,  0.1276,  0.2611,  0.2256], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.66015625\n",
            "search tensor([[[-0.0851, -0.2974, -0.1507],\n",
            "         [-0.5065,  0.1123,  0.1323],\n",
            "         [-0.0258, -0.4562,  0.0315],\n",
            "         [-0.3574, -0.3651,  0.1174],\n",
            "         [ 0.3293, -0.4025, -0.0188],\n",
            "         [-0.4989,  0.4005,  0.0429]]], device='cuda:0') tensor([ 0.2611,  0.2256,  0.5714, -0.7484, -0.1076,  1.0700], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[4, 1, 4, 1, 0, 3]], device='cuda:0')\n",
            "tcost icost 0.130859375 0.0\n",
            "tcost icost 0.240966796875 0.0\n",
            "tcost icost 0.1461181640625 0.0\n",
            "tcost icost -0.032562255859375 0.0\n",
            "tcost icost 0.004795074462890625 0.0\n",
            "tcost icost -0.2232666015625 0.0\n",
            "loss tensor([[0.3137]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.0667, -0.2848, -0.1513],\n",
            "         [-0.5187,  0.0655,  0.0793],\n",
            "         [ 0.0041, -0.4335,  0.0579],\n",
            "         [-0.3622, -0.3711,  0.1101],\n",
            "         [ 0.3249, -0.4077, -0.0274],\n",
            "         [-0.4945,  0.4078,  0.0485]]], device='cuda:0') tensor([ 0.2437,  0.2467,  0.5245, -0.7402, -0.1026,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.313720703125\n",
            "act tensor([[4, 1, 4, 1, 0, 3]], device='cuda:0')\n",
            "tcost icost 0.130859375 0.0\n",
            "tcost icost 0.232177734375 0.0\n",
            "tcost icost 0.1751708984375 0.0\n",
            "tcost icost -0.0267486572265625 0.0\n",
            "tcost icost 0.005420684814453125 0.0\n",
            "tcost icost -0.1961669921875 0.0\n",
            "loss tensor([[0.3501]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.0631, -0.2861, -0.1670],\n",
            "         [-0.5266,  0.0254,  0.0331],\n",
            "         [ 0.0359, -0.4084,  0.0843],\n",
            "         [-0.3672, -0.3773,  0.1022],\n",
            "         [ 0.3201, -0.4132, -0.0367],\n",
            "         [-0.4900,  0.4151,  0.0542]]], device='cuda:0') tensor([ 0.2464,  0.2581,  0.4745, -0.7322, -0.0976,  0.9885], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.35009765625\n",
            "act tensor([[ 4, 13,  4, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.130859375 0.0\n",
            "tcost icost 0.2364501953125 0.0\n",
            "tcost icost 0.1767578125 0.0\n",
            "tcost icost -0.0282745361328125 0.0\n",
            "tcost icost 0.005496978759765625 0.0\n",
            "tcost icost -0.208740234375 0.0\n",
            "loss tensor([[0.3467]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.0463, -0.2746, -0.1696],\n",
            "         [-0.5340, -0.0214, -0.0180],\n",
            "         [ 0.0680, -0.3817,  0.1106],\n",
            "         [-0.3721, -0.3835,  0.0945],\n",
            "         [ 0.3152, -0.4187, -0.0458],\n",
            "         [-0.4856,  0.4222,  0.0598]]], device='cuda:0') tensor([ 0.2308,  0.2780,  0.4249, -0.7232, -0.0918,  0.9774], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.3466796875\n",
            "act tensor([[ 4, 13,  4, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.1307373046875 0.0\n",
            "tcost icost 0.2259521484375 0.0\n",
            "tcost icost 0.2017822265625 0.0\n",
            "tcost icost -0.021942138671875 0.0\n",
            "tcost icost 0.00634765625 0.0\n",
            "tcost icost -0.1923828125 0.0\n",
            "loss tensor([[0.3721]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.0746, -0.3083, -0.2230],\n",
            "         [-0.5288, -0.0309, -0.0425],\n",
            "         [ 0.0920, -0.3623,  0.1250],\n",
            "         [-0.3773, -0.3899,  0.0861],\n",
            "         [ 0.3104, -0.4240, -0.0550],\n",
            "         [-0.4811,  0.4293,  0.0655]]], device='cuda:0') tensor([ 0.2685,  0.2604,  0.3889, -0.7144, -0.0868,  0.9659], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.3720703125\n",
            "act tensor([[ 4, 13,  4, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.130859375 0.0\n",
            "tcost icost 0.236328125 0.0\n",
            "tcost icost 0.2119140625 0.0\n",
            "tcost icost -0.02099609375 0.0\n",
            "tcost icost 0.007129669189453125 0.0\n",
            "tcost icost -0.19482421875 0.0\n",
            "loss tensor([[0.3894]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.0393, -0.2735, -0.2014],\n",
            "         [-0.5364, -0.0891, -0.0979],\n",
            "         [ 0.1219, -0.3344,  0.1493],\n",
            "         [-0.3821, -0.3960,  0.0784],\n",
            "         [ 0.3055, -0.4292, -0.0642],\n",
            "         [-0.4766,  0.4363,  0.0712]]], device='cuda:0') tensor([ 0.2220,  0.2906,  0.3396, -0.7058, -0.0816,  0.9545], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.389404296875\n",
            "act tensor([[ 4, 13,  4, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.1302490234375 0.0\n",
            "tcost icost 0.2181396484375 0.0\n",
            "tcost icost 0.21240234375 0.0\n",
            "tcost icost -0.012603759765625 0.0\n",
            "tcost icost 0.00824737548828125 0.0\n",
            "tcost icost -0.1693115234375 0.0\n",
            "loss tensor([[0.3945]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.0790, -0.3234, -0.2697],\n",
            "         [-0.5265, -0.0822, -0.1108],\n",
            "         [ 0.1424, -0.3179,  0.1585],\n",
            "         [-0.3874, -0.4022,  0.0703],\n",
            "         [ 0.2998, -0.4351, -0.0744],\n",
            "         [-0.4721,  0.4434,  0.0772]]], device='cuda:0') tensor([ 0.2762,  0.2566,  0.3108, -0.6971, -0.0755,  0.9429], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.39453125\n",
            "act tensor([[ 4,  4,  4, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.1307373046875 0.0\n",
            "tcost icost 0.22216796875 0.0\n",
            "tcost icost 0.23388671875 0.0\n",
            "tcost icost -0.01302337646484375 0.0\n",
            "tcost icost 0.00835418701171875 0.0\n",
            "tcost icost -0.172607421875 0.0\n",
            "loss tensor([[0.4141]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.0872, -0.3428, -0.3057],\n",
            "         [-0.5198, -0.0972, -0.1397],\n",
            "         [ 0.1666, -0.2974,  0.1699],\n",
            "         [-0.3925, -0.4083,  0.0622],\n",
            "         [ 0.2938, -0.4411, -0.0848],\n",
            "         [-0.4675,  0.4505,  0.0833]]], device='cuda:0') tensor([ 0.2913,  0.2448,  0.2749, -0.6883, -0.0692,  0.9312], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.4140625\n",
            "act tensor([[ 4,  4,  4, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.130615234375 0.0\n",
            "tcost icost 0.2200927734375 0.0\n",
            "tcost icost 0.2470703125 0.0\n",
            "tcost icost -0.0108184814453125 0.0\n",
            "tcost icost 0.00905609130859375 0.0\n",
            "tcost icost -0.1689453125 0.0\n",
            "loss tensor([[0.4272]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.0688, -0.3265, -0.3061],\n",
            "         [-0.5182, -0.1310, -0.1767],\n",
            "         [ 0.1905, -0.2756,  0.1820],\n",
            "         [-0.3974, -0.4141,  0.0543],\n",
            "         [ 0.2874, -0.4472, -0.0954],\n",
            "         [-0.4629,  0.4575,  0.0895]]], device='cuda:0') tensor([ 0.2653,  0.2537,  0.2365, -0.6796, -0.0625,  0.9193], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.42724609375\n",
            "act tensor([[ 4,  4,  7, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.130859375 0.0\n",
            "tcost icost 0.221923828125 0.0\n",
            "tcost icost 0.241455078125 0.0\n",
            "tcost icost -0.0128326416015625 0.0\n",
            "tcost icost 0.00811004638671875 0.0\n",
            "tcost icost -0.1695556640625 0.0\n",
            "loss tensor([[0.4224]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.0810, -0.3529, -0.3472],\n",
            "         [-0.5096, -0.1406, -0.2011],\n",
            "         [ 0.2141, -0.2553,  0.1916],\n",
            "         [-0.4022, -0.4197,  0.0464],\n",
            "         [ 0.2806, -0.4535, -0.1067],\n",
            "         [-0.4581,  0.4647,  0.0958]]], device='cuda:0') tensor([ 0.2875,  0.2366,  0.2027, -0.6713, -0.0556,  0.9073], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.42236328125\n",
            "act tensor([[ 4,  4,  7, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.130615234375 0.0\n",
            "tcost icost 0.2216796875 0.0\n",
            "tcost icost 0.2578125 0.0\n",
            "tcost icost -0.0115966796875 0.0\n",
            "tcost icost 0.008697509765625 0.0\n",
            "tcost icost -0.1688232421875 0.0\n",
            "loss tensor([[0.4370]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.0619, -0.3345, -0.3464],\n",
            "         [-0.5058, -0.1773, -0.2396],\n",
            "         [ 0.2411, -0.2290,  0.2065],\n",
            "         [-0.4067, -0.4252,  0.0388],\n",
            "         [ 0.2734, -0.4599, -0.1181],\n",
            "         [-0.4532,  0.4718,  0.1023]]], device='cuda:0') tensor([ 0.2600,  0.2483,  0.1598, -0.6631, -0.0484,  0.8951], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.43701171875\n",
            "act tensor([[ 4,  4,  7, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.130859375 0.0\n",
            "tcost icost 0.22265625 0.0\n",
            "tcost icost 0.25439453125 0.0\n",
            "tcost icost -0.005519866943359375 0.0\n",
            "tcost icost 0.0097503662109375 0.0\n",
            "tcost icost -0.148193359375 0.0\n",
            "loss tensor([[0.4521]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.0787, -0.3678, -0.3909],\n",
            "         [-0.4959, -0.1808, -0.2589],\n",
            "         [ 0.2610, -0.2112,  0.2143],\n",
            "         [-0.4114, -0.4308,  0.0311],\n",
            "         [ 0.2669, -0.4656, -0.1289],\n",
            "         [-0.4486,  0.4785,  0.1085]]], device='cuda:0') tensor([ 0.2882,  0.2254,  0.1302, -0.6543, -0.0420,  0.8832], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.4521484375\n",
            "act tensor([[10,  4,  7, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.12841796875 0.0\n",
            "tcost icost 0.2030029296875 0.0\n",
            "tcost icost 0.22802734375 0.0\n",
            "tcost icost -0.000705718994140625 0.0\n",
            "tcost icost 0.01019287109375 0.0\n",
            "tcost icost -0.11749267578125 0.0\n",
            "loss tensor([[0.4326]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.1141, -0.4264, -0.4471],\n",
            "         [-0.4870, -0.1795, -0.2733],\n",
            "         [ 0.2805, -0.1922,  0.2225],\n",
            "         [-0.4173, -0.4386,  0.0182],\n",
            "         [ 0.2559, -0.4751, -0.1481],\n",
            "         [-0.4426,  0.4870,  0.1158]]], device='cuda:0') tensor([ 0.3407,  0.1930,  0.0988, -0.6423, -0.0303,  0.8679], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.4326171875\n",
            "act tensor([[10,  4,  7, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.1297607421875 0.0\n",
            "tcost icost 0.2193603515625 0.0\n",
            "tcost icost 0.265625 0.0\n",
            "tcost icost -0.000812530517578125 0.0\n",
            "tcost icost 0.01087188720703125 0.0\n",
            "tcost icost -0.1295166015625 0.0\n",
            "loss tensor([[0.4729]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.1267, -0.4581, -0.4779],\n",
            "         [-0.4798, -0.1937, -0.2968],\n",
            "         [ 0.3039, -0.1687,  0.2332],\n",
            "         [-0.4227, -0.4460,  0.0061],\n",
            "         [ 0.2442, -0.4847, -0.1674],\n",
            "         [-0.4366,  0.4954,  0.1232]]], device='cuda:0') tensor([ 0.3603,  0.1776,  0.0603, -0.6307, -0.0180,  0.8525], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.472900390625\n",
            "act tensor([[10,  4,  7, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.1297607421875 0.0\n",
            "tcost icost 0.2181396484375 0.0\n",
            "tcost icost 0.284423828125 0.0\n",
            "tcost icost 0.0011501312255859375 0.0\n",
            "tcost icost 0.011932373046875 0.0\n",
            "tcost icost -0.12200927734375 0.0\n",
            "loss tensor([[0.4934]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.1225, -0.4615, -0.4901],\n",
            "         [-0.4750, -0.2165, -0.3217],\n",
            "         [ 0.3270, -0.1468,  0.2413],\n",
            "         [-0.4276, -0.4527, -0.0050],\n",
            "         [ 0.2321, -0.4942, -0.1866],\n",
            "         [-0.4305,  0.5037,  0.1308]]], device='cuda:0') tensor([ 0.3527,  0.1732,  0.0226, -0.6195, -0.0053,  0.8371], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.493408203125\n",
            "act tensor([[10,  4,  7, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.1297607421875 0.0\n",
            "tcost icost 0.2203369140625 0.0\n",
            "tcost icost 0.286865234375 0.0\n",
            "tcost icost 0.004169464111328125 0.0\n",
            "tcost icost 0.01348114013671875 0.0\n",
            "tcost icost -0.0986328125 0.0\n",
            "loss tensor([[0.5142]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.1204, -0.4672, -0.5028],\n",
            "         [-0.4699, -0.2385, -0.3459],\n",
            "         [ 0.3500, -0.1238,  0.2510],\n",
            "         [-0.4328, -0.4604, -0.0181],\n",
            "         [ 0.2181, -0.5047, -0.2087],\n",
            "         [-0.4242,  0.5122,  0.1384]]], device='cuda:0') tensor([ 0.3463,  0.1674, -0.0151, -0.6061,  0.0088,  0.8203], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.51416015625\n",
            "act tensor([[10,  4,  7, 13,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.1297607421875 0.0\n",
            "tcost icost 0.222412109375 0.0\n",
            "tcost icost 0.287841796875 0.0\n",
            "tcost icost 0.007053375244140625 0.0\n",
            "tcost icost 0.0137786865234375 0.0\n",
            "tcost icost -0.07330322265625 0.0\n",
            "loss tensor([[0.5347]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.1225, -0.4794, -0.5174],\n",
            "         [-0.4633, -0.2508, -0.3641],\n",
            "         [ 0.3684, -0.1074,  0.2562],\n",
            "         [-0.4380, -0.4687, -0.0319],\n",
            "         [ 0.2050, -0.5142, -0.2306],\n",
            "         [-0.4177,  0.5208,  0.1461]]], device='cuda:0') tensor([ 0.3440,  0.1535, -0.0430, -0.5921,  0.0208,  0.8026], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.53466796875\n",
            "act tensor([[10,  4,  7, 13, 10,  3]], device='cuda:0')\n",
            "tcost icost 0.1297607421875 0.0\n",
            "tcost icost 0.2236328125 0.0\n",
            "tcost icost 0.293212890625 0.0\n",
            "tcost icost 0.008697509765625 0.0\n",
            "tcost icost 0.01561737060546875 0.0\n",
            "tcost icost -0.09368896484375 0.0\n",
            "loss tensor([[0.5298]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.1322, -0.5036, -0.5350],\n",
            "         [-0.4554, -0.2501, -0.3750],\n",
            "         [ 0.3852, -0.0977,  0.2552],\n",
            "         [-0.4424, -0.4758, -0.0441],\n",
            "         [ 0.1906, -0.5241, -0.2522],\n",
            "         [-0.4113,  0.5290,  0.1538]]], device='cuda:0') tensor([ 0.3509,  0.1260, -0.0608, -0.5789,  0.0349,  0.7858], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.52978515625\n",
            "act tensor([[10,  4,  7, 13, 10,  3]], device='cuda:0')\n",
            "tcost icost 0.1297607421875 0.0\n",
            "tcost icost 0.221435546875 0.0\n",
            "tcost icost 0.314697265625 0.0\n",
            "tcost icost 0.0090179443359375 0.0\n",
            "tcost icost 0.015869140625 0.0\n",
            "tcost icost -0.09100341796875 0.0\n",
            "loss tensor([[0.5469]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.1416, -0.5257, -0.5470],\n",
            "         [-0.4484, -0.2567, -0.3892],\n",
            "         [ 0.4043, -0.0839,  0.2564],\n",
            "         [-0.4465, -0.4828, -0.0559],\n",
            "         [ 0.1762, -0.5335, -0.2731],\n",
            "         [-0.4049,  0.5371,  0.1616]]], device='cuda:0') tensor([ 0.3609,  0.1039, -0.0885, -0.5658,  0.0484,  0.7689], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.546875\n",
            "act tensor([[10,  4,  7, 13, 10,  3]], device='cuda:0')\n",
            "tcost icost 0.1298828125 0.0\n",
            "tcost icost 0.2147216796875 0.0\n",
            "tcost icost 0.33544921875 0.0\n",
            "tcost icost 0.0100555419921875 0.0\n",
            "tcost icost 0.016876220703125 0.0\n",
            "tcost icost -0.08428955078125 0.0\n",
            "loss tensor([[0.5635]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.1359, -0.5241, -0.5555],\n",
            "         [-0.4424, -0.2864, -0.4167],\n",
            "         [ 0.4279, -0.0613,  0.2635],\n",
            "         [-0.4503, -0.4917, -0.0722],\n",
            "         [ 0.1602, -0.5433, -0.2965],\n",
            "         [-0.3980,  0.5456,  0.1699]]], device='cuda:0') tensor([ 0.3471,  0.1045, -0.1291, -0.5478,  0.0645,  0.7514], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.5634765625\n",
            "act tensor([[10, 10,  7, 13, 10,  3]], device='cuda:0')\n",
            "tcost icost 0.1297607421875 0.0\n",
            "tcost icost 0.2032470703125 0.0\n",
            "tcost icost 0.3466796875 0.0\n",
            "tcost icost 0.00641632080078125 0.0\n",
            "tcost icost 0.0158843994140625 0.0\n",
            "tcost icost -0.096923828125 0.0\n",
            "loss tensor([[0.5513]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.1196, -0.5073, -0.5636],\n",
            "         [-0.4347, -0.3310, -0.4521],\n",
            "         [ 0.4530, -0.0346,  0.2739],\n",
            "         [-0.4534, -0.5000, -0.0875],\n",
            "         [ 0.1455, -0.5519, -0.3169],\n",
            "         [-0.3917,  0.5533,  0.1778]]], device='cuda:0') tensor([ 0.3166,  0.1201, -0.1808, -0.5304,  0.0784,  0.7347], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.55126953125\n",
            "search tensor([[[ 0.1455, -0.5519, -0.3169],\n",
            "         [-0.3917,  0.5533,  0.1778],\n",
            "         [ 0.3793, -0.0144,  0.0611],\n",
            "         [-0.3530, -0.0922,  0.2401],\n",
            "         [-0.3763, -0.4920, -0.3450],\n",
            "         [-0.3176, -0.5252,  0.4003]]], device='cuda:0') tensor([ 0.0784,  0.7347, -0.5350, -0.3679,  0.1969, -0.6787], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[10,  3,  0,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.146728515625 0.0\n",
            "tcost icost -0.17236328125 0.0\n",
            "tcost icost -0.08056640625 0.0\n",
            "tcost icost 0.00409698486328125 0.0\n",
            "tcost icost -0.03643798828125 0.0\n",
            "tcost icost -0.032989501953125 0.0\n",
            "loss tensor([[-0.1141]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.0762, -0.5903, -0.3819],\n",
            "         [-0.3746,  0.5713,  0.1966],\n",
            "         [ 0.3752, -0.0204,  0.0514],\n",
            "         [-0.3616, -0.1111,  0.2213],\n",
            "         [-0.3751, -0.4833, -0.3393],\n",
            "         [-0.3187, -0.5265,  0.3981]]], device='cuda:0') tensor([ 0.1614,  0.6988, -0.5282, -0.3591,  0.1786, -0.6765], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.11407470703125\n",
            "act tensor([[10,  3,  0,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1673583984375 0.0\n",
            "tcost icost -0.1275634765625 0.0\n",
            "tcost icost -0.072998046875 0.0\n",
            "tcost icost 0.004779815673828125 0.0\n",
            "tcost icost -0.0172882080078125 0.0\n",
            "tcost icost -0.0283966064453125 0.0\n",
            "loss tensor([[-0.0312]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.0090, -0.6204, -0.4413],\n",
            "         [-0.3539,  0.5926,  0.2208],\n",
            "         [ 0.3709, -0.0273,  0.0406],\n",
            "         [-0.3681, -0.1257,  0.2058],\n",
            "         [-0.3739, -0.4764, -0.3351],\n",
            "         [-0.3199, -0.5278,  0.3958]]], device='cuda:0') tensor([ 0.2337,  0.6557, -0.5202, -0.3520,  0.1644, -0.6743], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.031219482421875\n",
            "act tensor([[10,  3,  0,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.177001953125 0.0\n",
            "tcost icost -0.06884765625 0.0\n",
            "tcost icost -0.06390380859375 0.0\n",
            "tcost icost 0.005481719970703125 0.0\n",
            "tcost icost 0.000438690185546875 0.0\n",
            "tcost icost -0.0231781005859375 0.0\n",
            "loss tensor([[0.0539]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.0797, -0.6478, -0.5036],\n",
            "         [-0.3293,  0.6164,  0.2507],\n",
            "         [ 0.3655, -0.0357,  0.0268],\n",
            "         [-0.3751, -0.1420,  0.1877],\n",
            "         [-0.3723, -0.4695, -0.3318],\n",
            "         [-0.3211, -0.5292,  0.3935]]], device='cuda:0') tensor([ 0.3240,  0.6041, -0.5106, -0.3434,  0.1502, -0.6721], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.05389404296875\n",
            "act tensor([[10,  3,  0,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.179931640625 0.0\n",
            "tcost icost 0.01241302490234375 0.0\n",
            "tcost icost -0.05035400390625 0.0\n",
            "tcost icost 0.006671905517578125 0.0\n",
            "tcost icost 0.041107177734375 0.0\n",
            "tcost icost -0.01541900634765625 0.0\n",
            "loss tensor([[0.1731]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-1.7880e-01, -6.5500e-01, -5.4351e-01],\n",
            "         [-2.9768e-01,  6.4573e-01,  2.9349e-01],\n",
            "         [ 3.5688e-01, -4.9920e-02,  6.1774e-04],\n",
            "         [-3.8470e-01, -1.6732e-01,  1.5612e-01],\n",
            "         [-3.6885e-01, -4.6226e-01, -3.3146e-01],\n",
            "         [-3.2242e-01, -5.3063e-01,  3.9104e-01]]], device='cuda:0') tensor([ 0.4192,  0.5323, -0.4943, -0.3271,  0.1344, -0.6700], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.173095703125\n",
            "act tensor([[10,  3,  0,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1661376953125 0.0\n",
            "tcost icost 0.10919189453125 0.0\n",
            "tcost icost -0.031585693359375 0.0\n",
            "tcost icost 0.0089874267578125 0.0\n",
            "tcost icost 0.109375 0.0\n",
            "tcost icost -0.0045318603515625 0.0\n",
            "loss tensor([[0.3145]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.2005, -0.6495, -0.5496],\n",
            "         [-0.2669,  0.6711,  0.3395],\n",
            "         [ 0.3463, -0.0684, -0.0281],\n",
            "         [-0.3867, -0.1807,  0.1343],\n",
            "         [-0.3656, -0.4582, -0.3334],\n",
            "         [-0.3238, -0.5321,  0.3885]]], device='cuda:0') tensor([ 0.4786,  0.4574, -0.4729, -0.3184,  0.1249, -0.6676], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.314453125\n",
            "act tensor([[10,  3,  0,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.143798828125 0.0\n",
            "tcost icost 0.1710205078125 0.0\n",
            "tcost icost -0.01363372802734375 0.0\n",
            "tcost icost 0.010986328125 0.0\n",
            "tcost icost 0.154296875 0.0\n",
            "tcost icost 0.0034770965576171875 0.0\n",
            "loss tensor([[0.3982]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.1985, -0.6459, -0.5567],\n",
            "         [-0.2445,  0.6871,  0.3759],\n",
            "         [ 0.3329, -0.0922, -0.0622],\n",
            "         [-0.3798, -0.1787,  0.1249],\n",
            "         [-0.3615, -0.4596, -0.3409],\n",
            "         [-0.3251, -0.5335,  0.3861]]], device='cuda:0') tensor([ 0.4918,  0.3998, -0.4459, -0.3220,  0.1237, -0.6653], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.398193359375\n",
            "act tensor([[10,  3,  0,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1383056640625 0.0\n",
            "tcost icost 0.1861572265625 0.0\n",
            "tcost icost -0.003997802734375 0.0\n",
            "tcost icost 0.0133056640625 0.0\n",
            "tcost icost 0.16748046875 0.0\n",
            "tcost icost 0.007450103759765625 0.0\n",
            "loss tensor([[0.4265]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.1807, -0.6385, -0.5681],\n",
            "         [-0.2342,  0.6937,  0.3949],\n",
            "         [ 0.3194, -0.1161, -0.0931],\n",
            "         [-0.3652, -0.1596,  0.1341],\n",
            "         [-0.3588, -0.4664, -0.3504],\n",
            "         [-0.3264, -0.5349,  0.3836]]], device='cuda:0') tensor([ 0.4494,  0.3701, -0.4189, -0.3395,  0.1327, -0.6630], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.426513671875\n",
            "act tensor([[10,  3,  0,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1556396484375 0.0\n",
            "tcost icost 0.19287109375 0.0\n",
            "tcost icost -0.004291534423828125 0.0\n",
            "tcost icost 0.0145416259765625 0.0\n",
            "tcost icost 0.1826171875 0.0\n",
            "tcost icost 0.0059814453125 0.0\n",
            "loss tensor([[0.4595]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.1749, -0.6353, -0.5757],\n",
            "         [-0.2176,  0.7034,  0.4252],\n",
            "         [ 0.3042, -0.1404, -0.1271],\n",
            "         [-0.3528, -0.1463,  0.1353],\n",
            "         [-0.3550, -0.4718, -0.3605],\n",
            "         [-0.3276, -0.5363,  0.3812]]], device='cuda:0') tensor([ 0.4497,  0.3213, -0.3917, -0.3533,  0.1379, -0.6608], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.45947265625\n",
            "act tensor([[10,  3,  4,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.155517578125 0.0\n",
            "tcost icost 0.204345703125 0.0\n",
            "tcost icost -0.0005555152893066406 0.0\n",
            "tcost icost 0.0156707763671875 0.0\n",
            "tcost icost 0.181640625 0.0\n",
            "tcost icost 0.0045166015625 0.0\n",
            "loss tensor([[0.4722]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.1550, -0.6264, -0.5878],\n",
            "         [-0.2066,  0.7088,  0.4445],\n",
            "         [ 0.2860, -0.1671, -0.1613],\n",
            "         [-0.3370, -0.1260,  0.1446],\n",
            "         [-0.3515, -0.4778, -0.3697],\n",
            "         [-0.3288, -0.5376,  0.3788]]], device='cuda:0') tensor([ 0.4139,  0.2906, -0.3604, -0.3730,  0.1436, -0.6587], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.47216796875\n",
            "act tensor([[10,  3,  4,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.16748046875 0.0\n",
            "tcost icost 0.2099609375 0.0\n",
            "tcost icost 0.0005097389221191406 0.0\n",
            "tcost icost 0.0174407958984375 0.0\n",
            "tcost icost 0.1949462890625 0.0\n",
            "tcost icost 0.00350189208984375 0.0\n",
            "loss tensor([[0.4995]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.1669, -0.6315, -0.5904],\n",
            "         [-0.1873,  0.7174,  0.4773],\n",
            "         [ 0.2662, -0.1912, -0.1976],\n",
            "         [-0.3320, -0.1295,  0.1290],\n",
            "         [-0.3472, -0.4801, -0.3785],\n",
            "         [-0.3300, -0.5390,  0.3765]]], device='cuda:0') tensor([ 0.4397,  0.2296, -0.3312, -0.3727,  0.1429, -0.6566], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.49951171875\n",
            "act tensor([[10,  3,  4,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.159423828125 0.0\n",
            "tcost icost 0.21826171875 0.0\n",
            "tcost icost 0.01523590087890625 0.0\n",
            "tcost icost 0.0218963623046875 0.0\n",
            "tcost icost 0.2193603515625 0.0\n",
            "tcost icost 0.008544921875 0.0\n",
            "loss tensor([[0.5332]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.1552, -0.6260, -0.6001],\n",
            "         [-0.1692,  0.7231,  0.5092],\n",
            "         [ 0.2358, -0.2282, -0.2459],\n",
            "         [-0.3173, -0.1136,  0.1322],\n",
            "         [-0.3432, -0.4845, -0.3872],\n",
            "         [-0.3316, -0.5408,  0.3733]]], device='cuda:0') tensor([ 0.4193,  0.1782, -0.2835, -0.3917,  0.1424, -0.6532], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.533203125\n",
            "act tensor([[10,  3,  4,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.166015625 0.0\n",
            "tcost icost 0.2279052734375 0.0\n",
            "tcost icost 0.037200927734375 0.0\n",
            "tcost icost 0.03167724609375 0.0\n",
            "tcost icost 0.247314453125 0.0\n",
            "tcost icost 0.010711669921875 0.0\n",
            "loss tensor([[0.5928]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.1573, -0.6270, -0.6061],\n",
            "         [-0.1470,  0.7285,  0.5482],\n",
            "         [ 0.1977, -0.2697, -0.3025],\n",
            "         [-0.3084, -0.1113,  0.1207],\n",
            "         [-0.3388, -0.4876, -0.3960],\n",
            "         [-0.3332, -0.5427,  0.3700]]], device='cuda:0') tensor([ 0.4194,  0.1076, -0.2263, -0.3993,  0.1384, -0.6497], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.5927734375\n",
            "act tensor([[10,  3,  4,  1, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.166015625 0.0\n",
            "tcost icost 0.2237548828125 0.0\n",
            "tcost icost 0.1085205078125 0.0\n",
            "tcost icost 0.0931396484375 0.0\n",
            "tcost icost 0.28955078125 0.0\n",
            "tcost icost 0.0164947509765625 0.0\n",
            "loss tensor([[0.7231]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.2123, -0.6542, -0.5982],\n",
            "         [-0.1026,  0.7341,  0.6395],\n",
            "         [ 0.1200, -0.3491, -0.4203],\n",
            "         [-0.3017, -0.1282,  0.0802],\n",
            "         [-0.3321, -0.4933, -0.4090],\n",
            "         [-0.3347, -0.5446,  0.3668]]], device='cuda:0') tensor([ 0.4845, -0.0745, -0.0959, -0.3785,  0.1344, -0.6463], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.72314453125\n",
            "act tensor([[10,  5, 10,  4, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1414794921875 0.0\n",
            "tcost icost 0.1351318359375 0.0\n",
            "tcost icost 0.44580078125 0.0\n",
            "tcost icost 0.2191162109375 0.0\n",
            "tcost icost 0.272216796875 0.0\n",
            "tcost icost 0.0283355712890625 0.0\n",
            "loss tensor([[0.9790]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.1979, -0.6371, -0.6133],\n",
            "         [-0.1135,  0.7418,  0.6027],\n",
            "         [ 0.1299, -0.3373, -0.4537],\n",
            "         [-0.3157, -0.1865,  0.0028],\n",
            "         [-0.3251, -0.4990, -0.4225],\n",
            "         [-0.3372, -0.5481,  0.3608]]], device='cuda:0') tensor([ 0.3551, -0.0610, -0.1267, -0.3306,  0.1362, -0.6390], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.97900390625\n",
            "act tensor([[10,  3, 10,  4, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.17724609375 0.0\n",
            "tcost icost 0.225830078125 0.0\n",
            "tcost icost 0.305419921875 0.0\n",
            "tcost icost 0.25927734375 0.0\n",
            "tcost icost 0.26025390625 0.0\n",
            "tcost icost 0.018035888671875 0.0\n",
            "loss tensor([[0.9985]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.2310, -0.6595, -0.6051],\n",
            "         [-0.0836,  0.7449,  0.6533],\n",
            "         [ 0.0659, -0.3977, -0.5528],\n",
            "         [-0.3156, -0.2345, -0.0757],\n",
            "         [-0.3180, -0.5055, -0.4350],\n",
            "         [-0.3384, -0.5496,  0.3581]]], device='cuda:0') tensor([ 0.4115, -0.2389, -0.0410, -0.2861,  0.1350, -0.6364], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.99853515625\n",
            "act tensor([[10,  5, 10,  4, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1680908203125 0.0\n",
            "tcost icost 0.1165771484375 0.0\n",
            "tcost icost 0.3896484375 0.0\n",
            "tcost icost 0.174072265625 0.0\n",
            "tcost icost 0.2388916015625 0.0\n",
            "tcost icost 0.039306640625 0.0\n",
            "loss tensor([[0.8960]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.1950, -0.6180, -0.6312],\n",
            "         [-0.1353,  0.7591,  0.5376],\n",
            "         [ 0.1519, -0.3122, -0.5491],\n",
            "         [-0.3058, -0.2477, -0.1175],\n",
            "         [-0.3037, -0.5320, -0.4659],\n",
            "         [-0.3429, -0.5558,  0.3475]]], device='cuda:0') tensor([ 0.1568, -0.0178, -0.2097, -0.2816,  0.1736, -0.6216], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.89599609375\n",
            "act tensor([[10,  3, 10,  4, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1668701171875 0.0\n",
            "tcost icost 0.217041015625 0.0\n",
            "tcost icost 0.01025390625 0.0\n",
            "tcost icost 0.04156494140625 0.0\n",
            "tcost icost 0.1790771484375 0.0\n",
            "tcost icost -0.0006995201110839844 0.0\n",
            "loss tensor([[0.5181]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.2356, -0.6687, -0.6087],\n",
            "         [-0.1008,  0.7643,  0.5955],\n",
            "         [ 0.1176, -0.3463, -0.5821],\n",
            "         [-0.2953, -0.2469, -0.1308],\n",
            "         [-0.2995, -0.5306, -0.4701],\n",
            "         [-0.3440, -0.5571,  0.3451]]], device='cuda:0') tensor([ 0.3209, -0.1365, -0.1520, -0.2841,  0.1644, -0.6196], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.51806640625\n",
            "act tensor([[10,  3, 10,  4, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1800537109375 0.0\n",
            "tcost icost 0.22802734375 0.0\n",
            "tcost icost 0.354248046875 0.0\n",
            "tcost icost 0.322265625 0.0\n",
            "tcost icost 0.254150390625 0.0\n",
            "tcost icost 0.0174713134765625 0.0\n",
            "loss tensor([[1.0840]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.2362, -0.6701, -0.6150],\n",
            "         [-0.0892,  0.7662,  0.6190],\n",
            "         [ 0.0679, -0.3921, -0.6465],\n",
            "         [-0.2787, -0.2667, -0.1818],\n",
            "         [-0.2912, -0.5394, -0.4812],\n",
            "         [-0.3453, -0.5586,  0.3423]]], device='cuda:0') tensor([ 0.3443, -0.2470, -0.1017, -0.2750,  0.1678, -0.6168], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.083984375\n",
            "act tensor([[10,  3, 10,  4, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1785888671875 0.0\n",
            "tcost icost 0.1650390625 0.0\n",
            "tcost icost 0.471923828125 0.0\n",
            "tcost icost 0.247802734375 0.0\n",
            "tcost icost 0.278564453125 0.0\n",
            "tcost icost 0.0291748046875 0.0\n",
            "loss tensor([[1.0908]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.2193, -0.6453, -0.6310],\n",
            "         [-0.1205,  0.7810,  0.5146],\n",
            "         [ 0.1382, -0.3226, -0.6528],\n",
            "         [-0.2768, -0.3132, -0.2464],\n",
            "         [-0.2812, -0.5442, -0.4922],\n",
            "         [-0.3481, -0.5626,  0.3352]]], device='cuda:0') tensor([ 0.1774, -0.0934, -0.2279, -0.2333,  0.1685, -0.6084], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.0908203125\n",
            "act tensor([[10,  3, 10,  4, 10,  1]], device='cuda:0')\n",
            "tcost icost 0.1690673828125 0.0\n",
            "tcost icost 0.274169921875 0.0\n",
            "tcost icost 0.0489501953125 0.0\n",
            "tcost icost 0.08258056640625 0.0\n",
            "tcost icost 0.18017578125 0.0\n",
            "tcost icost 0.005130767822265625 0.0\n",
            "loss tensor([[0.6367]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.2460, -0.6850, -0.6128],\n",
            "         [-0.0801,  0.7855,  0.6106],\n",
            "         [ 0.0709, -0.3906, -0.7000],\n",
            "         [-0.2606, -0.2928, -0.2423],\n",
            "         [-0.2769, -0.5472, -0.4965],\n",
            "         [-0.3493, -0.5639,  0.3326]]], device='cuda:0') tensor([ 0.3586, -0.2641, -0.1083, -0.2556,  0.1684, -0.6064], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.63671875\n",
            "search tensor([[[-0.2769, -0.5472, -0.4965],\n",
            "         [-0.3493, -0.5639,  0.3326],\n",
            "         [ 0.2633, -0.0887, -0.1241],\n",
            "         [ 0.1751, -0.1996,  0.4338],\n",
            "         [ 0.2120, -0.2200,  0.3768],\n",
            "         [-0.4228, -0.2451,  0.2868]]], device='cuda:0') tensor([ 0.1684, -0.6064, -0.5547, -0.3670, -0.9082, -0.0175], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[10,  1,  4,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.139892578125 0.0\n",
            "tcost icost 0.08966064453125 0.0\n",
            "tcost icost 0.133056640625 0.0\n",
            "tcost icost 0.1988525390625 0.0\n",
            "tcost icost 0.0027179718017578125 0.0\n",
            "tcost icost -0.038238525390625 0.0\n",
            "loss tensor([[0.4526]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.3333, -0.3832, -0.4374],\n",
            "         [-0.4327, -0.6568,  0.1589],\n",
            "         [ 0.2918, -0.0654, -0.1148],\n",
            "         [ 0.1710, -0.2111,  0.4244],\n",
            "         [ 0.2102, -0.2239,  0.3752],\n",
            "         [-0.4160, -0.2343,  0.2942]]], device='cuda:0') tensor([-0.0300, -0.3919, -0.5633, -0.3557, -0.9081, -0.0321], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.45263671875\n",
            "act tensor([[10, 13,  4,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.055389404296875 0.0\n",
            "tcost icost 0.183837890625 0.0\n",
            "tcost icost 0.1082763671875 0.0\n",
            "tcost icost 0.0657958984375 0.0\n",
            "tcost icost -0.0043487548828125 0.0\n",
            "tcost icost -0.0037174224853515625 0.0\n",
            "loss tensor([[0.3513]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.3195, -0.6261, -0.5911],\n",
            "         [-0.3650, -0.6111,  0.2783],\n",
            "         [ 0.2342, -0.1590, -0.2389],\n",
            "         [ 0.1906, -0.1817,  0.4396],\n",
            "         [ 0.2083, -0.2286,  0.3725],\n",
            "         [-0.4118, -0.2276,  0.2987]]], device='cuda:0') tensor([ 0.2216, -0.5331, -0.4302, -0.3844, -0.9052, -0.0410], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.351318359375\n",
            "act tensor([[10,  1,  4,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.150390625 0.0\n",
            "tcost icost 0.09503173828125 0.0\n",
            "tcost icost 0.146484375 0.0\n",
            "tcost icost 0.1671142578125 0.0\n",
            "tcost icost 0.007328033447265625 0.0\n",
            "tcost icost -0.03302001953125 0.0\n",
            "loss tensor([[0.4617]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.3600, -0.5493, -0.5842],\n",
            "         [-0.4528, -0.7032,  0.0819],\n",
            "         [ 0.2976, -0.0964, -0.1907],\n",
            "         [ 0.1811, -0.2038,  0.4255],\n",
            "         [ 0.2062, -0.2329,  0.3704],\n",
            "         [-0.4050, -0.2165,  0.3060]]], device='cuda:0') tensor([ 0.0114, -0.3093, -0.4938, -0.3595, -0.9042, -0.0550], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.461669921875\n",
            "act tensor([[10, 13,  4,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.072265625 0.0\n",
            "tcost icost 0.2041015625 0.0\n",
            "tcost icost 0.1220703125 0.0\n",
            "tcost icost 0.10614013671875 0.0\n",
            "tcost icost -0.0009236335754394531 0.0\n",
            "tcost icost 0.003208160400390625 0.0\n",
            "loss tensor([[0.4336]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.2842, -0.7149, -0.6328],\n",
            "         [-0.3798, -0.6540,  0.2353],\n",
            "         [ 0.2170, -0.2087, -0.3406],\n",
            "         [ 0.2040, -0.1682,  0.4434],\n",
            "         [ 0.2046, -0.2371,  0.3680],\n",
            "         [-0.4012, -0.2101,  0.3101]]], device='cuda:0') tensor([ 0.3001, -0.5099, -0.3288, -0.3915, -0.9017, -0.0633], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.43359375\n",
            "act tensor([[10, 13,  4,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.1573486328125 0.0\n",
            "tcost icost 0.04351806640625 0.0\n",
            "tcost icost 0.0040740966796875 0.0\n",
            "tcost icost 0.05908203125 0.0\n",
            "tcost icost 0.00749969482421875 0.0\n",
            "tcost icost -0.044891357421875 0.0\n",
            "loss tensor([[0.2213]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.2852, -0.7055, -0.6330],\n",
            "         [-0.4195, -0.6955,  0.1366],\n",
            "         [ 0.2641, -0.1639, -0.3024],\n",
            "         [ 0.1947, -0.1883,  0.4322],\n",
            "         [ 0.2021, -0.2416,  0.3659],\n",
            "         [-0.3938, -0.1979,  0.3179]]], device='cuda:0') tensor([ 0.2043, -0.3992, -0.3824, -0.3669, -0.9003, -0.0777], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2213134765625\n",
            "act tensor([[10, 13, 11,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.147216796875 0.0\n",
            "tcost icost 0.1536865234375 0.0\n",
            "tcost icost 0.2337646484375 0.0\n",
            "tcost icost 0.189697265625 0.0\n",
            "tcost icost 0.0057373046875 0.0\n",
            "tcost icost -0.005290985107421875 0.0\n",
            "loss tensor([[0.6143]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.2823, -0.6830, -0.6380],\n",
            "         [-0.4917, -0.7634, -0.0708],\n",
            "         [ 0.3308, -0.0960, -0.2562],\n",
            "         [ 0.1860, -0.2113,  0.4151],\n",
            "         [ 0.2011, -0.2443,  0.3646],\n",
            "         [-0.3882, -0.1887,  0.3237]]], device='cuda:0') tensor([ 0.0077, -0.1751, -0.4759, -0.3444, -0.9004, -0.0887], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6142578125\n",
            "act tensor([[10, 13, 11,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.07037353515625 0.0\n",
            "tcost icost 0.175537109375 0.0\n",
            "tcost icost 0.02294921875 0.0\n",
            "tcost icost 0.0191650390625 0.0\n",
            "tcost icost 0.0005970001220703125 0.0\n",
            "tcost icost 0.01073455810546875 0.0\n",
            "loss tensor([[0.2676]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.2773, -0.7020, -0.6385],\n",
            "         [-0.4732, -0.7496, -0.0175],\n",
            "         [ 0.3034, -0.1325, -0.3037],\n",
            "         [ 0.1989, -0.1916,  0.4269],\n",
            "         [ 0.1996, -0.2476,  0.3625],\n",
            "         [-0.3861, -0.1852,  0.3260]]], device='cuda:0') tensor([ 0.1800, -0.2577, -0.4187, -0.3638, -0.8977, -0.0932], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.267578125\n",
            "act tensor([[10, 13, 11,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.142578125 0.0\n",
            "tcost icost 0.224609375 0.0\n",
            "tcost icost 0.334716796875 0.0\n",
            "tcost icost 0.288818359375 0.0\n",
            "tcost icost 0.0034351348876953125 0.0\n",
            "tcost icost 0.0071563720703125 0.0\n",
            "loss tensor([[0.8335]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.2702, -0.7057, -0.6477],\n",
            "         [-0.4530, -0.7418, -0.0015],\n",
            "         [ 0.2572, -0.1900, -0.4146],\n",
            "         [ 0.2163, -0.1755,  0.4255],\n",
            "         [ 0.1981, -0.2514,  0.3600],\n",
            "         [-0.3832, -0.1803,  0.3292]]], device='cuda:0') tensor([ 0.2820, -0.3301, -0.3441, -0.3840, -0.8950, -0.0991], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.83349609375\n",
            "act tensor([[10, 13, 11,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.1561279296875 0.0\n",
            "tcost icost 0.12060546875 0.0\n",
            "tcost icost 0.2164306640625 0.0\n",
            "tcost icost 0.1690673828125 0.0\n",
            "tcost icost 0.00543975830078125 0.0\n",
            "tcost icost -0.0029354095458984375 0.0\n",
            "loss tensor([[0.5645]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.2556, -0.6801, -0.6601],\n",
            "         [-0.4941, -0.7913, -0.1967],\n",
            "         [ 0.3238, -0.1228, -0.3740],\n",
            "         [ 0.2063, -0.2022,  0.4058],\n",
            "         [ 0.1969, -0.2544,  0.3582],\n",
            "         [-0.3783, -0.1722,  0.3343]]], device='cuda:0') tensor([ 0.0638, -0.1096, -0.4357, -0.3553, -0.8944, -0.1086], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.564453125\n",
            "act tensor([[10, 10, 11,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.09478759765625 0.0\n",
            "tcost icost 0.1973876953125 0.0\n",
            "tcost icost 0.08331298828125 0.0\n",
            "tcost icost 0.11932373046875 0.0\n",
            "tcost icost 0.006439208984375 0.0\n",
            "tcost icost 0.01361083984375 0.0\n",
            "loss tensor([[0.4390]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.2721, -0.7138, -0.6453],\n",
            "         [-0.4798, -0.7675, -0.0838],\n",
            "         [ 0.2484, -0.2061, -0.4981],\n",
            "         [ 0.2275, -0.1738,  0.4181],\n",
            "         [ 0.1955, -0.2579,  0.3556],\n",
            "         [-0.3765, -0.1691,  0.3365]]], device='cuda:0') tensor([ 0.3124, -0.2931, -0.2859, -0.3746, -0.8908, -0.1121], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.43896484375\n",
            "act tensor([[10, 13, 11,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.1578369140625 0.0\n",
            "tcost icost 0.1123046875 0.0\n",
            "tcost icost 0.195068359375 0.0\n",
            "tcost icost 0.1495361328125 0.0\n",
            "tcost icost 0.00495147705078125 0.0\n",
            "tcost icost -0.0007200241088867188 0.0\n",
            "loss tensor([[0.5288]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.2576, -0.6939, -0.6538],\n",
            "         [-0.5005, -0.8060, -0.2623],\n",
            "         [ 0.3112, -0.1416, -0.4642],\n",
            "         [ 0.2207, -0.1940,  0.4027],\n",
            "         [ 0.1941, -0.2611,  0.3534],\n",
            "         [-0.3732, -0.1636,  0.3399]]], device='cuda:0') tensor([ 0.1011, -0.0792, -0.3811, -0.3522, -0.8887, -0.1184], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.52880859375\n",
            "act tensor([[10, 10, 11,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.11181640625 0.0\n",
            "tcost icost 0.208984375 0.0\n",
            "tcost icost 0.13037109375 0.0\n",
            "tcost icost 0.1865234375 0.0\n",
            "tcost icost 0.008270263671875 0.0\n",
            "tcost icost 0.0147552490234375 0.0\n",
            "loss tensor([[0.5552]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.2734, -0.7152, -0.6432],\n",
            "         [-0.4946, -0.7872, -0.1655],\n",
            "         [ 0.2364, -0.2131, -0.5796],\n",
            "         [ 0.2385, -0.1770,  0.4032],\n",
            "         [ 0.1926, -0.2643,  0.3508],\n",
            "         [-0.3720, -0.1615,  0.3414]]], device='cuda:0') tensor([ 0.3458, -0.2708, -0.2422, -0.3588, -0.8853, -0.1208], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.55517578125\n",
            "act tensor([[10, 10, 11,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.1558837890625 0.0\n",
            "tcost icost 0.06451416015625 0.0\n",
            "tcost icost 0.03851318359375 0.0\n",
            "tcost icost 0.066162109375 0.0\n",
            "tcost icost 0.005542755126953125 0.0\n",
            "tcost icost -0.005825042724609375 0.0\n",
            "loss tensor([[0.2937]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.2645, -0.7012, -0.6485],\n",
            "         [-0.4947, -0.8074, -0.2756],\n",
            "         [ 0.2852, -0.1655, -0.5521],\n",
            "         [ 0.2296, -0.1989,  0.3878],\n",
            "         [ 0.1913, -0.2671,  0.3491],\n",
            "         [-0.3669, -0.1531,  0.3468]]], device='cuda:0') tensor([ 0.1955, -0.1231, -0.3059, -0.3333, -0.8846, -0.1303], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.293701171875\n",
            "act tensor([[10, 10, 11,  7,  7,  1]], device='cuda:0')\n",
            "tcost icost 0.145751953125 0.0\n",
            "tcost icost 0.2198486328125 0.0\n",
            "tcost icost 0.291015625 0.0\n",
            "tcost icost 0.28564453125 0.0\n",
            "tcost icost 0.00783538818359375 0.0\n",
            "tcost icost 0.013031005859375 0.0\n",
            "loss tensor([[0.8013]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.2650, -0.7000, -0.6543],\n",
            "         [-0.4804, -0.8082, -0.3220],\n",
            "         [ 0.2967, -0.1464, -0.5773],\n",
            "         [ 0.2237, -0.2251,  0.3585],\n",
            "         [ 0.1897, -0.2706,  0.3461],\n",
            "         [-0.3658, -0.1514,  0.3479]]], device='cuda:0') tensor([ 0.2380, -0.1295, -0.3351, -0.3101, -0.8805, -0.1320], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.80126953125\n",
            "act tensor([[10, 10, 11,  7,  2,  1]], device='cuda:0')\n",
            "tcost icost 0.15283203125 0.0\n",
            "tcost icost 0.2020263671875 0.0\n",
            "tcost icost 0.3203125 0.0\n",
            "tcost icost 0.279296875 0.0\n",
            "tcost icost 0.0033817291259765625 0.0\n",
            "tcost icost 0.0101776123046875 0.0\n",
            "loss tensor([[0.8062]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.2535, -0.6884, -0.6663],\n",
            "         [-0.4583, -0.8009, -0.3854],\n",
            "         [ 0.3038, -0.1287, -0.6036],\n",
            "         [ 0.2261, -0.2332,  0.3425],\n",
            "         [ 0.1882, -0.2740,  0.3432],\n",
            "         [-0.3648, -0.1497,  0.3491]]], device='cuda:0') tensor([ 0.1890, -0.0776, -0.3660, -0.3110, -0.8766, -0.1337], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.80615234375\n",
            "act tensor([[10, 10, 11,  7,  2,  1]], device='cuda:0')\n",
            "tcost icost 0.14453125 0.0\n",
            "tcost icost 0.2239990234375 0.0\n",
            "tcost icost 0.2384033203125 0.0\n",
            "tcost icost 0.256103515625 0.0\n",
            "tcost icost 0.003597259521484375 0.0\n",
            "tcost icost 0.01229095458984375 0.0\n",
            "loss tensor([[0.7354]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.2560, -0.7000, -0.6667],\n",
            "         [-0.4568, -0.7936, -0.3549],\n",
            "         [ 0.2495, -0.1696, -0.6833],\n",
            "         [ 0.2465, -0.2138,  0.3447],\n",
            "         [ 0.1867, -0.2775,  0.3402],\n",
            "         [-0.3636, -0.1478,  0.3504]]], device='cuda:0') tensor([ 0.3114, -0.1853, -0.2769, -0.3343, -0.8725, -0.1356], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7353515625\n",
            "act tensor([[10, 10, 11,  7,  2,  1]], device='cuda:0')\n",
            "tcost icost 0.1578369140625 0.0\n",
            "tcost icost 0.128173828125 0.0\n",
            "tcost icost 0.20654296875 0.0\n",
            "tcost icost 0.15234375 0.0\n",
            "tcost icost 0.003925323486328125 0.0\n",
            "tcost icost 0.00543975830078125 0.0\n",
            "loss tensor([[0.5576]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.2296, -0.6693, -0.6869],\n",
            "         [-0.4118, -0.7740, -0.4810],\n",
            "         [ 0.3067, -0.1063, -0.6592],\n",
            "         [ 0.2394, -0.2334,  0.3271],\n",
            "         [ 0.1851, -0.2810,  0.3375],\n",
            "         [-0.3607, -0.1431,  0.3536]]], device='cuda:0') tensor([ 0.1061,  0.0289, -0.3771, -0.3107, -0.8691, -0.1408], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.5576171875\n",
            "act tensor([[10, 10, 11,  7,  2,  1]], device='cuda:0')\n",
            "tcost icost 0.11407470703125 0.0\n",
            "tcost icost 0.178466796875 0.0\n",
            "tcost icost 0.03765869140625 0.0\n",
            "tcost icost 0.07305908203125 0.0\n",
            "tcost icost 0.0055084228515625 0.0\n",
            "tcost icost 0.01537322998046875 0.0\n",
            "loss tensor([[0.3711]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.2575, -0.7010, -0.6650],\n",
            "         [-0.4247, -0.7689, -0.4342],\n",
            "         [ 0.2623, -0.1443, -0.7078],\n",
            "         [ 0.2533, -0.2156,  0.3366],\n",
            "         [ 0.1837, -0.2845,  0.3344],\n",
            "         [-0.3599, -0.1416,  0.3546]]], device='cuda:0') tensor([ 0.3079, -0.1147, -0.2797, -0.3219, -0.8647, -0.1419], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.37109375\n",
            "act tensor([[10, 10, 11,  7,  2,  1]], device='cuda:0')\n",
            "tcost icost 0.15771484375 0.0\n",
            "tcost icost 0.162353515625 0.0\n",
            "tcost icost 0.29296875 0.0\n",
            "tcost icost 0.211181640625 0.0\n",
            "tcost icost 0.00415802001953125 0.0\n",
            "tcost icost 0.00919342041015625 0.0\n",
            "loss tensor([[0.7031]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.2415, -0.6785, -0.6807],\n",
            "         [-0.3755, -0.7556, -0.5367],\n",
            "         [ 0.3166, -0.0795, -0.6920],\n",
            "         [ 0.2431, -0.2476,  0.3045],\n",
            "         [ 0.1819, -0.2879,  0.3314],\n",
            "         [-0.3588, -0.1398,  0.3558]]], device='cuda:0') tensor([ 0.1663,  0.0408, -0.3821, -0.2858, -0.8606, -0.1438], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.703125\n",
            "act tensor([[10, 10, 11,  7,  2,  1]], device='cuda:0')\n",
            "tcost icost 0.1395263671875 0.0\n",
            "tcost icost 0.213623046875 0.0\n",
            "tcost icost 0.07421875 0.0\n",
            "tcost icost 0.090576171875 0.0\n",
            "tcost icost 0.0035419464111328125 0.0\n",
            "tcost icost 0.014495849609375 0.0\n",
            "loss tensor([[0.4690]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.2603, -0.7033, -0.6615],\n",
            "         [-0.3898, -0.7500, -0.4998],\n",
            "         [ 0.2592, -0.1282, -0.7510],\n",
            "         [ 0.2632, -0.2220,  0.3188],\n",
            "         [ 0.1806, -0.2910,  0.3285],\n",
            "         [-0.3582, -0.1389,  0.3565]]], device='cuda:0') tensor([ 0.3612, -0.1338, -0.2585, -0.3085, -0.8567, -0.1445], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.468994140625\n",
            "search tensor([[[ 0.1806, -0.2910,  0.3285],\n",
            "         [-0.3582, -0.1389,  0.3565],\n",
            "         [ 0.4276, -0.1087,  0.3294],\n",
            "         [ 0.0900, -0.3554,  0.0870],\n",
            "         [-0.2590, -0.2552,  0.0150],\n",
            "         [ 0.2576, -0.1107, -0.4880]]], device='cuda:0') tensor([-0.8567, -0.1445,  0.6603,  1.1438, -0.0207,  0.9669], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[ 2,  1,  7,  4,  4, 11]], device='cuda:0')\n",
            "tcost icost -0.006214141845703125 0.0\n",
            "tcost icost -0.052398681640625 0.0\n",
            "tcost icost 0.01078033447265625 0.0\n",
            "tcost icost -0.136962890625 0.0\n",
            "tcost icost -0.0003707408905029297 0.0\n",
            "tcost icost 0.0235595703125 0.0\n",
            "loss tensor([[-0.1309]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.1636, -0.3331,  0.2928],\n",
            "         [-0.3290, -0.0795,  0.3998],\n",
            "         [ 0.4193, -0.1516,  0.2868],\n",
            "         [ 0.1205, -0.3263,  0.1130],\n",
            "         [-0.2590, -0.2620,  0.0015],\n",
            "         [ 0.2530, -0.1146, -0.4964]]], device='cuda:0') tensor([-0.8143, -0.1990,  0.7000,  1.0000, -0.0109,  0.9749], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.130859375\n",
            "act tensor([[ 2,  1,  0,  4,  4, 11]], device='cuda:0')\n",
            "tcost icost -0.00426483154296875 0.0\n",
            "tcost icost -0.0360107421875 0.0\n",
            "tcost icost 0.01343536376953125 0.0\n",
            "tcost icost -0.057373046875 0.0\n",
            "tcost icost 0.01050567626953125 0.0\n",
            "tcost icost 0.052337646484375 0.0\n",
            "loss tensor([[-0.0298]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.1461, -0.3736,  0.2550],\n",
            "         [-0.3057, -0.0252,  0.4401],\n",
            "         [ 0.4067, -0.1981,  0.2376],\n",
            "         [ 0.1571, -0.2899,  0.1419],\n",
            "         [-0.2599, -0.2729, -0.0190],\n",
            "         [ 0.2482, -0.1186, -0.5059]]], device='cuda:0') tensor([-0.7672, -0.2463,  0.7404,  0.9482,  0.0041,  0.9838], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0298309326171875\n",
            "act tensor([[ 2,  1,  0,  4,  4, 11]], device='cuda:0')\n",
            "tcost icost -0.0019140243530273438 0.0\n",
            "tcost icost -0.0239410400390625 0.0\n",
            "tcost icost 0.013092041015625 0.0\n",
            "tcost icost 0.0173797607421875 0.0\n",
            "tcost icost 0.01739501953125 0.0\n",
            "tcost icost 0.0657958984375 0.0\n",
            "loss tensor([[0.0501]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.1332, -0.4021,  0.2255],\n",
            "         [-0.2966,  0.0016,  0.4586],\n",
            "         [ 0.3975, -0.2295,  0.1995],\n",
            "         [ 0.1876, -0.2575,  0.1641],\n",
            "         [-0.2612, -0.2876, -0.0455],\n",
            "         [ 0.2466, -0.1196, -0.5123]]], device='cuda:0') tensor([-0.7303, -0.2748,  0.7602,  0.9025,  0.0245,  0.9877], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.050079345703125\n",
            "act tensor([[ 2,  1,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 7.456541061401367e-05 0.0\n",
            "tcost icost -0.0162200927734375 0.0\n",
            "tcost icost 0.0120697021484375 0.0\n",
            "tcost icost 0.0369873046875 0.0\n",
            "tcost icost 0.00791168212890625 0.0\n",
            "tcost icost 0.06805419921875 0.0\n",
            "loss tensor([[0.0676]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.1193, -0.4291,  0.1956],\n",
            "         [-0.2894,  0.0228,  0.4720],\n",
            "         [ 0.3879, -0.2567,  0.1636],\n",
            "         [ 0.2154, -0.2271,  0.1825],\n",
            "         [-0.2612, -0.2997, -0.0695],\n",
            "         [ 0.2446, -0.1208, -0.5194]]], device='cuda:0') tensor([-0.6942, -0.3011,  0.7737,  0.8594,  0.0424,  0.9924], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.067626953125\n",
            "act tensor([[ 2,  1,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.0020160675048828125 0.0\n",
            "tcost icost -0.0095062255859375 0.0\n",
            "tcost icost 0.01256561279296875 0.0\n",
            "tcost icost 0.06781005859375 0.0\n",
            "tcost icost 0.01446533203125 0.0\n",
            "tcost icost 0.07574462890625 0.0\n",
            "loss tensor([[0.1073]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.1072, -0.4510,  0.1695],\n",
            "         [-0.2869,  0.0336,  0.4778],\n",
            "         [ 0.3795, -0.2788,  0.1317],\n",
            "         [ 0.2389, -0.2004,  0.1968],\n",
            "         [-0.2611, -0.3115, -0.0927],\n",
            "         [ 0.2439, -0.1207, -0.5245]]], device='cuda:0') tensor([-0.6622, -0.3192,  0.7812,  0.8199,  0.0595,  0.9945], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1072998046875\n",
            "act tensor([[ 4,  1,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.00197601318359375 0.0\n",
            "tcost icost -0.00902557373046875 0.0\n",
            "tcost icost 0.01068115234375 0.0\n",
            "tcost icost 0.079833984375 0.0\n",
            "tcost icost 0.0235748291015625 0.0\n",
            "tcost icost 0.0936279296875 0.0\n",
            "loss tensor([[0.1315]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.1025, -0.4632,  0.1537],\n",
            "         [-0.2946,  0.0266,  0.4703],\n",
            "         [ 0.3724, -0.2966,  0.1013],\n",
            "         [ 0.2634, -0.1723,  0.2096],\n",
            "         [-0.2612, -0.3265, -0.1225],\n",
            "         [ 0.2428, -0.1207, -0.5313]]], device='cuda:0') tensor([-0.6412, -0.3265,  0.7793,  0.7779,  0.0816,  0.9972], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1314697265625\n",
            "act tensor([[ 4,  1,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.003070831298828125 0.0\n",
            "tcost icost -0.006412506103515625 0.0\n",
            "tcost icost 0.012451171875 0.0\n",
            "tcost icost 0.0916748046875 0.0\n",
            "tcost icost 0.03363037109375 0.0\n",
            "tcost icost 0.10076904296875 0.0\n",
            "loss tensor([[0.1558]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.0983, -0.4718,  0.1421],\n",
            "         [-0.3059,  0.0095,  0.4551],\n",
            "         [ 0.3725, -0.3042,  0.0818],\n",
            "         [ 0.2797, -0.1544,  0.2153],\n",
            "         [-0.2618, -0.3411, -0.1489],\n",
            "         [ 0.2450, -0.1177, -0.5337]]], device='cuda:0') tensor([-0.6264, -0.3276,  0.7687,  0.7493,  0.1015,  0.9926], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15576171875\n",
            "act tensor([[ 4,  1,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.0038242340087890625 0.0\n",
            "tcost icost -0.005023956298828125 0.0\n",
            "tcost icost 0.014739990234375 0.0\n",
            "tcost icost 0.097900390625 0.0\n",
            "tcost icost 0.0413818359375 0.0\n",
            "tcost icost 0.10400390625 0.0\n",
            "loss tensor([[0.1711]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.0985, -0.4757,  0.1361],\n",
            "         [-0.3218, -0.0152,  0.4344],\n",
            "         [ 0.3733, -0.3091,  0.0650],\n",
            "         [ 0.2949, -0.1366,  0.2211],\n",
            "         [-0.2627, -0.3573, -0.1775],\n",
            "         [ 0.2475, -0.1142, -0.5362]]], device='cuda:0') tensor([-0.6175, -0.3237,  0.7538,  0.7208,  0.1236,  0.9872], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.171142578125\n",
            "act tensor([[ 4,  1,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.00426483154296875 0.0\n",
            "tcost icost -0.0048065185546875 0.0\n",
            "tcost icost 0.0172271728515625 0.0\n",
            "tcost icost 0.10382080078125 0.0\n",
            "tcost icost 0.051513671875 0.0\n",
            "tcost icost 0.1063232421875 0.0\n",
            "loss tensor([[0.1863]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.0992, -0.4773,  0.1333],\n",
            "         [-0.3395, -0.0452,  0.4096],\n",
            "         [ 0.3794, -0.3078,  0.0559],\n",
            "         [ 0.3059, -0.1251,  0.2228],\n",
            "         [-0.2635, -0.3736, -0.2041],\n",
            "         [ 0.2529, -0.1081, -0.5349]]], device='cuda:0') tensor([-0.6131, -0.3172,  0.7340,  0.7014,  0.1453,  0.9767], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.186279296875\n",
            "act tensor([[ 4,  1,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.004512786865234375 0.0\n",
            "tcost icost -0.00536346435546875 0.0\n",
            "tcost icost 0.0199127197265625 0.0\n",
            "tcost icost 0.10845947265625 0.0\n",
            "tcost icost 0.06243896484375 0.0\n",
            "tcost icost 0.11181640625 0.0\n",
            "loss tensor([[0.2018]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.1087, -0.4718,  0.1389],\n",
            "         [-0.3646, -0.0854,  0.3767],\n",
            "         [ 0.3856, -0.3067,  0.0439],\n",
            "         [ 0.3201, -0.1073,  0.2293],\n",
            "         [-0.2655, -0.3956, -0.2375],\n",
            "         [ 0.2584, -0.1019, -0.5336]]], device='cuda:0') tensor([-0.6189, -0.3053,  0.7107,  0.6746,  0.1753,  0.9660], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2017822265625\n",
            "act tensor([[ 4,  1,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.004177093505859375 0.0\n",
            "tcost icost -0.00742340087890625 0.0\n",
            "tcost icost 0.022247314453125 0.0\n",
            "tcost icost 0.11358642578125 0.0\n",
            "tcost icost 0.08465576171875 0.0\n",
            "tcost icost 0.1195068359375 0.0\n",
            "loss tensor([[0.2244]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.1089, -0.4759,  0.1321],\n",
            "         [-0.3814, -0.1107,  0.3547],\n",
            "         [ 0.3878, -0.3115,  0.0257],\n",
            "         [ 0.3352, -0.0894,  0.2354],\n",
            "         [-0.2667, -0.4167, -0.2692],\n",
            "         [ 0.2639, -0.0956, -0.5321]]], device='cuda:0') tensor([-0.6107, -0.3027,  0.6965,  0.6449,  0.2010,  0.9550], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.224365234375\n",
            "act tensor([[ 4,  1,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.0046234130859375 0.0\n",
            "tcost icost -0.00710296630859375 0.0\n",
            "tcost icost 0.024505615234375 0.0\n",
            "tcost icost 0.119873046875 0.0\n",
            "tcost icost 0.10174560546875 0.0\n",
            "tcost icost 0.122802734375 0.0\n",
            "loss tensor([[0.2448]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.1117, -0.4777,  0.1281],\n",
            "         [-0.3998, -0.1388,  0.3306],\n",
            "         [ 0.3891, -0.3169,  0.0053],\n",
            "         [ 0.3512, -0.0693,  0.2428],\n",
            "         [-0.2673, -0.4404, -0.3039],\n",
            "         [ 0.2693, -0.0893, -0.5308]]], device='cuda:0') tensor([-0.6059, -0.2990,  0.6813,  0.6114,  0.2286,  0.9439], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2447509765625\n",
            "act tensor([[ 4,  1,  0,  7, 10, 11]], device='cuda:0')\n",
            "tcost icost 0.00484466552734375 0.0\n",
            "tcost icost -0.00724029541015625 0.0\n",
            "tcost icost 0.0266876220703125 0.0\n",
            "tcost icost 0.1268310546875 0.0\n",
            "tcost icost 0.0909423828125 0.0\n",
            "tcost icost 0.1075439453125 0.0\n",
            "loss tensor([[0.2356]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.1162, -0.4774,  0.1268],\n",
            "         [-0.4199, -0.1714,  0.3031],\n",
            "         [ 0.3929, -0.3189, -0.0114],\n",
            "         [ 0.3666, -0.0512,  0.2486],\n",
            "         [-0.2668, -0.4648, -0.3381],\n",
            "         [ 0.2746, -0.0831, -0.5297]]], device='cuda:0') tensor([-0.6041, -0.2925,  0.6621,  0.5805,  0.2567,  0.9329], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.235595703125\n",
            "act tensor([[ 4,  1,  0,  7, 10, 11]], device='cuda:0')\n",
            "tcost icost 0.004978179931640625 0.0\n",
            "tcost icost -0.00806427001953125 0.0\n",
            "tcost icost 0.029083251953125 0.0\n",
            "tcost icost 0.133544921875 0.0\n",
            "tcost icost 0.11151123046875 0.0\n",
            "tcost icost 0.11199951171875 0.0\n",
            "loss tensor([[0.2581]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.1130, -0.4856,  0.1146],\n",
            "         [-0.4310, -0.1868,  0.2892],\n",
            "         [ 0.3926, -0.3271, -0.0352],\n",
            "         [ 0.3830, -0.0301,  0.2574],\n",
            "         [-0.2640, -0.4901, -0.3733],\n",
            "         [ 0.2803, -0.0762, -0.5286]]], device='cuda:0') tensor([-0.5898, -0.2959,  0.6544,  0.5452,  0.2825,  0.9203], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.258056640625\n",
            "act tensor([[ 4,  1,  0,  7, 10, 11]], device='cuda:0')\n",
            "tcost icost 0.0057220458984375 0.0\n",
            "tcost icost -0.006565093994140625 0.0\n",
            "tcost icost 0.03118896484375 0.0\n",
            "tcost icost 0.1400146484375 0.0\n",
            "tcost icost 0.130615234375 0.0\n",
            "tcost icost 0.11102294921875 0.0\n",
            "loss tensor([[0.2783]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.1133, -0.4884,  0.1098],\n",
            "         [-0.4483, -0.2171,  0.2619],\n",
            "         [ 0.3976, -0.3277, -0.0515],\n",
            "         [ 0.3966, -0.0168,  0.2582],\n",
            "         [-0.2583, -0.5160, -0.4080],\n",
            "         [ 0.2871, -0.0683, -0.5261]]], device='cuda:0') tensor([-0.5839, -0.2904,  0.6335,  0.5166,  0.3072,  0.9057], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2783203125\n",
            "act tensor([[ 4,  1,  0,  7, 10, 11]], device='cuda:0')\n",
            "tcost icost 0.0060577392578125 0.0\n",
            "tcost icost -0.006969451904296875 0.0\n",
            "tcost icost 0.034149169921875 0.0\n",
            "tcost icost 0.147705078125 0.0\n",
            "tcost icost 0.1495361328125 0.0\n",
            "tcost icost 0.115234375 0.0\n",
            "loss tensor([[0.3013]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.1155, -0.4897,  0.1071],\n",
            "         [-0.4670, -0.2486,  0.2320],\n",
            "         [ 0.4014, -0.3292, -0.0707],\n",
            "         [ 0.4119, -0.0021,  0.2587],\n",
            "         [-0.2478, -0.5444, -0.4446],\n",
            "         [ 0.2939, -0.0602, -0.5238]]], device='cuda:0') tensor([-0.5811, -0.2858,  0.6107,  0.4844,  0.3343,  0.8909], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.30126953125\n",
            "act tensor([[ 4,  1,  0,  7, 10, 11]], device='cuda:0')\n",
            "tcost icost 0.00621795654296875 0.0\n",
            "tcost icost -0.007534027099609375 0.0\n",
            "tcost icost 0.036773681640625 0.0\n",
            "tcost icost 0.1558837890625 0.0\n",
            "tcost icost 0.16943359375 0.0\n",
            "tcost icost 0.12042236328125 0.0\n",
            "loss tensor([[0.3250]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.1059, -0.4998,  0.0940],\n",
            "         [-0.4795, -0.2703,  0.2074],\n",
            "         [ 0.4058, -0.3295, -0.0868],\n",
            "         [ 0.4255,  0.0041,  0.2512],\n",
            "         [-0.2338, -0.5643, -0.4697],\n",
            "         [ 0.3004, -0.0519, -0.5224]]], device='cuda:0') tensor([-0.5660, -0.2872,  0.5902,  0.4619,  0.3512,  0.8758], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.324951171875\n",
            "act tensor([[ 4,  1,  0,  7, 10, 11]], device='cuda:0')\n",
            "tcost icost 0.0070953369140625 0.0\n",
            "tcost icost -0.006160736083984375 0.0\n",
            "tcost icost 0.04046630859375 0.0\n",
            "tcost icost 0.162353515625 0.0\n",
            "tcost icost 0.176513671875 0.0\n",
            "tcost icost 0.12396240234375 0.0\n",
            "loss tensor([[0.3418]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.0898, -0.5143,  0.0753],\n",
            "         [-0.4886, -0.2873,  0.1857],\n",
            "         [ 0.4102, -0.3293, -0.1010],\n",
            "         [ 0.4381,  0.0046,  0.2381],\n",
            "         [-0.2189, -0.5772, -0.4860],\n",
            "         [ 0.3066, -0.0437, -0.5215]]], device='cuda:0') tensor([-0.5439, -0.2919,  0.5702,  0.4444,  0.3610,  0.8606], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.341796875\n",
            "act tensor([[ 4,  1,  0,  7, 10, 11]], device='cuda:0')\n",
            "tcost icost 0.00843048095703125 0.0\n",
            "tcost icost -0.00324249267578125 0.0\n",
            "tcost icost 0.044708251953125 0.0\n",
            "tcost icost 0.1676025390625 0.0\n",
            "tcost icost 0.1812744140625 0.0\n",
            "tcost icost 0.1268310546875 0.0\n",
            "loss tensor([[0.3579]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.0774, -0.5247,  0.0611],\n",
            "         [-0.5021, -0.3127,  0.1554],\n",
            "         [ 0.4179, -0.3250, -0.1111],\n",
            "         [ 0.4504,  0.0029,  0.2225],\n",
            "         [-0.2049, -0.5874, -0.4963],\n",
            "         [ 0.3127, -0.0355, -0.5205]]], device='cuda:0') tensor([-0.5260, -0.2882,  0.5428,  0.4292,  0.3716,  0.8452], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.35791015625\n",
            "act tensor([[ 4, 13,  0,  7, 10, 11]], device='cuda:0')\n",
            "tcost icost 0.00943756103515625 0.0\n",
            "tcost icost 0.0023250579833984375 0.0\n",
            "tcost icost 0.045562744140625 0.0\n",
            "tcost icost 0.1527099609375 0.0\n",
            "tcost icost 0.1766357421875 0.0\n",
            "tcost icost 0.12017822265625 0.0\n",
            "loss tensor([[0.3467]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.0863, -0.5156,  0.0753],\n",
            "         [-0.5308, -0.3596,  0.1063],\n",
            "         [ 0.4328, -0.3117, -0.1112],\n",
            "         [ 0.4599,  0.0027,  0.2097],\n",
            "         [-0.1955, -0.5967, -0.5010],\n",
            "         [ 0.3193, -0.0271, -0.5186]]], device='cuda:0') tensor([-0.5376, -0.2631,  0.5000,  0.4163,  0.3914,  0.8292], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.3466796875\n",
            "search tensor([[[-0.1955, -0.5967, -0.5010],\n",
            "         [ 0.3193, -0.0271, -0.5186],\n",
            "         [ 0.4677, -0.2170, -0.1476],\n",
            "         [ 0.2608, -0.1509,  0.1012],\n",
            "         [ 0.0453, -0.5344, -0.4560],\n",
            "         [ 0.3489, -0.4581,  0.3314]]], device='cuda:0') tensor([ 0.3914,  0.8292,  0.1468, -0.6703, -0.4534, -0.3300], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[10, 11,  0,  7, 10,  2]], device='cuda:0')\n",
            "tcost icost 0.274658203125 0.0\n",
            "tcost icost -0.010162353515625 0.0\n",
            "tcost icost -0.0185394287109375 0.0\n",
            "tcost icost 0.007587432861328125 0.0\n",
            "tcost icost 0.0107421875 0.0\n",
            "tcost icost 0.17236328125 0.0\n",
            "loss tensor([[0.3650]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.1806, -0.5959, -0.5123],\n",
            "         [ 0.3395,  0.0063, -0.5021],\n",
            "         [ 0.4422, -0.2326, -0.1826],\n",
            "         [ 0.2621, -0.1642,  0.0636],\n",
            "         [ 0.0592, -0.5239, -0.4603],\n",
            "         [ 0.3325, -0.4888,  0.2960]]], device='cuda:0') tensor([ 0.3887,  0.7538,  0.1451, -0.6670, -0.4623, -0.2737], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.364990234375\n",
            "act tensor([[10, 11,  0,  7, 10,  2]], device='cuda:0')\n",
            "tcost icost 0.277099609375 0.0\n",
            "tcost icost 0.0201263427734375 0.0\n",
            "tcost icost -0.01192474365234375 0.0\n",
            "tcost icost 0.0089569091796875 0.0\n",
            "tcost icost 0.01201629638671875 0.0\n",
            "tcost icost 0.25439453125 0.0\n",
            "loss tensor([[0.4502]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.1743, -0.5968, -0.5204],\n",
            "         [ 0.3609,  0.0397, -0.4843],\n",
            "         [ 0.4176, -0.2449, -0.2139],\n",
            "         [ 0.2543, -0.1882,  0.0131],\n",
            "         [ 0.0721, -0.5144, -0.4678],\n",
            "         [ 0.3191, -0.5131,  0.2641]]], device='cuda:0') tensor([ 0.3866,  0.6778,  0.1383, -0.6534, -0.4741, -0.2285], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.4501953125\n",
            "act tensor([[10, 11,  0,  4, 10,  2]], device='cuda:0')\n",
            "tcost icost 0.27880859375 0.0\n",
            "tcost icost 0.054107666015625 0.0\n",
            "tcost icost -0.0012311935424804688 0.0\n",
            "tcost icost 0.00803375244140625 0.0\n",
            "tcost icost 0.0109100341796875 0.0\n",
            "tcost icost 0.2457275390625 0.0\n",
            "loss tensor([[0.4849]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.1753, -0.6006, -0.5270],\n",
            "         [ 0.3879,  0.0775, -0.4594],\n",
            "         [ 0.4007, -0.2530, -0.2362],\n",
            "         [ 0.2432, -0.2112, -0.0306],\n",
            "         [ 0.0790, -0.5092, -0.4760],\n",
            "         [ 0.3119, -0.5267,  0.2434]]], device='cuda:0') tensor([ 0.3912,  0.5983,  0.1380, -0.6313, -0.4773, -0.2003], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.48486328125\n",
            "act tensor([[10, 11,  0,  4, 10,  2]], device='cuda:0')\n",
            "tcost icost 0.274658203125 0.0\n",
            "tcost icost 0.09912109375 0.0\n",
            "tcost icost 0.0213470458984375 0.0\n",
            "tcost icost 0.010009765625 0.0\n",
            "tcost icost 0.01276397705078125 0.0\n",
            "tcost icost 0.251953125 0.0\n",
            "loss tensor([[0.5454]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.1705, -0.5993, -0.5364],\n",
            "         [ 0.3936,  0.0998, -0.4525],\n",
            "         [ 0.3919, -0.2518, -0.2553],\n",
            "         [ 0.2319, -0.2278, -0.0754],\n",
            "         [ 0.0483, -0.5265, -0.5124],\n",
            "         [ 0.3173, -0.5278,  0.2299]]], device='cuda:0') tensor([ 0.3794,  0.5286,  0.1180, -0.6122, -0.4377, -0.1869], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.54541015625\n",
            "act tensor([[10, 11, 11,  4, 10,  2]], device='cuda:0')\n",
            "tcost icost 0.285888671875 0.0\n",
            "tcost icost 0.129150390625 0.0\n",
            "tcost icost 0.02911376953125 0.0\n",
            "tcost icost 0.01099395751953125 0.0\n",
            "tcost icost 0.0140533447265625 0.0\n",
            "tcost icost 0.2235107421875 0.0\n",
            "loss tensor([[0.5752]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.1644, -0.5978, -0.5480],\n",
            "         [ 0.4047,  0.1269, -0.4389],\n",
            "         [ 0.3774, -0.2572, -0.2830],\n",
            "         [ 0.2218, -0.2404, -0.1146],\n",
            "         [ 0.0221, -0.5402, -0.5395],\n",
            "         [ 0.3198, -0.5308,  0.2157]]], device='cuda:0') tensor([ 0.3613,  0.4526,  0.1085, -0.6007, -0.4063, -0.1742], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.5751953125\n",
            "act tensor([[10, 11, 11,  4, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.3046875 0.0\n",
            "tcost icost 0.156494140625 0.0\n",
            "tcost icost 0.05535888671875 0.0\n",
            "tcost icost 0.01219940185546875 0.0\n",
            "tcost icost 0.01580810546875 0.0\n",
            "tcost icost 0.2081298828125 0.0\n",
            "loss tensor([[0.6328]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.1683, -0.6035, -0.5560],\n",
            "         [ 0.4220,  0.1593, -0.4167],\n",
            "         [ 0.3616, -0.2649, -0.3110],\n",
            "         [ 0.2104, -0.2529, -0.1508],\n",
            "         [-0.0009, -0.5521, -0.5603],\n",
            "         [ 0.3239, -0.5319,  0.2039]]], device='cuda:0') tensor([ 0.3679,  0.3690,  0.1043, -0.5893, -0.3801, -0.1671], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6328125\n",
            "act tensor([[10, 11, 11,  4, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.297607421875 0.0\n",
            "tcost icost 0.1912841796875 0.0\n",
            "tcost icost 0.12310791015625 0.0\n",
            "tcost icost 0.0138092041015625 0.0\n",
            "tcost icost 0.01727294921875 0.0\n",
            "tcost icost 0.20556640625 0.0\n",
            "loss tensor([[0.7124]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.1542, -0.5962, -0.5727],\n",
            "         [ 0.4411,  0.1942, -0.3896],\n",
            "         [ 0.3383, -0.2800, -0.3523],\n",
            "         [ 0.1997, -0.2633, -0.1844],\n",
            "         [-0.0253, -0.5645, -0.5791],\n",
            "         [ 0.3279, -0.5325,  0.1924]]], device='cuda:0') tensor([ 0.3222,  0.2767,  0.1102, -0.5792, -0.3502, -0.1606], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.71240234375\n",
            "act tensor([[10, 11, 11,  4, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.334228515625 0.0\n",
            "tcost icost 0.210693359375 0.0\n",
            "tcost icost 0.1788330078125 0.0\n",
            "tcost icost 0.01422119140625 0.0\n",
            "tcost icost 0.01788330078125 0.0\n",
            "tcost icost 0.2052001953125 0.0\n",
            "loss tensor([[0.8120]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.1318, -0.5822, -0.5901],\n",
            "         [ 0.4353,  0.1975, -0.4010],\n",
            "         [ 0.3382, -0.2732, -0.3674],\n",
            "         [ 0.1873, -0.2731, -0.2170],\n",
            "         [-0.0595, -0.5824, -0.5968],\n",
            "         [ 0.3363, -0.5278,  0.1865]]], device='cuda:0') tensor([ 0.2791,  0.2300,  0.0784, -0.5679, -0.3086, -0.1648], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.81201171875\n",
            "act tensor([[10, 11, 11,  4, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.36328125 0.0\n",
            "tcost icost 0.2088623046875 0.0\n",
            "tcost icost 0.1727294921875 0.0\n",
            "tcost icost 0.016876220703125 0.0\n",
            "tcost icost 0.01971435546875 0.0\n",
            "tcost icost 0.2449951171875 0.0\n",
            "loss tensor([[0.8608]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.1600, -0.6084, -0.5973],\n",
            "         [ 0.4542,  0.2326, -0.3725],\n",
            "         [ 0.3176, -0.2827, -0.4010],\n",
            "         [ 0.1710, -0.2883, -0.2596],\n",
            "         [-0.0767, -0.5900, -0.6085],\n",
            "         [ 0.3393, -0.5303,  0.1714]]], device='cuda:0') tensor([ 0.3236,  0.1335,  0.0736, -0.5521, -0.2832, -0.1549], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.86083984375\n",
            "act tensor([[10, 11, 11,  4, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.3330078125 0.0\n",
            "tcost icost 0.2435302734375 0.0\n",
            "tcost icost 0.2548828125 0.0\n",
            "tcost icost 0.0174407958984375 0.0\n",
            "tcost icost 0.0208740234375 0.0\n",
            "tcost icost 0.193115234375 0.0\n",
            "loss tensor([[0.8994]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.1063, -0.5711, -0.6260],\n",
            "         [ 0.4268,  0.2001, -0.4309],\n",
            "         [ 0.3398, -0.2569, -0.4018],\n",
            "         [ 0.1568, -0.2983, -0.2892],\n",
            "         [-0.1057, -0.6051, -0.6169],\n",
            "         [ 0.3496, -0.5232,  0.1679]]], device='cuda:0') tensor([ 0.2156,  0.1424,  0.0133, -0.5407, -0.2409, -0.1657], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8994140625\n",
            "act tensor([[10, 11, 11,  4, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.3935546875 0.0\n",
            "tcost icost 0.205810546875 0.0\n",
            "tcost icost 0.168701171875 0.0\n",
            "tcost icost 0.0225982666015625 0.0\n",
            "tcost icost 0.0260467529296875 0.0\n",
            "tcost icost 0.2685546875 0.0\n",
            "loss tensor([[0.9077]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.1869, -0.6370, -0.6272],\n",
            "         [ 0.4716,  0.2601, -0.3707],\n",
            "         [ 0.3128, -0.2738, -0.4349],\n",
            "         [ 0.1320, -0.3234, -0.3335],\n",
            "         [-0.0978, -0.6000, -0.6243],\n",
            "         [ 0.3453, -0.5322,  0.1496]]], device='cuda:0') tensor([ 0.3396,  0.0161,  0.0302, -0.5144, -0.2524, -0.1523], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.90771484375\n",
            "act tensor([[10, 11, 11,  4, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.321044921875 0.0\n",
            "tcost icost 0.22216796875 0.0\n",
            "tcost icost 0.292724609375 0.0\n",
            "tcost icost 0.0181732177734375 0.0\n",
            "tcost icost 0.0246429443359375 0.0\n",
            "tcost icost 0.151123046875 0.0\n",
            "loss tensor([[0.8765]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.1248, -0.5897, -0.6634],\n",
            "         [ 0.4172,  0.1855, -0.4891],\n",
            "         [ 0.3558, -0.2306, -0.4215],\n",
            "         [ 0.1198, -0.3322, -0.3549],\n",
            "         [-0.1249, -0.6153, -0.6276],\n",
            "         [ 0.3604, -0.5197,  0.1535]]], device='cuda:0') tensor([ 0.1630,  0.1038, -0.0547, -0.4987, -0.2060, -0.1733], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.87646484375\n",
            "act tensor([[10, 11, 11,  4, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.39794921875 0.0\n",
            "tcost icost 0.180908203125 0.0\n",
            "tcost icost 0.12158203125 0.0\n",
            "tcost icost 0.03021240234375 0.0\n",
            "tcost icost 0.0498046875 0.0\n",
            "tcost icost 0.25048828125 0.0\n",
            "loss tensor([[0.8618]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.2366, -0.6807, -0.6451],\n",
            "         [ 0.4744,  0.2619, -0.4209],\n",
            "         [ 0.3311, -0.2415, -0.4579],\n",
            "         [ 0.0845, -0.3683, -0.4128],\n",
            "         [-0.1118, -0.6073, -0.6358],\n",
            "         [ 0.3538, -0.5310,  0.1344]]], device='cuda:0') tensor([ 0.3905, -0.0623, -0.0407, -0.4504, -0.2207, -0.1556], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.86181640625\n",
            "act tensor([[10, 11, 11, 10, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.275146484375 0.0\n",
            "tcost icost 0.158935546875 0.0\n",
            "tcost icost 0.25439453125 0.0\n",
            "tcost icost 0.0197601318359375 0.0\n",
            "tcost icost 0.02947998046875 0.0\n",
            "tcost icost 0.124755859375 0.0\n",
            "loss tensor([[0.7319]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.1927, -0.6422, -0.6753],\n",
            "         [ 0.3868,  0.1426, -0.5945],\n",
            "         [ 0.3921, -0.1785, -0.4271],\n",
            "         [ 0.0726, -0.3776, -0.4298],\n",
            "         [-0.1323, -0.6195, -0.6370],\n",
            "         [ 0.3668, -0.5201,  0.1384]]], device='cuda:0') tensor([ 0.1421,  0.1033, -0.1580, -0.4336, -0.1797, -0.1735], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.73193359375\n",
            "act tensor([[10, 11, 11, 10, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.392822265625 0.0\n",
            "tcost icost 0.156982421875 0.0\n",
            "tcost icost 0.055511474609375 0.0\n",
            "tcost icost 0.03216552734375 0.0\n",
            "tcost icost 0.0960693359375 0.0\n",
            "tcost icost 0.179443359375 0.0\n",
            "loss tensor([[0.7715]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.2491, -0.6935, -0.6524],\n",
            "         [ 0.4415,  0.2153, -0.5411],\n",
            "         [ 0.3640, -0.1912, -0.4713],\n",
            "         [ 0.0417, -0.4102, -0.4813],\n",
            "         [-0.1210, -0.6116, -0.6458],\n",
            "         [ 0.3572, -0.5332,  0.1191]]], device='cuda:0') tensor([ 0.3669, -0.0571, -0.1416, -0.3918, -0.1921, -0.1509], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.771484375\n",
            "act tensor([[10, 11, 11, 10, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.298583984375 0.0\n",
            "tcost icost 0.1763916015625 0.0\n",
            "tcost icost 0.34033203125 0.0\n",
            "tcost icost 0.06744384765625 0.0\n",
            "tcost icost 0.05487060546875 0.0\n",
            "tcost icost 0.2344970703125 0.0\n",
            "loss tensor([[0.9570]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.2165, -0.6597, -0.6796],\n",
            "         [ 0.3272,  0.0907, -0.7094],\n",
            "         [ 0.4312, -0.1152, -0.4459],\n",
            "         [ 0.0088, -0.4408, -0.5164],\n",
            "         [-0.1443, -0.6265, -0.6467],\n",
            "         [ 0.3715, -0.5210,  0.1240]]], device='cuda:0') tensor([ 0.1143,  0.0843, -0.2735, -0.3423, -0.1405, -0.1758], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.95703125\n",
            "act tensor([[10, 11, 11, 10, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.378662109375 0.0\n",
            "tcost icost 0.1380615234375 0.0\n",
            "tcost icost 0.01552581787109375 0.0\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.1495361328125 0.0\n",
            "tcost icost 0.1514892578125 0.0\n",
            "loss tensor([[0.7266]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.2632, -0.7056, -0.6554],\n",
            "         [ 0.3773,  0.1589, -0.6683],\n",
            "         [ 0.3956, -0.1349, -0.5042],\n",
            "         [-0.0060, -0.4584, -0.5470],\n",
            "         [-0.1377, -0.6214, -0.6540],\n",
            "         [ 0.3600, -0.5355,  0.1032]]], device='cuda:0') tensor([ 0.3913, -0.0592, -0.2395, -0.3195, -0.1462, -0.1516], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7265625\n",
            "act tensor([[10, 11, 11, 10, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.274658203125 0.0\n",
            "tcost icost 0.1595458984375 0.0\n",
            "tcost icost 0.35107421875 0.0\n",
            "tcost icost 0.158935546875 0.0\n",
            "tcost icost 0.11328125 0.0\n",
            "tcost icost 0.298095703125 0.0\n",
            "loss tensor([[1.0684]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.2300, -0.6763, -0.6807],\n",
            "         [ 0.2608,  0.0611, -0.7639],\n",
            "         [ 0.4177, -0.0891, -0.5321],\n",
            "         [-0.0129, -0.4637, -0.5705],\n",
            "         [-0.1659, -0.6401, -0.6544],\n",
            "         [ 0.3686, -0.5305,  0.0959]]], device='cuda:0') tensor([ 0.1468,  0.0423, -0.3323, -0.3127, -0.0665, -0.1643], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.068359375\n",
            "act tensor([[10, 11, 11, 10, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.39404296875 0.0\n",
            "tcost icost 0.2001953125 0.0\n",
            "tcost icost 0.0310516357421875 0.0\n",
            "tcost icost 0.051422119140625 0.0\n",
            "tcost icost 0.1746826171875 0.0\n",
            "tcost icost 0.146240234375 0.0\n",
            "loss tensor([[0.8384]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.2549, -0.7008, -0.6662],\n",
            "         [ 0.3252,  0.1516, -0.7160],\n",
            "         [ 0.3642, -0.1208, -0.6081],\n",
            "         [-0.0146, -0.4696, -0.5890],\n",
            "         [-0.1610, -0.6367, -0.6597],\n",
            "         [ 0.3576, -0.5439,  0.0759]]], device='cuda:0') tensor([ 0.3458, -0.1285, -0.2562, -0.2983, -0.0730, -0.1418], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.83837890625\n",
            "act tensor([[10, 11, 11, 10, 10,  0]], device='cuda:0')\n",
            "tcost icost 0.31640625 0.0\n",
            "tcost icost 0.1650390625 0.0\n",
            "tcost icost 0.3427734375 0.0\n",
            "tcost icost 0.1585693359375 0.0\n",
            "tcost icost 0.1727294921875 0.0\n",
            "tcost icost 0.29150390625 0.0\n",
            "loss tensor([[1.1436]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.2161, -0.6664, -0.6964],\n",
            "         [ 0.2270,  0.0462, -0.7874],\n",
            "         [ 0.3806, -0.0757, -0.6293],\n",
            "         [-0.0265, -0.4818, -0.6091],\n",
            "         [-0.1667, -0.6407, -0.6632],\n",
            "         [ 0.3553, -0.5488,  0.0609]]], device='cuda:0') tensor([ 0.1079,  0.0064, -0.3328, -0.2936, -0.0513, -0.1395], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.1435546875\n",
            "search tensor([[[-0.1667, -0.6407, -0.6632],\n",
            "         [ 0.3553, -0.5488,  0.0609],\n",
            "         [-0.2748,  0.3903, -0.0639],\n",
            "         [-0.2607, -0.0229, -0.0101],\n",
            "         [ 0.1423,  0.3118, -0.4551],\n",
            "         [-0.3317,  0.3582, -0.4115]]], device='cuda:0') tensor([-0.0513, -0.1395,  0.2320, -0.2510, -0.6168, -0.2367], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[10,  0,  4,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.0792236328125 0.0\n",
            "tcost icost 0.09844970703125 0.0\n",
            "tcost icost 0.1510009765625 0.0\n",
            "tcost icost 0.01180267333984375 0.0\n",
            "tcost icost 0.0169830322265625 0.0\n",
            "tcost icost 0.0604248046875 0.0\n",
            "loss tensor([[0.3455]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.2035, -0.6677, -0.6509],\n",
            "         [ 0.3614, -0.5427,  0.0545],\n",
            "         [-0.2692,  0.4009, -0.0606],\n",
            "         [-0.2568, -0.0237, -0.0232],\n",
            "         [ 0.1417,  0.2938, -0.4823],\n",
            "         [-0.3302,  0.3535, -0.4197]]], device='cuda:0') tensor([ 0.0734, -0.1851,  0.1957, -0.2531, -0.5947, -0.2277], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.345458984375\n",
            "act tensor([[10,  0,  4,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1195068359375 0.0\n",
            "tcost icost 0.2481689453125 0.0\n",
            "tcost icost 0.2200927734375 0.0\n",
            "tcost icost 0.0048675537109375 0.0\n",
            "tcost icost 0.016021728515625 0.0\n",
            "tcost icost 0.02947998046875 0.0\n",
            "loss tensor([[0.5527]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.2179, -0.6775, -0.6478],\n",
            "         [ 0.3598, -0.5500,  0.0224],\n",
            "         [-0.2531,  0.4222, -0.0490],\n",
            "         [-0.2567, -0.0277, -0.0316],\n",
            "         [ 0.1409,  0.2845, -0.4950],\n",
            "         [-0.3289,  0.3535, -0.4216]]], device='cuda:0') tensor([ 0.1682, -0.2005,  0.1507, -0.2499, -0.5831, -0.2270], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.552734375\n",
            "act tensor([[10,  0,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.146728515625 0.0\n",
            "tcost icost 0.244140625 0.0\n",
            "tcost icost 0.10186767578125 0.0\n",
            "tcost icost -0.01251983642578125 0.0\n",
            "tcost icost 0.013153076171875 0.0\n",
            "tcost icost 0.01372528076171875 0.0\n",
            "loss tensor([[0.4565]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.1879, -0.6572, -0.6662],\n",
            "         [ 0.2900, -0.6228, -0.0965],\n",
            "         [-0.2258,  0.4650, -0.0104],\n",
            "         [-0.2586, -0.0331, -0.0366],\n",
            "         [ 0.1402,  0.2809, -0.4992],\n",
            "         [-0.3274,  0.3566, -0.4198]]], device='cuda:0') tensor([ 0.0603, -0.0789,  0.0854, -0.2448, -0.5797, -0.2322], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.45654296875\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.11553955078125 0.0\n",
            "tcost icost 0.23876953125 0.0\n",
            "tcost icost 0.174072265625 0.0\n",
            "tcost icost -0.00799560546875 0.0\n",
            "tcost icost 0.01226043701171875 0.0\n",
            "tcost icost 0.020294189453125 0.0\n",
            "loss tensor([[0.4854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.1900, -0.6582, -0.6687],\n",
            "         [ 0.2462, -0.6552, -0.1744],\n",
            "         [-0.2018,  0.4995,  0.0221],\n",
            "         [-0.2596, -0.0373, -0.0418],\n",
            "         [ 0.1398,  0.2773, -0.5036],\n",
            "         [-0.3266,  0.3577, -0.4195]]], device='cuda:0') tensor([ 0.0986, -0.0323,  0.0213, -0.2406, -0.5750, -0.2338], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.4853515625\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1270751953125 0.0\n",
            "tcost icost 0.2474365234375 0.0\n",
            "tcost icost 0.2320556640625 0.0\n",
            "tcost icost 0.0007662773132324219 0.0\n",
            "tcost icost 0.01361083984375 0.0\n",
            "tcost icost 0.0261383056640625 0.0\n",
            "loss tensor([[0.5625]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.1956, -0.6613, -0.6701],\n",
            "         [ 0.2086, -0.6769, -0.2436],\n",
            "         [-0.1785,  0.5292,  0.0505],\n",
            "         [-0.2603, -0.0416, -0.0480],\n",
            "         [ 0.1391,  0.2709, -0.5116],\n",
            "         [-0.3256,  0.3589, -0.4195]]], device='cuda:0') tensor([ 0.1428, -0.0020, -0.0390, -0.2367, -0.5675, -0.2357], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.5625\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1405029296875 0.0\n",
            "tcost icost 0.251708984375 0.0\n",
            "tcost icost 0.273193359375 0.0\n",
            "tcost icost 0.00806427001953125 0.0\n",
            "tcost icost 0.01519012451171875 0.0\n",
            "tcost icost 0.034515380859375 0.0\n",
            "loss tensor([[0.6245]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.1916, -0.6584, -0.6759],\n",
            "         [ 0.1544, -0.7005, -0.3202],\n",
            "         [-0.1555,  0.5563,  0.0756],\n",
            "         [-0.2606, -0.0469, -0.0575],\n",
            "         [ 0.1384,  0.2581, -0.5280],\n",
            "         [-0.3235,  0.3603, -0.4208]]], device='cuda:0') tensor([ 0.1549,  0.0361, -0.1043, -0.2317, -0.5523, -0.2373], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.62451171875\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1441650390625 0.0\n",
            "tcost icost 0.2457275390625 0.0\n",
            "tcost icost 0.28271484375 0.0\n",
            "tcost icost 0.0181427001953125 0.0\n",
            "tcost icost 0.017974853515625 0.0\n",
            "tcost icost 0.07110595703125 0.0\n",
            "loss tensor([[0.6611]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.2027, -0.6687, -0.6740],\n",
            "         [ 0.1429, -0.7006, -0.3448],\n",
            "         [-0.1441,  0.5656,  0.0753],\n",
            "         [-0.2587, -0.0490, -0.0693],\n",
            "         [ 0.1380,  0.2326, -0.5605],\n",
            "         [-0.3196,  0.3612, -0.4248]]], device='cuda:0') tensor([ 0.2237,  0.0093, -0.1451, -0.2312, -0.5224, -0.2370], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6611328125\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1513671875 0.0\n",
            "tcost icost 0.2349853515625 0.0\n",
            "tcost icost 0.3251953125 0.0\n",
            "tcost icost 0.0186004638671875 0.0\n",
            "tcost icost 0.0187225341796875 0.0\n",
            "tcost icost 0.05694580078125 0.0\n",
            "loss tensor([[0.6855]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.1594, -0.6381, -0.6987],\n",
            "         [ 0.0019, -0.7433, -0.4747],\n",
            "         [-0.1147,  0.6004,  0.1129],\n",
            "         [-0.2591, -0.0542, -0.0806],\n",
            "         [ 0.1381,  0.2070, -0.5904],\n",
            "         [-0.3151,  0.3651, -0.4263]]], device='cuda:0') tensor([ 0.0905,  0.1345, -0.2372, -0.2259, -0.4924, -0.2419], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.685546875\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1246337890625 0.0\n",
            "tcost icost 0.162841796875 0.0\n",
            "tcost icost 0.1370849609375 0.0\n",
            "tcost icost 0.061553955078125 0.0\n",
            "tcost icost 0.034912109375 0.0\n",
            "tcost icost 0.206787109375 0.0\n",
            "loss tensor([[0.5723]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.2421, -0.7002, -0.6710],\n",
            "         [ 0.0852, -0.7322, -0.4433],\n",
            "         [-0.1300,  0.5887,  0.0795],\n",
            "         [-0.2507, -0.0764, -0.1278],\n",
            "         [ 0.1538,  0.1906, -0.6193],\n",
            "         [-0.3162,  0.3384, -0.4568]]], device='cuda:0') tensor([ 0.3130, -0.0122, -0.2361, -0.2116, -0.4760, -0.2044], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.572265625\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.15087890625 0.0\n",
            "tcost icost 0.1676025390625 0.0\n",
            "tcost icost 0.294189453125 0.0\n",
            "tcost icost 0.036834716796875 0.0\n",
            "tcost icost 0.0233154296875 0.0\n",
            "tcost icost 0.0548095703125 0.0\n",
            "loss tensor([[0.6143]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.2060, -0.6702, -0.6927],\n",
            "         [-0.1422, -0.7693, -0.5992],\n",
            "         [-0.0871,  0.6468,  0.1636],\n",
            "         [-0.2538, -0.0935, -0.1473],\n",
            "         [ 0.1552,  0.1653, -0.6445],\n",
            "         [-0.3106,  0.3447, -0.4565]]], device='cuda:0') tensor([ 0.0874,  0.2113, -0.3766, -0.1929, -0.4440, -0.2126], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6142578125\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1236572265625 0.0\n",
            "tcost icost 0.11083984375 0.0\n",
            "tcost icost 0.02191162109375 0.0\n",
            "tcost icost 0.049468994140625 0.0\n",
            "tcost icost 0.09844970703125 0.0\n",
            "tcost icost 0.1865234375 0.0\n",
            "loss tensor([[0.4519]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.2549, -0.7053, -0.6614],\n",
            "         [-0.0906, -0.7723, -0.5987],\n",
            "         [-0.1066,  0.6265,  0.0964],\n",
            "         [-0.2317, -0.1006, -0.1910],\n",
            "         [ 0.1745,  0.1547, -0.6647],\n",
            "         [-0.3123,  0.3125, -0.4901]]], device='cuda:0') tensor([ 0.3062,  0.0554, -0.3475, -0.1918, -0.4270, -0.1689], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.451904296875\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1512451171875 0.0\n",
            "tcost icost 0.1942138671875 0.0\n",
            "tcost icost 0.403564453125 0.0\n",
            "tcost icost 0.136962890625 0.0\n",
            "tcost icost 0.038177490234375 0.0\n",
            "tcost icost 0.1888427734375 0.0\n",
            "loss tensor([[0.8892]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.2461, -0.6942, -0.6715],\n",
            "         [-0.1619, -0.7572, -0.6246],\n",
            "         [-0.0741,  0.6579,  0.1278],\n",
            "         [-0.2348, -0.1359, -0.2321],\n",
            "         [ 0.1799,  0.1081, -0.7057],\n",
            "         [-0.3026,  0.3197, -0.4942]]], device='cuda:0') tensor([ 0.2202,  0.1004, -0.4464, -0.1515, -0.3589, -0.1795], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.88916015625\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.151123046875 0.0\n",
            "tcost icost 0.2393798828125 0.0\n",
            "tcost icost 0.2005615234375 0.0\n",
            "tcost icost 0.2313232421875 0.0\n",
            "tcost icost 0.1253662109375 0.0\n",
            "tcost icost 0.29296875 0.0\n",
            "loss tensor([[0.9531]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.2698, -0.7085, -0.6521],\n",
            "         [-0.1103, -0.7500, -0.6390],\n",
            "         [-0.0965,  0.6252,  0.0079],\n",
            "         [-0.2067, -0.1657, -0.3096],\n",
            "         [ 0.1963,  0.0948, -0.7280],\n",
            "         [-0.3040,  0.2760, -0.5384]]], device='cuda:0') tensor([ 0.3707, -0.1222, -0.4101, -0.1284, -0.3199, -0.1247], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.953125\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1444091796875 0.0\n",
            "tcost icost 0.0711669921875 0.0\n",
            "tcost icost 0.1358642578125 0.0\n",
            "tcost icost 0.105712890625 0.0\n",
            "tcost icost 0.055419921875 0.0\n",
            "tcost icost 0.10418701171875 0.0\n",
            "loss tensor([[0.4937]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.2479, -0.6794, -0.6668],\n",
            "         [-0.2495, -0.7279, -0.6387],\n",
            "         [-0.0508,  0.6792,  0.0800],\n",
            "         [-0.2050, -0.1741, -0.3255],\n",
            "         [ 0.1867,  0.0613, -0.7538],\n",
            "         [-0.2941,  0.2882, -0.5370]]], device='cuda:0') tensor([ 0.0863,  0.1325, -0.5361, -0.1147, -0.2497, -0.1403], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.49365234375\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.12335205078125 0.0\n",
            "tcost icost 0.15966796875 0.0\n",
            "tcost icost 0.01611328125 0.0\n",
            "tcost icost 0.059234619140625 0.0\n",
            "tcost icost 0.168212890625 0.0\n",
            "tcost icost 0.142333984375 0.0\n",
            "loss tensor([[0.5176]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.2650, -0.7108, -0.6516],\n",
            "         [-0.2250, -0.7184, -0.6499],\n",
            "         [-0.0765,  0.6378, -0.0283],\n",
            "         [-0.1764, -0.1469, -0.3359],\n",
            "         [ 0.1952,  0.0673, -0.7559],\n",
            "         [-0.2971,  0.2564, -0.5644]]], device='cuda:0') tensor([ 0.3092, -0.0346, -0.4504, -0.1476, -0.2526, -0.0980], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.517578125\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.151123046875 0.0\n",
            "tcost icost 0.1624755859375 0.0\n",
            "tcost icost 0.387939453125 0.0\n",
            "tcost icost 0.209716796875 0.0\n",
            "tcost icost 0.1531982421875 0.0\n",
            "tcost icost 0.314208984375 0.0\n",
            "loss tensor([[1.0508]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.2420, -0.6874, -0.6674],\n",
            "         [-0.2783, -0.7167, -0.6395],\n",
            "         [-0.0385,  0.6722, -0.0090],\n",
            "         [-0.1678, -0.1634, -0.3758],\n",
            "         [ 0.1846,  0.0299, -0.7887],\n",
            "         [-0.2863,  0.2544, -0.5775]]], device='cuda:0') tensor([ 0.0715,  0.1484, -0.5781, -0.1419, -0.1728, -0.1063], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.05078125\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.11883544921875 0.0\n",
            "tcost icost 0.1319580078125 0.0\n",
            "tcost icost 0.00746917724609375 0.0\n",
            "tcost icost 0.029327392578125 0.0\n",
            "tcost icost 0.1861572265625 0.0\n",
            "tcost icost 0.112548828125 0.0\n",
            "loss tensor([[0.4536]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.2588, -0.7098, -0.6551],\n",
            "         [-0.2654, -0.7054, -0.6485],\n",
            "         [-0.0603,  0.6382, -0.0832],\n",
            "         [-0.1470, -0.1475, -0.3875],\n",
            "         [ 0.1949,  0.0411, -0.7875],\n",
            "         [-0.2886,  0.2248, -0.6016]]], device='cuda:0') tensor([ 0.2754,  0.0277, -0.5240, -0.1724, -0.1876, -0.0676], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.45361328125\n",
            "act tensor([[10, 10,  3,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.1517333984375 0.0\n",
            "tcost icost 0.205810546875 0.0\n",
            "tcost icost 0.310791015625 0.0\n",
            "tcost icost 0.295166015625 0.0\n",
            "tcost icost 0.2435302734375 0.0\n",
            "tcost icost 0.32958984375 0.0\n",
            "loss tensor([[1.1582]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.2695, -0.7090, -0.6517],\n",
            "         [-0.2451, -0.6832, -0.6716],\n",
            "         [-0.0739,  0.5713, -0.2410],\n",
            "         [-0.1380, -0.1901, -0.4574],\n",
            "         [ 0.1977,  0.0326, -0.8017],\n",
            "         [-0.2830,  0.2060, -0.6223]]], device='cuda:0') tensor([ 0.3548, -0.1532, -0.4372, -0.1428, -0.1707, -0.0549], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.158203125\n",
            "act tensor([[10, 10, 11,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.14697265625 0.0\n",
            "tcost icost 0.06951904296875 0.0\n",
            "tcost icost 0.1361083984375 0.0\n",
            "tcost icost 0.1629638671875 0.0\n",
            "tcost icost 0.09210205078125 0.0\n",
            "tcost icost 0.23828125 0.0\n",
            "loss tensor([[0.6396]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.2427, -0.6806, -0.6698],\n",
            "         [-0.2827, -0.7090, -0.6461],\n",
            "         [-0.0353,  0.6055, -0.2244],\n",
            "         [-0.1269, -0.1826, -0.4720],\n",
            "         [ 0.1777, -0.0083, -0.8318],\n",
            "         [-0.2701,  0.2202, -0.6226]]], device='cuda:0') tensor([ 0.0861,  0.0617, -0.5257, -0.1503, -0.0836, -0.0776], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6396484375\n",
            "act tensor([[10, 10, 11,  4, 11, 11]], device='cuda:0')\n",
            "tcost icost 0.123291015625 0.0\n",
            "tcost icost 0.201171875 0.0\n",
            "tcost icost 0.01348876953125 0.0\n",
            "tcost icost 0.026153564453125 0.0\n",
            "tcost icost 0.1507568359375 0.0\n",
            "tcost icost 0.09356689453125 0.0\n",
            "loss tensor([[0.4883]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-2.5246e-01, -7.0108e-01, -6.6222e-01],\n",
            "         [-2.7359e-01, -6.9710e-01, -6.5469e-01],\n",
            "         [-5.3771e-02,  5.6053e-01, -2.9649e-01],\n",
            "         [-1.0393e-01, -1.6058e-01, -4.7959e-01],\n",
            "         [ 1.8517e-01, -3.9032e-04, -8.3172e-01],\n",
            "         [-2.7152e-01,  1.9685e-01, -6.4035e-01]]], device='cuda:0') tensor([ 0.2438, -0.0410, -0.4713, -0.1840, -0.0932, -0.0466], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.48828125\n",
            "search tensor([[[ 1.8517e-01, -3.9032e-04, -8.3172e-01],\n",
            "         [-2.7152e-01,  1.9685e-01, -6.4035e-01],\n",
            "         [ 2.2596e-01,  3.8136e-01,  4.1461e-01],\n",
            "         [-5.0819e-01, -2.2751e-01, -4.3203e-01],\n",
            "         [-4.4371e-01,  3.8376e-01, -8.1249e-03],\n",
            "         [-3.4578e-01, -1.4271e-01,  5.0395e-01]]], device='cuda:0') tensor([-0.0932, -0.0466,  0.0383, -0.1063, -0.4960,  0.7777], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[11, 11, 12,  4,  3,  1]], device='cuda:0')\n",
            "tcost icost 0.0309906005859375 0.0\n",
            "tcost icost 0.1011962890625 0.0\n",
            "tcost icost 0.160888671875 0.0\n",
            "tcost icost 0.2230224609375 0.0\n",
            "tcost icost 0.031768798828125 0.0\n",
            "tcost icost -0.0212860107421875 0.0\n",
            "loss tensor([[0.4233]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.1606, -0.0228, -0.8451],\n",
            "         [-0.2683,  0.1397, -0.6916],\n",
            "         [ 0.2554,  0.4239,  0.4690],\n",
            "         [-0.5049, -0.2624, -0.4537],\n",
            "         [-0.4516,  0.3596, -0.0377],\n",
            "         [-0.3331, -0.1222,  0.5182]]], device='cuda:0') tensor([-0.0558, -0.0284, -0.0549, -0.0534, -0.4651,  0.7582], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.42333984375\n",
            "act tensor([[11, 11, 12, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.033416748046875 0.0\n",
            "tcost icost 0.119384765625 0.0\n",
            "tcost icost 0.175537109375 0.0\n",
            "tcost icost 0.273193359375 0.0\n",
            "tcost icost 0.0240325927734375 0.0\n",
            "tcost icost -0.05328369140625 0.0\n",
            "loss tensor([[0.4668]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.1392, -0.0518, -0.8621],\n",
            "         [-0.2595,  0.1158, -0.7175],\n",
            "         [ 0.2762,  0.4370,  0.4838],\n",
            "         [-0.5015, -0.2789, -0.4654],\n",
            "         [-0.4564,  0.3411, -0.0597],\n",
            "         [-0.3214, -0.1036,  0.5309]]], device='cuda:0') tensor([-0.0021, -0.0295, -0.1030, -0.0314, -0.4423,  0.7410], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.466796875\n",
            "act tensor([[11, 11, 12, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.036865234375 0.0\n",
            "tcost icost 0.1448974609375 0.0\n",
            "tcost icost 0.2083740234375 0.0\n",
            "tcost icost 0.283447265625 0.0\n",
            "tcost icost 0.021148681640625 0.0\n",
            "tcost icost -0.048126220703125 0.0\n",
            "loss tensor([[0.5278]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.1182, -0.0826, -0.8796],\n",
            "         [-0.2490,  0.0957, -0.7406],\n",
            "         [ 0.2978,  0.4439,  0.4894],\n",
            "         [-0.4977, -0.2925, -0.4755],\n",
            "         [-0.4602,  0.3248, -0.0783],\n",
            "         [-0.3095, -0.0846,  0.5437]]], device='cuda:0') tensor([ 0.0540, -0.0326, -0.1435, -0.0150, -0.4221,  0.7239], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.52783203125\n",
            "act tensor([[11, 11, 12, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.040740966796875 0.0\n",
            "tcost icost 0.1661376953125 0.0\n",
            "tcost icost 0.246826171875 0.0\n",
            "tcost icost 0.28271484375 0.0\n",
            "tcost icost 0.017730712890625 0.0\n",
            "tcost icost -0.046112060546875 0.0\n",
            "loss tensor([[0.5806]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.1054, -0.1019, -0.8914],\n",
            "         [-0.2407,  0.0732, -0.7626],\n",
            "         [ 0.3208,  0.4484,  0.4925],\n",
            "         [-0.4945, -0.3034, -0.4834],\n",
            "         [-0.4636,  0.3087, -0.0959],\n",
            "         [-0.2976, -0.0657,  0.5562]]], device='cuda:0') tensor([ 0.0900, -0.0324, -0.1809, -0.0017, -0.4023,  0.7070], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.58056640625\n",
            "act tensor([[11, 11, 12, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.04364013671875 0.0\n",
            "tcost icost 0.16455078125 0.0\n",
            "tcost icost 0.273193359375 0.0\n",
            "tcost icost 0.28662109375 0.0\n",
            "tcost icost 0.0157623291015625 0.0\n",
            "tcost icost -0.0389404296875 0.0\n",
            "loss tensor([[0.6094]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.1010, -0.1114, -0.8999],\n",
            "         [-0.2319,  0.0516, -0.7831],\n",
            "         [ 0.3435,  0.4476,  0.4853],\n",
            "         [-0.4914, -0.3084, -0.4888],\n",
            "         [-0.4667,  0.2928, -0.1130],\n",
            "         [-0.2856, -0.0464,  0.5685]]], device='cuda:0') tensor([ 0.1103, -0.0296, -0.2098,  0.0020, -0.3825,  0.6901], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.609375\n",
            "act tensor([[11, 11, 12, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.045989990234375 0.0\n",
            "tcost icost 0.1614990234375 0.0\n",
            "tcost icost 0.2890625 0.0\n",
            "tcost icost 0.2890625 0.0\n",
            "tcost icost 0.01538848876953125 0.0\n",
            "tcost icost -0.0294189453125 0.0\n",
            "loss tensor([[0.6289]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.0988, -0.1206, -0.9087],\n",
            "         [-0.2245,  0.0344, -0.7998],\n",
            "         [ 0.3660,  0.4427,  0.4669],\n",
            "         [-0.4875, -0.3064, -0.4925],\n",
            "         [-0.4696,  0.2766, -0.1300],\n",
            "         [-0.2740, -0.0275,  0.5801]]], device='cuda:0') tensor([ 0.1333, -0.0335, -0.2334, -0.0074, -0.3628,  0.6736], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.62890625\n",
            "act tensor([[11, 11, 12, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.04815673828125 0.0\n",
            "tcost icost 0.1561279296875 0.0\n",
            "tcost icost 0.311767578125 0.0\n",
            "tcost icost 0.285400390625 0.0\n",
            "tcost icost 0.0158538818359375 0.0\n",
            "tcost icost -0.026397705078125 0.0\n",
            "loss tensor([[0.6440]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.1030, -0.1212, -0.9142],\n",
            "         [-0.2201,  0.0184, -0.8134],\n",
            "         [ 0.3891,  0.4352,  0.4423],\n",
            "         [-0.4846, -0.3021, -0.4946],\n",
            "         [-0.4726,  0.2592, -0.1479],\n",
            "         [-0.2630, -0.0094,  0.5907]]], device='cuda:0') tensor([ 0.1393, -0.0362, -0.2505, -0.0211, -0.3426,  0.6575], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.64404296875\n",
            "act tensor([[11, 11, 12, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.04864501953125 0.0\n",
            "tcost icost 0.1541748046875 0.0\n",
            "tcost icost 0.319091796875 0.0\n",
            "tcost icost 0.2880859375 0.0\n",
            "tcost icost 0.01837158203125 0.0\n",
            "tcost icost -0.019378662109375 0.0\n",
            "loss tensor([[0.6572]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.1008, -0.1326, -0.9239],\n",
            "         [-0.2120,  0.0187, -0.8206],\n",
            "         [ 0.4142,  0.4210,  0.3998],\n",
            "         [-0.4805, -0.2934, -0.4967],\n",
            "         [-0.4756,  0.2406, -0.1665],\n",
            "         [-0.2519,  0.0091,  0.6010]]], device='cuda:0') tensor([ 0.1603, -0.0591, -0.2503, -0.0410, -0.3212,  0.6410], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6572265625\n",
            "act tensor([[11, 11, 12, 10,  4,  1]], device='cuda:0')\n",
            "tcost icost 0.0498046875 0.0\n",
            "tcost icost 0.142333984375 0.0\n",
            "tcost icost 0.345947265625 0.0\n",
            "tcost icost 0.27099609375 0.0\n",
            "tcost icost 0.021209716796875 0.0\n",
            "tcost icost -0.0260162353515625 0.0\n",
            "loss tensor([[0.6538]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.1384, -0.0783, -0.9096],\n",
            "         [-0.2226, -0.0597, -0.8578],\n",
            "         [ 0.4390,  0.4305,  0.4121],\n",
            "         [-0.4763, -0.2968, -0.5028],\n",
            "         [-0.4791,  0.2187, -0.1880],\n",
            "         [-0.2410,  0.0275,  0.6109]]], device='cuda:0') tensor([ 0.0872,  0.0158, -0.3207, -0.0402, -0.2964,  0.6244], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.65380859375\n",
            "act tensor([[11, 11, 12, 10,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.043426513671875 0.0\n",
            "tcost icost 0.16357421875 0.0\n",
            "tcost icost 0.2188720703125 0.0\n",
            "tcost icost 0.25146484375 0.0\n",
            "tcost icost 0.038787841796875 0.0\n",
            "tcost icost -0.005161285400390625 0.0\n",
            "loss tensor([[0.5737]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.0987, -0.1543, -0.9477],\n",
            "         [-0.1942,  0.0369, -0.8347],\n",
            "         [ 0.4707,  0.3782,  0.2830],\n",
            "         [-0.4709, -0.2497, -0.4918],\n",
            "         [-0.4837,  0.1866, -0.2185],\n",
            "         [-0.2286,  0.0488,  0.6220]]], device='cuda:0') tensor([ 0.1988, -0.1184, -0.1980, -0.0995, -0.2597,  0.6052], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.57373046875\n",
            "act tensor([[11, 11, 12, 10,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.05029296875 0.0\n",
            "tcost icost 0.10418701171875 0.0\n",
            "tcost icost 0.2587890625 0.0\n",
            "tcost icost 0.183837890625 0.0\n",
            "tcost icost 0.036773681640625 0.0\n",
            "tcost icost -0.0894775390625 0.0\n",
            "loss tensor([[0.4590]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.1893, -0.0139, -0.9055],\n",
            "         [-0.2324, -0.1424, -0.9103],\n",
            "         [ 0.4936,  0.4234,  0.3554],\n",
            "         [-0.4684, -0.2629, -0.5006],\n",
            "         [-0.4878,  0.1574, -0.2451],\n",
            "         [-0.2182,  0.0669,  0.6310]]], device='cuda:0') tensor([-0.0047,  0.0999, -0.3353, -0.0784, -0.2277,  0.5883], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.458984375\n",
            "act tensor([[11, 10, 12, 10,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.0367431640625 0.0\n",
            "tcost icost 0.1531982421875 0.0\n",
            "tcost icost 0.153076171875 0.0\n",
            "tcost icost 0.25390625 0.0\n",
            "tcost icost 0.07867431640625 0.0\n",
            "tcost icost 0.046630859375 0.0\n",
            "loss tensor([[0.5625]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.1414, -0.1007, -0.9488],\n",
            "         [-0.2125, -0.0380, -0.8889],\n",
            "         [ 0.5277,  0.3584,  0.2094],\n",
            "         [-0.4592, -0.1973, -0.4846],\n",
            "         [-0.4941,  0.1024, -0.2934],\n",
            "         [-0.2033,  0.0936,  0.6432]]], device='cuda:0') tensor([ 0.1297, -0.0508, -0.1971, -0.1544, -0.1666,  0.5630], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.5625\n",
            "act tensor([[11, 11, 14,  4,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.04791259765625 0.0\n",
            "tcost icost 0.15478515625 0.0\n",
            "tcost icost 0.2978515625 0.0\n",
            "tcost icost 0.282470703125 0.0\n",
            "tcost icost 0.06341552734375 0.0\n",
            "tcost icost 0.00690460205078125 0.0\n",
            "loss tensor([[0.6797]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.1617, -0.0768, -0.9446],\n",
            "         [-0.2222, -0.0857, -0.9078],\n",
            "         [ 0.5541,  0.3551,  0.1954],\n",
            "         [-0.4519, -0.1868, -0.4884],\n",
            "         [-0.4969,  0.0578, -0.3307],\n",
            "         [-0.1909,  0.1157,  0.6530]]], device='cuda:0') tensor([ 0.1004, -0.0193, -0.2476, -0.1721, -0.1187,  0.5420], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6796875\n",
            "act tensor([[11, 11, 14,  4,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.044830322265625 0.0\n",
            "tcost icost 0.163818359375 0.0\n",
            "tcost icost 0.2479248046875 0.0\n",
            "tcost icost 0.29638671875 0.0\n",
            "tcost icost 0.1029052734375 0.0\n",
            "tcost icost 0.07196044921875 0.0\n",
            "loss tensor([[0.7192]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.1660, -0.0831, -0.9518],\n",
            "         [-0.2233, -0.0750, -0.9058],\n",
            "         [ 0.5770,  0.3166,  0.1235],\n",
            "         [-0.4412, -0.1483, -0.4822],\n",
            "         [-0.4986,  0.0076, -0.3706],\n",
            "         [-0.1771,  0.1409,  0.6632]]], device='cuda:0') tensor([ 0.1195, -0.0507, -0.2194, -0.2251, -0.0672,  0.5166], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.71923828125\n",
            "act tensor([[11, 11, 14,  4,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.04705810546875 0.0\n",
            "tcost icost 0.1575927734375 0.0\n",
            "tcost icost 0.28955078125 0.0\n",
            "tcost icost 0.29248046875 0.0\n",
            "tcost icost 0.1490478515625 0.0\n",
            "tcost icost 0.07122802734375 0.0\n",
            "loss tensor([[0.7764]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.1784, -0.0718, -0.9515],\n",
            "         [-0.2318, -0.1039, -0.9162],\n",
            "         [ 0.6029,  0.3005,  0.0885],\n",
            "         [-0.4313, -0.1348, -0.4877],\n",
            "         [-0.4983, -0.0455, -0.4112],\n",
            "         [-0.1640,  0.1657,  0.6725]]], device='cuda:0') tensor([ 0.1082, -0.0440, -0.2482, -0.2485, -0.0128,  0.4906], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7763671875\n",
            "act tensor([[11, 11, 14,  4,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.0457763671875 0.0\n",
            "tcost icost 0.1607666015625 0.0\n",
            "tcost icost 0.2724609375 0.0\n",
            "tcost icost 0.30615234375 0.0\n",
            "tcost icost 0.184814453125 0.0\n",
            "tcost icost 0.12109375 0.0\n",
            "loss tensor([[0.8271]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.1866, -0.0664, -0.9536],\n",
            "         [-0.2377, -0.1198, -0.9227],\n",
            "         [ 0.6277,  0.2739,  0.0379],\n",
            "         [-0.4186, -0.1164, -0.4934],\n",
            "         [-0.4958, -0.1031, -0.4531],\n",
            "         [-0.1499,  0.1934,  0.6822]]], device='cuda:0') tensor([ 0.1028, -0.0537, -0.2591, -0.2781,  0.0449,  0.4606], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8271484375\n",
            "act tensor([[11, 11, 14,  4,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.04510498046875 0.0\n",
            "tcost icost 0.160888671875 0.0\n",
            "tcost icost 0.2744140625 0.0\n",
            "tcost icost 0.30859375 0.0\n",
            "tcost icost 0.2222900390625 0.0\n",
            "tcost icost 0.1593017578125 0.0\n",
            "loss tensor([[0.8774]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.1939, -0.0631, -0.9563],\n",
            "         [-0.2424, -0.1332, -0.9287],\n",
            "         [ 0.6497,  0.2441, -0.0174],\n",
            "         [-0.4024, -0.1000, -0.5034],\n",
            "         [-0.4893, -0.1604, -0.4945],\n",
            "         [-0.1372,  0.2203,  0.6908]]], device='cuda:0') tensor([ 0.1014, -0.0676, -0.2688, -0.3112,  0.0985,  0.4309], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.87744140625\n",
            "act tensor([[11, 10, 14,  4,  4,  3]], device='cuda:0')\n",
            "tcost icost 0.044952392578125 0.0\n",
            "tcost icost 0.149658203125 0.0\n",
            "tcost icost 0.31396484375 0.0\n",
            "tcost icost 0.2109375 0.0\n",
            "tcost icost 0.25830078125 0.0\n",
            "tcost icost 0.1019287109375 0.0\n",
            "loss tensor([[0.8174]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.2552,  0.0263, -0.9224],\n",
            "         [-0.2665, -0.2622, -0.9275],\n",
            "         [ 0.6873,  0.2832,  0.0258],\n",
            "         [-0.3923, -0.1210, -0.5288],\n",
            "         [-0.4791, -0.2087, -0.5313],\n",
            "         [-0.1268,  0.2431,  0.6975]]], device='cuda:0') tensor([-0.0310,  0.0890, -0.4008, -0.2951,  0.1426,  0.4050], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8173828125\n",
            "act tensor([[11, 10, 14,  4, 10,  3]], device='cuda:0')\n",
            "tcost icost 0.03515625 0.0\n",
            "tcost icost 0.1436767578125 0.0\n",
            "tcost icost 0.06719970703125 0.0\n",
            "tcost icost 0.128662109375 0.0\n",
            "tcost icost 0.173095703125 0.0\n",
            "tcost icost 0.2003173828125 0.0\n",
            "loss tensor([[0.5444]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.1728, -0.0895, -0.9790],\n",
            "         [-0.2276, -0.1329, -0.9235],\n",
            "         [ 0.6796,  0.2029, -0.1087],\n",
            "         [-0.3745, -0.0967, -0.5347],\n",
            "         [-0.4706, -0.2214, -0.5461],\n",
            "         [-0.1206,  0.2502,  0.6978]]], device='cuda:0') tensor([ 0.1480, -0.0914, -0.2443, -0.3237,  0.1418,  0.3878], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.54443359375\n",
            "act tensor([[11, 11, 14,  4, 10,  3]], device='cuda:0')\n",
            "tcost icost 0.049285888671875 0.0\n",
            "tcost icost 0.1396484375 0.0\n",
            "tcost icost 0.33251953125 0.0\n",
            "tcost icost 0.2459716796875 0.0\n",
            "tcost icost 0.2384033203125 0.0\n",
            "tcost icost 0.125244140625 0.0\n",
            "loss tensor([[0.8535]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.2173, -0.0230, -0.9580],\n",
            "         [-0.2484, -0.2296, -0.9410],\n",
            "         [ 0.7105,  0.2266, -0.0899],\n",
            "         [-0.3647, -0.1169, -0.5581],\n",
            "         [-0.4595, -0.2730, -0.5791],\n",
            "         [-0.1098,  0.2742,  0.7042]]], device='cuda:0') tensor([ 0.0391,  0.0058, -0.3429, -0.3077,  0.1915,  0.3591], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.853515625\n",
            "search tensor([[[-0.4595, -0.2730, -0.5791],\n",
            "         [-0.1098,  0.2742,  0.7042],\n",
            "         [ 0.2659, -0.2908,  0.1145],\n",
            "         [-0.2880,  0.4476,  0.2350],\n",
            "         [ 0.0502,  0.2428, -0.2151],\n",
            "         [ 0.2828,  0.2356,  0.1601]]], device='cuda:0') tensor([ 0.1915,  0.3591,  0.3593, -0.4368, -0.2027,  0.4337], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[10,  3,  0,  3, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.2021484375 0.0\n",
            "tcost icost 0.01580810546875 0.0\n",
            "tcost icost 0.043487548828125 0.0\n",
            "tcost icost 0.01322174072265625 0.0\n",
            "tcost icost 0.0201416015625 0.0\n",
            "tcost icost 0.211181640625 0.0\n",
            "loss tensor([[0.3992]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.4525, -0.3591, -0.6167],\n",
            "         [-0.0976,  0.3115,  0.7123],\n",
            "         [ 0.2733, -0.2845,  0.1168],\n",
            "         [-0.3015,  0.4289,  0.2039],\n",
            "         [ 0.0469,  0.2162, -0.2580],\n",
            "         [ 0.2927,  0.2493,  0.1715]]], device='cuda:0') tensor([ 0.2914,  0.3070,  0.3509, -0.4122, -0.1839,  0.4035], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.399169921875\n",
            "act tensor([[10,  8,  0,  3, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.20751953125 0.0\n",
            "tcost icost 0.140380859375 0.0\n",
            "tcost icost 0.12841796875 0.0\n",
            "tcost icost 0.0135345458984375 0.0\n",
            "tcost icost 0.0222320556640625 0.0\n",
            "tcost icost 0.18603515625 0.0\n",
            "loss tensor([[0.5723]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.4337, -0.4452, -0.6525],\n",
            "         [-0.0830,  0.3584,  0.7221],\n",
            "         [ 0.2833, -0.2796,  0.1154],\n",
            "         [-0.3137,  0.4117,  0.1776],\n",
            "         [ 0.0413,  0.1833, -0.3079],\n",
            "         [ 0.3031,  0.2652,  0.1858]]], device='cuda:0') tensor([ 0.3866,  0.2305,  0.3413, -0.3911, -0.1578,  0.3701], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.572265625\n",
            "act tensor([[10,  5,  0,  3, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.20166015625 0.0\n",
            "tcost icost 0.2322998046875 0.0\n",
            "tcost icost 0.241455078125 0.0\n",
            "tcost icost 0.015106201171875 0.0\n",
            "tcost icost 0.0236968994140625 0.0\n",
            "tcost icost 0.1087646484375 0.0\n",
            "loss tensor([[0.6973]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.4217, -0.4582, -0.6655],\n",
            "         [-0.0875,  0.3546,  0.7163],\n",
            "         [ 0.3127, -0.2525,  0.1332],\n",
            "         [-0.3237,  0.3980,  0.1580],\n",
            "         [ 0.0329,  0.1476, -0.3566],\n",
            "         [ 0.3130,  0.2816,  0.2025]]], device='cuda:0') tensor([ 0.3895,  0.2032,  0.3011, -0.3701, -0.1237,  0.3386], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.697265625\n",
            "act tensor([[10,  5,  0,  3, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.2010498046875 0.0\n",
            "tcost icost 0.240234375 0.0\n",
            "tcost icost 0.25439453125 0.0\n",
            "tcost icost 0.019073486328125 0.0\n",
            "tcost icost 0.0276641845703125 0.0\n",
            "tcost icost 0.1505126953125 0.0\n",
            "loss tensor([[0.7437]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.4115, -0.4547, -0.6763],\n",
            "         [-0.0963,  0.3371,  0.7055],\n",
            "         [ 0.3453, -0.2209,  0.1529],\n",
            "         [-0.3348,  0.3812,  0.1340],\n",
            "         [ 0.0246,  0.1091, -0.4077],\n",
            "         [ 0.3229,  0.2980,  0.2197]]], device='cuda:0') tensor([ 0.3658,  0.1897,  0.2535, -0.3456, -0.0864,  0.3051], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.74365234375\n",
            "act tensor([[10,  5,  0,  3, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.2041015625 0.0\n",
            "tcost icost 0.2410888671875 0.0\n",
            "tcost icost 0.2442626953125 0.0\n",
            "tcost icost 0.028839111328125 0.0\n",
            "tcost icost 0.047271728515625 0.0\n",
            "tcost icost 0.2327880859375 0.0\n",
            "loss tensor([[0.8086]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.4019, -0.4732, -0.6867],\n",
            "         [-0.1083,  0.3236,  0.6944],\n",
            "         [ 0.3805, -0.1823,  0.1760],\n",
            "         [-0.3484,  0.3555,  0.0960],\n",
            "         [ 0.0190,  0.0685, -0.4627],\n",
            "         [ 0.3328,  0.3138,  0.2360]]], device='cuda:0') tensor([ 0.3797,  0.1634,  0.1973, -0.3101, -0.0457,  0.2686], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.80859375\n",
            "act tensor([[10,  3,  0,  3, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.2030029296875 0.0\n",
            "tcost icost 0.2471923828125 0.0\n",
            "tcost icost 0.26708984375 0.0\n",
            "tcost icost 0.049407958984375 0.0\n",
            "tcost icost 0.0849609375 0.0\n",
            "tcost icost 0.275634765625 0.0\n",
            "loss tensor([[0.8960]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.3952, -0.4831, -0.6952],\n",
            "         [-0.1284,  0.3012,  0.6766],\n",
            "         [ 0.4228, -0.1330,  0.1998],\n",
            "         [-0.3610,  0.3230,  0.0454],\n",
            "         [ 0.0151,  0.0275, -0.5172],\n",
            "         [ 0.3410,  0.3251,  0.2468]]], device='cuda:0') tensor([ 0.3736,  0.1338,  0.1232, -0.2627,  0.0028,  0.2362], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.89599609375\n",
            "act tensor([[10,  3,  0,  3, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.2034912109375 0.0\n",
            "tcost icost 0.253173828125 0.0\n",
            "tcost icost 0.269775390625 0.0\n",
            "tcost icost 0.09735107421875 0.0\n",
            "tcost icost 0.191650390625 0.0\n",
            "tcost icost 0.320556640625 0.0\n",
            "loss tensor([[1.0361]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.3918, -0.5040, -0.7014],\n",
            "         [-0.1576,  0.2804,  0.6537],\n",
            "         [ 0.4695, -0.0751,  0.2206],\n",
            "         [-0.3747,  0.2742, -0.0302],\n",
            "         [ 0.0107, -0.0172, -0.5741],\n",
            "         [ 0.3476,  0.3329,  0.2541]]], device='cuda:0') tensor([ 0.3782,  0.0845,  0.0291, -0.1996,  0.0600,  0.2083], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.0361328125\n",
            "act tensor([[10,  3,  0,  4, 11, 12]], device='cuda:0')\n",
            "tcost icost 0.2032470703125 0.0\n",
            "tcost icost 0.25341796875 0.0\n",
            "tcost icost 0.28955078125 0.0\n",
            "tcost icost 0.09881591796875 0.0\n",
            "tcost icost 0.266845703125 0.0\n",
            "tcost icost 0.258544921875 0.0\n",
            "loss tensor([[1.0645]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.3914, -0.5295, -0.7040],\n",
            "         [-0.1554,  0.2916,  0.6484],\n",
            "         [ 0.4935, -0.0473,  0.2245],\n",
            "         [-0.3940,  0.2005, -0.1229],\n",
            "         [ 0.0192, -0.0288, -0.6025],\n",
            "         [ 0.3528,  0.3370,  0.2564]]], device='cuda:0') tensor([ 0.3912,  0.0205, -0.0199, -0.1296,  0.0624,  0.1895], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.064453125\n",
            "act tensor([[10,  3,  0,  4, 11, 12]], device='cuda:0')\n",
            "tcost icost 0.2005615234375 0.0\n",
            "tcost icost 0.23876953125 0.0\n",
            "tcost icost 0.353759765625 0.0\n",
            "tcost icost 0.1817626953125 0.0\n",
            "tcost icost 0.271240234375 0.0\n",
            "tcost icost 0.278076171875 0.0\n",
            "loss tensor([[1.1768]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.3897, -0.5295, -0.7100],\n",
            "         [-0.1750,  0.2515,  0.6156],\n",
            "         [ 0.5307, -0.0059,  0.2330],\n",
            "         [-0.4031,  0.1343, -0.2024],\n",
            "         [ 0.0220, -0.0498, -0.6344],\n",
            "         [ 0.3578,  0.3402,  0.2569]]], device='cuda:0') tensor([ 0.3599, -0.0103, -0.1014, -0.0694,  0.0803,  0.1706], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.1767578125\n",
            "act tensor([[10,  3,  0,  4, 11, 12]], device='cuda:0')\n",
            "tcost icost 0.2044677734375 0.0\n",
            "tcost icost 0.25390625 0.0\n",
            "tcost icost 0.32763671875 0.0\n",
            "tcost icost 0.31005859375 0.0\n",
            "tcost icost 0.27685546875 0.0\n",
            "tcost icost 0.303955078125 0.0\n",
            "loss tensor([[1.2852]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.3805, -0.5616, -0.7165],\n",
            "         [-0.1567,  0.2994,  0.6270],\n",
            "         [ 0.5538, -0.0215,  0.1865],\n",
            "         [-0.4018,  0.0774, -0.2734],\n",
            "         [ 0.0286, -0.0584, -0.6545],\n",
            "         [ 0.3631,  0.3406,  0.2525]]], device='cuda:0') tensor([ 0.3806, -0.1056, -0.1117, -0.0297,  0.0796,  0.1577], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.28515625\n",
            "act tensor([[10,  3,  0,  4, 11, 12]], device='cuda:0')\n",
            "tcost icost 0.2027587890625 0.0\n",
            "tcost icost 0.20654296875 0.0\n",
            "tcost icost 0.45849609375 0.0\n",
            "tcost icost 0.286865234375 0.0\n",
            "tcost icost 0.2484130859375 0.0\n",
            "tcost icost 0.298095703125 0.0\n",
            "loss tensor([[1.3076]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.3745, -0.5312, -0.7290],\n",
            "         [-0.1929,  0.2156,  0.5706],\n",
            "         [ 0.5932, -0.0081,  0.1688],\n",
            "         [-0.3932,  0.0481, -0.3175],\n",
            "         [ 0.0257, -0.0867, -0.6876],\n",
            "         [ 0.3688,  0.3444,  0.2532]]], device='cuda:0') tensor([ 0.3057, -0.0835, -0.1785, -0.0239,  0.1079,  0.1371], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.3076171875\n",
            "act tensor([[10,  3,  0,  4, 11, 12]], device='cuda:0')\n",
            "tcost icost 0.2073974609375 0.0\n",
            "tcost icost 0.278076171875 0.0\n",
            "tcost icost 0.3154296875 0.0\n",
            "tcost icost 0.3642578125 0.0\n",
            "tcost icost 0.2491455078125 0.0\n",
            "tcost icost 0.2998046875 0.0\n",
            "loss tensor([[1.3184]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.3532, -0.5810, -0.7333],\n",
            "         [-0.1389,  0.3328,  0.6282],\n",
            "         [ 0.6007, -0.0785,  0.0712],\n",
            "         [-0.3756,  0.0272, -0.3623],\n",
            "         [ 0.0353, -0.0851, -0.6994],\n",
            "         [ 0.3727,  0.3378,  0.2376]]], device='cuda:0') tensor([ 0.3648, -0.2189, -0.1212, -0.0324,  0.0954,  0.1406], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.318359375\n",
            "act tensor([[10,  3,  0,  4, 11, 12]], device='cuda:0')\n",
            "tcost icost 0.2039794921875 0.0\n",
            "tcost icost 0.17626953125 0.0\n",
            "tcost icost 0.421630859375 0.0\n",
            "tcost icost 0.2059326171875 0.0\n",
            "tcost icost 0.2666015625 0.0\n",
            "tcost icost 0.263427734375 0.0\n",
            "loss tensor([[1.1846]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-3.5892e-01, -4.4965e-01, -7.5927e-01],\n",
            "         [-2.4519e-01,  3.4593e-02,  4.5226e-01],\n",
            "         [ 6.5404e-01,  3.8803e-04,  1.1709e-01],\n",
            "         [-3.6595e-01,  8.1045e-03, -3.9436e-01],\n",
            "         [ 2.7632e-02, -1.2041e-01, -7.3399e-01],\n",
            "         [ 3.7932e-01,  3.4451e-01,  2.4177e-01]]], device='cuda:0') tensor([ 0.0904,  0.0299, -0.2840, -0.0298,  0.1396,  0.1165], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.1845703125\n",
            "act tensor([[10,  1,  0,  4, 11, 12]], device='cuda:0')\n",
            "tcost icost 0.1636962890625 0.0\n",
            "tcost icost 0.1800537109375 0.0\n",
            "tcost icost 0.032379150390625 0.0\n",
            "tcost icost 0.1387939453125 0.0\n",
            "tcost icost 0.1885986328125 0.0\n",
            "tcost icost 0.17431640625 0.0\n",
            "loss tensor([[0.6797]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.3288, -0.6236, -0.7093],\n",
            "         [-0.0952,  0.3418,  0.6570],\n",
            "         [ 0.6342, -0.1140, -0.0114],\n",
            "         [-0.3418,  0.0273, -0.4087],\n",
            "         [ 0.0439, -0.1014, -0.7341],\n",
            "         [ 0.3799,  0.3308,  0.2157]]], device='cuda:0') tensor([ 0.4173, -0.2081, -0.1249, -0.0605,  0.1057,  0.1391], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6796875\n",
            "act tensor([[10,  3,  0,  4, 11, 12]], device='cuda:0')\n",
            "tcost icost 0.1934814453125 0.0\n",
            "tcost icost 0.13525390625 0.0\n",
            "tcost icost 0.369873046875 0.0\n",
            "tcost icost 0.176513671875 0.0\n",
            "tcost icost 0.27294921875 0.0\n",
            "tcost icost 0.2410888671875 0.0\n",
            "loss tensor([[1.0645]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.3319, -0.4929, -0.7442],\n",
            "         [-0.1872,  0.0444,  0.4991],\n",
            "         [ 0.6905, -0.0292,  0.0478],\n",
            "         [-0.3336,  0.0136, -0.4339],\n",
            "         [ 0.0323, -0.1389, -0.7662],\n",
            "         [ 0.3859,  0.3384,  0.2215]]], device='cuda:0') tensor([ 0.0685,  0.0926, -0.2993, -0.0618,  0.1559,  0.1139], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.064453125\n",
            "act tensor([[10,  3,  0,  4, 11, 12]], device='cuda:0')\n",
            "tcost icost 0.15478515625 0.0\n",
            "tcost icost 0.03955078125 0.0\n",
            "tcost icost -0.00992584228515625 0.0\n",
            "tcost icost 0.0127716064453125 0.0\n",
            "tcost icost 0.10931396484375 0.0\n",
            "tcost icost 0.08673095703125 0.0\n",
            "loss tensor([[0.3147]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.3316, -0.5943, -0.7327],\n",
            "         [-0.1276,  0.1489,  0.5596],\n",
            "         [ 0.6794, -0.0536,  0.0156],\n",
            "         [-0.3271, -0.0123, -0.4641],\n",
            "         [ 0.0511, -0.1127, -0.7625],\n",
            "         [ 0.3852,  0.3261,  0.1990]]], device='cuda:0') tensor([ 0.2528,  0.0009, -0.2695, -0.0573,  0.1108,  0.1344], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.314697265625\n",
            "act tensor([[10,  3,  0,  4, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.2083740234375 0.0\n",
            "tcost icost 0.27197265625 0.0\n",
            "tcost icost 0.06976318359375 0.0\n",
            "tcost icost 0.1533203125 0.0\n",
            "tcost icost 0.2042236328125 0.0\n",
            "tcost icost 0.2139892578125 0.0\n",
            "loss tensor([[0.8823]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.2992, -0.6610, -0.6882],\n",
            "         [-0.0130,  0.4406,  0.7259],\n",
            "         [ 0.6408, -0.1678, -0.1253],\n",
            "         [-0.2993,  0.0083, -0.4807],\n",
            "         [ 0.0661, -0.0963, -0.7638],\n",
            "         [ 0.3858,  0.3110,  0.1705]]], device='cuda:0') tensor([ 0.4627, -0.2351, -0.0964, -0.0921,  0.0802,  0.1561], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.88232421875\n",
            "act tensor([[10,  8,  0,  4, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.177001953125 0.0\n",
            "tcost icost 0.08502197265625 0.0\n",
            "tcost icost 0.1982421875 0.0\n",
            "tcost icost 0.11114501953125 0.0\n",
            "tcost icost 0.26123046875 0.0\n",
            "tcost icost 0.1900634765625 0.0\n",
            "loss tensor([[0.7788]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.2950, -0.5781, -0.7165],\n",
            "         [-0.0482,  0.3222,  0.6609],\n",
            "         [ 0.7087, -0.0791, -0.0529],\n",
            "         [-0.2936, -0.0190, -0.5123],\n",
            "         [ 0.0551, -0.1300, -0.7933],\n",
            "         [ 0.3931,  0.3222,  0.1814]]], device='cuda:0') tensor([ 0.1469,  0.0464, -0.2661, -0.0726,  0.1318,  0.1271], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.77880859375\n",
            "act tensor([[10,  8,  0,  4, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.1898193359375 0.0\n",
            "tcost icost 0.1517333984375 0.0\n",
            "tcost icost 0.00716400146484375 0.0\n",
            "tcost icost 0.045806884765625 0.0\n",
            "tcost icost 0.1744384765625 0.0\n",
            "tcost icost 0.12396240234375 0.0\n",
            "loss tensor([[0.5532]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.2913, -0.6709, -0.6819],\n",
            "         [ 0.0036,  0.4501,  0.7186],\n",
            "         [ 0.6852, -0.1267, -0.1152],\n",
            "         [-0.2774, -0.0203, -0.5307],\n",
            "         [ 0.0722, -0.1077, -0.7905],\n",
            "         [ 0.3919,  0.3062,  0.1529]]], device='cuda:0') tensor([ 0.4000, -0.0970, -0.1922, -0.0898,  0.0903,  0.1522], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.55322265625\n",
            "act tensor([[10,  8,  0,  4, 11,  7]], device='cuda:0')\n",
            "tcost icost 0.198486328125 0.0\n",
            "tcost icost 0.194580078125 0.0\n",
            "tcost icost 0.4462890625 0.0\n",
            "tcost icost 0.33837890625 0.0\n",
            "tcost icost 0.283203125 0.0\n",
            "tcost icost 0.300537109375 0.0\n",
            "loss tensor([[1.3457]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.2832, -0.6628, -0.6926],\n",
            "         [ 0.0037,  0.4527,  0.7114],\n",
            "         [ 0.6818, -0.1443, -0.1648],\n",
            "         [-0.2669, -0.0536, -0.5686],\n",
            "         [ 0.0769, -0.1122, -0.8022],\n",
            "         [ 0.3970,  0.3066,  0.1483]]], device='cuda:0') tensor([ 0.3512, -0.1332, -0.1898, -0.0860,  0.0858,  0.1393], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.345703125\n",
            "search tensor([[[ 0.0769, -0.1122, -0.8022],\n",
            "         [ 0.3970,  0.3066,  0.1483],\n",
            "         [-0.0157,  0.4352, -0.2858],\n",
            "         [-0.0242, -0.2520, -0.1930],\n",
            "         [ 0.1947,  0.2548,  0.4875],\n",
            "         [-0.1085, -0.0912, -0.2238]]], device='cuda:0') tensor([ 0.0858,  0.1393, -0.2348,  0.3314,  0.3463, -0.0970], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[11,  7, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.1474609375 0.0\n",
            "tcost icost 0.2294921875 0.0\n",
            "tcost icost 0.06671142578125 0.0\n",
            "tcost icost -0.0357666015625 0.0\n",
            "tcost icost 0.10113525390625 0.0\n",
            "tcost icost -0.0020351409912109375 0.0\n",
            "loss tensor([[0.4470]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.0472, -0.1837, -0.8492],\n",
            "         [ 0.4252,  0.3830,  0.2606],\n",
            "         [-0.0380,  0.3535, -0.3917],\n",
            "         [ 0.0134, -0.2179, -0.1706],\n",
            "         [ 0.1963,  0.2487,  0.4801],\n",
            "         [-0.1094, -0.0934, -0.2258]]], device='cuda:0') tensor([ 0.1889,  0.0149, -0.1277,  0.2876,  0.3514, -0.0957], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.447021484375\n",
            "act tensor([[11, 12, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.1435546875 0.0\n",
            "tcost icost 0.1964111328125 0.0\n",
            "tcost icost 0.319091796875 0.0\n",
            "tcost icost 0.11822509765625 0.0\n",
            "tcost icost 0.12744140625 0.0\n",
            "tcost icost 0.00670623779296875 0.0\n",
            "loss tensor([[0.7524]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.0516, -0.2056, -0.8749],\n",
            "         [ 0.4332,  0.4335,  0.3448],\n",
            "         [-0.0536,  0.2452, -0.5228],\n",
            "         [ 0.0634, -0.1681, -0.1370],\n",
            "         [ 0.1966,  0.2376,  0.4677],\n",
            "         [-0.1106, -0.0966, -0.2285]]], device='cuda:0') tensor([ 0.2068, -0.0956, -0.0245,  0.2192,  0.3628, -0.0925], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.75244140625\n",
            "act tensor([[11, 12, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.138671875 0.0\n",
            "tcost icost 0.151611328125 0.0\n",
            "tcost icost 0.35791015625 0.0\n",
            "tcost icost 0.218505859375 0.0\n",
            "tcost icost 0.1851806640625 0.0\n",
            "tcost icost 0.03460693359375 0.0\n",
            "loss tensor([[0.8662]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.1499, -0.0765, -0.8488],\n",
            "         [ 0.4437,  0.3784,  0.2075],\n",
            "         [-0.0214,  0.2322, -0.5676],\n",
            "         [ 0.0970, -0.1354, -0.1260],\n",
            "         [ 0.1976,  0.2257,  0.4511],\n",
            "         [-0.1153, -0.1093, -0.2395]]], device='cuda:0') tensor([ 0.0290,  0.0258, -0.0786,  0.1575,  0.3731, -0.0790], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8662109375\n",
            "act tensor([[11, 12, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.1416015625 0.0\n",
            "tcost icost 0.2392578125 0.0\n",
            "tcost icost 0.156494140625 0.0\n",
            "tcost icost 0.1741943359375 0.0\n",
            "tcost icost 0.1656494140625 0.0\n",
            "tcost icost 0.001739501953125 0.0\n",
            "loss tensor([[0.7202]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.0715, -0.2100, -0.9232],\n",
            "         [ 0.4722,  0.4790,  0.3839],\n",
            "         [-0.0455,  0.0952, -0.6876],\n",
            "         [ 0.1407, -0.0905, -0.1001],\n",
            "         [ 0.2012,  0.2269,  0.4490],\n",
            "         [-0.1165, -0.1126, -0.2422]]], device='cuda:0') tensor([ 0.2440, -0.1789,  0.0784,  0.0913,  0.3646, -0.0758], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.72021484375\n",
            "act tensor([[11, 12, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.1280517578125 0.0\n",
            "tcost icost 0.08453369140625 0.0\n",
            "tcost icost 0.09906005859375 0.0\n",
            "tcost icost 0.1397705078125 0.0\n",
            "tcost icost 0.2415771484375 0.0\n",
            "tcost icost 0.1318359375 0.0\n",
            "loss tensor([[0.6226]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.1669, -0.0747, -0.8911],\n",
            "         [ 0.4991,  0.4284,  0.2326],\n",
            "         [-0.0105,  0.1362, -0.6830],\n",
            "         [ 0.1379, -0.1076, -0.1412],\n",
            "         [ 0.2126,  0.2334,  0.4466],\n",
            "         [-0.1251, -0.1361, -0.2617]]], device='cuda:0') tensor([ 0.0506, -0.0101, -0.0068,  0.0902,  0.3487, -0.0524], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.62255859375\n",
            "act tensor([[11, 12, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.14404296875 0.0\n",
            "tcost icost 0.25146484375 0.0\n",
            "tcost icost 0.23681640625 0.0\n",
            "tcost icost 0.2919921875 0.0\n",
            "tcost icost 0.2188720703125 0.0\n",
            "tcost icost 0.01425933837890625 0.0\n",
            "loss tensor([[0.9268]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.1234, -0.1449, -0.9279],\n",
            "         [ 0.5158,  0.4626,  0.2956],\n",
            "         [-0.0083,  0.0771, -0.7349],\n",
            "         [ 0.1522, -0.1042, -0.1639],\n",
            "         [ 0.2207,  0.2416,  0.4491],\n",
            "         [-0.1284, -0.1447, -0.2683]]], device='cuda:0') tensor([ 0.1670, -0.1278,  0.0423,  0.0625,  0.3284, -0.0429], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.9267578125\n",
            "act tensor([[11, 12, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.147705078125 0.0\n",
            "tcost icost 0.1693115234375 0.0\n",
            "tcost icost 0.288330078125 0.0\n",
            "tcost icost 0.20556640625 0.0\n",
            "tcost icost 0.259033203125 0.0\n",
            "tcost icost 0.10675048828125 0.0\n",
            "loss tensor([[0.9165]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.2161, -0.0209, -0.8896],\n",
            "         [ 0.5363,  0.4000,  0.1397],\n",
            "         [ 0.0280,  0.1056, -0.7406],\n",
            "         [ 0.1619, -0.1047, -0.1929],\n",
            "         [ 0.2317,  0.2481,  0.4466],\n",
            "         [-0.1376, -0.1691, -0.2872]]], device='cuda:0') tensor([-0.0069,  0.0237, -0.0522,  0.0362,  0.3120, -0.0165], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.91650390625\n",
            "act tensor([[11, 14, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.1361083984375 0.0\n",
            "tcost icost 0.2235107421875 0.0\n",
            "tcost icost 0.1796875 0.0\n",
            "tcost icost 0.2861328125 0.0\n",
            "tcost icost 0.2115478515625 0.0\n",
            "tcost icost 0.0161895751953125 0.0\n",
            "loss tensor([[0.8398]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.1270, -0.1474, -0.9551],\n",
            "         [ 0.5609,  0.4614,  0.2449],\n",
            "         [ 0.0236,  0.0418, -0.7885],\n",
            "         [ 0.1722, -0.1055, -0.2229],\n",
            "         [ 0.2402,  0.2573,  0.4505],\n",
            "         [-0.1408, -0.1779, -0.2937]]], device='cuda:0') tensor([ 0.2150, -0.1487,  0.0176,  0.0149,  0.2899, -0.0064], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.83984375\n",
            "act tensor([[11, 12, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.1363525390625 0.0\n",
            "tcost icost 0.123291015625 0.0\n",
            "tcost icost 0.2303466796875 0.0\n",
            "tcost icost 0.1802978515625 0.0\n",
            "tcost icost 0.263427734375 0.0\n",
            "tcost icost 0.16650390625 0.0\n",
            "loss tensor([[0.8364]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.2376,  0.0057, -0.9087],\n",
            "         [ 0.5799,  0.3796,  0.0502],\n",
            "         [ 0.0676,  0.0929, -0.7816],\n",
            "         [ 0.1717, -0.1183, -0.2676],\n",
            "         [ 0.2518,  0.2612,  0.4444],\n",
            "         [-0.1493, -0.2026, -0.3135]]], device='cuda:0') tensor([-0.0170,  0.0517, -0.1007,  0.0083,  0.2756,  0.0197], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.83642578125\n",
            "act tensor([[11, 14, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.1331787109375 0.0\n",
            "tcost icost 0.200927734375 0.0\n",
            "tcost icost 0.1177978515625 0.0\n",
            "tcost icost 0.243896484375 0.0\n",
            "tcost icost 0.19775390625 0.0\n",
            "tcost icost 0.0193023681640625 0.0\n",
            "loss tensor([[0.7285]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.1183, -0.1589, -0.9802],\n",
            "         [ 0.6127,  0.4791,  0.2209],\n",
            "         [ 0.0443, -0.0071, -0.8438],\n",
            "         [ 0.1956, -0.1018, -0.2853],\n",
            "         [ 0.2598,  0.2706,  0.4494],\n",
            "         [-0.1528, -0.2122, -0.3207]]], device='cuda:0') tensor([ 0.2676, -0.1835,  0.0341, -0.0270,  0.2519,  0.0311], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.728515625\n",
            "act tensor([[11, 14, 11,  4,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.12152099609375 0.0\n",
            "tcost icost 0.04913330078125 0.0\n",
            "tcost icost 0.016357421875 0.0\n",
            "tcost icost 0.07305908203125 0.0\n",
            "tcost icost 0.253662109375 0.0\n",
            "tcost icost 0.2166748046875 0.0\n",
            "loss tensor([[0.5264]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.1930, -0.0498, -0.9489],\n",
            "         [ 0.6262,  0.4230,  0.0901],\n",
            "         [ 0.0824,  0.0441, -0.8322],\n",
            "         [ 0.1800, -0.1311, -0.3428],\n",
            "         [ 0.2744,  0.2683,  0.4361],\n",
            "         [-0.1528, -0.2196, -0.3305]]], device='cuda:0') tensor([ 1.0154e-01, -2.8019e-02, -5.1624e-02, -1.4109e-04,  2.4063e-01,\n",
            "         3.3689e-02], device='cuda:0', grad_fn=<SqueezeBackward0>) 0.5263671875\n",
            "act tensor([[11, 14, 11, 11,  7,  4]], device='cuda:0')\n",
            "tcost icost 0.1485595703125 0.0\n",
            "tcost icost 0.2235107421875 0.0\n",
            "tcost icost 0.310791015625 0.0\n",
            "tcost icost 0.26220703125 0.0\n",
            "tcost icost 0.238525390625 0.0\n",
            "tcost icost 0.08380126953125 0.0\n",
            "loss tensor([[0.9985]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.1888, -0.0643, -0.9603],\n",
            "         [ 0.6406,  0.4269,  0.0904],\n",
            "         [ 0.0864,  0.0128, -0.8594],\n",
            "         [ 0.1922, -0.1262, -0.3770],\n",
            "         [ 0.2843,  0.2782,  0.4388],\n",
            "         [-0.1621, -0.2459, -0.3506]]], device='cuda:0') tensor([ 0.1306, -0.0833, -0.0398, -0.0298,  0.2180,  0.0640], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.99853515625\n",
            "act tensor([[11, 14, 11, 11, 12,  4]], device='cuda:0')\n",
            "tcost icost 0.150146484375 0.0\n",
            "tcost icost 0.1873779296875 0.0\n",
            "tcost icost 0.342041015625 0.0\n",
            "tcost icost 0.2159423828125 0.0\n",
            "tcost icost 0.26318359375 0.0\n",
            "tcost icost 0.12408447265625 0.0\n",
            "loss tensor([[0.9990]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.2347, -0.0014, -0.9391],\n",
            "         [ 0.6501,  0.3699, -0.0274],\n",
            "         [ 0.1186,  0.0392, -0.8618],\n",
            "         [ 0.1887, -0.1389, -0.4272],\n",
            "         [ 0.2983,  0.2914,  0.4441],\n",
            "         [-0.1692, -0.2666, -0.3665]]], device='cuda:0') tensor([ 0.0390,  0.0062, -0.1151, -0.0300,  0.1889,  0.0866], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.9990234375\n",
            "act tensor([[11, 14, 11, 11, 12,  4]], device='cuda:0')\n",
            "tcost icost 0.1427001953125 0.0\n",
            "tcost icost 0.2431640625 0.0\n",
            "tcost icost 0.2174072265625 0.0\n",
            "tcost icost 0.2734375 0.0\n",
            "tcost icost 0.21923828125 0.0\n",
            "tcost icost 0.0517578125 0.0\n",
            "loss tensor([[0.9116]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.1716, -0.0929, -0.9808],\n",
            "         [ 0.6767,  0.4228,  0.0590],\n",
            "         [ 0.1001, -0.0161, -0.8944],\n",
            "         [ 0.2016, -0.1300, -0.4580],\n",
            "         [ 0.3066,  0.2999,  0.4480],\n",
            "         [-0.1737, -0.2806, -0.3768]]], device='cuda:0') tensor([ 0.1904, -0.1479, -0.0469, -0.0563,  0.1665,  0.1028], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.91162109375\n",
            "act tensor([[11, 14, 11, 11, 12,  4]], device='cuda:0')\n",
            "tcost icost 0.1431884765625 0.0\n",
            "tcost icost 0.1163330078125 0.0\n",
            "tcost icost 0.2066650390625 0.0\n",
            "tcost icost 0.1485595703125 0.0\n",
            "tcost icost 0.261474609375 0.0\n",
            "tcost icost 0.2081298828125 0.0\n",
            "loss tensor([[0.8179]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.2804,  0.0610, -0.9278],\n",
            "         [ 0.6726,  0.3205, -0.1493],\n",
            "         [ 0.1474,  0.0434, -0.8825],\n",
            "         [ 0.1925, -0.1450, -0.5087],\n",
            "         [ 0.3220,  0.3040,  0.4425],\n",
            "         [-0.1779, -0.2989, -0.3928]]], device='cuda:0') tensor([-0.0518,  0.0742, -0.1684, -0.0414,  0.1501,  0.1207], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.81787109375\n",
            "act tensor([[11, 14, 11, 11, 12,  4]], device='cuda:0')\n",
            "tcost icost 0.1212158203125 0.0\n",
            "tcost icost 0.1505126953125 0.0\n",
            "tcost icost 0.037353515625 0.0\n",
            "tcost icost 0.09234619140625 0.0\n",
            "tcost icost 0.1671142578125 0.0\n",
            "tcost icost 0.0276947021484375 0.0\n",
            "loss tensor([[0.4805]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.1624, -0.0964, -0.9820],\n",
            "         [ 0.7205,  0.4218,  0.0104],\n",
            "         [ 0.1107, -0.0307, -0.9188],\n",
            "         [ 0.2169, -0.1205, -0.5127],\n",
            "         [ 0.3294,  0.3130,  0.4492],\n",
            "         [-0.1818, -0.3114, -0.4015]]], device='cuda:0') tensor([ 0.2377, -0.1332, -0.0349, -0.0737,  0.1269,  0.1351], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.48046875\n",
            "act tensor([[11, 14, 11, 11, 12, 10]], device='cuda:0')\n",
            "tcost icost 0.1297607421875 0.0\n",
            "tcost icost 0.08575439453125 0.0\n",
            "tcost icost 0.15625 0.0\n",
            "tcost icost 0.1265869140625 0.0\n",
            "tcost icost 0.2440185546875 0.0\n",
            "tcost icost 0.2088623046875 0.0\n",
            "loss tensor([[0.7095]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.2715,  0.0590, -0.9292],\n",
            "         [ 0.7068,  0.3227, -0.1938],\n",
            "         [ 0.1612,  0.0330, -0.9066],\n",
            "         [ 0.2149, -0.1229, -0.5573],\n",
            "         [ 0.3436,  0.3061,  0.4293],\n",
            "         [-0.1861, -0.3321, -0.4186]]], device='cuda:0') tensor([-0.0041,  0.0886, -0.1651, -0.0691,  0.1297,  0.1542], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.70947265625\n",
            "act tensor([[11, 14, 11, 11, 12, 10]], device='cuda:0')\n",
            "tcost icost 0.1365966796875 0.0\n",
            "tcost icost 0.1917724609375 0.0\n",
            "tcost icost 0.06268310546875 0.0\n",
            "tcost icost 0.12396240234375 0.0\n",
            "tcost icost 0.1873779296875 0.0\n",
            "tcost icost 0.0287017822265625 0.0\n",
            "loss tensor([[0.5903]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.1586, -0.0959, -0.9827],\n",
            "         [ 0.7598,  0.4360, -0.0182],\n",
            "         [ 0.1145, -0.0554, -0.9485],\n",
            "         [ 0.2412, -0.0941, -0.5619],\n",
            "         [ 0.3508,  0.3144,  0.4355],\n",
            "         [-0.1897, -0.3448, -0.4270]]], device='cuda:0') tensor([ 0.2539, -0.1495, -0.0024, -0.1028,  0.1068,  0.1685], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.59033203125\n",
            "act tensor([[11, 14, 11, 11, 12, 10]], device='cuda:0')\n",
            "tcost icost 0.1253662109375 0.0\n",
            "tcost icost 0.06793212890625 0.0\n",
            "tcost icost 0.08636474609375 0.0\n",
            "tcost icost 0.08685302734375 0.0\n",
            "tcost icost 0.2261962890625 0.0\n",
            "tcost icost 0.22021484375 0.0\n",
            "loss tensor([[0.5981]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.2502,  0.0375, -0.9372],\n",
            "         [ 0.7454,  0.3544, -0.1885],\n",
            "         [ 0.1621,  0.0043, -0.9371],\n",
            "         [ 0.2348, -0.0969, -0.6013],\n",
            "         [ 0.3630,  0.2996,  0.4059],\n",
            "         [-0.1887, -0.3518, -0.4353]]], device='cuda:0') tensor([ 0.0453,  0.0419, -0.1191, -0.0898,  0.1201,  0.1700], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.59814453125\n",
            "act tensor([[11, 14, 11, 11, 12, 10]], device='cuda:0')\n",
            "tcost icost 0.1434326171875 0.0\n",
            "tcost icost 0.234619140625 0.0\n",
            "tcost icost 0.1864013671875 0.0\n",
            "tcost icost 0.253662109375 0.0\n",
            "tcost icost 0.2314453125 0.0\n",
            "tcost icost 0.06829833984375 0.0\n",
            "loss tensor([[0.8828]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.1750, -0.0690, -0.9821],\n",
            "         [ 0.7786,  0.4180, -0.0932],\n",
            "         [ 0.1324, -0.0493, -0.9647],\n",
            "         [ 0.2403, -0.0855, -0.6362],\n",
            "         [ 0.3731,  0.3124,  0.4148],\n",
            "         [-0.1943, -0.3738, -0.4494]]], device='cuda:0') tensor([ 0.2104, -0.1394, -0.0386, -0.0995,  0.0890,  0.1940], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8828125\n",
            "search tensor([[[ 0.3731,  0.3124,  0.4148],\n",
            "         [-0.1943, -0.3738, -0.4494],\n",
            "         [-0.5037, -0.1063, -0.2600],\n",
            "         [-0.0765,  0.3861,  0.2872],\n",
            "         [-0.1666, -0.4118, -0.4831],\n",
            "         [-0.4086,  0.2927,  0.0510]]], device='cuda:0') tensor([ 0.0890,  0.1940,  0.1485,  0.3409,  0.6613, -0.1248], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[12, 10,  4,  3, 10,  3]], device='cuda:0')\n",
            "tcost icost 0.171630859375 0.0\n",
            "tcost icost 0.1689453125 0.0\n",
            "tcost icost 0.2130126953125 0.0\n",
            "tcost icost 0.166748046875 0.0\n",
            "tcost icost 0.0865478515625 0.0\n",
            "tcost icost 0.028839111328125 0.0\n",
            "loss tensor([[0.6914]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.4167,  0.4300,  0.5730],\n",
            "         [-0.2385, -0.5216, -0.5307],\n",
            "         [-0.4853, -0.0970, -0.2835],\n",
            "         [-0.0577,  0.4123,  0.3129],\n",
            "         [-0.1705, -0.4282, -0.4939],\n",
            "         [-0.4125,  0.2828,  0.0398]]], device='cuda:0') tensor([-0.0630,  0.3241,  0.0930,  0.2855,  0.6760, -0.1129], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.69140625\n",
            "act tensor([[12, 10,  4,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.134521484375 0.0\n",
            "tcost icost 0.25146484375 0.0\n",
            "tcost icost 0.2344970703125 0.0\n",
            "tcost icost 0.1854248046875 0.0\n",
            "tcost icost 0.134765625 0.0\n",
            "tcost icost -0.0038967132568359375 0.0\n",
            "loss tensor([[0.7720]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.4780,  0.3833,  0.4955],\n",
            "         [-0.2335, -0.4184, -0.5277],\n",
            "         [-0.4865, -0.2059, -0.3686],\n",
            "         [-0.0316,  0.4513,  0.3631],\n",
            "         [-0.1745, -0.4428, -0.5011],\n",
            "         [-0.4132,  0.2809,  0.0376]]], device='cuda:0') tensor([ 0.1499,  0.1909,  0.1807,  0.2110,  0.6873, -0.1111], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.77197265625\n",
            "act tensor([[12, 10,  4,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1651611328125 0.0\n",
            "tcost icost 0.12005615234375 0.0\n",
            "tcost icost 0.1334228515625 0.0\n",
            "tcost icost 0.16064453125 0.0\n",
            "tcost icost 0.1553955078125 0.0\n",
            "tcost icost 0.034942626953125 0.0\n",
            "loss tensor([[0.6211]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.4471,  0.4727,  0.6349],\n",
            "         [-0.2564, -0.5727, -0.5764],\n",
            "         [-0.4695, -0.1942, -0.3881],\n",
            "         [-0.0191,  0.4685,  0.3811],\n",
            "         [-0.1774, -0.4618, -0.5134],\n",
            "         [-0.4176,  0.2686,  0.0236]]], device='cuda:0') tensor([-0.0338,  0.3348,  0.1218,  0.1605,  0.7017, -0.0962], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.62109375\n",
            "act tensor([[12, 10,  4,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1474609375 0.0\n",
            "tcost icost 0.258056640625 0.0\n",
            "tcost icost 0.25439453125 0.0\n",
            "tcost icost 0.273193359375 0.0\n",
            "tcost icost 0.1806640625 0.0\n",
            "tcost icost 0.0004394054412841797 0.0\n",
            "loss tensor([[0.9043]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.4966,  0.4451,  0.5929],\n",
            "         [-0.2478, -0.5315, -0.5907],\n",
            "         [-0.4580, -0.2698, -0.4503],\n",
            "         [-0.0030,  0.4846,  0.3978],\n",
            "         [-0.1756, -0.4633, -0.5168],\n",
            "         [-0.4186,  0.2655,  0.0202]]], device='cuda:0') tensor([ 0.0880,  0.2375,  0.1732,  0.1066,  0.6963, -0.0931], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.904296875\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1715087890625 0.0\n",
            "tcost icost 0.1845703125 0.0\n",
            "tcost icost 0.294677734375 0.0\n",
            "tcost icost 0.2222900390625 0.0\n",
            "tcost icost 0.18408203125 0.0\n",
            "tcost icost 0.01390838623046875 0.0\n",
            "loss tensor([[0.8672]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.4636,  0.4824,  0.6419],\n",
            "         [-0.2515, -0.6206, -0.5868],\n",
            "         [-0.4389, -0.2614, -0.4715],\n",
            "         [ 0.0106,  0.4990,  0.4137],\n",
            "         [-0.1785, -0.4796, -0.5248],\n",
            "         [-0.4210,  0.2582,  0.0122]]], device='cuda:0') tensor([-0.0523,  0.3544,  0.1116,  0.0563,  0.7085, -0.0846], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8671875\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1405029296875 0.0\n",
            "tcost icost 0.243408203125 0.0\n",
            "tcost icost 0.19580078125 0.0\n",
            "tcost icost 0.2412109375 0.0\n",
            "tcost icost 0.15771484375 0.0\n",
            "tcost icost -0.003692626953125 0.0\n",
            "loss tensor([[0.7954]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.5321,  0.4379,  0.5784],\n",
            "         [-0.2420, -0.5737, -0.6066],\n",
            "         [-0.4273, -0.3811, -0.5436],\n",
            "         [ 0.0275,  0.5135,  0.4287],\n",
            "         [-0.1737, -0.4726, -0.5266],\n",
            "         [-0.4218,  0.2560,  0.0097]]], device='cuda:0') tensor([ 0.1561,  0.1848,  0.2255,  0.0023,  0.6938, -0.0824], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.79541015625\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1639404296875 0.0\n",
            "tcost icost 0.112060546875 0.0\n",
            "tcost icost 0.1383056640625 0.0\n",
            "tcost icost 0.1602783203125 0.0\n",
            "tcost icost 0.2159423828125 0.0\n",
            "tcost icost 0.07000732421875 0.0\n",
            "loss tensor([[0.6768]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.4787,  0.4913,  0.6485],\n",
            "         [-0.2442, -0.6439, -0.5958],\n",
            "         [-0.4157, -0.3623, -0.5552],\n",
            "         [ 0.0376,  0.5134,  0.4097],\n",
            "         [-0.1641, -0.4619, -0.5327],\n",
            "         [-0.4287,  0.2338, -0.0146]]], device='cuda:0') tensor([-0.0178,  0.3248,  0.1608, -0.0181,  0.6749, -0.0564], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6767578125\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1533203125 0.0\n",
            "tcost icost 0.259521484375 0.0\n",
            "tcost icost 0.25634765625 0.0\n",
            "tcost icost 0.298828125 0.0\n",
            "tcost icost 0.19873046875 0.0\n",
            "tcost icost 0.00696563720703125 0.0\n",
            "loss tensor([[0.9463]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.5125,  0.4748,  0.6196],\n",
            "         [-0.2288, -0.6300, -0.6096],\n",
            "         [-0.3953, -0.4110, -0.5899],\n",
            "         [ 0.0500,  0.5145,  0.3939],\n",
            "         [-0.1578, -0.4558, -0.5365],\n",
            "         [-0.4296,  0.2306, -0.0180]]], device='cuda:0') tensor([ 0.0654,  0.2467,  0.1885, -0.0411,  0.6583, -0.0529], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.9462890625\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1705322265625 0.0\n",
            "tcost icost 0.2047119140625 0.0\n",
            "tcost icost 0.304931640625 0.0\n",
            "tcost icost 0.23095703125 0.0\n",
            "tcost icost 0.22802734375 0.0\n",
            "tcost icost 0.0419921875 0.0\n",
            "loss tensor([[0.9448]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.4822,  0.5024,  0.6545],\n",
            "         [-0.2354, -0.6563, -0.6025],\n",
            "         [-0.3815, -0.3923, -0.6034],\n",
            "         [ 0.0613,  0.5120,  0.3697],\n",
            "         [-0.1491, -0.4462, -0.5411],\n",
            "         [-0.4343,  0.2147, -0.0344]]], device='cuda:0') tensor([-0.0333,  0.3427,  0.1177, -0.0550,  0.6402, -0.0347], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.94482421875\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.147705078125 0.0\n",
            "tcost icost 0.255615234375 0.0\n",
            "tcost icost 0.2276611328125 0.0\n",
            "tcost icost 0.29833984375 0.0\n",
            "tcost icost 0.19287109375 0.0\n",
            "tcost icost 0.007228851318359375 0.0\n",
            "loss tensor([[0.9111]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.5352,  0.4726,  0.6061],\n",
            "         [-0.2124, -0.6354, -0.6218],\n",
            "         [-0.3628, -0.4548, -0.6334],\n",
            "         [ 0.0747,  0.5130,  0.3538],\n",
            "         [-0.1429, -0.4401, -0.5445],\n",
            "         [-0.4352,  0.2114, -0.0378]]], device='cuda:0') tensor([ 0.1254,  0.2117,  0.1668, -0.0797,  0.6243, -0.0312], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.9111328125\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1693115234375 0.0\n",
            "tcost icost 0.1494140625 0.0\n",
            "tcost icost 0.2392578125 0.0\n",
            "tcost icost 0.19677734375 0.0\n",
            "tcost icost 0.234375 0.0\n",
            "tcost icost 0.09686279296875 0.0\n",
            "loss tensor([[0.8521]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.4876,  0.5126,  0.6636],\n",
            "         [-0.2360, -0.6674, -0.6076],\n",
            "         [-0.3501, -0.4346, -0.6475],\n",
            "         [ 0.0856,  0.5114,  0.3317],\n",
            "         [-0.1314, -0.4272, -0.5499],\n",
            "         [-0.4419,  0.1872, -0.0623]]], device='cuda:0') tensor([-0.0351,  0.3509,  0.0926, -0.1018,  0.6029, -0.0044], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.85205078125\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.14697265625 0.0\n",
            "tcost icost 0.251708984375 0.0\n",
            "tcost icost 0.2086181640625 0.0\n",
            "tcost icost 0.301025390625 0.0\n",
            "tcost icost 0.197509765625 0.0\n",
            "tcost icost 0.01189422607421875 0.0\n",
            "loss tensor([[0.8984]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.5452,  0.4791,  0.6103],\n",
            "         [-0.2108, -0.6463, -0.6272],\n",
            "         [-0.3345, -0.5007, -0.6680],\n",
            "         [ 0.1005,  0.5113,  0.3122],\n",
            "         [-0.1226, -0.4173, -0.5531],\n",
            "         [-0.4442,  0.1787, -0.0704]]], device='cuda:0') tensor([ 0.1534,  0.1934,  0.1532, -0.1257,  0.5832,  0.0051], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.8984375\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.16455078125 0.0\n",
            "tcost icost 0.118408203125 0.0\n",
            "tcost icost 0.2027587890625 0.0\n",
            "tcost icost 0.1705322265625 0.0\n",
            "tcost icost 0.2418212890625 0.0\n",
            "tcost icost 0.1473388671875 0.0\n",
            "loss tensor([[0.8052]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.4809,  0.5277,  0.6795],\n",
            "         [-0.2425, -0.6793, -0.6110],\n",
            "         [-0.3228, -0.4616, -0.6830],\n",
            "         [ 0.1092,  0.5004,  0.2705],\n",
            "         [-0.1100, -0.4050, -0.5594],\n",
            "         [-0.4496,  0.1557, -0.0932]]], device='cuda:0') tensor([-0.0295,  0.3630,  0.0544, -0.1221,  0.5613,  0.0289], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.80517578125\n",
            "act tensor([[12, 10, 10,  3, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1490478515625 0.0\n",
            "tcost icost 0.2486572265625 0.0\n",
            "tcost icost 0.189208984375 0.0\n",
            "tcost icost 0.29052734375 0.0\n",
            "tcost icost 0.1953125 0.0\n",
            "tcost icost 0.02044677734375 0.0\n",
            "loss tensor([[0.8784]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.5470,  0.4904,  0.6203],\n",
            "         [-0.2163, -0.6578, -0.6298],\n",
            "         [-0.3116, -0.5462, -0.6962],\n",
            "         [ 0.1258,  0.5044,  0.2588],\n",
            "         [-0.1002, -0.3939, -0.5624],\n",
            "         [-0.4525,  0.1431, -0.1050]]], device='cuda:0') tensor([ 0.1812,  0.1695,  0.1508, -0.1573,  0.5409,  0.0431], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.87841796875\n",
            "act tensor([[12, 10, 10, 12, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.15771484375 0.0\n",
            "tcost icost 0.08636474609375 0.0\n",
            "tcost icost 0.1490478515625 0.0\n",
            "tcost icost 0.133056640625 0.0\n",
            "tcost icost 0.2373046875 0.0\n",
            "tcost icost 0.1837158203125 0.0\n",
            "loss tensor([[0.7173]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.4770,  0.5400,  0.6925],\n",
            "         [-0.2474, -0.6866, -0.6143],\n",
            "         [-0.2970, -0.5040, -0.7152],\n",
            "         [ 0.1323,  0.4894,  0.2110],\n",
            "         [-0.0907, -0.3884, -0.5709],\n",
            "         [-0.4557,  0.1242, -0.1245]]], device='cuda:0') tensor([-0.0166,  0.3503,  0.0432, -0.1455,  0.5263,  0.0602], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.71728515625\n",
            "act tensor([[12, 10, 10, 12, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.15380859375 0.0\n",
            "tcost icost 0.2548828125 0.0\n",
            "tcost icost 0.2086181640625 0.0\n",
            "tcost icost 0.294677734375 0.0\n",
            "tcost icost 0.196533203125 0.0\n",
            "tcost icost 0.038543701171875 0.0\n",
            "loss tensor([[0.9189]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.5336,  0.5100,  0.6433],\n",
            "         [-0.2237, -0.6687, -0.6315],\n",
            "         [-0.2917, -0.5673, -0.7194],\n",
            "         [ 0.1493,  0.4903,  0.1917],\n",
            "         [-0.0803, -0.3767, -0.5741],\n",
            "         [-0.4594,  0.1070, -0.1397]]], device='cuda:0') tensor([ 0.1586,  0.1802,  0.1208, -0.1759,  0.5060,  0.0791], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.9189453125\n",
            "act tensor([[12, 10, 10, 12, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.163330078125 0.0\n",
            "tcost icost 0.10797119140625 0.0\n",
            "tcost icost 0.20703125 0.0\n",
            "tcost icost 0.158935546875 0.0\n",
            "tcost icost 0.2418212890625 0.0\n",
            "tcost icost 0.19873046875 0.0\n",
            "loss tensor([[0.8198]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.4677,  0.5457,  0.6953],\n",
            "         [-0.2512, -0.6914, -0.6197],\n",
            "         [-0.2736, -0.5319, -0.7404],\n",
            "         [ 0.1573,  0.4774,  0.1500],\n",
            "         [-0.0724, -0.3731, -0.5831],\n",
            "         [-0.4618,  0.0882, -0.1586]]], device='cuda:0') tensor([-0.0372,  0.3546,  0.0146, -0.1718,  0.4945,  0.0958], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.81982421875\n",
            "act tensor([[12, 10, 10, 12, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1461181640625 0.0\n",
            "tcost icost 0.2493896484375 0.0\n",
            "tcost icost 0.1685791015625 0.0\n",
            "tcost icost 0.26416015625 0.0\n",
            "tcost icost 0.1905517578125 0.0\n",
            "tcost icost 0.04852294921875 0.0\n",
            "loss tensor([[0.8540]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.5415,  0.5053,  0.6303],\n",
            "         [-0.2222, -0.6689, -0.6381],\n",
            "         [-0.2803, -0.6169, -0.7331],\n",
            "         [ 0.1786,  0.4894,  0.1521],\n",
            "         [-0.0636, -0.3638, -0.5861],\n",
            "         [-0.4647,  0.0725, -0.1719]]], device='cuda:0') tensor([ 0.2007,  0.1262,  0.1617, -0.2193,  0.4777,  0.1125], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.85400390625\n",
            "act tensor([[12, 10, 10, 12, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.1517333984375 0.0\n",
            "tcost icost 0.05810546875 0.0\n",
            "tcost icost 0.07421875 0.0\n",
            "tcost icost 0.0753173828125 0.0\n",
            "tcost icost 0.2296142578125 0.0\n",
            "tcost icost 0.1885986328125 0.0\n",
            "loss tensor([[0.5811]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.4731,  0.5452,  0.6921],\n",
            "         [-0.2557, -0.6942, -0.6214],\n",
            "         [-0.2575, -0.5805, -0.7568],\n",
            "         [ 0.1820,  0.4723,  0.1033],\n",
            "         [-0.0701, -0.3842, -0.6049],\n",
            "         [-0.4611,  0.0743, -0.1762]]], device='cuda:0') tensor([ 0.0187,  0.2950,  0.0457, -0.1999,  0.4925,  0.1045], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.5810546875\n",
            "act tensor([[12, 10, 10, 12, 10,  4]], device='cuda:0')\n",
            "tcost icost 0.164306640625 0.0\n",
            "tcost icost 0.24462890625 0.0\n",
            "tcost icost 0.29248046875 0.0\n",
            "tcost icost 0.307373046875 0.0\n",
            "tcost icost 0.2298583984375 0.0\n",
            "tcost icost 0.109130859375 0.0\n",
            "loss tensor([[1.0605]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.5025,  0.5332,  0.6662],\n",
            "         [-0.2384, -0.6832, -0.6355],\n",
            "         [-0.2546, -0.6004, -0.7581],\n",
            "         [ 0.1933,  0.4599,  0.0604],\n",
            "         [-0.0556, -0.3678, -0.6082],\n",
            "         [-0.4650,  0.0506, -0.1964]]], device='cuda:0') tensor([ 0.0995,  0.2045,  0.0699, -0.2103,  0.4644,  0.1277], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 1.060546875\n",
            "search tensor([[[-0.0556, -0.3678, -0.6082],\n",
            "         [-0.4650,  0.0506, -0.1964],\n",
            "         [ 0.3735, -0.4513,  0.3127],\n",
            "         [ 0.4269, -0.0259,  0.3397],\n",
            "         [ 0.0269, -0.4110,  0.1683],\n",
            "         [ 0.2159,  0.2194, -0.2246]]], device='cuda:0') tensor([ 0.4644,  0.1277,  0.0447,  0.7666, -0.1953, -0.0226], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "act tensor([[10,  4,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.12841796875 0.0\n",
            "tcost icost 0.11090087890625 0.0\n",
            "tcost icost 0.29443359375 0.0\n",
            "tcost icost 0.03155517578125 0.0\n",
            "tcost icost -0.03289794921875 0.0\n",
            "tcost icost 0.0136260986328125 0.0\n",
            "loss tensor([[0.4763]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.0766, -0.1884, -0.5729],\n",
            "         [-0.4897, -0.0866, -0.3038],\n",
            "         [ 0.3922, -0.4465,  0.2939],\n",
            "         [ 0.4383, -0.0025,  0.3540],\n",
            "         [ 0.0259, -0.4126,  0.1656],\n",
            "         [ 0.2155,  0.2187, -0.2256]]], device='cuda:0') tensor([ 0.2640,  0.2519,  0.0080,  0.7309, -0.1934, -0.0218], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.476318359375\n",
            "act tensor([[11,  4,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.149658203125 0.0\n",
            "tcost icost 0.18017578125 0.0\n",
            "tcost icost 0.283203125 0.0\n",
            "tcost icost -0.0111541748046875 0.0\n",
            "tcost icost -0.027496337890625 0.0\n",
            "tcost icost 0.01369476318359375 0.0\n",
            "loss tensor([[0.5229]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.0873, -0.1925, -0.6086],\n",
            "         [-0.4739, -0.0260, -0.2796],\n",
            "         [ 0.3726, -0.4935,  0.2345],\n",
            "         [ 0.4478,  0.0207,  0.3691],\n",
            "         [ 0.0249, -0.4139,  0.1632],\n",
            "         [ 0.2149,  0.2178, -0.2271]]], device='cuda:0') tensor([ 0.2547,  0.1812,  0.0652,  0.6978, -0.1921, -0.0207], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.52294921875\n",
            "act tensor([[11,  4,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.15283203125 0.0\n",
            "tcost icost 0.1748046875 0.0\n",
            "tcost icost 0.314697265625 0.0\n",
            "tcost icost 0.05987548828125 0.0\n",
            "tcost icost -0.0267333984375 0.0\n",
            "tcost icost 0.01462554931640625 0.0\n",
            "loss tensor([[0.5996]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.1158, -0.1702, -0.6225],\n",
            "         [-0.4630, -0.0092, -0.2809],\n",
            "         [ 0.3637, -0.5242,  0.1816],\n",
            "         [ 0.4588,  0.0467,  0.3859],\n",
            "         [ 0.0241, -0.4153,  0.1608],\n",
            "         [ 0.2146,  0.2173, -0.2279]]], device='cuda:0') tensor([ 0.2206,  0.1494,  0.0905,  0.6591, -0.1903, -0.0201], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.599609375\n",
            "act tensor([[11,  4,  0,  7,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1636962890625 0.0\n",
            "tcost icost 0.1873779296875 0.0\n",
            "tcost icost 0.302734375 0.0\n",
            "tcost icost 0.09619140625 0.0\n",
            "tcost icost -0.0238800048828125 0.0\n",
            "tcost icost 0.015472412109375 0.0\n",
            "loss tensor([[0.6416]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.1486, -0.1383, -0.6270],\n",
            "         [-0.4540, -0.0109, -0.2955],\n",
            "         [ 0.3614, -0.5426,  0.1399],\n",
            "         [ 0.4681,  0.0663,  0.3977],\n",
            "         [ 0.0233, -0.4167,  0.1583],\n",
            "         [ 0.2142,  0.2168, -0.2287]]], device='cuda:0') tensor([ 0.1819,  0.1339,  0.0986,  0.6263, -0.1887, -0.0195], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6416015625\n",
            "act tensor([[11,  4,  0,  6,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1708984375 0.0\n",
            "tcost icost 0.2059326171875 0.0\n",
            "tcost icost 0.287841796875 0.0\n",
            "tcost icost 0.10882568359375 0.0\n",
            "tcost icost -0.021636962890625 0.0\n",
            "tcost icost 0.0161895751953125 0.0\n",
            "loss tensor([[0.6646]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.1736, -0.1167, -0.6378],\n",
            "         [-0.4437, -0.0082, -0.3076],\n",
            "         [ 0.3572, -0.5605,  0.0977],\n",
            "         [ 0.4773,  0.0865,  0.4098],\n",
            "         [ 0.0224, -0.4182,  0.1558],\n",
            "         [ 0.2140,  0.2164, -0.2292]]], device='cuda:0') tensor([ 0.1655,  0.1143,  0.1072,  0.5916, -0.1867, -0.0189], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.66455078125\n",
            "act tensor([[11,  4,  0,  6,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.171142578125 0.0\n",
            "tcost icost 0.2125244140625 0.0\n",
            "tcost icost 0.283935546875 0.0\n",
            "tcost icost 0.1258544921875 0.0\n",
            "tcost icost -0.0191802978515625 0.0\n",
            "tcost icost 0.01690673828125 0.0\n",
            "loss tensor([[0.6812]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.2053, -0.0844, -0.6396],\n",
            "         [-0.4367, -0.0304, -0.3367],\n",
            "         [ 0.3619, -0.5670,  0.0687],\n",
            "         [ 0.4854,  0.1012,  0.4177],\n",
            "         [ 0.0215, -0.4197,  0.1532],\n",
            "         [ 0.2137,  0.2160, -0.2297]]], device='cuda:0') tensor([ 0.1373,  0.1146,  0.0956,  0.5627, -0.1847, -0.0183], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.68115234375\n",
            "act tensor([[11,  4,  0,  6,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.16845703125 0.0\n",
            "tcost icost 0.2275390625 0.0\n",
            "tcost icost 0.271728515625 0.0\n",
            "tcost icost 0.1361083984375 0.0\n",
            "tcost icost -0.0166168212890625 0.0\n",
            "tcost icost 0.0182037353515625 0.0\n",
            "loss tensor([[0.6924]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.2148, -0.0782, -0.6642],\n",
            "         [-0.4229, -0.0192, -0.3460],\n",
            "         [ 0.3511, -0.5865,  0.0232],\n",
            "         [ 0.4943,  0.1222,  0.4302],\n",
            "         [ 0.0204, -0.4215,  0.1498],\n",
            "         [ 0.2131,  0.2150, -0.2311]]], device='cuda:0') tensor([ 0.1485,  0.0869,  0.1094,  0.5251, -0.1818, -0.0166], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.6923828125\n",
            "act tensor([[11,  4,  0,  6,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.169677734375 0.0\n",
            "tcost icost 0.21826171875 0.0\n",
            "tcost icost 0.282958984375 0.0\n",
            "tcost icost 0.14697265625 0.0\n",
            "tcost icost -0.01476287841796875 0.0\n",
            "tcost icost 0.0192413330078125 0.0\n",
            "loss tensor([[0.7041]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.2439, -0.0469, -0.6640],\n",
            "         [-0.4173, -0.0473, -0.3772],\n",
            "         [ 0.3540, -0.5922, -0.0075],\n",
            "         [ 0.5017,  0.1371,  0.4382],\n",
            "         [ 0.0190, -0.4240,  0.1454],\n",
            "         [ 0.2129,  0.2145, -0.2320]]], device='cuda:0') tensor([ 0.1236,  0.0904,  0.0948,  0.4944, -0.1778, -0.0155], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7041015625\n",
            "act tensor([[11,  4,  0,  6,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1668701171875 0.0\n",
            "tcost icost 0.2330322265625 0.0\n",
            "tcost icost 0.2724609375 0.0\n",
            "tcost icost 0.1612548828125 0.0\n",
            "tcost icost -0.0120391845703125 0.0\n",
            "tcost icost 0.0213623046875 0.0\n",
            "loss tensor([[0.7197]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.2599, -0.0282, -0.6762],\n",
            "         [-0.4060, -0.0596, -0.4013],\n",
            "         [ 0.3526, -0.6002, -0.0404],\n",
            "         [ 0.5089,  0.1523,  0.4464],\n",
            "         [ 0.0175, -0.4264,  0.1410],\n",
            "         [ 0.2127,  0.2140, -0.2328]]], device='cuda:0') tensor([ 0.1193,  0.0811,  0.0876,  0.4623, -0.1737, -0.0144], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7197265625\n",
            "act tensor([[11,  4,  0,  6,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1663818359375 0.0\n",
            "tcost icost 0.2344970703125 0.0\n",
            "tcost icost 0.274169921875 0.0\n",
            "tcost icost 0.1715087890625 0.0\n",
            "tcost icost -0.01000213623046875 0.0\n",
            "tcost icost 0.02264404296875 0.0\n",
            "loss tensor([[0.7314]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.2749, -0.0089, -0.6863],\n",
            "         [-0.3948, -0.0729, -0.4253],\n",
            "         [ 0.3512, -0.6072, -0.0724],\n",
            "         [ 0.5154,  0.1671,  0.4548],\n",
            "         [ 0.0160, -0.4295,  0.1361],\n",
            "         [ 0.2133,  0.2145, -0.2322]]], device='cuda:0') tensor([ 0.1149,  0.0730,  0.0801,  0.4309, -0.1682, -0.0144], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7314453125\n",
            "act tensor([[11,  4,  4,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.165771484375 0.0\n",
            "tcost icost 0.236083984375 0.0\n",
            "tcost icost 0.279541015625 0.0\n",
            "tcost icost 0.1622314453125 0.0\n",
            "tcost icost -0.01154327392578125 0.0\n",
            "tcost icost 0.019989013671875 0.0\n",
            "loss tensor([[0.7271]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.2728, -0.0059, -0.7127],\n",
            "         [-0.3782, -0.0515, -0.4303],\n",
            "         [ 0.3284, -0.6289, -0.1266],\n",
            "         [ 0.5216,  0.1871,  0.4670],\n",
            "         [ 0.0147, -0.4313,  0.1325],\n",
            "         [ 0.2127,  0.2134, -0.2337]]], device='cuda:0') tensor([ 0.1396,  0.0329,  0.1020,  0.3903, -0.1651, -0.0126], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.72705078125\n",
            "act tensor([[11,  4, 10,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1685791015625 0.0\n",
            "tcost icost 0.2132568359375 0.0\n",
            "tcost icost 0.301513671875 0.0\n",
            "tcost icost 0.139404296875 0.0\n",
            "tcost icost -0.0142822265625 0.0\n",
            "tcost icost 0.019683837890625 0.0\n",
            "loss tensor([[0.7085]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.2806,  0.0052, -0.7259],\n",
            "         [-0.3667, -0.0379, -0.4352],\n",
            "         [ 0.2949, -0.6545, -0.1925],\n",
            "         [ 0.5258,  0.2108,  0.4828],\n",
            "         [ 0.0134, -0.4337,  0.1281],\n",
            "         [ 0.2125,  0.2129, -0.2344]]], device='cuda:0') tensor([ 0.1454,  0.0024,  0.1324,  0.3453, -0.1612, -0.0115], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.70849609375\n",
            "act tensor([[11,  4, 10,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1693115234375 0.0\n",
            "tcost icost 0.1995849609375 0.0\n",
            "tcost icost 0.299072265625 0.0\n",
            "tcost icost 0.1578369140625 0.0\n",
            "tcost icost -0.0113525390625 0.0\n",
            "tcost icost 0.02020263671875 0.0\n",
            "loss tensor([[0.7109]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.3288,  0.0606, -0.6976],\n",
            "         [-0.3683, -0.1077, -0.4875],\n",
            "         [ 0.2997, -0.6523, -0.2181],\n",
            "         [ 0.5289,  0.2232,  0.4900],\n",
            "         [ 0.0117, -0.4366,  0.1232],\n",
            "         [ 0.2130,  0.2132, -0.2340]]], device='cuda:0') tensor([ 0.0767,  0.0546,  0.1045,  0.3148, -0.1558, -0.0113], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7109375\n",
            "act tensor([[11,  4, 10,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.15966796875 0.0\n",
            "tcost icost 0.242919921875 0.0\n",
            "tcost icost 0.243408203125 0.0\n",
            "tcost icost 0.163818359375 0.0\n",
            "tcost icost -0.00641632080078125 0.0\n",
            "tcost icost 0.02337646484375 0.0\n",
            "loss tensor([[0.7046]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.2736,  0.0196, -0.7550],\n",
            "         [-0.3449, -0.0314, -0.4657],\n",
            "         [ 0.2419, -0.6855, -0.2960],\n",
            "         [ 0.5297,  0.2483,  0.5073],\n",
            "         [ 0.0106, -0.4382,  0.1197],\n",
            "         [ 0.2123,  0.2120, -0.2358]]], device='cuda:0') tensor([ 0.1802, -0.0358,  0.1682,  0.2636, -0.1534, -0.0094], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.70458984375\n",
            "act tensor([[11,  4, 10,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1708984375 0.0\n",
            "tcost icost 0.162109375 0.0\n",
            "tcost icost 0.206298828125 0.0\n",
            "tcost icost 0.16357421875 0.0\n",
            "tcost icost -0.006229400634765625 0.0\n",
            "tcost icost 0.0209197998046875 0.0\n",
            "loss tensor([[0.6113]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.3708,  0.1325, -0.6826],\n",
            "         [-0.3647, -0.1880, -0.5610],\n",
            "         [ 0.2792, -0.6662, -0.2885],\n",
            "         [ 0.5319,  0.2559,  0.5100],\n",
            "         [ 0.0085, -0.4414,  0.1142],\n",
            "         [ 0.2127,  0.2123, -0.2356]]], device='cuda:0') tensor([ 0.0063,  0.1137,  0.1002,  0.2406, -0.1478, -0.0090], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.611328125\n",
            "act tensor([[11, 10, 10,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.130615234375 0.0\n",
            "tcost icost 0.218017578125 0.0\n",
            "tcost icost 0.1927490234375 0.0\n",
            "tcost icost 0.2041015625 0.0\n",
            "tcost icost 0.0008530616760253906 0.0\n",
            "tcost icost 0.0262908935546875 0.0\n",
            "loss tensor([[0.6479]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.2752,  0.0450, -0.7681],\n",
            "         [-0.3451, -0.1306, -0.5545],\n",
            "         [ 0.2425, -0.6831, -0.3385],\n",
            "         [ 0.5336,  0.2700,  0.5170],\n",
            "         [ 0.0073, -0.4431,  0.1106],\n",
            "         [ 0.2120,  0.2111, -0.2373]]], device='cuda:0') tensor([ 0.1751,  0.0345,  0.1288,  0.2030, -0.1453, -0.0070], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.64794921875\n",
            "act tensor([[11,  4, 10,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1710205078125 0.0\n",
            "tcost icost 0.1888427734375 0.0\n",
            "tcost icost 0.3056640625 0.0\n",
            "tcost icost 0.203369140625 0.0\n",
            "tcost icost -0.0017957687377929688 0.0\n",
            "tcost icost 0.0276641845703125 0.0\n",
            "loss tensor([[0.7520]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.3270,  0.1066, -0.7358],\n",
            "         [-0.3453, -0.2036, -0.5982],\n",
            "         [ 0.2468, -0.6779, -0.3605],\n",
            "         [ 0.5358,  0.2805,  0.5216],\n",
            "         [ 0.0051, -0.4467,  0.1031],\n",
            "         [ 0.2116,  0.2096, -0.2399]]], device='cuda:0') tensor([ 0.0892,  0.0906,  0.0929,  0.1714, -0.1384, -0.0036], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.751953125\n",
            "act tensor([[11, 10, 10,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1619873046875 0.0\n",
            "tcost icost 0.2344970703125 0.0\n",
            "tcost icost 0.25830078125 0.0\n",
            "tcost icost 0.2200927734375 0.0\n",
            "tcost icost 0.0026569366455078125 0.0\n",
            "tcost icost 0.0364990234375 0.0\n",
            "loss tensor([[0.7656]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.3339,  0.1161, -0.7344],\n",
            "         [-0.3377, -0.2307, -0.6200],\n",
            "         [ 0.2291, -0.6816, -0.3944],\n",
            "         [ 0.5390,  0.2924,  0.5249],\n",
            "         [ 0.0016, -0.4523,  0.0903],\n",
            "         [ 0.2102,  0.2056, -0.2465]]], device='cuda:0') tensor([ 0.0886,  0.0950,  0.0790,  0.1345, -0.1269,  0.0046], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.765625\n",
            "act tensor([[11, 10, 10,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.161865234375 0.0\n",
            "tcost icost 0.235595703125 0.0\n",
            "tcost icost 0.25732421875 0.0\n",
            "tcost icost 0.2318115234375 0.0\n",
            "tcost icost 0.00630950927734375 0.0\n",
            "tcost icost 0.0489501953125 0.0\n",
            "loss tensor([[0.7842]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.3405,  0.1243, -0.7330],\n",
            "         [-0.3306, -0.2546, -0.6386],\n",
            "         [ 0.2056, -0.6862, -0.4305],\n",
            "         [ 0.5419,  0.3059,  0.5288],\n",
            "         [-0.0021, -0.4583,  0.0749],\n",
            "         [ 0.2078,  0.1994, -0.2566]]], device='cuda:0') tensor([ 0.0875,  0.0954,  0.0675,  0.0922, -0.1142,  0.0166], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.7841796875\n",
            "act tensor([[11, 10, 10,  9,  4, 11]], device='cuda:0')\n",
            "tcost icost 0.1617431640625 0.0\n",
            "tcost icost 0.236083984375 0.0\n",
            "tcost icost 0.257080078125 0.0\n",
            "tcost icost 0.2440185546875 0.0\n",
            "tcost icost 0.01036834716796875 0.0\n",
            "tcost icost 0.076171875 0.0\n",
            "loss tensor([[0.8120]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.3465,  0.1284, -0.7335],\n",
            "         [-0.3261, -0.2767, -0.6540],\n",
            "         [ 0.1688, -0.6929, -0.4742],\n",
            "         [ 0.5441,  0.3255,  0.5347],\n",
            "         [-0.0074, -0.4670,  0.0493],\n",
            "         [ 0.2032,  0.1869, -0.2773]]], device='cuda:0') tensor([ 0.0940,  0.0893,  0.0551,  0.0332, -0.0945,  0.0399], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.81201171875\n",
            "dided\n",
            "time\n",
            "[11, 11, 11, 0, 11, 0, 10, 11, 10, 7, 4, 4, 11, 11, 11, 4, 11, 11, 10, 10, 10, 11, 12, 10, 11, 11, 11, 11, 9, 10, 7, 13, 10, 10, 4, 4, 11, 7, 10, 8, 10, 11, 10, 10, 10, 10, 2, 4, 8, 10, 0, 10, 11, 4, 12, 9, 11, 10, 10, 12, 0, 11, 11, 11, 7, 0, 10, 10, 10, 10, 12, 11, 11, 0, 11, 11, 0, 10, 11, 10, 11, 10, 12, 10, 14, 11, 10, 11, 12, 10, 11, 4, 11, 10, 8, 10, 5, 10, 10, 10, 11, 0, 10, 4, 11, 11, 7, 10, 0, 4, 10, 3, 11, 12, 10, 0, 10, 10, 11, 10, 11, 11, 11, 10, 11, 10, 4, 11, 11, 10, 10, 10, 11, 10, 11, 11, 10, 11, 10, 10, 11, 11, 11, 11, 12, 10, 10, 10, 12, 10, 8, 4, 5, 11, 10, 11, 10, 11, 10, 11, 10, 10, 11, 0, 3, 10, 4, 10, 11, 10, 11, 11, 10, 13, 11, 11, 10, 11, 3, 10, 10, 11, 10, 11, 10, 10, 14, 4, 11, 4, 11, 10, 11, 4, 13, 14, 12, 10, 10, 3, 10, 12, 11, 1, 11, 10, 10, 7, 13, 10, 11, 11, 11, 11, 10, 10, 10, 11, 2, 10, 11, 0, 11, 12, 10, 3, 13, 14, 11, 10, 10, 14, 10, 10, 10, 3, 10, 12, 10, 6, 13, 11, 10, 10, 5, 10, 11, 10, 11, 10, 10, 11, 3, 10, 12, 10, 10, 10, 7, 13, 10, 3, 10, 4, 10, 10, 11, 7, 4, 13, 0, 0, 10, 11, 11, 10, 10, 10, 11, 4, 11, 10, 14, 4, 10, 8, 0, 4, 11, 14, 11, 11, 12, 10, 10, 7, 11, 10]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "\n",
        "env = TimeLimit(env, max_episode_steps=600)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6]) torch.Size([1, 256]\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # out = agent(state, zip(*out)[k:])\n",
        "            act = agent(state, k)\n",
        "            # act = zip(*out)[0].cpu()[0,:k].tolist()\n",
        "            # act = out[0].cpu()[0,:k].tolist()\n",
        "            # h0=lh0[k-1].unsqueeze(0)\n",
        "            # , lx, lz\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"dided\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9cm6KjvBrnNO"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(10):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "    c_loader = make_weighted(buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    # train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# print(optim.param_groups[0][\"lr\"])\n",
        "optim.param_groups[0][\"lr\"] = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "b8zxYU9jpE8K",
        "outputId": "81f1443e-8e5b-48f7-a9af-09b1e33e803d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "256c75e8-e0c1-446e-a588-6de3dee18e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  self.hub = sentry_sdk.Hub(client)\n",
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240902_034119-3gthhome</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/3gthhome' target=\"_blank\">smooth-elevator-27</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/3gthhome' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/3gthhome</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mhkK_9AQm8_q",
        "wUhKd009Qvk3"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}