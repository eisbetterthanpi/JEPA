{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "5b0fb9c0-a928-47a2-b8a1-0ce428fec0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq procgen faiss-cpu vector-quantize-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0mXVAUnVYX-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEY9MmwZhA8a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# conv = Conv().to(device)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# # input = torch.rand((4,1,256,256), device=device)\n",
        "# out = conv(input)\n",
        "# print(out.shape)\n",
        "\n",
        "# conv = Deconv(256).to(device)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# input = torch.rand((4,256), device=device)\n",
        "# out = conv(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "import faiss\n",
        "import torch\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "c0b77b29-a2d9-49c4-b306-ba738b8c9c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuA25qQknUAX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        # self.enc = nn.Sequential(nn.Linear(in_dim, d_model), nn.ReLU(),)\n",
        "        # self.enc = get_res(d_model)\n",
        "        # # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc = Conv(d_model)\n",
        "\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        # self.pred = gru(emb_dim, rnn_units, num_layers)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=10.0 # 25.0 # λ\n",
        "        self.std_coeff=10.0 # 25.0 # µ\n",
        "        self.cov_coeff=1.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        # return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy):\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.rand((batch,self.dim_z),device=device)*2 -1)#*self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "        optim = torch.optim.SGD([z], lr=3e3)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        num_steps = 10\n",
        "        for i in range(num_steps):\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            sy_ = self.pred(sxaz)\n",
        "            # print(\"y_, y\",y_.shape, y.shape)\n",
        "            loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "        if loss.item()>0.1: print(\"argm\",loss.item(), z[0].item())\n",
        "        return z#.detach()\n",
        "\n",
        "    def loss(self, x, y, a, z=None):\n",
        "        sx, sy = self.enc(x), self.enc(y)\n",
        "        z = self.argm(sx, a, sy)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "        # v_c_loss = self.v_creg(self.exp(sx))\n",
        "        vx, vy = self.exp(sx), self.exp(sy)\n",
        "        v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "        return repr_loss + v_c_loss\n",
        "\n",
        "    def forward(self, sx, a): # state, ctrl\n",
        "        batch=sx.size(dim=0)\n",
        "        z=torch.zeros((batch,self.dim_z),device=device)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29O1eyvhnRSD"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "sim_coeff, std_coeff, cov_coeff = 10,10,10 # 0.08, 0.48, 0.004 # 0.005 0.42 0.024\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(sxaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    decay=0.999\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "\n",
        "                    step=0.01\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.deconv(sy.detach())\n",
        "                    # state_ = self.deconv(sy)\n",
        "                    tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "                    conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "\n",
        "\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach())\n",
        "                    state_ = self.deconv(sy)\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, costloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(train_loader)\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "        # for batch, Sar, cs in enumerate(zip(dataloader, costloader)):\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(sxaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy_))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    decay=0.999\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.01\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "\n",
        "                    # # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach())\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "                    # # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "                    # # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # # loss = jloss + conv_loss\n",
        "\n",
        "\n",
        "                    loss = loss + jloss# + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "\n",
        "                    try:\n",
        "                        st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(train_loader)\n",
        "                    closs = F.mse_loss(self.tcost(self.jepa.enc(st)), r)\n",
        "\n",
        "\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    # c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                # else:\n",
        "                #     scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "# agent = Agent().to(device)\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = Agent(d_model=1024,dim_v=4096).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShHQ_ynlwoyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3bfd5d3-199e-49e6-8ba8-1ada6e9bdb98",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "def save(folder, name='agent.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    # agent.mem.save(file=folder+name)\n",
        "    # with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer512.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# save(folder, name='agent_jepa753333256.pth')\n",
        "# buffer = load(folder)\n",
        "# save('/content/')\n",
        "# buffer = load('/content/')\n",
        "\n",
        "# name='agent.pth'\n",
        "# print(folder+name)\n",
        "# torch.load(folder+name, map_location='o')\n",
        "# with open(folder+'buffer512down.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "with open(folder+'buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = self.data_process(buffer)\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    # def data_process(self, data): # str 10780437\n",
        "    #     return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "        # return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        return state, action, reward\n",
        "        # state, action, reward = self.data[idx]\n",
        "        # # print(\"__getitem__\",state)\n",
        "        # state = self.transform(state)\n",
        "        # # print(\"__getitem__\",type(state))\n",
        "        # return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 32 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "train_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "train_data = Datasetme(train_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "1e3fpbtNOiz1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(train_loader)\n",
        "images, labels = next(trainiter)\n",
        "imshow(torchvision.utils.make_grid(images,nrow=4))\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "DnSDZjMSXQAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OksdjCeJYpYh"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "for i in range(30):\n",
        "    print(i)\n",
        "    agent.train_ae(train_loader, optim)\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    state = buffer[7][80][0]\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    out= agent.deconv(sx_).squeeze(0)\n",
        "    print(out.shape)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n",
        "\n",
        "# 10 epochs 15m23s\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "8uivwksBdwVH",
        "outputId": "d69a092a-6b07-4a31-c2fa-df87c8468652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32, 32])\n",
            "(3, 64, 64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 32, 32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PraFUAPB3j7v"
      },
      "outputs": [],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    # out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    out = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9cm6KjvBrnNO"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(10):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    # buffer = simulate(agent, buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    # train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "###save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "batch=sx.size(dim=0)\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone())\n",
        "print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e2)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 20\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "d230d9f3-a513-4972-c07b-97fb9291bbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.9.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.9.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240714_141632-4lcmbisw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/4lcmbisw' target=\"_blank\">swept-donkey-12</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/4lcmbisw' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/4lcmbisw</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjm2kV3H7ZVR",
        "outputId": "d4040132-28f1-4347-8028-2e951476da85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6872065\n"
          ]
        }
      ],
      "source": [
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzzjgoXCnhT7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "### trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wUhKd009Qvk3"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}