{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "d09e4d1e-e9a6-43fd-b80f-9336575c44ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq procgen faiss-cpu vector-quantize-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uT9m-J1BUWyz"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"vicreg\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([]), torch.tensor([])\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros(1)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros(1)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(1)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(1,3,64,64)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [3,3,2]) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# print(xhat[0])\n",
        "# print(indices[0])\n",
        "\n",
        "# assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfdrlKFtBp_E",
        "outputId": "38f2d3cd-dc80-4c6b-e499-4b518a2976ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 16])\n",
            "tensor(0.1260)\n"
          ]
        }
      ],
      "source": [
        "n=100\n",
        "d_model=16\n",
        "linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "linsx = torch.zeros(n, d_model)\n",
        "n = n\n",
        "p=(n-1)/n\n",
        "len_seq=5\n",
        "lsx = torch.rand(len_seq, d_model)\n",
        "# def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "# if linsx==None: linsx = self.linsx\n",
        "len_seq = lsx.shape[0]\n",
        "linsx = torch.cat([linsx, lsx], dim=0)\n",
        "weights = 1-p**torch.cat([torch.ones(n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "idx = torch.multinomial(weights, len_seq)\n",
        "mask = torch.ones(n+len_seq, dtype=bool)\n",
        "mask[idx] = False\n",
        "linsx = linsx[mask]\n",
        "print(linsx.shape)\n",
        "bore = (linsx@lsx[-1].T).sum()/n # [-1,1]\n",
        "print(bore)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "import faiss\n",
        "import torch\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros(n, d_model)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "\n",
        "        bore = (linsx@lsx[-1].T).sum()/self.n # [-1,1]\n",
        "        # bore = -((linsx*self.linmul)@lsx.T).sum() # [-1,1]\n",
        "        return bore\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z5-_pfGZTsip"
      },
      "outputs": [],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "cf3b4b32-5725-4854-a720-a22f0740c28b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = nn.Sequential(nn.Linear(in_dim, d_model),)\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        # self.pred = gru(emb_dim, rnn_units, num_layers)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v),# nn.ReLU(True),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25.0 # 25.0 # λ\n",
        "        self.std_coeff=1.0 # 25.0 # µ\n",
        "        self.cov_coeff=25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy):\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.rand((batch,self.dim_z),device=device)*2 -1)#*self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "        optim = torch.optim.SGD([z], lr=1e3)\n",
        "        # optim = torch.optim.AdamW([z], lr=3e1)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        num_steps = 10\n",
        "        for i in range(num_steps):\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            sy_ = self.pred(sxaz)\n",
        "            # print(\"y_, y\",y_.shape, y.shape)\n",
        "            loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            # print(\"argm\",loss.item(), z[0].item())\n",
        "        return z#.detach()\n",
        "\n",
        "    def loss(self, x, y, a, z=None):\n",
        "        sx, sy = self.enc(x), self.enc(y)\n",
        "        z = self.argm(sx, a, sy)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "        # v_c_loss = self.v_creg(self.exp(sx))\n",
        "        vx, vy = self.exp(sx), self.exp(sy)\n",
        "        v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "        return repr_loss + v_c_loss\n",
        "\n",
        "    def forward(self, sx, a): # state, ctrl\n",
        "        batch=sx.size(dim=0)\n",
        "        z=torch.zeros((batch,self.dim_z),device=device)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=32):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros(d_model, d_model) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=20)\n",
        "        a, act = la[0][0], lact[0][0]\n",
        "        return act\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.AdamW([x], lr=1e0)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "                # print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None): # state, ctrl\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t]\n",
        "            # cost += self.tcost(sx) + self.icost(sx)\n",
        "            cost += self.tcost(sx) + self.icost.boredom(lsx, linsx=None)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            sx = self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros(batch_size, self.d_model, self.d_model) # Sum i] vi kiT\n",
        "            sx = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "\n",
        "            lst=list(range(0,len(Sar),bptt))[1:]+[len(Sar)] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # c,c_= 0,0\n",
        "            c,c_= torch.tensor([]), torch.tensor([])\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                sx_ = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                # sxa = torch.cat([sx, a], dim=-1)\n",
        "                # c_ = c_ + self.tcost(sx_).squeeze(-1) #sxa\n",
        "                c_ = torch.cat([c_, self.tcost(sx_).squeeze(-1)]) #sxa\n",
        "                # c = c + self.icost(world_state_) + reward.to(torch.float32)\n",
        "                # c = c + self.icost(sx_) + reward.to(torch.float32)\n",
        "                c = torch.cat([c, self.icost(sx_) + reward.to(torch.float32)])\n",
        "                a = self.quantizer.indices_to_codes(action)\n",
        "                z = self.jepa.argm(sx, a, sx_)\n",
        "                sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "                sy_ = self.jepa.pred(sxaz)\n",
        "                repr_loss = self.jepa.sim_coeff * F.mse_loss(sx_, sy_) # s(sy, sy~) # invariance loss\n",
        "                std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sx_))\n",
        "                jloss = repr_loss + std_loss + cov_loss\n",
        "                loss = loss + jloss\n",
        "\n",
        "                if i in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), F.mse_loss(c_, c).item())\n",
        "                    loss = loss + F.mse_loss(c_, c)\n",
        "                    loss.backward()\n",
        "                    optim.step()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx = sx_.detach()\n",
        "                    loss=0\n",
        "                    # c,c_= 0,0\n",
        "                    c,c_= torch.tensor([]), torch.tensor([])\n",
        "\n",
        "                try: wandb.log({\"train loss\": loss.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "    def save(self, folder):\n",
        "        torch.save(self.state_dict(), folder+'agent.pth')\n",
        "        self.mem.save(file='mem.pkl')\n",
        "\n",
        "    def load(self, folder):\n",
        "        self.load_state_dict(torch.load(folder+'agent.pth'), strict=False)\n",
        "        # self.mem.load(file='mem.pkl')\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "6be989d1-22a3-4b39-a8d8-3db74c4f85f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "import pickle\n",
        "def save(folder=''):\n",
        "    agent.save(folder)\n",
        "    with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder=''):\n",
        "    agent.load(folder)\n",
        "    with open(folder+'buffer.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "buffer = load(folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = self.data_process(buffer)\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    # def data_process(self, data): # str 10780437\n",
        "    #     return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        # state = list(state)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(\"__getitem__\",state)\n",
        "        return state, action, reward\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "    def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "        lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "        with torch.no_grad():\n",
        "            imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "            data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "            # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "            data=data.flatten(start_dim=-3)\n",
        "            data=lin(data) # random projection\n",
        "            data = F.normalize(data, dim=-1)\n",
        "            idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "            sample = data[idx]\n",
        "            index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "            # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "            index.add(data)\n",
        "            D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "            priority = (2**-D).sum(-1) # L2\n",
        "            # priority = -D.sum(-1) # IP\n",
        "            topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "            index_list = idx[topk.values] # most clustered\n",
        "            for i in reversed(index_list): data.pop(i)\n",
        "        return data\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 16 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "j-nT5j864BIn",
        "outputId": "6546d4f4-ed9a-4f22-f610-d404827959f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ded\n",
            "time\n"
          ]
        }
      ],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    # done = False\n",
        "    episode=[]\n",
        "    # for i in range(200):\n",
        "    while True:\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0)\n",
        "        action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cm6KjvBrnNO",
        "outputId": "5afe0041-4af5-4246-aa3c-a8c43c9fb6d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 #### train ####\n",
            "repr, std, cov, closs 0.14888893067836761 0.47741347551345825 0.0043539260514080524 0.02142232656478882\n",
            "repr, std, cov, closs 1.6424458026885986 0.4640554189682007 0.034666698426008224 0.14835655689239502\n",
            "repr, std, cov, closs 1.9469552040100098 0.47405749559402466 0.006936035584658384 1.9876030683517456\n",
            "repr, std, cov, closs 0.737529456615448 0.45419251918792725 0.08040814101696014 1.6376090049743652\n",
            "repr, std, cov, closs 0.470330148935318 0.47570371627807617 0.008434310555458069 0.752399742603302\n",
            "repr, std, cov, closs 0.47741416096687317 0.4626300632953644 0.045338958501815796 0.27787619829177856\n",
            "1 #### train ####\n",
            "repr, std, cov, closs 0.5155083537101746 0.4788396954536438 0.003727920586243272 0.9981303215026855\n",
            "repr, std, cov, closs 0.17256388068199158 0.4858447015285492 0.0005129328346811235 1.4875526428222656\n",
            "repr, std, cov, closs 0.7763500213623047 0.4795583486557007 0.002809915691614151 1.790307879447937\n",
            "repr, std, cov, closs 0.5934298634529114 0.4819680452346802 0.002560131251811981 0.8830576539039612\n",
            "repr, std, cov, closs 0.6558274626731873 0.4822569191455841 0.0013715997338294983 0.7358725070953369\n",
            "repr, std, cov, closs 0.2906920611858368 0.48315462470054626 0.0016197842778638005 0.5733721256256104\n",
            "2 #### train ####\n",
            "repr, std, cov, closs 0.23932549357414246 0.48646241426467896 0.0004937374615110457 0.882911741733551\n",
            "repr, std, cov, closs 0.2175244241952896 0.4874737560749054 0.0003144895890727639 0.5884543657302856\n",
            "repr, std, cov, closs 0.18005326390266418 0.47556108236312866 0.007230428978800774 0.19590583443641663\n",
            "repr, std, cov, closs 0.24464984238147736 0.4630174934864044 0.03781060129404068 0.38147756457328796\n",
            "repr, std, cov, closs 0.2683333456516266 0.46576350927352905 0.02522132731974125 0.41456344723701477\n",
            "repr, std, cov, closs 0.2582234740257263 0.4749290645122528 0.00765514699742198 0.33146804571151733\n",
            "3 #### train ####\n",
            "repr, std, cov, closs 0.31042394042015076 0.4824272394180298 0.002135372022166848 0.25460752844810486\n",
            "repr, std, cov, closs 0.16466571390628815 0.4826121926307678 0.002216731198132038 0.17325443029403687\n",
            "repr, std, cov, closs 0.13473477959632874 0.48274344205856323 0.0020031994208693504 0.5385351777076721\n",
            "repr, std, cov, closs 0.11316867172718048 0.482924222946167 0.0018373973434790969 0.9392938613891602\n",
            "repr, std, cov, closs 0.15132249891757965 0.48339295387268066 0.001592102344147861 1.2265068292617798\n",
            "repr, std, cov, closs 0.2512444853782654 0.4845827519893646 0.001173898228444159 0.7843114733695984\n",
            "4 #### train ####\n",
            "repr, std, cov, closs 0.36483192443847656 0.4866185784339905 0.000607827736530453 0.38791579008102417\n",
            "repr, std, cov, closs 0.37230077385902405 0.48785078525543213 0.00030621240148320794 0.19092199206352234\n",
            "repr, std, cov, closs 0.3797612190246582 0.48510926961898804 0.000919635291211307 0.11790215224027634\n",
            "repr, std, cov, closs 0.18533004820346832 0.4831831455230713 0.0015457577537745237 0.20194464921951294\n",
            "repr, std, cov, closs 0.09716138243675232 0.4825020432472229 0.0016814414411783218 0.8570529222488403\n",
            "repr, std, cov, closs 0.0997013971209526 0.4837521016597748 0.0011864480329677463 0.5863268971443176\n",
            "5 #### train ####\n",
            "repr, std, cov, closs 0.1703556329011917 0.4683052599430084 0.016716336831450462 0.1792830526828766\n",
            "repr, std, cov, closs 0.2364191710948944 0.4622555375099182 0.03283700346946716 0.0612759105861187\n",
            "repr, std, cov, closs 0.2204492688179016 0.4669302701950073 0.01969212479889393 0.0853860005736351\n",
            "repr, std, cov, closs 0.15158522129058838 0.47822970151901245 0.0037561908829957247 0.02975543960928917\n",
            "repr, std, cov, closs 0.09422171860933304 0.48653531074523926 0.0005523861618712544 0.09757880121469498\n",
            "repr, std, cov, closs 0.06998701393604279 0.4859652519226074 0.0007202430278994143 0.46440601348876953\n",
            "6 #### train ####\n",
            "repr, std, cov, closs 0.08414392173290253 0.4850411117076874 0.0008472076151520014 0.1014469563961029\n",
            "repr, std, cov, closs 0.11612895131111145 0.48418012261390686 0.0011002722894772887 0.6152930855751038\n",
            "repr, std, cov, closs 0.28096291422843933 0.4581243097782135 0.061017390340566635 0.03320914879441261\n",
            "repr, std, cov, closs 0.44541415572166443 0.4492977559566498 0.11691348254680634 0.08352148532867432\n",
            "repr, std, cov, closs 0.29501381516456604 0.46486902236938477 0.025997839868068695 0.2049461007118225\n",
            "repr, std, cov, closs 0.20592673122882843 0.48272356390953064 0.0018003099830821157 0.3048522174358368\n",
            "7 #### train ####\n",
            "repr, std, cov, closs 0.3051421046257019 0.46749165654182434 0.02493971213698387 0.13131794333457947\n",
            "repr, std, cov, closs 0.36695918440818787 0.47152042388916016 0.01540177222341299 0.01726626418530941\n",
            "repr, std, cov, closs 0.20451843738555908 0.4835602641105652 0.0012432343792170286 0.020117349922657013\n",
            "repr, std, cov, closs 0.17184659838676453 0.4706471264362335 0.014454547315835953 0.18478558957576752\n",
            "repr, std, cov, closs 0.2745056748390198 0.4711782932281494 0.0127109345048666 0.01270311325788498\n",
            "repr, std, cov, closs 0.20842799544334412 0.479682981967926 0.003973987884819508 0.004972751252353191\n",
            "8 #### train ####\n",
            "repr, std, cov, closs 0.12489751726388931 0.4840701222419739 0.0013622434344142675 0.052164480090141296\n",
            "repr, std, cov, closs 0.11869652569293976 0.48350363969802856 0.0014250855892896652 0.04893924295902252\n",
            "repr, std, cov, closs 0.15263837575912476 0.48377957940101624 0.0011044730199500918 0.005506790243089199\n",
            "repr, std, cov, closs 0.14629803597927094 0.48520559072494507 0.0007656092057004571 6.506012439727783\n",
            "repr, std, cov, closs 0.4921296536922455 0.444832444190979 0.21924491226673126 1.3689271211624146\n",
            "repr, std, cov, closs 1.040273666381836 0.4320540428161621 0.5480830669403076 1.7904243469238281\n",
            "9 #### train ####\n",
            "repr, std, cov, closs 0.9293110966682434 0.4640435576438904 0.049366824328899384 0.20150189101696014\n",
            "repr, std, cov, closs 0.5085022449493408 0.47031307220458984 0.01200254075229168 2.138949155807495\n",
            "repr, std, cov, closs 0.26003298163414 0.44152340292930603 0.19868235290050507 0.44422322511672974\n",
            "repr, std, cov, closs 0.41646039485931396 0.4405113458633423 0.25275182723999023 0.9443359375\n",
            "repr, std, cov, closs 0.5203620791435242 0.47087836265563965 0.017123347148299217 1.405777096748352\n",
            "repr, std, cov, closs 0.37792545557022095 0.47784367203712463 0.005111764185130596 0.48545438051223755\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(10):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    # buffer = simulate(agent, buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    agent.train_jepa(train_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6a49EohtiJX"
      },
      "outputs": [],
      "source": [
        "!grep MemTotal /proc/meminfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b8zxYU9jpE8K",
        "outputId": "d30100f0-5c42-46fb-eed8-bc38427ed40b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAUYltZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2MyByMzA2MCA1ZGI2YWE2IC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyMSAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTIgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTIwIHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAADdWWIhAAr//mDB/+C8M1LTXOsMOa9vcA1WgZ7zqLlGRtwTfgrpWkV/lCkgPtjvZ45482OFN8GKX2oeLCl/GoIwty9fAfQI0nJT+0z+U0UbHDYTUZdWy5jwmdHSuDZCbQoIXexPpzE3VkuRfQ/cLKOiUvgHZKckMISmwaYb97SOvRazqblYbEp4SzdIz1KtaleR52SEz57xn2Jhj3S78ueQEgtekABQQMnW9vlKy8VLSLtY4S4DeSJShd4m524aW5ViujIYzJGXzs5YimTHFVF8ZRQ5D9/LKxLdj4ukj05emb74aW7SSpqZ8YQDtwLopzJ7o6n2SIAiTCi2OE36gJrXnOb/9CoTmJJb1Of78Pf1mKZUcakc3/eptF28K26/zTUlRYihfznxLJxyNvP1YRXOzx8+Q9GGFNM6aS2xnMLPvXr+F8D6b3XwLyHSEoi1eOvaw2DGss1XGfdkbYr38Xub+5jYbtGS1KXk3Na5nY17m7yCJV/eoVJIdzAjzVD6I3UfIqPgO/Ck+NT32W8ZvAKU3kNRaPjXTlMdR7faT/rQhpnUzPWJv61ANX4uarN8WvYAw2E4bBghW3L2Y/rQ1JAYXVI9tLUVe0KV7a+6wH+7sHBBCZ1E6NnCEMmSjNDYRq3SZDnX/tZV2AhZ0nN1EmVQ4d5K5UKK1+rrmDkUtxRGu9TShzkRF09aNosMNtqnKCr3CB0qLBlxJCu6wKdQr+nmqljOI3IKwF1HA31+0QCqTtSSGGIfFN9Iz1lNoZo+EqfAXUQyp7WtLK5+4JiNV9qHmJol4P0yD5r1tDCeYk2iAf89r0OGIl1xxXosZ4Kzo1m4S36n7sjSWRn1mNMxLWqAlbq+TV+fSLph2uKKuWp32kmY0+Goo4Q2fiuy+FrF8wnJ941BPZdWAVMh2plFk4ZtDq//MzqH5Eip4xn1/888hfXto952xxF8xCGDEXFSvLcZBW/KcqKPoudrFYOSi/vQ+CMERLRLmGnHwTMkO8rzBvM14GOxAiQ3IBQ5e1KiR9ssiDg+cCG3Cq77pc3dGojJQzfnXsry5dFz5oaWBB2HkVBFLmCyB7gLCh4t3kyeaYLJsodRH3QnkwqtKUfoO3TCAxD7EiB5CJvyEAr8zhb+Qqtkz4yyP80ItUzE8rDy/ciU7gflg2Lp+w606uj4Kjq6Q9rOsr/NwAAAA5BmiRsQn/8XMDSJVWqrwAAAAhBnkJ4n/8kYQAAAAcBnmF0XywgAAAABwGeY2pfLCEAAAAQQZpoSahBaJlMCE///fGtgQAAAAlBnoZFESz/JGEAAAAHAZ6ldF8sIQAAAAcBnqdqXywgAAAANUGarEmoQWyZTAhP//xKxmA/WrisREygMCt3eT6Np0iJUyqCAWugmFrDg81gs4GF5hK+spPwAAAAE0GeykUVLP+GJMBOsx6hLswezMEAAAAHAZ7pdF8sIAAAAAkBnutqX4Yhj4AAAAA0QZrwSahBbJlMCE///ErGYEi9U8b+GXy4xdusaPzCR/nd+hdBX6Sv2gGiOFQb79aO85TlrwAAAApBnw5FFSz/OAPvAAAABwGfLXRfLCEAAAAHAZ8val8sIAAAAG1BmzRJqEFsmUwIT//8VcUgEFTDRIplNTwAbKryN6w7C/Am5gBgk+z3NRD9+FVvizmo3kabLUTcVghxWwwGTUBXvhfgjY64alDU3PpQe/C1jUMxw07UiLbLU0ANN7vo4qgJA6ZXl2QJrWQPzvl4AAAAKkGfUkUVLP+IuPgJPzlwOj51GMv/gexRNDqyav9SWsDAv0Hj/887to3aQQAAABEBn3F0X5AfcQ//quZmvIXZ8AAAAAcBn3NqXywgAAAArEGbeEmoQWyZTAhH//vRCda/SiQJU+0akEaou2fw9ufeq9mMa/zruM5nMi89hOnAC5XDdhqBafRom3C2QZNTOmq5TeLe1/O+sZOZlKrUXM50JLn1a3LNV3Gc11Wxjqb99tI/lI7QolL58JJdLvgecndFB37TpmhTxVryqiHi/XSIxU56FRVZNdti/GjPO9dUrALSFrJgvVDy5K0G8mcJqlOmFwel0Yk9LzysrTkAAAA7QZ+WRRUs/4iNcRz+Q2OJiKQFJ26emQoGdGEYFtpivv7d1E1XR6mtkoPUvAhghohvKraMWS96jsUAjR4AAAAIAZ+1dF85eX8AAAAOAZ+3al+I72IwBcLqOqEAAACiQZu8SahBbJlMCEf/+84JtmuFIR7yoBCCXxqJbhEaAD7BrkAoDq4nqBf1RzLjZjNNqV1D9XU2uRwNSURSqp+1ZMRs20zStTob70bnGtJKS5dAOqtQDjuafjYvbbt6nfqfrwm610tAIDhDWHAcUtFxW9OR6NGNSM4RUA5FbbrizilB9t+uLsD5GrziuFFkde48BiX37C/DJcUBZvCgU1D/fm/AAAAANEGf2kUVLP94U6rg2wiWLYrr3opYUj4TjEElFVTD21G41Lrv4OQ+6P+W2EQ/JbFz10Kqds8AAAAXAZ/5dF+I8ppSBjqUjiYIk0sCuA8xtZwAAAAlAZ/7al+I8mlmviKPCnCMKp6KLbnFwGWTWxvvfRrqcsI71UCXIQAAAIdBm/1JqEFsmUwIR//7zoQZZ41FeGxhcZG4pk99HgL7TPNYPLRkuWCFsdhY1zSvZ11kZjy2/udMCqdwjBoJtess2vax3i+kL3Fgq2/hu/zk/EFuxJnKXc4C7B8YgmGfO+VtzN8dnJ5y/E6KcyIm2B76KyPZ/KuI+cdeciZIf9ESneLXHaX6dIUAAACEQZoeSeEKUmUwIR/7898Mrxvg6pkVxlT6hv6cfhGpvYKmw0SuOJEdCjL4fznrC/jzLf+4RXPBurG8EQ94NurlGrRlRVu2glGhAzjcs/9ZUIFd/iSH+QeZSZZADdrw6C3swOpa5FLJx/m5faF0lKWMRpN2XorQ0PyHG8bCnvtg/5QRv7UgAAAAtUGaIUnhDomUwI//+d9wrQiEyV2KxpAsun7DA+67vSgQwtHzaq77gFsUGi+iqmkMyh0VdNk1EOBCZBwu8OINvsPtVrxDMv8yg7hWTLZDm2Nr9otpgebNluYVtM5j00oJGD8J1sIkBEw44CMd9JCrytRZuwoNF00R3wFfLmAaL0qltNOMi6OGzyqNLAQo4w9h/TO6hAAEA2wvxAuWxffBz7vukrps9wvJO13PsKaR9OGviqYcBuAAAAAnQZ5fRRE8v4rroCK9RJRvRiANOvCBmaNQojg3FUZK4l/PGH1IpkofAAAAJAGeYGpfc0g/NqRdR2qoFGxuAbnVD6VGouXXePOCzKjcfQjiUAAAAI9BmmJJqEFomUwIR//V6K5/xRIo7d9jpVs2N3x23gD/foq2BE9nVBDcQ5ZLWMbhSlWoSJ/m10G/VrrPK9hli4+aWZnX3lX3nDpxHO4os1nyuPYWFm8E4Xm9m+HWohteTEKIDFx1bWsI9ZH9t614c1mUV6Vh2IaBDQsEHmmXY9xB2cilzqaCHInDU4bsdHJLUQAAAGtBmoNJ4QpSZTAhH/1hkbF4AvQUbN63XQsJr5nJJYALd1vK+dzhdjdV2l1F24qYzlKWBEOOzNcgkEj0SdY34tbA6QcFHvHAWJI6Nbd6ZqbMgIU/tJSmRnIAKyDQqy5LVdwSXWkALmMuSPIeeAAAAKxBmqRJ4Q6JlMCEf/vN3ikDCucL6PiTqkQT9Xxy+ByeFEIwb9Np/XC7lVflXaHMGnFBlB3VJdjuPGGy9TUnsQMUisH7MjnIVk/vozpFjoSWHTKbgdJCEE7K+xYXtCNM90g5mxfn17N1YZUzzDsUI4dD7VTM+lq6PdstB+Anayzkg41Wbr7Hlsi7WJj/+xz3BcIPaI2CsExWj4foaEdatwiXowEzxXq+nvnLayjBAAAAfkGaxUnhDyZTAhH/1X1DXYzZhfvgSIY810DSgQuzrCbGVQaHlzvo5sDR5fCN7bmDjIhY1nt6G6q2ZHnmtwyJcnLAWWcy6IxDGiSr5oqdgLo+57PZEwMZxPELrMO34GsgIhZ37diLEVJF/tfnAStdy/W+C+iAB6KUAlPdXa60QQAAAOdBmuhJ4Q8mUwI///weJvXwIoEyDCRsyZkIp16MvrjJr2coiqlwe9Qv2VXSi8uQyd/fenN2nZ1gRGiRYOdLPmDX+8YHJM2Xx7I0jbkD1skZ+mWCv+9G/mJyLtAya4qXKDXhi58I6z9ndM+6ifHYlj13A8w+7zMdRfpS/ooLT3QzkIvtXnwCgtN4PBum9AjgPiMslrkL78ENtvGVcO7CMJgHxjurjUCX27yPIjxOyxB0F60MYcbq+vmwseXPj/0znYd85ZGZhuzzyIkETtD3BCXdmnwJ6Pk9UxLya2ekDeeQGkzmlGk2Kf0AAABTQZ8GRRE8v2/4TmMqEu35I23CSTBBu/toZRnCv4e7OeMLyHq6/aJpWTOVOQuDrYD41i4PvVOf9WzupZeAZDDzrZar4/bY2XfBtCKPgk20oeayDeEAAAAeAZ8nal9+APDWeIDDGkYgD2V1M8QsgXZHwZdAMvwqAAAAkUGbKUmoQWiZTAj/+d9u2atSOKMHK14I4vXXgFFUuEATtjKtQWqfG66NcOs5x/s+ZNDaLSQl13tSUFTJXRqOgOHnnmOploISpa5ug2LL3zYZniZOlM6eWghS/T5xJPdcQc6/S+Z+ChpZL4MB9sUwq1xrbj1GdGIx1Nb9HtULiOdRFOiCqfz7Q4fWBJu/mrmWLcwAAACNQZtKSeEKUmUwI//700xR0fgm/DRJk8bcwwhYswfpv9jl1xFbCGlzN7zg6fnhajRwbb1FXcXUOoZQ/xzhsC3Nu7tVV7XZV6fgNaxwxbmyDa5E+KkTkb2knQzrkQyuBwDsQ7UIbTLQbNJD0/ZfX2gyFzy8AmcL9bg13wqcCOrojiHxksxb412KpUX/OkzxAAAAjEGba0nhDomUwI///AS74JTe+Tax3CrMLEw2b9PL/R905l+zeMgCyhdBP3HIK5I1g1UFHLLrZFRx4maWoob6ebqDaBOmzyytnipIAbCCslAQUzVgXGR+G/aZbDqZTWmMP8J1q2IqOYT21bwTfdABqKlEYRQwuwJ8zBnNtLxfgwIh6his16cjoI2012sgAAAAlkGbjEnhDyZTAj//+9aXmZMPPCU3V97AjaDZUqG5bsfa9+BgDOgMuK8vAkMsE6wC2h84Axz6/b5bCObG1e7Xb9Q7CnRCmFwRvYd/vbnFo/TkcJVt+kA9Z0WADJGQFxsHKA8Q6fwSFVwbOV/n9YXD5t7xIV40xEVhU8WlbHi0ypBX2C3H4BzkCXhxo7dM6VastZClcd0JgAAAAMFBm61J4Q8mUwI///vUGbJqYcqrFwmcFsQhWq0h2m1t8wCk6FPTsfLWZlGnlDuTJhOqJJGrq4KRdtOCdHcWRQdmDXEgOvfrgDd0Szhs4iBKvXKl2Ph+meqPvKzvbEYvh+cXAgd8hiVJA5y9aJ0BQhYfnZn/s/qKOStaMh8Z1Af/aZX84uQ86BqkTJq6iOk8uKCQWVXeaH4wn9IfhOEe3z3jtSYzCD0j4gy7P+1fUweRrdRSTmpARj9rQitP4ejetSB5AAAAxEGbzknhDyZTAj//+9OWCRutF3ilNj8cAm1pe7qiDsYhNTFFtgJv8y6OFVT0Dksl0a4vDijBiuRipgXKneyTIct1s+CPBvoMnopOww7SvDSJvPvGGbzFrJ6LJ6lXGTun4aVQShTlgrKdJK8duoGUBj8LFHBBy1cLmG1v7jjPm/c5Xn4KZo0gEefdoILyX//dpPF0t928D/lUffBnFJXY0G8ksdtywzy417R4BcSfkkzXd50gG3IQTi3Cvfgps6OZ1JyKg1cAAAFMQZvwSeEPJlMFETx/+9aWUMXaRfqlw5Pf9MxGC+9bE5sGK+CXpushegeNeBeCAISrjBCi4k9+log1BL8MXIzHDfse+5M8JT6dvs3g1et5c8AdWTYwrpGdLYx43dau/sFm6cLgD9LRuQln8cn6usiy5aURyk3+Um+NzqOgm1vEnFLTmn89qm3gxaNL+zMFZ7FkGDNNx3I3VIQkmfqKU5NpNgG48XX6fGNPsFgd4coCi3gim4d9S0zB1nwQnX1KlUbRbLaetdAdEaG6u6cHQYZJBqFKWtrbQzUF3Z4Zp/rBy02+Jz2+j4xvhcOnxjLkwVU9x2iHoh1tupssw7wHEtvgYr/vMPiRscrZeRBLC+tXjwmznZ94yIvj5/P86YbFo6obgPPLz+xDoQNkiPjW4BYEwwThwr8Zmrh8eB5RKoBRnuzr9w5YXKbMTNRTU98AAAAvAZ4Pal99/FOQiIILUFelIDDerQJxl79Nh5gx4PbstVqcO1MjG+dCovg/x4VRgUAAAACrQZoRSeEPJlMCP//8V7vPluAl3EZANuvxKUIVMvkhKxL7w+yCZ7ltwi0UP0qlHdngdS97J8SSSOL+7A9apCNnwcxAyuYtMP6l2OhD/OddHbGOgj9Wu/iAcXm8UWnaEsN5ptUKbMvoxk5oCNprAbsVS2A1BGkAPsAC9AMvOgp3wqaQbobEHgrNv0DghJTll0e08KigmLphrQ5iW3jMypaKlZkyYr48Lzkck3mAAAAAuUGaMknhDyZTAj//+/8194FfDumz1G4dZaaAxF7tdJEW1rUc2rM+173eaTWNIW8ZEIvQGrBuNGxNxpTNm/PZGNOq5Cfau0GpsxxFYtUBq2Du1ZLZG3NRD0m9C6w7ujcV/OW09tvkIvosiEcgWTReAF/iygKX+8tzBASiFHohgjMtkcuZFyhc+sXn872PYr1H/yjlwhVPtfYqmOdVXpfP2fCw8yqDU2JV89BTXVdIPWKgSg8WkjrGO6PBAAAA+UGaU0nhDyZTAj//+9Q+xrCAEAmADawE1Br3QytRLsu2EoZmWj3PID208z9+aDxDIP8yiFILg8FnPgjwTH7/xD+50NadZqT9hJoB8lT3zYaliUTJQJmjYTvxhXb3Gsbe5uASc0LGhajZVjsRZLjvXGnwkTJAIn50HRMOVuwU5HFvBnWM0VHVR0vyQfRHMiyW1L7q16QBpLs4oQAEvwzdWi2PzbkQnRfD782jSb0Mll1DMJEpUwPGAa7zpdCwqJutve6mJWNTG7BLuLyOk2M2OGAx7klzYYfATowSJNiUsREX2+PLBEX89+AwRq6V6fBD39lCsojJJj7b+AAAAJ9BmnRJ4Q8mUwI///3s5vBgARVLzt0Pd/Jvo8XM2YxRivLNTFjcLdDtr/lqf5fluLohh81Ar1VKEBfpUyyXrUenOQiG4ppWDD0NEo0VLK3pjDh0eaRnQvQw+cs7b+vFP59IkbrpifRGrFywMfrkZyYW5w+CHpXwlZ/86u1D6f3xeYhdosTxJAXgzLSGLcIT8BN5rOhD9zgaJkSnwqy6nXgAAAD2QZqVSeEPJlMCP//9xU1ZkWqQee20f+MFobJIHY21PvXHQPCeKVJ9/5EIphnfvbDgRKRF6lnMJGlz4nzCNiEBbTKSoQTeedlqD/rFf40qoPORnrYpT3Z5RK/s69R0v2Hc22pAvmmmCHbl8yCbQhKuiER2oSEv4xOe6nfKJRXvUYD8UhLbNDgpab7k05g8y931ACSamkjvZ8RGuJfPEwVH8jb5v52rDNKjRdvRr1Ft8CRMyWuf3Yy4kzgJGNrFPxOALRBnJCfue7G4PBfwg3XXfLhaM9Tq1FRlSJET2iRVv/1LUAp/K2lgXqxHQxSsUweIkUk4NQDhAAAAxkGatknhDyZTAj///eymq8wk69s9zshFRFRRTzOJvlp1xXe9tqkk8u6O2Cw8+9wz9B7xEd+UJO8L/tSDcqstbCnJveqsdDFyX05ChHSh9bDWJaV6GwkID5UvVQrYcKIyP2nxemeaThtoMz71DUhZPn34izW75B447C1LPH2UOXsaReySGQd78H76DRFZC2E0W2WgWDslLaL/5T06bz7nGidtQX0SdDZA5tDAt5x7Nvh1iXEl9YUgiTeEh0KyMOe0pcwrS4RvgAAAAOJBmtdJ4Q8mUwI///4GB40igU6nPOWhq/I102g+otrY4baU+VKDiV7oEeTcY2jyDhxbhZZpDAF7d/FvHkQhNp3JAutFyRgY3vo40ARO+hSTsb916C7yZ0oD2korxfE54aypjIM8a96OqAxrmbhyts/zhMLLfGUhRqSwpyaF0kwecaSJElGIeg1wVlUTTJbgwIFVlQ7Zn/knJ1VMV3pRYT24FUN3Okm35I7hOdNswe2+0Kmw6Nl0nuszChy09vD8e7wxpMuYuHxt+FEBpIQm4+fB3C2rLXofnMQv47RxC5g/8LvhAAAA1kGa+EnhDyZTAj///ezyYek8FI5upSSo5RL/YO5pEmLgOj5IrBi8958U44xGJW+G/PyfCsFU9kMcT67wagn745N0hsCijB7ZqpDL/m3eia92VIf21FnXUCos269mNiCyg8lTkR0hua3nk6JeUIw2H+yIatRdR9SeDAxbDtRar8piIGvzybJbybrBLLKX8iD8KHsarcHwS+12DxEOWf5PEULmxmvEuu5V3ie25oYfKEoZsHSa8ProQx3p5fcZiTrZeDV3haR6BvqNdEiB4rsPXDILPm0Rj8EAAADEQZsZSeEPJlMCN//9dy8mdIPxHgS6KRSOgvHkEKSp/3+VQ/ixQEsPpK7SCmytsL3FcH3/b6rvlWPwRHwVjY10mW6QgUMqKqjob23OEOoDtAMWPwQafcA0qTxRJbOQchRJ0inhU05dqStdX9zyKi61c4sGDbXE1KMbS2U+jTDNxqLqyGwUoR0Q/cgCXDd8DnrvJMU/zxcnegyjbkMLu7SQX7NaHeps9qi4THtKzWUi8E/jKMyi+PzQNqedNPJ4yxhuKjFL8AAAAM5BmzpJ4Q8mUwI3//0fwJ5ZtagQa3sly0nnKzSHj0LhoYlXi0fZovfXuHbIgMUVX3ajmjY6NRyR1Vnqz3ezPlVj28i16GBl1aPpd9G+g2ML1wx78P9EjcLzmn/SOArLx7mfGMopVi4csERUQqRcuoZBzxmrGGPKY/xhOIU2JTY5EyQATHcONmZ+2/v2TqPN//lSHJX5kYVyL0kuw+RsnVVPyQCCfaBfNaO+2IoTkpFTawPe85y2vwdSPXLXh6W6k/O5sxyvTJkdWzDkLf47lwAAALpBm1tJ4Q8mUwI3//vBvfVJHyOvAYNKUwXXG3JoO27RWjxR7p0WA9H56WKrVL3ZBU0NODmhMfoe1o59BSQJFRij/rIfQZVLmEZZ5GuYLBhwQEIbCX3JwLpNXZUsJVuQKTxtMqG6obbhVwcXzqp6c7/OX5coPNg+kfPto4WAJMHb6zpcP9amI+H3+ClmtSfEddiW6R1oWCs++a3z9o85SE3LSfk6X21gneQOn2c1rtmBsjQSF6S/Rg3HmvgAAAE7QZt8SeEPJlMCN//9HYNv+jmuSb0g+eHKK9WBYhSI60N91653FNiSKe1u+2aIIOis4XzBfKVsWa2Vs4jlaLaObhepIiyyqJd7obPioD3WbzHidLW5/H50Mgc+dZFLVaZhWyADlcbhucmz7GyBZTLLd62xMDOzP2c2X42eir8geYRqNQ6QnAQwkU91vZUS+WOT8LWsiG2+EfYeQHcr1LiAvRHIn/EhpEjeDqbhSqULvhPjVVCTwmXgsTJ8Voym7i22ZRSpg6lrCzrA9zkcuIbKBpx3oHysi9UWyKBj21vAYN0uzM7Mmu1jjuNdFnnV1K6dPimsnA/ZWWfyqAY720LeK76aQRDel8lp20dHvZpFGC9IGvF8sYe10104deISsOGNQpNEQdwJ+ZQ8i8QBth3dng2MH9KzoPq5cJOBAAAA0EGbnUnhDyZTAjf//Bc11JZQeVQFZmn2TqoltINA1cwrZYFztFwF096Ukf02PnhBvOc6ISvV51+JumJONuqDpQHTmXtePtnsp+byc2ziEnejHZ8uAJQv94Xex76rbCr4KuaD72gI55L0fQyBJfM5wmHBPU5+Vl/dJIwg+1tl9UPe9xG70A/vc6TVW0RRTDY/JIlv+ecCcAo7lYikkRattKoWVYQZwLgC8POhD0twu8JokVhY1g4uI8sGiI2focI26sBQJ7LAtlp+9mOn2ADm5fkAAADXQZu+SeEPJlMCN//9dLeP98EjZA1gZ6aon0mUG8VmqGGwxwqKpT8iIYXwgiBwIqPgFRF4uHgg523hu46nv74hfUL8b/JJLeagmPV7NguK/i3jVQdnPwGvuzjRppL7M1ipeBcGQgkkzFt+8lKs3wnKVPzJCB5BMycvcXm4WEA03jk3Q2AD628vD7nDJx1UrCbGH0zmsDpNV9rDqGr07xYcyx/JguR4GB66ork5fVeQh3VAjLny7gR+vchdDYagIoofa6gVsZi5/bZRIavNMrukEkICvBaj8/wAAADZQZvfSeEPJlMCN//8FyQwoIq+dh8BmBPGKSVQ3C2CvBsmC3RCeuNrkiyI5GubiQvhVizuwmFllN34k35+TcGc5Y9/a+AUsfAOEfx06jIq6uZWvlVMD7YA4OzuKuPDzN3k56Uq1PwN6emYdX0tBdQG3JHBA5mwEqRTY+kzIbVp9HVhuT7/uCkeqGoBmgYsTJH55dpEHiFH2IrXDsNFxJ+mjDCotfZRyADZXh10I5CofG49K5kqRug44GX4x2TZfkVZO0uEEgjX7oydbA0P0xLDYP7VJSmA3XnL3wAAAN1Bm+BJ4Q8mUwIv//wXI/4wUlB+bIQ1+8h/WK5yVDNLwchRHj2wv5BdO8g6v3i4noJnVROEEofYkrRF9tEJPLJw2LW2OV3UCh8zNvXpNYa8F/mODPju0xMm8AQTHjXRoC3+i1PcdV5/gNCi3YXqF5e7iJUQpIqQisyKlUxshtE914qNdYOgi8cretZs/nTXCm3K+u6ZYylpw1ZR1CDarqItZQ0fH0c+rI7iLDNBL7UOaVaK+9ii7nibVIUDbuxmQ+U3A3Dr+aBHpw9dbVQAvHbsn3qxAilkgoTsUgzB+QAAAPRBmgFJ4Q8mUwIv//10fZAtErx5cs99I8UPM+GlbBalGB9MwMX/9WAaVz7+ifcmiSBtg/qqym2eo2sLVntXXuvCipnoV941wgDO/9n6GDw5/ojQH4na2b2dGQSGMm4SdY1UJMH2gKmgSHPYyP7M99l51tr6v3IwnCoCnUUcr29X0zpdZqViQeLy1415Z+uYDx3TapexJvF6QJz2rJuKVWoSDmqLfg4GRIuIDpsVf/8gel7v03E3Cm79gC+gOSoTH28zlnYthoIYx7FcmcL/7FCB7kB+1ptujwAidyjRHdr0UMTmSEjz1B23wU+2lvUgFAgmRxC/AAAA5UGaIknhDyZTAif/98G4t/LEZPFPMLNx4RWvF6a8LriMOrbe6XdHeATtGJo0t79MdcfyD9ZneoUCIGdP2KnZgErMoEUwLyXJYC7LhqDcn2HXqMmxY5c3A5JGY2qpg2Mzqjg1LfxgTOzg9JkuyzAGc/YW/E3hbrH5T49dkSKm5ieTR5dqSOpUiFnUc1R/bOnHMaCUhNRCbn5/yHgnLq8woYx4m1J7K24nA414ADIbT9xEc1WVQF4t2656vqcRqR+XC/rMk8wf4v25UTfwvGCxKHpA/g4kDwSnKnoz01fB1Z/53dCUp+EAAAEiQZpFSeEPJlMC/wdIDoDGFtirsjexQwY6pXw8qEieZI0Djd/Db2xWPHQyGGXn9K3FDMlaDVM9/8WMZWgm4wqIYTKbtXxuciZdJkrIXCNCVaFiMjaTyTdpSXMgNKIL07RldKuqwq1v0Gg4iz6xHJ1mq82wSJ/fPEyYHhU7BhjS3I/imY9s7aNDpUhcVTpFWySTw4ZtepMQG5G4R3xFslm6vaXmFq4G18NVw64oCrmBjw3LI9wH6EN3RNYwqRnz1F3ar5+4Nvs5f50uoralHk6ZxggZnXoBHImcCPv8NZvDPJ064OWhrkSmxXXFLjNGEBIPv92EJAbwYEAefH2XldJM8FnV6gRum/OnNB9OpZhEEk9xdZpGBpI3mXYOgvQsn5gIIPAAAABcQZ5jRRE8v2huoqSEqCoCqPzkzj/ttPfe9j4QM8XM0TNaZ+7VkTzVBEHVHlmoRjYTDQfFipDaRVZtpcQGYPPiZTZxbI7kz9HG0PgAaybmlnc+HuUnyp1DIuXhLYEAAABRAZ6Eal9mRAKZBPeHd4MUhY7FDlL/xLE2x7k4ZZGm1Yn0GcaDSR+N8oCA2uAhyF9uIp9ABINF0dvUSy7FgI/ef6EC5zJZUz+0NsBFYHCwFN9TAAAA90GahkmoQWiZTAiPs+JL9nSkV6Pve1trDaA3BWn5QDd5A6MR/wAYxB4uu3eN4/JSEoxaC4Zkm8ymFGESXKkYfC8MN3/7zaT6L3CLNbAgr9TIY1XIN3so+1rF84RgsPB6vCAxTHidnD026sd9BptN3G1/fONv/akWfmheWc+xTa/JztmJuWnmh7FaT+rnJYrGyCTkruryyDawLKSWtif/iVOCUU5CDLK6XvxBUPay8fv9cjuviiQY7bwwzwAaks7ruNK7H2Np64HsRigEmHdMR8hNDEyEUYtPDj043YG279Dy1ks9SVuJUexlqOnwLf1tftb+8hd++V0AAAEKQZqnSeEKUmUwIz/5g8+Q8FcP+KFcjraf4XXe4LFHS91tdYxII5yDTsg6DuNIs1S/1S8B65g3UarhDiKVQE4iuTBv2q5wV6I7HYLmhNIJI9+CXc+8qW3J5Ehyy/AVEOIqJQwxnv0SBCAVfI6zP9aJ7wG14ao3wASOt53umzPBBUFLuSUffEjvvFgrVRQTym20w7f2vKdukAa3DIQx+Dk2hne6Mjhm13dW+qVLQtjYg47ETDEZ24bvJwbYJ0bWLOuAZ67tqGDs73eNI7vaO0I6ww+m7faC+zw0ovrg6ZVMH9LOIGO0Y6kI7cOPI8n0pynJiaz64vk8tCdOVRUKVzhwW+vA/10dtnEqa/kAAAGBQZrJSeEOiZTBTRM38kdhkpWweAaxyTZig2qakEV1+uh+3+SJ0bksfYlQQgO49e7gh6yAmzTxknZQXRO1XaSRdK5iQm5hKu8nct+MnXW3GdXFuEGvqZPj7zW11zU+N7egIMWJpDVdsWnJFttJ6ZmKIlMtb7j3xuc0VYeskofHbkUJ0S5S2bQvCWbTP9qtD7HlFpjkeRgino9bFxHZczuFnH2HW+z+witl1+oqvV2hXaZNoUTDeK1+Mw0fnAoPlUHSKYnTkDzy7ohhvEbWjM2WIFQ/pohTDKtFhFqerQ6RATQiq3xDfg7oZdOOzXF6LVFvA0NdQWlt1njhg60BU8gtWH1FAgN8qwq21UB81VUcDhvL0iiQvfXXs/IyBmP/KJGJe719aPPQunsdJmr5LK1IsTFSM709G9pppwbNrCI4mkCNf7zgNQ4kwqFFzzuvWjFKiV3ZEUisc2Ukz2kom6VEfzhubZ73vu9otBk3FsMWWznkDeSK2EQK49NSvQyHu0UEQAAAAGUBnuhqR3/vwz5+Q0n9Voo5e1vcI4H38PlrroIVD2Jxb3n3Lm4LJdds7uXIrskLeWimq+zooohstZcAnUwDlMhn+HCH/NsgEUAZP75ICt0Oragoyz3YhkPci5r1j6CNBlvTRIkseAAAAZBBmu1J4Q8mUwJv8Gs97vfge5n/nHucOpfKTEnYvTPmaIDl0gK/6iu5tmi77Q47Ro1Os0sIv9SLt8OGUVIwNS+3ozz4JsgLpQ7/EG+xY1naG4qgGruDzW8UmpSYgfQ71mgdNIj7JPIv3aoFYcjd4/ArNl3hoa60FZz57TJKlT5KH+KFXqGMUORLY08kok4nkg7jpN8trLlWjcyHayvGr6ZUcJIquVsHBlzxwVIswJej2DJjGlSkNvMI7+6NLvKjSABqVlhd93MobA+bYwkIm4zZdaL1Vnf0GhSrSXNkroGeaNpQ6U0cLlzicPjhGYG49M/xt9/9uFihN32IgvlydGLheJEr/wR6l/rURcx38WQM9Y0Un+rGE2LAPTko5Lzz5vD3fJqcBjSWRn9teUuoFzLNgdLHAPPaH+ocbXTbxz+7JhDj0ILtTSXqEBNTBCmLtI8wo1hxftKPXKRoGhP79+keUWdjMrXbvTkN4izbazgzDRKhnF21+9V3s0TEyVFZyOj1T4+tefWx8wL6gpjp4LQJAAAAikGfC0URPHfvK8thMdyBBXEeX+5QkD9m+VxhNn54d7Rtza+eGOF97UMKYBnimJfpUzNuxRecziS/wCFPIbumwQbYYEO+NBL6sRRmz1BCMSGv+mUDXB0TSR/9ZfW/u27ADCWpfKYMChGEzgmMd/+Pl8MLejpaGdY8iz6xG6ek44qJQEWqWNxYRIHBkwAAAFUBnyp0Qh/8TTLLLnFkryzsstzfeFVfAfB52pQ3YoUmTQbxZZ7I9UrhfNb7X9yUwVSz9T2SRvxfvWgTWx4bet9YkPHECDl6SfI4X4zWTfLtmbD/pJOLAAAATQGfLGpCX/LdneCQvLPIE2KrySBnE5cWbs+9yYAMp9r0RoEadQOpXlZziqaDASw79uIfZOxabsg2LDYMMgdJ4RmlyfZJGPS9vhNTiJ0xAAABr0GbL0moQWiZTBTzf+qPvLlPN2Fl+fuHN93yJsbiJc6qpXyO+nb7HZPpREjFzRaYBSCy2PyAKLuP1fH7+R74fKbaaD/ulsNx1MhQCWy5aCtId1uiIeKkJD3A5oAprvoMNJYTUyigJxdJweixTTOXB5ormVAVIP8owx2Z82P9obDWhGwRKp0ib30cTXK6ZbBnm7QFX/+WfA6WKn4j91pI5YCKfgPRoqVd2uOPIbI+WDoUfiqUx95luyYNqdeXeW42Zec2eC17wvcsadCB+Bv+lQoxTtQG++EulKh/peazaz36ZBBIIAfjcGGEXopvtVoR8+6RonzHEOOfvbTCy3EQpLwE6n0qtER3QvDfwBtpe0RH52oJw/TmqjZLFJP+f9EXRRRGKa0Z8/MYqvCieeiyUPCeyMYiYZK1SeFBVjtO9obfeEWQwFCCJJbexR7QWO63yZiYYzDS5SHBr4TMm6LeVgAfeLXxLdg0Vsq3TorwtxdQWuJsBpY+gwI+/L2qGtQufsFh2KwXIEF4YjXyNJNm1UwT21MX4lMUUWhCGsjRd8pqjO54Jq4rKRl06XsJUNNZAAAATQGfTmpCX/LcqyHQxUtWbbeg6jMturEX22VpDAYKxsuM63RzVnH/27wILBk+AZMYFy82hLFWeaAhU5tETXYol+s/1T9OsNCwr2cam1aBAAABaUGbUUnhClJlMFLN/+qPsb62gGihx6QO8oKXe5Ft8Tagf/PJli4iAZzgadhoh+oMiIoC/d99XvpRPo5hcyXQaFJVH7dePd6VIkdW8/viwZSG9PQ2rG61g7vuRY0hpyKSgGJKN88lYKra7CsaFkjcMNzVUqIuZYl1GooTlN9k1bsbEsPO9J2zN/XVIuv4GI8Mdp4xhsUCScdohjByzdAlf2Ip5xUZkonXhloUSmYtmQWhtze6zUZrrA8T6+Zp4obCoLxHa0EnCsdcewp5EBbOTLq/ioB/cpzH3UyoYZCHKCnTk2mOakymyY42HvQ95uD12X4Ni+XixmM/e475xjDDCu/oVUHicSp+hz/3PGWE1k/mbxaaBqOU8Aq08W/ZTxA0wbepVuVILdCPJhT8g6qwxmYUcSCmjP+3D6mGHQrK8aCcSFLOhn8RIFUAjrrAVLT9PxVpl6e3ZjO7r3i0GtztjKdLMSJDSenBMQEAAABRAZ9wakJf8t2eA0uQGhODzFvXQTU2kTEkRqijPslVLyj6/3mIMIlvPhIyS3kTFej8hq6/wsMNmqYR76gNtoqfEb/F30BM/+9HkZWkgcLseM+aAAAA+UGbcknhDomUwJv/8QdV+M3Tx/qWtMocotc+RXKP2pFw9sslD9H7CmVo8Q39IjNpdVHMMC6leLw/u0Q5U37W8nVJ/vW+KSRcF//NLd8r1+j/AKMWhM2mbGKhsk1Q6VAWBW8c8jgNFahpQalKGdTP3bb4HkP3tR6iwoQQVqf33WEd6nDc/l+Fpkv7srZVWbzxVciE4FmQ80oeGQu9wW7q1hMqljw5ihXi79EkuJ176lRg02VnpLp0ohcBEJQInlgbha8lRlxtuMIQ7rPlk/mveYUmhghqbDXZxHh0/BL0FJY284y7jd25/G8kNgbBQqFgmqlwhVNQGkZUcQAAAQlBm5NJ4Q8mUwJv0ve1X5S0Bb9Xf2eNW8tVNiD/VprJ8hJS4Cz5VP4qsXGtor5CtIL8pMkOohIsJ4fxRy+kGuqMYherB1SYPNL6kpBNRmkWe6skhfcIqqLSVXfTPa0ngBThXYsDyqaVo6rsw89AtehIX6tMi+iVtqQeRhCKBmgVo+u51bphdELCTXWS7tGaq9QwNSbNx+W9urdgvCe2o2u2JNyHfpFGmvtfzo+jN0l3zMmjZSMk4QeN40JaYgvUJqEk5czTfzMks7R7yT/63kUUVItG1okpRDGn8OOhQJwSmdCGmrpWRNb942McjKVW1AAPMz+usjw5dPC16/8G8lk/u5KsJ4ix8uzgAAABhUGbtEnhDyZTAm/tpXeX5DSeoG6PigSP0c7EPR41Ha7QAwmpbRoRJT/WTq27J2/ASdIqkt3y9NSOqw0/7q/TfPqVtJLDhJ/qJxUdDmWNsF/GU98r40M5espPvACkzBngU5/0hNspRl91E90sUlekRHlqAO5vgVomQbi6oZ2unsuVS+VbgHdMtr/+H+Tx+0fvhP44fUTd56esMOYFRBYkm+4v0X1dfPIUJy56s7u5l3HPZynAEtU17al+WgoYQ/BRtoSBwxm5KSAjzlXQyPYdPofIe7wIVnR42rNB/4CKSyXpbRsL7sIxDOZKqmF6aW0kmzecWA04hnMJItanFRlq3r8dc1l+ODO5SB8d59tbnP4whBae8b0intd+TH+SAExVEyAQRB9dmk+EJ9cO88ssqeV8Ia3vpDXlrB9BXy5g1V6ttmaUm/UlOWWu5VJdxzSn80fZ0q9U6LcsRb+7oeIMI8+ZYcv8wiicqdP7xk7/WmRpq2gGwwRhCm3tId7TcvXG3sE1z9qsAAAAykGb1UnhDyZTAm+rEGk8ggLgEeKBAOPlAWNVYWqcqDwMpVeVAV74xBEUu3erWdlTMBrbLixYhnEZO+ZIJOuKyFYx/dfwTbDtAKH+ecyq8zwPa3Hv1r9w6g6L7h+Olv+hT1mCR8mNMx/RJ6e5DLlUQ5W15TlDhTI0pukTIbnf2EEaFtbpomm2quT3DgZolj2k438LJxR6PHtIPkrTVftaY/j/IS5/F+HOVLsnZGjy9JzZzPLQuPLQquZDalkdpn1+VNm7IGIZ7YXryaEAAAFBQZv3SeEPJlMFETyf8Ga5jsi0Zao8xhKj5lngUpt/GsNavDc9bHrtnBRAlG+SCAOQO+x9CB1lOJyQTTOpkp17BxTcs+Ddb6hlGFuy0Avj8IIcol2H2AwmV4Ut1qgKLCdbEa9Ibd53sBf19V16r70ObKr3kO1kyAX44gR+HRCQ3EPuVKs8BqvsIBGxoXdD/cyxBkpUASAjwLZDnn1GFbTn49M8TygtDArR+t4T+YDxGVhHUZ2locONZxghRYwC1SJy62174RmojHHknEOv1jxCbq63QM25Sm0pZ6KbjN6z4LW7jd46dWyQ5PxB74KditpjzBMUbcOv8YUQSMF3mgTDK14NO8cEyudj5GYF+jtozyRI0MNycBt7GtzUv23hAEzKZsqohd5zFVdgzSffh4W8VT+zYcLBiiaH1flxjRC4wJggAAAAKwGeFmpCX/LfX5fw/wk9SJIWXR4zPuzriltkwJkzB/gHLULEZbALmRFu0EcAAAFTQZobSeEPJlMCT8k7z8f7HGI0+WQKVGjBkTGLDqM4u2joFO7vZQsAkbH//4XcG7fB8276zN6VcP4oJF8wlv7EXbQ8psLfIPUAvYNClu8YMgAT0KzunZ6d7Q0mzxMBr2Zipaonv1W0L8fpTddHB5oyGKGFflL1ZDXgGHC/5EMsnKFyTxHXc/iPT7CcxaTwSasD8xOz8TJFb03FITtZNZRKiJEzE5w57M1jjT0DU7gF6wr6T41kzqKquX53GiW5Sm0fC3/YHdz4jc3w/2xHgYNtq5IVf8l2fPglHGCgqxkvWFn3rq0EnrcJFwrwUXq/Ypvq3iWm3g6ZCjzI5KoshLa4INYYNBCsJbBX7s55R7XckXhJpD+k1R1iu/1AjXXFG0kMJsmZlloiwrjfTfTFSDB/e0gl/8HeBqLgJxFecj23bM8slWzxpHzahmbEaFWcBFnFXrfBAAAAakGeOUURPGfug1uj/Tg1/cQdXaRMLdwiJXAfc6ydxE8Jkx191yHvwf3HYA/0fVJdKQSKuStumc+UJUU7IZ2ybLgu6ATJc+fGrbzs3n7cCtXNn9SvamoPKEhy+JdomttE1rJDafzZRKuvzIAAAAA0AZ5YdEIf+Vvuw3lWRcLBXyfWZkcypHS9oBABNkfEydcvMqhxx9NnpCJCzXTrBgLBS8SuMQAAACkBnlpqR3/vxA0yS/Q/14/dxDNrWHuznat9+5wQNPS2gK0BZrjLh7kqEAAAAKpBmlxJqEFomUwJP6Esqh0g0rFVWT6/1myEqahgk6ZJ28Q2tGnKyAr5bp9KXCJQ9mf6InpbTftPFqQ/afsEmPhBSaw5ZAl+X2jo2E9LlYpyA5xfvSAlneZb8TCaFO6hjrjb+n+/7xxaPGBjgFCbKKDHbVY/xmdgS9Z8dtfGI087Q4d7C7klTd6s+Be0gJZWl+dZ4VSLUyewgwbJb/OhBLhn0jUR4ClX/7D7QQAAANVBmn1J4QpSZTAv/9AFh0e6pLel51+E0Fwt9Wf5KWDLkpSqP0k/dBSu3helRDv7TpGUdkhta51KOjnhqi4czDx/nqYKvIsNeJQt3ziimAvlb1ZB/jc0/uSV0jGsXijzBRhouaJMTviJ13k3FYpVRzW9zsFDLWs2373IixDxTobkLD7dBOS6heYivNz2u0GAHcW/bh2oky3FYMYYbCpcjKUVWm6Lkj38ZOCCK8zKbn6lVOmr/UEFF1CtUE9KbgpHZDcsLNJASRVxt1h/ByKLFRkVRWPJI40AAADXQZqfSeEOiZTBTRMb/1P0VmjNsclyNs+ptWwa61ofhUfvYoxmxu7iILhBXGCV/+8N6VBZy3pJ7X71vYSN39WsvH4vcPXY7fyV1/MQVeNt9p4G4nAADPZIx49ivrp/BqEKyBmLZEoyWBjcPVohs00IG2tN8gRNrQxKh+2vfpgIIXifaX/zDf6+JnzINOcuy8CT9J/Ri4LHP7Ve5/BXkRw5ubyYmBpppnnoMFnVB0kvs63uaFzkT/JJ4wVhoQ6Rwm37xK9+4NhnezO2ZjFJlQCTLeE4PLWxfPwAAAAkAZ6+an/mRPa98SxYakNE8NCvrH3HwJ2N+kcfTqXEuh0MFmtgAAABcEGaoEnhDyZTAhf/tDynJzdCysZNpaZYpM0CfA3OBazGYxSANgd2BERCWwZaGHht01DFJt/731nBoPktBYw0sPSne3rNpB6oxn7rTiUYDMwcn89BskP2SoXKq5+e/WAQoGRefh7CfcL+CEp/OvPdEb5crSC0FHocCqsQOVbqFQ+YHDPDInH+ufnNqRfRQH5uV8etK4n+o+FaX9Wrsux6riUNsvAeJ+XDGM06UyEpcwUq9Wbc2WDtITqKlZNQ1GGG2zk1ZOXZzALk6kTEGQG3wLaAMvTEj3Y8VwRd2GnafI6fArodbSY7BPUueGtBWlaQu0bdJIjb41C5Oxzli0kCVEvAxYVXWxlEmSGPK0zOvtp7sMI3idDO6oJf5zVVWbpJv6OxZ/2Zi7Do3ova85d3rWgy3g7WMdLfcv1bqCxEoVIs/Hn1i3df13Cmt9yllhKbKt/MIVJ+he2FmSWYiV4/jp1n2wT6WepPn2FND4KlDv9hAAAAz0GawknhDyZTBRE8K//mFXM5dIBYa5gWMkzKg3hAE1so9q1Ckl3ab7BwoQmwyEnfZwcYIfpzdtI6mcmEiJJPpKIxnL+LT/+c4+ozmQjKZExdl4W8VZgE8kPeBab4SQyRQWP9rJpzPLJhufNrNWz9vKTOk3HF9hUG83UFLu64dlNemiyMFYjdEikKXjmeVIuM4JGDDG0Dv0t5aQe0wgsobQrQMVXRWZPSUS7O6YVAKnk1cFsNkcWzkap89MuXFUb51Rff4xZMr6QfdTVzHh6LkAAAAC4BnuFqX8JzvxqK0ICEpGrVSrvamUmVU6E+e8pPXFReYWOFBRfZtMOZidbCtnmfAAAAuEGa5EnhDyZTBTx/brt3x6K5U55lbgJnpaa4YHJ3LhhLoDn7ZiOx5eGdDiNR+gOMKG7CoCHTBM17wlrCCgToFW7MdAwJ4xUMLXr72qcyo4xElWT6WIzjq5wRFtWE4S0vuxx5sNpQ2PrlW+KfV3oq8U7/pzGYAFy458Eg+3XCUrlOnuHzcTSbQdQozHkPn9h8Olyesx2Lf4WujBwVXGCdo2EIsttRzUDr2nSqePshlPfapo3lK+feYeAAAAA5AZ8Dal/IcNRenBABeY/P03dqN0L417TEkYMKQuLU2NWvojdXLapx/P7v2/XZd/gkyDEDw+8MjIAhAAAAjkGbBUnhDyZTAl9bZ3YfGQjs5LpaHxDnhJiA6BWTiOd6rm97mpPBcuSGd7nCL2uzmi2UdVzVBnBPL6LbI9dGueyS0gpIkVW0BH2BF7iZmAE8wXORZARjS3gVUgW+kF5Je9JFwfRMkPnNuf3PRvmIULKujmnAt37V76ULAnEN/ydS6b6PVQYn0b06uz0qaDUAAACpQZsmSeEPJlMC/8PQKfJ1ZigVSh3MH/sdK9e2PYFQ/cj9aQ8J4XMqhLxKi5GbySl5ZJFyav8d9Ya8Rg7PsQMz4Orfw1iB5t5Rjfkr3gfO4Jkdhu9yeMQXCh+UW5CKOudk6GopAowJqoOxm08kNkKo2JVk9UY0yXvuIglneuxBbG7DQuT/2Ev9IVgDWEVLAl/ZRkoXzUxF06NBfJH+YL3bmqqOIcc/xvtJcwAAAKNBm0dJ4Q8mUwK/re3ozTzzh9mfMA+N9HV2lLXC/zA39avBcQiLPuu5ZPUPmdSnBTLCnsj7QiBbqjvO/eszLYfq1L2dOu78t54FNA2KX2cST0/2+kGlWsPW8gkwCqL6jOpirvSV0BgdHHWlPXich4aE9Yc0Bn+itPjUs67coU97JBhX8sZO0DwGs88fvBq7jZhYhobRWMyS/q1CzrEkitkPqvtpAAAAqEGbaEnhDyZTAk/wZ2vrCPcjTCXrl2JSAAkDlTX0g7WP33apZKkkCI3KId/veWpt7+j0qwLNBOR2Aieb/+Evo/ZsxhINnhb3Y2lwQq2JS95Ikn9lOJYz5OD/63GI1KGegSiSK9MME3iLr/+k2bptX91rnhDRbmobAY22N77OQXmWXeUPSe4GaDP+nX4crIYOu6w15nCLIvQHw37sCy3st07fezhvQ88SLQAAAMxBm4pJ4Q8mUwURPN+rEeDaBpAg0m5ZNJj5UZ5jxPCamdcHaNi6dC8b17DD/FmhHOXv5RA3EJzXwiukZi/kHEPck/4py8W08It0C6H/cORD7w8sd26ifjVgJ6gebLpCv3rA0SmnJpqE9psWafy1SWnaUYE123ADRNoYVc5sxEAB8UqFAynXb+h9CPDHfrbZTTXRbqbQmEDLqyeE+9E+0GGUOottW0bEacOKWQfjLJH8Glqu3XrLOTO5LUfORLWV5CTNu45ZhYfk7TuOugkAAAAyAZ+pakZ/72U/dP5uSx6hR1dylsUCltgbu5vVhSWle6X6tsgHZjObg37B5ZV+Sg8QhIEAAACxQZurSeEPJlMCb63GSmNeB0ZHNMtZVfrcBsFx0PnYyFbat+8/mp8gtiAgGHIjpO2oXxjKt1fTtnlsWsMDJ+efWZGwcpnhTPjpHEQxzcMUcYbyF+Ab04w5e47gYl/F5C6vXdI7o7RkQa9/4wnup+OmBxD1cW19x9jDZW0NDaSpSss37bfj0hPfx3XCyHV1y52sy21NDJej6T7GARPN4RO7fBBJ1cC3SqjYyiy+MVcaKCqAAAAA+EGbzEnhDyZTAm/3xFYyKvl7nzDF1eTWNPyov626i+n/iMCbuHChIJK+6ef/uL0u2/NHekz16qilA2hIW+jHfzWHKep/ncDfRobxeFmvZX+LW3IbRR8taoE5PVvgk3gHdigqE2tJxlvDotluykjEUWfimVao3CXon7XHcKfGYRxip67d9An/bs2DbarHzmhSs2jeV2oIKiDw1Xs4AkKT253IORz5RCsWyOVGwXbba3h6STy7P+kmEcn/EKtTONbvj5l2YsKclBqHbWl0bidg61C0QoIvKtK0EG8sMYlAf9YHyv8ZzySe93Y53vET0pHY5b9F9Mzm6TceAAABV0Gb70nhDyZTAiP/9Jq7DUsNhutZXxfS38vWVdBBgDGBEgWtMAXcIlDP9Nj1a1DX19rwvNB7HhiZ8jmPUnLrza9fJg4MFw0TGXp3zpM/kQotKUTwY34Gm/HrhmdX0g43LU6JtOAePHau7zEuEkkPFviiiYwIzLNp6j80T6LW3hcUfQmJUAdjhueivI0VgjmFccrJF48+EzzexoNfcJiz/qvLQjtrlfbMP0Y3UABqk7kdlR3/Q22Id3rpgpiVwkkOHB9NCaCMh9MMRmrUf6dfPuqVJVTl5nl/ayivxknmw+MaHdaF5e04/D80R+QiPPM1ZUEgkl1G309AW1mkR6Z7IxNUOzrt3XaSu2vShArTYzF6S4FHauPWSL36X2au3IijeWRy4KdoHzoxC2yuWGWTLzC3NXTKaO+OJUWJBAqXF6GLiKOxMDujgd23Or93P5GoWUE9tOdeTesAAAA5QZ4NRRE8V/3dwfKiwMB1uOFwKUL8P7RqjrpBfA2x+2WaqJXJG+hn39+mjzPHPwT/kwa6pm2lewCBAAAAIAGeLmpHf/ANIPg8pmSy5IsR/7Na2yapze4zHA62RSFxAAAAtkGaMEmoQWiZTAiPv1wGQdRvJ2SlbJ+JoS3a2PMIKOp8Khj7iecdWdiOzOV46UFmAfp+cQ4yP2e7WPrxLokp7CG36YpXJRqbRf7KeY+Eed86HBobPRDqvKXMFrmHIew12iuA16o6yD02wuafvbmMtbJU4je4k81jBJmlJ8lB71d314eAEAioPP/ERttTvYCdTig4tqfrQH89sscibFRFOP8DpqF0zH4jIgLR6he6ZyFTZWvB3rpAAAAAx0GaUUnhClJlMCI/v17ay1RZDeaxy2jbImBq53gqOBCDgFb+yAPT9Y9nj0BQ6p52Fs/13vDGFGCFBdNDGCkZQ96EmN+RoDt0rHvdeuIuG32ONygvmeNlNNTvHnjF3c9vneU8MOIWPqC829hvgjetRqDrUQTbAbUxKUgOzfnDMqeCQyZsRxCwq7vRw/2PlbX8oUcwH/sCNKywquScylYCsY5hsKBXLg33vZEgIQRmj4e84xBQ7lr1a8IaFHGmIK5saFRgPfcewZoAAADcQZpySeEOiZTAm/+sRjVDRFQoGeLfdTGJoKtElHNci2IlKoq2v2+s9OS42Lq9zaZKyC8mmvzihCmRJcyq12BMq/E/j2P7DVpi/x/LVREYqHJZ55u6ahxAwnBhHEgksDimWfzCgor8waXezWKpAkhGOHoRmM9iX6iU/juc7b3XfXt2YPSToiC1jrNBab3IxACYsZ1XzrUnD4jUTDvB2g2EDNyHs1kGaW9A0VIWY+jY0eLgMLnrwNEhY6mvX+P0bA0xiex6UNWg6dZkX+zVx0gAzgCFTRCI3+2YjIxyQQAAANJBmpNJ4Q8mUwIj/8DhWX3fP4MpNZAQ9gEfOIp4WXJqoRF1mLybWhyuhbdL8PlvK/U/7GONHQTqFTQENy5zuAB7aMVhSNgdKDgW7wzYAA485Ni9Z4yWz6tSBP4eZbSQkt9l7HeG0P/UV/enTq0fV54l3+x1HwhlEcj+NTb48eOJlFyKn8/t8T6nmy9zwHwujMZ7TNs4MMx4/koYpXNNwBAIG2n+cvjkKa+hxXb++qvXiIPhxxZM43F2YYDhbZU4BYlsMuDk/V15lRA+nsk5FrtZ54AAAADqQZq0SeEPJlMCb/LL/9b3RnXBBMV4FCZgknu6daWQweChTkCoxJRx+Tk8IW7e0DUKMKKlr4OcRaxhGzIXFsHunThAJRJ9v+vsuCiYVniq/nelqLh1fuAeshNbVA7u5uYkWxhNyckjOeOCnj6BD7PoRz+vqkA2xskgCw4zRooFdCDgYMnsSPyQKoDMYD9/1IyLNHstwn7q16mTeV6XDA70WLydEO58CnDzslcDgzaHQ0lQAVrRE0oM+Qp8Zuz+zB2IkjX/KutZLODA7bjqEFJal1hwIBzQi7AUOKREokWdLUfu/xQU8kq0vKtQAAAA0UGa1UnhDyZTAm+tybB6C47H3aOprWX09oKBYBh00ycCfLLn/n8YmqixWjyM203rTBk3/NY0QfM21kKJN+OnYAZTl6q/T2ep83muphpKOmvy5axRK+K6V6ibuG0pfg86wr5K1n8SCHcDtPoUbEbirWsHEvySiFtDQdR8tUpn2K63SBAOwDWO5QXNN0avkpxSrLVUG+JsXGY/aXgDI/vOiUxGRlDHCwqiK/AtO4Ega3cm66unR2oPTcTjZnglx9w1CEmerIbzrDXgpjWhI0JJCKiBAAAA2UGa9knhDyZTAm+rErhbMcciFBu6jeJFPfGY2kx9GkDzXCLHlWzruzkOnggZHqgcNiCpdYuwStFF78lIf8caDDBozBpLoYomoQVgWPAARKFIV1c+U/0i64p04nXxraXIXPc6d18/jIb225fZ8aO1ASbmRAAJ4ExCKiexdXixCCV+4gvlJNcMmX1qOsjri1BkXAxhmtHVZgIpAb1eA14ZkV5C4nyrZP8MZTA5J53YrRrM1sSzNfAt5ta18Z57t+AewarA1mz9SkHtXOQRejTNGos0cMrUfElGiwwAAAFbQZsZSeEPJlMCb9J0UCQVFXHGDS4g/CuZEdkjSt7ndI4Gknq/RnfjB8alhYUjdl/TiVgbsyMX4Af6UqmB+jh/rzl48jchEiA6hPVlLPexi5r78WxW4ghsAT408pgSHsNLSxfVOznb8Fn8WsFuenTRo+KQ8938/phasZU+yfv4m4tjGhk9AnnrYlbVAiVLO5ijR1PAMgLD9LzZfI6XlcxHDrAs1r13FdpwkZJerUO1QHQ1CidwuJOafYt1CU3wsmSSersJkfalMXhVDkZ0y75L0JUAra4M8luYo+N07jVC4gjyjky/te/AYU46Umi0W9Lrcxs+bcb/zoEq+2bvaNMGXfAgUzDVzrEjh9Ht8VGnMSAtjfcPgnAU4uxxN6UuYSnVmfR0kqQ06TYJZoMWrWZ9e3/LJU7cejxDVlqK2EFRtW2W2KEDlqhzFx1M8LIHO16aNkFUyhFd7oNfylkAAABaQZ83RRE8Z+7oXmGfMfD6yqqlN31wG9fjRQ4fYR6xUNPcSTBmI9IAn5cna+yTsMnCoyVU4JT/mocjHFMJarZrGfcw8rH4+6mYBwIHpbg3V9I8Eqf5TqS9jRHhAAAAXgGfWGpHf/cOpSA1zmueWEirbDttOTkg+0QZHhmXJ0B8eQWQ/ucwb/oCEd3zRdsz9jFs7w+K8kzb0TGbE/LLyVl7GtPEgKBQP3WNQIpWaEjYVWPnB63wqJXC2QI40LYAAADVQZtaSahBaJlMCb/xCJGzoPK8kZ1DfrPvbtw+Crr7mFfRblOUe6Z34tReICgH3qWTMmNXryA0wHj2OXJVHgP2v9v7rtVtbzlCiwk0r8VGSX/NPUzt3LE9aQ4cYSN1u7ZJBEgqe+UY5GLFS6SB+hYsqMgY0EAkh3FWErF+7n4Sb7OZnoa6qYzH0M3BbUTasVUeji517vIG76jtfybEOSOn9th5ZiRuIhEZU/xdp/wl0DJOj5TowLgj6KiURXCYKGyE3E4lv7UR/8NTa1VvWPf3eg0Z+K6BAAAA2kGbfEnhClJlMFESxH++KgYPe3staTknCeLmGZ+sbQ3ZQ3WiFRNih0IPOOssEk8kYx81ErKiKArLJawN5PotmHZbVrH+IjxlgCtZSGeXX/Y9IAcKZQznw7tHQqm2z/2tBmlkENgO8DDb3lequs9PRAjpWsEPBgAxVLrzry37vFOmqF/fh4fhjTW9pS9TZnc0Fi91M/E9aStGLuhN5fZK2yB/LzY6gzLvoQtC+bSa9hlh1l91gwQBSgJ+kDcpfItXULoaxc7VfmBjv/cYHqxErraLTeovZOaVcqNAAAAAQgGfm2pHf/C2nJoG1uNInNIkgcJ8X4gIzRvff7vpnuDjkJCG6vOkdfHCF2rOD0r118sNsOAul2Cs+vhM68/LOjDFgQAAAOxBm55J4Q6JlMFExX/FPrK1J5+F8/8wuSyRBfkudtfRQMNpWtzVkj+Qcp+o84Rykh9T6l6AvBRKP4okLGWTB4KSjZd7JxU+ZmXgyKEmsqlrACWarLEBrB//8VB/dN6hg2zRQbHD1Y+In6Yc7PoV0/ibqzlWMXZBMrSmqoE8AYbQh9T+PGRteuzzdcoV4NxM6qHFv+XuespkCOZLm82TTOGk65Qzh4Xn2HGRVJnBO/ydIiQnoa9FnDtmbv/1CbIoXYcSUiHO8tLprdkS9/bFobI/PfokJKq4XPsZn9yWBPre58D75FIFF8a5PrX0XwAAAEgBn71qR3/3EhQONA+ql/8fWsJJq4LG7SBZE8EC097DN5HN5B0CFp/Al++ajQGXPTvTEcpoGvK3g7XnjkB0+bSqu5Mzf/lmToAAAAASQZu/SeEPJlMCO/907FQGOBJqAAAHym1vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAABkAAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAb0dHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAABkAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAABAAAAAQAAAAAAAJGVkdHMAAAAcZWxzdAAAAAAAAAABAAAZAAAABAAAAQAAAAAGbG1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAKAAAAQAAVcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAABhdtaW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAXXc3RibAAAAL9zdHNkAAAAAAAAAAEAAACvYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAABAAEAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADVhdmNDAWQACv/hABhnZAAKrNlEJsBEAAADAAQAAAMAoDxIllgBAAZo6+PLIsD9+PgAAAAAEHBhc3AAAAABAAAAAQAAABRidHJ0AAAAAAAAZeEAAGXhAAAAGHN0dHMAAAAAAAAAAQAAAIAAAAIAAAAAFHN0c3MAAAAAAAAAAQAAAAEAAAKgY3R0cwAAAAAAAABSAAAAAQAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAIAAAQAAAAAAQAACAAAAAACAAACAAAAAAQAAAQAAAAAAQAACAAAAAACAAACAAAAAAYAAAQAAAAAAQAABgAAAAABAAACAAAAABIAAAQAAAAAAQAACAAAAAACAAACAAAAAAIAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAAEAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAIAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAAEAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAACAAAEAAAAAAEAAAgAAAAAAgAAAgAAAAAHAAAEAAAAAAEAAAgAAAAAAgAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABAAAAAAcc3RzYwAAAAAAAAABAAAAAQAAAIAAAAABAAACFHN0c3oAAAAAAAAAAAAAAIAAAAYrAAAAEgAAAAwAAAALAAAACwAAABQAAAANAAAACwAAAAsAAAA5AAAAFwAAAAsAAAANAAAAOAAAAA4AAAALAAAACwAAAHEAAAAuAAAAFQAAAAsAAACwAAAAPwAAAAwAAAASAAAApgAAADgAAAAbAAAAKQAAAIsAAACIAAAAuQAAACsAAAAoAAAAkwAAAG8AAACwAAAAggAAAOsAAABXAAAAIgAAAJUAAACRAAAAkAAAAJoAAADFAAAAyAAAAVAAAAAzAAAArwAAAL0AAAD9AAAAowAAAPoAAADKAAAA5gAAANoAAADIAAAA0gAAAL4AAAE/AAAA1AAAANsAAADdAAAA4QAAAPgAAADpAAABJgAAAGAAAABVAAAA+wAAAQ4AAAGFAAAAaQAAAZQAAACOAAAAWQAAAFEAAAGzAAAAUQAAAW0AAABVAAAA/QAAAQ0AAAGJAAAAzgAAAUUAAAAvAAABVwAAAG4AAAA4AAAALQAAAK4AAADZAAAA2wAAACgAAAF0AAAA0wAAADIAAAC8AAAAPQAAAJIAAACtAAAApwAAAKwAAADQAAAANgAAALUAAAD8AAABWwAAAD0AAAAkAAAAugAAAMsAAADgAAAA1gAAAO4AAADVAAAA3QAAAV8AAABeAAAAYgAAANkAAADeAAAARgAAAPAAAABMAAAAFgAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1OC43Ni4xMDA=\" type=\"video/mp4\"></video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sYNqgSf-hmtX"
      },
      "outputs": [],
      "source": [
        "save(folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "###save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "# T=20\n",
        "# bptt=None\n",
        "# if T==None: T = 256\n",
        "# if bptt==None: bptt = min(T,32)\n",
        "# d_model=agent.d_model\n",
        "# # sx=torch.randn(1, d_model)\n",
        "# # batch=sx.size(dim=0)\n",
        "# # x_ = torch.rand((batch, T, 3),device=device)\n",
        "# # x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# # x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# # x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "#*self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# # x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "# x=nn.Parameter(x_.clone())\n",
        "# # optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "# # optim = torch.optim.SGD([x], lr=1e3)\n",
        "# # optim = torch.optim.AdamW([x], lr=3e-1)\n",
        "# optim = torch.optim.AdamW([x], lr=1e-0)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(20): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "\n",
        "z=nn.Parameter(z_.clone())\n",
        "print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([z], lr=1e3)\n",
        "# optim = torch.optim.AdamW([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 10\n",
        "agent.jepa.eval()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "### trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mhkK_9AQm8_q",
        "wUhKd009Qvk3"
      ],
      "provenance": [],
      "mount_file_id": "1X2rEy9liqB6p-8y0dU_Qo5qAkZzUibWY",
      "authorship_tag": "ABX9TyNZV+PhI7b7UFu4An5Dk4IA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}