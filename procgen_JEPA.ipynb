{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WkwnVjJTrW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4726dd8c-bf16-48f2-d715-a35355b10eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "ff23b2a8-6990-42c3-f16a-fa416091fd5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model=256, drop=0.5):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Dropout(p=drop),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title transfer_sd store_sd load_sd\n",
        "\n",
        "def transfer_sd(tgt_sd, src_sd): #\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            # print(wht_name, tgt_wht.shape, src_wht.shape)\n",
        "            if tgt_wht.shape==src_wht.shape:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "                continue\n",
        "            if tgt_wht.shape[0] != src_wht.shape[0]: continue # output dim diff\n",
        "            if len(tgt_wht.shape)==2: tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "    return tgt_sd\n",
        "\n",
        "def store_sd(all_sd, new_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in new_sd.keys():\n",
        "            if not wht_name in all_sd.keys():\n",
        "                # print(wht_name, new_sd[wht_name].shape)\n",
        "                all_sd[wht_name] = (new_sd[wht_name],)\n",
        "                continue\n",
        "            all_tpl, new_wht = all_sd[wht_name], new_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                print(wht_name, all_wht.shape, new_wht.shape)\n",
        "                if all_wht.shape==new_wht.shape:\n",
        "                    all_wht = new_wht\n",
        "                    break\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: continue # diff output shape\n",
        "                if len(all_wht.shape)==2: all_wht[:, :new_wht.shape[1]] = new_wht[:, :all_wht.shape[1]]\n",
        "                break\n",
        "            if len(all_wht.shape)>=2 and len(all_wht.shape)>=2:\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: all_tpl = all_tpl + (new_wht,) # wht not in all_wht\n",
        "    return all_sd\n",
        "\n",
        "def load_sd(tgt_sd, all_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in all_sd.keys(): continue\n",
        "            tgt_wht, all_tpl = tgt_sd[wht_name], all_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                # try: print(wht_name, tgt_wht.shape, all_wht.shape)\n",
        "                # except: print(wht_name, tgt_wht, all_wht)\n",
        "                if tgt_wht.shape==all_wht.shape:\n",
        "                    tgt_wht.copy_(all_wht)\n",
        "                    break\n",
        "                if tgt_wht.shape[0] != all_wht.shape[0]: continue # output dim diff\n",
        "                if len(tgt_wht.shape)==2: tgt_wht[:, :all_wht.shape[1]].copy_(all_wht[:, :tgt_wht.shape[1]])\n",
        "                break\n",
        "    return tgt_sd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# modelsd = torch.load('agent.pkl', map_location=device).values()\n",
        "# tgt_sd = transfer_sd(agent.state_dict(), modelsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = {}\n",
        "# all_sd = store_sd(all_sd, agent1.state_dict())\n",
        "# print(all_sd.keys())\n",
        "# checkpoint = {'model': all_sd}\n",
        "# torch.save(checkpoint, 'all_sd.pkl')\n",
        "\n",
        "# agent3 = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "# agent3.tcost = tcost3\n",
        "# tgt_sd = load_sd(agent3.state_dict(), all_sd)\n",
        "# agent3.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "# for x,y in zip(agent1.state_dict().values(), agent3.state_dict().values()):\n",
        "#     print((x==y).all())\n",
        "\n",
        "# print(agent1.jepa.enc.cnn[1].num_batches_tracked)\n",
        "# jepa.enc.cnn.0.weight\n",
        "# print(agent1.jepa.enc.cnn[0].weight.shape)\n",
        "# print(agent1.jepa.enc.cnn[0].weight[0][0])\n",
        "# print(agent3.jepa.enc.cnn[0].weight[0][0])\n"
      ],
      "metadata": {
        "id": "Bos81kQf1dwh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rename_sd\n",
        "def rename_sd(agent_sd):\n",
        "    sd_={}\n",
        "    convert={}\n",
        "    na_=''\n",
        "    for wht_name, wht in agent_sd.items():\n",
        "        o=wht_name.split('.')\n",
        "        # print(\"####\", wht_name)\n",
        "        name=wht_name\n",
        "        for i in range(len(o)):\n",
        "            c = o[i]\n",
        "            if c.isnumeric():\n",
        "                na, me = '.'.join(o[:i]), '.'.join(o[i+1:])\n",
        "                c=int(c)\n",
        "                if na!=na_: # param name diff\n",
        "                    j=0 # reset num\n",
        "                    c_=c # track wht_name num\n",
        "                    na_=na # track param name\n",
        "                elif c_<c: # same param name, diff num\n",
        "                    j+=1\n",
        "                    c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "        # print(name)\n",
        "        sd_[name] = wht\n",
        "        convert[name] = wht_name\n",
        "    return sd_, convert\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# # modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, _ = rename_sd(modelsd)\n",
        "\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "SFVbGqMDqcDR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title transfer_optim me\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# def transfer_optim(tgt_sd, src_sd, tgt_optim, src_optim): #\n",
        "def transfer_optim(tgt_sd, src_sd, tgt_optim_sd, src_optim_sd): #\n",
        "    non_lst = ['running_mean', 'running_var', 'num_batches_tracked', 'num_batches_tracked', 'loss_fn']\n",
        "    tgt_lst, src_lst = [], []\n",
        "    for i, (k,v) in enumerate(tgt_sd.items()):\n",
        "        # print(i, k, v.shape, any(s in k for s in non_lst))\n",
        "        if not any(s in k for s in non_lst): tgt_lst.append(k)\n",
        "    for i, (k,v) in enumerate(src_sd.items()):\n",
        "        if not any(s in k for s in non_lst): src_lst.append(k)\n",
        "\n",
        "    # tgt_optim_st, src_optim_st = tgt_optim.state_dict()['state'], src_optim.state_dict()['state']\n",
        "    tgt_optim_st, src_optim_st = tgt_optim_sd['state'], src_optim_sd['state']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, wht_name in enumerate(tgt_lst):\n",
        "            if not wht_name in src_lst: continue\n",
        "            tgt_wht, src_wht = tgt_optim_st[tgt_lst.index(wht_name)], src_optim_st[src_lst.index(wht_name)]\n",
        "            # print(wht_name, tgt_wht, src_wht)\n",
        "            tgt_shp, src_shp = tgt_wht['exp_avg'].shape, src_wht['exp_avg'].shape\n",
        "            if tgt_shp==src_shp:\n",
        "                tgt_wht = src_wht\n",
        "                continue\n",
        "            if tgt_shp[0] != src_shp[0]: continue # output dim diff\n",
        "            if len(tgt_shp)==2:\n",
        "                tgt_wht['step'] = src_wht['step']\n",
        "                tgt_wht['exp_avg'][:, :src_shp[1]] = src_wht['exp_avg'][:, :tgt_shp[1]]\n",
        "                tgt_wht['exp_avg_sq'][:, :src_shp[1]] = src_wht['exp_avg_sq'][:, :tgt_shp[1]]\n",
        "    # return tgt_optim.state_dict()\n",
        "    return tgt_optim_sd\n",
        "\n",
        "# model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "# model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "# source_optimizer = optim.AdamW(model_src.parameters())\n",
        "# target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "# dummy_input = torch.randn(3, 10)\n",
        "# dummy_target = torch.randn(3, 5)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "# output = model_src(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# source_optimizer.step()\n",
        "\n",
        "# dummy_input = torch.randn(3, 20)\n",
        "# output = model_tgt(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# target_optimizer.step()\n",
        "\n",
        "\n",
        "# print(source_optimizer.state_dict())\n",
        "# print(target_optimizer.state_dict())\n",
        "\n",
        "# optimsd = transfer_optim(model_tgt.state_dict(), model_src.state_dict(), target_optimizer, source_optimizer)\n",
        "# target_optimizer.load_state_dict(optimsd)\n",
        "# print(target_optimizer.state_dict())\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "riBHnAAkkzrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AfjFbveH64Io"
      },
      "outputs": [],
      "source": [
        "# @title TCost\n",
        "class TCost(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=256): # in_dim=(1+self.jepa.pred.num_layers)*d_model\n",
        "        super().__init__()\n",
        "        self.tc = torch.tensor([-1., 0.], device=device).T\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_dim, 2, bias=False), nn.Softmax(),\n",
        "            # nn.Linear(in_dim, d_model), nn.ReLU(),\n",
        "            # nn.Dropout(p=0.5),\n",
        "            # nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, 2), nn.Softmax(),\n",
        "            )\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def update_loss_weight(self, train_data):\n",
        "        a = len(train_data.buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "        # self.data = [step for episode in buffer for step in episode]\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tcost(x)@self.tc\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        out = self.tcost(x)\n",
        "        # print(\"ctost loss\", out, y)\n",
        "        y = torch.where(y < -0.5, 0, 1)\n",
        "        return self.loss_fn(out, y)\n",
        "\n",
        "\n",
        "# tcost=TCost(1024)\n",
        "# x=torch.rand(256,1024)\n",
        "# import time\n",
        "# start = time.time()\n",
        "# out=tcost(x)\n",
        "# # out=F.gumbel_softmax(out)\n",
        "# print(time.time()-start)\n",
        "# # nn.AdaptiveLogSoftmaxWithLoss(in_features=2, n_classes=2, cutoffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "76cdf849-f549-48d6-cde7-a03faa82cca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-5d08c402e0cb>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v, drop=0.2):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=drop)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)# + self.z_coeff * torch.norm(z)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "# torch.norm(z, dim=-1)\n",
        "# -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "# in RL, distribution of action, if certainty is high, entropy is low\n",
        "\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "96865f78-a529-497b-874d-c9b8fd08ff6e",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-0c232f84ef29>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=8, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.closs_coeff=100.\n",
        "        self.zloss_coeff=1.\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # e = d_model**-0.5\n",
        "        # self.h0 = torch.empty((self.jepa.pred.num_layers, 1, d_model), device=device).uniform_(-e, e) # [num_layers, batch, d_model]\n",
        "        # self.h0 = torch.normal(mean=0, std=e, size=(self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # torch.nn.init.xavier_uniform_(self.h0)\n",
        "        # torch.nn.init.kaiming_normal_(self.h0)\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z) # [batch,T,dim_az]\n",
        "        self.lx, self.lz = torch.empty((0,dim_a),device=device), torch.empty((0,dim_z),device=device) # [T,dim_az]\n",
        "        state = torch.zeros((1, 3,64,64))\n",
        "        self.sx = self.jepa.enc(state)\n",
        "        self.la = torch.empty(0,device=device)\n",
        "\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        # if len(self.la)>0 or laction!=None:\n",
        "        if len(lstate)>1:\n",
        "            self.update_h0(lstate, laction)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "            #     # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            #     # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            #     # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            #     # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "                self.sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "            #     # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(self.sx, T=20, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.la, self.lx, self.lz = lact, lx, lz\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx - torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    self.la = torch.cat([torch.tensor(laction, device=device), self.la[len(laction):]], dim=-1)\n",
        "                la = self.emb(self.la[:seq_len])\n",
        "\n",
        "        # lz = nn.Parameter(torch.zeros((batch, seq_len, self.dim_z),device=device))\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        optim_z = torch.optim.SGD([lz], lr=1e1) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e0 ; 3e-2 1e-1\n",
        "        lsx, la = lsx.detach(), la.detach() # [T, d_model], [T, dim_a]\n",
        "        # print(\"update_h0 lz\", lz.data)\n",
        "        for i in range(5): # num epochs\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0.detach()) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                loss = F.mse_loss(out_, out.squeeze(0))\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            print(\"update_h0 loss, lz\",i,loss.item(), lz.data)\n",
        "            with torch.no_grad(): lz.clamp_(min=-1, max=1)\n",
        "            # with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1].unsqueeze(0)\n",
        "        self.la, self.lx, self.lz = self.la[seq_len:], self.lx[seq_len:], self.lz[seq_len:] # [T, dim_a], [T, dim_z]\n",
        "        return h0\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].unsqueeze(0).repeat(batch,1,1), self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(10): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"search loss\", loss.squeeze().data)\n",
        "            # print(i, \"search x act z\", torch.cat([x[0],torch.argmin(dist,dim=-1)[0].unsqueeze(-1),z[0]],dim=-1).squeeze().data)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        print(\"c\",torch.stack(c)[:,idx])\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "\n",
        "\n",
        "        # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "        # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "        # print(\"reward, pred\", reward[mask].data, pred[mask].data)\n",
        "        # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "        # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        # sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(1)\n",
        "        c=[]\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                # out, h0 = self.jepa.pred(sxaz[:,t], h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            c.append(tcost)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def argm(self, sy, sy_, h0, a, reward, lr=3e3): # 3e3\n",
        "        batch_size = sy.shape[0] # [batch_size, d_model]\n",
        "        z = nn.Parameter(torch.empty((batch_size, self.dim_z), device=device))\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim = torch.optim.SGD([z], lr=lr)\n",
        "        optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.95))\n",
        "        sy, sy_ = sy.detach(), sy_.detach()\n",
        "        out = sy - sy_\n",
        "        h0, a, reward = h0.detach(), a.detach(), reward.detach()\n",
        "        for i in range(10): # 10\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                repr_loss = F.mse_loss(out, out_[:, -1, :])\n",
        "                syh0 = torch.cat([sy.flatten(1),F.dropout(h0_, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                z_loss = torch.abs(z).sum() # z_loss = torch.norm(z)\n",
        "                # print(\"z_loss\",z_loss)\n",
        "                cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            # with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z.detach()\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sx=sy_\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "\n",
        "                    # z = self.jepa.argm(sy_, a, sy)\n",
        "                    z = self.argm(sy, sy_, h0, a, reward)\n",
        "                    z_norm = torch.norm(z)\n",
        "\n",
        "                    # with torch.no_grad(): z.mul_(F.dropout(torch.rand_like(z), p=0.5))\n",
        "                    with torch.no_grad(): z.mul_(torch.rand_like(z)).mul_((torch.rand_like(z)>0.5).bool())\n",
        "\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "\n",
        "                    # cost loss\n",
        "                    # syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "\n",
        "                    # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "                    # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "                    # print(\"reward, pred\", reward[mask].data, pred[mask].data)\n",
        "                    # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    # torch.norm(sy-sx, dim=-1)\n",
        "                    # sx=sy\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    norm = torch.norm(sy, dim=-1)[0].item()\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "# !pip show torch triton\n",
        "# # !pip install --upgrade torch\n",
        "# !pip install --upgrade triton\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "# print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "# print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "# print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "09fdf32e-2565-4707-e5a2-871aa2290f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II\n",
            "From (redirected): https://drive.google.com/uc?id=1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II&confirm=t&uuid=e07dca37-d91c-4572-9dcb-155615d2565c\n",
            "To: /content/agentoptim.pkl\n",
            "100% 28.1M/28.1M [00:00<00:00, 28.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB\n",
            "From (redirected): https://drive.google.com/uc?id=1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB&confirm=t&uuid=ffec0fe8-79c2-44b2-b0e3-655f13ed89f1\n",
            "To: /content/buffergo.pkl\n",
            "100% 786M/786M [00:17<00:00, 46.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1 gru3 tcost1\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2 gru3 tcost1\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3\n",
        "!gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4 gru1 tcost1 drop\n",
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 1UDgNtFsWGAhvqR9lwA0QbMLhUtmip4ne -O agentoptim.pkl # M1 agentoptimgru3tcost1\n",
        "# !gdown 1-0oc6yucS5JXLHX1zqbYe3NTVMuhP_5r -O agentoptim.pkl # A2 agentoptim25251c25z3\n",
        "\n",
        "# !gdown 1sCW9uvcdCJkCH5HQDdISLws5rMvmkmFR -O all_sd.pkl # M1 all_sd\n",
        "\n",
        "import pickle\n",
        "# !gdown 1j9hOq8_752duPB0PMYUJqabNvYoGLysX -O buffer512down.pkl # S\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "# with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB -O buffergo.pkl # S3\n",
        "!gdown 1egXy0t_kn0M0oL6sbwixoVr7bqMfcB8j -O buffergo.pkl # T4\n",
        "with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "6eb6b503-83aa-4339-9c61-bde461e8f8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-0145f385af3d>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "# with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# # # modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptimargm.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "modelsd = transfer_sd(agent.state_dict(), modelsd)\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "# # optimsd = transfer_optim(agent.state_dict(), modelsd, optim.state_dict(), optimsd)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = torch.load(folder+'all_sd.pkl', map_location=device)\n",
        "# # all_sd = torch.load('all_sd.pkl', map_location=device)\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in all_sd.items())\n",
        "# allsd = {}\n",
        "# for (k, v) in all_sd.items():\n",
        "#     try: allsd[convert[k]] = v\n",
        "#     except Exception as e: print('dict err', e)\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# tgt_sd = load_sd(agent.state_dict(), allsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# for i, (k,v) in enumerate(modelsd.items()):\n",
        "# for i, (k,v) in enumerate(agent.state_dict().items()):\n",
        "#     print(i,k,v.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# checkpoint = {'model': agentsd, 'optimizer': optim.state_dict(),}\n",
        "# # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# # torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "# torch.save(checkpoint, folder+'agentoptimargm.pkl')\n",
        "# # torch.save(checkpoint, 'agentoptim.pkl')\n",
        "\n",
        "# all_sd = {}\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# all_sd = store_sd(all_sd, agentsd)\n",
        "# torch.save(all_sd, 'all_sd.pkl')\n",
        "# # torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVcknabHMxH6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.buffer = self.process(buffer)\n",
        "        self.data = [step for episode in self.buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 80):] for episode in cleaned]\n",
        "        random.shuffle(cleaned)\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 #512\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e3fpbtNOiz1",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "9c944347-25c4-4f1d-8669-382b739e73e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.])\n",
            "tensor([ 0., -1.,  0., -1., -1., -1., -1., -1., -1.,  0.])\n",
            "tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0., -1., -1., -1.])\n",
            "tensor([-1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-9.9379e-01, -9.4889e-01, -1.4014e-12, -4.5385e-14, -8.0587e-01,\n",
            "        -1.5923e-11, -5.8945e-18, -5.9986e-16, -3.2164e-13, -6.8444e-21])\n",
            "tensor([-2.0735e-12, -9.9641e-01, -3.5738e-11, -9.9049e-01, -5.7662e-01,\n",
            "        -9.9900e-01, -1.0000e+00, -4.8257e-15, -5.0000e-01, -1.7090e-14])\n",
            "tensor([-3.4244e-15, -1.4717e-13, -1.7288e-13, -5.6475e-10, -9.3495e-09,\n",
            "        -9.9988e-01, -7.7362e-13, -9.9428e-01, -9.9785e-01, -1.6338e-01])\n",
            "tensor([-1.0000e+00, -1.6130e-12, -9.9996e-01, -4.5936e-11, -2.3056e-10,\n",
            "        -5.6134e-13, -1.7468e-16, -2.4984e-14, -1.2474e-07, -9.8383e-01])\n",
            "tensor(0.4602, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor(0.0542)\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "reward, pred tensor([-1., -1.]) tensor([-4.8257e-15, -1.6338e-01])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "ctrain_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    ctrain_data=list(zip(state,reward))\n",
        "    ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# images, labels = images.to(device), labels.to(device)\n",
        "batch=40\n",
        "images, labels = images[:batch], labels[:batch]\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range(len(labels)//10):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# # try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch, agent.d_model), device=device)\n",
        "    # h0 = torch.empty((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device)\n",
        "    # torch.nn.init.xavier_normal_(h0)\n",
        "    sy = agent.jepa.enc(images.to(device)) # [batch_size, d_model]\n",
        "    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "    pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "\n",
        "    # print(pred)\n",
        "    for x in range(len(pred)//10):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(agent.tcost.loss(syh0, labels.to(device)).squeeze(-1))\n",
        "print(F.mse_loss(labels, pred))\n",
        "\n",
        "# torch.where(abs(labels- pred)>0.5,1,0)\n",
        "for x in range(len(pred)//10):\n",
        "    print(torch.where(abs(labels- pred)>0.5,1,0)[10*x:10*x+10])\n",
        "\n",
        "mask = torch.where(abs(labels- pred)>0.5,1,0).bool()\n",
        "print(\"reward, pred\", labels[mask].data, pred[mask].data)\n",
        "try: imshow(torchvision.utils.make_grid(images[mask], nrow=10))\n",
        "except ZeroDivisionError: pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labels =\n",
        "print(F.mse_loss(labels, pred))\n",
        "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "\n"
      ],
      "metadata": {
        "id": "viimAIpYSJq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OksdjCeJYpYh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1e35680-fb69-4ca2-9364-2e3e691af9b4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n",
            "<ipython-input-10-1fc6065034a3>:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repr, std, cov, clossl, z, norm 0.09168028086423874 0.0234832763671875 4.840979099273682 0.32667893171310425 1.356770396232605 3.283203125\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (262) must match the size of tensor b (267) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1244eed1b781>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# agent.train_ae(train_loader, optim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# agent.train_jepa(train_loader, optim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_jepa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1fc6065034a3>\u001b[0m in \u001b[0;36mtrain_jepa\u001b[0;34m(self, dataloader, c_loader, optim, bptt)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                             )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    225\u001b[0m             )\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_lerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_mul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (262) must match the size of tensor b (267) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "\n",
        "for i in range(100):\n",
        "    print(i)\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "    # agent.train_ae(train_loader, optim)\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # torch.save(checkpoint, folder+'agentoptimargm.pkl')\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z5-_pfGZTsip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86107aae-a10c-40b9-ec4d-206363a70843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "# ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n",
        "\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "env = TimeLimit(env, max_episode_steps=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PraFUAPB3j7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1482781a-50ea-4a08-a369-d440af80b7ab",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c tensor([1.4305e-06, 1.7226e-05, -0.0000e+00, 2.3842e-07, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, 5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        2.2471e-05, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "<ipython-input-10-1fc6065034a3>:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-10-1fc6065034a3>:104: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update_h0 loss, lz 0 0.05182647332549095 tensor([[ 6.3019e-02,  6.1035e-02,  6.4888e-02, -2.0802e-03,  9.3281e-04,\n",
            "         -2.5153e-04, -3.0088e-03,  2.1756e-03],\n",
            "        [ 1.8759e-02,  3.1090e-02,  3.1300e-02,  1.1533e-03, -2.7514e-03,\n",
            "         -1.3578e-03,  1.0145e-03, -2.7514e-03],\n",
            "        [ 4.8065e-02,  5.0468e-02,  5.1041e-02,  1.1659e-03,  3.1114e-03,\n",
            "         -7.6056e-04, -5.2452e-05,  1.2636e-04],\n",
            "        [ 2.4624e-02,  2.7142e-02,  2.6417e-02,  1.2064e-03,  1.0073e-03,\n",
            "         -5.7757e-04,  1.1224e-03, -8.8930e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.04960630461573601 tensor([[ 1.1265e-01,  1.0860e-01,  1.1494e-01, -3.7992e-03,  1.7226e-03,\n",
            "         -2.8253e-04, -5.6791e-03,  4.0841e-03],\n",
            "        [ 4.5862e-02,  7.0267e-02,  7.1430e-02,  1.9872e-03, -5.4407e-03,\n",
            "         -2.9266e-03,  1.7887e-03, -5.2381e-03],\n",
            "        [ 8.6842e-02,  9.1515e-02,  9.2087e-02,  2.5332e-03,  6.1011e-03,\n",
            "         -1.3703e-03,  1.6391e-04,  1.2517e-05],\n",
            "        [ 5.1727e-02,  5.6572e-02,  5.5237e-02,  2.1547e-03,  1.9163e-03,\n",
            "         -1.1224e-03,  1.9819e-03, -1.4836e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.04783182218670845 tensor([[ 0.1537,  0.1477,  0.1557, -0.0053,  0.0025, -0.0002, -0.0081,  0.0058],\n",
            "        [ 0.0777,  0.1141,  0.1167,  0.0026, -0.0081, -0.0046,  0.0024, -0.0075],\n",
            "        [ 0.1202,  0.1271,  0.1274,  0.0040,  0.0090, -0.0019,  0.0005, -0.0002],\n",
            "        [ 0.0800,  0.0871,  0.0853,  0.0029,  0.0027, -0.0016,  0.0027, -0.0019]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.046245723962783813 tensor([[ 1.8946e-01,  1.8156e-01,  1.9081e-01, -6.5851e-03,  3.2324e-03,\n",
            "         -2.3246e-05, -1.0358e-02,  7.3719e-03],\n",
            "        [ 1.1191e-01,  1.6026e-01,  1.6466e-01,  2.9826e-03, -1.0810e-02,\n",
            "         -6.3825e-03,  2.9010e-03, -9.5856e-03],\n",
            "        [ 1.5059e-01,  1.5942e-01,  1.5928e-01,  5.4634e-03,  1.1833e-02,\n",
            "         -2.3824e-03,  9.5665e-04, -4.7743e-04],\n",
            "        [ 1.0874e-01,  1.1803e-01,  1.1572e-01,  3.6025e-03,  3.4851e-03,\n",
            "         -2.1303e-03,  3.2318e-03, -2.1219e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.044770095497369766 tensor([[ 2.2173e-01,  2.1227e-01,  2.2255e-01, -7.7903e-03,  4.0311e-03,\n",
            "          1.7166e-04, -1.2482e-02,  8.8358e-03],\n",
            "        [ 1.4693e-01,  2.0729e-01,  2.1364e-01,  3.2520e-03, -1.3542e-02,\n",
            "         -8.1694e-03,  3.3319e-03, -1.1531e-02],\n",
            "        [ 1.7910e-01,  1.8978e-01,  1.8919e-01,  6.9284e-03,  1.4598e-02,\n",
            "         -2.8467e-03,  1.4150e-03, -7.4148e-04],\n",
            "        [ 1.3741e-01,  1.4889e-01,  1.4616e-01,  4.2009e-03,  4.1646e-03,\n",
            "         -2.5856e-03,  3.7146e-03, -2.2680e-03]], device='cuda:0')\n",
            "c tensor([-0.0000e+00, 8.9407e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        7.8738e-05, -0.0000e+00, -0.0000e+00, 3.5763e-07, 4.4281e-02, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.0000e+00,\n",
            "        8.7500e-05, 1.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.04846885800361633 tensor([[ 5.5161e-02,  6.0883e-02,  6.1073e-02,  1.3506e-03,  1.5020e-04,\n",
            "         -2.2638e-03,  2.5630e-04,  8.2850e-05],\n",
            "        [ 1.2779e-02,  1.4410e-02,  1.1301e-02,  1.0896e-03, -1.4246e-04,\n",
            "          9.5308e-04,  6.1393e-05, -1.8775e-03],\n",
            "        [ 6.1646e-02,  7.8011e-02,  7.8049e-02,  1.3912e-03, -6.4611e-04,\n",
            "         -3.7503e-03, -5.9068e-04,  7.1406e-04],\n",
            "        [ 1.6022e-02,  1.6804e-02,  1.6108e-02, -7.5221e-04,  2.4724e-03,\n",
            "         -1.1301e-03,  1.8346e-03, -1.1694e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.045905910432338715 tensor([[ 9.6703e-02,  1.0757e-01,  1.0700e-01,  2.8694e-03,  5.9009e-05,\n",
            "         -4.0090e-03,  8.1480e-04,  7.6890e-05],\n",
            "        [ 3.6488e-02,  4.0274e-02,  3.4857e-02,  1.7470e-03, -2.3544e-04,\n",
            "          1.4752e-03, -2.8491e-04, -3.5465e-03],\n",
            "        [ 1.1215e-01,  1.4580e-01,  1.4523e-01,  3.1555e-03, -1.3411e-03,\n",
            "         -7.3409e-03, -9.5189e-04,  1.1700e-03],\n",
            "        [ 3.5324e-02,  3.6201e-02,  3.4952e-02, -1.8066e-03,  4.9067e-03,\n",
            "         -2.2268e-03,  3.3712e-03, -1.9723e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.043942902237176895 tensor([[ 1.3008e-01,  1.4570e-01,  1.4381e-01,  4.4572e-03, -1.5438e-04,\n",
            "         -5.4491e-03,  1.5557e-03,  3.2783e-05],\n",
            "        [ 6.6910e-02,  7.3290e-02,  6.6137e-02,  2.1130e-03, -2.9624e-04,\n",
            "          1.7124e-03, -8.9765e-04, -5.0616e-03],\n",
            "        [ 1.5533e-01,  2.0679e-01,  2.0523e-01,  5.1069e-03, -2.0981e-03,\n",
            "         -1.0808e-02, -1.2290e-03,  1.5229e-03],\n",
            "        [ 5.6648e-02,  5.7144e-02,  5.5456e-02, -3.0845e-03,  7.2873e-03,\n",
            "         -3.3218e-03,  4.6873e-03, -2.4867e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.042239852249622345 tensor([[ 1.5873e-01,  1.7887e-01,  1.7540e-01,  6.0689e-03, -4.2200e-04,\n",
            "         -6.7210e-03,  2.4068e-03, -2.8014e-05],\n",
            "        [ 1.0080e-01,  1.1012e-01,  1.0159e-01,  2.2954e-03, -3.3915e-04,\n",
            "          1.7822e-03, -1.6701e-03, -6.4743e-03],\n",
            "        [ 1.9381e-01,  2.6333e-01,  2.6054e-01,  7.1335e-03, -2.9266e-03,\n",
            "         -1.4193e-02, -1.5104e-03,  1.8698e-03],\n",
            "        [ 7.9117e-02,  7.8869e-02,  7.6818e-02, -4.5186e-03,  9.6071e-03,\n",
            "         -4.4191e-03,  5.8484e-03, -2.7859e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.04067649692296982 tensor([[ 1.8454e-01,  2.0906e-01,  2.0390e-01,  7.6807e-03, -7.0751e-04,\n",
            "         -7.8994e-03,  3.3283e-03, -9.5963e-05],\n",
            "        [ 1.3624e-01,  1.4874e-01,  1.3907e-01,  2.3663e-03, -3.7491e-04,\n",
            "          1.7571e-03, -2.5392e-03, -7.8189e-03],\n",
            "        [ 2.2902e-01,  3.1673e-01,  3.1258e-01,  9.1684e-03, -3.8302e-03,\n",
            "         -1.7524e-02, -1.8448e-03,  2.2614e-03],\n",
            "        [ 1.0218e-01,  1.0090e-01,  9.8543e-02, -6.0660e-03,  1.1868e-02,\n",
            "         -5.5182e-03,  6.8939e-03, -2.9188e-03]], device='cuda:0')\n",
            "c tensor([3.5458e-03, 1.0729e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, 8.3447e-07, 1.7881e-07, 1.0000e+00, 7.3291e-01,\n",
            "        1.4901e-06, 1.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.054813746362924576 tensor([[ 0.0649,  0.0806,  0.0805, -0.0001, -0.0005, -0.0021, -0.0003,  0.0002],\n",
            "        [ 0.0204,  0.0185,  0.0185,  0.0010, -0.0001,  0.0003,  0.0015, -0.0008],\n",
            "        [ 0.0344,  0.0367,  0.0346, -0.0002,  0.0006, -0.0007, -0.0010, -0.0006],\n",
            "        [ 0.0338,  0.0486,  0.0472,  0.0015, -0.0001, -0.0030,  0.0010, -0.0016]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.05214562267065048 tensor([[ 1.2058e-01,  1.5270e-01,  1.5186e-01, -5.3644e-06, -1.1992e-03,\n",
            "         -4.0257e-03, -5.3823e-04,  2.8849e-04],\n",
            "        [ 4.4842e-02,  4.1037e-02,  4.1361e-02,  1.8734e-03, -2.7359e-04,\n",
            "          5.6088e-04,  2.8396e-03, -1.5670e-03],\n",
            "        [ 6.3152e-02,  6.7902e-02,  6.3362e-02, -2.9147e-04,  1.2088e-03,\n",
            "         -1.3727e-03, -1.8591e-03, -1.4031e-03],\n",
            "        [ 6.8703e-02,  9.8305e-02,  9.5634e-02,  2.8396e-03, -3.5048e-04,\n",
            "         -6.1202e-03,  1.8948e-03, -2.9397e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.049856994301080704 tensor([[ 1.6796e-01,  2.1687e-01,  2.1484e-01,  2.8968e-04, -1.9312e-03,\n",
            "         -5.7948e-03, -6.7115e-04,  2.7835e-04],\n",
            "        [ 7.2784e-02,  6.6996e-02,  6.7911e-02,  2.5314e-03, -4.1664e-04,\n",
            "          7.2718e-04,  3.9780e-03, -2.1553e-03],\n",
            "        [ 8.8139e-02,  9.5272e-02,  8.8291e-02, -1.8835e-04,  1.7297e-03,\n",
            "         -1.9616e-03, -2.5845e-03, -2.2852e-03],\n",
            "        [ 1.0397e-01,  1.4858e-01,  1.4469e-01,  4.0674e-03, -6.4373e-04,\n",
            "         -9.2554e-03,  2.6900e-03, -4.1652e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.047848235815763474 tensor([[ 2.0859e-01,  2.7447e-01,  2.7096e-01,  7.3791e-04, -2.6888e-03,\n",
            "         -7.4315e-03, -7.3075e-04,  1.8895e-04],\n",
            "        [ 1.0332e-01,  9.5358e-02,  9.7113e-02,  2.9850e-03, -6.0618e-04,\n",
            "          8.4400e-04,  4.9382e-03, -2.5874e-03],\n",
            "        [ 1.1072e-01,  1.2026e-01,  1.1082e-01,  1.0729e-05,  2.1899e-03,\n",
            "         -2.5183e-03, -3.2353e-03, -3.2377e-03],\n",
            "        [ 1.3910e-01,  1.9897e-01,  1.9382e-01,  5.2273e-03, -1.0008e-03,\n",
            "         -1.2410e-02,  3.4267e-03, -5.2851e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.04603256285190582 tensor([[ 2.4395e-01,  3.2692e-01,  3.2166e-01,  1.3024e-03, -3.4386e-03,\n",
            "         -8.9669e-03, -7.3135e-04,  3.8743e-05],\n",
            "        [ 1.3550e-01,  1.2515e-01,  1.2794e-01,  3.2526e-03, -8.6308e-04,\n",
            "          9.3162e-04,  5.7453e-03, -2.8712e-03],\n",
            "        [ 1.3186e-01,  1.4380e-01,  1.3193e-01,  2.6226e-04,  2.5880e-03,\n",
            "         -3.0643e-03, -3.8505e-03, -4.2260e-03],\n",
            "        [ 1.7368e-01,  2.4902e-01,  2.4265e-01,  6.3527e-03, -1.4156e-03,\n",
            "         -1.5569e-02,  4.1330e-03, -6.3324e-03]], device='cuda:0')\n",
            "c tensor([7.7486e-07, -0.0000e+00, -0.0000e+00, 7.5531e-04, 1.6749e-05, -0.0000e+00,\n",
            "        -0.0000e+00, 5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 1.0000e+00, 5.2452e-05, 1.9670e-06, 6.0616e-03, -0.0000e+00,\n",
            "        1.0000e+00, 5.3644e-07], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.06174752861261368 tensor([[ 5.1498e-04, -1.0014e-02, -1.2007e-02,  1.1790e-03,  2.5630e-05,\n",
            "          1.3959e-03,  8.9169e-04, -4.9710e-04],\n",
            "        [ 9.5749e-02,  1.1208e-01,  1.1604e-01, -1.2851e-03,  7.3671e-04,\n",
            "         -2.0850e-03, -9.1851e-04,  4.7505e-04],\n",
            "        [ 7.8011e-03,  3.7742e-03,  3.2425e-04,  5.5790e-04,  4.9770e-04,\n",
            "          8.7023e-05,  1.0449e-03, -1.2529e-03],\n",
            "        [ 4.2801e-02,  5.3139e-02,  5.4398e-02,  6.4015e-04, -1.5318e-04,\n",
            "         -4.1938e-03,  3.6359e-05,  1.8656e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.05752747133374214 tensor([[ 3.9196e-03, -1.6322e-02, -2.0118e-02,  2.2858e-03,  6.7353e-05,\n",
            "          2.5636e-03,  1.6880e-03, -1.0741e-03],\n",
            "        [ 1.8509e-01,  2.1919e-01,  2.2667e-01, -2.4682e-03,  1.2928e-03,\n",
            "         -4.2975e-03, -1.7494e-03,  8.8930e-04],\n",
            "        [ 1.5759e-02,  7.7510e-03,  5.8770e-04,  1.0371e-03,  9.4831e-04,\n",
            "          4.6551e-04,  2.0045e-03, -2.4939e-03],\n",
            "        [ 8.2779e-02,  1.0368e-01,  1.0620e-01,  1.4204e-03, -1.4007e-04,\n",
            "         -8.2517e-03,  2.0385e-04,  3.4690e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.0538196824491024 tensor([[ 1.1544e-02, -1.7450e-02, -2.2769e-02,  3.3146e-03,  1.6749e-04,\n",
            "          3.4565e-03,  2.3627e-03, -1.7339e-03],\n",
            "        [ 2.6413e-01,  3.1670e-01,  3.2684e-01, -3.4970e-03,  1.6749e-03,\n",
            "         -6.4647e-03, -2.4164e-03,  1.1909e-03],\n",
            "        [ 2.6155e-02,  1.4141e-02,  3.2175e-03,  1.3250e-03,  1.3459e-03,\n",
            "          1.0043e-03,  2.7776e-03, -3.5578e-03],\n",
            "        [ 1.1932e-01,  1.5095e-01,  1.5469e-01,  2.4015e-03,  4.0531e-05,\n",
            "         -1.2131e-02,  5.4896e-04,  3.8922e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.05081355571746826 tensor([[ 2.3971e-02, -1.2777e-02, -1.9283e-02,  4.2742e-03,  3.5465e-04,\n",
            "          4.0698e-03,  2.9141e-03, -2.4700e-03],\n",
            "        [ 3.3051e-01,  4.0146e-01,  4.1328e-01, -4.3440e-03,  1.9199e-03,\n",
            "         -8.4364e-03, -2.8676e-03,  1.3542e-03],\n",
            "        [ 4.1051e-02,  2.4917e-02,  1.0418e-02,  1.3131e-03,  1.6946e-03,\n",
            "          1.5569e-03,  3.2550e-03, -4.3172e-03],\n",
            "        [ 1.5219e-01,  1.9466e-01,  1.9955e-01,  3.6120e-03,  3.4869e-04,\n",
            "         -1.5807e-02,  1.0961e-03,  2.6166e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.04850491136312485 tensor([[ 4.0545e-02, -3.1167e-03, -1.0500e-02,  5.1826e-03,  6.3002e-04,\n",
            "          4.4507e-03,  3.3659e-03, -3.2651e-03],\n",
            "        [ 3.8498e-01,  4.7371e-01,  4.8645e-01, -5.0360e-03,  2.0838e-03,\n",
            "         -1.0165e-02, -3.1120e-03,  1.3995e-03],\n",
            "        [ 6.0964e-02,  4.0529e-02,  2.2787e-02,  9.7096e-04,  2.0015e-03,\n",
            "          2.0415e-03,  3.4004e-03, -4.7380e-03],\n",
            "        [ 1.8175e-01,  2.3525e-01,  2.4117e-01,  5.0271e-03,  7.2598e-04,\n",
            "         -1.9310e-02,  1.8167e-03, -2.7418e-05]], device='cuda:0')\n",
            "c tensor([3.5763e-07, 5.9605e-08, -0.0000e+00, -0.0000e+00, 1.5414e-04, 5.9605e-08,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, 1.4014e-01, 3.3665e-04, -0.0000e+00, 1.0612e-02,\n",
            "        1.0000e+00, 2.7955e-05], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.05386942997574806 tensor([[ 2.9469e-02,  3.5896e-02,  3.3474e-02,  1.5557e-03,  2.7537e-04,\n",
            "          6.4135e-04,  6.4135e-04, -1.6367e-03],\n",
            "        [ 4.8370e-02,  5.4779e-02,  5.5008e-02,  8.8394e-04, -1.9252e-04,\n",
            "         -2.5630e-04, -2.1219e-04, -4.6730e-04],\n",
            "        [ 2.6340e-02,  2.5902e-02,  2.3956e-02,  1.9908e-04,  7.3433e-04,\n",
            "         -2.0802e-04, -2.9206e-05,  2.1100e-04],\n",
            "        [ 3.1185e-02,  3.4008e-02,  3.4676e-02,  7.2837e-04,  1.1450e-03,\n",
            "         -1.6904e-03,  1.3709e-04, -9.2387e-05]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.052193399518728256 tensor([[ 0.0592,  0.0724,  0.0678,  0.0031,  0.0006,  0.0012,  0.0013, -0.0033],\n",
            "        [ 0.0945,  0.1075,  0.1078,  0.0017, -0.0005, -0.0004, -0.0005, -0.0009],\n",
            "        [ 0.0532,  0.0522,  0.0484,  0.0003,  0.0014, -0.0004, -0.0002,  0.0006],\n",
            "        [ 0.0613,  0.0670,  0.0683,  0.0015,  0.0023, -0.0033,  0.0002, -0.0002]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.05058635398745537 tensor([[ 0.0889,  0.1092,  0.1024,  0.0046,  0.0009,  0.0017,  0.0020, -0.0049],\n",
            "        [ 0.1387,  0.1585,  0.1587,  0.0024, -0.0008, -0.0004, -0.0009, -0.0012],\n",
            "        [ 0.0805,  0.0787,  0.0732,  0.0003,  0.0021, -0.0006, -0.0004,  0.0010],\n",
            "        [ 0.0905,  0.0990,  0.1011,  0.0023,  0.0033, -0.0049,  0.0003, -0.0002]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.04904085025191307 tensor([[ 1.1831e-01,  1.4593e-01,  1.3716e-01,  6.0797e-03,  1.2237e-03,\n",
            "          2.0796e-03,  2.6351e-03, -6.5482e-03],\n",
            "        [ 1.8101e-01,  2.0763e-01,  2.0779e-01,  3.0965e-03, -1.1748e-03,\n",
            "         -3.2187e-04, -1.3059e-03, -1.5223e-03],\n",
            "        [ 1.0786e-01,  1.0521e-01,  9.8000e-02,  1.3828e-04,  2.7496e-03,\n",
            "         -6.9678e-04, -7.7367e-04,  1.6284e-03],\n",
            "        [ 1.1879e-01,  1.3012e-01,  1.3290e-01,  3.0458e-03,  4.3464e-03,\n",
            "         -6.5172e-03,  3.6776e-04, -1.6630e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.047573287039995193 tensor([[ 1.4723e-01,  1.8242e-01,  1.7176e-01,  7.5793e-03,  1.5676e-03,\n",
            "          2.4378e-03,  3.3450e-03, -8.1933e-03],\n",
            "        [ 2.2156e-01,  2.5509e-01,  2.5501e-01,  3.6752e-03, -1.6046e-03,\n",
            "         -1.4424e-04, -1.8382e-03, -1.7625e-03],\n",
            "        [ 1.3529e-01,  1.3157e-01,  1.2276e-01, -8.8811e-05,  3.3563e-03,\n",
            "         -7.9811e-04, -1.2261e-03,  2.3508e-03],\n",
            "        [ 1.4622e-01,  1.6035e-01,  1.6388e-01,  3.8433e-03,  5.3340e-03,\n",
            "         -8.0490e-03,  3.6955e-04, -1.1325e-04]], device='cuda:0')\n",
            "c tensor([7.6294e-04, 4.7684e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        3.4571e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.7817e-01, 4.5868e-02, 9.6436e-01, 1.0000e+00, 1.0000e+00,\n",
            "        1.0000e+00, 5.9605e-08], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.05723924562335014 tensor([[ 7.3051e-02,  8.2703e-02,  8.5297e-02,  1.3530e-03,  5.4240e-05,\n",
            "         -3.3379e-03, -5.6505e-04, -4.2081e-04],\n",
            "        [ 1.3971e-02,  2.1877e-02,  1.7147e-02,  3.6478e-04,  6.0022e-04,\n",
            "         -9.3222e-04,  5.0247e-04, -2.1601e-03],\n",
            "        [ 5.6877e-02,  7.0610e-02,  7.0000e-02,  2.8014e-05,  1.4210e-03,\n",
            "         -1.0264e-03, -4.2319e-04, -1.4472e-03],\n",
            "        [ 2.8191e-02,  2.4586e-02,  2.6073e-02,  1.7548e-03,  3.4630e-04,\n",
            "         -1.1474e-03,  1.8835e-03,  2.9683e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.05396294593811035 tensor([[ 1.2970e-01,  1.4847e-01,  1.5251e-01,  3.0887e-03, -4.4107e-05,\n",
            "         -6.1965e-03, -7.6115e-04, -1.0294e-03],\n",
            "        [ 3.8500e-02,  5.4493e-02,  4.5662e-02,  3.3379e-04,  1.1510e-03,\n",
            "         -2.2399e-03,  5.6744e-04, -4.0877e-03],\n",
            "        [ 1.0117e-01,  1.2890e-01,  1.2699e-01,  4.9651e-04,  2.7955e-03,\n",
            "         -1.8662e-03, -4.9353e-04, -3.2222e-03],\n",
            "        [ 5.9910e-02,  5.2013e-02,  5.5256e-02,  3.1972e-03,  6.0022e-04,\n",
            "         -2.2280e-03,  3.4630e-03,  8.9288e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.051523417234420776 tensor([[ 1.7513e-01,  2.0245e-01,  2.0718e-01,  5.0616e-03, -2.0325e-04,\n",
            "         -8.7190e-03, -6.9439e-04, -1.7560e-03],\n",
            "        [ 7.0238e-02,  9.4509e-02,  8.2054e-02, -1.0133e-05,  1.6314e-03,\n",
            "         -3.8207e-03,  2.9147e-04, -5.7685e-03],\n",
            "        [ 1.3758e-01,  1.7941e-01,  1.7582e-01,  1.2010e-03,  4.1366e-03,\n",
            "         -2.5815e-03, -3.6716e-04, -5.1844e-03],\n",
            "        [ 9.3632e-02,  8.1139e-02,  8.6308e-02,  4.3988e-03,  7.5936e-04,\n",
            "         -3.2908e-03,  4.8292e-03,  1.7208e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.04947386309504509 tensor([[ 2.1381e-01,  2.4933e-01,  2.5436e-01,  7.1585e-03, -3.7193e-04,\n",
            "         -1.1047e-02, -4.5836e-04, -2.5463e-03],\n",
            "        [ 1.0575e-01,  1.3853e-01,  1.2276e-01, -5.6565e-04,  2.0301e-03,\n",
            "         -5.5635e-03, -2.1875e-04, -7.2563e-03],\n",
            "        [ 1.6945e-01,  2.2537e-01,  2.1988e-01,  2.0015e-03,  5.4455e-03,\n",
            "         -3.2288e-03, -1.6272e-04, -7.2253e-03],\n",
            "        [ 1.2823e-01,  1.1101e-01,  1.1822e-01,  5.4407e-03,  8.3029e-04,\n",
            "         -4.3410e-03,  6.0558e-03,  2.7066e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.047607921063899994 tensor([[ 2.4843e-01,  2.9186e-01,  2.9709e-01,  9.3210e-03, -5.3108e-04,\n",
            "         -1.3269e-02, -1.1325e-04, -3.3706e-03],\n",
            "        [ 1.4285e-01,  1.8431e-01,  1.6541e-01, -1.2553e-03,  2.3431e-03,\n",
            "         -7.3946e-03, -8.9347e-04, -8.5926e-03],\n",
            "        [ 1.9863e-01,  2.6859e-01,  2.6112e-01,  2.8193e-03,  6.7234e-03,\n",
            "         -3.8415e-03,  5.0664e-05, -9.2852e-03],\n",
            "        [ 1.6306e-01,  1.4105e-01,  1.5038e-01,  6.3753e-03,  8.2016e-04,\n",
            "         -5.3763e-03,  7.1841e-03,  3.8022e-03]], device='cuda:0')\n",
            "c tensor([2.3842e-07, 2.9802e-07, -0.0000e+00, 4.5896e-06, 6.5565e-07, -0.0000e+00,\n",
            "        4.0364e-04, 1.7847e-01, 4.7583e-01, 6.8909e-02, -0.0000e+00, 7.7486e-07,\n",
            "        2.3901e-05, -0.0000e+00, 5.9605e-08, 2.6822e-05, 3.4695e-03, -0.0000e+00,\n",
            "        6.6797e-01, 9.9854e-01], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.052423834800720215 tensor([[ 4.5624e-02,  5.5962e-02,  5.4893e-02,  2.4033e-03,  1.4341e-03,\n",
            "         -9.5606e-04, -1.9073e-05, -1.2279e-03],\n",
            "        [ 2.4052e-02,  2.2755e-02,  2.0771e-02, -1.8597e-04,  8.1956e-04,\n",
            "         -1.6475e-03, -5.4836e-05, -1.6272e-04],\n",
            "        [ 4.9400e-02,  6.0616e-02,  5.9280e-02, -4.2856e-04, -6.3002e-04,\n",
            "         -2.9504e-04, -1.5855e-04, -2.5463e-03],\n",
            "        [ 2.1400e-02,  2.3079e-02,  2.0485e-02,  9.7871e-04,  2.1744e-03,\n",
            "         -9.0122e-04,  2.3007e-03, -3.4094e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.05044826120138168 tensor([[ 8.2035e-02,  1.0242e-01,  9.9754e-02,  4.8733e-03,  2.7728e-03,\n",
            "         -1.6016e-03,  8.6427e-05, -2.4796e-03],\n",
            "        [ 5.5408e-02,  5.3215e-02,  4.9877e-02, -7.0453e-04,  1.6791e-03,\n",
            "         -3.5560e-03, -3.8862e-04, -1.1325e-04],\n",
            "        [ 9.1629e-02,  1.1444e-01,  1.1147e-01, -7.4148e-04, -1.3149e-03,\n",
            "         -4.5955e-04, -1.9789e-04, -5.2476e-03],\n",
            "        [ 4.4727e-02,  4.7798e-02,  4.2686e-02,  1.7369e-03,  4.3237e-03,\n",
            "         -1.7452e-03,  4.4048e-03, -4.6194e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.04875843599438667 tensor([[ 1.1314e-01,  1.4343e-01,  1.3889e-01,  7.3361e-03,  4.0865e-03,\n",
            "         -2.0778e-03,  2.4796e-04, -3.7253e-03],\n",
            "        [ 9.0523e-02,  8.7776e-02,  8.3485e-02, -1.4448e-03,  2.5356e-03,\n",
            "         -5.6076e-03, -9.0957e-04,  8.8215e-05],\n",
            "        [ 1.2955e-01,  1.6411e-01,  1.5930e-01, -1.0204e-03, -2.0540e-03,\n",
            "         -5.4479e-04, -2.0564e-04, -7.9966e-03],\n",
            "        [ 6.8989e-02,  7.3261e-02,  6.5689e-02,  2.3478e-03,  6.4421e-03,\n",
            "         -2.5475e-03,  6.3837e-03, -4.3988e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.04721200466156006 tensor([[ 0.1410,  0.1811,  0.1746,  0.0098,  0.0054, -0.0025,  0.0004, -0.0050],\n",
            "        [ 0.1274,  0.1244,  0.1194, -0.0023,  0.0034, -0.0077, -0.0016,  0.0004],\n",
            "        [ 0.1647,  0.2110,  0.2043, -0.0013, -0.0029, -0.0006, -0.0002, -0.0107],\n",
            "        [ 0.0937,  0.0990,  0.0890,  0.0029,  0.0085, -0.0033,  0.0083, -0.0003]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.045755691826343536 tensor([[ 1.6691e-01,  2.1671e-01,  2.0819e-01,  1.2139e-02,  6.7496e-03,\n",
            "         -2.7871e-03,  6.2406e-04, -6.1518e-03],\n",
            "        [ 1.6466e-01,  1.6169e-01,  1.5615e-01, -3.3426e-03,  4.1884e-03,\n",
            "         -9.8801e-03, -2.3139e-03,  8.0585e-04],\n",
            "        [ 1.9787e-01,  2.5600e-01,  2.4734e-01, -1.6314e-03, -3.7140e-03,\n",
            "         -5.7280e-04, -2.8014e-04, -1.3452e-02],\n",
            "        [ 1.1841e-01,  1.2461e-01,  1.1221e-01,  3.2997e-03,  1.0570e-02,\n",
            "         -4.0275e-03,  1.0111e-02, -1.3888e-04]], device='cuda:0')\n",
            "c tensor([2.2233e-05, 1.5140e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.2589e-04,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.9444e-03, -0.0000e+00, 1.0000e+00,\n",
            "        3.3736e-04, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.05290593206882477 tensor([[ 6.8970e-02,  7.4425e-02,  7.7896e-02, -8.5354e-04,  4.9829e-04,\n",
            "         -2.2781e-03, -1.3721e-03, -7.5698e-05],\n",
            "        [ 7.5626e-03,  1.6098e-02,  1.1806e-02,  2.0826e-03,  3.5703e-04,\n",
            "          2.0623e-04,  6.1512e-04, -1.2767e-03],\n",
            "        [ 6.8207e-02,  7.1602e-02,  7.4005e-02,  1.8084e-03,  9.5963e-05,\n",
            "         -3.1066e-03, -3.6955e-04,  1.3649e-03],\n",
            "        [ 1.9054e-02,  2.0561e-02,  1.8349e-02, -3.6180e-04,  1.5438e-03,\n",
            "          7.2300e-04,  6.3837e-04, -3.3879e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.04988964647054672 tensor([[ 1.2066e-01,  1.3119e-01,  1.3691e-01, -1.3530e-03,  7.7188e-04,\n",
            "         -3.9792e-03, -2.3448e-03, -3.5822e-04],\n",
            "        [ 2.7018e-02,  4.4060e-02,  3.6259e-02,  3.6752e-03,  7.2956e-04,\n",
            "          1.1921e-05,  7.4208e-04, -2.2274e-03],\n",
            "        [ 1.2348e-01,  1.3100e-01,  1.3493e-01,  3.9196e-03,  8.4043e-05,\n",
            "         -6.0034e-03, -4.5896e-04,  2.4253e-03],\n",
            "        [ 4.1580e-02,  4.3888e-02,  3.9597e-02, -9.3222e-04,  2.9516e-03,\n",
            "          1.4681e-03,  1.0443e-03, -6.5494e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.04772969335317612 tensor([[ 1.6113e-01,  1.7628e-01,  1.8356e-01, -1.6451e-03,  9.2924e-04,\n",
            "         -5.3060e-03, -3.0482e-03, -7.6652e-04],\n",
            "        [ 5.4483e-02,  8.0147e-02,  6.9447e-02,  4.8906e-03,  1.1170e-03,\n",
            "         -4.6253e-04,  5.0068e-04, -2.8980e-03],\n",
            "        [ 1.7021e-01,  1.8227e-01,  1.8726e-01,  6.1917e-03, -2.0862e-05,\n",
            "         -8.7094e-03, -3.8266e-04,  3.3110e-03],\n",
            "        [ 6.6242e-02,  6.8836e-02,  6.2618e-02, -1.6636e-03,  4.2331e-03,\n",
            "          2.1726e-03,  1.2517e-03, -9.4938e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.04595749080181122 tensor([[ 1.9516e-01,  2.1463e-01,  2.2312e-01, -1.8352e-03,  1.0341e-03,\n",
            "         -6.4290e-03, -3.5888e-03, -1.2392e-03],\n",
            "        [ 8.6164e-02,  1.2062e-01,  1.0738e-01,  5.8490e-03,  1.5062e-03,\n",
            "         -1.0842e-03,  3.5763e-06, -3.3671e-03],\n",
            "        [ 2.1175e-01,  2.2858e-01,  2.3438e-01,  8.5235e-03, -2.1160e-04,\n",
            "         -1.1280e-02, -2.4378e-04,  4.1264e-03],\n",
            "        [ 9.1953e-02,  9.4509e-02,  8.6441e-02, -2.4956e-03,  5.4097e-03,\n",
            "          2.8384e-03,  1.3185e-03, -1.2279e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.044359657913446426 tensor([[ 2.2541e-01,  2.4908e-01,  2.5860e-01, -1.9771e-03,  1.1200e-03,\n",
            "         -7.4452e-03, -4.0287e-03, -1.7434e-03],\n",
            "        [ 1.1970e-01,  1.6315e-01,  1.4759e-01,  6.6340e-03,  1.8919e-03,\n",
            "         -1.7703e-03, -6.7353e-04, -3.6895e-03],\n",
            "        [ 2.5000e-01,  2.7168e-01,  2.7809e-01,  1.0864e-02, -4.7982e-04,\n",
            "         -1.3750e-02, -9.9540e-05,  4.9275e-03],\n",
            "        [ 1.1808e-01,  1.2033e-01,  1.1049e-01, -3.3873e-03,  6.4969e-03,\n",
            "          3.4750e-03,  1.2869e-03, -1.4942e-02]], device='cuda:0')\n",
            "c tensor([2.2292e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07,\n",
            "        5.3644e-07, 9.5367e-07, -0.0000e+00, 9.9087e-04, 1.0000e+00, 4.7684e-07,\n",
            "        2.1744e-04, 9.9463e-01, 1.0000e+00, 5.9605e-08, 7.1289e-01, 3.3838e-01,\n",
            "        -0.0000e+00, 5.4777e-05], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.051718272268772125 tensor([[ 6.8550e-02,  7.6294e-02,  7.8087e-02, -7.7546e-04, -8.2374e-04,\n",
            "         -1.2803e-03,  1.2398e-04, -6.7949e-04],\n",
            "        [ 1.5421e-02,  2.1343e-02,  2.0504e-02,  2.4867e-03,  1.4830e-03,\n",
            "         -1.7381e-03, -1.3888e-04, -4.0650e-04],\n",
            "        [ 4.9133e-02,  5.3406e-02,  5.2948e-02,  2.0325e-04, -1.0073e-04,\n",
            "          1.5676e-04,  5.9009e-05, -1.5187e-03],\n",
            "        [ 2.3880e-02,  2.6913e-02,  2.4166e-02,  1.0234e-03,  1.8883e-03,\n",
            "         -1.2434e-03,  4.9353e-04, -1.2863e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.04912445694208145 tensor([[ 0.1248,  0.1405,  0.1433, -0.0013, -0.0019, -0.0022,  0.0005, -0.0015],\n",
            "        [ 0.0386,  0.0505,  0.0493,  0.0047,  0.0030, -0.0037, -0.0007, -0.0006],\n",
            "        [ 0.0895,  0.0984,  0.0972,  0.0006, -0.0002,  0.0004,  0.0003, -0.0033],\n",
            "        [ 0.0499,  0.0557,  0.0503,  0.0019,  0.0037, -0.0025,  0.0007, -0.0024]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.047056857496500015 tensor([[ 0.1724,  0.1962,  0.1994, -0.0017, -0.0030, -0.0030,  0.0010, -0.0024],\n",
            "        [ 0.0670,  0.0847,  0.0835,  0.0067,  0.0044, -0.0059, -0.0015, -0.0005],\n",
            "        [ 0.1245,  0.1382,  0.1361,  0.0011, -0.0003,  0.0008,  0.0008, -0.0051],\n",
            "        [ 0.0770,  0.0854,  0.0774,  0.0026,  0.0054, -0.0037,  0.0008, -0.0033]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.04527081549167633 tensor([[ 2.1427e-01,  2.4601e-01,  2.4948e-01, -2.0581e-03, -4.2611e-03,\n",
            "         -3.6645e-03,  1.5581e-03, -3.4136e-03],\n",
            "        [ 9.8238e-02,  1.2169e-01,  1.2072e-01,  8.4686e-03,  5.7375e-03,\n",
            "         -8.0991e-03, -2.6357e-03, -2.2113e-04],\n",
            "        [ 1.5625e-01,  1.7494e-01,  1.7191e-01,  1.6397e-03, -3.9101e-04,\n",
            "          1.2487e-03,  1.2398e-03, -6.9654e-03],\n",
            "        [ 1.0452e-01,  1.1532e-01,  1.0471e-01,  3.3146e-03,  6.9821e-03,\n",
            "         -4.9102e-03,  7.5758e-04, -4.2278e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.04365205019712448 tensor([[ 2.5217e-01,  2.9190e-01,  2.9545e-01, -2.3502e-03, -5.4926e-03,\n",
            "         -4.2593e-03,  2.2215e-03, -4.4209e-03],\n",
            "        [ 1.3099e-01,  1.6010e-01,  1.5949e-01,  1.0087e-02,  7.0000e-03,\n",
            "         -1.0352e-02, -3.9387e-03,  2.0742e-04],\n",
            "        [ 1.8595e-01,  2.0960e-01,  2.0565e-01,  2.1833e-03, -5.1856e-04,\n",
            "          1.7190e-03,  1.7309e-03, -8.8274e-03],\n",
            "        [ 1.3195e-01,  1.4507e-01,  1.3193e-01,  3.9756e-03,  8.5402e-03,\n",
            "         -6.0934e-03,  6.3598e-04, -5.0569e-03]], device='cuda:0')\n",
            "c tensor([9.8419e-03, 8.8978e-04, -0.0000e+00, 9.9957e-05, 7.1526e-07, 1.3147e-01,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 7.2527e-04,\n",
            "        -0.0000e+00, -0.0000e+00, 1.3184e-01, 3.3319e-05, 5.4199e-01, 9.9951e-01,\n",
            "        5.9605e-08, 1.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.058656394481658936 tensor([[ 1.0328e-02,  1.4992e-02,  1.1339e-02,  1.4544e-03, -1.3256e-03,\n",
            "         -5.4061e-04,  8.2493e-04, -3.6657e-04],\n",
            "        [ 6.1340e-02,  7.4692e-02,  7.6599e-02, -1.8907e-03,  9.0480e-04,\n",
            "         -3.3259e-03, -5.3227e-04, -3.4571e-05],\n",
            "        [ 1.8463e-02,  1.7776e-02,  1.7242e-02,  2.5821e-03,  8.8811e-04,\n",
            "         -3.2902e-04,  1.8215e-03,  7.9870e-05],\n",
            "        [ 3.9825e-02,  4.4098e-02,  4.4937e-02,  9.3281e-04,  2.7299e-04,\n",
            "         -1.8251e-03,  7.2956e-04, -3.0732e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0565074123442173 tensor([[ 0.0262,  0.0363,  0.0295,  0.0027, -0.0026, -0.0013,  0.0015, -0.0007],\n",
            "        [ 0.1151,  0.1423,  0.1456, -0.0036,  0.0016, -0.0065, -0.0010, -0.0002],\n",
            "        [ 0.0397,  0.0381,  0.0372,  0.0050,  0.0018, -0.0007,  0.0034,  0.0004],\n",
            "        [ 0.0771,  0.0861,  0.0878,  0.0019,  0.0005, -0.0036,  0.0015, -0.0062]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.05460476875305176 tensor([[ 0.0461,  0.0620,  0.0525,  0.0038, -0.0038, -0.0023,  0.0022, -0.0010],\n",
            "        [ 0.1626,  0.2039,  0.2082, -0.0053,  0.0022, -0.0095, -0.0014, -0.0004],\n",
            "        [ 0.0633,  0.0605,  0.0595,  0.0072,  0.0026, -0.0012,  0.0048,  0.0008],\n",
            "        [ 0.1121,  0.1263,  0.1288,  0.0030,  0.0008, -0.0053,  0.0024, -0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.05287373811006546 tensor([[ 0.0685,  0.0907,  0.0787,  0.0048, -0.0050, -0.0034,  0.0028, -0.0012],\n",
            "        [ 0.2051,  0.2605,  0.2657, -0.0070,  0.0027, -0.0124, -0.0018, -0.0006],\n",
            "        [ 0.0886,  0.0845,  0.0836,  0.0092,  0.0034, -0.0017,  0.0059,  0.0015],\n",
            "        [ 0.1454,  0.1649,  0.1683,  0.0041,  0.0010, -0.0070,  0.0033, -0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.05126781761646271 tensor([[ 0.0922,  0.1211,  0.1068,  0.0058, -0.0061, -0.0046,  0.0033, -0.0014],\n",
            "        [ 0.2438,  0.3133,  0.3189, -0.0086,  0.0031, -0.0152, -0.0022, -0.0008],\n",
            "        [ 0.1152,  0.1096,  0.1089,  0.0111,  0.0042, -0.0023,  0.0069,  0.0022],\n",
            "        [ 0.1772,  0.2022,  0.2065,  0.0053,  0.0012, -0.0087,  0.0041, -0.0161]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.6000e-03, 5.1260e-06, -0.0000e+00, 2.3246e-06, 6.0211e-02, 1.5235e-04,\n",
            "        5.9605e-07, 1.4317e-04, -0.0000e+00, 9.5032e-02, 3.7842e-02, -0.0000e+00,\n",
            "        4.7684e-07, -0.0000e+00, -0.0000e+00, 3.3975e-05, 1.0187e-01, -0.0000e+00,\n",
            "        9.4141e-01, 9.9463e-01], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[4, 4, 2, 6, 5, 13, 4, 2, 4, 6, 7, 11, 7, 1, 4, 6, 6, 3, 6, 11, 2, 5, 1, 6, 5, 6, 9, 8, 9, 5, 2, 8, 6, 6, 11, 5, 9, 7, 9, 9, 8]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    lstate=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6], [1, 256]\n",
        "        lstate.append(state)\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # act = agent(state, k)\n",
        "            act = agent(lstate, k=k)\n",
        "            lstate=[]\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"dided\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cm6KjvBrnNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f9f8d2-0163-4a47-bfa1-386f729e8b2a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c tensor([3.5763e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01644444279372692 tensor([[ 0.0496,  0.0443,  0.0496, -0.0041,  0.0188, -0.0176, -0.0028,  0.0169],\n",
            "        [ 0.0540,  0.0528,  0.0597, -0.0038,  0.0203, -0.0191, -0.0036,  0.0196],\n",
            "        [ 0.0411,  0.0308,  0.0340, -0.0026,  0.0139, -0.0166, -0.0004,  0.0137],\n",
            "        [ 0.0229,  0.0194,  0.0231, -0.0014,  0.0084, -0.0067, -0.0017,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014477042481303215 tensor([[ 8.0433e-02,  6.9885e-02,  7.8888e-02, -7.5555e-03,  3.0766e-02,\n",
            "         -2.7866e-02, -4.5884e-03,  2.5563e-02],\n",
            "        [ 8.5983e-02,  8.4686e-02,  9.6092e-02, -6.6113e-03,  3.2330e-02,\n",
            "         -2.9821e-02, -6.2585e-03,  3.0060e-02],\n",
            "        [ 6.8130e-02,  4.8094e-02,  5.2919e-02, -4.4847e-03,  2.2430e-02,\n",
            "         -2.7952e-02, -9.7752e-05,  2.1214e-02],\n",
            "        [ 4.1666e-02,  3.5229e-02,  4.2458e-02, -2.7156e-03,  1.5144e-02,\n",
            "         -1.1430e-02, -3.2544e-03,  1.4215e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013715805485844612 tensor([[ 0.1011,  0.0852,  0.0968, -0.0105,  0.0390, -0.0343, -0.0058,  0.0299],\n",
            "        [ 0.1053,  0.1041,  0.1186, -0.0089,  0.0396, -0.0356, -0.0083,  0.0352],\n",
            "        [ 0.0869,  0.0575,  0.0631, -0.0060,  0.0279, -0.0360,  0.0007,  0.0251],\n",
            "        [ 0.0572,  0.0483,  0.0588, -0.0038,  0.0206, -0.0147, -0.0046,  0.0187]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.013387540355324745 tensor([[ 0.1160,  0.0945,  0.1082, -0.0132,  0.0452, -0.0384, -0.0066,  0.0318],\n",
            "        [ 0.1173,  0.1165,  0.1332, -0.0108,  0.0441, -0.0385, -0.0100,  0.0374],\n",
            "        [ 0.1008,  0.0624,  0.0682, -0.0073,  0.0316, -0.0422,  0.0018,  0.0269],\n",
            "        [ 0.0701,  0.0591,  0.0727, -0.0049,  0.0252, -0.0170, -0.0059,  0.0219]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013226283714175224 tensor([[ 0.1273,  0.1002,  0.1155, -0.0157,  0.0500, -0.0412, -0.0073,  0.0323],\n",
            "        [ 0.1249,  0.1245,  0.1429, -0.0125,  0.0470, -0.0397, -0.0115,  0.0377],\n",
            "        [ 0.1118,  0.0645,  0.0704, -0.0084,  0.0342, -0.0472,  0.0031,  0.0273],\n",
            "        [ 0.0809,  0.0680,  0.0845, -0.0058,  0.0289, -0.0183, -0.0071,  0.0242]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-0c232f84ef29>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-21-0c232f84ef29>:103: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        1.7881e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02195845916867256 tensor([[ 0.0742,  0.0727,  0.0790, -0.0041,  0.0254, -0.0275, -0.0028,  0.0248],\n",
            "        [ 0.0589,  0.0520,  0.0597, -0.0046,  0.0227, -0.0206, -0.0044,  0.0212],\n",
            "        [ 0.0571,  0.0540,  0.0591, -0.0045,  0.0197, -0.0211, -0.0018,  0.0199],\n",
            "        [ 0.0295,  0.0198,  0.0224, -0.0017,  0.0112, -0.0103, -0.0018,  0.0111]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01848674938082695 tensor([[ 0.1187,  0.1159,  0.1259, -0.0071,  0.0399, -0.0435, -0.0042,  0.0370],\n",
            "        [ 0.0927,  0.0796,  0.0927, -0.0081,  0.0364, -0.0314, -0.0075,  0.0314],\n",
            "        [ 0.0928,  0.0877,  0.0956, -0.0083,  0.0315, -0.0339, -0.0027,  0.0306],\n",
            "        [ 0.0535,  0.0342,  0.0388, -0.0031,  0.0205, -0.0183, -0.0033,  0.0197]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01724318601191044 tensor([[ 0.1460,  0.1417,  0.1534, -0.0094,  0.0483, -0.0530, -0.0048,  0.0423],\n",
            "        [ 0.1126,  0.0934,  0.1105, -0.0109,  0.0450, -0.0366, -0.0100,  0.0356],\n",
            "        [ 0.1148,  0.1083,  0.1174, -0.0114,  0.0381, -0.0415, -0.0030,  0.0354],\n",
            "        [ 0.0736,  0.0447,  0.0509, -0.0045,  0.0282, -0.0245, -0.0047,  0.0263]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01677681691944599 tensor([[ 0.1638,  0.1576,  0.1701, -0.0113,  0.0531, -0.0590, -0.0050,  0.0438],\n",
            "        [ 0.1251,  0.0999,  0.1202, -0.0133,  0.0508, -0.0388, -0.0120,  0.0365],\n",
            "        [ 0.1285,  0.1209,  0.1305, -0.0142,  0.0417, -0.0458, -0.0029,  0.0368],\n",
            "        [ 0.0907,  0.0523,  0.0598, -0.0057,  0.0349, -0.0295, -0.0059,  0.0314]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016580622643232346 tensor([[ 0.1760,  0.1678,  0.1803, -0.0129,  0.0559, -0.0629, -0.0048,  0.0431],\n",
            "        [ 0.1337,  0.1023,  0.1254, -0.0155,  0.0551, -0.0394, -0.0137,  0.0357],\n",
            "        [ 0.1372,  0.1288,  0.1382, -0.0168,  0.0435, -0.0482, -0.0026,  0.0361],\n",
            "        [ 0.1055,  0.0579,  0.0664, -0.0069,  0.0409, -0.0336, -0.0070,  0.0355]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.9802e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018844470381736755 tensor([[ 0.0586,  0.0541,  0.0625, -0.0043,  0.0223, -0.0186, -0.0042,  0.0184],\n",
            "        [ 0.0510,  0.0459,  0.0496, -0.0026,  0.0169, -0.0198, -0.0016,  0.0187],\n",
            "        [ 0.0535,  0.0522,  0.0595, -0.0054,  0.0194, -0.0186, -0.0027,  0.0177],\n",
            "        [ 0.0218,  0.0184,  0.0217, -0.0018,  0.0075, -0.0074, -0.0016,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016235385090112686 tensor([[ 0.0989,  0.0902,  0.1057, -0.0081,  0.0380, -0.0299, -0.0075,  0.0286],\n",
            "        [ 0.0841,  0.0749,  0.0802, -0.0042,  0.0273, -0.0327, -0.0024,  0.0300],\n",
            "        [ 0.0875,  0.0857,  0.0981, -0.0100,  0.0315, -0.0299, -0.0045,  0.0272],\n",
            "        [ 0.0394,  0.0326,  0.0391, -0.0034,  0.0132, -0.0128, -0.0031,  0.0174]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015099771320819855 tensor([[ 0.1273,  0.1144,  0.1358, -0.0114,  0.0493, -0.0367, -0.0103,  0.0339],\n",
            "        [ 0.1056,  0.0927,  0.0983, -0.0052,  0.0336, -0.0412, -0.0026,  0.0366],\n",
            "        [ 0.1093,  0.1070,  0.1232, -0.0142,  0.0390, -0.0363, -0.0058,  0.0313],\n",
            "        [ 0.0536,  0.0437,  0.0532, -0.0048,  0.0175, -0.0167, -0.0044,  0.0233]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014588076621294022 tensor([[ 0.1481,  0.1309,  0.1575, -0.0145,  0.0579, -0.0405, -0.0126,  0.0360],\n",
            "        [ 0.1202,  0.1036,  0.1086, -0.0058,  0.0374, -0.0469, -0.0025,  0.0403],\n",
            "        [ 0.1236,  0.1210,  0.1402, -0.0180,  0.0437, -0.0399, -0.0068,  0.0322],\n",
            "        [ 0.0651,  0.0523,  0.0647, -0.0062,  0.0208, -0.0195, -0.0055,  0.0279]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014339298009872437 tensor([[ 0.1639,  0.1423,  0.1734, -0.0174,  0.0646, -0.0425, -0.0146,  0.0360],\n",
            "        [ 0.1304,  0.1103,  0.1142, -0.0062,  0.0395, -0.0510, -0.0022,  0.0423],\n",
            "        [ 0.1334,  0.1305,  0.1521, -0.0216,  0.0466, -0.0416, -0.0076,  0.0311],\n",
            "        [ 0.0744,  0.0590,  0.0740, -0.0075,  0.0232, -0.0214, -0.0066,  0.0314]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020741183310747147 tensor([[ 0.0707,  0.0656,  0.0723, -0.0050,  0.0252, -0.0248, -0.0031,  0.0209],\n",
            "        [ 0.0533,  0.0409,  0.0455, -0.0029,  0.0187, -0.0187, -0.0031,  0.0205],\n",
            "        [ 0.0352,  0.0322,  0.0356, -0.0025,  0.0124, -0.0125, -0.0019,  0.0118],\n",
            "        [ 0.0278,  0.0212,  0.0260, -0.0024,  0.0099, -0.0100, -0.0014,  0.0087]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01825099252164364 tensor([[ 0.1163,  0.1067,  0.1182, -0.0092,  0.0415, -0.0399, -0.0051,  0.0311],\n",
            "        [ 0.0898,  0.0655,  0.0731, -0.0050,  0.0315, -0.0312, -0.0056,  0.0341],\n",
            "        [ 0.0577,  0.0522,  0.0574, -0.0043,  0.0200, -0.0200, -0.0032,  0.0183],\n",
            "        [ 0.0506,  0.0378,  0.0472, -0.0046,  0.0179, -0.0179, -0.0025,  0.0149]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017208337783813477 tensor([[ 0.1453,  0.1312,  0.1459, -0.0127,  0.0519, -0.0486, -0.0063,  0.0342],\n",
            "        [ 0.1157,  0.0795,  0.0890, -0.0067,  0.0405, -0.0395, -0.0076,  0.0433],\n",
            "        [ 0.0725,  0.0650,  0.0713, -0.0057,  0.0247, -0.0245, -0.0042,  0.0216],\n",
            "        [ 0.0694,  0.0509,  0.0645, -0.0067,  0.0244, -0.0240, -0.0035,  0.0192]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01676366664469242 tensor([[ 0.1644,  0.1455,  0.1625, -0.0158,  0.0587, -0.0536, -0.0070,  0.0333],\n",
            "        [ 0.1352,  0.0871,  0.0978, -0.0082,  0.0471, -0.0452, -0.0094,  0.0496],\n",
            "        [ 0.0828,  0.0734,  0.0802, -0.0068,  0.0277, -0.0271, -0.0048,  0.0229],\n",
            "        [ 0.0851,  0.0612,  0.0788, -0.0087,  0.0297, -0.0289, -0.0043,  0.0221]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016553137451410294 tensor([[ 0.1775,  0.1538,  0.1723, -0.0187,  0.0634, -0.0564, -0.0074,  0.0300],\n",
            "        [ 0.1507,  0.0906,  0.1023, -0.0095,  0.0522, -0.0494, -0.0111,  0.0544],\n",
            "        [ 0.0900,  0.0790,  0.0860, -0.0077,  0.0296, -0.0286, -0.0053,  0.0230],\n",
            "        [ 0.0984,  0.0693,  0.0906, -0.0106,  0.0342, -0.0328, -0.0051,  0.0237]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021309472620487213 tensor([[ 0.0528,  0.0502,  0.0533, -0.0031,  0.0189, -0.0188, -0.0033,  0.0170],\n",
            "        [ 0.0607,  0.0560,  0.0652, -0.0049,  0.0224, -0.0208, -0.0039,  0.0191],\n",
            "        [ 0.0430,  0.0381,  0.0422, -0.0031,  0.0150, -0.0158, -0.0017,  0.0140],\n",
            "        [ 0.0268,  0.0243,  0.0278, -0.0026,  0.0105, -0.0080, -0.0027,  0.0088]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018831202760338783 tensor([[ 0.0866,  0.0817,  0.0862, -0.0056,  0.0310, -0.0302, -0.0056,  0.0258],\n",
            "        [ 0.1012,  0.0927,  0.1093, -0.0091,  0.0373, -0.0337, -0.0068,  0.0294],\n",
            "        [ 0.0732,  0.0640,  0.0707, -0.0056,  0.0254, -0.0265, -0.0030,  0.0226],\n",
            "        [ 0.0483,  0.0435,  0.0499, -0.0050,  0.0188, -0.0138, -0.0051,  0.0149]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017733165994286537 tensor([[ 0.1092,  0.1017,  0.1065, -0.0076,  0.0391, -0.0371, -0.0075,  0.0298],\n",
            "        [ 0.1273,  0.1154,  0.1381, -0.0127,  0.0469, -0.0409, -0.0090,  0.0334],\n",
            "        [ 0.0948,  0.0815,  0.0900, -0.0077,  0.0327, -0.0336, -0.0041,  0.0274],\n",
            "        [ 0.0655,  0.0589,  0.0678, -0.0073,  0.0256, -0.0178, -0.0075,  0.0190]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017235811799764633 tensor([[ 0.1252,  0.1149,  0.1192, -0.0093,  0.0447, -0.0416, -0.0090,  0.0311],\n",
            "        [ 0.1446,  0.1294,  0.1573, -0.0160,  0.0532, -0.0446, -0.0106,  0.0337],\n",
            "        [ 0.1106,  0.0935,  0.1032, -0.0095,  0.0379, -0.0384, -0.0049,  0.0298],\n",
            "        [ 0.0793,  0.0711,  0.0822, -0.0095,  0.0311, -0.0203, -0.0096,  0.0215]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01699625328183174 tensor([[ 0.1370,  0.1239,  0.1272, -0.0108,  0.0488, -0.0444, -0.0103,  0.0307],\n",
            "        [ 0.1564,  0.1379,  0.1705, -0.0190,  0.0574, -0.0462, -0.0120,  0.0316],\n",
            "        [ 0.1225,  0.1016,  0.1122, -0.0111,  0.0417, -0.0416, -0.0056,  0.0306],\n",
            "        [ 0.0904,  0.0809,  0.0939, -0.0116,  0.0356, -0.0218, -0.0117,  0.0227]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02054017409682274 tensor([[ 0.0572,  0.0474,  0.0528, -0.0027,  0.0196, -0.0209, -0.0022,  0.0193],\n",
            "        [ 0.0559,  0.0534,  0.0602, -0.0037,  0.0205, -0.0195, -0.0038,  0.0205],\n",
            "        [ 0.0347,  0.0277,  0.0310, -0.0025,  0.0116, -0.0121, -0.0015,  0.0111],\n",
            "        [ 0.0201,  0.0181,  0.0209, -0.0018,  0.0080, -0.0061, -0.0018,  0.0062]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018450729548931122 tensor([[ 0.0985,  0.0790,  0.0887, -0.0049,  0.0335, -0.0355, -0.0036,  0.0316],\n",
            "        [ 0.0918,  0.0880,  0.0994, -0.0063,  0.0338, -0.0316, -0.0068,  0.0329],\n",
            "        [ 0.0574,  0.0438,  0.0490, -0.0043,  0.0186, -0.0195, -0.0024,  0.0169],\n",
            "        [ 0.0363,  0.0326,  0.0381, -0.0034,  0.0146, -0.0104, -0.0036,  0.0103]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017519526183605194 tensor([[ 0.1290,  0.0996,  0.1126, -0.0067,  0.0435, -0.0457, -0.0044,  0.0392],\n",
            "        [ 0.1148,  0.1101,  0.1247, -0.0082,  0.0424, -0.0386, -0.0092,  0.0399],\n",
            "        [ 0.0725,  0.0526,  0.0591, -0.0057,  0.0228, -0.0241, -0.0028,  0.0193],\n",
            "        [ 0.0496,  0.0445,  0.0525, -0.0048,  0.0202, -0.0133, -0.0053,  0.0131]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017091767862439156 tensor([[ 0.1524,  0.1129,  0.1285, -0.0083,  0.0508, -0.0531, -0.0048,  0.0439],\n",
            "        [ 0.1299,  0.1245,  0.1413, -0.0097,  0.0481, -0.0426, -0.0114,  0.0437],\n",
            "        [ 0.0830,  0.0570,  0.0644, -0.0070,  0.0252, -0.0268, -0.0028,  0.0197],\n",
            "        [ 0.0606,  0.0542,  0.0647, -0.0060,  0.0250, -0.0151, -0.0069,  0.0147]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016879860311746597 tensor([[ 0.1711,  0.1212,  0.1390, -0.0098,  0.0565, -0.0587, -0.0051,  0.0466],\n",
            "        [ 0.1399,  0.1341,  0.1525, -0.0108,  0.0520, -0.0447, -0.0133,  0.0453],\n",
            "        [ 0.0909,  0.0589,  0.0669, -0.0080,  0.0266, -0.0285, -0.0027,  0.0188],\n",
            "        [ 0.0697,  0.0623,  0.0749, -0.0072,  0.0291, -0.0161, -0.0085,  0.0154]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022512711584568024 tensor([[ 0.0648,  0.0587,  0.0656, -0.0040,  0.0235, -0.0234, -0.0044,  0.0237],\n",
            "        [ 0.0595,  0.0493,  0.0542, -0.0042,  0.0210, -0.0222, -0.0026,  0.0201],\n",
            "        [ 0.0583,  0.0548,  0.0598, -0.0039,  0.0206, -0.0209, -0.0032,  0.0227],\n",
            "        [ 0.0246,  0.0237,  0.0272, -0.0019,  0.0098, -0.0093, -0.0015,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019357608631253242 tensor([[ 0.1076,  0.0958,  0.1080, -0.0073,  0.0390, -0.0381, -0.0077,  0.0375],\n",
            "        [ 0.1020,  0.0824,  0.0906, -0.0078,  0.0361, -0.0376, -0.0047,  0.0325],\n",
            "        [ 0.0985,  0.0920,  0.0998, -0.0071,  0.0347, -0.0349, -0.0057,  0.0378],\n",
            "        [ 0.0438,  0.0423,  0.0488, -0.0035,  0.0176, -0.0164, -0.0027,  0.0180]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01793782413005829 tensor([[ 0.1355,  0.1178,  0.1339, -0.0101,  0.0492, -0.0470, -0.0101,  0.0448],\n",
            "        [ 0.1318,  0.1031,  0.1134, -0.0108,  0.0468, -0.0477, -0.0062,  0.0391],\n",
            "        [ 0.1257,  0.1164,  0.1257, -0.0098,  0.0442, -0.0440, -0.0077,  0.0474],\n",
            "        [ 0.0588,  0.0571,  0.0662, -0.0049,  0.0237, -0.0216, -0.0037,  0.0238]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01732044108211994 tensor([[ 0.1542,  0.1303,  0.1494, -0.0124,  0.0559, -0.0524, -0.0119,  0.0483],\n",
            "        [ 0.1534,  0.1156,  0.1273, -0.0134,  0.0545, -0.0545, -0.0074,  0.0421],\n",
            "        [ 0.1446,  0.1326,  0.1425, -0.0121,  0.0507, -0.0499, -0.0094,  0.0535],\n",
            "        [ 0.0705,  0.0688,  0.0802, -0.0062,  0.0286, -0.0256, -0.0045,  0.0280]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017038289457559586 tensor([[ 0.1674,  0.1371,  0.1585, -0.0145,  0.0607, -0.0556, -0.0134,  0.0495],\n",
            "        [ 0.1697,  0.1229,  0.1355, -0.0157,  0.0603, -0.0590, -0.0084,  0.0427],\n",
            "        [ 0.1582,  0.1435,  0.1534, -0.0142,  0.0553, -0.0537, -0.0109,  0.0572],\n",
            "        [ 0.0797,  0.0782,  0.0916, -0.0074,  0.0326, -0.0285, -0.0052,  0.0312]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01955575682222843 tensor([[ 0.0680,  0.0620,  0.0685, -0.0040,  0.0242, -0.0248, -0.0035,  0.0209],\n",
            "        [ 0.0640,  0.0562,  0.0647, -0.0040,  0.0232, -0.0220, -0.0026,  0.0206],\n",
            "        [ 0.0504,  0.0451,  0.0514, -0.0035,  0.0191, -0.0175, -0.0030,  0.0176],\n",
            "        [ 0.0273,  0.0190,  0.0227, -0.0022,  0.0097, -0.0092, -0.0007,  0.0070]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016398005187511444 tensor([[ 0.1136,  0.1021,  0.1134, -0.0073,  0.0405, -0.0408, -0.0059,  0.0321],\n",
            "        [ 0.1058,  0.0910,  0.1059, -0.0068,  0.0384, -0.0354, -0.0043,  0.0316],\n",
            "        [ 0.0850,  0.0756,  0.0866, -0.0063,  0.0325, -0.0290, -0.0053,  0.0283],\n",
            "        [ 0.0501,  0.0336,  0.0408, -0.0041,  0.0176, -0.0164, -0.0012,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015040013939142227 tensor([[ 0.1441,  0.1268,  0.1416, -0.0099,  0.0512, -0.0509, -0.0075,  0.0370],\n",
            "        [ 0.1321,  0.1103,  0.1300, -0.0088,  0.0480, -0.0426, -0.0053,  0.0359],\n",
            "        [ 0.1079,  0.0954,  0.1098, -0.0085,  0.0416, -0.0360, -0.0070,  0.0340],\n",
            "        [ 0.0695,  0.0450,  0.0555, -0.0058,  0.0242, -0.0220, -0.0016,  0.0145]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014474812895059586 tensor([[ 0.1651,  0.1418,  0.1590, -0.0121,  0.0586, -0.0574, -0.0085,  0.0381],\n",
            "        [ 0.1491,  0.1203,  0.1437, -0.0103,  0.0542, -0.0461, -0.0059,  0.0364],\n",
            "        [ 0.1235,  0.1081,  0.1253, -0.0104,  0.0480, -0.0400, -0.0083,  0.0366],\n",
            "        [ 0.0862,  0.0539,  0.0675, -0.0074,  0.0297, -0.0264, -0.0018,  0.0160]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01422423031181097 tensor([[ 0.1803,  0.1508,  0.1697, -0.0140,  0.0638, -0.0617, -0.0093,  0.0370],\n",
            "        [ 0.1609,  0.1248,  0.1515, -0.0115,  0.0584, -0.0474, -0.0063,  0.0346],\n",
            "        [ 0.1344,  0.1164,  0.1358, -0.0120,  0.0526, -0.0423, -0.0093,  0.0372],\n",
            "        [ 0.1006,  0.0609,  0.0774, -0.0089,  0.0345, -0.0299, -0.0019,  0.0164]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.0254337340593338 tensor([[ 0.0464,  0.0459,  0.0523, -0.0041,  0.0178, -0.0157, -0.0045,  0.0169],\n",
            "        [ 0.0439,  0.0348,  0.0404, -0.0028,  0.0148, -0.0159, -0.0015,  0.0128],\n",
            "        [ 0.0655,  0.0623,  0.0680, -0.0044,  0.0225, -0.0221, -0.0031,  0.0220],\n",
            "        [ 0.0282,  0.0174,  0.0203, -0.0020,  0.0095, -0.0112, -0.0009,  0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.023030584678053856 tensor([[ 0.0743,  0.0734,  0.0843, -0.0075,  0.0289, -0.0243, -0.0081,  0.0260],\n",
            "        [ 0.0705,  0.0526,  0.0626, -0.0048,  0.0233, -0.0247, -0.0021,  0.0178],\n",
            "        [ 0.1090,  0.1035,  0.1122, -0.0079,  0.0369, -0.0361, -0.0052,  0.0348],\n",
            "        [ 0.0516,  0.0301,  0.0354, -0.0037,  0.0171, -0.0202, -0.0017,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.022063780575990677 tensor([[ 0.0920,  0.0906,  0.1048, -0.0105,  0.0362, -0.0292, -0.0113,  0.0311],\n",
            "        [ 0.0873,  0.0610,  0.0746, -0.0064,  0.0282, -0.0295, -0.0021,  0.0185],\n",
            "        [ 0.1374,  0.1300,  0.1398, -0.0108,  0.0457, -0.0444, -0.0067,  0.0416],\n",
            "        [ 0.0710,  0.0391,  0.0466, -0.0053,  0.0233, -0.0275, -0.0022,  0.0196]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02165844663977623 tensor([[ 0.1039,  0.1019,  0.1187, -0.0133,  0.0414, -0.0318, -0.0142,  0.0338],\n",
            "        [ 0.0989,  0.0642,  0.0809, -0.0077,  0.0311, -0.0321, -0.0017,  0.0170],\n",
            "        [ 0.1564,  0.1472,  0.1571, -0.0133,  0.0511, -0.0493, -0.0077,  0.0445],\n",
            "        [ 0.0874,  0.0454,  0.0548, -0.0067,  0.0284, -0.0334, -0.0026,  0.0224]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.021472208201885223 tensor([[ 0.1122,  0.1095,  0.1285, -0.0159,  0.0453, -0.0331, -0.0169,  0.0352],\n",
            "        [ 0.1075,  0.0645,  0.0841, -0.0089,  0.0329, -0.0334, -0.0011,  0.0143],\n",
            "        [ 0.1695,  0.1585,  0.1676, -0.0155,  0.0542, -0.0520, -0.0085,  0.0451],\n",
            "        [ 0.1017,  0.0495,  0.0607, -0.0080,  0.0326, -0.0383, -0.0030,  0.0241]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.5497e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02156751975417137 tensor([[ 0.0591,  0.0561,  0.0630, -0.0040,  0.0225, -0.0208, -0.0036,  0.0183],\n",
            "        [ 0.0695,  0.0647,  0.0739, -0.0044,  0.0254, -0.0252, -0.0036,  0.0251],\n",
            "        [ 0.0462,  0.0393,  0.0431, -0.0033,  0.0164, -0.0172, -0.0029,  0.0181],\n",
            "        [ 0.0179,  0.0187,  0.0222, -0.0032,  0.0073, -0.0068, -0.0016,  0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018529783934354782 tensor([[ 0.0983,  0.0928,  0.1052, -0.0074,  0.0378, -0.0336, -0.0062,  0.0276],\n",
            "        [ 0.1176,  0.1091,  0.1253, -0.0079,  0.0433, -0.0422, -0.0064,  0.0409],\n",
            "        [ 0.0769,  0.0637,  0.0695, -0.0057,  0.0273, -0.0287, -0.0050,  0.0298],\n",
            "        [ 0.0314,  0.0334,  0.0401, -0.0063,  0.0127, -0.0118, -0.0030,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017175279557704926 tensor([[ 0.1240,  0.1158,  0.1325, -0.0103,  0.0482, -0.0410, -0.0082,  0.0313],\n",
            "        [ 0.1492,  0.1372,  0.1587, -0.0108,  0.0552, -0.0528, -0.0085,  0.0497],\n",
            "        [ 0.0972,  0.0780,  0.0845, -0.0075,  0.0344, -0.0363, -0.0067,  0.0371],\n",
            "        [ 0.0417,  0.0453,  0.0550, -0.0093,  0.0169, -0.0155, -0.0041,  0.0149]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01660442352294922 tensor([[ 0.1414,  0.1302,  0.1503, -0.0129,  0.0556, -0.0451, -0.0097,  0.0316],\n",
            "        [ 0.1701,  0.1545,  0.1802, -0.0133,  0.0632, -0.0592, -0.0102,  0.0540],\n",
            "        [ 0.1112,  0.0861,  0.0925, -0.0089,  0.0392, -0.0413, -0.0080,  0.0416],\n",
            "        [ 0.0496,  0.0551,  0.0676, -0.0123,  0.0200, -0.0183, -0.0050,  0.0171]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01635563373565674 tensor([[ 0.1536,  0.1391,  0.1621, -0.0153,  0.0610, -0.0471, -0.0110,  0.0298],\n",
            "        [ 0.1842,  0.1650,  0.1940, -0.0155,  0.0687, -0.0629, -0.0116,  0.0555],\n",
            "        [ 0.1214,  0.0905,  0.0964, -0.0101,  0.0425, -0.0449, -0.0091,  0.0444],\n",
            "        [ 0.0556,  0.0633,  0.0783, -0.0151,  0.0224, -0.0204, -0.0057,  0.0184]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02371617779135704 tensor([[ 0.0605,  0.0510,  0.0584, -0.0028,  0.0211, -0.0207, -0.0028,  0.0189],\n",
            "        [ 0.0516,  0.0446,  0.0493, -0.0038,  0.0187, -0.0187, -0.0034,  0.0165],\n",
            "        [ 0.0513,  0.0499,  0.0551, -0.0039,  0.0179, -0.0180, -0.0024,  0.0173],\n",
            "        [ 0.0272,  0.0223,  0.0275, -0.0035,  0.0103, -0.0101, -0.0018,  0.0097]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02117430791258812 tensor([[ 0.1021,  0.0834,  0.0965, -0.0049,  0.0353, -0.0339, -0.0046,  0.0298],\n",
            "        [ 0.0863,  0.0730,  0.0808, -0.0070,  0.0315, -0.0306, -0.0060,  0.0252],\n",
            "        [ 0.0854,  0.0835,  0.0917, -0.0070,  0.0293, -0.0295, -0.0040,  0.0274],\n",
            "        [ 0.0490,  0.0393,  0.0494, -0.0068,  0.0185, -0.0179, -0.0035,  0.0169]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02004236727952957 tensor([[ 0.1307,  0.1028,  0.1202, -0.0065,  0.0447, -0.0422, -0.0058,  0.0354],\n",
            "        [ 0.1100,  0.0904,  0.1003, -0.0097,  0.0403, -0.0380, -0.0081,  0.0289],\n",
            "        [ 0.1084,  0.1068,  0.1166, -0.0097,  0.0367, -0.0368, -0.0053,  0.0329],\n",
            "        [ 0.0665,  0.0522,  0.0668, -0.0100,  0.0250, -0.0240, -0.0050,  0.0220]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.0195305235683918 tensor([[ 0.1511,  0.1138,  0.1344, -0.0077,  0.0511, -0.0475, -0.0065,  0.0379],\n",
            "        [ 0.1269,  0.1012,  0.1124, -0.0120,  0.0467, -0.0425, -0.0099,  0.0293],\n",
            "        [ 0.1245,  0.1235,  0.1341, -0.0120,  0.0414, -0.0413, -0.0062,  0.0355],\n",
            "        [ 0.0806,  0.0618,  0.0807, -0.0131,  0.0301, -0.0286, -0.0064,  0.0256]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019283197820186615 tensor([[ 0.1663,  0.1196,  0.1428, -0.0086,  0.0556, -0.0508, -0.0070,  0.0385],\n",
            "        [ 0.1396,  0.1078,  0.1200, -0.0141,  0.0515, -0.0452, -0.0115,  0.0279],\n",
            "        [ 0.1361,  0.1357,  0.1467, -0.0142,  0.0445, -0.0441, -0.0070,  0.0361],\n",
            "        [ 0.0920,  0.0690,  0.0918, -0.0161,  0.0343, -0.0322, -0.0077,  0.0280]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020023854449391365 tensor([[ 0.0607,  0.0495,  0.0565, -0.0025,  0.0215, -0.0203, -0.0028,  0.0198],\n",
            "        [ 0.0340,  0.0256,  0.0293, -0.0026,  0.0123, -0.0123, -0.0021,  0.0111],\n",
            "        [ 0.0395,  0.0381,  0.0429, -0.0027,  0.0123, -0.0132, -0.0003,  0.0113],\n",
            "        [ 0.0252,  0.0216,  0.0253, -0.0021,  0.0102, -0.0083, -0.0023,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01826643943786621 tensor([[ 1.0429e-01,  8.2607e-02,  9.5024e-02, -4.2844e-03,  3.6783e-02,\n",
            "         -3.4084e-02, -4.7338e-03,  3.2187e-02],\n",
            "        [ 5.6973e-02,  4.0703e-02,  4.7159e-02, -4.7421e-03,  2.0938e-02,\n",
            "         -2.0332e-02, -3.7432e-03,  1.7352e-02],\n",
            "        [ 6.4049e-02,  6.2008e-02,  6.9618e-02, -4.8292e-03,  1.8950e-02,\n",
            "         -2.0838e-02, -4.3511e-05,  1.6675e-02],\n",
            "        [ 4.4937e-02,  3.8071e-02,  4.5204e-02, -4.0257e-03,  1.8311e-02,\n",
            "         -1.4138e-02, -4.3190e-03,  1.2736e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01746700331568718 tensor([[ 0.1358,  0.1037,  0.1204, -0.0056,  0.0477, -0.0432, -0.0061,  0.0394],\n",
            "        [ 0.0729,  0.0490,  0.0577, -0.0066,  0.0272, -0.0255, -0.0051,  0.0204],\n",
            "        [ 0.0798,  0.0774,  0.0869, -0.0065,  0.0222, -0.0252,  0.0006,  0.0185],\n",
            "        [ 0.0605,  0.0507,  0.0609, -0.0058,  0.0248, -0.0182, -0.0062,  0.0155]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017096184194087982 tensor([[ 0.1591,  0.1165,  0.1366, -0.0065,  0.0555, -0.0493, -0.0070,  0.0433],\n",
            "        [ 0.0846,  0.0531,  0.0637, -0.0083,  0.0319, -0.0288, -0.0062,  0.0215],\n",
            "        [ 0.0902,  0.0878,  0.0984, -0.0080,  0.0235, -0.0275,  0.0016,  0.0182],\n",
            "        [ 0.0728,  0.0604,  0.0733, -0.0074,  0.0302, -0.0209, -0.0078,  0.0168]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016915159299969673 tensor([[ 0.1768,  0.1238,  0.1466, -0.0072,  0.0614, -0.0532, -0.0077,  0.0448],\n",
            "        [ 0.0935,  0.0547,  0.0668, -0.0100,  0.0357, -0.0311, -0.0073,  0.0215],\n",
            "        [ 0.0973,  0.0950,  0.1064, -0.0093,  0.0235, -0.0286,  0.0026,  0.0167],\n",
            "        [ 0.0828,  0.0678,  0.0832, -0.0089,  0.0346, -0.0226, -0.0094,  0.0168]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022215478122234344 tensor([[ 0.0633,  0.0513,  0.0560, -0.0043,  0.0226, -0.0237, -0.0035,  0.0215],\n",
            "        [ 0.0513,  0.0495,  0.0550, -0.0031,  0.0182, -0.0185, -0.0022,  0.0171],\n",
            "        [ 0.0576,  0.0554,  0.0650, -0.0043,  0.0227, -0.0206, -0.0044,  0.0190],\n",
            "        [ 0.0288,  0.0202,  0.0232, -0.0022,  0.0097, -0.0116, -0.0010,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019276633858680725 tensor([[ 0.1100,  0.0865,  0.0947, -0.0081,  0.0392, -0.0408, -0.0061,  0.0356],\n",
            "        [ 0.0845,  0.0820,  0.0912, -0.0054,  0.0298, -0.0300, -0.0037,  0.0263],\n",
            "        [ 0.0981,  0.0946,  0.1121, -0.0079,  0.0390, -0.0347, -0.0081,  0.0305],\n",
            "        [ 0.0527,  0.0356,  0.0413, -0.0043,  0.0177, -0.0210, -0.0019,  0.0161]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01785428635776043 tensor([[ 0.1449,  0.1101,  0.1206, -0.0115,  0.0517, -0.0533, -0.0083,  0.0447],\n",
            "        [ 0.1051,  0.1022,  0.1137, -0.0072,  0.0368, -0.0364, -0.0045,  0.0301],\n",
            "        [ 0.1252,  0.1209,  0.1448, -0.0110,  0.0502, -0.0437, -0.0111,  0.0363],\n",
            "        [ 0.0726,  0.0472,  0.0554, -0.0063,  0.0241, -0.0286, -0.0026,  0.0209]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01718231663107872 tensor([[ 0.1722,  0.1259,  0.1379, -0.0146,  0.0615, -0.0630, -0.0100,  0.0508],\n",
            "        [ 0.1179,  0.1146,  0.1276, -0.0086,  0.0408, -0.0397, -0.0049,  0.0305],\n",
            "        [ 0.1433,  0.1384,  0.1677, -0.0137,  0.0581, -0.0492, -0.0137,  0.0382],\n",
            "        [ 0.0893,  0.0560,  0.0666, -0.0081,  0.0293, -0.0347, -0.0032,  0.0242]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01684688590466976 tensor([[ 0.1942,  0.1364,  0.1494, -0.0175,  0.0693, -0.0706, -0.0115,  0.0547],\n",
            "        [ 0.1259,  0.1222,  0.1363, -0.0099,  0.0430, -0.0410, -0.0051,  0.0290],\n",
            "        [ 0.1556,  0.1501,  0.1843, -0.0161,  0.0637, -0.0524, -0.0160,  0.0377],\n",
            "        [ 0.1037,  0.0625,  0.0754, -0.0099,  0.0336, -0.0398, -0.0037,  0.0263]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02170625329017639 tensor([[ 0.0631,  0.0577,  0.0641, -0.0042,  0.0221, -0.0231, -0.0025,  0.0194],\n",
            "        [ 0.0769,  0.0730,  0.0826, -0.0049,  0.0297, -0.0268, -0.0046,  0.0245],\n",
            "        [ 0.0370,  0.0278,  0.0300, -0.0023,  0.0125, -0.0150, -0.0020,  0.0139],\n",
            "        [ 0.0229,  0.0154,  0.0207, -0.0020,  0.0084, -0.0058, -0.0011,  0.0052]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01847035251557827 tensor([[ 0.1042,  0.0940,  0.1050, -0.0076,  0.0362, -0.0375, -0.0038,  0.0291],\n",
            "        [ 0.1316,  0.1256,  0.1427, -0.0089,  0.0514, -0.0450, -0.0083,  0.0394],\n",
            "        [ 0.0623,  0.0442,  0.0471, -0.0041,  0.0210, -0.0255, -0.0037,  0.0230],\n",
            "        [ 0.0416,  0.0268,  0.0372, -0.0037,  0.0151, -0.0097, -0.0021,  0.0084]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017005782574415207 tensor([[ 0.1306,  0.1154,  0.1298, -0.0105,  0.0450, -0.0462, -0.0045,  0.0328],\n",
            "        [ 0.1663,  0.1586,  0.1810, -0.0120,  0.0656, -0.0556, -0.0111,  0.0458],\n",
            "        [ 0.0798,  0.0529,  0.0557, -0.0054,  0.0267, -0.0330, -0.0051,  0.0288],\n",
            "        [ 0.0572,  0.0354,  0.0506, -0.0053,  0.0207, -0.0123, -0.0028,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016417596489191055 tensor([[ 0.1482,  0.1278,  0.1446, -0.0130,  0.0507, -0.0516, -0.0047,  0.0331],\n",
            "        [ 0.1879,  0.1785,  0.2048, -0.0146,  0.0749, -0.0611, -0.0132,  0.0467],\n",
            "        [ 0.0927,  0.0572,  0.0593, -0.0065,  0.0306, -0.0387, -0.0065,  0.0327],\n",
            "        [ 0.0705,  0.0418,  0.0618, -0.0068,  0.0254, -0.0138, -0.0035,  0.0109]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016172418370842934 tensor([[ 0.1606,  0.1349,  0.1536, -0.0152,  0.0545, -0.0549, -0.0046,  0.0315],\n",
            "        [ 0.2017,  0.1905,  0.2199, -0.0168,  0.0813, -0.0637, -0.0150,  0.0443],\n",
            "        [ 0.1028,  0.0587,  0.0600, -0.0074,  0.0335, -0.0431, -0.0077,  0.0355],\n",
            "        [ 0.0821,  0.0466,  0.0711, -0.0082,  0.0294, -0.0146, -0.0041,  0.0108]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02045121043920517 tensor([[ 0.0657,  0.0676,  0.0760, -0.0054,  0.0255, -0.0224, -0.0063,  0.0226],\n",
            "        [ 0.0598,  0.0508,  0.0577, -0.0043,  0.0220, -0.0232, -0.0029,  0.0209],\n",
            "        [ 0.0410,  0.0370,  0.0417, -0.0030,  0.0151, -0.0154, -0.0034,  0.0139],\n",
            "        [ 0.0302,  0.0262,  0.0276, -0.0023,  0.0089, -0.0126, -0.0002,  0.0089]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017514754086732864 tensor([[ 1.0635e-01,  1.1055e-01,  1.2497e-01, -9.8896e-03,  4.1895e-02,\n",
            "         -3.5181e-02, -1.1444e-02,  3.4733e-02],\n",
            "        [ 9.8515e-02,  8.1596e-02,  9.3441e-02, -7.7128e-03,  3.6602e-02,\n",
            "         -3.8433e-02, -4.8852e-03,  3.2873e-02],\n",
            "        [ 6.8207e-02,  6.0825e-02,  6.8722e-02, -5.2643e-03,  2.5072e-02,\n",
            "         -2.5291e-02, -6.1822e-03,  2.1763e-02],\n",
            "        [ 5.5161e-02,  4.7474e-02,  4.9782e-02, -4.4811e-03,  1.5697e-02,\n",
            "         -2.2964e-02, -1.1325e-05,  1.5326e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01628975197672844 tensor([[ 0.1311,  0.1372,  0.1558, -0.0138,  0.0524, -0.0419, -0.0158,  0.0404],\n",
            "        [ 0.1232,  0.0988,  0.1142, -0.0106,  0.0460, -0.0482, -0.0062,  0.0388],\n",
            "        [ 0.0867,  0.0763,  0.0865, -0.0071,  0.0318, -0.0317, -0.0086,  0.0257],\n",
            "        [ 0.0758,  0.0650,  0.0676, -0.0065,  0.0208, -0.0317,  0.0004,  0.0198]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015768500044941902 tensor([[ 0.1465,  0.1541,  0.1757, -0.0172,  0.0594, -0.0451, -0.0197,  0.0424],\n",
            "        [ 0.1397,  0.1078,  0.1262, -0.0132,  0.0524, -0.0546, -0.0072,  0.0411],\n",
            "        [ 0.0999,  0.0865,  0.0986, -0.0087,  0.0366, -0.0359, -0.0107,  0.0273],\n",
            "        [ 0.0931,  0.0794,  0.0821, -0.0083,  0.0245, -0.0390,  0.0010,  0.0227]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015524215996265411 tensor([[ 0.1563,  0.1652,  0.1891, -0.0204,  0.0643, -0.0462, -0.0234,  0.0423],\n",
            "        [ 0.1513,  0.1120,  0.1328, -0.0156,  0.0569, -0.0591, -0.0079,  0.0414],\n",
            "        [ 0.1094,  0.0934,  0.1069, -0.0101,  0.0400, -0.0387, -0.0127,  0.0274],\n",
            "        [ 0.1076,  0.0912,  0.0937, -0.0101,  0.0273, -0.0452,  0.0017,  0.0243]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, 1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02525748685002327 tensor([[ 0.0757,  0.0660,  0.0752, -0.0047,  0.0286, -0.0251, -0.0052,  0.0237],\n",
            "        [ 0.0736,  0.0658,  0.0733, -0.0042,  0.0267, -0.0271, -0.0036,  0.0257],\n",
            "        [ 0.0615,  0.0508,  0.0592, -0.0048,  0.0236, -0.0217, -0.0026,  0.0193],\n",
            "        [ 0.0297,  0.0216,  0.0245, -0.0020,  0.0110, -0.0109, -0.0017,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021093180403113365 tensor([[ 0.1269,  0.1081,  0.1249, -0.0088,  0.0481, -0.0403, -0.0092,  0.0361],\n",
            "        [ 0.1251,  0.1114,  0.1241, -0.0075,  0.0457, -0.0458, -0.0064,  0.0420],\n",
            "        [ 0.1082,  0.0879,  0.1035, -0.0090,  0.0422, -0.0377, -0.0048,  0.0316],\n",
            "        [ 0.0555,  0.0392,  0.0446, -0.0038,  0.0207, -0.0204, -0.0033,  0.0192]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019111821427941322 tensor([[ 0.1607,  0.1326,  0.1556, -0.0122,  0.0612, -0.0487, -0.0123,  0.0410],\n",
            "        [ 0.1576,  0.1385,  0.1542, -0.0099,  0.0579, -0.0572, -0.0085,  0.0503],\n",
            "        [ 0.1421,  0.1128,  0.1347, -0.0126,  0.0562, -0.0484, -0.0065,  0.0380],\n",
            "        [ 0.0780,  0.0538,  0.0613, -0.0053,  0.0294, -0.0286, -0.0047,  0.0266]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018255919218063354 tensor([[ 0.1839,  0.1462,  0.1743, -0.0153,  0.0704, -0.0529, -0.0148,  0.0416],\n",
            "        [ 0.1775,  0.1531,  0.1706, -0.0118,  0.0656, -0.0637, -0.0100,  0.0535],\n",
            "        [ 0.1664,  0.1285,  0.1560, -0.0159,  0.0669, -0.0552, -0.0077,  0.0399],\n",
            "        [ 0.0979,  0.0660,  0.0752, -0.0067,  0.0372, -0.0357, -0.0061,  0.0327]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017871707677841187 tensor([[ 0.2009,  0.1534,  0.1858, -0.0180,  0.0773, -0.0547, -0.0170,  0.0398],\n",
            "        [ 0.1900,  0.1603,  0.1786, -0.0133,  0.0705, -0.0674, -0.0112,  0.0538],\n",
            "        [ 0.1843,  0.1379,  0.1705, -0.0188,  0.0752, -0.0595, -0.0087,  0.0389],\n",
            "        [ 0.1155,  0.0762,  0.0870, -0.0079,  0.0442, -0.0419, -0.0073,  0.0378]],\n",
            "       device='cuda:0')\n",
            "c tensor([7.5996e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02219545841217041 tensor([[ 0.0624,  0.0610,  0.0675, -0.0034,  0.0230, -0.0211, -0.0037,  0.0214],\n",
            "        [ 0.0585,  0.0523,  0.0605, -0.0038,  0.0221, -0.0203, -0.0039,  0.0206],\n",
            "        [ 0.0603,  0.0550,  0.0621, -0.0045,  0.0209, -0.0226, -0.0019,  0.0200],\n",
            "        [ 0.0231,  0.0156,  0.0191, -0.0022,  0.0092, -0.0070, -0.0017,  0.0066]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018993575125932693 tensor([[ 0.1073,  0.1054,  0.1170, -0.0062,  0.0396, -0.0353, -0.0066,  0.0350],\n",
            "        [ 0.0964,  0.0847,  0.0990, -0.0066,  0.0367, -0.0327, -0.0068,  0.0324],\n",
            "        [ 0.1001,  0.0907,  0.1027, -0.0082,  0.0342, -0.0373, -0.0031,  0.0311],\n",
            "        [ 0.0411,  0.0264,  0.0328, -0.0042,  0.0167, -0.0118, -0.0031,  0.0106]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017558086663484573 tensor([[ 0.1392,  0.1369,  0.1522, -0.0086,  0.0515, -0.0444, -0.0088,  0.0431],\n",
            "        [ 0.1200,  0.1030,  0.1219, -0.0087,  0.0461, -0.0396, -0.0090,  0.0382],\n",
            "        [ 0.1251,  0.1124,  0.1278, -0.0113,  0.0419, -0.0463, -0.0036,  0.0360],\n",
            "        [ 0.0555,  0.0337,  0.0428, -0.0061,  0.0227, -0.0151, -0.0044,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01693650707602501 tensor([[ 0.1621,  0.1594,  0.1775, -0.0106,  0.0602, -0.0502, -0.0107,  0.0476],\n",
            "        [ 0.1351,  0.1128,  0.1354, -0.0104,  0.0523, -0.0431, -0.0108,  0.0403],\n",
            "        [ 0.1410,  0.1256,  0.1434, -0.0139,  0.0463, -0.0518, -0.0038,  0.0370],\n",
            "        [ 0.0675,  0.0386,  0.0502, -0.0078,  0.0278, -0.0172, -0.0055,  0.0136]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01665152609348297 tensor([[ 0.1791,  0.1758,  0.1961, -0.0124,  0.0667, -0.0537, -0.0123,  0.0497],\n",
            "        [ 0.1452,  0.1177,  0.1434, -0.0118,  0.0567, -0.0447, -0.0122,  0.0404],\n",
            "        [ 0.1515,  0.1336,  0.1532, -0.0164,  0.0487, -0.0551, -0.0036,  0.0359],\n",
            "        [ 0.0777,  0.0419,  0.0557, -0.0095,  0.0322, -0.0186, -0.0065,  0.0136]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019945912063121796 tensor([[ 0.0730,  0.0688,  0.0753, -0.0044,  0.0259, -0.0258, -0.0044,  0.0245],\n",
            "        [ 0.0569,  0.0492,  0.0544, -0.0034,  0.0206, -0.0216, -0.0028,  0.0201],\n",
            "        [ 0.0567,  0.0543,  0.0634, -0.0040,  0.0210, -0.0189, -0.0025,  0.0183],\n",
            "        [ 0.0289,  0.0201,  0.0222, -0.0021,  0.0100, -0.0115, -0.0013,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016429301351308823 tensor([[ 0.1250,  0.1175,  0.1287, -0.0081,  0.0443, -0.0435, -0.0078,  0.0400],\n",
            "        [ 0.0949,  0.0809,  0.0894, -0.0060,  0.0347, -0.0358, -0.0048,  0.0318],\n",
            "        [ 0.0965,  0.0929,  0.1093, -0.0073,  0.0357, -0.0314, -0.0042,  0.0292],\n",
            "        [ 0.0534,  0.0360,  0.0397, -0.0040,  0.0185, -0.0212, -0.0024,  0.0217]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.014750403352081776 tensor([[ 0.1604,  0.1497,  0.1638, -0.0112,  0.0565, -0.0549, -0.0105,  0.0488],\n",
            "        [ 0.1188,  0.0987,  0.1093, -0.0079,  0.0438, -0.0444, -0.0061,  0.0372],\n",
            "        [ 0.1229,  0.1189,  0.1411, -0.0100,  0.0453, -0.0386, -0.0054,  0.0343],\n",
            "        [ 0.0742,  0.0484,  0.0534, -0.0058,  0.0256, -0.0293, -0.0034,  0.0299]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014010037295520306 tensor([[ 0.1847,  0.1705,  0.1861, -0.0138,  0.0647, -0.0622, -0.0127,  0.0534],\n",
            "        [ 0.1338,  0.1079,  0.1197, -0.0094,  0.0497, -0.0493, -0.0071,  0.0387],\n",
            "        [ 0.1403,  0.1361,  0.1634, -0.0124,  0.0515, -0.0424, -0.0061,  0.0356],\n",
            "        [ 0.0921,  0.0582,  0.0642, -0.0074,  0.0317, -0.0363, -0.0043,  0.0368]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013674352318048477 tensor([[ 0.2019,  0.1839,  0.2001, -0.0161,  0.0703, -0.0668, -0.0146,  0.0552],\n",
            "        [ 0.1438,  0.1118,  0.1245, -0.0106,  0.0537, -0.0521, -0.0077,  0.0379],\n",
            "        [ 0.1519,  0.1477,  0.1794, -0.0146,  0.0555, -0.0438, -0.0065,  0.0344],\n",
            "        [ 0.1077,  0.0659,  0.0727, -0.0089,  0.0369, -0.0422, -0.0052,  0.0426]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025872385129332542 tensor([[ 0.0650,  0.0620,  0.0702, -0.0048,  0.0231, -0.0228, -0.0025,  0.0201],\n",
            "        [ 0.0438,  0.0407,  0.0464, -0.0025,  0.0170, -0.0159, -0.0035,  0.0166],\n",
            "        [ 0.0503,  0.0452,  0.0520, -0.0043,  0.0191, -0.0177, -0.0036,  0.0164],\n",
            "        [ 0.0280,  0.0207,  0.0229, -0.0020,  0.0092, -0.0101, -0.0006,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02333773672580719 tensor([[ 0.1054,  0.0999,  0.1138, -0.0088,  0.0372, -0.0364, -0.0039,  0.0302],\n",
            "        [ 0.0667,  0.0611,  0.0702, -0.0040,  0.0261, -0.0237, -0.0059,  0.0243],\n",
            "        [ 0.0845,  0.0752,  0.0869, -0.0079,  0.0322, -0.0290, -0.0064,  0.0258],\n",
            "        [ 0.0509,  0.0360,  0.0399, -0.0037,  0.0165, -0.0179, -0.0012,  0.0166]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.022343143820762634 tensor([[ 0.1323,  0.1243,  0.1424, -0.0121,  0.0463, -0.0449, -0.0047,  0.0348],\n",
            "        [ 0.0782,  0.0702,  0.0817, -0.0051,  0.0308, -0.0272, -0.0076,  0.0271],\n",
            "        [ 0.1080,  0.0948,  0.1102, -0.0110,  0.0414, -0.0362, -0.0088,  0.0306],\n",
            "        [ 0.0699,  0.0473,  0.0526, -0.0054,  0.0223, -0.0239, -0.0016,  0.0218]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02191280573606491 tensor([[ 0.1515,  0.1408,  0.1623, -0.0151,  0.0526, -0.0505, -0.0052,  0.0363],\n",
            "        [ 0.0838,  0.0735,  0.0867, -0.0059,  0.0332, -0.0282, -0.0090,  0.0274],\n",
            "        [ 0.1245,  0.1076,  0.1260, -0.0138,  0.0479, -0.0407, -0.0107,  0.0324],\n",
            "        [ 0.0858,  0.0556,  0.0621, -0.0069,  0.0271, -0.0287, -0.0019,  0.0257]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02170303836464882 tensor([[ 0.1659,  0.1526,  0.1768, -0.0179,  0.0572, -0.0543, -0.0055,  0.0359],\n",
            "        [ 0.0863,  0.0736,  0.0883, -0.0066,  0.0345, -0.0281, -0.0101,  0.0264],\n",
            "        [ 0.1365,  0.1159,  0.1368, -0.0163,  0.0527, -0.0435, -0.0125,  0.0322],\n",
            "        [ 0.0993,  0.0616,  0.0691, -0.0083,  0.0310, -0.0325, -0.0022,  0.0285]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018806204199790955 tensor([[ 0.0595,  0.0543,  0.0604, -0.0037,  0.0219, -0.0220, -0.0041,  0.0203],\n",
            "        [ 0.0678,  0.0612,  0.0698, -0.0048,  0.0237, -0.0246, -0.0023,  0.0229],\n",
            "        [ 0.0432,  0.0346,  0.0399, -0.0025,  0.0152, -0.0143, -0.0031,  0.0142],\n",
            "        [ 0.0276,  0.0223,  0.0239, -0.0015,  0.0095, -0.0106, -0.0010,  0.0092]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016007322818040848 tensor([[ 0.0982,  0.0882,  0.0988, -0.0066,  0.0363, -0.0357, -0.0070,  0.0312],\n",
            "        [ 0.1114,  0.0995,  0.1141, -0.0087,  0.0385, -0.0401, -0.0036,  0.0357],\n",
            "        [ 0.0717,  0.0552,  0.0643, -0.0042,  0.0248, -0.0228, -0.0055,  0.0217],\n",
            "        [ 0.0505,  0.0402,  0.0429, -0.0029,  0.0174, -0.0192, -0.0019,  0.0163]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01485577691346407 tensor([[ 0.1243,  0.1093,  0.1233, -0.0091,  0.0462, -0.0444, -0.0093,  0.0368],\n",
            "        [ 0.1385,  0.1219,  0.1407, -0.0119,  0.0473, -0.0494, -0.0043,  0.0418],\n",
            "        [ 0.0903,  0.0663,  0.0782, -0.0054,  0.0306, -0.0274, -0.0073,  0.0246],\n",
            "        [ 0.0697,  0.0549,  0.0585, -0.0042,  0.0239, -0.0263, -0.0026,  0.0216]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01437472552061081 tensor([[ 0.1429,  0.1228,  0.1394, -0.0113,  0.0533, -0.0502, -0.0111,  0.0393],\n",
            "        [ 0.1561,  0.1348,  0.1568, -0.0146,  0.0525, -0.0549, -0.0046,  0.0441],\n",
            "        [ 0.1029,  0.0717,  0.0858, -0.0062,  0.0341, -0.0295, -0.0086,  0.0248],\n",
            "        [ 0.0861,  0.0671,  0.0712, -0.0053,  0.0294, -0.0322, -0.0031,  0.0257]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014151671901345253 tensor([[ 0.1568,  0.1315,  0.1502, -0.0133,  0.0587, -0.0543, -0.0127,  0.0399],\n",
            "        [ 0.1680,  0.1421,  0.1667, -0.0172,  0.0557, -0.0582, -0.0046,  0.0440],\n",
            "        [ 0.1119,  0.0738,  0.0898, -0.0069,  0.0362, -0.0301, -0.0098,  0.0233],\n",
            "        [ 0.1002,  0.0772,  0.0817, -0.0063,  0.0342, -0.0372, -0.0035,  0.0287]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01928946003317833 tensor([[ 0.0793,  0.0733,  0.0832, -0.0055,  0.0298, -0.0273, -0.0040,  0.0239],\n",
            "        [ 0.0706,  0.0631,  0.0711, -0.0042,  0.0263, -0.0248, -0.0037,  0.0233],\n",
            "        [ 0.0540,  0.0473,  0.0527, -0.0038,  0.0198, -0.0193, -0.0030,  0.0196],\n",
            "        [ 0.0280,  0.0214,  0.0255, -0.0016,  0.0108, -0.0105, -0.0011,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015123012475669384 tensor([[ 0.1350,  0.1245,  0.1423, -0.0102,  0.0511, -0.0454, -0.0070,  0.0376],\n",
            "        [ 0.1218,  0.1087,  0.1232, -0.0076,  0.0460, -0.0419, -0.0067,  0.0377],\n",
            "        [ 0.0945,  0.0819,  0.0910, -0.0068,  0.0349, -0.0336, -0.0054,  0.0335],\n",
            "        [ 0.0515,  0.0384,  0.0464, -0.0029,  0.0200, -0.0190, -0.0019,  0.0169]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013033751398324966 tensor([[ 0.1718,  0.1566,  0.1806, -0.0141,  0.0654, -0.0561, -0.0090,  0.0437],\n",
            "        [ 0.1562,  0.1378,  0.1575, -0.0104,  0.0596, -0.0523, -0.0091,  0.0447],\n",
            "        [ 0.1240,  0.1058,  0.1172, -0.0092,  0.0461, -0.0436, -0.0074,  0.0427],\n",
            "        [ 0.0712,  0.0520,  0.0636, -0.0039,  0.0279, -0.0259, -0.0026,  0.0225]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.012097533792257309 tensor([[ 0.1961,  0.1758,  0.2046, -0.0175,  0.0750, -0.0621, -0.0105,  0.0449],\n",
            "        [ 0.1790,  0.1553,  0.1791, -0.0127,  0.0689, -0.0581, -0.0110,  0.0466],\n",
            "        [ 0.1456,  0.1220,  0.1350, -0.0111,  0.0543, -0.0505, -0.0091,  0.0484],\n",
            "        [ 0.0879,  0.0628,  0.0779, -0.0047,  0.0347, -0.0314, -0.0030,  0.0266]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.011677801609039307 tensor([[ 0.2125,  0.1871,  0.2196, -0.0205,  0.0819, -0.0652, -0.0116,  0.0432],\n",
            "        [ 0.1945,  0.1655,  0.1927, -0.0146,  0.0756, -0.0609, -0.0125,  0.0455],\n",
            "        [ 0.1618,  0.1330,  0.1469, -0.0127,  0.0606, -0.0552, -0.0105,  0.0518],\n",
            "        [ 0.1021,  0.0714,  0.0898, -0.0053,  0.0406, -0.0360, -0.0034,  0.0295]],\n",
            "       device='cuda:0')\n",
            "c tensor([3.2067e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022730305790901184 tensor([[ 0.0639,  0.0665,  0.0728, -0.0048,  0.0217, -0.0224, -0.0029,  0.0182],\n",
            "        [ 0.0564,  0.0475,  0.0528, -0.0031,  0.0209, -0.0200, -0.0037,  0.0201],\n",
            "        [ 0.0423,  0.0355,  0.0394, -0.0031,  0.0139, -0.0160, -0.0021,  0.0149],\n",
            "        [ 0.0270,  0.0216,  0.0252, -0.0016,  0.0099, -0.0081, -0.0012,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02003752812743187 tensor([[ 0.1062,  0.1120,  0.1228, -0.0090,  0.0355, -0.0364, -0.0048,  0.0272],\n",
            "        [ 0.0940,  0.0772,  0.0862, -0.0054,  0.0355, -0.0326, -0.0068,  0.0319],\n",
            "        [ 0.0731,  0.0603,  0.0665, -0.0058,  0.0234, -0.0278, -0.0036,  0.0248],\n",
            "        [ 0.0488,  0.0380,  0.0448, -0.0030,  0.0180, -0.0137, -0.0022,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01882701739668846 tensor([[ 0.1322,  0.1413,  0.1548, -0.0126,  0.0433, -0.0444, -0.0059,  0.0298],\n",
            "        [ 0.1187,  0.0944,  0.1058, -0.0071,  0.0454, -0.0401, -0.0093,  0.0380],\n",
            "        [ 0.0954,  0.0771,  0.0847, -0.0080,  0.0297, -0.0365, -0.0047,  0.0312],\n",
            "        [ 0.0665,  0.0504,  0.0601, -0.0041,  0.0246, -0.0173, -0.0032,  0.0141]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01830970123410225 tensor([[ 0.1478,  0.1600,  0.1750, -0.0158,  0.0473, -0.0486, -0.0066,  0.0285],\n",
            "        [ 0.1358,  0.1040,  0.1171, -0.0084,  0.0526, -0.0444, -0.0115,  0.0407],\n",
            "        [ 0.1118,  0.0884,  0.0965, -0.0099,  0.0337, -0.0428, -0.0055,  0.0351],\n",
            "        [ 0.0810,  0.0598,  0.0723, -0.0052,  0.0301, -0.0196, -0.0041,  0.0151]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01807820424437523 tensor([[ 0.1571,  0.1722,  0.1879, -0.0187,  0.0491, -0.0504, -0.0070,  0.0247],\n",
            "        [ 0.1484,  0.1090,  0.1235, -0.0095,  0.0581, -0.0467, -0.0135,  0.0413],\n",
            "        [ 0.1240,  0.0959,  0.1040, -0.0116,  0.0361, -0.0476, -0.0061,  0.0374],\n",
            "        [ 0.0933,  0.0669,  0.0821, -0.0062,  0.0348, -0.0207, -0.0049,  0.0149]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021130908280611038 tensor([[ 0.0548,  0.0434,  0.0482, -0.0041,  0.0202, -0.0200, -0.0033,  0.0190],\n",
            "        [ 0.0628,  0.0631,  0.0692, -0.0028,  0.0213, -0.0209, -0.0031,  0.0208],\n",
            "        [ 0.0462,  0.0412,  0.0464, -0.0037,  0.0171, -0.0162, -0.0026,  0.0158],\n",
            "        [ 0.0216,  0.0178,  0.0221, -0.0021,  0.0084, -0.0078, -0.0014,  0.0085]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018505122512578964 tensor([[ 0.0960,  0.0740,  0.0827, -0.0078,  0.0357, -0.0346, -0.0059,  0.0318],\n",
            "        [ 0.1065,  0.1082,  0.1183, -0.0048,  0.0356, -0.0343, -0.0055,  0.0334],\n",
            "        [ 0.0766,  0.0671,  0.0758, -0.0066,  0.0283, -0.0262, -0.0045,  0.0246],\n",
            "        [ 0.0392,  0.0316,  0.0401, -0.0042,  0.0152, -0.0136, -0.0027,  0.0149]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017238203436136246 tensor([[ 0.1274,  0.0950,  0.1070, -0.0112,  0.0477, -0.0453, -0.0079,  0.0403],\n",
            "        [ 0.1351,  0.1384,  0.1509, -0.0062,  0.0444, -0.0420, -0.0072,  0.0398],\n",
            "        [ 0.0961,  0.0825,  0.0935, -0.0090,  0.0356, -0.0318, -0.0060,  0.0285],\n",
            "        [ 0.0535,  0.0425,  0.0549, -0.0061,  0.0208, -0.0180, -0.0039,  0.0197]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016647204756736755 tensor([[ 0.1523,  0.1095,  0.1240, -0.0142,  0.0573, -0.0534, -0.0096,  0.0461],\n",
            "        [ 0.1536,  0.1585,  0.1722, -0.0072,  0.0494, -0.0458, -0.0084,  0.0420],\n",
            "        [ 0.1092,  0.0915,  0.1041, -0.0110,  0.0403, -0.0347, -0.0071,  0.0295],\n",
            "        [ 0.0654,  0.0512,  0.0673, -0.0080,  0.0255, -0.0213, -0.0051,  0.0234]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01635603979229927 tensor([[ 0.1725,  0.1193,  0.1359, -0.0171,  0.0652, -0.0598, -0.0111,  0.0499],\n",
            "        [ 0.1657,  0.1722,  0.1863, -0.0079,  0.0520, -0.0472, -0.0094,  0.0417],\n",
            "        [ 0.1182,  0.0966,  0.1105, -0.0128,  0.0436, -0.0359, -0.0080,  0.0286],\n",
            "        [ 0.0753,  0.0580,  0.0776, -0.0098,  0.0293, -0.0238, -0.0061,  0.0261]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021502742543816566 tensor([[ 0.0620,  0.0497,  0.0558, -0.0049,  0.0224, -0.0219, -0.0033,  0.0184],\n",
            "        [ 0.0697,  0.0671,  0.0739, -0.0035,  0.0246, -0.0240, -0.0037,  0.0236],\n",
            "        [ 0.0491,  0.0414,  0.0478, -0.0038,  0.0178, -0.0177, -0.0022,  0.0168],\n",
            "        [ 0.0244,  0.0222,  0.0250, -0.0025,  0.0096, -0.0090, -0.0014,  0.0091]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01836840622127056 tensor([[ 0.1078,  0.0841,  0.0952, -0.0092,  0.0391, -0.0372, -0.0058,  0.0293],\n",
            "        [ 0.1178,  0.1138,  0.1251, -0.0061,  0.0416, -0.0398, -0.0066,  0.0379],\n",
            "        [ 0.0834,  0.0689,  0.0799, -0.0069,  0.0300, -0.0299, -0.0036,  0.0270],\n",
            "        [ 0.0442,  0.0400,  0.0453, -0.0048,  0.0175, -0.0161, -0.0027,  0.0161]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01685693860054016 tensor([[ 0.1415,  0.1068,  0.1220, -0.0130,  0.0514, -0.0478, -0.0076,  0.0352],\n",
            "        [ 0.1491,  0.1440,  0.1581, -0.0080,  0.0524, -0.0492, -0.0087,  0.0450],\n",
            "        [ 0.1069,  0.0858,  0.1004, -0.0094,  0.0382, -0.0377, -0.0045,  0.0325],\n",
            "        [ 0.0604,  0.0546,  0.0619, -0.0071,  0.0241, -0.0216, -0.0039,  0.0214]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01616538316011429 tensor([[ 0.1673,  0.1215,  0.1400, -0.0164,  0.0608, -0.0553, -0.0089,  0.0378],\n",
            "        [ 0.1695,  0.1634,  0.1791, -0.0095,  0.0594, -0.0545, -0.0104,  0.0476],\n",
            "        [ 0.1234,  0.0961,  0.1136, -0.0117,  0.0436, -0.0428, -0.0050,  0.0347],\n",
            "        [ 0.0738,  0.0665,  0.0756, -0.0093,  0.0296, -0.0259, -0.0049,  0.0253]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015834562480449677 tensor([[ 0.1877,  0.1309,  0.1521, -0.0194,  0.0683, -0.0608, -0.0100,  0.0383],\n",
            "        [ 0.1831,  0.1759,  0.1926, -0.0107,  0.0639, -0.0572, -0.0117,  0.0474],\n",
            "        [ 0.1354,  0.1020,  0.1221, -0.0137,  0.0473, -0.0461, -0.0052,  0.0350],\n",
            "        [ 0.0849,  0.0763,  0.0870, -0.0114,  0.0343, -0.0293, -0.0058,  0.0283]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, 1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 6.5565e-07, -0.0000e+00,\n",
            "        5.9605e-08, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02237485907971859 tensor([[ 0.0710,  0.0636,  0.0700, -0.0042,  0.0249, -0.0254, -0.0037,  0.0244],\n",
            "        [ 0.0593,  0.0526,  0.0604, -0.0031,  0.0217, -0.0193, -0.0045,  0.0189],\n",
            "        [ 0.0579,  0.0526,  0.0585, -0.0033,  0.0207, -0.0205, -0.0025,  0.0191],\n",
            "        [ 0.0270,  0.0161,  0.0203, -0.0023,  0.0097, -0.0096, -0.0007,  0.0092]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019129127264022827 tensor([[ 0.1152,  0.1016,  0.1116, -0.0073,  0.0402, -0.0409, -0.0062,  0.0381],\n",
            "        [ 0.0946,  0.0819,  0.0951, -0.0050,  0.0347, -0.0291, -0.0078,  0.0269],\n",
            "        [ 0.0989,  0.0899,  0.1000, -0.0058,  0.0356, -0.0344, -0.0044,  0.0309],\n",
            "        [ 0.0504,  0.0282,  0.0365, -0.0044,  0.0181, -0.0176, -0.0012,  0.0165]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017842883244156837 tensor([[ 0.1425,  0.1232,  0.1347, -0.0095,  0.0495, -0.0502, -0.0080,  0.0454],\n",
            "        [ 0.1137,  0.0949,  0.1119, -0.0061,  0.0419, -0.0326, -0.0103,  0.0277],\n",
            "        [ 0.1268,  0.1150,  0.1281, -0.0077,  0.0458, -0.0432, -0.0057,  0.0369],\n",
            "        [ 0.0707,  0.0375,  0.0495, -0.0064,  0.0253, -0.0242, -0.0016,  0.0223]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017342546954751015 tensor([[ 0.1603,  0.1353,  0.1470, -0.0112,  0.0553, -0.0561, -0.0093,  0.0493],\n",
            "        [ 0.1240,  0.0990,  0.1190, -0.0067,  0.0458, -0.0326, -0.0122,  0.0247],\n",
            "        [ 0.1461,  0.1321,  0.1472, -0.0092,  0.0530, -0.0487, -0.0066,  0.0393],\n",
            "        [ 0.0887,  0.0445,  0.0602, -0.0082,  0.0316, -0.0297, -0.0019,  0.0270]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.0171219315379858 tensor([[ 0.1727,  0.1421,  0.1533, -0.0127,  0.0593, -0.0601, -0.0103,  0.0513],\n",
            "        [ 0.1299,  0.0986,  0.1210, -0.0071,  0.0481, -0.0308, -0.0139,  0.0198],\n",
            "        [ 0.1600,  0.1438,  0.1605, -0.0105,  0.0581, -0.0520, -0.0073,  0.0394],\n",
            "        [ 0.1047,  0.0496,  0.0688, -0.0099,  0.0372, -0.0344, -0.0021,  0.0307]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.8610e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02776467055082321 tensor([[ 0.0565,  0.0490,  0.0527, -0.0038,  0.0202, -0.0213, -0.0038,  0.0202],\n",
            "        [ 0.0758,  0.0736,  0.0806, -0.0043,  0.0256, -0.0260, -0.0031,  0.0244],\n",
            "        [ 0.0584,  0.0475,  0.0552, -0.0038,  0.0226, -0.0219, -0.0033,  0.0219],\n",
            "        [ 0.0270,  0.0225,  0.0241, -0.0019,  0.0092, -0.0110, -0.0014,  0.0107]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.024280063807964325 tensor([[ 0.0945,  0.0802,  0.0863, -0.0070,  0.0338, -0.0352, -0.0066,  0.0320],\n",
            "        [ 0.1256,  0.1224,  0.1336, -0.0076,  0.0413, -0.0420, -0.0052,  0.0377],\n",
            "        [ 0.0983,  0.0771,  0.0907, -0.0068,  0.0389, -0.0365, -0.0062,  0.0358],\n",
            "        [ 0.0498,  0.0411,  0.0434, -0.0036,  0.0165, -0.0204, -0.0025,  0.0195]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.022785063832998276 tensor([[ 0.1210,  0.0999,  0.1075, -0.0098,  0.0434, -0.0445, -0.0086,  0.0388],\n",
            "        [ 0.1552,  0.1512,  0.1642, -0.0101,  0.0495, -0.0502, -0.0063,  0.0427],\n",
            "        [ 0.1253,  0.0938,  0.1123, -0.0091,  0.0507, -0.0459, -0.0085,  0.0439],\n",
            "        [ 0.0690,  0.0564,  0.0590, -0.0050,  0.0224, -0.0284, -0.0034,  0.0265]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.022175362333655357 tensor([[ 0.1408,  0.1129,  0.1214, -0.0123,  0.0505, -0.0513, -0.0102,  0.0429],\n",
            "        [ 0.1725,  0.1678,  0.1812, -0.0122,  0.0530, -0.0539, -0.0068,  0.0429],\n",
            "        [ 0.1447,  0.1027,  0.1255, -0.0110,  0.0597, -0.0521, -0.0105,  0.0485],\n",
            "        [ 0.0853,  0.0691,  0.0717, -0.0064,  0.0272, -0.0352, -0.0042,  0.0321]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.021901793777942657 tensor([[ 0.1565,  0.1217,  0.1307, -0.0146,  0.0561, -0.0564, -0.0116,  0.0452],\n",
            "        [ 0.1828,  0.1773,  0.1902, -0.0139,  0.0538, -0.0550, -0.0070,  0.0404],\n",
            "        [ 0.1594,  0.1069,  0.1335, -0.0127,  0.0669, -0.0563, -0.0123,  0.0511],\n",
            "        [ 0.0992,  0.0796,  0.0820, -0.0077,  0.0311, -0.0409, -0.0048,  0.0366]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 7.7486e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02106493152678013 tensor([[ 0.0609,  0.0553,  0.0645, -0.0046,  0.0233, -0.0188, -0.0043,  0.0169],\n",
            "        [ 0.0575,  0.0517,  0.0568, -0.0034,  0.0200, -0.0212, -0.0030,  0.0196],\n",
            "        [ 0.0642,  0.0595,  0.0669, -0.0044,  0.0232, -0.0235, -0.0029,  0.0224],\n",
            "        [ 0.0270,  0.0159,  0.0193, -0.0018,  0.0094, -0.0088, -0.0014,  0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017839431762695312 tensor([[ 0.1038,  0.0933,  0.1104, -0.0087,  0.0401, -0.0305, -0.0077,  0.0258],\n",
            "        [ 0.0959,  0.0855,  0.0935, -0.0059,  0.0330, -0.0352, -0.0051,  0.0312],\n",
            "        [ 0.1093,  0.1010,  0.1137, -0.0082,  0.0394, -0.0399, -0.0050,  0.0366],\n",
            "        [ 0.0495,  0.0275,  0.0341, -0.0032,  0.0174, -0.0155, -0.0026,  0.0156]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01633174531161785 tensor([[ 0.1345,  0.1189,  0.1429, -0.0124,  0.0523, -0.0373, -0.0105,  0.0295],\n",
            "        [ 0.1207,  0.1062,  0.1155, -0.0077,  0.0411, -0.0440, -0.0066,  0.0371],\n",
            "        [ 0.1389,  0.1275,  0.1439, -0.0115,  0.0499, -0.0505, -0.0064,  0.0443],\n",
            "        [ 0.0684,  0.0357,  0.0453, -0.0044,  0.0242, -0.0206, -0.0036,  0.0205]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015660777688026428 tensor([[ 0.1571,  0.1364,  0.1664, -0.0158,  0.0616, -0.0411, -0.0128,  0.0299],\n",
            "        [ 0.1369,  0.1186,  0.1283, -0.0090,  0.0460, -0.0495, -0.0077,  0.0397],\n",
            "        [ 0.1579,  0.1439,  0.1628, -0.0143,  0.0566, -0.0571, -0.0073,  0.0476],\n",
            "        [ 0.0848,  0.0416,  0.0540, -0.0054,  0.0301, -0.0244, -0.0046,  0.0240]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015349836088716984 tensor([[ 0.1745,  0.1485,  0.1840, -0.0190,  0.0690, -0.0428, -0.0149,  0.0281],\n",
            "        [ 0.1479,  0.1259,  0.1353, -0.0100,  0.0490, -0.0529, -0.0085,  0.0402],\n",
            "        [ 0.1702,  0.1537,  0.1746, -0.0169,  0.0608, -0.0612, -0.0078,  0.0481],\n",
            "        [ 0.0991,  0.0456,  0.0607, -0.0064,  0.0353, -0.0273, -0.0055,  0.0266]],\n",
            "       device='cuda:0')\n",
            "c tensor([7.1526e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.027340594679117203 tensor([[ 0.0597,  0.0631,  0.0699, -0.0040,  0.0230, -0.0229, -0.0043,  0.0185],\n",
            "        [ 0.0750,  0.0629,  0.0712, -0.0043,  0.0269, -0.0261, -0.0036,  0.0254],\n",
            "        [ 0.0497,  0.0399,  0.0454, -0.0031,  0.0174, -0.0168, -0.0029,  0.0177],\n",
            "        [ 0.0231,  0.0154,  0.0189, -0.0016,  0.0065, -0.0070,  0.0007,  0.0053]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02402273379266262 tensor([[ 0.0990,  0.1068,  0.1188, -0.0074,  0.0386, -0.0376, -0.0077,  0.0278],\n",
            "        [ 0.1287,  0.1058,  0.1201, -0.0078,  0.0465, -0.0441, -0.0066,  0.0417],\n",
            "        [ 0.0851,  0.0665,  0.0759, -0.0055,  0.0298, -0.0284, -0.0053,  0.0294],\n",
            "        [ 0.0422,  0.0271,  0.0338, -0.0031,  0.0114, -0.0121,  0.0017,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.022495687007904053 tensor([[ 0.1230,  0.1354,  0.1511, -0.0103,  0.0486, -0.0463, -0.0104,  0.0307],\n",
            "        [ 0.1639,  0.1302,  0.1486, -0.0105,  0.0594, -0.0549, -0.0089,  0.0500],\n",
            "        [ 0.1089,  0.0821,  0.0941, -0.0074,  0.0380, -0.0356, -0.0072,  0.0359],\n",
            "        [ 0.0585,  0.0361,  0.0459, -0.0044,  0.0153, -0.0159,  0.0029,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.021888909861445427 tensor([[ 0.1371,  0.1537,  0.1722, -0.0129,  0.0550, -0.0513, -0.0125,  0.0299],\n",
            "        [ 0.1869,  0.1423,  0.1634, -0.0126,  0.0680, -0.0610, -0.0107,  0.0533],\n",
            "        [ 0.1250,  0.0904,  0.1043, -0.0088,  0.0435, -0.0398, -0.0087,  0.0392],\n",
            "        [ 0.0726,  0.0432,  0.0560, -0.0055,  0.0182, -0.0187,  0.0042,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02164698764681816 tensor([[ 0.1449,  0.1658,  0.1861, -0.0151,  0.0590, -0.0539, -0.0144,  0.0267],\n",
            "        [ 0.2027,  0.1470,  0.1701, -0.0144,  0.0741, -0.0644, -0.0123,  0.0536],\n",
            "        [ 0.1367,  0.0945,  0.1097, -0.0101,  0.0472, -0.0423, -0.0100,  0.0404],\n",
            "        [ 0.0850,  0.0488,  0.0644, -0.0066,  0.0205, -0.0207,  0.0057,  0.0114]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.030160978436470032 tensor([[ 0.0460,  0.0478,  0.0489, -0.0034,  0.0167, -0.0197, -0.0039,  0.0169],\n",
            "        [ 0.0652,  0.0588,  0.0663, -0.0051,  0.0225, -0.0227, -0.0036,  0.0240],\n",
            "        [ 0.0369,  0.0297,  0.0365, -0.0034,  0.0161, -0.0119, -0.0053,  0.0135],\n",
            "        [ 0.0307,  0.0221,  0.0250, -0.0019,  0.0096, -0.0119,  0.0004,  0.0079]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02786608599126339 tensor([[ 0.0745,  0.0784,  0.0791, -0.0060,  0.0270, -0.0326, -0.0068,  0.0265],\n",
            "        [ 0.1061,  0.0938,  0.1059, -0.0094,  0.0359, -0.0366, -0.0062,  0.0382],\n",
            "        [ 0.0604,  0.0457,  0.0583, -0.0062,  0.0278, -0.0184, -0.0102,  0.0210],\n",
            "        [ 0.0565,  0.0394,  0.0449, -0.0035,  0.0174, -0.0217,  0.0010,  0.0135]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02695271372795105 tensor([[ 0.0931,  0.0992,  0.0984, -0.0082,  0.0336, -0.0416, -0.0091,  0.0322],\n",
            "        [ 0.1307,  0.1123,  0.1271, -0.0131,  0.0431, -0.0444, -0.0080,  0.0458],\n",
            "        [ 0.0758,  0.0532,  0.0711, -0.0086,  0.0367, -0.0216, -0.0148,  0.0245],\n",
            "        [ 0.0785,  0.0532,  0.0610, -0.0050,  0.0238, -0.0298,  0.0018,  0.0173]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.026574421674013138 tensor([[ 0.1059,  0.1140,  0.1112, -0.0100,  0.0381, -0.0483, -0.0112,  0.0357],\n",
            "        [ 0.1457,  0.1213,  0.1376, -0.0164,  0.0468, -0.0487, -0.0094,  0.0496],\n",
            "        [ 0.0863,  0.0558,  0.0787, -0.0107,  0.0438, -0.0227, -0.0193,  0.0259],\n",
            "        [ 0.0974,  0.0642,  0.0739, -0.0063,  0.0291, -0.0366,  0.0027,  0.0196]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.026394158601760864 tensor([[ 0.1150,  0.1253,  0.1199, -0.0116,  0.0413, -0.0536, -0.0131,  0.0378],\n",
            "        [ 0.1554,  0.1250,  0.1421, -0.0195,  0.0486, -0.0511, -0.0105,  0.0513],\n",
            "        [ 0.0942,  0.0556,  0.0832, -0.0127,  0.0500, -0.0226, -0.0236,  0.0260],\n",
            "        [ 0.1139,  0.0729,  0.0843, -0.0076,  0.0335, -0.0425,  0.0037,  0.0209]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.016251305118203163 tensor([[ 0.0673,  0.0629,  0.0684, -0.0038,  0.0237, -0.0243, -0.0034,  0.0232],\n",
            "        [ 0.0676,  0.0609,  0.0712, -0.0053,  0.0259, -0.0233, -0.0034,  0.0221],\n",
            "        [ 0.0499,  0.0424,  0.0461, -0.0037,  0.0180, -0.0189, -0.0029,  0.0193],\n",
            "        [ 0.0284,  0.0254,  0.0282, -0.0014,  0.0100, -0.0096, -0.0014,  0.0087]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.012923885136842728 tensor([[ 0.1105,  0.1025,  0.1116, -0.0066,  0.0387, -0.0392, -0.0056,  0.0357],\n",
            "        [ 0.1113,  0.0994,  0.1180, -0.0098,  0.0432, -0.0374, -0.0057,  0.0337],\n",
            "        [ 0.0854,  0.0711,  0.0767, -0.0068,  0.0308, -0.0325, -0.0051,  0.0325],\n",
            "        [ 0.0512,  0.0457,  0.0507, -0.0025,  0.0180, -0.0167, -0.0025,  0.0147]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.011503053829073906 tensor([[ 0.1387,  0.1269,  0.1380, -0.0088,  0.0482, -0.0482, -0.0070,  0.0418],\n",
            "        [ 0.1385,  0.1215,  0.1470, -0.0136,  0.0542, -0.0450, -0.0072,  0.0380],\n",
            "        [ 0.1108,  0.0901,  0.0966, -0.0095,  0.0398, -0.0420, -0.0069,  0.0413],\n",
            "        [ 0.0696,  0.0621,  0.0689, -0.0035,  0.0243, -0.0218, -0.0036,  0.0185]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.010896651074290276 tensor([[ 0.1580,  0.1422,  0.1545, -0.0106,  0.0546, -0.0538, -0.0079,  0.0443],\n",
            "        [ 0.1559,  0.1340,  0.1654, -0.0170,  0.0616, -0.0488, -0.0080,  0.0382],\n",
            "        [ 0.1295,  0.1028,  0.1094, -0.0119,  0.0464, -0.0490, -0.0083,  0.0473],\n",
            "        [ 0.0847,  0.0753,  0.0837, -0.0042,  0.0294, -0.0255, -0.0044,  0.0206]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01061263121664524 tensor([[ 0.1721,  0.1521,  0.1650, -0.0121,  0.0591, -0.0575, -0.0086,  0.0446],\n",
            "        [ 0.1676,  0.1408,  0.1776, -0.0201,  0.0668, -0.0502, -0.0086,  0.0359],\n",
            "        [ 0.1438,  0.1113,  0.1175, -0.0141,  0.0512, -0.0542, -0.0095,  0.0515],\n",
            "        [ 0.0970,  0.0861,  0.0958, -0.0049,  0.0335, -0.0280, -0.0052,  0.0214]],\n",
            "       device='cuda:0')\n",
            "c tensor([9.5367e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018906351178884506 tensor([[ 0.0493,  0.0422,  0.0459, -0.0033,  0.0177, -0.0184, -0.0025,  0.0161],\n",
            "        [ 0.0632,  0.0629,  0.0692, -0.0039,  0.0221, -0.0218, -0.0038,  0.0227],\n",
            "        [ 0.0589,  0.0532,  0.0596, -0.0043,  0.0204, -0.0216, -0.0019,  0.0226],\n",
            "        [ 0.0225,  0.0133,  0.0168, -0.0024,  0.0097, -0.0072, -0.0022,  0.0088]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016092246398329735 tensor([[ 0.0837,  0.0700,  0.0765, -0.0061,  0.0301, -0.0307, -0.0041,  0.0255],\n",
            "        [ 0.1030,  0.1030,  0.1133, -0.0070,  0.0358, -0.0345, -0.0065,  0.0353],\n",
            "        [ 0.0982,  0.0879,  0.0979, -0.0077,  0.0334, -0.0358, -0.0031,  0.0372],\n",
            "        [ 0.0402,  0.0214,  0.0281, -0.0047,  0.0178, -0.0123, -0.0043,  0.0152]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.014907912351191044 tensor([[ 0.1088,  0.0886,  0.0971, -0.0084,  0.0392, -0.0393, -0.0053,  0.0310],\n",
            "        [ 0.1283,  0.1285,  0.1412, -0.0095,  0.0440, -0.0416, -0.0086,  0.0417],\n",
            "        [ 0.1232,  0.1089,  0.1205, -0.0105,  0.0409, -0.0446, -0.0037,  0.0459],\n",
            "        [ 0.0543,  0.0260,  0.0356, -0.0068,  0.0246, -0.0157, -0.0063,  0.0197]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01441235188394785 tensor([[ 0.1279,  0.1013,  0.1113, -0.0105,  0.0461, -0.0456, -0.0061,  0.0339],\n",
            "        [ 0.1449,  0.1452,  0.1596, -0.0117,  0.0490, -0.0452, -0.0101,  0.0444],\n",
            "        [ 0.1391,  0.1210,  0.1332, -0.0129,  0.0451, -0.0501, -0.0040,  0.0510],\n",
            "        [ 0.0660,  0.0281,  0.0406, -0.0089,  0.0304, -0.0181, -0.0081,  0.0230]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014190038666129112 tensor([[ 0.1431,  0.1101,  0.1211, -0.0124,  0.0516, -0.0504, -0.0068,  0.0354],\n",
            "        [ 0.1561,  0.1565,  0.1719, -0.0137,  0.0520, -0.0467, -0.0115,  0.0449],\n",
            "        [ 0.1495,  0.1278,  0.1398, -0.0151,  0.0472, -0.0534, -0.0040,  0.0538],\n",
            "        [ 0.0760,  0.0285,  0.0437, -0.0109,  0.0357, -0.0198, -0.0099,  0.0254]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7941e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02369895949959755 tensor([[ 0.0685,  0.0679,  0.0739, -0.0043,  0.0240, -0.0261, -0.0023,  0.0202],\n",
            "        [ 0.0651,  0.0547,  0.0608, -0.0040,  0.0242, -0.0243, -0.0031,  0.0223],\n",
            "        [ 0.0551,  0.0478,  0.0545, -0.0035,  0.0196, -0.0190, -0.0035,  0.0198],\n",
            "        [ 0.0273,  0.0236,  0.0275, -0.0016,  0.0096, -0.0105, -0.0008,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020241186022758484 tensor([[ 0.1138,  0.1133,  0.1235, -0.0079,  0.0394, -0.0430, -0.0036,  0.0302],\n",
            "        [ 0.1115,  0.0918,  0.1018, -0.0070,  0.0417, -0.0415, -0.0055,  0.0366],\n",
            "        [ 0.0939,  0.0800,  0.0916, -0.0062,  0.0336, -0.0316, -0.0065,  0.0324],\n",
            "        [ 0.0491,  0.0420,  0.0493, -0.0030,  0.0171, -0.0186, -0.0014,  0.0160]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01864929124712944 tensor([[ 0.1431,  0.1429,  0.1558, -0.0109,  0.0491, -0.0538, -0.0040,  0.0337],\n",
            "        [ 0.1437,  0.1148,  0.1270, -0.0095,  0.0538, -0.0533, -0.0071,  0.0449],\n",
            "        [ 0.1206,  0.1003,  0.1158, -0.0085,  0.0433, -0.0392, -0.0091,  0.0395],\n",
            "        [ 0.0667,  0.0563,  0.0667, -0.0042,  0.0230, -0.0248, -0.0018,  0.0205]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01794762909412384 tensor([[ 0.1625,  0.1624,  0.1770, -0.0135,  0.0552, -0.0608, -0.0040,  0.0332],\n",
            "        [ 0.1665,  0.1283,  0.1417, -0.0115,  0.0624, -0.0613, -0.0083,  0.0494],\n",
            "        [ 0.1394,  0.1128,  0.1315, -0.0104,  0.0501, -0.0436, -0.0114,  0.0432],\n",
            "        [ 0.0809,  0.0676,  0.0810, -0.0053,  0.0276, -0.0295, -0.0021,  0.0235]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.0176253579556942 tensor([[ 0.1757,  0.1756,  0.1912, -0.0158,  0.0589, -0.0654, -0.0037,  0.0302],\n",
            "        [ 0.1832,  0.1358,  0.1497, -0.0132,  0.0688, -0.0671, -0.0092,  0.0514],\n",
            "        [ 0.1530,  0.1203,  0.1416, -0.0120,  0.0552, -0.0459, -0.0134,  0.0446],\n",
            "        [ 0.0926,  0.0765,  0.0926, -0.0063,  0.0313, -0.0331, -0.0022,  0.0252]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018987415358424187 tensor([[ 0.0594,  0.0556,  0.0620, -0.0034,  0.0218, -0.0202, -0.0041,  0.0213],\n",
            "        [ 0.0513,  0.0473,  0.0521, -0.0042,  0.0186, -0.0190, -0.0033,  0.0167],\n",
            "        [ 0.0507,  0.0450,  0.0499, -0.0028,  0.0185, -0.0183, -0.0023,  0.0186],\n",
            "        [ 0.0272,  0.0170,  0.0208, -0.0020,  0.0089, -0.0107, -0.0006,  0.0106]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01643604226410389 tensor([[ 0.1003,  0.0932,  0.1043, -0.0062,  0.0368, -0.0333, -0.0072,  0.0346],\n",
            "        [ 0.0853,  0.0784,  0.0863, -0.0076,  0.0310, -0.0311, -0.0058,  0.0258],\n",
            "        [ 0.0850,  0.0745,  0.0825, -0.0050,  0.0311, -0.0304, -0.0041,  0.0302],\n",
            "        [ 0.0511,  0.0308,  0.0382, -0.0038,  0.0163, -0.0201, -0.0010,  0.0197]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015264244750142097 tensor([[ 0.1284,  0.1182,  0.1327, -0.0084,  0.0471, -0.0415, -0.0097,  0.0427],\n",
            "        [ 0.1072,  0.0981,  0.1079, -0.0104,  0.0390, -0.0384, -0.0078,  0.0297],\n",
            "        [ 0.1085,  0.0936,  0.1036, -0.0068,  0.0398, -0.0383, -0.0053,  0.0373],\n",
            "        [ 0.0720,  0.0421,  0.0527, -0.0055,  0.0226, -0.0282, -0.0012,  0.0275]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014716057106852531 tensor([[ 0.1484,  0.1348,  0.1519, -0.0102,  0.0545, -0.0466, -0.0118,  0.0475],\n",
            "        [ 0.1217,  0.1105,  0.1215, -0.0128,  0.0443, -0.0427, -0.0093,  0.0304],\n",
            "        [ 0.1254,  0.1063,  0.1177, -0.0083,  0.0461, -0.0435, -0.0063,  0.0414],\n",
            "        [ 0.0905,  0.0511,  0.0647, -0.0071,  0.0279, -0.0353, -0.0013,  0.0341]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014442161656916142 tensor([[ 0.1631,  0.1460,  0.1649, -0.0119,  0.0599, -0.0497, -0.0136,  0.0503],\n",
            "        [ 0.1314,  0.1184,  0.1300, -0.0149,  0.0477, -0.0450, -0.0106,  0.0289],\n",
            "        [ 0.1380,  0.1149,  0.1273, -0.0097,  0.0508, -0.0469, -0.0071,  0.0437],\n",
            "        [ 0.1067,  0.0582,  0.0747, -0.0086,  0.0324, -0.0416, -0.0012,  0.0397]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02075652778148651 tensor([[ 0.0453,  0.0484,  0.0525, -0.0036,  0.0163, -0.0187, -0.0038,  0.0167],\n",
            "        [ 0.0414,  0.0320,  0.0379, -0.0033,  0.0151, -0.0138, -0.0028,  0.0122],\n",
            "        [ 0.0441,  0.0371,  0.0445, -0.0041,  0.0155, -0.0146, -0.0019,  0.0126],\n",
            "        [ 0.0211,  0.0265,  0.0266, -0.0016,  0.0068, -0.0080, -0.0015,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019044864922761917 tensor([[ 0.0737,  0.0801,  0.0865, -0.0066,  0.0265, -0.0309, -0.0067,  0.0263],\n",
            "        [ 0.0664,  0.0481,  0.0581, -0.0059,  0.0243, -0.0211, -0.0050,  0.0174],\n",
            "        [ 0.0743,  0.0609,  0.0743, -0.0076,  0.0260, -0.0238, -0.0030,  0.0190],\n",
            "        [ 0.0371,  0.0485,  0.0482, -0.0030,  0.0116, -0.0139, -0.0028,  0.0148]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018321126699447632 tensor([[ 0.0918,  0.1014,  0.1091, -0.0092,  0.0328, -0.0393, -0.0091,  0.0318],\n",
            "        [ 0.0816,  0.0546,  0.0676, -0.0079,  0.0298, -0.0246, -0.0067,  0.0186],\n",
            "        [ 0.0956,  0.0760,  0.0945, -0.0107,  0.0331, -0.0294, -0.0037,  0.0214],\n",
            "        [ 0.0491,  0.0668,  0.0658, -0.0044,  0.0149, -0.0182, -0.0039,  0.0190]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017991535365581512 tensor([[ 0.1035,  0.1163,  0.1245, -0.0115,  0.0369, -0.0452, -0.0112,  0.0347],\n",
            "        [ 0.0914,  0.0557,  0.0712, -0.0097,  0.0334, -0.0259, -0.0082,  0.0176],\n",
            "        [ 0.1115,  0.0857,  0.1087, -0.0137,  0.0381, -0.0328, -0.0041,  0.0213],\n",
            "        [ 0.0579,  0.0823,  0.0803, -0.0057,  0.0169, -0.0211, -0.0049,  0.0218]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017823047935962677 tensor([[ 0.1112,  0.1270,  0.1353, -0.0136,  0.0395, -0.0496, -0.0131,  0.0360],\n",
            "        [ 0.0982,  0.0540,  0.0714, -0.0112,  0.0358, -0.0261, -0.0096,  0.0155],\n",
            "        [ 0.1239,  0.0919,  0.1191, -0.0164,  0.0419, -0.0348, -0.0043,  0.0197],\n",
            "        [ 0.0642,  0.0954,  0.0923, -0.0069,  0.0181, -0.0230, -0.0059,  0.0235]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023857301101088524 tensor([[ 0.0771,  0.0653,  0.0726, -0.0044,  0.0283, -0.0286, -0.0047,  0.0270],\n",
            "        [ 0.0480,  0.0364,  0.0434, -0.0039,  0.0175, -0.0176, -0.0023,  0.0158],\n",
            "        [ 0.0451,  0.0406,  0.0469, -0.0034,  0.0172, -0.0155, -0.0020,  0.0121],\n",
            "        [ 0.0280,  0.0212,  0.0273, -0.0026,  0.0107, -0.0088, -0.0014,  0.0089]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021071940660476685 tensor([[ 0.1297,  0.1067,  0.1190, -0.0079,  0.0477, -0.0477, -0.0081,  0.0434],\n",
            "        [ 0.0808,  0.0586,  0.0712, -0.0070,  0.0297, -0.0293, -0.0040,  0.0250],\n",
            "        [ 0.0773,  0.0694,  0.0806, -0.0063,  0.0299, -0.0261, -0.0036,  0.0186],\n",
            "        [ 0.0516,  0.0379,  0.0499, -0.0049,  0.0197, -0.0155, -0.0026,  0.0155]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01980024389922619 tensor([[ 0.1659,  0.1315,  0.1472, -0.0107,  0.0610, -0.0605, -0.0107,  0.0529],\n",
            "        [ 0.1030,  0.0706,  0.0879, -0.0098,  0.0380, -0.0367, -0.0053,  0.0297],\n",
            "        [ 0.1004,  0.0894,  0.1046, -0.0088,  0.0392, -0.0330, -0.0047,  0.0211],\n",
            "        [ 0.0714,  0.0509,  0.0687, -0.0071,  0.0274, -0.0205, -0.0037,  0.0203]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019214048981666565 tensor([[ 0.1921,  0.1462,  0.1640, -0.0131,  0.0708, -0.0696, -0.0128,  0.0582],\n",
            "        [ 0.1186,  0.0760,  0.0973, -0.0121,  0.0440, -0.0415, -0.0062,  0.0316],\n",
            "        [ 0.1170,  0.1032,  0.1219, -0.0111,  0.0463, -0.0375, -0.0056,  0.0208],\n",
            "        [ 0.0882,  0.0610,  0.0844, -0.0091,  0.0340, -0.0242, -0.0047,  0.0236]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018923349678516388 tensor([[ 0.2120,  0.1544,  0.1737, -0.0152,  0.0782, -0.0763, -0.0145,  0.0610],\n",
            "        [ 0.1300,  0.0773,  0.1022, -0.0143,  0.0484, -0.0445, -0.0070,  0.0318],\n",
            "        [ 0.1293,  0.1129,  0.1346, -0.0131,  0.0518, -0.0402, -0.0063,  0.0186],\n",
            "        [ 0.1025,  0.0687,  0.0975, -0.0111,  0.0396, -0.0269, -0.0057,  0.0257]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02185012213885784 tensor([[ 0.0752,  0.0699,  0.0751, -0.0046,  0.0261, -0.0271, -0.0042,  0.0242],\n",
            "        [ 0.0616,  0.0566,  0.0634, -0.0031,  0.0225, -0.0227, -0.0039,  0.0227],\n",
            "        [ 0.0554,  0.0489,  0.0547, -0.0037,  0.0199, -0.0206, -0.0027,  0.0187],\n",
            "        [ 0.0264,  0.0239,  0.0263, -0.0032,  0.0106, -0.0105, -0.0026,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018235964700579643 tensor([[ 0.1270,  0.1171,  0.1257, -0.0084,  0.0436, -0.0451, -0.0074,  0.0382],\n",
            "        [ 0.1022,  0.0932,  0.1049, -0.0052,  0.0374, -0.0371, -0.0070,  0.0362],\n",
            "        [ 0.0955,  0.0832,  0.0931, -0.0065,  0.0342, -0.0354, -0.0046,  0.0307],\n",
            "        [ 0.0474,  0.0427,  0.0472, -0.0061,  0.0195, -0.0185, -0.0052,  0.0163]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016585318371653557 tensor([[ 0.1624,  0.1478,  0.1582, -0.0115,  0.0552, -0.0568, -0.0097,  0.0453],\n",
            "        [ 0.1270,  0.1144,  0.1294, -0.0066,  0.0466, -0.0454, -0.0092,  0.0429],\n",
            "        [ 0.1241,  0.1065,  0.1190, -0.0087,  0.0443, -0.0458, -0.0061,  0.0378],\n",
            "        [ 0.0641,  0.0576,  0.0640, -0.0089,  0.0268, -0.0247, -0.0076,  0.0210]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015861615538597107 tensor([[ 0.1875,  0.1681,  0.1790, -0.0142,  0.0631, -0.0647, -0.0115,  0.0483],\n",
            "        [ 0.1423,  0.1260,  0.1435, -0.0076,  0.0522, -0.0498, -0.0110,  0.0455],\n",
            "        [ 0.1448,  0.1221,  0.1365, -0.0104,  0.0513, -0.0530, -0.0072,  0.0416],\n",
            "        [ 0.0775,  0.0694,  0.0775, -0.0115,  0.0329, -0.0293, -0.0100,  0.0241]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015529504045844078 tensor([[ 0.2062,  0.1818,  0.1924, -0.0165,  0.0686, -0.0701, -0.0130,  0.0489],\n",
            "        [ 0.1519,  0.1318,  0.1514, -0.0084,  0.0558, -0.0519, -0.0124,  0.0457],\n",
            "        [ 0.1603,  0.1326,  0.1481, -0.0118,  0.0564, -0.0582, -0.0080,  0.0431],\n",
            "        [ 0.0881,  0.0788,  0.0884, -0.0139,  0.0381, -0.0328, -0.0122,  0.0258]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022178739309310913 tensor([[ 0.0476,  0.0388,  0.0439, -0.0031,  0.0159, -0.0172, -0.0019,  0.0154],\n",
            "        [ 0.0521,  0.0481,  0.0540, -0.0038,  0.0190, -0.0187, -0.0028,  0.0179],\n",
            "        [ 0.0518,  0.0458,  0.0547, -0.0032,  0.0196, -0.0161, -0.0035,  0.0161],\n",
            "        [ 0.0295,  0.0249,  0.0287, -0.0033,  0.0113, -0.0110, -0.0017,  0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01991080678999424 tensor([[ 0.0823,  0.0649,  0.0741, -0.0057,  0.0270, -0.0292, -0.0030,  0.0251],\n",
            "        [ 0.0862,  0.0789,  0.0887, -0.0070,  0.0316, -0.0306, -0.0049,  0.0283],\n",
            "        [ 0.0876,  0.0760,  0.0923, -0.0058,  0.0335, -0.0259, -0.0063,  0.0251],\n",
            "        [ 0.0538,  0.0450,  0.0522, -0.0064,  0.0207, -0.0198, -0.0031,  0.0155]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018846649676561356 tensor([[ 0.1084,  0.0824,  0.0950, -0.0081,  0.0350, -0.0377, -0.0037,  0.0313],\n",
            "        [ 0.1076,  0.0973,  0.1096, -0.0096,  0.0396, -0.0376, -0.0065,  0.0338],\n",
            "        [ 0.1122,  0.0948,  0.1173, -0.0078,  0.0431, -0.0312, -0.0085,  0.0289],\n",
            "        [ 0.0739,  0.0611,  0.0716, -0.0094,  0.0286, -0.0267, -0.0045,  0.0199]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018347924575209618 tensor([[ 0.1290,  0.0941,  0.1095, -0.0102,  0.0410, -0.0441, -0.0041,  0.0352],\n",
            "        [ 0.1209,  0.1077,  0.1216, -0.0119,  0.0448, -0.0416, -0.0078,  0.0360],\n",
            "        [ 0.1293,  0.1062,  0.1342, -0.0095,  0.0500, -0.0334, -0.0104,  0.0294],\n",
            "        [ 0.0907,  0.0740,  0.0875, -0.0122,  0.0353, -0.0322, -0.0057,  0.0226]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01809501275420189 tensor([[ 0.1456,  0.1019,  0.1198, -0.0122,  0.0455, -0.0489, -0.0043,  0.0376],\n",
            "        [ 0.1293,  0.1132,  0.1281, -0.0140,  0.0481, -0.0437, -0.0089,  0.0364],\n",
            "        [ 0.1418,  0.1128,  0.1460, -0.0110,  0.0552, -0.0338, -0.0120,  0.0278],\n",
            "        [ 0.1047,  0.0845,  0.1007, -0.0149,  0.0410, -0.0365, -0.0068,  0.0241]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023421039804816246 tensor([[ 0.0644,  0.0610,  0.0670, -0.0045,  0.0234, -0.0235, -0.0033,  0.0221],\n",
            "        [ 0.0597,  0.0527,  0.0590, -0.0036,  0.0210, -0.0213, -0.0035,  0.0206],\n",
            "        [ 0.0572,  0.0506,  0.0577, -0.0033,  0.0205, -0.0208, -0.0026,  0.0199],\n",
            "        [ 0.0261,  0.0188,  0.0215, -0.0020,  0.0088, -0.0103, -0.0010,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02032017707824707 tensor([[ 0.1061,  0.1002,  0.1102, -0.0081,  0.0387, -0.0381, -0.0056,  0.0344],\n",
            "        [ 0.0985,  0.0854,  0.0956, -0.0064,  0.0343, -0.0344, -0.0063,  0.0321],\n",
            "        [ 0.0957,  0.0837,  0.0961, -0.0057,  0.0344, -0.0342, -0.0046,  0.0317],\n",
            "        [ 0.0483,  0.0338,  0.0387, -0.0039,  0.0160, -0.0192, -0.0018,  0.0185]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019011855125427246 tensor([[ 0.1337,  0.1251,  0.1378, -0.0112,  0.0488, -0.0473, -0.0072,  0.0408],\n",
            "        [ 0.1229,  0.1038,  0.1164, -0.0086,  0.0422, -0.0420, -0.0084,  0.0375],\n",
            "        [ 0.1211,  0.1041,  0.1203, -0.0074,  0.0435, -0.0423, -0.0060,  0.0377],\n",
            "        [ 0.0675,  0.0457,  0.0525, -0.0058,  0.0220, -0.0268, -0.0025,  0.0253]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018465187400579453 tensor([[ 0.1529,  0.1415,  0.1560, -0.0139,  0.0560, -0.0532, -0.0084,  0.0439],\n",
            "        [ 0.1385,  0.1135,  0.1277, -0.0105,  0.0469, -0.0460, -0.0102,  0.0393],\n",
            "        [ 0.1382,  0.1164,  0.1358, -0.0088,  0.0496, -0.0471, -0.0071,  0.0402],\n",
            "        [ 0.0840,  0.0553,  0.0638, -0.0076,  0.0269, -0.0333, -0.0030,  0.0309]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018216555938124657 tensor([[ 0.1669,  0.1525,  0.1682, -0.0163,  0.0612, -0.0572, -0.0094,  0.0449],\n",
            "        [ 0.1490,  0.1182,  0.1333, -0.0121,  0.0497, -0.0481, -0.0117,  0.0390],\n",
            "        [ 0.1504,  0.1239,  0.1460, -0.0099,  0.0538, -0.0498, -0.0080,  0.0404],\n",
            "        [ 0.0984,  0.0629,  0.0728, -0.0093,  0.0310, -0.0390, -0.0034,  0.0356]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023919569328427315 tensor([[ 0.0567,  0.0563,  0.0619, -0.0045,  0.0204, -0.0200, -0.0040,  0.0180],\n",
            "        [ 0.0486,  0.0447,  0.0527, -0.0042,  0.0196, -0.0154, -0.0044,  0.0163],\n",
            "        [ 0.0449,  0.0384,  0.0419, -0.0027,  0.0147, -0.0171, -0.0018,  0.0148],\n",
            "        [ 0.0305,  0.0296,  0.0302, -0.0016,  0.0097, -0.0135, -0.0015,  0.0118]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02156205289065838 tensor([[ 0.0919,  0.0915,  0.1005, -0.0082,  0.0332, -0.0318, -0.0072,  0.0270],\n",
            "        [ 0.0788,  0.0712,  0.0856, -0.0077,  0.0325, -0.0235, -0.0080,  0.0242],\n",
            "        [ 0.0754,  0.0631,  0.0686, -0.0048,  0.0242, -0.0284, -0.0029,  0.0236],\n",
            "        [ 0.0556,  0.0544,  0.0548, -0.0029,  0.0171, -0.0247, -0.0027,  0.0209]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020563041791319847 tensor([[ 0.1139,  0.1133,  0.1242, -0.0113,  0.0411, -0.0387, -0.0097,  0.0306],\n",
            "        [ 0.0968,  0.0855,  0.1053, -0.0108,  0.0410, -0.0266, -0.0110,  0.0266],\n",
            "        [ 0.0973,  0.0795,  0.0862, -0.0066,  0.0305, -0.0364, -0.0036,  0.0286],\n",
            "        [ 0.0762,  0.0753,  0.0750, -0.0039,  0.0226, -0.0339, -0.0036,  0.0278]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020114410668611526 tensor([[ 0.1281,  0.1271,  0.1390, -0.0140,  0.0463, -0.0426, -0.0119,  0.0311],\n",
            "        [ 0.1078,  0.0926,  0.1172, -0.0137,  0.0469, -0.0269, -0.0136,  0.0259],\n",
            "        [ 0.1139,  0.0909,  0.0983, -0.0081,  0.0349, -0.0421, -0.0039,  0.0314],\n",
            "        [ 0.0930,  0.0928,  0.0915, -0.0048,  0.0268, -0.0417, -0.0043,  0.0329]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019888438284397125 tensor([[ 0.1377,  0.1361,  0.1485, -0.0165,  0.0497, -0.0449, -0.0140,  0.0298],\n",
            "        [ 0.1146,  0.0954,  0.1245, -0.0163,  0.0512, -0.0255, -0.0160,  0.0233],\n",
            "        [ 0.1271,  0.0989,  0.1067, -0.0096,  0.0380, -0.0464, -0.0041,  0.0327],\n",
            "        [ 0.1067,  0.1077,  0.1049, -0.0056,  0.0298, -0.0481, -0.0049,  0.0365]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[4, 7, 9, 7, 4, 1, 8, 6, 4, 11, 6, 0, 6, 5, 2, 6, 4, 6, 2, 6, 6, 11, 6, 11, 6, 5, 11, 5, 4, 12, 4, 4, 4, 6, 12, 6, 4, 2, 1, 4, 6, 4, 11, 5, 4, 6, 11, 4, 4, 4, 6, 6, 4, 4, 2, 6, 4, 4, 12, 6, 6, 5, 4, 6, 4, 13, 1, 4, 4, 4, 6, 9, 4, 4, 2, 2, 6, 2, 6, 6, 4, 6, 12, 6, 4, 6, 11, 6, 4, 4, 4, 2, 6, 6, 4, 11, 11, 6, 6, 3, 4, 14, 7, 4, 6, 11, 6, 6, 6, 2, 4, 11, 6, 12, 12, 11, 4, 4, 3, 6, 4, 6, 11, 4, 4, 5, 12, 6, 4, 6, 6, 2, 4, 11, 6, 13, 4, 2, 6, 11, 6, 6, 11, 4, 6, 12, 4, 9, 4, 6, 6, 10, 4, 4, 6, 4, 2]\n",
            "c tensor([1.2517e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02056107483804226 tensor([[ 0.0648,  0.0679,  0.0768, -0.0056,  0.0249, -0.0218, -0.0041,  0.0190],\n",
            "        [ 0.0600,  0.0526,  0.0586, -0.0038,  0.0217, -0.0221, -0.0039,  0.0210],\n",
            "        [ 0.0474,  0.0401,  0.0473, -0.0032,  0.0179, -0.0152, -0.0031,  0.0142],\n",
            "        [ 0.0241,  0.0191,  0.0212, -0.0022,  0.0082, -0.0105, -0.0006,  0.0080]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01761188730597496 tensor([[ 0.1021,  0.1091,  0.1245, -0.0102,  0.0398, -0.0330, -0.0069,  0.0263],\n",
            "        [ 0.0997,  0.0860,  0.0957, -0.0068,  0.0359, -0.0364, -0.0069,  0.0332],\n",
            "        [ 0.0798,  0.0656,  0.0786, -0.0056,  0.0304, -0.0244, -0.0056,  0.0216],\n",
            "        [ 0.0446,  0.0349,  0.0389, -0.0042,  0.0152, -0.0194, -0.0009,  0.0142]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01642371341586113 tensor([[ 0.1236,  0.1341,  0.1546, -0.0140,  0.0488, -0.0381, -0.0090,  0.0271],\n",
            "        [ 0.1249,  0.1054,  0.1172, -0.0092,  0.0449, -0.0451, -0.0094,  0.0392],\n",
            "        [ 0.1019,  0.0809,  0.0988, -0.0075,  0.0390, -0.0295, -0.0076,  0.0243],\n",
            "        [ 0.0621,  0.0483,  0.0539, -0.0060,  0.0210, -0.0271, -0.0011,  0.0191]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015936102718114853 tensor([[ 0.1362,  0.1501,  0.1745, -0.0175,  0.0546, -0.0398, -0.0106,  0.0245],\n",
            "        [ 0.1414,  0.1162,  0.1291, -0.0113,  0.0507, -0.0504, -0.0114,  0.0416],\n",
            "        [ 0.1175,  0.0898,  0.1121, -0.0090,  0.0453, -0.0320, -0.0092,  0.0241],\n",
            "        [ 0.0771,  0.0595,  0.0666, -0.0077,  0.0260, -0.0338, -0.0011,  0.0229]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015713702887296677 tensor([[ 0.1437,  0.1606,  0.1884, -0.0206,  0.0584, -0.0396, -0.0120,  0.0199],\n",
            "        [ 0.1527,  0.1219,  0.1353, -0.0130,  0.0545, -0.0536, -0.0133,  0.0417],\n",
            "        [ 0.1293,  0.0948,  0.1210, -0.0105,  0.0500, -0.0328, -0.0107,  0.0222],\n",
            "        [ 0.0901,  0.0689,  0.0772, -0.0094,  0.0302, -0.0396, -0.0011,  0.0257]],\n",
            "       device='cuda:0')\n",
            "c tensor([3.5763e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01578923873603344 tensor([[ 0.0643,  0.0570,  0.0645, -0.0040,  0.0242, -0.0217, -0.0049,  0.0215],\n",
            "        [ 0.0568,  0.0518,  0.0574, -0.0038,  0.0210, -0.0205, -0.0033,  0.0187],\n",
            "        [ 0.0500,  0.0478,  0.0534, -0.0036,  0.0185, -0.0181, -0.0026,  0.0169],\n",
            "        [ 0.0233,  0.0180,  0.0204, -0.0013,  0.0087, -0.0082, -0.0013,  0.0087]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.012907379306852818 tensor([[ 0.1113,  0.0974,  0.1112, -0.0075,  0.0422, -0.0365, -0.0089,  0.0353],\n",
            "        [ 0.0964,  0.0873,  0.0968, -0.0069,  0.0359, -0.0344, -0.0058,  0.0297],\n",
            "        [ 0.0846,  0.0811,  0.0907, -0.0065,  0.0314, -0.0303, -0.0046,  0.0271],\n",
            "        [ 0.0420,  0.0319,  0.0363, -0.0023,  0.0159, -0.0145, -0.0025,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.011493529193103313 tensor([[ 0.1454,  0.1247,  0.1436, -0.0106,  0.0554, -0.0463, -0.0122,  0.0435],\n",
            "        [ 0.1231,  0.1100,  0.1222, -0.0093,  0.0461, -0.0431, -0.0077,  0.0350],\n",
            "        [ 0.1078,  0.1036,  0.1160, -0.0089,  0.0402, -0.0381, -0.0061,  0.0324],\n",
            "        [ 0.0573,  0.0428,  0.0489, -0.0031,  0.0218, -0.0194, -0.0035,  0.0201]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01082993671298027 tensor([[ 0.1705,  0.1429,  0.1661, -0.0133,  0.0654, -0.0526, -0.0151,  0.0481],\n",
            "        [ 0.1413,  0.1243,  0.1384, -0.0114,  0.0533, -0.0484, -0.0092,  0.0365],\n",
            "        [ 0.1234,  0.1190,  0.1334, -0.0110,  0.0461, -0.0428, -0.0073,  0.0345],\n",
            "        [ 0.0698,  0.0514,  0.0589, -0.0036,  0.0269, -0.0231, -0.0043,  0.0238]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.010509470477700233 tensor([[ 0.1896,  0.1548,  0.1816, -0.0157,  0.0732, -0.0567, -0.0176,  0.0502],\n",
            "        [ 0.1542,  0.1332,  0.1487, -0.0132,  0.0585, -0.0515, -0.0103,  0.0357],\n",
            "        [ 0.1341,  0.1296,  0.1455, -0.0128,  0.0502, -0.0455, -0.0082,  0.0345],\n",
            "        [ 0.0803,  0.0582,  0.0669, -0.0041,  0.0312, -0.0259, -0.0050,  0.0264]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.3644e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021162210032343864 tensor([[ 0.0639,  0.0596,  0.0663, -0.0052,  0.0224, -0.0230, -0.0039,  0.0198],\n",
            "        [ 0.0627,  0.0617,  0.0693, -0.0035,  0.0238, -0.0217, -0.0040,  0.0219],\n",
            "        [ 0.0603,  0.0520,  0.0594, -0.0040,  0.0222, -0.0216, -0.0037,  0.0210],\n",
            "        [ 0.0272,  0.0201,  0.0224, -0.0023,  0.0103, -0.0107, -0.0011,  0.0084]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017797401174902916 tensor([[ 0.1054,  0.0968,  0.1085, -0.0096,  0.0365, -0.0371, -0.0067,  0.0298],\n",
            "        [ 0.1006,  0.0996,  0.1123, -0.0058,  0.0387, -0.0339, -0.0068,  0.0331],\n",
            "        [ 0.1014,  0.0854,  0.0985, -0.0073,  0.0377, -0.0357, -0.0066,  0.0336],\n",
            "        [ 0.0494,  0.0352,  0.0393, -0.0043,  0.0189, -0.0191, -0.0020,  0.0142]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016434475779533386 tensor([[ 0.1331,  0.1200,  0.1353, -0.0135,  0.0457, -0.0460, -0.0088,  0.0342],\n",
            "        [ 0.1221,  0.1214,  0.1374, -0.0074,  0.0476, -0.0398, -0.0088,  0.0375],\n",
            "        [ 0.1285,  0.1051,  0.1225, -0.0100,  0.0480, -0.0444, -0.0089,  0.0402],\n",
            "        [ 0.0677,  0.0465,  0.0522, -0.0062,  0.0262, -0.0257, -0.0028,  0.0181]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015890173614025116 tensor([[ 0.1527,  0.1349,  0.1529, -0.0171,  0.0519, -0.0517, -0.0104,  0.0353],\n",
            "        [ 0.1343,  0.1337,  0.1521, -0.0085,  0.0530, -0.0420, -0.0103,  0.0380],\n",
            "        [ 0.1468,  0.1162,  0.1370, -0.0124,  0.0551, -0.0496, -0.0108,  0.0430],\n",
            "        [ 0.0832,  0.0552,  0.0622, -0.0079,  0.0325, -0.0311, -0.0035,  0.0206]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015654075890779495 tensor([[ 0.1674,  0.1446,  0.1647, -0.0205,  0.0563, -0.0555, -0.0118,  0.0345],\n",
            "        [ 0.1411,  0.1406,  0.1607, -0.0094,  0.0563, -0.0422, -0.0116,  0.0363],\n",
            "        [ 0.1597,  0.1219,  0.1456, -0.0145,  0.0603, -0.0527, -0.0124,  0.0435],\n",
            "        [ 0.0965,  0.0618,  0.0700, -0.0096,  0.0380, -0.0356, -0.0042,  0.0220]],\n",
            "       device='cuda:0')\n",
            "c tensor([3.8743e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.016102315858006477 tensor([[ 0.0534,  0.0469,  0.0525, -0.0023,  0.0190, -0.0187, -0.0028,  0.0174],\n",
            "        [ 0.0550,  0.0495,  0.0566, -0.0041,  0.0204, -0.0186, -0.0024,  0.0169],\n",
            "        [ 0.0353,  0.0320,  0.0358, -0.0022,  0.0126, -0.0126, -0.0021,  0.0124],\n",
            "        [ 0.0172,  0.0161,  0.0192, -0.0019,  0.0060, -0.0045, -0.0011,  0.0047]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014101345092058182 tensor([[ 0.0923,  0.0798,  0.0898, -0.0040,  0.0328, -0.0315, -0.0049,  0.0283],\n",
            "        [ 0.0931,  0.0829,  0.0955, -0.0075,  0.0347, -0.0306, -0.0041,  0.0264],\n",
            "        [ 0.0579,  0.0517,  0.0581, -0.0037,  0.0205, -0.0202, -0.0037,  0.0194],\n",
            "        [ 0.0308,  0.0290,  0.0349, -0.0037,  0.0105, -0.0074, -0.0021,  0.0076]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013160953298211098 tensor([[ 0.1208,  0.1023,  0.1157, -0.0054,  0.0427, -0.0403, -0.0065,  0.0348],\n",
            "        [ 0.1188,  0.1041,  0.1210, -0.0104,  0.0445, -0.0377, -0.0054,  0.0306],\n",
            "        [ 0.0725,  0.0636,  0.0718, -0.0047,  0.0255, -0.0247, -0.0047,  0.0228],\n",
            "        [ 0.0417,  0.0396,  0.0482, -0.0053,  0.0139, -0.0090, -0.0029,  0.0091]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.012724557891488075 tensor([[ 0.1422,  0.1175,  0.1336, -0.0065,  0.0501, -0.0464, -0.0078,  0.0385],\n",
            "        [ 0.1364,  0.1173,  0.1376, -0.0128,  0.0513, -0.0415, -0.0064,  0.0313],\n",
            "        [ 0.0821,  0.0710,  0.0805, -0.0056,  0.0286, -0.0273, -0.0055,  0.0240],\n",
            "        [ 0.0504,  0.0483,  0.0594, -0.0068,  0.0165, -0.0097, -0.0036,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.012513097375631332 tensor([[ 0.1586,  0.1278,  0.1460, -0.0074,  0.0557, -0.0505, -0.0087,  0.0401],\n",
            "        [ 0.1489,  0.1253,  0.1486, -0.0151,  0.0563, -0.0433, -0.0071,  0.0297],\n",
            "        [ 0.0888,  0.0755,  0.0861, -0.0062,  0.0306, -0.0286, -0.0061,  0.0240],\n",
            "        [ 0.0574,  0.0555,  0.0690, -0.0083,  0.0185, -0.0096, -0.0043,  0.0093]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018531998619437218 tensor([[ 0.0761,  0.0683,  0.0759, -0.0050,  0.0270, -0.0284, -0.0039,  0.0256],\n",
            "        [ 0.0596,  0.0577,  0.0631, -0.0040,  0.0224, -0.0217, -0.0043,  0.0197],\n",
            "        [ 0.0540,  0.0484,  0.0552, -0.0048,  0.0211, -0.0205, -0.0034,  0.0196],\n",
            "        [ 0.0265,  0.0223,  0.0249, -0.0019,  0.0092, -0.0095, -0.0019,  0.0092]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014970322139561176 tensor([[ 0.1270,  0.1119,  0.1249, -0.0091,  0.0449, -0.0471, -0.0065,  0.0403],\n",
            "        [ 0.0994,  0.0969,  0.1057, -0.0070,  0.0376, -0.0356, -0.0078,  0.0305],\n",
            "        [ 0.0917,  0.0808,  0.0929, -0.0092,  0.0367, -0.0348, -0.0062,  0.0324],\n",
            "        [ 0.0476,  0.0393,  0.0444, -0.0035,  0.0165, -0.0164, -0.0036,  0.0157]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013400758616626263 tensor([[ 0.1607,  0.1383,  0.1550, -0.0125,  0.0565, -0.0593, -0.0083,  0.0480],\n",
            "        [ 0.1240,  0.1209,  0.1316, -0.0094,  0.0472, -0.0434, -0.0105,  0.0346],\n",
            "        [ 0.1179,  0.1017,  0.1179, -0.0131,  0.0482, -0.0446, -0.0087,  0.0403],\n",
            "        [ 0.0645,  0.0525,  0.0597, -0.0049,  0.0223, -0.0215, -0.0051,  0.0200]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.012739608995616436 tensor([[ 0.1843,  0.1543,  0.1736, -0.0155,  0.0645, -0.0677, -0.0095,  0.0517],\n",
            "        [ 0.1389,  0.1352,  0.1469, -0.0114,  0.0532, -0.0473, -0.0128,  0.0344],\n",
            "        [ 0.1364,  0.1150,  0.1346, -0.0167,  0.0569, -0.0515, -0.0109,  0.0450],\n",
            "        [ 0.0782,  0.0627,  0.0718, -0.0062,  0.0268, -0.0251, -0.0065,  0.0227]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.012447403743863106 tensor([[ 0.2018,  0.1640,  0.1851, -0.0182,  0.0702, -0.0737, -0.0104,  0.0531],\n",
            "        [ 0.1480,  0.1436,  0.1557, -0.0131,  0.0570, -0.0488, -0.0147,  0.0317],\n",
            "        [ 0.1500,  0.1233,  0.1458, -0.0201,  0.0639, -0.0563, -0.0129,  0.0475],\n",
            "        [ 0.0894,  0.0706,  0.0815, -0.0074,  0.0305, -0.0276, -0.0078,  0.0241]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01966002769768238 tensor([[ 0.0552,  0.0486,  0.0557, -0.0028,  0.0199, -0.0176, -0.0023,  0.0141],\n",
            "        [ 0.0584,  0.0547,  0.0600, -0.0043,  0.0209, -0.0217, -0.0026,  0.0201],\n",
            "        [ 0.0428,  0.0410,  0.0468, -0.0038,  0.0169, -0.0158, -0.0040,  0.0176],\n",
            "        [ 0.0279,  0.0258,  0.0300, -0.0022,  0.0097, -0.0106, -0.0009,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0171330738812685 tensor([[ 0.0940,  0.0810,  0.0942, -0.0051,  0.0338, -0.0284, -0.0038,  0.0207],\n",
            "        [ 0.0983,  0.0919,  0.1005, -0.0078,  0.0353, -0.0363, -0.0045,  0.0322],\n",
            "        [ 0.0707,  0.0674,  0.0773, -0.0071,  0.0283, -0.0259, -0.0075,  0.0289],\n",
            "        [ 0.0503,  0.0466,  0.0544, -0.0041,  0.0173, -0.0188, -0.0016,  0.0129]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015948902815580368 tensor([[ 0.1217,  0.1021,  0.1204, -0.0069,  0.0437, -0.0348, -0.0046,  0.0224],\n",
            "        [ 0.1254,  0.1166,  0.1272, -0.0108,  0.0450, -0.0459, -0.0058,  0.0388],\n",
            "        [ 0.0892,  0.0844,  0.0976, -0.0100,  0.0362, -0.0323, -0.0105,  0.0361],\n",
            "        [ 0.0681,  0.0632,  0.0744, -0.0059,  0.0232, -0.0251, -0.0022,  0.0158]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015382649376988411 tensor([[ 0.1423,  0.1158,  0.1386, -0.0084,  0.0509, -0.0383, -0.0051,  0.0211],\n",
            "        [ 0.1444,  0.1332,  0.1450, -0.0134,  0.0517, -0.0522, -0.0069,  0.0418],\n",
            "        [ 0.1018,  0.0957,  0.1116, -0.0126,  0.0419, -0.0364, -0.0133,  0.0408],\n",
            "        [ 0.0823,  0.0766,  0.0908, -0.0075,  0.0277, -0.0298, -0.0026,  0.0171]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01509535126388073 tensor([[ 0.1581,  0.1244,  0.1512, -0.0096,  0.0564, -0.0400, -0.0053,  0.0179],\n",
            "        [ 0.1581,  0.1445,  0.1570, -0.0158,  0.0566, -0.0565, -0.0078,  0.0426],\n",
            "        [ 0.1105,  0.1032,  0.1214, -0.0151,  0.0462, -0.0390, -0.0158,  0.0439],\n",
            "        [ 0.0936,  0.0873,  0.1043, -0.0089,  0.0312, -0.0334, -0.0029,  0.0170]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.9802e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.014662213623523712 tensor([[ 0.0373,  0.0339,  0.0375, -0.0041,  0.0132, -0.0144, -0.0023,  0.0113],\n",
            "        [ 0.0487,  0.0470,  0.0539, -0.0039,  0.0172, -0.0175, -0.0034,  0.0164],\n",
            "        [ 0.0423,  0.0382,  0.0441, -0.0038,  0.0155, -0.0148, -0.0034,  0.0139],\n",
            "        [ 0.0265,  0.0192,  0.0218, -0.0016,  0.0094, -0.0094, -0.0010,  0.0058]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.013018676079809666 tensor([[ 0.0602,  0.0534,  0.0592, -0.0075,  0.0214, -0.0231, -0.0039,  0.0166],\n",
            "        [ 0.0773,  0.0742,  0.0859, -0.0071,  0.0268, -0.0273, -0.0059,  0.0245],\n",
            "        [ 0.0686,  0.0606,  0.0708, -0.0070,  0.0250, -0.0232, -0.0061,  0.0209],\n",
            "        [ 0.0480,  0.0337,  0.0386, -0.0028,  0.0169, -0.0165, -0.0017,  0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.012395190075039864 tensor([[ 0.0758,  0.0653,  0.0727, -0.0106,  0.0269, -0.0290, -0.0051,  0.0189],\n",
            "        [ 0.0950,  0.0907,  0.1059, -0.0099,  0.0322, -0.0331, -0.0078,  0.0281],\n",
            "        [ 0.0849,  0.0732,  0.0867, -0.0098,  0.0308, -0.0276, -0.0085,  0.0236],\n",
            "        [ 0.0658,  0.0446,  0.0516, -0.0039,  0.0231, -0.0219, -0.0022,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.012132326140999794 tensor([[ 0.0872,  0.0728,  0.0813, -0.0134,  0.0309, -0.0333, -0.0061,  0.0195],\n",
            "        [ 0.1065,  0.1009,  0.1191, -0.0125,  0.0353, -0.0365, -0.0094,  0.0292],\n",
            "        [ 0.0953,  0.0798,  0.0961, -0.0124,  0.0344, -0.0297, -0.0105,  0.0238],\n",
            "        [ 0.0807,  0.0530,  0.0617, -0.0048,  0.0281, -0.0261, -0.0026,  0.0100]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01200816035270691 tensor([[ 0.0960,  0.0777,  0.0870, -0.0162,  0.0339, -0.0365, -0.0069,  0.0191],\n",
            "        [ 0.1142,  0.1075,  0.1280, -0.0149,  0.0370, -0.0384, -0.0108,  0.0288],\n",
            "        [ 0.1022,  0.0828,  0.1016, -0.0148,  0.0367, -0.0303, -0.0125,  0.0226],\n",
            "        [ 0.0935,  0.0593,  0.0696, -0.0056,  0.0324, -0.0294, -0.0029,  0.0088]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018040059134364128 tensor([[ 0.0561,  0.0530,  0.0594, -0.0049,  0.0200, -0.0217, -0.0037,  0.0191],\n",
            "        [ 0.0536,  0.0453,  0.0500, -0.0036,  0.0187, -0.0185, -0.0027,  0.0154],\n",
            "        [ 0.0478,  0.0453,  0.0511, -0.0033,  0.0181, -0.0171, -0.0035,  0.0169],\n",
            "        [ 0.0246,  0.0214,  0.0245, -0.0018,  0.0090, -0.0093, -0.0010,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01563763990998268 tensor([[ 0.0923,  0.0864,  0.0974, -0.0091,  0.0327, -0.0357, -0.0065,  0.0300],\n",
            "        [ 0.0883,  0.0721,  0.0798, -0.0065,  0.0306, -0.0295, -0.0047,  0.0223],\n",
            "        [ 0.0813,  0.0773,  0.0872, -0.0059,  0.0309, -0.0287, -0.0064,  0.0277],\n",
            "        [ 0.0443,  0.0381,  0.0441, -0.0034,  0.0163, -0.0162, -0.0019,  0.0131]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01460155751556158 tensor([[ 0.1151,  0.1065,  0.1206, -0.0127,  0.0406, -0.0447, -0.0086,  0.0358],\n",
            "        [ 0.1109,  0.0867,  0.0961, -0.0089,  0.0381, -0.0354, -0.0063,  0.0238],\n",
            "        [ 0.1044,  0.0994,  0.1123, -0.0080,  0.0398, -0.0362, -0.0088,  0.0341],\n",
            "        [ 0.0602,  0.0513,  0.0599, -0.0048,  0.0222, -0.0214, -0.0027,  0.0164]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014155062846839428 tensor([[ 0.1298,  0.1184,  0.1345, -0.0159,  0.0456, -0.0507, -0.0104,  0.0385],\n",
            "        [ 0.1263,  0.0941,  0.1046, -0.0110,  0.0430, -0.0385, -0.0075,  0.0221],\n",
            "        [ 0.1208,  0.1148,  0.1301, -0.0098,  0.0462, -0.0410, -0.0109,  0.0376],\n",
            "        [ 0.0731,  0.0616,  0.0729, -0.0062,  0.0271, -0.0252, -0.0033,  0.0183]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013949139975011349 tensor([[ 0.1396,  0.1253,  0.1428, -0.0189,  0.0487, -0.0550, -0.0118,  0.0395],\n",
            "        [ 0.1377,  0.0973,  0.1086, -0.0128,  0.0465, -0.0398, -0.0086,  0.0187],\n",
            "        [ 0.1325,  0.1257,  0.1428, -0.0114,  0.0509, -0.0440, -0.0128,  0.0392],\n",
            "        [ 0.0837,  0.0698,  0.0835, -0.0074,  0.0311, -0.0280, -0.0039,  0.0190]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02169313281774521 tensor([[ 0.0636,  0.0598,  0.0694, -0.0044,  0.0241, -0.0211, -0.0049,  0.0199],\n",
            "        [ 0.0526,  0.0421,  0.0478, -0.0040,  0.0189, -0.0192, -0.0030,  0.0163],\n",
            "        [ 0.0518,  0.0511,  0.0586, -0.0044,  0.0197, -0.0184, -0.0037,  0.0166],\n",
            "        [ 0.0226,  0.0160,  0.0197, -0.0018,  0.0086, -0.0072, -0.0007,  0.0046]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01895969547331333 tensor([[ 0.1065,  0.0995,  0.1169, -0.0080,  0.0406, -0.0342, -0.0088,  0.0312],\n",
            "        [ 0.0869,  0.0666,  0.0764, -0.0073,  0.0315, -0.0313, -0.0053,  0.0247],\n",
            "        [ 0.0853,  0.0851,  0.0979, -0.0082,  0.0325, -0.0301, -0.0066,  0.0256],\n",
            "        [ 0.0411,  0.0279,  0.0351, -0.0032,  0.0158, -0.0123, -0.0012,  0.0069]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017787382006645203 tensor([[ 0.1343,  0.1241,  0.1476, -0.0108,  0.0515, -0.0416, -0.0119,  0.0366],\n",
            "        [ 0.1096,  0.0796,  0.0925, -0.0100,  0.0400, -0.0388, -0.0072,  0.0279],\n",
            "        [ 0.1063,  0.1071,  0.1238, -0.0114,  0.0406, -0.0370, -0.0089,  0.0292],\n",
            "        [ 0.0565,  0.0370,  0.0474, -0.0044,  0.0218, -0.0159, -0.0017,  0.0076]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01730155199766159 tensor([[ 0.1524,  0.1386,  0.1671, -0.0132,  0.0588, -0.0453, -0.0146,  0.0383],\n",
            "        [ 0.1256,  0.0857,  0.1012, -0.0124,  0.0460, -0.0435, -0.0088,  0.0282],\n",
            "        [ 0.1194,  0.1215,  0.1413, -0.0144,  0.0458, -0.0410, -0.0108,  0.0296],\n",
            "        [ 0.0696,  0.0438,  0.0574, -0.0055,  0.0269, -0.0183, -0.0020,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01708749309182167 tensor([[ 0.1643,  0.1469,  0.1794, -0.0152,  0.0638, -0.0468, -0.0169,  0.0378],\n",
            "        [ 0.1377,  0.0879,  0.1055, -0.0146,  0.0507, -0.0466, -0.0102,  0.0268],\n",
            "        [ 0.1275,  0.1313,  0.1536, -0.0172,  0.0491, -0.0432, -0.0125,  0.0279],\n",
            "        [ 0.0808,  0.0490,  0.0655, -0.0065,  0.0314, -0.0200, -0.0023,  0.0057]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.1921e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018330495804548264 tensor([[ 0.0698,  0.0645,  0.0722, -0.0053,  0.0253, -0.0253, -0.0036,  0.0212],\n",
            "        [ 0.0630,  0.0573,  0.0632, -0.0038,  0.0234, -0.0237, -0.0041,  0.0230],\n",
            "        [ 0.0478,  0.0439,  0.0496, -0.0029,  0.0173, -0.0165, -0.0038,  0.0176],\n",
            "        [ 0.0245,  0.0153,  0.0178, -0.0018,  0.0080, -0.0086, -0.0007,  0.0060]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015117338858544827 tensor([[ 0.1201,  0.1104,  0.1246, -0.0101,  0.0436, -0.0429, -0.0062,  0.0339],\n",
            "        [ 0.1063,  0.0957,  0.1054, -0.0069,  0.0399, -0.0402, -0.0075,  0.0377],\n",
            "        [ 0.0789,  0.0717,  0.0812, -0.0050,  0.0285, -0.0266, -0.0069,  0.0282],\n",
            "        [ 0.0448,  0.0262,  0.0311, -0.0034,  0.0143, -0.0151, -0.0012,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013613523915410042 tensor([[ 0.1553,  0.1412,  0.1604, -0.0142,  0.0565, -0.0547, -0.0081,  0.0403],\n",
            "        [ 0.1351,  0.1198,  0.1318, -0.0094,  0.0512, -0.0511, -0.0104,  0.0462],\n",
            "        [ 0.0991,  0.0891,  0.1011, -0.0064,  0.0356, -0.0326, -0.0095,  0.0342],\n",
            "        [ 0.0620,  0.0340,  0.0410, -0.0048,  0.0194, -0.0200, -0.0015,  0.0115]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01294238492846489 tensor([[ 0.1801,  0.1614,  0.1846, -0.0179,  0.0656, -0.0624, -0.0094,  0.0427],\n",
            "        [ 0.1549,  0.1348,  0.1482, -0.0116,  0.0592, -0.0584, -0.0129,  0.0508],\n",
            "        [ 0.1125,  0.1000,  0.1139, -0.0075,  0.0401, -0.0360, -0.0117,  0.0374],\n",
            "        [ 0.0768,  0.0395,  0.0486, -0.0060,  0.0236, -0.0237, -0.0018,  0.0121]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.012636052444577217 tensor([[ 0.1979,  0.1745,  0.2008, -0.0213,  0.0722, -0.0676, -0.0104,  0.0423],\n",
            "        [ 0.1690,  0.1440,  0.1582, -0.0135,  0.0651, -0.0635, -0.0153,  0.0529],\n",
            "        [ 0.1216,  0.1069,  0.1221, -0.0084,  0.0430, -0.0378, -0.0137,  0.0388],\n",
            "        [ 0.0898,  0.0432,  0.0543, -0.0072,  0.0271, -0.0266, -0.0020,  0.0119]],\n",
            "       device='cuda:0')\n",
            "c tensor([3.3617e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021752487868070602 tensor([[ 0.0487,  0.0392,  0.0442, -0.0037,  0.0172, -0.0183, -0.0027,  0.0154],\n",
            "        [ 0.0520,  0.0485,  0.0550, -0.0038,  0.0185, -0.0176, -0.0026,  0.0157],\n",
            "        [ 0.0565,  0.0584,  0.0672, -0.0049,  0.0228, -0.0180, -0.0048,  0.0183],\n",
            "        [ 0.0269,  0.0185,  0.0200, -0.0018,  0.0088, -0.0106, -0.0009,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01923571340739727 tensor([[ 0.0830,  0.0644,  0.0732, -0.0069,  0.0292, -0.0310, -0.0047,  0.0246],\n",
            "        [ 0.0867,  0.0801,  0.0917, -0.0069,  0.0307, -0.0283, -0.0044,  0.0238],\n",
            "        [ 0.0938,  0.0985,  0.1139, -0.0091,  0.0385, -0.0287, -0.0088,  0.0282],\n",
            "        [ 0.0497,  0.0330,  0.0356, -0.0033,  0.0161, -0.0193, -0.0018,  0.0178]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018086601048707962 tensor([[ 0.1076,  0.0801,  0.0917, -0.0096,  0.0377, -0.0400, -0.0061,  0.0299],\n",
            "        [ 0.1093,  0.0996,  0.1152, -0.0094,  0.0385, -0.0342, -0.0055,  0.0266],\n",
            "        [ 0.1173,  0.1252,  0.1459, -0.0129,  0.0492, -0.0341, -0.0123,  0.0319],\n",
            "        [ 0.0692,  0.0443,  0.0477, -0.0045,  0.0222, -0.0265, -0.0025,  0.0239]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017563987523317337 tensor([[ 0.1261,  0.0895,  0.1033, -0.0121,  0.0441, -0.0466, -0.0073,  0.0327],\n",
            "        [ 0.1244,  0.1116,  0.1305, -0.0117,  0.0436, -0.0371, -0.0063,  0.0261],\n",
            "        [ 0.1321,  0.1433,  0.1684, -0.0164,  0.0565, -0.0361, -0.0153,  0.0318],\n",
            "        [ 0.0860,  0.0532,  0.0570, -0.0056,  0.0273, -0.0324, -0.0031,  0.0288]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017308613285422325 tensor([[ 0.1406,  0.0949,  0.1105, -0.0144,  0.0490, -0.0517, -0.0083,  0.0340],\n",
            "        [ 0.1348,  0.1188,  0.1406, -0.0137,  0.0470, -0.0381, -0.0069,  0.0236],\n",
            "        [ 0.1413,  0.1559,  0.1847, -0.0197,  0.0618, -0.0358, -0.0181,  0.0292],\n",
            "        [ 0.1005,  0.0600,  0.0640, -0.0066,  0.0316, -0.0375, -0.0037,  0.0326]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019189640879631042 tensor([[ 0.0599,  0.0549,  0.0620, -0.0050,  0.0215, -0.0213, -0.0038,  0.0196],\n",
            "        [ 0.0630,  0.0591,  0.0671, -0.0042,  0.0237, -0.0237, -0.0033,  0.0227],\n",
            "        [ 0.0529,  0.0515,  0.0574, -0.0029,  0.0207, -0.0194, -0.0038,  0.0169],\n",
            "        [ 0.0293,  0.0172,  0.0212, -0.0033,  0.0109, -0.0102, -0.0011,  0.0081]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01621202751994133 tensor([[ 0.0964,  0.0862,  0.0984, -0.0091,  0.0344, -0.0333, -0.0064,  0.0290],\n",
            "        [ 0.0978,  0.0906,  0.1037, -0.0072,  0.0370, -0.0368, -0.0054,  0.0339],\n",
            "        [ 0.0864,  0.0850,  0.0946, -0.0047,  0.0343, -0.0313, -0.0066,  0.0251],\n",
            "        [ 0.0544,  0.0300,  0.0377, -0.0066,  0.0204, -0.0184, -0.0022,  0.0139]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015136837027966976 tensor([[ 0.1201,  0.1044,  0.1205, -0.0126,  0.0427, -0.0404, -0.0082,  0.0331],\n",
            "        [ 0.1172,  0.1069,  0.1234, -0.0096,  0.0447, -0.0441, -0.0069,  0.0389],\n",
            "        [ 0.1063,  0.1056,  0.1175, -0.0058,  0.0427, -0.0380, -0.0087,  0.0275],\n",
            "        [ 0.0762,  0.0394,  0.0508, -0.0097,  0.0288, -0.0250, -0.0033,  0.0181]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014718979597091675 tensor([[ 0.1369,  0.1153,  0.1346, -0.0158,  0.0484, -0.0448, -0.0097,  0.0344],\n",
            "        [ 0.1285,  0.1152,  0.1342, -0.0116,  0.0493, -0.0482, -0.0080,  0.0406],\n",
            "        [ 0.1179,  0.1184,  0.1314, -0.0065,  0.0481, -0.0415, -0.0103,  0.0263],\n",
            "        [ 0.0955,  0.0464,  0.0612, -0.0127,  0.0364, -0.0305, -0.0043,  0.0211]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014532524161040783 tensor([[ 0.1494,  0.1219,  0.1438, -0.0189,  0.0526, -0.0476, -0.0109,  0.0339],\n",
            "        [ 0.1354,  0.1191,  0.1402, -0.0134,  0.0524, -0.0506, -0.0089,  0.0407],\n",
            "        [ 0.1247,  0.1264,  0.1400, -0.0069,  0.0517, -0.0432, -0.0115,  0.0231],\n",
            "        [ 0.1128,  0.0513,  0.0695, -0.0156,  0.0432, -0.0351, -0.0052,  0.0231]],\n",
            "       device='cuda:0')\n",
            "c tensor([9.5963e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019572563469409943 tensor([[ 0.0579,  0.0559,  0.0611, -0.0036,  0.0202, -0.0211, -0.0034,  0.0195],\n",
            "        [ 0.0653,  0.0657,  0.0737, -0.0042,  0.0242, -0.0235, -0.0043,  0.0217],\n",
            "        [ 0.0534,  0.0428,  0.0495, -0.0039,  0.0202, -0.0203, -0.0033,  0.0177],\n",
            "        [ 0.0296,  0.0187,  0.0225, -0.0018,  0.0102, -0.0098, -0.0014,  0.0084]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01651933044195175 tensor([[ 0.0941,  0.0904,  0.0989, -0.0065,  0.0325, -0.0338, -0.0057,  0.0296],\n",
            "        [ 0.1028,  0.1048,  0.1176, -0.0072,  0.0380, -0.0364, -0.0073,  0.0319],\n",
            "        [ 0.0902,  0.0697,  0.0816, -0.0072,  0.0345, -0.0339, -0.0059,  0.0281],\n",
            "        [ 0.0548,  0.0329,  0.0403, -0.0034,  0.0190, -0.0174, -0.0027,  0.0145]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015332909300923347 tensor([[ 0.1181,  0.1125,  0.1231, -0.0089,  0.0404, -0.0419, -0.0074,  0.0347],\n",
            "        [ 0.1225,  0.1265,  0.1419, -0.0094,  0.0451, -0.0426, -0.0096,  0.0348],\n",
            "        [ 0.1153,  0.0854,  0.1014, -0.0100,  0.0445, -0.0430, -0.0080,  0.0334],\n",
            "        [ 0.0767,  0.0437,  0.0546, -0.0047,  0.0266, -0.0235, -0.0039,  0.0187]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014854514971375465 tensor([[ 0.1353,  0.1276,  0.1394, -0.0111,  0.0458, -0.0473, -0.0086,  0.0370],\n",
            "        [ 0.1324,  0.1385,  0.1554, -0.0111,  0.0484, -0.0450, -0.0113,  0.0339],\n",
            "        [ 0.1329,  0.0938,  0.1132, -0.0125,  0.0518, -0.0490, -0.0097,  0.0356],\n",
            "        [ 0.0959,  0.0520,  0.0663, -0.0060,  0.0333, -0.0283, -0.0050,  0.0215]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014634395949542522 tensor([[ 0.1482,  0.1384,  0.1510, -0.0130,  0.0497, -0.0512, -0.0097,  0.0376],\n",
            "        [ 0.1370,  0.1452,  0.1629, -0.0126,  0.0498, -0.0453, -0.0129,  0.0310],\n",
            "        [ 0.1458,  0.0977,  0.1202, -0.0148,  0.0573, -0.0533, -0.0112,  0.0359],\n",
            "        [ 0.1130,  0.0583,  0.0757, -0.0071,  0.0392, -0.0322, -0.0060,  0.0233]],\n",
            "       device='cuda:0')\n",
            "c tensor([7.2122e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02401648834347725 tensor([[ 0.0579,  0.0579,  0.0636, -0.0036,  0.0204, -0.0213, -0.0030,  0.0201],\n",
            "        [ 0.0688,  0.0616,  0.0705, -0.0056,  0.0264, -0.0243, -0.0044,  0.0224],\n",
            "        [ 0.0463,  0.0399,  0.0456, -0.0039,  0.0182, -0.0163, -0.0045,  0.0164],\n",
            "        [ 0.0316,  0.0264,  0.0295, -0.0025,  0.0111, -0.0119, -0.0012,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02089541405439377 tensor([[ 0.0944,  0.0950,  0.1044, -0.0064,  0.0330, -0.0343, -0.0049,  0.0312],\n",
            "        [ 0.1159,  0.1028,  0.1183, -0.0104,  0.0451, -0.0403, -0.0082,  0.0354],\n",
            "        [ 0.0789,  0.0662,  0.0761, -0.0072,  0.0317, -0.0272, -0.0084,  0.0266],\n",
            "        [ 0.0583,  0.0481,  0.0539, -0.0047,  0.0205, -0.0215, -0.0023,  0.0138]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01950322464108467 tensor([[ 0.1166,  0.1175,  0.1292, -0.0087,  0.0403, -0.0419, -0.0061,  0.0367],\n",
            "        [ 0.1454,  0.1265,  0.1468, -0.0144,  0.0573, -0.0493, -0.0114,  0.0410],\n",
            "        [ 0.1011,  0.0821,  0.0953, -0.0100,  0.0415, -0.0341, -0.0120,  0.0321],\n",
            "        [ 0.0808,  0.0661,  0.0742, -0.0067,  0.0284, -0.0293, -0.0033,  0.0174]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01892711967229843 tensor([[ 0.1301,  0.1313,  0.1442, -0.0106,  0.0445, -0.0463, -0.0068,  0.0389],\n",
            "        [ 0.1637,  0.1390,  0.1627, -0.0178,  0.0654, -0.0541, -0.0141,  0.0421],\n",
            "        [ 0.1167,  0.0912,  0.1070, -0.0126,  0.0488, -0.0383, -0.0151,  0.0347],\n",
            "        [ 0.1000,  0.0811,  0.0913, -0.0085,  0.0351, -0.0356, -0.0041,  0.0194]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018670879304409027 tensor([[ 0.1387,  0.1399,  0.1534, -0.0123,  0.0468, -0.0488, -0.0073,  0.0392],\n",
            "        [ 0.1754,  0.1449,  0.1712, -0.0209,  0.0711, -0.0563, -0.0165,  0.0405],\n",
            "        [ 0.1280,  0.0961,  0.1142, -0.0150,  0.0546, -0.0408, -0.0181,  0.0353],\n",
            "        [ 0.1164,  0.0937,  0.1058, -0.0102,  0.0408, -0.0408, -0.0048,  0.0200]],\n",
            "       device='cuda:0')\n",
            "c tensor([3.5763e-07, 1.0133e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026917878538370132 tensor([[ 0.0592,  0.0544,  0.0623, -0.0038,  0.0219, -0.0204, -0.0049,  0.0209],\n",
            "        [ 0.0533,  0.0467,  0.0514, -0.0031,  0.0189, -0.0188, -0.0031,  0.0162],\n",
            "        [ 0.0520,  0.0533,  0.0597, -0.0027,  0.0187, -0.0182, -0.0034,  0.0173],\n",
            "        [ 0.0279,  0.0139,  0.0159, -0.0020,  0.0093, -0.0099, -0.0006,  0.0088]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.024256713688373566 tensor([[ 0.0994,  0.0903,  0.1043, -0.0069,  0.0368, -0.0336, -0.0088,  0.0339],\n",
            "        [ 0.0863,  0.0736,  0.0810, -0.0053,  0.0307, -0.0296, -0.0052,  0.0235],\n",
            "        [ 0.0858,  0.0900,  0.1010, -0.0047,  0.0308, -0.0294, -0.0060,  0.0268],\n",
            "        [ 0.0521,  0.0239,  0.0275, -0.0039,  0.0172, -0.0181, -0.0010,  0.0158]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02312558889389038 tensor([[ 0.1258,  0.1126,  0.1310, -0.0095,  0.0468, -0.0416, -0.0120,  0.0417],\n",
            "        [ 0.1064,  0.0873,  0.0963, -0.0069,  0.0377, -0.0351, -0.0067,  0.0249],\n",
            "        [ 0.1065,  0.1146,  0.1287, -0.0061,  0.0379, -0.0356, -0.0081,  0.0307],\n",
            "        [ 0.0733,  0.0310,  0.0359, -0.0056,  0.0240, -0.0250, -0.0015,  0.0213]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.022655576467514038 tensor([[ 0.1435,  0.1259,  0.1477, -0.0116,  0.0534, -0.0464, -0.0148,  0.0461],\n",
            "        [ 0.1192,  0.0936,  0.1034, -0.0081,  0.0422, -0.0376, -0.0078,  0.0230],\n",
            "        [ 0.1189,  0.1315,  0.1479, -0.0070,  0.0419, -0.0386, -0.0097,  0.0312],\n",
            "        [ 0.0922,  0.0359,  0.0417, -0.0072,  0.0299, -0.0308, -0.0018,  0.0257]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.022444428876042366 tensor([[ 0.1556,  0.1336,  0.1580, -0.0135,  0.0581, -0.0493, -0.0173,  0.0486],\n",
            "        [ 0.1280,  0.0956,  0.1061, -0.0091,  0.0451, -0.0385, -0.0086,  0.0194],\n",
            "        [ 0.1261,  0.1436,  0.1616, -0.0077,  0.0440, -0.0397, -0.0110,  0.0295],\n",
            "        [ 0.1093,  0.0389,  0.0457, -0.0088,  0.0352, -0.0357, -0.0021,  0.0292]],\n",
            "       device='cuda:0')\n",
            "c tensor([4.5598e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, 5.9605e-08,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.0205019973218441 tensor([[ 0.0541,  0.0523,  0.0579, -0.0030,  0.0201, -0.0191, -0.0044,  0.0208],\n",
            "        [ 0.0636,  0.0557,  0.0648, -0.0037,  0.0234, -0.0210, -0.0027,  0.0194],\n",
            "        [ 0.0349,  0.0273,  0.0313, -0.0020,  0.0134, -0.0137, -0.0027,  0.0133],\n",
            "        [ 0.0249,  0.0207,  0.0255, -0.0020,  0.0079, -0.0093,  0.0003,  0.0075]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018117234110832214 tensor([[ 0.0892,  0.0861,  0.0957, -0.0055,  0.0334, -0.0307, -0.0077,  0.0331],\n",
            "        [ 0.1081,  0.0935,  0.1098, -0.0067,  0.0400, -0.0344, -0.0047,  0.0304],\n",
            "        [ 0.0574,  0.0424,  0.0492, -0.0034,  0.0224, -0.0227, -0.0048,  0.0213],\n",
            "        [ 0.0454,  0.0376,  0.0470, -0.0040,  0.0140, -0.0167,  0.0008,  0.0129]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01705123856663704 tensor([[ 0.1121,  0.1074,  0.1198, -0.0076,  0.0421, -0.0376, -0.0104,  0.0403],\n",
            "        [ 0.1378,  0.1168,  0.1387, -0.0090,  0.0512, -0.0420, -0.0062,  0.0351],\n",
            "        [ 0.0719,  0.0496,  0.0583, -0.0043,  0.0287, -0.0286, -0.0066,  0.0258],\n",
            "        [ 0.0624,  0.0516,  0.0655, -0.0057,  0.0187, -0.0226,  0.0016,  0.0167]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016587955877184868 tensor([[ 0.1274,  0.1211,  0.1355, -0.0094,  0.0481, -0.0416, -0.0127,  0.0444],\n",
            "        [ 0.1577,  0.1303,  0.1567, -0.0108,  0.0587, -0.0458, -0.0072,  0.0358],\n",
            "        [ 0.0818,  0.0522,  0.0625, -0.0049,  0.0332, -0.0326, -0.0081,  0.0282],\n",
            "        [ 0.0767,  0.0632,  0.0813, -0.0074,  0.0224, -0.0273,  0.0026,  0.0193]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01637330651283264 tensor([[ 0.1379,  0.1299,  0.1457, -0.0110,  0.0523, -0.0438, -0.0147,  0.0466],\n",
            "        [ 0.1715,  0.1378,  0.1679, -0.0123,  0.0641, -0.0471, -0.0081,  0.0340],\n",
            "        [ 0.0891,  0.0521,  0.0638, -0.0054,  0.0368, -0.0354, -0.0094,  0.0293],\n",
            "        [ 0.0885,  0.0728,  0.0949, -0.0089,  0.0252, -0.0311,  0.0037,  0.0208]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020293008536100388 tensor([[ 0.0482,  0.0436,  0.0481, -0.0041,  0.0180, -0.0176, -0.0048,  0.0171],\n",
            "        [ 0.0583,  0.0558,  0.0620, -0.0036,  0.0200, -0.0220, -0.0032,  0.0188],\n",
            "        [ 0.0440,  0.0407,  0.0449, -0.0027,  0.0156, -0.0178, -0.0020,  0.0153],\n",
            "        [ 0.0196,  0.0147,  0.0174, -0.0023,  0.0069, -0.0049, -0.0016,  0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01812780275940895 tensor([[ 0.0783,  0.0692,  0.0766, -0.0075,  0.0296, -0.0280, -0.0087,  0.0263],\n",
            "        [ 0.0954,  0.0913,  0.1013, -0.0064,  0.0321, -0.0357, -0.0055,  0.0285],\n",
            "        [ 0.0719,  0.0662,  0.0727, -0.0046,  0.0252, -0.0296, -0.0034,  0.0238],\n",
            "        [ 0.0358,  0.0262,  0.0313, -0.0044,  0.0123, -0.0083, -0.0029,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017267363145947456 tensor([[ 0.0980,  0.0843,  0.0936, -0.0105,  0.0373, -0.0345, -0.0120,  0.0314],\n",
            "        [ 0.1191,  0.1133,  0.1259, -0.0087,  0.0394, -0.0441, -0.0071,  0.0324],\n",
            "        [ 0.0902,  0.0825,  0.0902, -0.0061,  0.0312, -0.0376, -0.0042,  0.0281],\n",
            "        [ 0.0493,  0.0353,  0.0425, -0.0065,  0.0166, -0.0105, -0.0041,  0.0158]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016908764839172363 tensor([[ 0.1118,  0.0932,  0.1037, -0.0132,  0.0429, -0.0386, -0.0150,  0.0340],\n",
            "        [ 0.1347,  0.1273,  0.1416, -0.0106,  0.0436, -0.0494, -0.0083,  0.0330],\n",
            "        [ 0.1027,  0.0933,  0.1016, -0.0072,  0.0350, -0.0432, -0.0048,  0.0298],\n",
            "        [ 0.0608,  0.0426,  0.0516, -0.0086,  0.0200, -0.0119, -0.0052,  0.0187]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01674514450132847 tensor([[ 0.1218,  0.0984,  0.1096, -0.0158,  0.0472, -0.0414, -0.0178,  0.0353],\n",
            "        [ 0.1455,  0.1364,  0.1518, -0.0123,  0.0461, -0.0527, -0.0093,  0.0314],\n",
            "        [ 0.1116,  0.1006,  0.1093, -0.0082,  0.0375, -0.0474, -0.0053,  0.0298],\n",
            "        [ 0.0705,  0.0483,  0.0589, -0.0106,  0.0227, -0.0126, -0.0062,  0.0209]],\n",
            "       device='cuda:0')\n",
            "c tensor([4.1723e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02155492641031742 tensor([[ 0.0509,  0.0415,  0.0451, -0.0031,  0.0171, -0.0191, -0.0031,  0.0167],\n",
            "        [ 0.0685,  0.0660,  0.0757, -0.0055,  0.0243, -0.0259, -0.0025,  0.0215],\n",
            "        [ 0.0636,  0.0579,  0.0669, -0.0053,  0.0253, -0.0238, -0.0041,  0.0223],\n",
            "        [ 0.0282,  0.0165,  0.0189, -0.0019,  0.0105, -0.0103, -0.0013,  0.0089]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018355395644903183 tensor([[ 0.0832,  0.0646,  0.0702, -0.0055,  0.0277, -0.0308, -0.0051,  0.0251],\n",
            "        [ 0.1086,  0.1048,  0.1213, -0.0099,  0.0381, -0.0410, -0.0036,  0.0312],\n",
            "        [ 0.1062,  0.0961,  0.1120, -0.0099,  0.0427, -0.0395, -0.0073,  0.0355],\n",
            "        [ 0.0518,  0.0284,  0.0326, -0.0035,  0.0195, -0.0185, -0.0025,  0.0156]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017110351473093033 tensor([[ 0.1057,  0.0776,  0.0845, -0.0075,  0.0348, -0.0386, -0.0065,  0.0293],\n",
            "        [ 0.1324,  0.1274,  0.1488, -0.0136,  0.0457, -0.0497, -0.0039,  0.0341],\n",
            "        [ 0.1325,  0.1184,  0.1396, -0.0138,  0.0539, -0.0489, -0.0097,  0.0416],\n",
            "        [ 0.0720,  0.0371,  0.0429, -0.0050,  0.0272, -0.0253, -0.0035,  0.0205]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016620803624391556 tensor([[ 0.1228,  0.0852,  0.0927, -0.0092,  0.0399, -0.0443, -0.0076,  0.0312],\n",
            "        [ 0.1470,  0.1409,  0.1664, -0.0168,  0.0498, -0.0550, -0.0038,  0.0333],\n",
            "        [ 0.1486,  0.1308,  0.1563, -0.0174,  0.0613, -0.0544, -0.0116,  0.0435],\n",
            "        [ 0.0898,  0.0435,  0.0506, -0.0064,  0.0340, -0.0310, -0.0044,  0.0243]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01640527881681919 tensor([[ 0.1367,  0.0896,  0.0974, -0.0108,  0.0440, -0.0487, -0.0086,  0.0318],\n",
            "        [ 0.1565,  0.1492,  0.1781, -0.0199,  0.0520, -0.0582, -0.0034,  0.0305],\n",
            "        [ 0.1586,  0.1372,  0.1663, -0.0207,  0.0663, -0.0575, -0.0132,  0.0429],\n",
            "        [ 0.1056,  0.0482,  0.0564, -0.0076,  0.0401, -0.0358, -0.0053,  0.0271]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020868591964244843 tensor([[ 0.0528,  0.0483,  0.0541, -0.0028,  0.0189, -0.0184, -0.0034,  0.0182],\n",
            "        [ 0.0447,  0.0436,  0.0476, -0.0026,  0.0161, -0.0172, -0.0029,  0.0160],\n",
            "        [ 0.0491,  0.0455,  0.0522, -0.0038,  0.0185, -0.0183, -0.0026,  0.0160],\n",
            "        [ 0.0203,  0.0145,  0.0193, -0.0026,  0.0083, -0.0054, -0.0019,  0.0060]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018750518560409546 tensor([[ 0.0915,  0.0828,  0.0933, -0.0053,  0.0327, -0.0312, -0.0062,  0.0301],\n",
            "        [ 0.0723,  0.0711,  0.0771, -0.0046,  0.0260, -0.0281, -0.0051,  0.0251],\n",
            "        [ 0.0798,  0.0731,  0.0846, -0.0069,  0.0302, -0.0294, -0.0045,  0.0242],\n",
            "        [ 0.0371,  0.0252,  0.0347, -0.0051,  0.0152, -0.0090, -0.0036,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017815109342336655 tensor([[ 0.1205,  0.1076,  0.1216, -0.0074,  0.0429, -0.0401, -0.0085,  0.0379],\n",
            "        [ 0.0889,  0.0881,  0.0946, -0.0059,  0.0319, -0.0349, -0.0067,  0.0299],\n",
            "        [ 0.0983,  0.0890,  0.1040, -0.0094,  0.0375, -0.0359, -0.0060,  0.0272],\n",
            "        [ 0.0512,  0.0332,  0.0472, -0.0074,  0.0210, -0.0113, -0.0052,  0.0123]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017389558255672455 tensor([[ 0.1427,  0.1255,  0.1425, -0.0093,  0.0507, -0.0465, -0.0105,  0.0428],\n",
            "        [ 0.0989,  0.0988,  0.1051, -0.0070,  0.0353, -0.0391, -0.0080,  0.0321],\n",
            "        [ 0.1098,  0.0979,  0.1156, -0.0117,  0.0422, -0.0396, -0.0071,  0.0273],\n",
            "        [ 0.0631,  0.0391,  0.0573, -0.0097,  0.0259, -0.0126, -0.0067,  0.0137]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01717880181968212 tensor([[ 0.1604,  0.1387,  0.1581, -0.0110,  0.0568, -0.0510, -0.0123,  0.0457],\n",
            "        [ 0.1048,  0.1057,  0.1113, -0.0078,  0.0371, -0.0419, -0.0091,  0.0328],\n",
            "        [ 0.1170,  0.1027,  0.1228, -0.0138,  0.0453, -0.0416, -0.0081,  0.0256],\n",
            "        [ 0.0734,  0.0433,  0.0657, -0.0118,  0.0301, -0.0132, -0.0082,  0.0142]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023367498070001602 tensor([[ 0.0708,  0.0715,  0.0783, -0.0047,  0.0266, -0.0244, -0.0062,  0.0242],\n",
            "        [ 0.0714,  0.0601,  0.0682, -0.0051,  0.0259, -0.0262, -0.0029,  0.0245],\n",
            "        [ 0.0341,  0.0240,  0.0269, -0.0018,  0.0124, -0.0145, -0.0027,  0.0141],\n",
            "        [ 0.0246,  0.0273,  0.0311, -0.0020,  0.0081, -0.0102, -0.0005,  0.0076]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02015969157218933 tensor([[ 0.1115,  0.1132,  0.1239, -0.0082,  0.0422, -0.0374, -0.0109,  0.0353],\n",
            "        [ 0.1201,  0.0988,  0.1129, -0.0093,  0.0437, -0.0431, -0.0052,  0.0389],\n",
            "        [ 0.0570,  0.0367,  0.0416, -0.0030,  0.0211, -0.0249, -0.0049,  0.0238],\n",
            "        [ 0.0442,  0.0501,  0.0575, -0.0039,  0.0143, -0.0182, -0.0008,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01889790967106819 tensor([[ 0.1348,  0.1371,  0.1497, -0.0111,  0.0514, -0.0437, -0.0147,  0.0391],\n",
            "        [ 0.1509,  0.1199,  0.1384, -0.0127,  0.0551, -0.0530, -0.0068,  0.0458],\n",
            "        [ 0.0726,  0.0421,  0.0483, -0.0038,  0.0274, -0.0323, -0.0068,  0.0303],\n",
            "        [ 0.0598,  0.0695,  0.0801, -0.0057,  0.0188, -0.0246, -0.0008,  0.0162]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018413348123431206 tensor([[ 0.1485,  0.1511,  0.1646, -0.0135,  0.0570, -0.0464, -0.0179,  0.0391],\n",
            "        [ 0.1705,  0.1300,  0.1516, -0.0156,  0.0624, -0.0586, -0.0080,  0.0480],\n",
            "        [ 0.0839,  0.0431,  0.0504, -0.0045,  0.0322, -0.0380, -0.0085,  0.0349],\n",
            "        [ 0.0722,  0.0860,  0.0997, -0.0074,  0.0222, -0.0296, -0.0007,  0.0182]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018202470615506172 tensor([[ 0.1570,  0.1596,  0.1732, -0.0156,  0.0606, -0.0472, -0.0208,  0.0371],\n",
            "        [ 0.1838,  0.1338,  0.1577, -0.0183,  0.0674, -0.0616, -0.0089,  0.0477],\n",
            "        [ 0.0928,  0.0418,  0.0500, -0.0050,  0.0363, -0.0427, -0.0101,  0.0385],\n",
            "        [ 0.0820,  0.1001,  0.1165, -0.0089,  0.0246, -0.0336, -0.0004,  0.0190]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021389992907643318 tensor([[ 0.0700,  0.0606,  0.0675, -0.0047,  0.0261, -0.0244, -0.0056,  0.0215],\n",
            "        [ 0.0622,  0.0554,  0.0616, -0.0038,  0.0222, -0.0227, -0.0037,  0.0212],\n",
            "        [ 0.0617,  0.0576,  0.0649, -0.0041,  0.0231, -0.0228, -0.0027,  0.0208],\n",
            "        [ 0.0277,  0.0196,  0.0222, -0.0018,  0.0099, -0.0104, -0.0020,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017815325409173965 tensor([[ 0.1165,  0.0986,  0.1106, -0.0086,  0.0436, -0.0397, -0.0098,  0.0328],\n",
            "        [ 0.1054,  0.0929,  0.1034, -0.0070,  0.0378, -0.0380, -0.0065,  0.0338],\n",
            "        [ 0.1076,  0.1006,  0.1134, -0.0077,  0.0406, -0.0396, -0.0048,  0.0347],\n",
            "        [ 0.0512,  0.0349,  0.0397, -0.0033,  0.0184, -0.0189, -0.0038,  0.0206]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01612882688641548 tensor([[ 0.1468,  0.1205,  0.1359, -0.0117,  0.0552, -0.0490, -0.0132,  0.0374],\n",
            "        [ 0.1340,  0.1157,  0.1291, -0.0095,  0.0480, -0.0474, -0.0087,  0.0399],\n",
            "        [ 0.1400,  0.1308,  0.1476, -0.0107,  0.0530, -0.0514, -0.0063,  0.0429],\n",
            "        [ 0.0711,  0.0467,  0.0536, -0.0047,  0.0256, -0.0257, -0.0055,  0.0281]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015382938086986542 tensor([[ 0.1673,  0.1324,  0.1501, -0.0143,  0.0633, -0.0545, -0.0160,  0.0381],\n",
            "        [ 0.1532,  0.1290,  0.1446, -0.0117,  0.0548, -0.0529, -0.0104,  0.0418],\n",
            "        [ 0.1629,  0.1516,  0.1715, -0.0134,  0.0620, -0.0594, -0.0074,  0.0468],\n",
            "        [ 0.0882,  0.0558,  0.0646, -0.0058,  0.0318, -0.0314, -0.0072,  0.0341]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015042385086417198 tensor([[ 0.1819,  0.1383,  0.1576, -0.0166,  0.0692, -0.0580, -0.0184,  0.0367],\n",
            "        [ 0.1668,  0.1364,  0.1537, -0.0137,  0.0595, -0.0562, -0.0119,  0.0412],\n",
            "        [ 0.1791,  0.1660,  0.1882, -0.0159,  0.0684, -0.0648, -0.0083,  0.0480],\n",
            "        [ 0.1029,  0.0628,  0.0732, -0.0069,  0.0372, -0.0359, -0.0087,  0.0390]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.2934e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020184721797704697 tensor([[ 0.0612,  0.0562,  0.0644, -0.0046,  0.0222, -0.0199, -0.0031,  0.0167],\n",
            "        [ 0.0626,  0.0591,  0.0662, -0.0046,  0.0233, -0.0240, -0.0036,  0.0220],\n",
            "        [ 0.0526,  0.0491,  0.0549, -0.0036,  0.0199, -0.0188, -0.0039,  0.0189],\n",
            "        [ 0.0271,  0.0255,  0.0269, -0.0019,  0.0091, -0.0108, -0.0010,  0.0092]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017041269689798355 tensor([[ 0.1018,  0.0921,  0.1069, -0.0085,  0.0368, -0.0316, -0.0050,  0.0242],\n",
            "        [ 0.1045,  0.0990,  0.1114, -0.0084,  0.0391, -0.0400, -0.0064,  0.0350],\n",
            "        [ 0.0883,  0.0820,  0.0915, -0.0066,  0.0334, -0.0313, -0.0072,  0.0305],\n",
            "        [ 0.0490,  0.0461,  0.0483, -0.0037,  0.0161, -0.0194, -0.0019,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015634316951036453 tensor([[ 0.1287,  0.1141,  0.1342, -0.0118,  0.0465, -0.0381, -0.0063,  0.0259],\n",
            "        [ 0.1313,  0.1244,  0.1406, -0.0116,  0.0495, -0.0500, -0.0084,  0.0415],\n",
            "        [ 0.1119,  0.1032,  0.1151, -0.0092,  0.0424, -0.0392, -0.0098,  0.0368],\n",
            "        [ 0.0669,  0.0630,  0.0655, -0.0054,  0.0217, -0.0262, -0.0026,  0.0206]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01502012275159359 tensor([[ 0.1474,  0.1276,  0.1522, -0.0147,  0.0531, -0.0414, -0.0072,  0.0242],\n",
            "        [ 0.1486,  0.1406,  0.1596, -0.0144,  0.0564, -0.0563, -0.0100,  0.0439],\n",
            "        [ 0.1278,  0.1169,  0.1304, -0.0115,  0.0486, -0.0441, -0.0121,  0.0397],\n",
            "        [ 0.0815,  0.0769,  0.0793, -0.0069,  0.0260, -0.0317, -0.0032,  0.0239]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014734422788023949 tensor([[ 0.1611,  0.1359,  0.1644, -0.0174,  0.0578, -0.0427, -0.0078,  0.0205],\n",
            "        [ 0.1602,  0.1510,  0.1723, -0.0170,  0.0612, -0.0603, -0.0114,  0.0439],\n",
            "        [ 0.1389,  0.1258,  0.1405, -0.0137,  0.0529, -0.0471, -0.0141,  0.0404],\n",
            "        [ 0.0936,  0.0884,  0.0904, -0.0084,  0.0293, -0.0361, -0.0038,  0.0259]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022443369030952454 tensor([[ 0.0713,  0.0657,  0.0739, -0.0054,  0.0267, -0.0255, -0.0053,  0.0245],\n",
            "        [ 0.0646,  0.0574,  0.0622, -0.0033,  0.0224, -0.0243, -0.0040,  0.0244],\n",
            "        [ 0.0578,  0.0499,  0.0585, -0.0052,  0.0226, -0.0206, -0.0033,  0.0190],\n",
            "        [ 0.0305,  0.0254,  0.0284, -0.0017,  0.0115, -0.0113, -0.0014,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018849391490221024 tensor([[ 0.1166,  0.1062,  0.1206, -0.0099,  0.0439, -0.0410, -0.0092,  0.0377],\n",
            "        [ 0.1068,  0.0934,  0.1005, -0.0057,  0.0368, -0.0399, -0.0071,  0.0392],\n",
            "        [ 0.0981,  0.0827,  0.0982, -0.0098,  0.0389, -0.0343, -0.0059,  0.0303],\n",
            "        [ 0.0551,  0.0449,  0.0504, -0.0030,  0.0209, -0.0199, -0.0025,  0.0173]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017351148650050163 tensor([[ 0.1451,  0.1297,  0.1487, -0.0137,  0.0550, -0.0500, -0.0122,  0.0439],\n",
            "        [ 0.1333,  0.1135,  0.1216, -0.0075,  0.0454, -0.0492, -0.0095,  0.0474],\n",
            "        [ 0.1251,  0.1025,  0.1235, -0.0138,  0.0503, -0.0427, -0.0079,  0.0359],\n",
            "        [ 0.0751,  0.0598,  0.0674, -0.0041,  0.0287, -0.0264, -0.0035,  0.0221]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016755547374486923 tensor([[ 0.1639,  0.1433,  0.1659, -0.0171,  0.0625, -0.0554, -0.0146,  0.0462],\n",
            "        [ 0.1507,  0.1243,  0.1324, -0.0090,  0.0507, -0.0549, -0.0115,  0.0517],\n",
            "        [ 0.1435,  0.1137,  0.1393, -0.0174,  0.0585, -0.0478, -0.0095,  0.0379],\n",
            "        [ 0.0914,  0.0713,  0.0808, -0.0051,  0.0351, -0.0313, -0.0044,  0.0250]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016501842066645622 tensor([[ 0.1768,  0.1510,  0.1764, -0.0203,  0.0679, -0.0587, -0.0167,  0.0463],\n",
            "        [ 0.1627,  0.1296,  0.1373, -0.0103,  0.0540, -0.0583, -0.0132,  0.0539],\n",
            "        [ 0.1564,  0.1195,  0.1490, -0.0207,  0.0646, -0.0507, -0.0108,  0.0377],\n",
            "        [ 0.1051,  0.0802,  0.0914, -0.0060,  0.0406, -0.0349, -0.0051,  0.0266]],\n",
            "       device='cuda:0')\n",
            "c tensor([6.5565e-07, 5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.017581317573785782 tensor([[ 0.0607,  0.0519,  0.0591, -0.0037,  0.0215, -0.0205, -0.0028,  0.0190],\n",
            "        [ 0.0721,  0.0702,  0.0775, -0.0043,  0.0262, -0.0248, -0.0041,  0.0244],\n",
            "        [ 0.0510,  0.0440,  0.0502, -0.0034,  0.0191, -0.0197, -0.0027,  0.0190],\n",
            "        [ 0.0268,  0.0260,  0.0292, -0.0025,  0.0099, -0.0127, -0.0016,  0.0115]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014187440276145935 tensor([[ 0.1020,  0.0854,  0.0983, -0.0066,  0.0360, -0.0333, -0.0046,  0.0296],\n",
            "        [ 0.1207,  0.1183,  0.1306, -0.0076,  0.0440, -0.0404, -0.0073,  0.0383],\n",
            "        [ 0.0865,  0.0735,  0.0844, -0.0061,  0.0328, -0.0335, -0.0047,  0.0314],\n",
            "        [ 0.0487,  0.0472,  0.0531, -0.0050,  0.0177, -0.0236, -0.0029,  0.0210]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.012639602646231651 tensor([[ 0.1295,  0.1051,  0.1225, -0.0089,  0.0456, -0.0407, -0.0058,  0.0346],\n",
            "        [ 0.1510,  0.1482,  0.1637, -0.0102,  0.0552, -0.0489, -0.0096,  0.0442],\n",
            "        [ 0.1109,  0.0923,  0.1069, -0.0083,  0.0424, -0.0429, -0.0061,  0.0389],\n",
            "        [ 0.0667,  0.0647,  0.0729, -0.0074,  0.0240, -0.0329, -0.0040,  0.0287]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.011977775022387505 tensor([[ 0.1482,  0.1163,  0.1369, -0.0108,  0.0520, -0.0449, -0.0065,  0.0364],\n",
            "        [ 0.1698,  0.1665,  0.1841, -0.0123,  0.0623, -0.0529, -0.0114,  0.0451],\n",
            "        [ 0.1283,  0.1045,  0.1221, -0.0101,  0.0493, -0.0494, -0.0072,  0.0433],\n",
            "        [ 0.0814,  0.0793,  0.0895, -0.0097,  0.0290, -0.0409, -0.0050,  0.0349]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.011675226502120495 tensor([[ 0.1618,  0.1221,  0.1454, -0.0123,  0.0565, -0.0471, -0.0070,  0.0362],\n",
            "        [ 0.1819,  0.1780,  0.1968, -0.0141,  0.0669, -0.0542, -0.0128,  0.0431],\n",
            "        [ 0.1414,  0.1123,  0.1327, -0.0117,  0.0546, -0.0542, -0.0080,  0.0457],\n",
            "        [ 0.0935,  0.0914,  0.1034, -0.0119,  0.0330, -0.0478, -0.0059,  0.0399]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02072756178677082 tensor([[ 0.0663,  0.0599,  0.0659, -0.0040,  0.0234, -0.0227, -0.0030,  0.0189],\n",
            "        [ 0.0658,  0.0599,  0.0663, -0.0042,  0.0248, -0.0231, -0.0046,  0.0217],\n",
            "        [ 0.0512,  0.0440,  0.0508, -0.0039,  0.0189, -0.0181, -0.0021,  0.0171],\n",
            "        [ 0.0210,  0.0162,  0.0185, -0.0019,  0.0084, -0.0072, -0.0024,  0.0097]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01755565032362938 tensor([[ 0.1115,  0.0994,  0.1098, -0.0073,  0.0391, -0.0370, -0.0050,  0.0285],\n",
            "        [ 0.1128,  0.1022,  0.1131, -0.0076,  0.0431, -0.0389, -0.0084,  0.0351],\n",
            "        [ 0.0886,  0.0749,  0.0872, -0.0071,  0.0329, -0.0308, -0.0037,  0.0280],\n",
            "        [ 0.0388,  0.0291,  0.0334, -0.0036,  0.0156, -0.0131, -0.0048,  0.0178]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.0160493366420269 tensor([[ 0.1413,  0.1233,  0.1368, -0.0100,  0.0492, -0.0454, -0.0061,  0.0318],\n",
            "        [ 0.1446,  0.1294,  0.1432, -0.0102,  0.0559, -0.0486, -0.0115,  0.0419],\n",
            "        [ 0.1153,  0.0955,  0.1124, -0.0098,  0.0430, -0.0391, -0.0050,  0.0341],\n",
            "        [ 0.0539,  0.0393,  0.0454, -0.0051,  0.0219, -0.0179, -0.0071,  0.0246]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015389485284686089 tensor([[ 0.1612,  0.1371,  0.1526, -0.0122,  0.0557, -0.0501, -0.0067,  0.0312],\n",
            "        [ 0.1664,  0.1464,  0.1622, -0.0124,  0.0650, -0.0543, -0.0142,  0.0442],\n",
            "        [ 0.1347,  0.1091,  0.1299, -0.0121,  0.0505, -0.0445, -0.0059,  0.0370],\n",
            "        [ 0.0668,  0.0475,  0.0552, -0.0065,  0.0275, -0.0217, -0.0092,  0.0304]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.0150900948792696 tensor([[ 0.1750,  0.1447,  0.1616, -0.0141,  0.0600, -0.0526, -0.0070,  0.0285],\n",
            "        [ 0.1818,  0.1570,  0.1741, -0.0143,  0.0718, -0.0575, -0.0165,  0.0438],\n",
            "        [ 0.1493,  0.1180,  0.1422, -0.0142,  0.0562, -0.0479, -0.0067,  0.0377],\n",
            "        [ 0.0780,  0.0541,  0.0631, -0.0078,  0.0323, -0.0249, -0.0114,  0.0354]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021061699837446213 tensor([[ 0.0680,  0.0627,  0.0703, -0.0050,  0.0237, -0.0242, -0.0020,  0.0184],\n",
            "        [ 0.0669,  0.0607,  0.0669, -0.0038,  0.0241, -0.0238, -0.0039,  0.0234],\n",
            "        [ 0.0572,  0.0533,  0.0598, -0.0034,  0.0210, -0.0207, -0.0032,  0.0201],\n",
            "        [ 0.0231,  0.0198,  0.0211, -0.0015,  0.0079, -0.0093, -0.0011,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01748541370034218 tensor([[ 0.1144,  0.1046,  0.1183, -0.0094,  0.0396, -0.0400, -0.0030,  0.0271],\n",
            "        [ 0.1143,  0.1034,  0.1137, -0.0068,  0.0414, -0.0401, -0.0068,  0.0383],\n",
            "        [ 0.0974,  0.0904,  0.1015, -0.0061,  0.0359, -0.0348, -0.0057,  0.0326],\n",
            "        [ 0.0425,  0.0361,  0.0383, -0.0027,  0.0143, -0.0170, -0.0020,  0.0154]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015801437199115753 tensor([[ 0.1454,  0.1309,  0.1496, -0.0131,  0.0501, -0.0497, -0.0032,  0.0293],\n",
            "        [ 0.1456,  0.1302,  0.1432, -0.0091,  0.0529, -0.0501, -0.0090,  0.0464],\n",
            "        [ 0.1246,  0.1149,  0.1294, -0.0082,  0.0460, -0.0438, -0.0076,  0.0394],\n",
            "        [ 0.0589,  0.0499,  0.0527, -0.0038,  0.0195, -0.0233, -0.0027,  0.0207]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01506449468433857 tensor([[ 0.1665,  0.1473,  0.1698, -0.0163,  0.0569, -0.0556, -0.0030,  0.0275],\n",
            "        [ 0.1663,  0.1464,  0.1609, -0.0110,  0.0605, -0.0560, -0.0107,  0.0500],\n",
            "        [ 0.1435,  0.1313,  0.1485, -0.0099,  0.0530, -0.0495, -0.0090,  0.0424],\n",
            "        [ 0.0727,  0.0616,  0.0646, -0.0047,  0.0237, -0.0284, -0.0032,  0.0249]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01473173126578331 tensor([[ 0.1815,  0.1573,  0.1831, -0.0192,  0.0616, -0.0593, -0.0025,  0.0234],\n",
            "        [ 0.1804,  0.1560,  0.1713, -0.0126,  0.0658, -0.0593, -0.0120,  0.0509],\n",
            "        [ 0.1570,  0.1425,  0.1619, -0.0113,  0.0581, -0.0530, -0.0102,  0.0429],\n",
            "        [ 0.0845,  0.0715,  0.0746, -0.0055,  0.0271, -0.0327, -0.0036,  0.0281]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.9802e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019268957898020744 tensor([[ 0.0728,  0.0691,  0.0784, -0.0055,  0.0261, -0.0245, -0.0049,  0.0234],\n",
            "        [ 0.0615,  0.0535,  0.0591, -0.0036,  0.0224, -0.0234, -0.0037,  0.0214],\n",
            "        [ 0.0503,  0.0446,  0.0508, -0.0037,  0.0186, -0.0184, -0.0024,  0.0165],\n",
            "        [ 0.0266,  0.0192,  0.0212, -0.0023,  0.0098, -0.0101, -0.0013,  0.0089]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015959683805704117 tensor([[ 0.1199,  0.1131,  0.1299, -0.0101,  0.0428, -0.0391, -0.0086,  0.0359],\n",
            "        [ 0.1025,  0.0875,  0.0966, -0.0062,  0.0374, -0.0391, -0.0065,  0.0341],\n",
            "        [ 0.0854,  0.0748,  0.0855, -0.0067,  0.0315, -0.0310, -0.0042,  0.0263],\n",
            "        [ 0.0485,  0.0340,  0.0374, -0.0043,  0.0179, -0.0182, -0.0025,  0.0155]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.014542494900524616 tensor([[ 0.1499,  0.1397,  0.1622, -0.0141,  0.0532, -0.0471, -0.0113,  0.0414],\n",
            "        [ 0.1289,  0.1071,  0.1182, -0.0083,  0.0471, -0.0490, -0.0086,  0.0405],\n",
            "        [ 0.1095,  0.0944,  0.1086, -0.0092,  0.0404, -0.0393, -0.0055,  0.0315],\n",
            "        [ 0.0667,  0.0452,  0.0497, -0.0062,  0.0247, -0.0247, -0.0034,  0.0204]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.013957390561699867 tensor([[ 0.1696,  0.1557,  0.1827, -0.0175,  0.0598, -0.0514, -0.0135,  0.0430],\n",
            "        [ 0.1466,  0.1180,  0.1303, -0.0101,  0.0537, -0.0554, -0.0103,  0.0431],\n",
            "        [ 0.1263,  0.1071,  0.1239, -0.0113,  0.0465, -0.0448, -0.0065,  0.0336],\n",
            "        [ 0.0820,  0.0538,  0.0591, -0.0080,  0.0305, -0.0300, -0.0042,  0.0239]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013701025396585464 tensor([[ 0.1829,  0.1651,  0.1960, -0.0207,  0.0641, -0.0533, -0.0154,  0.0422],\n",
            "        [ 0.1591,  0.1236,  0.1367, -0.0116,  0.0583, -0.0598, -0.0118,  0.0435],\n",
            "        [ 0.1382,  0.1152,  0.1341, -0.0132,  0.0509, -0.0484, -0.0072,  0.0337],\n",
            "        [ 0.0952,  0.0604,  0.0663, -0.0097,  0.0355, -0.0344, -0.0050,  0.0264]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, 2.6882e-05, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021147891879081726 tensor([[ 0.0595,  0.0569,  0.0642, -0.0038,  0.0223, -0.0206, -0.0039,  0.0178],\n",
            "        [ 0.0675,  0.0613,  0.0687, -0.0046,  0.0243, -0.0247, -0.0028,  0.0223],\n",
            "        [ 0.0560,  0.0519,  0.0589, -0.0044,  0.0216, -0.0188, -0.0054,  0.0213],\n",
            "        [ 0.0286,  0.0193,  0.0210, -0.0023,  0.0095, -0.0125, -0.0007,  0.0104]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017797039821743965 tensor([[ 0.1011,  0.0964,  0.1099, -0.0071,  0.0381, -0.0340, -0.0068,  0.0275],\n",
            "        [ 0.1129,  0.1017,  0.1141, -0.0082,  0.0407, -0.0410, -0.0049,  0.0353],\n",
            "        [ 0.0935,  0.0859,  0.0980, -0.0080,  0.0365, -0.0307, -0.0101,  0.0343],\n",
            "        [ 0.0537,  0.0352,  0.0383, -0.0045,  0.0176, -0.0236, -0.0012,  0.0192]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016264591366052628 tensor([[ 0.1300,  0.1229,  0.1416, -0.0100,  0.0492, -0.0423, -0.0090,  0.0318],\n",
            "        [ 0.1417,  0.1259,  0.1413, -0.0108,  0.0511, -0.0509, -0.0063,  0.0414],\n",
            "        [ 0.1173,  0.1064,  0.1221, -0.0110,  0.0464, -0.0371, -0.0142,  0.0410],\n",
            "        [ 0.0758,  0.0484,  0.0525, -0.0064,  0.0245, -0.0333, -0.0016,  0.0265]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015595054253935814 tensor([[ 0.1505,  0.1408,  0.1638, -0.0126,  0.0572, -0.0474, -0.0107,  0.0326],\n",
            "        [ 0.1603,  0.1400,  0.1572, -0.0130,  0.0578, -0.0568, -0.0075,  0.0434],\n",
            "        [ 0.1326,  0.1184,  0.1369, -0.0136,  0.0531, -0.0403, -0.0178,  0.0439],\n",
            "        [ 0.0952,  0.0593,  0.0640, -0.0082,  0.0304, -0.0419, -0.0019,  0.0326]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01528098713606596 tensor([[ 0.1655,  0.1530,  0.1797, -0.0150,  0.0632, -0.0503, -0.0121,  0.0313],\n",
            "        [ 0.1728,  0.1480,  0.1664, -0.0147,  0.0622, -0.0604, -0.0083,  0.0430],\n",
            "        [ 0.1429,  0.1254,  0.1462, -0.0160,  0.0580, -0.0414, -0.0212,  0.0445],\n",
            "        [ 0.1123,  0.0682,  0.0734, -0.0099,  0.0355, -0.0496, -0.0021,  0.0376]],\n",
            "       device='cuda:0')\n",
            "c tensor([0.0007, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.016988757997751236 tensor([[ 0.0551,  0.0505,  0.0574, -0.0035,  0.0202, -0.0185, -0.0035,  0.0177],\n",
            "        [ 0.0495,  0.0500,  0.0565, -0.0033,  0.0183, -0.0171, -0.0031,  0.0167],\n",
            "        [ 0.0439,  0.0387,  0.0426, -0.0032,  0.0151, -0.0166, -0.0021,  0.0148],\n",
            "        [ 0.0246,  0.0154,  0.0202, -0.0026,  0.0090, -0.0093, -0.0003,  0.0074]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014798779040575027 tensor([[ 0.0927,  0.0839,  0.0963, -0.0064,  0.0342, -0.0303, -0.0061,  0.0276],\n",
            "        [ 0.0799,  0.0819,  0.0930, -0.0060,  0.0298, -0.0267, -0.0053,  0.0252],\n",
            "        [ 0.0725,  0.0628,  0.0684, -0.0056,  0.0242, -0.0274, -0.0035,  0.0231],\n",
            "        [ 0.0442,  0.0260,  0.0353, -0.0051,  0.0162, -0.0164, -0.0005,  0.0122]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013865664601325989 tensor([[ 0.1191,  0.1061,  0.1229, -0.0089,  0.0441, -0.0378, -0.0079,  0.0327],\n",
            "        [ 0.0986,  0.1022,  0.1170, -0.0082,  0.0369, -0.0317, -0.0069,  0.0285],\n",
            "        [ 0.0914,  0.0775,  0.0837, -0.0076,  0.0296, -0.0345, -0.0044,  0.0274],\n",
            "        [ 0.0602,  0.0332,  0.0467, -0.0075,  0.0220, -0.0219, -0.0004,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.013449542224407196 tensor([[ 0.1383,  0.1210,  0.1415, -0.0111,  0.0514, -0.0426, -0.0094,  0.0348],\n",
            "        [ 0.1103,  0.1157,  0.1334, -0.0101,  0.0414, -0.0339, -0.0082,  0.0287],\n",
            "        [ 0.1044,  0.0867,  0.0926, -0.0092,  0.0327, -0.0394, -0.0050,  0.0292],\n",
            "        [ 0.0734,  0.0379,  0.0556, -0.0098,  0.0268, -0.0262, -0.0002,  0.0172]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013249238952994347 tensor([[ 0.1530,  0.1311,  0.1548, -0.0130,  0.0571, -0.0456, -0.0106,  0.0350],\n",
            "        [ 0.1177,  0.1248,  0.1451, -0.0119,  0.0443, -0.0343, -0.0093,  0.0270],\n",
            "        [ 0.1138,  0.0924,  0.0976, -0.0107,  0.0343, -0.0428, -0.0054,  0.0296],\n",
            "        [ 0.0847,  0.0407,  0.0625, -0.0121,  0.0309, -0.0296,  0.0002,  0.0180]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.017492074519395828 tensor([[ 0.0562,  0.0519,  0.0568, -0.0035,  0.0219, -0.0199, -0.0043,  0.0182],\n",
            "        [ 0.0525,  0.0457,  0.0528, -0.0033,  0.0194, -0.0182, -0.0037,  0.0182],\n",
            "        [ 0.0434,  0.0351,  0.0396, -0.0034,  0.0149, -0.0157, -0.0020,  0.0139],\n",
            "        [ 0.0219,  0.0229,  0.0257, -0.0023,  0.0075, -0.0066, -0.0020,  0.0062]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015265297144651413 tensor([[ 0.0961,  0.0883,  0.0967, -0.0065,  0.0379, -0.0334, -0.0079,  0.0293],\n",
            "        [ 0.0894,  0.0765,  0.0891, -0.0059,  0.0331, -0.0305, -0.0067,  0.0296],\n",
            "        [ 0.0738,  0.0575,  0.0653, -0.0061,  0.0252, -0.0263, -0.0035,  0.0222],\n",
            "        [ 0.0391,  0.0414,  0.0466, -0.0045,  0.0132, -0.0110, -0.0038,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.014189427718520164 tensor([[ 0.1248,  0.1136,  0.1244, -0.0091,  0.0498, -0.0425, -0.0110,  0.0358],\n",
            "        [ 0.1145,  0.0957,  0.1127, -0.0079,  0.0425, -0.0383, -0.0092,  0.0360],\n",
            "        [ 0.0954,  0.0715,  0.0816, -0.0083,  0.0324, -0.0335, -0.0046,  0.0266],\n",
            "        [ 0.0526,  0.0565,  0.0639, -0.0066,  0.0174, -0.0138, -0.0054,  0.0118]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.013672852888703346 tensor([[ 0.1460,  0.1313,  0.1437, -0.0115,  0.0589, -0.0487, -0.0137,  0.0392],\n",
            "        [ 0.1318,  0.1071,  0.1275, -0.0096,  0.0490, -0.0430, -0.0113,  0.0390],\n",
            "        [ 0.1115,  0.0801,  0.0921, -0.0102,  0.0376, -0.0384, -0.0054,  0.0286],\n",
            "        [ 0.0631,  0.0690,  0.0782, -0.0086,  0.0205, -0.0153, -0.0070,  0.0122]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013414418324828148 tensor([[ 0.1621,  0.1439,  0.1574, -0.0136,  0.0662, -0.0530, -0.0161,  0.0406],\n",
            "        [ 0.1439,  0.1135,  0.1367, -0.0110,  0.0535, -0.0456, -0.0132,  0.0399],\n",
            "        [ 0.1239,  0.0852,  0.0988, -0.0119,  0.0414, -0.0418, -0.0060,  0.0289],\n",
            "        [ 0.0713,  0.0794,  0.0904, -0.0105,  0.0228, -0.0158, -0.0084,  0.0116]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.9802e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019806355237960815 tensor([[ 0.0611,  0.0567,  0.0622, -0.0034,  0.0228, -0.0231, -0.0037,  0.0205],\n",
            "        [ 0.0663,  0.0644,  0.0738, -0.0046,  0.0256, -0.0219, -0.0043,  0.0205],\n",
            "        [ 0.0571,  0.0496,  0.0541, -0.0040,  0.0198, -0.0222, -0.0025,  0.0204],\n",
            "        [ 0.0224,  0.0123,  0.0158, -0.0016,  0.0088, -0.0076, -0.0014,  0.0080]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016581866890192032 tensor([[ 0.1018,  0.0934,  0.1027, -0.0061,  0.0383, -0.0381, -0.0063,  0.0320],\n",
            "        [ 0.1057,  0.1028,  0.1191, -0.0083,  0.0414, -0.0334, -0.0073,  0.0292],\n",
            "        [ 0.0974,  0.0839,  0.0908, -0.0071,  0.0333, -0.0381, -0.0042,  0.0336],\n",
            "        [ 0.0412,  0.0207,  0.0275, -0.0029,  0.0165, -0.0135, -0.0027,  0.0140]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01525796763598919 tensor([[ 0.1288,  0.1164,  0.1280, -0.0082,  0.0488, -0.0479, -0.0082,  0.0381],\n",
            "        [ 0.1279,  0.1238,  0.1453, -0.0112,  0.0508, -0.0381, -0.0096,  0.0304],\n",
            "        [ 0.1250,  0.1064,  0.1143, -0.0095,  0.0421, -0.0490, -0.0053,  0.0415],\n",
            "        [ 0.0573,  0.0263,  0.0364, -0.0041,  0.0233, -0.0180, -0.0039,  0.0187]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014726758934557438 tensor([[ 0.1476,  0.1309,  0.1439, -0.0099,  0.0563, -0.0546, -0.0098,  0.0409],\n",
            "        [ 0.1406,  0.1352,  0.1610, -0.0137,  0.0566, -0.0391, -0.0114,  0.0274],\n",
            "        [ 0.1443,  0.1212,  0.1291, -0.0115,  0.0478, -0.0567, -0.0059,  0.0458],\n",
            "        [ 0.0714,  0.0298,  0.0430, -0.0052,  0.0294, -0.0217, -0.0051,  0.0222]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014494170434772968 tensor([[ 0.1613,  0.1402,  0.1540, -0.0114,  0.0620, -0.0594, -0.0110,  0.0417],\n",
            "        [ 0.1482,  0.1412,  0.1709, -0.0159,  0.0605, -0.0379, -0.0129,  0.0221],\n",
            "        [ 0.1580,  0.1308,  0.1380, -0.0132,  0.0513, -0.0622, -0.0063,  0.0480],\n",
            "        [ 0.0838,  0.0318,  0.0481, -0.0063,  0.0349, -0.0246, -0.0063,  0.0250]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.0862e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01756436564028263 tensor([[ 0.0537,  0.0519,  0.0583, -0.0037,  0.0193, -0.0191, -0.0042,  0.0181],\n",
            "        [ 0.0640,  0.0609,  0.0695, -0.0054,  0.0240, -0.0227, -0.0034,  0.0183],\n",
            "        [ 0.0612,  0.0515,  0.0591, -0.0045,  0.0222, -0.0223, -0.0025,  0.0208],\n",
            "        [ 0.0249,  0.0198,  0.0232, -0.0020,  0.0097, -0.0087, -0.0012,  0.0073]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014484315179288387 tensor([[ 0.0866,  0.0833,  0.0941, -0.0065,  0.0311, -0.0299, -0.0074,  0.0272],\n",
            "        [ 0.1035,  0.0984,  0.1132, -0.0100,  0.0390, -0.0359, -0.0057,  0.0256],\n",
            "        [ 0.1051,  0.0863,  0.0997, -0.0083,  0.0378, -0.0380, -0.0042,  0.0341],\n",
            "        [ 0.0447,  0.0345,  0.0409, -0.0037,  0.0176, -0.0151, -0.0022,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013210251927375793 tensor([[ 0.1070,  0.1021,  0.1160, -0.0088,  0.0383, -0.0361, -0.0100,  0.0314],\n",
            "        [ 0.1274,  0.1203,  0.1399, -0.0140,  0.0482, -0.0429, -0.0072,  0.0261],\n",
            "        [ 0.1359,  0.1083,  0.1261, -0.0115,  0.0485, -0.0486, -0.0055,  0.0419],\n",
            "        [ 0.0606,  0.0456,  0.0547, -0.0052,  0.0240, -0.0198, -0.0030,  0.0148]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.012684619054198265 tensor([[ 0.1204,  0.1138,  0.1300, -0.0107,  0.0431, -0.0395, -0.0123,  0.0328],\n",
            "        [ 0.1423,  0.1331,  0.1566, -0.0176,  0.0541, -0.0463, -0.0083,  0.0227],\n",
            "        [ 0.1579,  0.1219,  0.1432, -0.0143,  0.0558, -0.0559, -0.0063,  0.0460],\n",
            "        [ 0.0735,  0.0539,  0.0655, -0.0065,  0.0293, -0.0232, -0.0036,  0.0162]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.012448729947209358 tensor([[ 0.1296,  0.1212,  0.1392, -0.0124,  0.0463, -0.0414, -0.0143,  0.0327],\n",
            "        [ 0.1519,  0.1407,  0.1676, -0.0210,  0.0580, -0.0476, -0.0092,  0.0169],\n",
            "        [ 0.1743,  0.1300,  0.1542, -0.0169,  0.0611, -0.0609, -0.0069,  0.0478],\n",
            "        [ 0.0842,  0.0602,  0.0740, -0.0077,  0.0338, -0.0257, -0.0041,  0.0165]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023129835724830627 tensor([[ 0.0616,  0.0589,  0.0657, -0.0039,  0.0224, -0.0207, -0.0044,  0.0201],\n",
            "        [ 0.0703,  0.0658,  0.0743, -0.0042,  0.0254, -0.0250, -0.0032,  0.0225],\n",
            "        [ 0.0565,  0.0481,  0.0534, -0.0034,  0.0191, -0.0220, -0.0016,  0.0192],\n",
            "        [ 0.0205,  0.0158,  0.0199, -0.0018,  0.0092, -0.0059, -0.0027,  0.0075]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019729504361748695 tensor([[ 0.1028,  0.0982,  0.1101, -0.0072,  0.0374, -0.0332, -0.0079,  0.0313],\n",
            "        [ 0.1165,  0.1089,  0.1234, -0.0075,  0.0420, -0.0406, -0.0053,  0.0344],\n",
            "        [ 0.0970,  0.0816,  0.0904, -0.0060,  0.0324, -0.0380, -0.0026,  0.0318],\n",
            "        [ 0.0369,  0.0271,  0.0351, -0.0033,  0.0168, -0.0098, -0.0053,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01822926476597786 tensor([[ 0.1298,  0.1233,  0.1387, -0.0099,  0.0473, -0.0403, -0.0107,  0.0368],\n",
            "        [ 0.1449,  0.1345,  0.1533, -0.0099,  0.0520, -0.0493, -0.0065,  0.0385],\n",
            "        [ 0.1253,  0.1035,  0.1146, -0.0080,  0.0411, -0.0492, -0.0030,  0.0391],\n",
            "        [ 0.0503,  0.0353,  0.0469, -0.0047,  0.0232, -0.0124, -0.0078,  0.0165]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01761121302843094 tensor([[ 0.1480,  0.1394,  0.1575, -0.0123,  0.0540, -0.0440, -0.0131,  0.0389],\n",
            "        [ 0.1627,  0.1494,  0.1715, -0.0119,  0.0581, -0.0538, -0.0072,  0.0381],\n",
            "        [ 0.1454,  0.1178,  0.1303, -0.0095,  0.0467, -0.0572, -0.0030,  0.0431],\n",
            "        [ 0.0614,  0.0413,  0.0564, -0.0060,  0.0286, -0.0140, -0.0102,  0.0190]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01734086498618126 tensor([[ 0.1609,  0.1500,  0.1699, -0.0144,  0.0588, -0.0458, -0.0152,  0.0390],\n",
            "        [ 0.1744,  0.1580,  0.1828, -0.0136,  0.0618, -0.0558, -0.0076,  0.0350],\n",
            "        [ 0.1603,  0.1272,  0.1405, -0.0108,  0.0503, -0.0631, -0.0027,  0.0448],\n",
            "        [ 0.0708,  0.0456,  0.0641, -0.0071,  0.0334, -0.0148, -0.0126,  0.0205]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023920267820358276 tensor([[ 0.0597,  0.0536,  0.0597, -0.0049,  0.0205, -0.0213, -0.0028,  0.0167],\n",
            "        [ 0.0619,  0.0598,  0.0655, -0.0039,  0.0225, -0.0235, -0.0035,  0.0241],\n",
            "        [ 0.0424,  0.0362,  0.0423, -0.0030,  0.0160, -0.0136, -0.0043,  0.0156],\n",
            "        [ 0.0265,  0.0215,  0.0251, -0.0016,  0.0098, -0.0115, -0.0006,  0.0080]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021232781931757927 tensor([[ 0.1017,  0.0898,  0.1006, -0.0093,  0.0346, -0.0355, -0.0046,  0.0257],\n",
            "        [ 0.1016,  0.0985,  0.1071, -0.0068,  0.0370, -0.0387, -0.0063,  0.0391],\n",
            "        [ 0.0690,  0.0567,  0.0673, -0.0054,  0.0261, -0.0212, -0.0078,  0.0243],\n",
            "        [ 0.0481,  0.0387,  0.0457, -0.0030,  0.0177, -0.0208, -0.0010,  0.0137]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02006598934531212 tensor([[ 0.1314,  0.1137,  0.1280, -0.0133,  0.0442, -0.0449, -0.0059,  0.0298],\n",
            "        [ 0.1263,  0.1225,  0.1323, -0.0089,  0.0460, -0.0482, -0.0084,  0.0480],\n",
            "        [ 0.0857,  0.0675,  0.0816, -0.0073,  0.0325, -0.0249, -0.0107,  0.0285],\n",
            "        [ 0.0660,  0.0526,  0.0629, -0.0042,  0.0241, -0.0285, -0.0011,  0.0176]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019553592428565025 tensor([[ 0.1533,  0.1295,  0.1466, -0.0169,  0.0510, -0.0513, -0.0068,  0.0308],\n",
            "        [ 0.1420,  0.1375,  0.1474, -0.0105,  0.0518, -0.0541, -0.0103,  0.0532],\n",
            "        [ 0.0967,  0.0727,  0.0897, -0.0090,  0.0366, -0.0263, -0.0132,  0.0302],\n",
            "        [ 0.0809,  0.0640,  0.0773, -0.0053,  0.0294, -0.0350, -0.0011,  0.0200]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01930873468518257 tensor([[ 0.1701,  0.1401,  0.1593, -0.0204,  0.0559, -0.0557, -0.0074,  0.0299],\n",
            "        [ 0.1523,  0.1471,  0.1564, -0.0119,  0.0555, -0.0580, -0.0120,  0.0561],\n",
            "        [ 0.1046,  0.0747,  0.0943, -0.0105,  0.0396, -0.0264, -0.0156,  0.0305],\n",
            "        [ 0.0933,  0.0733,  0.0894, -0.0063,  0.0338, -0.0404, -0.0010,  0.0214]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025423437356948853 tensor([[ 0.0637,  0.0678,  0.0769, -0.0060,  0.0239, -0.0211, -0.0055,  0.0183],\n",
            "        [ 0.0732,  0.0605,  0.0676, -0.0047,  0.0257, -0.0283, -0.0037,  0.0240],\n",
            "        [ 0.0609,  0.0522,  0.0605, -0.0045,  0.0226, -0.0208, -0.0021,  0.0172],\n",
            "        [ 0.0258,  0.0182,  0.0200, -0.0016,  0.0096, -0.0092, -0.0018,  0.0092]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021789517253637314 tensor([[ 0.0971,  0.1051,  0.1206, -0.0110,  0.0366, -0.0307, -0.0094,  0.0243],\n",
            "        [ 0.1207,  0.0960,  0.1073, -0.0082,  0.0420, -0.0468, -0.0062,  0.0370],\n",
            "        [ 0.1038,  0.0875,  0.1025, -0.0082,  0.0387, -0.0343, -0.0036,  0.0261],\n",
            "        [ 0.0477,  0.0325,  0.0357, -0.0029,  0.0179, -0.0167, -0.0034,  0.0164]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020422406494617462 tensor([[ 0.1133,  0.1244,  0.1443, -0.0153,  0.0429, -0.0337, -0.0125,  0.0238],\n",
            "        [ 0.1510,  0.1143,  0.1278, -0.0109,  0.0521, -0.0587, -0.0080,  0.0429],\n",
            "        [ 0.1323,  0.1090,  0.1295, -0.0112,  0.0493, -0.0421, -0.0043,  0.0284],\n",
            "        [ 0.0664,  0.0441,  0.0483, -0.0040,  0.0253, -0.0228, -0.0049,  0.0220]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019918859004974365 tensor([[ 0.1207,  0.1344,  0.1576, -0.0191,  0.0460, -0.0335, -0.0151,  0.0203],\n",
            "        [ 0.1715,  0.1227,  0.1373, -0.0130,  0.0586, -0.0668, -0.0094,  0.0449],\n",
            "        [ 0.1514,  0.1214,  0.1465, -0.0138,  0.0565, -0.0461, -0.0045,  0.0265],\n",
            "        [ 0.0827,  0.0536,  0.0586, -0.0049,  0.0317, -0.0279, -0.0063,  0.0264]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01970776729285717 tensor([[ 0.1236,  0.1396,  0.1655, -0.0227,  0.0473, -0.0317, -0.0174,  0.0153],\n",
            "        [ 0.1867,  0.1256,  0.1407, -0.0148,  0.0631, -0.0729, -0.0105,  0.0448],\n",
            "        [ 0.1649,  0.1283,  0.1575, -0.0161,  0.0616, -0.0479, -0.0045,  0.0222],\n",
            "        [ 0.0971,  0.0614,  0.0670, -0.0056,  0.0375, -0.0321, -0.0077,  0.0299]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020061403512954712 tensor([[ 0.0512,  0.0454,  0.0507, -0.0038,  0.0187, -0.0195, -0.0035,  0.0172],\n",
            "        [ 0.0718,  0.0691,  0.0773, -0.0038,  0.0244, -0.0238, -0.0024,  0.0229],\n",
            "        [ 0.0486,  0.0426,  0.0480, -0.0029,  0.0191, -0.0188, -0.0033,  0.0191],\n",
            "        [ 0.0256,  0.0201,  0.0243, -0.0027,  0.0099, -0.0078, -0.0017,  0.0059]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017086211591959 tensor([[ 0.0875,  0.0763,  0.0859, -0.0073,  0.0321, -0.0328, -0.0062,  0.0274],\n",
            "        [ 0.1193,  0.1145,  0.1284, -0.0068,  0.0399, -0.0382, -0.0039,  0.0354],\n",
            "        [ 0.0819,  0.0707,  0.0797, -0.0048,  0.0327, -0.0318, -0.0058,  0.0318],\n",
            "        [ 0.0462,  0.0353,  0.0435, -0.0053,  0.0180, -0.0132, -0.0033,  0.0091]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01575285941362381 tensor([[ 0.1135,  0.0969,  0.1101, -0.0103,  0.0419, -0.0421, -0.0083,  0.0332],\n",
            "        [ 0.1497,  0.1429,  0.1607, -0.0091,  0.0491, -0.0461, -0.0046,  0.0406],\n",
            "        [ 0.1038,  0.0878,  0.0990, -0.0060,  0.0420, -0.0402, -0.0079,  0.0397],\n",
            "        [ 0.0631,  0.0470,  0.0591, -0.0078,  0.0248, -0.0168, -0.0046,  0.0104]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015171430073678493 tensor([[ 0.1328,  0.1108,  0.1267, -0.0131,  0.0492, -0.0488, -0.0100,  0.0362],\n",
            "        [ 0.1698,  0.1608,  0.1813, -0.0109,  0.0545, -0.0499, -0.0049,  0.0415],\n",
            "        [ 0.1183,  0.0977,  0.1104, -0.0069,  0.0487, -0.0458, -0.0095,  0.0446],\n",
            "        [ 0.0770,  0.0561,  0.0718, -0.0101,  0.0305, -0.0190, -0.0058,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01490150298923254 tensor([[ 0.1476,  0.1201,  0.1383, -0.0158,  0.0548, -0.0537, -0.0116,  0.0375],\n",
            "        [ 0.1835,  0.1722,  0.1948, -0.0125,  0.0576, -0.0512, -0.0049,  0.0396],\n",
            "        [ 0.1281,  0.1031,  0.1167, -0.0074,  0.0536, -0.0496, -0.0108,  0.0474],\n",
            "        [ 0.0887,  0.0631,  0.0824, -0.0123,  0.0354, -0.0203, -0.0069,  0.0089]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02053164690732956 tensor([[ 0.0729,  0.0659,  0.0738, -0.0047,  0.0256, -0.0262, -0.0039,  0.0241],\n",
            "        [ 0.0669,  0.0624,  0.0697, -0.0047,  0.0257, -0.0243, -0.0039,  0.0216],\n",
            "        [ 0.0559,  0.0517,  0.0571, -0.0038,  0.0214, -0.0208, -0.0037,  0.0198],\n",
            "        [ 0.0275,  0.0184,  0.0226, -0.0019,  0.0099, -0.0096, -0.0006,  0.0085]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016801100224256516 tensor([[ 0.1205,  0.1074,  0.1208, -0.0085,  0.0420, -0.0426, -0.0067,  0.0373],\n",
            "        [ 0.1112,  0.1038,  0.1163, -0.0085,  0.0435, -0.0397, -0.0069,  0.0331],\n",
            "        [ 0.0962,  0.0887,  0.0978, -0.0071,  0.0374, -0.0358, -0.0071,  0.0329],\n",
            "        [ 0.0509,  0.0326,  0.0408, -0.0036,  0.0185, -0.0174, -0.0010,  0.0149]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015162261202931404 tensor([[ 0.1506,  0.1314,  0.1486, -0.0116,  0.0520, -0.0526, -0.0086,  0.0435],\n",
            "        [ 0.1375,  0.1277,  0.1437, -0.0116,  0.0548, -0.0480, -0.0091,  0.0368],\n",
            "        [ 0.1238,  0.1135,  0.1250, -0.0099,  0.0488, -0.0458, -0.0100,  0.0403],\n",
            "        [ 0.0710,  0.0437,  0.0557, -0.0051,  0.0258, -0.0237, -0.0012,  0.0198]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01450518798083067 tensor([[ 0.1706,  0.1451,  0.1647, -0.0142,  0.0583, -0.0588, -0.0100,  0.0457],\n",
            "        [ 0.1526,  0.1404,  0.1589, -0.0142,  0.0621, -0.0519, -0.0108,  0.0357],\n",
            "        [ 0.1426,  0.1298,  0.1428, -0.0124,  0.0570, -0.0523, -0.0125,  0.0439],\n",
            "        [ 0.0885,  0.0525,  0.0680, -0.0065,  0.0323, -0.0289, -0.0014,  0.0235]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014226964674890041 tensor([[ 0.1845,  0.1526,  0.1741, -0.0165,  0.0625, -0.0628, -0.0111,  0.0458],\n",
            "        [ 0.1614,  0.1468,  0.1671, -0.0165,  0.0671, -0.0534, -0.0121,  0.0319],\n",
            "        [ 0.1557,  0.1406,  0.1546, -0.0147,  0.0633, -0.0567, -0.0148,  0.0451],\n",
            "        [ 0.1039,  0.0594,  0.0783, -0.0078,  0.0381, -0.0332, -0.0015,  0.0263]],\n",
            "       device='cuda:0')\n",
            "c tensor([7.4506e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.016412945464253426 tensor([[ 0.0701,  0.0629,  0.0701, -0.0043,  0.0261, -0.0252, -0.0042,  0.0242],\n",
            "        [ 0.0632,  0.0548,  0.0619, -0.0038,  0.0227, -0.0225, -0.0030,  0.0213],\n",
            "        [ 0.0352,  0.0313,  0.0351, -0.0025,  0.0132, -0.0127, -0.0028,  0.0128],\n",
            "        [ 0.0271,  0.0245,  0.0272, -0.0021,  0.0089, -0.0104, -0.0005,  0.0092]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.013400610536336899 tensor([[ 0.1228,  0.1099,  0.1231, -0.0080,  0.0460, -0.0435, -0.0075,  0.0404],\n",
            "        [ 0.1113,  0.0956,  0.1082, -0.0069,  0.0399, -0.0392, -0.0054,  0.0360],\n",
            "        [ 0.0595,  0.0523,  0.0586, -0.0045,  0.0225, -0.0212, -0.0051,  0.0208],\n",
            "        [ 0.0489,  0.0441,  0.0490, -0.0040,  0.0157, -0.0185, -0.0008,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.011770523153245449 tensor([[ 0.1612,  0.1431,  0.1610, -0.0112,  0.0608, -0.0562, -0.0102,  0.0504],\n",
            "        [ 0.1456,  0.1232,  0.1397, -0.0094,  0.0521, -0.0507, -0.0072,  0.0447],\n",
            "        [ 0.0762,  0.0660,  0.0741, -0.0061,  0.0291, -0.0267, -0.0070,  0.0254],\n",
            "        [ 0.0664,  0.0600,  0.0667, -0.0058,  0.0210, -0.0248, -0.0010,  0.0207]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.010954318568110466 tensor([[ 0.1894,  0.1659,  0.1876, -0.0140,  0.0718, -0.0649, -0.0124,  0.0561],\n",
            "        [ 0.1698,  0.1409,  0.1603, -0.0114,  0.0605, -0.0583, -0.0085,  0.0489],\n",
            "        [ 0.0878,  0.0749,  0.0843, -0.0074,  0.0338, -0.0301, -0.0087,  0.0277],\n",
            "        [ 0.0807,  0.0729,  0.0812, -0.0075,  0.0251, -0.0298, -0.0010,  0.0241]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.010550089180469513 tensor([[ 0.2104,  0.1815,  0.2063, -0.0164,  0.0803, -0.0709, -0.0142,  0.0589],\n",
            "        [ 0.1872,  0.1518,  0.1734, -0.0131,  0.0664, -0.0632, -0.0094,  0.0501],\n",
            "        [ 0.0960,  0.0807,  0.0911, -0.0086,  0.0372, -0.0321, -0.0101,  0.0284],\n",
            "        [ 0.0924,  0.0835,  0.0931, -0.0092,  0.0282, -0.0337, -0.0009,  0.0264]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021811701357364655 tensor([[ 0.0578,  0.0491,  0.0547, -0.0039,  0.0210, -0.0202, -0.0036,  0.0187],\n",
            "        [ 0.0711,  0.0696,  0.0765, -0.0044,  0.0257, -0.0251, -0.0041,  0.0254],\n",
            "        [ 0.0515,  0.0433,  0.0508, -0.0035,  0.0202, -0.0184, -0.0040,  0.0197],\n",
            "        [ 0.0325,  0.0215,  0.0237, -0.0015,  0.0107, -0.0124, -0.0005,  0.0087]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0186576209962368 tensor([[ 0.0949,  0.0779,  0.0877, -0.0072,  0.0344, -0.0321, -0.0059,  0.0278],\n",
            "        [ 0.1150,  0.1134,  0.1241, -0.0078,  0.0414, -0.0398, -0.0072,  0.0394],\n",
            "        [ 0.0857,  0.0701,  0.0831, -0.0062,  0.0341, -0.0303, -0.0073,  0.0322],\n",
            "        [ 0.0598,  0.0380,  0.0420, -0.0026,  0.0196, -0.0226, -0.0007,  0.0148]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017400650307536125 tensor([[ 0.1196,  0.0943,  0.1072, -0.0100,  0.0434, -0.0391, -0.0076,  0.0317],\n",
            "        [ 0.1407,  0.1392,  0.1517, -0.0105,  0.0504, -0.0475, -0.0096,  0.0459],\n",
            "        [ 0.1080,  0.0854,  0.1025, -0.0085,  0.0435, -0.0376, -0.0101,  0.0395],\n",
            "        [ 0.0831,  0.0509,  0.0562, -0.0034,  0.0270, -0.0309, -0.0007,  0.0188]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016899393871426582 tensor([[ 0.1375,  0.1037,  0.1191, -0.0125,  0.0499, -0.0434, -0.0089,  0.0327],\n",
            "        [ 0.1562,  0.1549,  0.1680, -0.0128,  0.0555, -0.0513, -0.0115,  0.0480],\n",
            "        [ 0.1228,  0.0933,  0.1137, -0.0104,  0.0501, -0.0420, -0.0126,  0.0438],\n",
            "        [ 0.1032,  0.0609,  0.0672, -0.0041,  0.0333, -0.0379, -0.0006,  0.0213]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016675082966685295 tensor([[ 0.1512,  0.1090,  0.1263, -0.0148,  0.0549, -0.0462, -0.0099,  0.0320],\n",
            "        [ 0.1659,  0.1647,  0.1777, -0.0149,  0.0584, -0.0528, -0.0132,  0.0478],\n",
            "        [ 0.1330,  0.0968,  0.1199, -0.0120,  0.0550, -0.0447, -0.0148,  0.0462],\n",
            "        [ 0.1209,  0.0686,  0.0758, -0.0047,  0.0388, -0.0438, -0.0003,  0.0226]],\n",
            "       device='cuda:0')\n",
            "c tensor([4.1723e-07, -0.0000e+00, -0.0000e+00, 1.7881e-07, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[4, 4, 6, 2, 4, 9, 2, 9, 6, 10, 1, 4, 4, 1, 6, 12, 6, 4, 11, 4, 4, 1, 5, 2, 4, 4, 12, 4, 4, 4, 6, 9, 9, 6, 9, 12, 4, 6, 11, 12, 6, 4, 9, 11, 4, 13, 4, 2, 4, 13, 6, 6, 9, 14, 6, 6, 1, 6, 6, 11, 4, 2, 6, 11, 6, 6, 6, 3, 6, 4, 4, 4, 6, 11, 6, 11, 4, 11, 6, 11, 6, 6, 10, 14, 4, 6, 6, 11, 4, 6, 11, 4, 2, 4, 14, 7, 4, 6, 8, 8, 4, 6, 4, 2, 4, 6, 2, 4, 4, 11, 6, 11, 4, 4, 3, 4, 4, 2, 11, 6, 4, 4, 10, 8, 6, 4, 3, 4, 6, 4, 6, 6, 6, 8, 6, 6, 4, 7, 4, 4, 4, 12, 11, 6, 8, 4, 4, 2, 4, 6, 11, 13, 4, 6, 11, 6, 4, 5]\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01868290640413761 tensor([[ 0.0673,  0.0591,  0.0647, -0.0039,  0.0236, -0.0243, -0.0030,  0.0202],\n",
            "        [ 0.0626,  0.0523,  0.0595, -0.0038,  0.0218, -0.0213, -0.0029,  0.0226],\n",
            "        [ 0.0434,  0.0417,  0.0486, -0.0034,  0.0171, -0.0141, -0.0027,  0.0145],\n",
            "        [ 0.0237,  0.0172,  0.0194, -0.0019,  0.0086, -0.0084, -0.0013,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015861503779888153 tensor([[ 0.1167,  0.1008,  0.1107, -0.0073,  0.0407, -0.0414, -0.0051,  0.0325],\n",
            "        [ 0.1046,  0.0847,  0.0968, -0.0067,  0.0363, -0.0350, -0.0050,  0.0369],\n",
            "        [ 0.0694,  0.0665,  0.0787, -0.0060,  0.0278, -0.0214, -0.0047,  0.0213],\n",
            "        [ 0.0437,  0.0309,  0.0350, -0.0037,  0.0160, -0.0150, -0.0025,  0.0143]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01459513045847416 tensor([[ 0.1528,  0.1292,  0.1422, -0.0102,  0.0529, -0.0533, -0.0064,  0.0393],\n",
            "        [ 0.1323,  0.1031,  0.1183, -0.0090,  0.0456, -0.0432, -0.0067,  0.0455],\n",
            "        [ 0.0841,  0.0802,  0.0966, -0.0082,  0.0342, -0.0242, -0.0060,  0.0229],\n",
            "        [ 0.0607,  0.0419,  0.0477, -0.0053,  0.0223, -0.0203, -0.0036,  0.0189]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014032043516635895 tensor([[ 0.1798,  0.1483,  0.1634, -0.0128,  0.0618, -0.0617, -0.0074,  0.0424],\n",
            "        [ 0.1515,  0.1129,  0.1304, -0.0109,  0.0518, -0.0482, -0.0079,  0.0506],\n",
            "        [ 0.0923,  0.0877,  0.1077, -0.0101,  0.0382, -0.0244, -0.0070,  0.0217],\n",
            "        [ 0.0754,  0.0508,  0.0581, -0.0069,  0.0277, -0.0245, -0.0047,  0.0224]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01376296766102314 tensor([[ 0.2007,  0.1611,  0.1778, -0.0151,  0.0684, -0.0677, -0.0080,  0.0430],\n",
            "        [ 0.1657,  0.1177,  0.1368, -0.0125,  0.0560, -0.0513, -0.0090,  0.0538],\n",
            "        [ 0.0969,  0.0916,  0.1150, -0.0119,  0.0409, -0.0232, -0.0078,  0.0188],\n",
            "        [ 0.0881,  0.0579,  0.0666, -0.0084,  0.0324, -0.0279, -0.0057,  0.0249]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.016906393691897392 tensor([[ 0.0642,  0.0640,  0.0734, -0.0056,  0.0251, -0.0225, -0.0056,  0.0218],\n",
            "        [ 0.0552,  0.0512,  0.0568, -0.0039,  0.0192, -0.0203, -0.0032,  0.0166],\n",
            "        [ 0.0607,  0.0579,  0.0654, -0.0044,  0.0218, -0.0220, -0.0025,  0.0191],\n",
            "        [ 0.0277,  0.0198,  0.0221, -0.0021,  0.0098, -0.0108, -0.0019,  0.0107]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.013608411885797977 tensor([[ 0.1044,  0.1042,  0.1207, -0.0102,  0.0415, -0.0359, -0.0100,  0.0334],\n",
            "        [ 0.0885,  0.0814,  0.0907, -0.0068,  0.0306, -0.0319, -0.0054,  0.0234],\n",
            "        [ 0.1023,  0.0978,  0.1109, -0.0080,  0.0364, -0.0365, -0.0041,  0.0299],\n",
            "        [ 0.0507,  0.0350,  0.0393, -0.0039,  0.0180, -0.0195, -0.0037,  0.0192]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.012229081243276596 tensor([[ 0.1294,  0.1290,  0.1508, -0.0142,  0.0523, -0.0436, -0.0136,  0.0390],\n",
            "        [ 0.1084,  0.0984,  0.1103, -0.0091,  0.0372, -0.0380, -0.0069,  0.0244],\n",
            "        [ 0.1301,  0.1243,  0.1416, -0.0111,  0.0458, -0.0457, -0.0051,  0.0347],\n",
            "        [ 0.0701,  0.0467,  0.0527, -0.0055,  0.0249, -0.0265, -0.0054,  0.0258]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.011646158993244171 tensor([[ 0.1454,  0.1445,  0.1706, -0.0177,  0.0599, -0.0479, -0.0168,  0.0411],\n",
            "        [ 0.1207,  0.1079,  0.1215, -0.0111,  0.0409, -0.0410, -0.0080,  0.0221],\n",
            "        [ 0.1489,  0.1421,  0.1627, -0.0138,  0.0518, -0.0514, -0.0057,  0.0356],\n",
            "        [ 0.0864,  0.0556,  0.0631, -0.0070,  0.0306, -0.0323, -0.0069,  0.0309]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.011381595395505428 tensor([[ 0.1559,  0.1544,  0.1840, -0.0209,  0.0655, -0.0503, -0.0198,  0.0411],\n",
            "        [ 0.1287,  0.1129,  0.1280, -0.0128,  0.0431, -0.0422, -0.0090,  0.0179],\n",
            "        [ 0.1620,  0.1542,  0.1775, -0.0162,  0.0557, -0.0547, -0.0060,  0.0341],\n",
            "        [ 0.1004,  0.0624,  0.0712, -0.0083,  0.0355, -0.0370, -0.0083,  0.0350]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.7418e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01567218080163002 tensor([[ 0.0491,  0.0504,  0.0567, -0.0037,  0.0183, -0.0174, -0.0029,  0.0146],\n",
            "        [ 0.0532,  0.0471,  0.0537, -0.0026,  0.0190, -0.0188, -0.0035,  0.0185],\n",
            "        [ 0.0495,  0.0469,  0.0516, -0.0025,  0.0181, -0.0203, -0.0023,  0.0184],\n",
            "        [ 0.0269,  0.0187,  0.0235, -0.0025,  0.0099, -0.0089, -0.0010,  0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.013357413932681084 tensor([[ 0.0781,  0.0810,  0.0918, -0.0066,  0.0294, -0.0268, -0.0048,  0.0207],\n",
            "        [ 0.0871,  0.0750,  0.0864, -0.0047,  0.0310, -0.0300, -0.0062,  0.0285],\n",
            "        [ 0.0820,  0.0780,  0.0854, -0.0041,  0.0300, -0.0343, -0.0039,  0.0299],\n",
            "        [ 0.0486,  0.0324,  0.0419, -0.0049,  0.0178, -0.0153, -0.0018,  0.0108]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01242062821984291 tensor([[ 0.0956,  0.0999,  0.1141, -0.0090,  0.0364, -0.0319, -0.0063,  0.0221],\n",
            "        [ 0.1092,  0.0907,  0.1058, -0.0063,  0.0386, -0.0364, -0.0082,  0.0331],\n",
            "        [ 0.1033,  0.0987,  0.1075, -0.0051,  0.0378, -0.0441, -0.0049,  0.0369],\n",
            "        [ 0.0663,  0.0425,  0.0565, -0.0070,  0.0243, -0.0199, -0.0024,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.012024059891700745 tensor([[ 0.1064,  0.1120,  0.1288, -0.0112,  0.0409, -0.0343, -0.0074,  0.0208],\n",
            "        [ 0.1244,  0.0994,  0.1174, -0.0077,  0.0437, -0.0400, -0.0099,  0.0347],\n",
            "        [ 0.1176,  0.1128,  0.1221, -0.0057,  0.0429, -0.0512, -0.0056,  0.0410],\n",
            "        [ 0.0811,  0.0497,  0.0681, -0.0091,  0.0297, -0.0231, -0.0028,  0.0133]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01184084266424179 tensor([[ 0.1131,  0.1199,  0.1388, -0.0131,  0.0439, -0.0353, -0.0083,  0.0179],\n",
            "        [ 0.1355,  0.1038,  0.1244, -0.0090,  0.0473, -0.0419, -0.0114,  0.0345],\n",
            "        [ 0.1272,  0.1224,  0.1317, -0.0060,  0.0464, -0.0566, -0.0060,  0.0433],\n",
            "        [ 0.0935,  0.0548,  0.0773, -0.0110,  0.0342, -0.0253, -0.0031,  0.0125]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01745520532131195 tensor([[ 0.0570,  0.0528,  0.0580, -0.0029,  0.0193, -0.0203, -0.0029,  0.0178],\n",
            "        [ 0.0623,  0.0574,  0.0665, -0.0048,  0.0243, -0.0205, -0.0047,  0.0207],\n",
            "        [ 0.0427,  0.0371,  0.0401, -0.0023,  0.0144, -0.0161, -0.0018,  0.0148],\n",
            "        [ 0.0218,  0.0209,  0.0255, -0.0025,  0.0095, -0.0073, -0.0021,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014850377105176449 tensor([[ 0.0951,  0.0871,  0.0959, -0.0053,  0.0318, -0.0331, -0.0049,  0.0274],\n",
            "        [ 0.1050,  0.0962,  0.1125, -0.0091,  0.0414, -0.0334, -0.0086,  0.0326],\n",
            "        [ 0.0726,  0.0622,  0.0668, -0.0040,  0.0242, -0.0272, -0.0031,  0.0241],\n",
            "        [ 0.0393,  0.0377,  0.0465, -0.0048,  0.0176, -0.0126, -0.0040,  0.0121]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013670659624040127 tensor([[ 0.1204,  0.1085,  0.1196, -0.0071,  0.0396, -0.0411, -0.0063,  0.0320],\n",
            "        [ 0.1324,  0.1197,  0.1418, -0.0127,  0.0528, -0.0403, -0.0118,  0.0377],\n",
            "        [ 0.0934,  0.0787,  0.0838, -0.0053,  0.0308, -0.0347, -0.0040,  0.0294],\n",
            "        [ 0.0534,  0.0512,  0.0643, -0.0070,  0.0244, -0.0163, -0.0059,  0.0154]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01316459197551012 tensor([[ 0.1378,  0.1219,  0.1343, -0.0086,  0.0445, -0.0460, -0.0074,  0.0335],\n",
            "        [ 0.1499,  0.1334,  0.1602, -0.0160,  0.0605, -0.0435, -0.0145,  0.0386],\n",
            "        [ 0.1082,  0.0896,  0.0947, -0.0063,  0.0352, -0.0398, -0.0047,  0.0321],\n",
            "        [ 0.0648,  0.0623,  0.0793, -0.0090,  0.0302, -0.0188, -0.0078,  0.0173]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.012935271486639977 tensor([[ 0.1504,  0.1302,  0.1435, -0.0098,  0.0477, -0.0490, -0.0083,  0.0331],\n",
            "        [ 0.1615,  0.1410,  0.1719, -0.0190,  0.0660, -0.0443, -0.0168,  0.0369],\n",
            "        [ 0.1193,  0.0968,  0.1016, -0.0073,  0.0382, -0.0433, -0.0052,  0.0332],\n",
            "        [ 0.0741,  0.0714,  0.0922, -0.0110,  0.0352, -0.0205, -0.0095,  0.0183]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021969206631183624 tensor([[ 0.0734,  0.0701,  0.0785, -0.0054,  0.0265, -0.0279, -0.0032,  0.0225],\n",
            "        [ 0.0495,  0.0371,  0.0416, -0.0033,  0.0169, -0.0191, -0.0023,  0.0181],\n",
            "        [ 0.0450,  0.0425,  0.0483, -0.0026,  0.0160, -0.0142, -0.0030,  0.0137],\n",
            "        [ 0.0282,  0.0226,  0.0269, -0.0030,  0.0110, -0.0120, -0.0016,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01914672926068306 tensor([[ 0.1203,  0.1146,  0.1290, -0.0098,  0.0434, -0.0456, -0.0052,  0.0336],\n",
            "        [ 0.0815,  0.0577,  0.0649, -0.0060,  0.0277, -0.0315, -0.0039,  0.0289],\n",
            "        [ 0.0741,  0.0694,  0.0793, -0.0045,  0.0260, -0.0222, -0.0052,  0.0205],\n",
            "        [ 0.0519,  0.0410,  0.0496, -0.0058,  0.0203, -0.0219, -0.0031,  0.0164]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01797226443886757 tensor([[ 0.1502,  0.1420,  0.1607, -0.0135,  0.0542, -0.0569, -0.0064,  0.0378],\n",
            "        [ 0.1027,  0.0680,  0.0768, -0.0082,  0.0345, -0.0397, -0.0052,  0.0353],\n",
            "        [ 0.0933,  0.0864,  0.0994, -0.0060,  0.0322, -0.0263, -0.0069,  0.0230],\n",
            "        [ 0.0717,  0.0561,  0.0687, -0.0084,  0.0281, -0.0301, -0.0044,  0.0215]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017463380470871925 tensor([[ 0.1700,  0.1592,  0.1811, -0.0167,  0.0613, -0.0643, -0.0071,  0.0377],\n",
            "        [ 0.1177,  0.0723,  0.0822, -0.0100,  0.0392, -0.0455, -0.0062,  0.0391],\n",
            "        [ 0.1066,  0.0975,  0.1129, -0.0073,  0.0361, -0.0280, -0.0084,  0.0229],\n",
            "        [ 0.0884,  0.0683,  0.0847, -0.0109,  0.0348, -0.0370, -0.0056,  0.0252]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017216810956597328 tensor([[ 0.1837,  0.1702,  0.1945, -0.0196,  0.0662, -0.0694, -0.0076,  0.0352],\n",
            "        [ 0.1290,  0.0731,  0.0837, -0.0117,  0.0424, -0.0499, -0.0070,  0.0415],\n",
            "        [ 0.1163,  0.1048,  0.1223, -0.0085,  0.0386, -0.0282, -0.0096,  0.0211],\n",
            "        [ 0.1023,  0.0781,  0.0980, -0.0133,  0.0405, -0.0428, -0.0066,  0.0276]],\n",
            "       device='cuda:0')\n",
            "c tensor([3.2187e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.017276151105761528 tensor([[ 0.0525,  0.0473,  0.0509, -0.0026,  0.0190, -0.0193, -0.0029,  0.0178],\n",
            "        [ 0.0574,  0.0531,  0.0620, -0.0038,  0.0222, -0.0184, -0.0047,  0.0184],\n",
            "        [ 0.0420,  0.0379,  0.0409, -0.0028,  0.0147, -0.0177, -0.0014,  0.0155],\n",
            "        [ 0.0225,  0.0185,  0.0222, -0.0023,  0.0082, -0.0063, -0.0016,  0.0057]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01502971351146698 tensor([[ 0.0881,  0.0782,  0.0840, -0.0046,  0.0320, -0.0318, -0.0049,  0.0281],\n",
            "        [ 0.0952,  0.0870,  0.1030, -0.0068,  0.0374, -0.0290, -0.0086,  0.0281],\n",
            "        [ 0.0701,  0.0626,  0.0670, -0.0049,  0.0243, -0.0301, -0.0022,  0.0251],\n",
            "        [ 0.0402,  0.0327,  0.0397, -0.0043,  0.0145, -0.0103, -0.0029,  0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.014057204127311707 tensor([[ 0.1124,  0.0976,  0.1046, -0.0062,  0.0409, -0.0398, -0.0063,  0.0338],\n",
            "        [ 0.1194,  0.1071,  0.1290, -0.0092,  0.0475, -0.0341, -0.0119,  0.0317],\n",
            "        [ 0.0893,  0.0787,  0.0836, -0.0066,  0.0306, -0.0389, -0.0026,  0.0307],\n",
            "        [ 0.0544,  0.0437,  0.0537, -0.0062,  0.0194, -0.0128, -0.0040,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.013641325756907463 tensor([[ 0.1295,  0.1098,  0.1172, -0.0075,  0.0471, -0.0452, -0.0075,  0.0367],\n",
            "        [ 0.1353,  0.1188,  0.1457, -0.0113,  0.0545, -0.0360, -0.0148,  0.0316],\n",
            "        [ 0.1028,  0.0896,  0.0945, -0.0080,  0.0349, -0.0455, -0.0027,  0.0338],\n",
            "        [ 0.0658,  0.0523,  0.0651, -0.0079,  0.0232, -0.0142, -0.0050,  0.0107]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013448933139443398 tensor([[ 0.1422,  0.1174,  0.1246, -0.0086,  0.0518, -0.0488, -0.0084,  0.0378],\n",
            "        [ 0.1463,  0.1253,  0.1569, -0.0131,  0.0597, -0.0357, -0.0174,  0.0293],\n",
            "        [ 0.1127,  0.0970,  0.1015, -0.0091,  0.0379, -0.0506, -0.0026,  0.0354],\n",
            "        [ 0.0751,  0.0590,  0.0744, -0.0096,  0.0263, -0.0147, -0.0058,  0.0099]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023758605122566223 tensor([[ 4.4937e-02,  4.5624e-02,  5.0087e-02, -3.7456e-03,  1.7481e-02,\n",
            "         -1.7691e-02, -4.3511e-03,  1.6193e-02],\n",
            "        [ 5.0011e-02,  4.7531e-02,  5.0735e-02, -2.8014e-03,  1.6031e-02,\n",
            "         -2.1687e-02, -1.4472e-03,  1.9073e-02],\n",
            "        [ 4.6577e-02,  4.0512e-02,  4.9858e-02, -4.1676e-03,  1.9255e-02,\n",
            "         -1.3866e-02, -4.1628e-03,  1.4105e-02],\n",
            "        [ 2.4452e-02,  1.2941e-02,  1.5335e-02, -1.2070e-03,  7.5388e-03,\n",
            "         -9.8610e-03,  9.7752e-05,  8.7786e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02182493545114994 tensor([[ 0.0747,  0.0760,  0.0835, -0.0071,  0.0293, -0.0293, -0.0079,  0.0259],\n",
            "        [ 0.0768,  0.0725,  0.0759, -0.0045,  0.0235, -0.0348, -0.0021,  0.0291],\n",
            "        [ 0.0731,  0.0615,  0.0781, -0.0074,  0.0311, -0.0199, -0.0072,  0.0192],\n",
            "        [ 0.0460,  0.0232,  0.0278, -0.0023,  0.0139, -0.0184,  0.0004,  0.0160]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02112138830125332 tensor([[ 0.0953,  0.0971,  0.1066, -0.0101,  0.0378, -0.0373, -0.0111,  0.0320],\n",
            "        [ 0.0911,  0.0854,  0.0872, -0.0057,  0.0264, -0.0432, -0.0023,  0.0344],\n",
            "        [ 0.0885,  0.0715,  0.0942, -0.0102,  0.0388, -0.0216, -0.0096,  0.0193],\n",
            "        [ 0.0650,  0.0313,  0.0378, -0.0032,  0.0193, -0.0258,  0.0009,  0.0221]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02082284539937973 tensor([[ 0.1101,  0.1124,  0.1232, -0.0129,  0.0442, -0.0431, -0.0140,  0.0358],\n",
            "        [ 0.0992,  0.0921,  0.0915, -0.0065,  0.0270, -0.0493, -0.0022,  0.0374],\n",
            "        [ 0.0982,  0.0758,  0.1040, -0.0127,  0.0443, -0.0210, -0.0117,  0.0167],\n",
            "        [ 0.0819,  0.0376,  0.0457, -0.0040,  0.0239, -0.0324,  0.0014,  0.0271]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020670484751462936 tensor([[ 0.1211,  0.1237,  0.1354, -0.0156,  0.0491, -0.0473, -0.0167,  0.0379],\n",
            "        [ 0.1039,  0.0956,  0.0923, -0.0072,  0.0263, -0.0542, -0.0021,  0.0391],\n",
            "        [ 0.1049,  0.0771,  0.1104, -0.0150,  0.0487, -0.0192, -0.0135,  0.0129],\n",
            "        [ 0.0970,  0.0422,  0.0519, -0.0048,  0.0279, -0.0382,  0.0021,  0.0314]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.6093e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02494804933667183 tensor([[ 0.0678,  0.0667,  0.0740, -0.0055,  0.0248, -0.0250, -0.0032,  0.0209],\n",
            "        [ 0.0607,  0.0539,  0.0617, -0.0034,  0.0217, -0.0206, -0.0038,  0.0208],\n",
            "        [ 0.0506,  0.0418,  0.0478, -0.0034,  0.0189, -0.0187, -0.0029,  0.0158],\n",
            "        [ 0.0273,  0.0253,  0.0281, -0.0025,  0.0103, -0.0106, -0.0015,  0.0087]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02181665226817131 tensor([[ 0.1091,  0.1075,  0.1197, -0.0101,  0.0400, -0.0398, -0.0052,  0.0304],\n",
            "        [ 0.1004,  0.0877,  0.1013, -0.0059,  0.0356, -0.0331, -0.0066,  0.0323],\n",
            "        [ 0.0849,  0.0677,  0.0783, -0.0060,  0.0320, -0.0308, -0.0050,  0.0243],\n",
            "        [ 0.0494,  0.0461,  0.0513, -0.0048,  0.0186, -0.0189, -0.0028,  0.0149]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020540492609143257 tensor([[ 0.1345,  0.1322,  0.1476, -0.0140,  0.0495, -0.0486, -0.0066,  0.0335],\n",
            "        [ 0.1254,  0.1070,  0.1249, -0.0077,  0.0439, -0.0397, -0.0085,  0.0375],\n",
            "        [ 0.1084,  0.0831,  0.0972, -0.0081,  0.0413, -0.0386, -0.0067,  0.0281],\n",
            "        [ 0.0674,  0.0634,  0.0705, -0.0069,  0.0254, -0.0256, -0.0038,  0.0191]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02001393772661686 tensor([[ 0.1508,  0.1476,  0.1652, -0.0175,  0.0558, -0.0539, -0.0075,  0.0329],\n",
            "        [ 0.1415,  0.1174,  0.1385, -0.0090,  0.0488, -0.0429, -0.0100,  0.0390],\n",
            "        [ 0.1254,  0.0920,  0.1092, -0.0099,  0.0482, -0.0438, -0.0081,  0.0289],\n",
            "        [ 0.0822,  0.0779,  0.0867, -0.0089,  0.0309, -0.0308, -0.0047,  0.0218]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019772477447986603 tensor([[ 0.1618,  0.1575,  0.1765, -0.0207,  0.0601, -0.0572, -0.0083,  0.0302],\n",
            "        [ 0.1523,  0.1225,  0.1463, -0.0101,  0.0518, -0.0440, -0.0112,  0.0383],\n",
            "        [ 0.1383,  0.0970,  0.1169, -0.0115,  0.0536, -0.0472, -0.0092,  0.0280],\n",
            "        [ 0.0943,  0.0900,  0.1003, -0.0108,  0.0355, -0.0350, -0.0054,  0.0231]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02118675783276558 tensor([[ 0.0607,  0.0562,  0.0615, -0.0034,  0.0219, -0.0211, -0.0040,  0.0198],\n",
            "        [ 0.0469,  0.0386,  0.0439, -0.0036,  0.0166, -0.0176, -0.0022,  0.0168],\n",
            "        [ 0.0504,  0.0549,  0.0620, -0.0031,  0.0196, -0.0155, -0.0042,  0.0153],\n",
            "        [ 0.0295,  0.0195,  0.0224, -0.0028,  0.0101, -0.0120, -0.0010,  0.0088]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018610449507832527 tensor([[ 0.1043,  0.0957,  0.1048, -0.0064,  0.0375, -0.0354, -0.0073,  0.0322],\n",
            "        [ 0.0766,  0.0606,  0.0692, -0.0065,  0.0273, -0.0288, -0.0036,  0.0265],\n",
            "        [ 0.0806,  0.0908,  0.1030, -0.0054,  0.0317, -0.0232, -0.0076,  0.0219],\n",
            "        [ 0.0542,  0.0342,  0.0396, -0.0054,  0.0183, -0.0218, -0.0019,  0.0151]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017464064061641693 tensor([[ 0.1354,  0.1226,  0.1344, -0.0089,  0.0486, -0.0448, -0.0099,  0.0395],\n",
            "        [ 0.0957,  0.0719,  0.0828, -0.0088,  0.0341, -0.0359, -0.0046,  0.0318],\n",
            "        [ 0.0977,  0.1142,  0.1298, -0.0070,  0.0388, -0.0259, -0.0102,  0.0228],\n",
            "        [ 0.0751,  0.0454,  0.0529, -0.0079,  0.0251, -0.0298, -0.0026,  0.0194]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016944456845521927 tensor([[ 0.1581,  0.1409,  0.1544, -0.0111,  0.0566, -0.0510, -0.0123,  0.0434],\n",
            "        [ 0.1086,  0.0771,  0.0896, -0.0109,  0.0388, -0.0407, -0.0053,  0.0346],\n",
            "        [ 0.1068,  0.1299,  0.1480, -0.0083,  0.0430, -0.0255, -0.0125,  0.0203],\n",
            "        [ 0.0931,  0.0537,  0.0632, -0.0103,  0.0308, -0.0366, -0.0031,  0.0222]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016689885407686234 tensor([[ 0.1751,  0.1533,  0.1679, -0.0131,  0.0626, -0.0551, -0.0144,  0.0451],\n",
            "        [ 0.1180,  0.0787,  0.0925, -0.0128,  0.0421, -0.0440, -0.0058,  0.0359],\n",
            "        [ 0.1111,  0.1408,  0.1610, -0.0093,  0.0453, -0.0233, -0.0145,  0.0157],\n",
            "        [ 0.1087,  0.0598,  0.0712, -0.0126,  0.0356, -0.0423, -0.0036,  0.0239]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01929095759987831 tensor([[ 0.0472,  0.0429,  0.0497, -0.0030,  0.0173, -0.0171, -0.0032,  0.0167],\n",
            "        [ 0.0561,  0.0557,  0.0629, -0.0044,  0.0204, -0.0195, -0.0039,  0.0167],\n",
            "        [ 0.0497,  0.0455,  0.0512, -0.0039,  0.0184, -0.0183, -0.0021,  0.0165],\n",
            "        [ 0.0306,  0.0213,  0.0241, -0.0025,  0.0099, -0.0113, -0.0014,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016895171254873276 tensor([[ 0.0774,  0.0690,  0.0811, -0.0054,  0.0285, -0.0276, -0.0056,  0.0260],\n",
            "        [ 0.0913,  0.0912,  0.1036, -0.0080,  0.0329, -0.0307, -0.0069,  0.0240],\n",
            "        [ 0.0809,  0.0734,  0.0826, -0.0072,  0.0300, -0.0295, -0.0035,  0.0253],\n",
            "        [ 0.0563,  0.0379,  0.0430, -0.0048,  0.0178, -0.0204, -0.0026,  0.0172]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01590387523174286 tensor([[ 0.0976,  0.0850,  0.1015, -0.0074,  0.0360, -0.0342, -0.0075,  0.0313],\n",
            "        [ 0.1131,  0.1136,  0.1298, -0.0111,  0.0405, -0.0365, -0.0093,  0.0255],\n",
            "        [ 0.1010,  0.0903,  0.1020, -0.0101,  0.0375, -0.0364, -0.0044,  0.0292],\n",
            "        [ 0.0780,  0.0509,  0.0579, -0.0069,  0.0242, -0.0278, -0.0036,  0.0226]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01546949427574873 tensor([[ 0.1119,  0.0949,  0.1151, -0.0091,  0.0414, -0.0386, -0.0090,  0.0342],\n",
            "        [ 0.1270,  0.1280,  0.1471, -0.0138,  0.0450, -0.0392, -0.0113,  0.0234],\n",
            "        [ 0.1146,  0.1008,  0.1143, -0.0127,  0.0426, -0.0407, -0.0051,  0.0304],\n",
            "        [ 0.0964,  0.0610,  0.0695, -0.0089,  0.0294, -0.0338, -0.0044,  0.0264]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015259181149303913 tensor([[ 0.1225,  0.1010,  0.1246, -0.0107,  0.0454, -0.0416, -0.0104,  0.0356],\n",
            "        [ 0.1360,  0.1376,  0.1591, -0.0164,  0.0478, -0.0399, -0.0130,  0.0193],\n",
            "        [ 0.1240,  0.1073,  0.1222, -0.0151,  0.0461, -0.0433, -0.0057,  0.0299],\n",
            "        [ 0.1123,  0.0686,  0.0785, -0.0107,  0.0336, -0.0388, -0.0052,  0.0290]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, 5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022213954478502274 tensor([[ 0.0654,  0.0634,  0.0719, -0.0049,  0.0240, -0.0233, -0.0042,  0.0230],\n",
            "        [ 0.0533,  0.0438,  0.0482, -0.0038,  0.0182, -0.0205, -0.0025,  0.0182],\n",
            "        [ 0.0506,  0.0525,  0.0601, -0.0038,  0.0193, -0.0173, -0.0045,  0.0178],\n",
            "        [ 0.0294,  0.0175,  0.0200, -0.0030,  0.0096, -0.0110, -0.0008,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0193814504891634 tensor([[ 0.1051,  0.1017,  0.1161, -0.0088,  0.0387, -0.0370, -0.0071,  0.0358],\n",
            "        [ 0.0849,  0.0665,  0.0733, -0.0065,  0.0288, -0.0326, -0.0039,  0.0270],\n",
            "        [ 0.0808,  0.0857,  0.0986, -0.0067,  0.0309, -0.0271, -0.0079,  0.0269],\n",
            "        [ 0.0535,  0.0297,  0.0342, -0.0057,  0.0171, -0.0194, -0.0012,  0.0136]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018336057662963867 tensor([[ 0.1288,  0.1241,  0.1425, -0.0120,  0.0477, -0.0448, -0.0094,  0.0426],\n",
            "        [ 0.1048,  0.0775,  0.0857, -0.0086,  0.0352, -0.0401, -0.0047,  0.0305],\n",
            "        [ 0.0982,  0.1068,  0.1234, -0.0089,  0.0377, -0.0322, -0.0106,  0.0308],\n",
            "        [ 0.0736,  0.0381,  0.0442, -0.0083,  0.0230, -0.0261, -0.0015,  0.0170]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017936697229743004 tensor([[ 0.1434,  0.1373,  0.1583, -0.0148,  0.0534, -0.0494, -0.0112,  0.0459],\n",
            "        [ 0.1186,  0.0823,  0.0914, -0.0105,  0.0394, -0.0450, -0.0052,  0.0312],\n",
            "        [ 0.1079,  0.1205,  0.1399, -0.0107,  0.0416, -0.0345, -0.0128,  0.0316],\n",
            "        [ 0.0908,  0.0438,  0.0513, -0.0109,  0.0278, -0.0314, -0.0017,  0.0189]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01776697300374508 tensor([[ 0.1525,  0.1449,  0.1679, -0.0173,  0.0571, -0.0520, -0.0128,  0.0473],\n",
            "        [ 0.1289,  0.0837,  0.0934, -0.0122,  0.0424, -0.0485, -0.0055,  0.0304],\n",
            "        [ 0.1129,  0.1298,  0.1514, -0.0122,  0.0437, -0.0351, -0.0148,  0.0306],\n",
            "        [ 0.1058,  0.0474,  0.0562, -0.0133,  0.0318, -0.0358, -0.0017,  0.0197]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.1260e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, 3.5763e-07, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018054194748401642 tensor([[ 0.0598,  0.0570,  0.0627, -0.0036,  0.0212, -0.0215, -0.0022,  0.0172],\n",
            "        [ 0.0634,  0.0580,  0.0657, -0.0039,  0.0232, -0.0240, -0.0031,  0.0223],\n",
            "        [ 0.0485,  0.0422,  0.0478, -0.0036,  0.0180, -0.0173, -0.0024,  0.0148],\n",
            "        [ 0.0236,  0.0191,  0.0229, -0.0018,  0.0091, -0.0074, -0.0018,  0.0074]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01516630407422781 tensor([[ 0.1002,  0.0951,  0.1051, -0.0065,  0.0353, -0.0353, -0.0036,  0.0260],\n",
            "        [ 0.1073,  0.0974,  0.1108, -0.0070,  0.0392, -0.0405, -0.0053,  0.0364],\n",
            "        [ 0.0816,  0.0694,  0.0790, -0.0064,  0.0303, -0.0285, -0.0041,  0.0226],\n",
            "        [ 0.0427,  0.0341,  0.0414, -0.0033,  0.0166, -0.0127, -0.0034,  0.0125]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013855049386620522 tensor([[ 0.1272,  0.1197,  0.1327, -0.0090,  0.0447, -0.0438, -0.0045,  0.0294],\n",
            "        [ 0.1362,  0.1221,  0.1396, -0.0095,  0.0497, -0.0513, -0.0066,  0.0442],\n",
            "        [ 0.1040,  0.0864,  0.0990, -0.0086,  0.0387, -0.0353, -0.0053,  0.0258],\n",
            "        [ 0.0584,  0.0459,  0.0567, -0.0045,  0.0228, -0.0164, -0.0047,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.013283215463161469 tensor([[ 0.1458,  0.1357,  0.1508, -0.0110,  0.0510, -0.0491, -0.0051,  0.0295],\n",
            "        [ 0.1555,  0.1372,  0.1577, -0.0116,  0.0565, -0.0584, -0.0075,  0.0480],\n",
            "        [ 0.1197,  0.0970,  0.1119, -0.0106,  0.0445, -0.0395, -0.0061,  0.0259],\n",
            "        [ 0.0713,  0.0555,  0.0694, -0.0056,  0.0279, -0.0189, -0.0060,  0.0180]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013022147119045258 tensor([[ 0.1592,  0.1462,  0.1629, -0.0128,  0.0554, -0.0524, -0.0054,  0.0276],\n",
            "        [ 0.1687,  0.1461,  0.1691, -0.0135,  0.0611, -0.0631, -0.0081,  0.0493],\n",
            "        [ 0.1312,  0.1035,  0.1204, -0.0123,  0.0487, -0.0418, -0.0067,  0.0242],\n",
            "        [ 0.0822,  0.0631,  0.0801, -0.0066,  0.0323, -0.0205, -0.0071,  0.0191]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023824498057365417 tensor([[ 0.0657,  0.0605,  0.0693, -0.0054,  0.0244, -0.0224, -0.0033,  0.0189],\n",
            "        [ 0.0550,  0.0496,  0.0550, -0.0035,  0.0199, -0.0203, -0.0041,  0.0204],\n",
            "        [ 0.0562,  0.0540,  0.0591, -0.0039,  0.0190, -0.0221, -0.0010,  0.0179],\n",
            "        [ 0.0238,  0.0176,  0.0195, -0.0012,  0.0082, -0.0082, -0.0019,  0.0109]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02078821510076523 tensor([[ 0.1099,  0.1001,  0.1158, -0.0101,  0.0410, -0.0363, -0.0057,  0.0286],\n",
            "        [ 0.0912,  0.0816,  0.0905, -0.0064,  0.0333, -0.0332, -0.0075,  0.0327],\n",
            "        [ 0.0934,  0.0903,  0.0981, -0.0070,  0.0309, -0.0372, -0.0012,  0.0278],\n",
            "        [ 0.0439,  0.0319,  0.0352, -0.0022,  0.0149, -0.0147, -0.0036,  0.0199]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019457556307315826 tensor([[ 0.1393,  0.1249,  0.1460, -0.0141,  0.0522, -0.0447, -0.0074,  0.0323],\n",
            "        [ 0.1146,  0.1008,  0.1121, -0.0087,  0.0420, -0.0411, -0.0103,  0.0393],\n",
            "        [ 0.1175,  0.1140,  0.1231, -0.0096,  0.0379, -0.0475, -0.0007,  0.0320],\n",
            "        [ 0.0610,  0.0435,  0.0480, -0.0031,  0.0203, -0.0199, -0.0051,  0.0274]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018887439742684364 tensor([[ 1.5951e-01,  1.4030e-01,  1.6575e-01, -1.7700e-02,  6.0120e-02,\n",
            "         -4.9644e-02, -8.6749e-03,  3.2339e-02],\n",
            "        [ 1.3005e-01,  1.1224e-01,  1.2498e-01, -1.0791e-02,  4.7965e-02,\n",
            "         -4.5731e-02, -1.2701e-02,  4.2503e-02],\n",
            "        [ 1.3357e-01,  1.2998e-01,  1.3950e-01, -1.1833e-02,  4.1769e-02,\n",
            "         -5.4541e-02,  7.6294e-05,  3.2718e-02],\n",
            "        [ 7.5665e-02,  5.2896e-02,  5.8298e-02, -3.8481e-03,  2.4703e-02,\n",
            "         -2.4009e-02, -6.4969e-03,  3.3669e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018626075237989426 tensor([[ 0.1738,  0.1497,  0.1788, -0.0210,  0.0659, -0.0524, -0.0097,  0.0302],\n",
            "        [ 0.1408,  0.1188,  0.1326, -0.0126,  0.0522, -0.0485, -0.0149,  0.0436],\n",
            "        [ 0.1445,  0.1411,  0.1506, -0.0139,  0.0438, -0.0596,  0.0011,  0.0312],\n",
            "        [ 0.0883,  0.0605,  0.0666, -0.0045,  0.0283, -0.0273, -0.0078,  0.0390]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[4, 11, 4, 11, 4, 6, 9, 9, 6, 6, 11, 6, 6, 4, 6, 8, 4, 7, 6, 12, 4, 6, 11, 4, 4, 11, 4, 11, 4, 6, 6, 6, 6, 12, 4, 6, 4, 6, 2, 1, 11, 4, 2, 4, 4, 4, 4, 6, 4, 6, 4, 5, 6, 6]\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02274128422141075 tensor([[ 0.0676,  0.0619,  0.0681, -0.0049,  0.0242, -0.0249, -0.0045,  0.0255],\n",
            "        [ 0.0473,  0.0415,  0.0483, -0.0032,  0.0177, -0.0171, -0.0038,  0.0164],\n",
            "        [ 0.0491,  0.0448,  0.0503, -0.0038,  0.0172, -0.0181, -0.0017,  0.0159],\n",
            "        [ 0.0238,  0.0198,  0.0219, -0.0024,  0.0077, -0.0075, -0.0022,  0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020116105675697327 tensor([[ 0.1132,  0.1025,  0.1128, -0.0091,  0.0404, -0.0414, -0.0079,  0.0418],\n",
            "        [ 0.0771,  0.0661,  0.0782, -0.0056,  0.0290, -0.0273, -0.0066,  0.0250],\n",
            "        [ 0.0814,  0.0736,  0.0829, -0.0069,  0.0282, -0.0297, -0.0026,  0.0245],\n",
            "        [ 0.0432,  0.0356,  0.0394, -0.0048,  0.0135, -0.0131, -0.0041,  0.0157]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018996605649590492 tensor([[ 0.1440,  0.1283,  0.1412, -0.0127,  0.0514, -0.0525, -0.0106,  0.0521],\n",
            "        [ 0.0956,  0.0796,  0.0959, -0.0076,  0.0360, -0.0330, -0.0089,  0.0286],\n",
            "        [ 0.1030,  0.0920,  0.1039, -0.0096,  0.0353, -0.0371, -0.0030,  0.0286],\n",
            "        [ 0.0591,  0.0482,  0.0535, -0.0070,  0.0180, -0.0172, -0.0059,  0.0207]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018513742834329605 tensor([[ 0.1659,  0.1449,  0.1594, -0.0160,  0.0592, -0.0602, -0.0129,  0.0587],\n",
            "        [ 0.1073,  0.0864,  0.1063, -0.0093,  0.0404, -0.0361, -0.0107,  0.0292],\n",
            "        [ 0.1179,  0.1040,  0.1179, -0.0119,  0.0399, -0.0418, -0.0032,  0.0298],\n",
            "        [ 0.0723,  0.0583,  0.0648, -0.0092,  0.0215, -0.0201, -0.0076,  0.0243]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01828848198056221 tensor([[ 0.1821,  0.1558,  0.1711, -0.0190,  0.0649, -0.0658, -0.0149,  0.0631],\n",
            "        [ 0.1151,  0.0894,  0.1124, -0.0108,  0.0434, -0.0375, -0.0124,  0.0280],\n",
            "        [ 0.1286,  0.1118,  0.1274, -0.0141,  0.0430, -0.0448, -0.0031,  0.0292],\n",
            "        [ 0.0832,  0.0665,  0.0739, -0.0113,  0.0241, -0.0220, -0.0092,  0.0268]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021069886162877083 tensor([[ 0.0487,  0.0431,  0.0473, -0.0033,  0.0187, -0.0185, -0.0035,  0.0172],\n",
            "        [ 0.0539,  0.0518,  0.0569, -0.0036,  0.0188, -0.0201, -0.0028,  0.0179],\n",
            "        [ 0.0578,  0.0481,  0.0558, -0.0044,  0.0210, -0.0203, -0.0029,  0.0192],\n",
            "        [ 0.0237,  0.0214,  0.0235, -0.0025,  0.0089, -0.0070, -0.0022,  0.0076]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018662402406334877 tensor([[ 0.0801,  0.0690,  0.0760, -0.0060,  0.0310, -0.0303, -0.0060,  0.0270],\n",
            "        [ 0.0859,  0.0825,  0.0904, -0.0063,  0.0295, -0.0318, -0.0044,  0.0264],\n",
            "        [ 0.0993,  0.0806,  0.0945, -0.0082,  0.0361, -0.0342, -0.0052,  0.0313],\n",
            "        [ 0.0426,  0.0381,  0.0419, -0.0047,  0.0161, -0.0118, -0.0042,  0.0127]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01765284687280655 tensor([[ 0.1016,  0.0852,  0.0940, -0.0082,  0.0397, -0.0383, -0.0081,  0.0328],\n",
            "        [ 0.1044,  0.0997,  0.1089, -0.0085,  0.0349, -0.0382, -0.0054,  0.0293],\n",
            "        [ 0.1290,  0.1016,  0.1203, -0.0114,  0.0469, -0.0434, -0.0070,  0.0384],\n",
            "        [ 0.0578,  0.0511,  0.0563, -0.0067,  0.0219, -0.0148, -0.0061,  0.0158]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017216293141245842 tensor([[ 0.1176,  0.0957,  0.1056, -0.0102,  0.0464, -0.0442, -0.0098,  0.0364],\n",
            "        [ 0.1150,  0.1091,  0.1188, -0.0104,  0.0374, -0.0417, -0.0058,  0.0291],\n",
            "        [ 0.1508,  0.1148,  0.1375, -0.0143,  0.0548, -0.0495, -0.0083,  0.0420],\n",
            "        [ 0.0701,  0.0614,  0.0676, -0.0087,  0.0265, -0.0166, -0.0080,  0.0175]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017010759562253952 tensor([[ 0.1301,  0.1027,  0.1133, -0.0120,  0.0519, -0.0488, -0.0114,  0.0387],\n",
            "        [ 0.1212,  0.1141,  0.1238, -0.0120,  0.0382, -0.0434, -0.0060,  0.0271],\n",
            "        [ 0.1674,  0.1229,  0.1489, -0.0169,  0.0608, -0.0536, -0.0094,  0.0435],\n",
            "        [ 0.0802,  0.0695,  0.0766, -0.0106,  0.0304, -0.0173, -0.0098,  0.0180]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01952436938881874 tensor([[ 0.0604,  0.0564,  0.0629, -0.0033,  0.0221, -0.0231, -0.0022,  0.0199],\n",
            "        [ 0.0589,  0.0512,  0.0592, -0.0036,  0.0216, -0.0199, -0.0042,  0.0213],\n",
            "        [ 0.0479,  0.0470,  0.0518, -0.0041,  0.0181, -0.0181, -0.0025,  0.0153],\n",
            "        [ 0.0270,  0.0168,  0.0207, -0.0009,  0.0099, -0.0082, -0.0005,  0.0074]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016783054918050766 tensor([[ 0.1011,  0.0935,  0.1050, -0.0060,  0.0371, -0.0384, -0.0034,  0.0312],\n",
            "        [ 0.0972,  0.0825,  0.0965, -0.0064,  0.0358, -0.0320, -0.0075,  0.0337],\n",
            "        [ 0.0790,  0.0781,  0.0861, -0.0076,  0.0302, -0.0297, -0.0044,  0.0232],\n",
            "        [ 0.0495,  0.0294,  0.0370, -0.0015,  0.0181, -0.0143, -0.0008,  0.0124]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015611839480698109 tensor([[ 0.1286,  0.1172,  0.1326, -0.0081,  0.0472, -0.0485, -0.0039,  0.0371],\n",
            "        [ 0.1215,  0.0998,  0.1181, -0.0085,  0.0449, -0.0387, -0.0102,  0.0403],\n",
            "        [ 0.0983,  0.0977,  0.1077, -0.0106,  0.0378, -0.0366, -0.0058,  0.0259],\n",
            "        [ 0.0686,  0.0389,  0.0500, -0.0019,  0.0251, -0.0188, -0.0009,  0.0158]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01512099802494049 tensor([[ 0.1479,  0.1324,  0.1507, -0.0100,  0.0543, -0.0555, -0.0041,  0.0397],\n",
            "        [ 0.1374,  0.1086,  0.1304, -0.0103,  0.0508, -0.0421, -0.0125,  0.0433],\n",
            "        [ 0.1101,  0.1101,  0.1215, -0.0132,  0.0427, -0.0405, -0.0068,  0.0252],\n",
            "        [ 0.0851,  0.0462,  0.0605, -0.0021,  0.0312, -0.0223, -0.0009,  0.0180]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01490087527781725 tensor([[ 0.1618,  0.1421,  0.1628, -0.0116,  0.0595, -0.0604, -0.0041,  0.0401],\n",
            "        [ 0.1484,  0.1125,  0.1372, -0.0118,  0.0550, -0.0435, -0.0145,  0.0443],\n",
            "        [ 0.1175,  0.1182,  0.1305, -0.0157,  0.0460, -0.0427, -0.0076,  0.0226],\n",
            "        [ 0.0996,  0.0518,  0.0692, -0.0023,  0.0366, -0.0249, -0.0007,  0.0192]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01604817435145378 tensor([[ 0.0618,  0.0615,  0.0670, -0.0043,  0.0222, -0.0222, -0.0047,  0.0210],\n",
            "        [ 0.0608,  0.0581,  0.0637, -0.0045,  0.0222, -0.0229, -0.0037,  0.0213],\n",
            "        [ 0.0540,  0.0477,  0.0542, -0.0034,  0.0196, -0.0192, -0.0027,  0.0188],\n",
            "        [ 0.0291,  0.0232,  0.0261, -0.0023,  0.0112, -0.0105, -0.0018,  0.0100]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01293149497359991 tensor([[ 0.1027,  0.1023,  0.1117, -0.0078,  0.0368, -0.0362, -0.0084,  0.0332],\n",
            "        [ 0.0965,  0.0917,  0.1002, -0.0081,  0.0353, -0.0361, -0.0063,  0.0319],\n",
            "        [ 0.0906,  0.0788,  0.0901, -0.0059,  0.0326, -0.0315, -0.0046,  0.0299],\n",
            "        [ 0.0526,  0.0409,  0.0464, -0.0044,  0.0204, -0.0184, -0.0035,  0.0171]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.011646663770079613 tensor([[ 0.1301,  0.1295,  0.1412, -0.0107,  0.0463, -0.0452, -0.0115,  0.0401],\n",
            "        [ 0.1171,  0.1102,  0.1204, -0.0111,  0.0429, -0.0434, -0.0082,  0.0360],\n",
            "        [ 0.1150,  0.0983,  0.1130, -0.0078,  0.0411, -0.0389, -0.0059,  0.0356],\n",
            "        [ 0.0717,  0.0546,  0.0624, -0.0063,  0.0280, -0.0244, -0.0051,  0.0221]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.011104573495686054 tensor([[ 0.1492,  0.1480,  0.1611, -0.0133,  0.0529, -0.0510, -0.0142,  0.0437],\n",
            "        [ 0.1296,  0.1204,  0.1314, -0.0137,  0.0475, -0.0473, -0.0097,  0.0365],\n",
            "        [ 0.1317,  0.1102,  0.1277, -0.0094,  0.0467, -0.0433, -0.0067,  0.0381],\n",
            "        [ 0.0876,  0.0652,  0.0750, -0.0082,  0.0344, -0.0290, -0.0065,  0.0255]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.010856418870389462 tensor([[ 0.1630,  0.1611,  0.1749, -0.0156,  0.0575, -0.0548, -0.0167,  0.0453],\n",
            "        [ 0.1374,  0.1259,  0.1373, -0.0162,  0.0504, -0.0494, -0.0110,  0.0350],\n",
            "        [ 0.1435,  0.1172,  0.1370, -0.0107,  0.0504, -0.0457, -0.0074,  0.0386],\n",
            "        [ 0.1009,  0.0735,  0.0850, -0.0099,  0.0400, -0.0325, -0.0078,  0.0276]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.7418e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020646337419748306 tensor([[ 0.0374,  0.0341,  0.0374, -0.0017,  0.0125, -0.0132, -0.0020,  0.0117],\n",
            "        [ 0.0671,  0.0646,  0.0726, -0.0050,  0.0237, -0.0246, -0.0035,  0.0222],\n",
            "        [ 0.0527,  0.0462,  0.0528, -0.0037,  0.0203, -0.0183, -0.0040,  0.0182],\n",
            "        [ 0.0322,  0.0212,  0.0247, -0.0014,  0.0110, -0.0117, -0.0003,  0.0089]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018039599061012268 tensor([[ 0.0607,  0.0540,  0.0596, -0.0030,  0.0199, -0.0207, -0.0032,  0.0173],\n",
            "        [ 0.1099,  0.1056,  0.1193, -0.0091,  0.0383, -0.0400, -0.0060,  0.0342],\n",
            "        [ 0.0890,  0.0765,  0.0882, -0.0068,  0.0347, -0.0301, -0.0076,  0.0290],\n",
            "        [ 0.0595,  0.0380,  0.0445, -0.0025,  0.0202, -0.0212, -0.0003,  0.0155]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016931330785155296 tensor([[ 7.6075e-02,  6.5765e-02,  7.2956e-02, -3.8999e-03,  2.4366e-02,\n",
            "         -2.4970e-02, -3.9524e-03,  1.9583e-02],\n",
            "        [ 1.3588e-01,  1.2991e-01,  1.4744e-01, -1.2627e-02,  4.6735e-02,\n",
            "         -4.9043e-02, -7.5698e-03,  3.9406e-02],\n",
            "        [ 1.1349e-01,  9.5091e-02,  1.1059e-01, -9.3842e-03,  4.4804e-02,\n",
            "         -3.7074e-02, -1.0748e-02,  3.4566e-02],\n",
            "        [ 8.2779e-02,  5.1241e-02,  6.0425e-02, -3.2830e-03,  2.7976e-02,\n",
            "         -2.9063e-02, -8.6427e-05,  2.0058e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01646602340042591 tensor([[ 0.0870,  0.0729,  0.0813, -0.0046,  0.0272, -0.0275, -0.0045,  0.0202],\n",
            "        [ 0.1520,  0.1442,  0.1645, -0.0158,  0.0514, -0.0543, -0.0087,  0.0407],\n",
            "        [ 0.1303,  0.1060,  0.1246, -0.0116,  0.0521, -0.0409, -0.0137,  0.0367],\n",
            "        [ 0.1029,  0.0616,  0.0733, -0.0039,  0.0346, -0.0356,  0.0003,  0.0231]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01625019684433937 tensor([[ 0.0953,  0.0774,  0.0867, -0.0052,  0.0290, -0.0290, -0.0048,  0.0198],\n",
            "        [ 0.1622,  0.1526,  0.1749, -0.0186,  0.0540, -0.0574, -0.0095,  0.0397],\n",
            "        [ 0.1423,  0.1121,  0.1332, -0.0137,  0.0576, -0.0428, -0.0165,  0.0368],\n",
            "        [ 0.1206,  0.0698,  0.0836, -0.0043,  0.0402, -0.0410,  0.0009,  0.0249]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.2517e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019222747534513474 tensor([[ 0.0550,  0.0510,  0.0566, -0.0038,  0.0200, -0.0189, -0.0039,  0.0196],\n",
            "        [ 0.0485,  0.0484,  0.0534, -0.0037,  0.0175, -0.0167, -0.0030,  0.0171],\n",
            "        [ 0.0502,  0.0481,  0.0554, -0.0032,  0.0198, -0.0174, -0.0041,  0.0179],\n",
            "        [ 0.0285,  0.0173,  0.0216, -0.0013,  0.0106, -0.0097, -0.0007,  0.0064]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01676790602505207 tensor([[ 0.0942,  0.0865,  0.0964, -0.0071,  0.0343, -0.0316, -0.0069,  0.0322],\n",
            "        [ 0.0789,  0.0792,  0.0875, -0.0067,  0.0282, -0.0262, -0.0051,  0.0264],\n",
            "        [ 0.0836,  0.0803,  0.0930, -0.0057,  0.0332, -0.0285, -0.0075,  0.0286],\n",
            "        [ 0.0517,  0.0292,  0.0377, -0.0023,  0.0194, -0.0167, -0.0012,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01565594971179962 tensor([[ 0.1225,  0.1112,  0.1245, -0.0100,  0.0447, -0.0401, -0.0095,  0.0404],\n",
            "        [ 0.0975,  0.0985,  0.1086, -0.0093,  0.0346, -0.0312, -0.0067,  0.0307],\n",
            "        [ 0.1051,  0.1009,  0.1175, -0.0077,  0.0421, -0.0350, -0.0104,  0.0344],\n",
            "        [ 0.0709,  0.0372,  0.0497, -0.0030,  0.0269, -0.0219, -0.0017,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01514669694006443 tensor([[ 0.1436,  0.1287,  0.1445, -0.0127,  0.0524, -0.0459, -0.0117,  0.0456],\n",
            "        [ 0.1090,  0.1107,  0.1220, -0.0115,  0.0384, -0.0333, -0.0080,  0.0320],\n",
            "        [ 0.1187,  0.1139,  0.1336, -0.0094,  0.0480, -0.0385, -0.0130,  0.0369],\n",
            "        [ 0.0871,  0.0425,  0.0589, -0.0037,  0.0333, -0.0256, -0.0020,  0.0111]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014900285750627518 tensor([[ 0.1599,  0.1413,  0.1592, -0.0151,  0.0584, -0.0498, -0.0137,  0.0488],\n",
            "        [ 0.1163,  0.1187,  0.1306, -0.0136,  0.0405, -0.0337, -0.0090,  0.0315],\n",
            "        [ 0.1272,  0.1220,  0.1442, -0.0108,  0.0521, -0.0402, -0.0154,  0.0373],\n",
            "        [ 0.1012,  0.0456,  0.0658, -0.0042,  0.0390, -0.0283, -0.0022,  0.0098]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.017128603532910347 tensor([[ 0.0681,  0.0678,  0.0728, -0.0040,  0.0228, -0.0255, -0.0030,  0.0237],\n",
            "        [ 0.0514,  0.0436,  0.0481, -0.0038,  0.0185, -0.0186, -0.0029,  0.0181],\n",
            "        [ 0.0381,  0.0358,  0.0402, -0.0035,  0.0146, -0.0139, -0.0039,  0.0161],\n",
            "        [ 0.0235,  0.0199,  0.0209, -0.0020,  0.0081, -0.0093, -0.0012,  0.0091]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01453829649835825 tensor([[ 0.1146,  0.1147,  0.1229, -0.0072,  0.0377, -0.0427, -0.0050,  0.0383],\n",
            "        [ 0.0845,  0.0698,  0.0769, -0.0069,  0.0305, -0.0301, -0.0051,  0.0284],\n",
            "        [ 0.0644,  0.0603,  0.0677, -0.0067,  0.0246, -0.0234, -0.0072,  0.0271],\n",
            "        [ 0.0425,  0.0357,  0.0374, -0.0039,  0.0147, -0.0166, -0.0022,  0.0161]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013371121138334274 tensor([[ 0.1458,  0.1464,  0.1562, -0.0098,  0.0471, -0.0542, -0.0063,  0.0468],\n",
            "        [ 0.1059,  0.0844,  0.0927, -0.0093,  0.0381, -0.0368, -0.0068,  0.0334],\n",
            "        [ 0.0824,  0.0769,  0.0866, -0.0095,  0.0315, -0.0296, -0.0101,  0.0345],\n",
            "        [ 0.0581,  0.0483,  0.0505, -0.0057,  0.0201, -0.0225, -0.0031,  0.0217]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.012846329249441624 tensor([[ 0.1673,  0.1681,  0.1785, -0.0121,  0.0530, -0.0621, -0.0071,  0.0515],\n",
            "        [ 0.1203,  0.0919,  0.1009, -0.0115,  0.0431, -0.0407, -0.0082,  0.0353],\n",
            "        [ 0.0949,  0.0884,  0.0996, -0.0122,  0.0363, -0.0337, -0.0127,  0.0395],\n",
            "        [ 0.0710,  0.0584,  0.0609, -0.0074,  0.0245, -0.0274, -0.0039,  0.0260]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01259952038526535 tensor([[ 0.1823,  0.1833,  0.1935, -0.0140,  0.0565, -0.0676, -0.0077,  0.0537],\n",
            "        [ 0.1305,  0.0953,  0.1046, -0.0135,  0.0466, -0.0428, -0.0093,  0.0353],\n",
            "        [ 0.1039,  0.0963,  0.1088, -0.0148,  0.0397, -0.0364, -0.0151,  0.0429],\n",
            "        [ 0.0817,  0.0666,  0.0691, -0.0091,  0.0282, -0.0313, -0.0047,  0.0294]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.1458e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019375374540686607 tensor([[ 0.0628,  0.0577,  0.0659, -0.0033,  0.0226, -0.0220, -0.0033,  0.0196],\n",
            "        [ 0.0638,  0.0553,  0.0624, -0.0044,  0.0232, -0.0238, -0.0037,  0.0220],\n",
            "        [ 0.0485,  0.0454,  0.0511, -0.0040,  0.0178, -0.0170, -0.0034,  0.0159],\n",
            "        [ 0.0309,  0.0259,  0.0288, -0.0021,  0.0113, -0.0132, -0.0014,  0.0098]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016319824382662773 tensor([[ 0.1031,  0.0932,  0.1079, -0.0056,  0.0370, -0.0351, -0.0053,  0.0295],\n",
            "        [ 0.1076,  0.0919,  0.1045, -0.0079,  0.0394, -0.0398, -0.0067,  0.0354],\n",
            "        [ 0.0813,  0.0758,  0.0856, -0.0074,  0.0298, -0.0280, -0.0061,  0.0250],\n",
            "        [ 0.0571,  0.0475,  0.0529, -0.0040,  0.0207, -0.0243, -0.0025,  0.0172]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.014955177903175354 tensor([[ 0.1287,  0.1137,  0.1334, -0.0073,  0.0460, -0.0425, -0.0066,  0.0333],\n",
            "        [ 0.1371,  0.1147,  0.1313, -0.0107,  0.0505, -0.0501, -0.0089,  0.0427],\n",
            "        [ 0.1035,  0.0959,  0.1087, -0.0103,  0.0378, -0.0348, -0.0083,  0.0294],\n",
            "        [ 0.0793,  0.0656,  0.0730, -0.0057,  0.0284, -0.0339, -0.0035,  0.0227]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014342527836561203 tensor([[ 0.1455,  0.1250,  0.1488, -0.0085,  0.0519, -0.0466, -0.0073,  0.0337],\n",
            "        [ 0.1578,  0.1288,  0.1486, -0.0131,  0.0585, -0.0570, -0.0108,  0.0462],\n",
            "        [ 0.1192,  0.1095,  0.1247, -0.0131,  0.0433, -0.0389, -0.0102,  0.0309],\n",
            "        [ 0.0981,  0.0808,  0.0898, -0.0073,  0.0349, -0.0420, -0.0042,  0.0267]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01404185313731432 tensor([[ 0.1570,  0.1308,  0.1580, -0.0094,  0.0558, -0.0487, -0.0077,  0.0322],\n",
            "        [ 0.1731,  0.1374,  0.1599, -0.0152,  0.0644, -0.0617, -0.0123,  0.0474],\n",
            "        [ 0.1305,  0.1189,  0.1361, -0.0156,  0.0471, -0.0413, -0.0120,  0.0305],\n",
            "        [ 0.1141,  0.0935,  0.1038, -0.0087,  0.0403, -0.0489, -0.0049,  0.0293]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.4305e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018734754994511604 tensor([[ 0.0508,  0.0477,  0.0558, -0.0035,  0.0190, -0.0160, -0.0038,  0.0158],\n",
            "        [ 0.0406,  0.0350,  0.0377, -0.0027,  0.0144, -0.0161, -0.0018,  0.0142],\n",
            "        [ 0.0463,  0.0414,  0.0468, -0.0031,  0.0157, -0.0153, -0.0031,  0.0161],\n",
            "        [ 0.0272,  0.0222,  0.0257, -0.0021,  0.0092, -0.0102, -0.0011,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016848154366016388 tensor([[ 0.0854,  0.0793,  0.0941, -0.0065,  0.0320, -0.0257, -0.0068,  0.0245],\n",
            "        [ 0.0661,  0.0555,  0.0593, -0.0047,  0.0234, -0.0264, -0.0030,  0.0219],\n",
            "        [ 0.0762,  0.0669,  0.0757, -0.0056,  0.0253, -0.0245, -0.0054,  0.0252],\n",
            "        [ 0.0489,  0.0391,  0.0457, -0.0041,  0.0163, -0.0177, -0.0021,  0.0157]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01603715866804123 tensor([[ 0.1092,  0.1001,  0.1205, -0.0092,  0.0410, -0.0312, -0.0092,  0.0285],\n",
            "        [ 0.0824,  0.0671,  0.0711, -0.0063,  0.0293, -0.0330, -0.0038,  0.0258],\n",
            "        [ 0.0960,  0.0826,  0.0936, -0.0075,  0.0310, -0.0300, -0.0071,  0.0301],\n",
            "        [ 0.0663,  0.0520,  0.0614, -0.0059,  0.0218, -0.0233, -0.0030,  0.0201]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01567247323691845 tensor([[ 0.1261,  0.1136,  0.1390, -0.0117,  0.0475, -0.0341, -0.0114,  0.0297],\n",
            "        [ 0.0935,  0.0735,  0.0773, -0.0077,  0.0332, -0.0375, -0.0044,  0.0274],\n",
            "        [ 0.1098,  0.0923,  0.1049, -0.0092,  0.0344, -0.0331, -0.0085,  0.0324],\n",
            "        [ 0.0804,  0.0618,  0.0737, -0.0077,  0.0261, -0.0274, -0.0037,  0.0229]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015497143380343914 tensor([[ 0.1384,  0.1225,  0.1523, -0.0140,  0.0522, -0.0352, -0.0133,  0.0290],\n",
            "        [ 0.1014,  0.0769,  0.0801, -0.0089,  0.0360, -0.0407, -0.0048,  0.0277],\n",
            "        [ 0.1198,  0.0984,  0.1119, -0.0107,  0.0364, -0.0348, -0.0097,  0.0330],\n",
            "        [ 0.0920,  0.0693,  0.0835, -0.0093,  0.0295, -0.0304, -0.0044,  0.0245]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018724525347352028 tensor([[ 0.0750,  0.0668,  0.0745, -0.0045,  0.0272, -0.0263, -0.0041,  0.0237],\n",
            "        [ 0.0525,  0.0454,  0.0505, -0.0039,  0.0199, -0.0196, -0.0042,  0.0198],\n",
            "        [ 0.0591,  0.0556,  0.0616, -0.0037,  0.0206, -0.0205, -0.0029,  0.0187],\n",
            "        [ 0.0281,  0.0222,  0.0252, -0.0026,  0.0104, -0.0110, -0.0009,  0.0098]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015338210389018059 tensor([[ 0.1276,  0.1123,  0.1260, -0.0082,  0.0463, -0.0439, -0.0070,  0.0376],\n",
            "        [ 0.0871,  0.0737,  0.0818, -0.0070,  0.0336, -0.0325, -0.0076,  0.0319],\n",
            "        [ 0.0994,  0.0938,  0.1036, -0.0066,  0.0343, -0.0337, -0.0051,  0.0294],\n",
            "        [ 0.0512,  0.0397,  0.0451, -0.0050,  0.0189, -0.0200, -0.0015,  0.0173]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013786537572741508 tensor([[ 0.1641,  0.1415,  0.1599, -0.0114,  0.0594, -0.0554, -0.0091,  0.0448],\n",
            "        [ 0.1094,  0.0897,  0.0997, -0.0095,  0.0430, -0.0407, -0.0105,  0.0388],\n",
            "        [ 0.1264,  0.1193,  0.1314, -0.0089,  0.0430, -0.0418, -0.0066,  0.0344],\n",
            "        [ 0.0704,  0.0535,  0.0608, -0.0073,  0.0260, -0.0274, -0.0020,  0.0229]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01309441588819027 tensor([[ 0.1900,  0.1600,  0.1819, -0.0141,  0.0687, -0.0628, -0.0106,  0.0477],\n",
            "        [ 0.1243,  0.0983,  0.1095, -0.0117,  0.0497, -0.0459, -0.0131,  0.0424],\n",
            "        [ 0.1449,  0.1368,  0.1502, -0.0108,  0.0485, -0.0465, -0.0078,  0.0357],\n",
            "        [ 0.0863,  0.0642,  0.0732, -0.0094,  0.0318, -0.0334, -0.0023,  0.0271]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.012769042514264584 tensor([[ 0.2092,  0.1715,  0.1962, -0.0166,  0.0755, -0.0678, -0.0118,  0.0480],\n",
            "        [ 0.1348,  0.1024,  0.1143, -0.0137,  0.0548, -0.0493, -0.0154,  0.0440],\n",
            "        [ 0.1579,  0.1490,  0.1632, -0.0125,  0.0520, -0.0491, -0.0087,  0.0347],\n",
            "        [ 0.0998,  0.0726,  0.0830, -0.0114,  0.0367, -0.0384, -0.0026,  0.0301]],\n",
            "       device='cuda:0')\n",
            "c tensor([0.0003, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02324068360030651 tensor([[ 0.0564,  0.0527,  0.0591, -0.0036,  0.0203, -0.0189, -0.0048,  0.0199],\n",
            "        [ 0.0692,  0.0636,  0.0708, -0.0038,  0.0255, -0.0244, -0.0030,  0.0235],\n",
            "        [ 0.0494,  0.0427,  0.0486, -0.0031,  0.0186, -0.0183, -0.0023,  0.0162],\n",
            "        [ 0.0255,  0.0205,  0.0225, -0.0014,  0.0087, -0.0094, -0.0012,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02032030187547207 tensor([[ 0.0888,  0.0813,  0.0922, -0.0063,  0.0318, -0.0285, -0.0084,  0.0293],\n",
            "        [ 0.1133,  0.1033,  0.1152, -0.0064,  0.0421, -0.0390, -0.0048,  0.0362],\n",
            "        [ 0.0835,  0.0710,  0.0813, -0.0055,  0.0317, -0.0305, -0.0040,  0.0255],\n",
            "        [ 0.0467,  0.0371,  0.0409, -0.0025,  0.0159, -0.0170, -0.0021,  0.0172]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019176118075847626 tensor([[ 0.1083,  0.0967,  0.1107, -0.0085,  0.0387, -0.0331, -0.0112,  0.0334],\n",
            "        [ 0.1404,  0.1262,  0.1410, -0.0083,  0.0525, -0.0470, -0.0059,  0.0418],\n",
            "        [ 0.1066,  0.0885,  0.1024, -0.0075,  0.0408, -0.0381, -0.0051,  0.0297],\n",
            "        [ 0.0645,  0.0508,  0.0561, -0.0034,  0.0219, -0.0231, -0.0029,  0.0232]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018727872520685196 tensor([[ 0.1208,  0.1050,  0.1213, -0.0104,  0.0430, -0.0350, -0.0136,  0.0347],\n",
            "        [ 0.1575,  0.1391,  0.1555, -0.0097,  0.0594, -0.0510, -0.0064,  0.0433],\n",
            "        [ 0.1225,  0.0989,  0.1158, -0.0091,  0.0471, -0.0428, -0.0058,  0.0306],\n",
            "        [ 0.0796,  0.0622,  0.0688, -0.0042,  0.0269, -0.0281, -0.0035,  0.0280]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01853344775736332 tensor([[ 0.1295,  0.1092,  0.1273, -0.0121,  0.0460, -0.0354, -0.0158,  0.0346],\n",
            "        [ 0.1688,  0.1462,  0.1635, -0.0108,  0.0641, -0.0529, -0.0067,  0.0425],\n",
            "        [ 0.1340,  0.1050,  0.1244, -0.0105,  0.0518, -0.0457, -0.0064,  0.0295],\n",
            "        [ 0.0925,  0.0717,  0.0795, -0.0049,  0.0312, -0.0322, -0.0039,  0.0317]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.8477e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.016694359481334686 tensor([[ 0.0586,  0.0510,  0.0569, -0.0036,  0.0207, -0.0204, -0.0029,  0.0191],\n",
            "        [ 0.0752,  0.0743,  0.0842, -0.0050,  0.0279, -0.0262, -0.0048,  0.0268],\n",
            "        [ 0.0541,  0.0462,  0.0531, -0.0036,  0.0209, -0.0194, -0.0027,  0.0176],\n",
            "        [ 0.0251,  0.0181,  0.0197, -0.0013,  0.0085, -0.0099, -0.0007,  0.0089]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.013249393552541733 tensor([[ 0.0989,  0.0839,  0.0946, -0.0068,  0.0348, -0.0333, -0.0047,  0.0301],\n",
            "        [ 0.1217,  0.1213,  0.1376, -0.0089,  0.0452, -0.0417, -0.0085,  0.0418],\n",
            "        [ 0.0887,  0.0737,  0.0857, -0.0063,  0.0350, -0.0311, -0.0047,  0.0263],\n",
            "        [ 0.0469,  0.0332,  0.0360, -0.0023,  0.0157, -0.0183, -0.0012,  0.0161]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01186192873865366 tensor([[ 0.1278,  0.1051,  0.1196, -0.0095,  0.0447, -0.0417, -0.0060,  0.0364],\n",
            "        [ 0.1471,  0.1475,  0.1675, -0.0118,  0.0546, -0.0495, -0.0113,  0.0486],\n",
            "        [ 0.1100,  0.0878,  0.1039, -0.0084,  0.0443, -0.0372, -0.0061,  0.0287],\n",
            "        [ 0.0659,  0.0460,  0.0499, -0.0032,  0.0219, -0.0255, -0.0015,  0.0220]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.011326936073601246 tensor([[ 0.1498,  0.1193,  0.1368, -0.0120,  0.0521, -0.0475, -0.0068,  0.0400],\n",
            "        [ 0.1606,  0.1616,  0.1838, -0.0141,  0.0595, -0.0529, -0.0136,  0.0508],\n",
            "        [ 0.1236,  0.0944,  0.1140, -0.0102,  0.0508, -0.0402, -0.0071,  0.0275],\n",
            "        [ 0.0826,  0.0569,  0.0615, -0.0039,  0.0272, -0.0317, -0.0017,  0.0267]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.011088402010500431 tensor([[ 0.1674,  0.1289,  0.1490, -0.0143,  0.0579, -0.0515, -0.0075,  0.0419],\n",
            "        [ 0.1675,  0.1692,  0.1927, -0.0161,  0.0620, -0.0539, -0.0156,  0.0505],\n",
            "        [ 0.1333,  0.0968,  0.1197, -0.0118,  0.0559, -0.0415, -0.0080,  0.0243],\n",
            "        [ 0.0973,  0.0660,  0.0713, -0.0045,  0.0319, -0.0371, -0.0017,  0.0305]],\n",
            "       device='cuda:0')\n",
            "c tensor([0.0012, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02113453298807144 tensor([[ 0.0596,  0.0570,  0.0648, -0.0048,  0.0230, -0.0182, -0.0052,  0.0189],\n",
            "        [ 0.0603,  0.0533,  0.0583, -0.0033,  0.0205, -0.0225, -0.0022,  0.0219],\n",
            "        [ 0.0371,  0.0314,  0.0348, -0.0033,  0.0132, -0.0140, -0.0025,  0.0119],\n",
            "        [ 0.0245,  0.0168,  0.0197, -0.0015,  0.0079, -0.0069, -0.0003,  0.0057]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018672682344913483 tensor([[ 0.1016,  0.0969,  0.1114, -0.0091,  0.0397, -0.0293, -0.0095,  0.0299],\n",
            "        [ 0.1018,  0.0890,  0.0967, -0.0056,  0.0343, -0.0381, -0.0036,  0.0363],\n",
            "        [ 0.0619,  0.0513,  0.0567, -0.0059,  0.0216, -0.0232, -0.0045,  0.0183],\n",
            "        [ 0.0449,  0.0298,  0.0354, -0.0028,  0.0142, -0.0120, -0.0004,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017511609941720963 tensor([[ 0.1311,  0.1241,  0.1442, -0.0130,  0.0519, -0.0355, -0.0132,  0.0355],\n",
            "        [ 0.1304,  0.1123,  0.1211, -0.0073,  0.0435, -0.0490, -0.0045,  0.0454],\n",
            "        [ 0.0782,  0.0634,  0.0698, -0.0082,  0.0268, -0.0291, -0.0059,  0.0209],\n",
            "        [ 0.0623,  0.0400,  0.0481, -0.0040,  0.0193, -0.0158, -0.0003,  0.0118]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016964469105005264 tensor([[ 1.5219e-01,  1.4264e-01,  1.6756e-01, -1.6620e-02,  6.0968e-02,\n",
            "         -3.8421e-02, -1.6396e-02,  3.7569e-02],\n",
            "        [ 1.5085e-01,  1.2774e-01,  1.3665e-01, -8.5318e-03,  4.9853e-02,\n",
            "         -5.6691e-02, -5.0092e-03,  5.1289e-02],\n",
            "        [ 8.9188e-02,  7.0548e-02,  7.7543e-02, -1.0260e-02,  3.0031e-02,\n",
            "         -3.2876e-02, -7.0035e-03,  2.1166e-02],\n",
            "        [ 7.7353e-02,  4.8203e-02,  5.8527e-02, -5.0807e-03,  2.3572e-02,\n",
            "         -1.8520e-02, -7.8082e-05,  1.3047e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016690682619810104 tensor([[ 0.1675,  0.1553,  0.1844, -0.0200,  0.0680, -0.0392, -0.0193,  0.0373],\n",
            "        [ 0.1662,  0.1380,  0.1465, -0.0095,  0.0543, -0.0624, -0.0053,  0.0550],\n",
            "        [ 0.0967,  0.0746,  0.0818, -0.0121,  0.0319, -0.0353, -0.0079,  0.0199],\n",
            "        [ 0.0905,  0.0547,  0.0672, -0.0061,  0.0271, -0.0205,  0.0003,  0.0135]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02002246305346489 tensor([[ 0.0678,  0.0700,  0.0766, -0.0048,  0.0258, -0.0249, -0.0048,  0.0232],\n",
            "        [ 0.0639,  0.0541,  0.0626, -0.0043,  0.0234, -0.0232, -0.0031,  0.0232],\n",
            "        [ 0.0500,  0.0439,  0.0512, -0.0028,  0.0189, -0.0178, -0.0026,  0.0169],\n",
            "        [ 0.0295,  0.0228,  0.0255, -0.0025,  0.0108, -0.0101, -0.0012,  0.0075]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016648022457957268 tensor([[ 0.1112,  0.1165,  0.1274, -0.0087,  0.0428, -0.0403, -0.0085,  0.0362],\n",
            "        [ 0.1065,  0.0880,  0.1028, -0.0076,  0.0394, -0.0382, -0.0053,  0.0372],\n",
            "        [ 0.0850,  0.0737,  0.0868, -0.0050,  0.0322, -0.0299, -0.0045,  0.0272],\n",
            "        [ 0.0546,  0.0412,  0.0463, -0.0048,  0.0199, -0.0182, -0.0022,  0.0127]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015167256817221642 tensor([[ 0.1377,  0.1457,  0.1591, -0.0118,  0.0535, -0.0492, -0.0115,  0.0425],\n",
            "        [ 0.1336,  0.1064,  0.1262, -0.0104,  0.0497, -0.0471, -0.0067,  0.0444],\n",
            "        [ 0.1091,  0.0932,  0.1111, -0.0066,  0.0416, -0.0377, -0.0059,  0.0328],\n",
            "        [ 0.0760,  0.0563,  0.0633, -0.0069,  0.0275, -0.0247, -0.0030,  0.0160]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014545688405632973 tensor([[ 0.1536,  0.1642,  0.1787, -0.0145,  0.0604, -0.0543, -0.0139,  0.0449],\n",
            "        [ 0.1515,  0.1154,  0.1392, -0.0127,  0.0566, -0.0523, -0.0077,  0.0475],\n",
            "        [ 0.1261,  0.1058,  0.1279, -0.0079,  0.0482, -0.0428, -0.0068,  0.0353],\n",
            "        [ 0.0945,  0.0687,  0.0774, -0.0089,  0.0341, -0.0298, -0.0037,  0.0179]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014265869744122028 tensor([[ 0.1632,  0.1761,  0.1908, -0.0168,  0.0650, -0.0570, -0.0161,  0.0449],\n",
            "        [ 0.1640,  0.1189,  0.1461, -0.0147,  0.0616, -0.0552, -0.0085,  0.0482],\n",
            "        [ 0.1385,  0.1141,  0.1399, -0.0089,  0.0532, -0.0460, -0.0075,  0.0357],\n",
            "        [ 0.1106,  0.0789,  0.0891, -0.0108,  0.0398, -0.0340, -0.0042,  0.0187]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01934118941426277 tensor([[ 0.0687,  0.0666,  0.0743, -0.0048,  0.0255, -0.0245, -0.0054,  0.0243],\n",
            "        [ 0.0574,  0.0518,  0.0572, -0.0042,  0.0204, -0.0214, -0.0033,  0.0203],\n",
            "        [ 0.0558,  0.0495,  0.0547, -0.0024,  0.0200, -0.0200, -0.0020,  0.0180],\n",
            "        [ 0.0281,  0.0221,  0.0253, -0.0017,  0.0103, -0.0114, -0.0017,  0.0123]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016038713976740837 tensor([[ 0.1152,  0.1115,  0.1251, -0.0088,  0.0429, -0.0404, -0.0097,  0.0388],\n",
            "        [ 0.0955,  0.0857,  0.0946, -0.0078,  0.0340, -0.0353, -0.0060,  0.0323],\n",
            "        [ 0.0940,  0.0825,  0.0911, -0.0041,  0.0337, -0.0332, -0.0032,  0.0282],\n",
            "        [ 0.0525,  0.0406,  0.0467, -0.0031,  0.0192, -0.0213, -0.0032,  0.0229]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01453264057636261 tensor([[ 0.1460,  0.1405,  0.1585, -0.0123,  0.0545, -0.0502, -0.0133,  0.0468],\n",
            "        [ 0.1193,  0.1059,  0.1169, -0.0108,  0.0424, -0.0437, -0.0080,  0.0382],\n",
            "        [ 0.1200,  0.1036,  0.1145, -0.0051,  0.0429, -0.0415, -0.0039,  0.0330],\n",
            "        [ 0.0737,  0.0561,  0.0647, -0.0044,  0.0269, -0.0297, -0.0046,  0.0319]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.013865125365555286 tensor([[ 0.1671,  0.1594,  0.1808, -0.0155,  0.0625, -0.0563, -0.0163,  0.0507],\n",
            "        [ 0.1342,  0.1174,  0.1295, -0.0134,  0.0475, -0.0485, -0.0097,  0.0403],\n",
            "        [ 0.1382,  0.1173,  0.1298, -0.0059,  0.0493, -0.0466, -0.0042,  0.0344],\n",
            "        [ 0.0921,  0.0689,  0.0798, -0.0056,  0.0336, -0.0370, -0.0058,  0.0396]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013547606766223907 tensor([[ 0.1820,  0.1719,  0.1959, -0.0184,  0.0683, -0.0600, -0.0189,  0.0521],\n",
            "        [ 0.1436,  0.1237,  0.1363, -0.0158,  0.0506, -0.0511, -0.0112,  0.0403],\n",
            "        [ 0.1516,  0.1262,  0.1400, -0.0064,  0.0539, -0.0498, -0.0042,  0.0335],\n",
            "        [ 0.1079,  0.0795,  0.0924, -0.0067,  0.0393, -0.0432, -0.0070,  0.0462]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022260844707489014 tensor([[ 0.0591,  0.0544,  0.0624, -0.0053,  0.0221, -0.0191, -0.0048,  0.0187],\n",
            "        [ 0.0489,  0.0428,  0.0475, -0.0025,  0.0177, -0.0194, -0.0025,  0.0159],\n",
            "        [ 0.0663,  0.0619,  0.0705, -0.0047,  0.0243, -0.0225, -0.0029,  0.0221],\n",
            "        [ 0.0316,  0.0213,  0.0242, -0.0024,  0.0116, -0.0109, -0.0019,  0.0103]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01920129731297493 tensor([[ 0.0980,  0.0887,  0.1032, -0.0100,  0.0369, -0.0305, -0.0086,  0.0286],\n",
            "        [ 0.0816,  0.0706,  0.0786, -0.0044,  0.0296, -0.0324, -0.0040,  0.0245],\n",
            "        [ 0.1126,  0.1050,  0.1202, -0.0086,  0.0413, -0.0374, -0.0051,  0.0359],\n",
            "        [ 0.0577,  0.0372,  0.0422, -0.0043,  0.0215, -0.0193, -0.0036,  0.0176]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017819855362176895 tensor([[ 0.1240,  0.1099,  0.1295, -0.0141,  0.0469, -0.0368, -0.0117,  0.0330],\n",
            "        [ 0.1032,  0.0878,  0.0982, -0.0059,  0.0375, -0.0409, -0.0049,  0.0283],\n",
            "        [ 0.1425,  0.1323,  0.1521, -0.0120,  0.0523, -0.0462, -0.0067,  0.0432],\n",
            "        [ 0.0794,  0.0486,  0.0554, -0.0059,  0.0297, -0.0257, -0.0052,  0.0226]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01723044365644455 tensor([[ 0.1421,  0.1230,  0.1470, -0.0179,  0.0541, -0.0402, -0.0144,  0.0341],\n",
            "        [ 0.1181,  0.0985,  0.1106, -0.0070,  0.0428, -0.0466, -0.0053,  0.0291],\n",
            "        [ 0.1611,  0.1486,  0.1718, -0.0148,  0.0591, -0.0509, -0.0077,  0.0462],\n",
            "        [ 0.0977,  0.0566,  0.0649, -0.0072,  0.0368, -0.0305, -0.0066,  0.0257]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01696947030723095 tensor([[ 0.1554,  0.1310,  0.1588, -0.0215,  0.0595, -0.0417, -0.0168,  0.0332],\n",
            "        [ 0.1287,  0.1051,  0.1186, -0.0080,  0.0466, -0.0506, -0.0055,  0.0283],\n",
            "        [ 0.1727,  0.1580,  0.1836, -0.0173,  0.0633, -0.0530, -0.0085,  0.0464],\n",
            "        [ 0.1135,  0.0622,  0.0717, -0.0084,  0.0429, -0.0341, -0.0079,  0.0275]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.1861e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.016729852184653282 tensor([[ 0.0558,  0.0528,  0.0598, -0.0030,  0.0211, -0.0194, -0.0039,  0.0191],\n",
            "        [ 0.0482,  0.0433,  0.0476, -0.0029,  0.0167, -0.0182, -0.0030,  0.0174],\n",
            "        [ 0.0492,  0.0458,  0.0502, -0.0031,  0.0170, -0.0176, -0.0024,  0.0163],\n",
            "        [ 0.0271,  0.0172,  0.0206, -0.0020,  0.0100, -0.0088, -0.0010,  0.0081]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014412382617592812 tensor([[ 0.0936,  0.0882,  0.1005, -0.0054,  0.0357, -0.0318, -0.0069,  0.0306],\n",
            "        [ 0.0797,  0.0707,  0.0778, -0.0052,  0.0274, -0.0299, -0.0050,  0.0275],\n",
            "        [ 0.0825,  0.0767,  0.0836, -0.0056,  0.0280, -0.0291, -0.0042,  0.0260],\n",
            "        [ 0.0492,  0.0292,  0.0357, -0.0037,  0.0184, -0.0153, -0.0020,  0.0137]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013380849733948708 tensor([[ 0.1197,  0.1118,  0.1284, -0.0074,  0.0459, -0.0397, -0.0094,  0.0371],\n",
            "        [ 0.1005,  0.0879,  0.0967, -0.0070,  0.0341, -0.0374, -0.0065,  0.0328],\n",
            "        [ 0.1041,  0.0966,  0.1046, -0.0076,  0.0347, -0.0363, -0.0054,  0.0309],\n",
            "        [ 0.0674,  0.0374,  0.0468, -0.0053,  0.0254, -0.0200, -0.0028,  0.0174]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.012920014560222626 tensor([[ 0.1383,  0.1279,  0.1478, -0.0090,  0.0533, -0.0448, -0.0114,  0.0406],\n",
            "        [ 0.1149,  0.0987,  0.1087, -0.0085,  0.0383, -0.0422, -0.0075,  0.0352],\n",
            "        [ 0.1180,  0.1092,  0.1173, -0.0093,  0.0385, -0.0406, -0.0062,  0.0326],\n",
            "        [ 0.0829,  0.0429,  0.0550, -0.0068,  0.0316, -0.0234, -0.0036,  0.0196]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.012703513726592064 tensor([[ 0.1521,  0.1390,  0.1618, -0.0105,  0.0590, -0.0480, -0.0132,  0.0422],\n",
            "        [ 0.1251,  0.1056,  0.1164, -0.0098,  0.0410, -0.0454, -0.0083,  0.0358],\n",
            "        [ 0.1270,  0.1172,  0.1248, -0.0107,  0.0405, -0.0431, -0.0068,  0.0325],\n",
            "        [ 0.0963,  0.0464,  0.0610, -0.0082,  0.0371, -0.0260, -0.0044,  0.0209]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.9802e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, 1.1921e-07, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023103035986423492 tensor([[ 0.0495,  0.0489,  0.0528, -0.0031,  0.0170, -0.0196, -0.0026,  0.0179],\n",
            "        [ 0.0595,  0.0496,  0.0589, -0.0053,  0.0213, -0.0198, -0.0029,  0.0183],\n",
            "        [ 0.0596,  0.0540,  0.0609, -0.0032,  0.0226, -0.0204, -0.0032,  0.0200],\n",
            "        [ 0.0255,  0.0174,  0.0197, -0.0019,  0.0088, -0.0083, -0.0014,  0.0076]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020397407934069633 tensor([[ 0.0802,  0.0796,  0.0856, -0.0054,  0.0271, -0.0319, -0.0043,  0.0280],\n",
            "        [ 0.0978,  0.0785,  0.0952, -0.0100,  0.0347, -0.0312, -0.0047,  0.0273],\n",
            "        [ 0.1031,  0.0929,  0.1050, -0.0058,  0.0392, -0.0346, -0.0058,  0.0330],\n",
            "        [ 0.0468,  0.0307,  0.0350, -0.0035,  0.0163, -0.0147, -0.0026,  0.0130]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019207846373319626 tensor([[ 0.0998,  0.0993,  0.1061, -0.0072,  0.0332, -0.0400, -0.0055,  0.0337],\n",
            "        [ 0.1217,  0.0932,  0.1160, -0.0141,  0.0428, -0.0371, -0.0058,  0.0302],\n",
            "        [ 0.1338,  0.1195,  0.1354, -0.0078,  0.0510, -0.0439, -0.0077,  0.0404],\n",
            "        [ 0.0647,  0.0408,  0.0470, -0.0049,  0.0228, -0.0195, -0.0038,  0.0167]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018692154437303543 tensor([[ 0.1129,  0.1124,  0.1193, -0.0087,  0.0368, -0.0457, -0.0065,  0.0371],\n",
            "        [ 0.1372,  0.0995,  0.1275, -0.0178,  0.0478, -0.0396, -0.0064,  0.0295],\n",
            "        [ 0.1555,  0.1374,  0.1561, -0.0095,  0.0593, -0.0497, -0.0093,  0.0440],\n",
            "        [ 0.0801,  0.0486,  0.0564, -0.0061,  0.0283, -0.0230, -0.0050,  0.0191]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.0184544138610363 tensor([[ 0.1221,  0.1216,  0.1280, -0.0100,  0.0391, -0.0499, -0.0072,  0.0390],\n",
            "        [ 0.1478,  0.1008,  0.1335, -0.0213,  0.0509, -0.0402, -0.0068,  0.0269],\n",
            "        [ 0.1711,  0.1492,  0.1701, -0.0109,  0.0655, -0.0532, -0.0105,  0.0450],\n",
            "        [ 0.0935,  0.0546,  0.0638, -0.0072,  0.0332, -0.0257, -0.0060,  0.0205]],\n",
            "       device='cuda:0')\n",
            "c tensor([4.2915e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023867538198828697 tensor([[ 0.0609,  0.0563,  0.0643, -0.0043,  0.0235, -0.0205, -0.0050,  0.0216],\n",
            "        [ 0.0439,  0.0365,  0.0394, -0.0026,  0.0147, -0.0176, -0.0022,  0.0150],\n",
            "        [ 0.0504,  0.0484,  0.0570, -0.0046,  0.0188, -0.0180, -0.0017,  0.0142],\n",
            "        [ 0.0291,  0.0185,  0.0228, -0.0024,  0.0098, -0.0106, -0.0016,  0.0101]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02138880267739296 tensor([[ 0.1055,  0.0971,  0.1117, -0.0080,  0.0410, -0.0346, -0.0092,  0.0360],\n",
            "        [ 0.0741,  0.0600,  0.0643, -0.0046,  0.0246, -0.0299, -0.0038,  0.0240],\n",
            "        [ 0.0835,  0.0806,  0.0956, -0.0086,  0.0310, -0.0295, -0.0027,  0.0213],\n",
            "        [ 0.0531,  0.0317,  0.0397, -0.0046,  0.0175, -0.0189, -0.0029,  0.0174]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020179135724902153 tensor([[ 0.1381,  0.1261,  0.1462, -0.0113,  0.0542, -0.0441, -0.0128,  0.0453],\n",
            "        [ 0.0951,  0.0745,  0.0792, -0.0062,  0.0311, -0.0386, -0.0049,  0.0290],\n",
            "        [ 0.1048,  0.1017,  0.1218, -0.0121,  0.0389, -0.0367, -0.0033,  0.0237],\n",
            "        [ 0.0728,  0.0406,  0.0522, -0.0067,  0.0236, -0.0253, -0.0040,  0.0226]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019586801528930664 tensor([[ 0.1623,  0.1468,  0.1713, -0.0142,  0.0642, -0.0505, -0.0159,  0.0512],\n",
            "        [ 0.1101,  0.0833,  0.0879, -0.0076,  0.0354, -0.0449, -0.0057,  0.0314],\n",
            "        [ 0.1187,  0.1156,  0.1399, -0.0152,  0.0438, -0.0410, -0.0035,  0.0231],\n",
            "        [ 0.0893,  0.0464,  0.0613, -0.0086,  0.0285, -0.0304, -0.0050,  0.0262]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01928548887372017 tensor([[ 0.1806,  0.1615,  0.1898, -0.0169,  0.0721, -0.0546, -0.0188,  0.0547],\n",
            "        [ 0.1214,  0.0883,  0.0925, -0.0088,  0.0384, -0.0496, -0.0063,  0.0322],\n",
            "        [ 0.1276,  0.1249,  0.1527, -0.0181,  0.0469, -0.0434, -0.0035,  0.0206],\n",
            "        [ 0.1034,  0.0499,  0.0679, -0.0104,  0.0325, -0.0344, -0.0058,  0.0285]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022352561354637146 tensor([[ 0.0679,  0.0650,  0.0713, -0.0044,  0.0251, -0.0257, -0.0043,  0.0226],\n",
            "        [ 0.0653,  0.0559,  0.0625, -0.0050,  0.0235, -0.0243, -0.0037,  0.0230],\n",
            "        [ 0.0502,  0.0434,  0.0490, -0.0032,  0.0184, -0.0167, -0.0032,  0.0163],\n",
            "        [ 0.0265,  0.0206,  0.0220, -0.0016,  0.0087, -0.0088, -0.0010,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019165735691785812 tensor([[ 0.1107,  0.1056,  0.1162, -0.0079,  0.0412, -0.0416, -0.0074,  0.0346],\n",
            "        [ 0.1097,  0.0922,  0.1028, -0.0090,  0.0393, -0.0409, -0.0065,  0.0372],\n",
            "        [ 0.0848,  0.0720,  0.0817, -0.0056,  0.0315, -0.0271, -0.0059,  0.0255],\n",
            "        [ 0.0490,  0.0374,  0.0397, -0.0028,  0.0158, -0.0159, -0.0017,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01781081035733223 tensor([[ 0.1387,  0.1312,  0.1446, -0.0109,  0.0517, -0.0518, -0.0098,  0.0403],\n",
            "        [ 0.1377,  0.1124,  0.1252, -0.0123,  0.0492, -0.0513, -0.0084,  0.0447],\n",
            "        [ 0.1081,  0.0897,  0.1028, -0.0076,  0.0406, -0.0327, -0.0081,  0.0294],\n",
            "        [ 0.0681,  0.0513,  0.0540, -0.0039,  0.0216, -0.0216, -0.0023,  0.0205]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01725107803940773 tensor([[ 0.1578,  0.1477,  0.1629, -0.0135,  0.0590, -0.0586, -0.0118,  0.0427],\n",
            "        [ 0.1555,  0.1224,  0.1361, -0.0151,  0.0554, -0.0578, -0.0099,  0.0481],\n",
            "        [ 0.1243,  0.1007,  0.1165, -0.0092,  0.0471, -0.0354, -0.0100,  0.0300],\n",
            "        [ 0.0846,  0.0629,  0.0658, -0.0048,  0.0265, -0.0262, -0.0027,  0.0244]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017000917345285416 tensor([[ 0.1714,  0.1585,  0.1750, -0.0159,  0.0643, -0.0633, -0.0134,  0.0428],\n",
            "        [ 0.1672,  0.1265,  0.1402, -0.0177,  0.0593, -0.0620, -0.0110,  0.0491],\n",
            "        [ 0.1362,  0.1074,  0.1256, -0.0106,  0.0521, -0.0362, -0.0116,  0.0286],\n",
            "        [ 0.0990,  0.0726,  0.0754, -0.0056,  0.0307, -0.0300, -0.0030,  0.0274]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.3113e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02028229460120201 tensor([[ 0.0541,  0.0484,  0.0552, -0.0029,  0.0202, -0.0193, -0.0030,  0.0184],\n",
            "        [ 0.0400,  0.0390,  0.0431, -0.0030,  0.0135, -0.0159, -0.0019,  0.0150],\n",
            "        [ 0.0394,  0.0366,  0.0405, -0.0039,  0.0129, -0.0150, -0.0023,  0.0124],\n",
            "        [ 0.0203,  0.0137,  0.0172, -0.0030,  0.0078, -0.0057, -0.0020,  0.0060]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0185103677213192 tensor([[ 0.0933,  0.0824,  0.0948, -0.0052,  0.0350, -0.0328, -0.0052,  0.0303],\n",
            "        [ 0.0647,  0.0632,  0.0697, -0.0052,  0.0214, -0.0261, -0.0030,  0.0237],\n",
            "        [ 0.0642,  0.0596,  0.0659, -0.0071,  0.0204, -0.0243, -0.0037,  0.0186],\n",
            "        [ 0.0365,  0.0231,  0.0299, -0.0060,  0.0141, -0.0096, -0.0040,  0.0100]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017703764140605927 tensor([[ 0.1225,  0.1064,  0.1233, -0.0072,  0.0461, -0.0425, -0.0069,  0.0380],\n",
            "        [ 0.0807,  0.0791,  0.0868, -0.0069,  0.0259, -0.0329, -0.0036,  0.0289],\n",
            "        [ 0.0801,  0.0743,  0.0821, -0.0099,  0.0244, -0.0302, -0.0047,  0.0209],\n",
            "        [ 0.0498,  0.0296,  0.0395, -0.0090,  0.0194, -0.0122, -0.0059,  0.0127]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01731320470571518 tensor([[ 0.1449,  0.1234,  0.1441, -0.0089,  0.0547, -0.0494, -0.0084,  0.0427],\n",
            "        [ 0.0915,  0.0899,  0.0983, -0.0083,  0.0285, -0.0378, -0.0039,  0.0319],\n",
            "        [ 0.0903,  0.0839,  0.0926, -0.0124,  0.0263, -0.0339, -0.0053,  0.0208],\n",
            "        [ 0.0610,  0.0339,  0.0468, -0.0120,  0.0239, -0.0138, -0.0078,  0.0143]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017109567299485207 tensor([[ 0.1625,  0.1355,  0.1596, -0.0104,  0.0615, -0.0546, -0.0096,  0.0454],\n",
            "        [ 0.0990,  0.0976,  0.1063, -0.0096,  0.0299, -0.0413, -0.0040,  0.0335],\n",
            "        [ 0.0969,  0.0901,  0.0994, -0.0148,  0.0269, -0.0361, -0.0056,  0.0192],\n",
            "        [ 0.0705,  0.0366,  0.0523, -0.0150,  0.0277, -0.0148, -0.0097,  0.0152]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021725647151470184 tensor([[ 0.0637,  0.0546,  0.0605, -0.0037,  0.0217, -0.0238, -0.0024,  0.0217],\n",
            "        [ 0.0557,  0.0521,  0.0588, -0.0034,  0.0205, -0.0185, -0.0052,  0.0204],\n",
            "        [ 0.0579,  0.0531,  0.0583, -0.0033,  0.0208, -0.0203, -0.0019,  0.0189],\n",
            "        [ 0.0275,  0.0212,  0.0240, -0.0020,  0.0106, -0.0109, -0.0018,  0.0112]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018735092133283615 tensor([[ 0.1075,  0.0900,  0.1003, -0.0068,  0.0364, -0.0398, -0.0037,  0.0349],\n",
            "        [ 0.0928,  0.0862,  0.0979, -0.0062,  0.0342, -0.0298, -0.0096,  0.0323],\n",
            "        [ 0.0968,  0.0880,  0.0965, -0.0057,  0.0350, -0.0331, -0.0033,  0.0295],\n",
            "        [ 0.0507,  0.0382,  0.0433, -0.0038,  0.0197, -0.0202, -0.0034,  0.0203]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017402881756424904 tensor([[ 0.1384,  0.1124,  0.1258, -0.0093,  0.0463, -0.0509, -0.0045,  0.0427],\n",
            "        [ 0.1163,  0.1067,  0.1221, -0.0085,  0.0428, -0.0357, -0.0132,  0.0381],\n",
            "        [ 0.1223,  0.1098,  0.1204, -0.0074,  0.0443, -0.0406, -0.0041,  0.0341],\n",
            "        [ 0.0705,  0.0519,  0.0587, -0.0055,  0.0275, -0.0279, -0.0049,  0.0276]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016819274052977562 tensor([[ 0.1611,  0.1267,  0.1424, -0.0114,  0.0533, -0.0589, -0.0048,  0.0474],\n",
            "        [ 0.1313,  0.1187,  0.1369, -0.0105,  0.0482, -0.0382, -0.0164,  0.0402],\n",
            "        [ 0.1396,  0.1235,  0.1354, -0.0087,  0.0506, -0.0447, -0.0046,  0.0351],\n",
            "        [ 0.0873,  0.0629,  0.0711, -0.0070,  0.0341, -0.0345, -0.0062,  0.0335]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016544656828045845 tensor([[ 0.1787,  0.1357,  0.1531, -0.0133,  0.0585, -0.0649, -0.0049,  0.0500],\n",
            "        [ 0.1410,  0.1253,  0.1459, -0.0122,  0.0516, -0.0387, -0.0192,  0.0400],\n",
            "        [ 0.1518,  0.1322,  0.1451, -0.0098,  0.0551, -0.0467, -0.0049,  0.0339],\n",
            "        [ 0.1017,  0.0717,  0.0810, -0.0085,  0.0399, -0.0401, -0.0075,  0.0382]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019394436851143837 tensor([[ 0.0607,  0.0526,  0.0596, -0.0047,  0.0207, -0.0224, -0.0030,  0.0196],\n",
            "        [ 0.0597,  0.0523,  0.0590, -0.0036,  0.0215, -0.0198, -0.0036,  0.0185],\n",
            "        [ 0.0432,  0.0365,  0.0409, -0.0033,  0.0146, -0.0161, -0.0020,  0.0165],\n",
            "        [ 0.0173,  0.0172,  0.0201, -0.0030,  0.0064, -0.0049, -0.0027,  0.0073]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016914574429392815 tensor([[ 0.1031,  0.0878,  0.1003, -0.0087,  0.0349, -0.0377, -0.0051,  0.0313],\n",
            "        [ 0.1027,  0.0889,  0.1007, -0.0065,  0.0370, -0.0329, -0.0064,  0.0294],\n",
            "        [ 0.0735,  0.0608,  0.0682, -0.0060,  0.0247, -0.0273, -0.0034,  0.0276],\n",
            "        [ 0.0315,  0.0313,  0.0368, -0.0060,  0.0114, -0.0084, -0.0052,  0.0129]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015732407569885254 tensor([[ 0.1324,  0.1100,  0.1267, -0.0122,  0.0444, -0.0481, -0.0064,  0.0377],\n",
            "        [ 0.1324,  0.1126,  0.1282, -0.0087,  0.0476, -0.0407, -0.0086,  0.0344],\n",
            "        [ 0.0946,  0.0764,  0.0859, -0.0082,  0.0315, -0.0349, -0.0046,  0.0348],\n",
            "        [ 0.0432,  0.0431,  0.0511, -0.0089,  0.0152, -0.0109, -0.0076,  0.0173]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015195975080132484 tensor([[ 0.1531,  0.1235,  0.1435, -0.0153,  0.0507, -0.0551, -0.0074,  0.0406],\n",
            "        [ 0.1530,  0.1274,  0.1458, -0.0105,  0.0549, -0.0450, -0.0104,  0.0354],\n",
            "        [ 0.1098,  0.0863,  0.0973, -0.0102,  0.0362, -0.0402, -0.0054,  0.0394],\n",
            "        [ 0.0528,  0.0530,  0.0632, -0.0117,  0.0183, -0.0126, -0.0098,  0.0207]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014947673305869102 tensor([[ 0.1681,  0.1313,  0.1539, -0.0180,  0.0550, -0.0601, -0.0080,  0.0415],\n",
            "        [ 0.1677,  0.1363,  0.1570, -0.0121,  0.0600, -0.0469, -0.0119,  0.0339],\n",
            "        [ 0.1212,  0.0925,  0.1048, -0.0119,  0.0395, -0.0440, -0.0061,  0.0425],\n",
            "        [ 0.0607,  0.0613,  0.0737, -0.0144,  0.0206, -0.0136, -0.0118,  0.0232]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02077842690050602 tensor([[ 0.0681,  0.0632,  0.0682, -0.0035,  0.0240, -0.0256, -0.0029,  0.0229],\n",
            "        [ 0.0562,  0.0515,  0.0568, -0.0033,  0.0200, -0.0193, -0.0035,  0.0198],\n",
            "        [ 0.0602,  0.0520,  0.0598, -0.0039,  0.0217, -0.0206, -0.0017,  0.0195],\n",
            "        [ 0.0270,  0.0226,  0.0261, -0.0020,  0.0102, -0.0083, -0.0021,  0.0084]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01754906214773655 tensor([[ 0.1137,  0.1046,  0.1128, -0.0063,  0.0400, -0.0424, -0.0047,  0.0363],\n",
            "        [ 0.0922,  0.0835,  0.0922, -0.0057,  0.0326, -0.0308, -0.0062,  0.0307],\n",
            "        [ 0.1030,  0.0874,  0.1011, -0.0070,  0.0370, -0.0345, -0.0027,  0.0315],\n",
            "        [ 0.0484,  0.0397,  0.0462, -0.0036,  0.0184, -0.0141, -0.0041,  0.0138]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016130276024341583 tensor([[ 0.1448,  0.1315,  0.1415, -0.0084,  0.0507, -0.0538, -0.0058,  0.0439],\n",
            "        [ 0.1148,  0.1021,  0.1128, -0.0077,  0.0402, -0.0369, -0.0082,  0.0359],\n",
            "        [ 0.1326,  0.1101,  0.1283, -0.0095,  0.0473, -0.0432, -0.0031,  0.0379],\n",
            "        [ 0.0654,  0.0525,  0.0618, -0.0051,  0.0251, -0.0178, -0.0060,  0.0170]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01551417913287878 tensor([[ 0.1670,  0.1496,  0.1604, -0.0100,  0.0582, -0.0618, -0.0064,  0.0480],\n",
            "        [ 0.1295,  0.1128,  0.1247, -0.0094,  0.0448, -0.0398, -0.0097,  0.0377],\n",
            "        [ 0.1534,  0.1242,  0.1461, -0.0116,  0.0545, -0.0486, -0.0032,  0.0407],\n",
            "        [ 0.0792,  0.0622,  0.0739, -0.0064,  0.0305, -0.0200, -0.0077,  0.0184]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015232089906930923 tensor([[ 0.1835,  0.1619,  0.1728, -0.0115,  0.0637, -0.0676, -0.0068,  0.0499],\n",
            "        [ 0.1394,  0.1187,  0.1313, -0.0109,  0.0477, -0.0408, -0.0111,  0.0376],\n",
            "        [ 0.1686,  0.1328,  0.1577, -0.0134,  0.0594, -0.0518, -0.0030,  0.0412],\n",
            "        [ 0.0904,  0.0695,  0.0834, -0.0075,  0.0351, -0.0211, -0.0093,  0.0186]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020567387342453003 tensor([[ 0.0515,  0.0455,  0.0514, -0.0040,  0.0189, -0.0204, -0.0035,  0.0177],\n",
            "        [ 0.0721,  0.0648,  0.0721, -0.0042,  0.0257, -0.0253, -0.0033,  0.0221],\n",
            "        [ 0.0526,  0.0423,  0.0487, -0.0043,  0.0194, -0.0197, -0.0022,  0.0190],\n",
            "        [ 0.0275,  0.0214,  0.0237, -0.0018,  0.0088, -0.0095, -0.0015,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017566468566656113 tensor([[ 0.0869,  0.0757,  0.0864, -0.0075,  0.0321, -0.0343, -0.0062,  0.0282],\n",
            "        [ 0.1231,  0.1094,  0.1223, -0.0076,  0.0437, -0.0423, -0.0057,  0.0349],\n",
            "        [ 0.0915,  0.0717,  0.0829, -0.0079,  0.0340, -0.0342, -0.0039,  0.0321],\n",
            "        [ 0.0508,  0.0389,  0.0430, -0.0034,  0.0160, -0.0172, -0.0029,  0.0188]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016118250787258148 tensor([[ 0.1109,  0.0946,  0.1091, -0.0105,  0.0412, -0.0437, -0.0083,  0.0340],\n",
            "        [ 0.1569,  0.1371,  0.1538, -0.0103,  0.0556, -0.0527, -0.0073,  0.0404],\n",
            "        [ 0.1198,  0.0908,  0.1057, -0.0110,  0.0447, -0.0446, -0.0052,  0.0406],\n",
            "        [ 0.0705,  0.0531,  0.0588, -0.0048,  0.0217, -0.0234, -0.0041,  0.0255]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01546395756304264 tensor([[ 0.1275,  0.1060,  0.1237, -0.0132,  0.0476, -0.0502, -0.0100,  0.0369],\n",
            "        [ 0.1795,  0.1535,  0.1730, -0.0127,  0.0634, -0.0587, -0.0083,  0.0411],\n",
            "        [ 0.1408,  0.1029,  0.1207, -0.0137,  0.0528, -0.0521, -0.0062,  0.0459],\n",
            "        [ 0.0872,  0.0649,  0.0717, -0.0062,  0.0264, -0.0283, -0.0051,  0.0308]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015156712383031845 tensor([[ 0.1396,  0.1128,  0.1332, -0.0156,  0.0522, -0.0550, -0.0114,  0.0381],\n",
            "        [ 0.1953,  0.1628,  0.1845, -0.0147,  0.0686, -0.0621, -0.0091,  0.0390],\n",
            "        [ 0.1569,  0.1104,  0.1305, -0.0161,  0.0590, -0.0576, -0.0070,  0.0490],\n",
            "        [ 0.1016,  0.0745,  0.0822, -0.0074,  0.0302, -0.0323, -0.0060,  0.0349]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022376881912350655 tensor([[ 0.0555,  0.0539,  0.0599, -0.0036,  0.0212, -0.0214, -0.0027,  0.0173],\n",
            "        [ 0.0690,  0.0601,  0.0689, -0.0050,  0.0236, -0.0248, -0.0039,  0.0251],\n",
            "        [ 0.0468,  0.0414,  0.0468, -0.0036,  0.0186, -0.0157, -0.0040,  0.0140],\n",
            "        [ 0.0278,  0.0252,  0.0298, -0.0024,  0.0108, -0.0109, -0.0008,  0.0089]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019407715648412704 tensor([[ 0.0922,  0.0899,  0.1003, -0.0066,  0.0356, -0.0354, -0.0045,  0.0264],\n",
            "        [ 0.1171,  0.1000,  0.1152, -0.0092,  0.0392, -0.0418, -0.0069,  0.0415],\n",
            "        [ 0.0772,  0.0666,  0.0759, -0.0063,  0.0317, -0.0247, -0.0075,  0.0205],\n",
            "        [ 0.0503,  0.0455,  0.0544, -0.0047,  0.0196, -0.0195, -0.0014,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018077190965414047 tensor([[ 0.1175,  0.1143,  0.1283, -0.0091,  0.0459, -0.0449, -0.0056,  0.0307],\n",
            "        [ 0.1483,  0.1233,  0.1429, -0.0127,  0.0485, -0.0526, -0.0091,  0.0512],\n",
            "        [ 0.0962,  0.0805,  0.0927, -0.0085,  0.0408, -0.0290, -0.0105,  0.0219],\n",
            "        [ 0.0687,  0.0620,  0.0750, -0.0069,  0.0270, -0.0263, -0.0019,  0.0197]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.0174981988966465 tensor([[ 0.1357,  0.1316,  0.1482, -0.0114,  0.0535, -0.0516, -0.0064,  0.0322],\n",
            "        [ 0.1686,  0.1356,  0.1582, -0.0157,  0.0536, -0.0592, -0.0107,  0.0567],\n",
            "        [ 0.1086,  0.0877,  0.1023, -0.0103,  0.0475, -0.0305, -0.0132,  0.0201],\n",
            "        [ 0.0838,  0.0755,  0.0923, -0.0089,  0.0332, -0.0317, -0.0022,  0.0225]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.0172240249812603 tensor([[ 0.1493,  0.1441,  0.1631, -0.0134,  0.0595, -0.0565, -0.0069,  0.0319],\n",
            "        [ 0.1822,  0.1414,  0.1661, -0.0184,  0.0562, -0.0635, -0.0121,  0.0596],\n",
            "        [ 0.1172,  0.0910,  0.1078, -0.0120,  0.0528, -0.0303, -0.0156,  0.0165],\n",
            "        [ 0.0963,  0.0866,  0.1070, -0.0109,  0.0384, -0.0359, -0.0024,  0.0242]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022845838218927383 tensor([[ 0.0727,  0.0677,  0.0766, -0.0040,  0.0259, -0.0242, -0.0039,  0.0240],\n",
            "        [ 0.0702,  0.0647,  0.0731, -0.0048,  0.0264, -0.0256, -0.0033,  0.0218],\n",
            "        [ 0.0552,  0.0450,  0.0513, -0.0037,  0.0215, -0.0198, -0.0034,  0.0198],\n",
            "        [ 0.0296,  0.0244,  0.0271, -0.0020,  0.0106, -0.0110, -0.0011,  0.0106]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019079219549894333 tensor([[ 0.1171,  0.1077,  0.1229, -0.0069,  0.0414, -0.0377, -0.0064,  0.0359],\n",
            "        [ 0.1155,  0.1065,  0.1211, -0.0087,  0.0437, -0.0414, -0.0056,  0.0325],\n",
            "        [ 0.0953,  0.0753,  0.0863, -0.0067,  0.0380, -0.0339, -0.0064,  0.0330],\n",
            "        [ 0.0547,  0.0445,  0.0495, -0.0037,  0.0196, -0.0200, -0.0021,  0.0189]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017504870891571045 tensor([[ 0.1448,  0.1307,  0.1504, -0.0091,  0.0507, -0.0447, -0.0080,  0.0409],\n",
            "        [ 0.1424,  0.1306,  0.1495, -0.0119,  0.0543, -0.0498, -0.0070,  0.0353],\n",
            "        [ 0.1242,  0.0947,  0.1093, -0.0093,  0.0506, -0.0436, -0.0090,  0.0412],\n",
            "        [ 0.0760,  0.0611,  0.0681, -0.0051,  0.0272, -0.0274, -0.0030,  0.0255]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.0168552678078413 tensor([[ 0.1632,  0.1442,  0.1674, -0.0110,  0.0567, -0.0482, -0.0091,  0.0423],\n",
            "        [ 0.1582,  0.1437,  0.1659, -0.0146,  0.0606, -0.0539, -0.0079,  0.0334],\n",
            "        [ 0.1458,  0.1068,  0.1242, -0.0115,  0.0606, -0.0504, -0.0112,  0.0462],\n",
            "        [ 0.0942,  0.0749,  0.0835, -0.0065,  0.0337, -0.0335, -0.0037,  0.0305]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016556894406676292 tensor([[ 0.1763,  0.1522,  0.1782, -0.0125,  0.0608, -0.0498, -0.0100,  0.0416],\n",
            "        [ 0.1675,  0.1505,  0.1752, -0.0171,  0.0646, -0.0555, -0.0085,  0.0290],\n",
            "        [ 0.1625,  0.1141,  0.1339, -0.0134,  0.0689, -0.0552, -0.0133,  0.0491],\n",
            "        [ 0.1097,  0.0863,  0.0963, -0.0077,  0.0393, -0.0385, -0.0044,  0.0343]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022554166615009308 tensor([[ 0.0595,  0.0528,  0.0591, -0.0034,  0.0215, -0.0203, -0.0034,  0.0195],\n",
            "        [ 0.0739,  0.0701,  0.0777, -0.0046,  0.0271, -0.0263, -0.0029,  0.0237],\n",
            "        [ 0.0461,  0.0350,  0.0401, -0.0030,  0.0168, -0.0165, -0.0023,  0.0156],\n",
            "        [ 0.0227,  0.0192,  0.0235, -0.0017,  0.0081, -0.0070, -0.0003,  0.0053]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019331401214003563 tensor([[ 0.1020,  0.0893,  0.1008, -0.0062,  0.0367, -0.0336, -0.0060,  0.0309],\n",
            "        [ 0.1296,  0.1236,  0.1372, -0.0086,  0.0477, -0.0454, -0.0051,  0.0393],\n",
            "        [ 0.0806,  0.0590,  0.0679, -0.0055,  0.0299, -0.0287, -0.0043,  0.0262],\n",
            "        [ 0.0408,  0.0342,  0.0423, -0.0032,  0.0144, -0.0120, -0.0004,  0.0084]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017653271555900574 tensor([[ 0.1312,  0.1127,  0.1282, -0.0086,  0.0471, -0.0417, -0.0078,  0.0366],\n",
            "        [ 0.1671,  0.1595,  0.1771, -0.0120,  0.0617, -0.0575, -0.0067,  0.0470],\n",
            "        [ 0.1055,  0.0737,  0.0857, -0.0075,  0.0396, -0.0370, -0.0059,  0.0326],\n",
            "        [ 0.0554,  0.0461,  0.0579, -0.0045,  0.0195, -0.0155, -0.0003,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016904456540942192 tensor([[ 1.5154e-01,  1.2710e-01,  1.4574e-01, -1.0595e-02,  5.4317e-02,\n",
            "         -4.6425e-02, -9.1541e-03,  3.8737e-02],\n",
            "        [ 1.9110e-01,  1.8173e-01,  2.0201e-01, -1.4770e-02,  7.0658e-02,\n",
            "         -6.4373e-02, -7.6205e-03,  4.9173e-02],\n",
            "        [ 1.2368e-01,  8.1973e-02,  9.6531e-02, -9.1124e-03,  4.6954e-02,\n",
            "         -4.2572e-02, -7.2956e-03,  3.5970e-02],\n",
            "        [ 6.7415e-02,  5.5714e-02,  7.1058e-02, -5.5939e-03,  2.3654e-02,\n",
            "         -1.7780e-02, -1.0610e-04,  1.0164e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01658095233142376 tensor([[ 1.6625e-01,  1.3572e-01,  1.5690e-01, -1.2308e-02,  5.9457e-02,\n",
            "         -4.8974e-02, -1.0208e-02,  3.8682e-02],\n",
            "        [ 2.0633e-01,  1.9515e-01,  2.1711e-01, -1.7192e-02,  7.6418e-02,\n",
            "         -6.7856e-02, -8.1837e-03,  4.7691e-02],\n",
            "        [ 1.3761e-01,  8.6107e-02,  1.0281e-01, -1.0521e-02,  5.2738e-02,\n",
            "         -4.6368e-02, -8.5002e-03,  3.7438e-02],\n",
            "        [ 7.7429e-02,  6.3591e-02,  8.2283e-02, -6.5863e-03,  2.7025e-02,\n",
            "         -1.9220e-02,  1.6868e-04,  9.5034e-03]], device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[6, 4, 8, 4, 4, 4, 10, 4, 4, 2, 4, 6, 6, 6, 11, 4, 4, 6, 2, 6, 4, 6, 2, 6, 4, 6, 5, 11, 4, 6, 6, 6, 6, 12, 11, 5, 6, 13, 5, 11, 4, 1, 4, 6, 4, 2, 4, 12, 4, 2, 4, 6, 4, 9, 9, 9, 6, 5, 6, 11, 4, 6, 11, 4, 6, 6, 2, 4, 4, 6, 6, 6, 4, 6, 11, 6, 12, 11, 6, 6, 6, 11, 6, 2, 6, 6, 5, 2, 4, 6, 6, 6, 4, 6, 11, 6, 4, 6, 3, 6, 4, 8, 6, 8, 4, 6, 10, 13, 4, 4, 4, 4, 4, 6]\n",
            "c tensor([5.5432e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01720639504492283 tensor([[ 0.0691,  0.0639,  0.0711, -0.0042,  0.0252, -0.0244, -0.0039,  0.0235],\n",
            "        [ 0.0558,  0.0505,  0.0571, -0.0040,  0.0209, -0.0203, -0.0034,  0.0191],\n",
            "        [ 0.0605,  0.0568,  0.0647, -0.0045,  0.0227, -0.0207, -0.0027,  0.0188],\n",
            "        [ 0.0297,  0.0230,  0.0264, -0.0022,  0.0107, -0.0120, -0.0013,  0.0121]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.013715740293264389 tensor([[ 0.1185,  0.1089,  0.1219, -0.0077,  0.0432, -0.0410, -0.0067,  0.0383],\n",
            "        [ 0.0940,  0.0846,  0.0958, -0.0072,  0.0357, -0.0338, -0.0061,  0.0306],\n",
            "        [ 0.1020,  0.0956,  0.1095, -0.0083,  0.0388, -0.0340, -0.0047,  0.0293],\n",
            "        [ 0.0555,  0.0421,  0.0486, -0.0041,  0.0199, -0.0223, -0.0023,  0.0223]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.012022064998745918 tensor([[ 0.1536,  0.1397,  0.1571, -0.0106,  0.0560, -0.0521, -0.0089,  0.0470],\n",
            "        [ 0.1183,  0.1053,  0.1195, -0.0098,  0.0454, -0.0420, -0.0082,  0.0362],\n",
            "        [ 0.1294,  0.1204,  0.1391, -0.0116,  0.0497, -0.0418, -0.0061,  0.0334],\n",
            "        [ 0.0778,  0.0579,  0.0671, -0.0059,  0.0276, -0.0311, -0.0031,  0.0308]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.011233380064368248 tensor([[ 0.1792,  0.1608,  0.1815, -0.0132,  0.0653, -0.0596, -0.0105,  0.0518],\n",
            "        [ 0.1335,  0.1172,  0.1332, -0.0119,  0.0518, -0.0466, -0.0099,  0.0382],\n",
            "        [ 0.1479,  0.1364,  0.1590, -0.0144,  0.0573, -0.0460, -0.0071,  0.0336],\n",
            "        [ 0.0972,  0.0710,  0.0827, -0.0074,  0.0341, -0.0386, -0.0037,  0.0378]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01084485650062561 tensor([[ 0.1985,  0.1754,  0.1988, -0.0155,  0.0722, -0.0646, -0.0119,  0.0541],\n",
            "        [ 0.1432,  0.1235,  0.1406, -0.0137,  0.0561, -0.0492, -0.0113,  0.0378],\n",
            "        [ 0.1608,  0.1468,  0.1728, -0.0171,  0.0630, -0.0479, -0.0079,  0.0312],\n",
            "        [ 0.1139,  0.0816,  0.0956, -0.0089,  0.0396, -0.0450, -0.0042,  0.0437]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02603219822049141 tensor([[ 0.0673,  0.0642,  0.0725, -0.0050,  0.0245, -0.0226, -0.0037,  0.0211],\n",
            "        [ 0.0650,  0.0604,  0.0693, -0.0048,  0.0256, -0.0228, -0.0053,  0.0232],\n",
            "        [ 0.0517,  0.0486,  0.0529, -0.0031,  0.0183, -0.0219, -0.0011,  0.0189],\n",
            "        [ 0.0315,  0.0199,  0.0218, -0.0015,  0.0116, -0.0106, -0.0019,  0.0112]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.022618109360337257 tensor([[ 0.1112,  0.1058,  0.1206, -0.0090,  0.0405, -0.0360, -0.0062,  0.0318],\n",
            "        [ 0.1052,  0.0971,  0.1126, -0.0089,  0.0423, -0.0362, -0.0095,  0.0356],\n",
            "        [ 0.0866,  0.0809,  0.0876, -0.0054,  0.0301, -0.0373, -0.0015,  0.0304],\n",
            "        [ 0.0582,  0.0348,  0.0382, -0.0027,  0.0216, -0.0190, -0.0037,  0.0199]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02118859253823757 tensor([[ 0.1396,  0.1320,  0.1516, -0.0124,  0.0509, -0.0434, -0.0079,  0.0362],\n",
            "        [ 0.1296,  0.1180,  0.1386, -0.0123,  0.0531, -0.0435, -0.0130,  0.0412],\n",
            "        [ 0.1094,  0.1016,  0.1094, -0.0074,  0.0373, -0.0480, -0.0012,  0.0367],\n",
            "        [ 0.0813,  0.0459,  0.0504, -0.0037,  0.0304, -0.0256, -0.0054,  0.0268]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020591238513588905 tensor([[ 0.1587,  0.1486,  0.1722, -0.0154,  0.0578, -0.0474, -0.0092,  0.0368],\n",
            "        [ 0.1450,  0.1299,  0.1547, -0.0155,  0.0605, -0.0472, -0.0160,  0.0429],\n",
            "        [ 0.1244,  0.1148,  0.1229, -0.0090,  0.0416, -0.0557, -0.0007,  0.0397],\n",
            "        [ 0.1014,  0.0542,  0.0594, -0.0046,  0.0383, -0.0309, -0.0070,  0.0322]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02031828463077545 tensor([[ 1.7214e-01,  1.5945e-01,  1.8619e-01, -1.8084e-02,  6.2692e-02,\n",
            "         -4.9261e-02, -1.0208e-02,  3.5135e-02],\n",
            "        [ 1.5521e-01,  1.3647e-01,  1.6509e-01, -1.8394e-02,  6.5975e-02,\n",
            "         -4.8934e-02, -1.8637e-02,  4.2316e-02],\n",
            "        [ 1.3439e-01,  1.2320e-01,  1.3109e-01, -1.0394e-02,  4.3964e-02,\n",
            "         -6.1312e-02,  1.4544e-04,  4.0572e-02],\n",
            "        [ 1.1923e-01,  6.0143e-02,  6.5951e-02, -5.2857e-03,  4.5285e-02,\n",
            "         -3.5248e-02, -8.5986e-03,  3.6521e-02]], device='cuda:0')\n",
            "c tensor([2.1636e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019291354343295097 tensor([[ 0.0585,  0.0519,  0.0587, -0.0040,  0.0207, -0.0201, -0.0033,  0.0188],\n",
            "        [ 0.0647,  0.0578,  0.0655, -0.0040,  0.0239, -0.0221, -0.0035,  0.0216],\n",
            "        [ 0.0407,  0.0351,  0.0398, -0.0022,  0.0144, -0.0146, -0.0022,  0.0148],\n",
            "        [ 0.0251,  0.0233,  0.0272, -0.0031,  0.0100, -0.0094, -0.0020,  0.0072]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016613641753792763 tensor([[ 0.1007,  0.0880,  0.1004, -0.0076,  0.0354, -0.0336, -0.0057,  0.0302],\n",
            "        [ 0.1099,  0.0972,  0.1105, -0.0071,  0.0409, -0.0367, -0.0061,  0.0347],\n",
            "        [ 0.0682,  0.0577,  0.0657, -0.0036,  0.0239, -0.0241, -0.0037,  0.0237],\n",
            "        [ 0.0453,  0.0420,  0.0495, -0.0060,  0.0182, -0.0166, -0.0039,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015328672714531422 tensor([[ 0.1310,  0.1121,  0.1291, -0.0107,  0.0459, -0.0423, -0.0075,  0.0365],\n",
            "        [ 0.1406,  0.1224,  0.1397, -0.0096,  0.0527, -0.0458, -0.0082,  0.0416],\n",
            "        [ 0.0864,  0.0714,  0.0817, -0.0045,  0.0300, -0.0300, -0.0047,  0.0286],\n",
            "        [ 0.0618,  0.0572,  0.0681, -0.0087,  0.0251, -0.0222, -0.0057,  0.0150]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014730216935276985 tensor([[ 0.1532,  0.1279,  0.1488, -0.0135,  0.0533, -0.0479, -0.0088,  0.0395],\n",
            "        [ 0.1617,  0.1383,  0.1586, -0.0116,  0.0612, -0.0512, -0.0099,  0.0444],\n",
            "        [ 0.0987,  0.0794,  0.0916, -0.0051,  0.0337, -0.0335, -0.0054,  0.0309],\n",
            "        [ 0.0752,  0.0696,  0.0837, -0.0113,  0.0310, -0.0265, -0.0073,  0.0165]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014440128579735756 tensor([[ 0.1699,  0.1382,  0.1623, -0.0161,  0.0588, -0.0515, -0.0098,  0.0404],\n",
            "        [ 0.1769,  0.1482,  0.1709, -0.0133,  0.0674, -0.0542, -0.0112,  0.0447],\n",
            "        [ 0.1072,  0.0838,  0.0975, -0.0055,  0.0361, -0.0354, -0.0058,  0.0315],\n",
            "        [ 0.0862,  0.0798,  0.0968, -0.0138,  0.0359, -0.0297, -0.0088,  0.0169]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01949978992342949 tensor([[ 0.0692,  0.0578,  0.0647, -0.0034,  0.0249, -0.0234, -0.0033,  0.0216],\n",
            "        [ 0.0608,  0.0574,  0.0631, -0.0035,  0.0219, -0.0218, -0.0032,  0.0204],\n",
            "        [ 0.0573,  0.0528,  0.0612, -0.0033,  0.0206, -0.0191, -0.0026,  0.0178],\n",
            "        [ 0.0254,  0.0202,  0.0238, -0.0020,  0.0100, -0.0101, -0.0010,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01612148806452751 tensor([[ 0.1212,  0.0996,  0.1122, -0.0064,  0.0436, -0.0400, -0.0058,  0.0357],\n",
            "        [ 0.1027,  0.0972,  0.1065, -0.0063,  0.0372, -0.0364, -0.0056,  0.0326],\n",
            "        [ 0.0981,  0.0898,  0.1048, -0.0058,  0.0349, -0.0318, -0.0044,  0.0281],\n",
            "        [ 0.0466,  0.0363,  0.0433, -0.0037,  0.0186, -0.0185, -0.0018,  0.0144]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.014433358795940876 tensor([[ 0.1598,  0.1284,  0.1454, -0.0088,  0.0574, -0.0515, -0.0076,  0.0442],\n",
            "        [ 0.1299,  0.1228,  0.1341, -0.0084,  0.0472, -0.0453, -0.0073,  0.0384],\n",
            "        [ 0.1255,  0.1137,  0.1339, -0.0077,  0.0440, -0.0392, -0.0057,  0.0325],\n",
            "        [ 0.0645,  0.0494,  0.0595, -0.0053,  0.0259, -0.0253, -0.0025,  0.0189]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.013637020252645016 tensor([[ 0.1892,  0.1479,  0.1684, -0.0109,  0.0679, -0.0596, -0.0090,  0.0492],\n",
            "        [ 0.1475,  0.1390,  0.1513, -0.0101,  0.0537, -0.0504, -0.0086,  0.0401],\n",
            "        [ 0.1440,  0.1290,  0.1535, -0.0092,  0.0497, -0.0431, -0.0064,  0.0329],\n",
            "        [ 0.0796,  0.0599,  0.0729, -0.0067,  0.0322, -0.0309, -0.0031,  0.0221]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013249396346509457 tensor([[ 0.2121,  0.1610,  0.1842, -0.0126,  0.0760, -0.0652, -0.0101,  0.0517],\n",
            "        [ 0.1592,  0.1493,  0.1619, -0.0115,  0.0581, -0.0533, -0.0096,  0.0393],\n",
            "        [ 0.1569,  0.1387,  0.1670, -0.0105,  0.0533, -0.0446, -0.0069,  0.0308],\n",
            "        [ 0.0925,  0.0683,  0.0841, -0.0080,  0.0377, -0.0355, -0.0036,  0.0243]],\n",
            "       device='cuda:0')\n",
            "c tensor([3.5763e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018000837415456772 tensor([[ 0.0669,  0.0608,  0.0675, -0.0049,  0.0255, -0.0227, -0.0051,  0.0219],\n",
            "        [ 0.0631,  0.0553,  0.0608, -0.0031,  0.0219, -0.0228, -0.0033,  0.0214],\n",
            "        [ 0.0565,  0.0485,  0.0574, -0.0041,  0.0215, -0.0186, -0.0026,  0.0179],\n",
            "        [ 0.0276,  0.0211,  0.0244, -0.0019,  0.0106, -0.0095, -0.0013,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014646528288722038 tensor([[ 0.1157,  0.1046,  0.1168, -0.0092,  0.0444, -0.0384, -0.0094,  0.0359],\n",
            "        [ 0.1085,  0.0941,  0.1033, -0.0055,  0.0376, -0.0388, -0.0058,  0.0351],\n",
            "        [ 0.0981,  0.0829,  0.0995, -0.0076,  0.0377, -0.0313, -0.0048,  0.0291],\n",
            "        [ 0.0505,  0.0378,  0.0441, -0.0035,  0.0196, -0.0170, -0.0024,  0.0150]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.012931734323501587 tensor([[ 0.1503,  0.1344,  0.1508, -0.0130,  0.0581, -0.0485, -0.0128,  0.0440],\n",
            "        [ 0.1396,  0.1190,  0.1304, -0.0071,  0.0482, -0.0493, -0.0078,  0.0426],\n",
            "        [ 0.1278,  0.1056,  0.1289, -0.0105,  0.0496, -0.0392, -0.0064,  0.0350],\n",
            "        [ 0.0697,  0.0510,  0.0600, -0.0049,  0.0273, -0.0227, -0.0033,  0.0195]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.0121176578104496 tensor([[ 0.1749,  0.1541,  0.1737, -0.0164,  0.0683, -0.0550, -0.0158,  0.0482],\n",
            "        [ 0.1610,  0.1342,  0.1470, -0.0083,  0.0552, -0.0559, -0.0093,  0.0460],\n",
            "        [ 0.1490,  0.1201,  0.1493, -0.0130,  0.0584, -0.0438, -0.0077,  0.0371],\n",
            "        [ 0.0860,  0.0615,  0.0731, -0.0062,  0.0339, -0.0272, -0.0041,  0.0226]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.011728626675903797 tensor([[ 0.1928,  0.1671,  0.1890, -0.0195,  0.0759, -0.0589, -0.0183,  0.0498],\n",
            "        [ 0.1762,  0.1432,  0.1567, -0.0093,  0.0600, -0.0601, -0.0105,  0.0468],\n",
            "        [ 0.1645,  0.1291,  0.1636, -0.0153,  0.0650, -0.0461, -0.0087,  0.0368],\n",
            "        [ 0.0999,  0.0699,  0.0838, -0.0073,  0.0397, -0.0307, -0.0048,  0.0245]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7285e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021554697304964066 tensor([[ 0.0617,  0.0617,  0.0664, -0.0029,  0.0216, -0.0216, -0.0032,  0.0203],\n",
            "        [ 0.0741,  0.0671,  0.0768, -0.0051,  0.0274, -0.0265, -0.0030,  0.0249],\n",
            "        [ 0.0524,  0.0450,  0.0500, -0.0037,  0.0199, -0.0198, -0.0032,  0.0195],\n",
            "        [ 0.0282,  0.0252,  0.0295, -0.0032,  0.0111, -0.0108, -0.0016,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0179341621696949 tensor([[ 0.1036,  0.1041,  0.1121, -0.0053,  0.0359, -0.0353, -0.0053,  0.0315],\n",
            "        [ 0.1263,  0.1138,  0.1311, -0.0094,  0.0472, -0.0444, -0.0053,  0.0401],\n",
            "        [ 0.0884,  0.0743,  0.0821, -0.0066,  0.0340, -0.0337, -0.0058,  0.0322],\n",
            "        [ 0.0512,  0.0457,  0.0539, -0.0063,  0.0202, -0.0195, -0.0030,  0.0167]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016230899840593338 tensor([[ 0.1315,  0.1324,  0.1424, -0.0071,  0.0452, -0.0435, -0.0066,  0.0368],\n",
            "        [ 0.1604,  0.1429,  0.1661, -0.0129,  0.0604, -0.0551, -0.0068,  0.0478],\n",
            "        [ 0.1125,  0.0918,  0.1011, -0.0088,  0.0438, -0.0431, -0.0077,  0.0398],\n",
            "        [ 0.0700,  0.0624,  0.0744, -0.0093,  0.0277, -0.0265, -0.0042,  0.0217]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015484140254557133 tensor([[ 0.1506,  0.1514,  0.1627, -0.0086,  0.0512, -0.0483, -0.0075,  0.0384],\n",
            "        [ 0.1826,  0.1601,  0.1879, -0.0159,  0.0693, -0.0613, -0.0079,  0.0506],\n",
            "        [ 0.1293,  0.1021,  0.1120, -0.0107,  0.0508, -0.0497, -0.0092,  0.0442],\n",
            "        [ 0.0854,  0.0763,  0.0916, -0.0122,  0.0338, -0.0320, -0.0051,  0.0252]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015141802839934826 tensor([[ 0.1639,  0.1646,  0.1764, -0.0099,  0.0550, -0.0509, -0.0081,  0.0378],\n",
            "        [ 0.1974,  0.1700,  0.2014, -0.0185,  0.0755, -0.0645, -0.0086,  0.0503],\n",
            "        [ 0.1416,  0.1078,  0.1180, -0.0124,  0.0562, -0.0545, -0.0105,  0.0466],\n",
            "        [ 0.0982,  0.0878,  0.1063, -0.0150,  0.0390, -0.0364, -0.0060,  0.0274]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024220455437898636 tensor([[ 0.0607,  0.0520,  0.0561, -0.0022,  0.0203, -0.0204, -0.0035,  0.0202],\n",
            "        [ 0.0637,  0.0578,  0.0654, -0.0045,  0.0229, -0.0225, -0.0030,  0.0230],\n",
            "        [ 0.0435,  0.0342,  0.0402, -0.0032,  0.0149, -0.0157, -0.0020,  0.0145],\n",
            "        [ 0.0288,  0.0257,  0.0268, -0.0016,  0.0103, -0.0100, -0.0013,  0.0084]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02150586061179638 tensor([[ 0.1044,  0.0876,  0.0946, -0.0040,  0.0344, -0.0339, -0.0062,  0.0326],\n",
            "        [ 0.1062,  0.0958,  0.1085, -0.0080,  0.0381, -0.0372, -0.0052,  0.0372],\n",
            "        [ 0.0757,  0.0580,  0.0688, -0.0061,  0.0257, -0.0270, -0.0034,  0.0237],\n",
            "        [ 0.0525,  0.0464,  0.0481, -0.0030,  0.0188, -0.0178, -0.0024,  0.0144]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020220361649990082 tensor([[ 0.1364,  0.1113,  0.1203, -0.0054,  0.0442, -0.0427, -0.0082,  0.0399],\n",
            "        [ 0.1334,  0.1190,  0.1348, -0.0108,  0.0477, -0.0460, -0.0068,  0.0450],\n",
            "        [ 0.0990,  0.0734,  0.0883, -0.0085,  0.0332, -0.0347, -0.0043,  0.0287],\n",
            "        [ 0.0721,  0.0632,  0.0653, -0.0042,  0.0260, -0.0239, -0.0034,  0.0184]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019624829292297363 tensor([[ 0.1605,  0.1273,  0.1373, -0.0065,  0.0511, -0.0486, -0.0098,  0.0441],\n",
            "        [ 0.1508,  0.1325,  0.1505, -0.0131,  0.0538, -0.0513, -0.0080,  0.0488],\n",
            "        [ 0.1159,  0.0827,  0.1011, -0.0107,  0.0383, -0.0398, -0.0048,  0.0308],\n",
            "        [ 0.0885,  0.0771,  0.0793, -0.0053,  0.0321, -0.0286, -0.0043,  0.0210]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019337134435772896 tensor([[ 0.1796,  0.1380,  0.1485, -0.0075,  0.0561, -0.0525, -0.0111,  0.0462],\n",
            "        [ 0.1623,  0.1402,  0.1595, -0.0151,  0.0577, -0.0542, -0.0089,  0.0502],\n",
            "        [ 0.1285,  0.0879,  0.1095, -0.0127,  0.0418, -0.0431, -0.0050,  0.0309],\n",
            "        [ 0.1023,  0.0886,  0.0908, -0.0064,  0.0374, -0.0323, -0.0051,  0.0224]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[4, 11, 4, 3, 4, 4, 5, 11, 4, 12, 5, 6, 6, 4, 6, 2, 4, 6, 12, 6, 4, 2, 4, 1, 6, 11, 6, 6, 6, 6]\n",
            "0 #### train ####\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-0c232f84ef29>:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "update_h0 loss, lz 4 0.014816704206168652 tensor([[ 0.1457,  0.1072,  0.1202, -0.0119,  0.0676, -0.0437, -0.0206,  0.0125],\n",
            "        [ 0.0738,  0.0411,  0.0483, -0.0144,  0.0352, -0.0222, -0.0147,  0.0092],\n",
            "        [ 0.0687,  0.0681,  0.0865, -0.0082,  0.0285, -0.0108, -0.0070, -0.0019],\n",
            "        [ 0.0253,  0.0086,  0.0188, -0.0052,  0.0174, -0.0020, -0.0060, -0.0047]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019147371873259544 tensor([[ 0.0616,  0.0580,  0.0681, -0.0067,  0.0307, -0.0257, -0.0088,  0.0194],\n",
            "        [ 0.0502,  0.0473,  0.0496, -0.0043,  0.0238, -0.0220, -0.0092,  0.0207],\n",
            "        [ 0.0533,  0.0430,  0.0486, -0.0050,  0.0258, -0.0241, -0.0083,  0.0233],\n",
            "        [ 0.0118,  0.0088,  0.0112, -0.0019,  0.0069, -0.0028, -0.0028,  0.0035]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017471997067332268 tensor([[ 0.0697,  0.0615,  0.0770, -0.0097,  0.0341, -0.0244, -0.0082,  0.0107],\n",
            "        [ 0.0434,  0.0377,  0.0375, -0.0046,  0.0187, -0.0152, -0.0086,  0.0104],\n",
            "        [ 0.0718,  0.0507,  0.0595, -0.0077,  0.0337, -0.0300, -0.0103,  0.0269],\n",
            "        [ 0.0173,  0.0114,  0.0165, -0.0032,  0.0107, -0.0015, -0.0044,  0.0027]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.0173586867749691 tensor([[ 0.0758,  0.0631,  0.0838, -0.0126,  0.0364, -0.0220, -0.0072,  0.0008],\n",
            "        [ 0.0431,  0.0347,  0.0326, -0.0054,  0.0170, -0.0117, -0.0091,  0.0041],\n",
            "        [ 0.0844,  0.0525,  0.0640, -0.0101,  0.0385, -0.0328, -0.0112,  0.0270],\n",
            "        [ 0.0209,  0.0122,  0.0198, -0.0045,  0.0135,  0.0008, -0.0056,  0.0006]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017309900373220444 tensor([[ 0.0805,  0.0633,  0.0891, -0.0154,  0.0380, -0.0191, -0.0060, -0.0098],\n",
            "        [ 0.0447,  0.0334,  0.0296, -0.0062,  0.0163, -0.0092, -0.0100, -0.0009],\n",
            "        [ 0.0939,  0.0513,  0.0652, -0.0123,  0.0417, -0.0341, -0.0116,  0.0254],\n",
            "        [ 0.0235,  0.0121,  0.0222, -0.0056,  0.0159,  0.0036, -0.0067, -0.0020]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017272943630814552 tensor([[ 0.0846,  0.0630,  0.0937, -0.0182,  0.0394, -0.0158, -0.0047, -0.0206],\n",
            "        [ 0.0471,  0.0331,  0.0276, -0.0070,  0.0160, -0.0072, -0.0109, -0.0054],\n",
            "        [ 0.1019,  0.0487,  0.0648, -0.0144,  0.0442, -0.0346, -0.0117,  0.0230],\n",
            "        [ 0.0257,  0.0116,  0.0242, -0.0067,  0.0180,  0.0066, -0.0077, -0.0049]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02255091443657875 tensor([[ 0.0790,  0.0736,  0.0791, -0.0073,  0.0394, -0.0355, -0.0132,  0.0315],\n",
            "        [ 0.1090,  0.1030,  0.1173, -0.0086,  0.0549, -0.0450, -0.0162,  0.0423],\n",
            "        [ 0.0707,  0.0599,  0.0652, -0.0056,  0.0361, -0.0336, -0.0122,  0.0310],\n",
            "        [ 0.0279,  0.0216,  0.0239, -0.0027,  0.0141, -0.0125, -0.0051,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01841755025088787 tensor([[ 0.0690,  0.0581,  0.0607, -0.0082,  0.0344, -0.0273, -0.0113,  0.0175],\n",
            "        [ 0.1025,  0.0903,  0.1097, -0.0094,  0.0504, -0.0308, -0.0115,  0.0209],\n",
            "        [ 0.0932,  0.0713,  0.0788, -0.0076,  0.0485, -0.0414, -0.0164,  0.0342],\n",
            "        [ 0.0436,  0.0308,  0.0348, -0.0043,  0.0219, -0.0179, -0.0082,  0.0156]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018161263316869736 tensor([[ 0.0720,  0.0558,  0.0569, -0.0100,  0.0359, -0.0256, -0.0116,  0.0103],\n",
            "        [ 0.1084,  0.0901,  0.1155, -0.0109,  0.0524, -0.0232, -0.0092,  0.0072],\n",
            "        [ 0.1089,  0.0762,  0.0852, -0.0093,  0.0571, -0.0457, -0.0193,  0.0334],\n",
            "        [ 0.0554,  0.0361,  0.0414, -0.0057,  0.0277, -0.0212, -0.0106,  0.0171]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018065165728330612 tensor([[ 0.0759,  0.0545,  0.0541, -0.0118,  0.0379, -0.0244, -0.0120,  0.0038],\n",
            "        [ 0.1128,  0.0884,  0.1196, -0.0123,  0.0536, -0.0151, -0.0066, -0.0069],\n",
            "        [ 0.1193,  0.0759,  0.0860, -0.0106,  0.0631, -0.0473, -0.0213,  0.0297],\n",
            "        [ 0.0646,  0.0387,  0.0453, -0.0069,  0.0322, -0.0230, -0.0124,  0.0169]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018000297248363495 tensor([[ 0.0805,  0.0540,  0.0523, -0.0137,  0.0403, -0.0235, -0.0125, -0.0021],\n",
            "        [ 0.1177,  0.0871,  0.1241, -0.0136,  0.0552, -0.0074, -0.0043, -0.0203],\n",
            "        [ 0.1277,  0.0736,  0.0846, -0.0118,  0.0680, -0.0479, -0.0230,  0.0250],\n",
            "        [ 0.0721,  0.0398,  0.0475, -0.0081,  0.0358, -0.0238, -0.0140,  0.0157]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021975073963403702 tensor([[ 0.0894,  0.0863,  0.0942, -0.0062,  0.0451, -0.0401, -0.0153,  0.0370],\n",
            "        [ 0.0860,  0.0787,  0.0877, -0.0082,  0.0443, -0.0381, -0.0151,  0.0360],\n",
            "        [ 0.0532,  0.0463,  0.0504, -0.0047,  0.0257, -0.0237, -0.0081,  0.0216],\n",
            "        [ 0.0121,  0.0106,  0.0124, -0.0008,  0.0064, -0.0044, -0.0025,  0.0038]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017861519008874893 tensor([[ 0.1132,  0.1067,  0.1173, -0.0084,  0.0566, -0.0467, -0.0188,  0.0383],\n",
            "        [ 0.1074,  0.0935,  0.1072, -0.0122,  0.0564, -0.0424, -0.0192,  0.0354],\n",
            "        [ 0.0671,  0.0529,  0.0576, -0.0066,  0.0313, -0.0270, -0.0096,  0.0212],\n",
            "        [ 0.0176,  0.0144,  0.0179, -0.0008,  0.0086, -0.0048, -0.0034,  0.0035]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017649836838245392 tensor([[ 0.1219,  0.1115,  0.1234, -0.0096,  0.0604, -0.0457, -0.0197,  0.0319],\n",
            "        [ 0.1137,  0.0930,  0.1102, -0.0151,  0.0607, -0.0389, -0.0205,  0.0263],\n",
            "        [ 0.0738,  0.0521,  0.0571, -0.0081,  0.0332, -0.0264, -0.0098,  0.0167],\n",
            "        [ 0.0212,  0.0166,  0.0216, -0.0006,  0.0098, -0.0041, -0.0039,  0.0021]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01760641671717167 tensor([[ 0.1277,  0.1134,  0.1263, -0.0106,  0.0627, -0.0433, -0.0201,  0.0239],\n",
            "        [ 0.1184,  0.0908,  0.1113, -0.0178,  0.0641, -0.0345, -0.0216,  0.0162],\n",
            "        [ 0.0796,  0.0505,  0.0556, -0.0094,  0.0346, -0.0255, -0.0099,  0.0117],\n",
            "        [ 0.0239,  0.0179,  0.0243, -0.0004,  0.0105, -0.0030, -0.0043,  0.0002]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01756891794502735 tensor([[ 1.3265e-01,  1.1433e-01,  1.2818e-01, -1.1556e-02,  6.4532e-02,\n",
            "         -4.0517e-02, -2.0302e-02,  1.5651e-02],\n",
            "        [ 1.2292e-01,  8.8648e-02,  1.1237e-01, -2.0549e-02,  6.7582e-02,\n",
            "         -3.0103e-02, -2.2619e-02,  6.2871e-03],\n",
            "        [ 8.5268e-02,  4.8810e-02,  5.4027e-02, -1.0802e-02,  3.6006e-02,\n",
            "         -2.4439e-02, -9.9921e-03,  6.7699e-03],\n",
            "        [ 2.6085e-02,  1.8678e-02,  2.6491e-02, -1.2755e-04,  1.0864e-02,\n",
            "         -1.5390e-03, -4.4954e-03, -1.9932e-03]], device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02618483640253544 tensor([[ 0.1156,  0.1065,  0.1159, -0.0100,  0.0583, -0.0526, -0.0201,  0.0484],\n",
            "        [ 0.0877,  0.0795,  0.0872, -0.0065,  0.0441, -0.0397, -0.0151,  0.0371],\n",
            "        [ 0.0726,  0.0656,  0.0732, -0.0057,  0.0373, -0.0320, -0.0116,  0.0294],\n",
            "        [ 0.0284,  0.0226,  0.0248, -0.0032,  0.0146, -0.0141, -0.0053,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020119627937674522 tensor([[ 0.1484,  0.1305,  0.1430, -0.0150,  0.0745, -0.0630, -0.0256,  0.0515],\n",
            "        [ 0.1001,  0.0836,  0.0931, -0.0083,  0.0497, -0.0405, -0.0174,  0.0320],\n",
            "        [ 0.1042,  0.0912,  0.1038, -0.0083,  0.0541, -0.0413, -0.0158,  0.0339],\n",
            "        [ 0.0467,  0.0340,  0.0376, -0.0055,  0.0239, -0.0227, -0.0086,  0.0191]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01963912695646286 tensor([[ 0.1620,  0.1346,  0.1485, -0.0187,  0.0808, -0.0638, -0.0277,  0.0445],\n",
            "        [ 0.1002,  0.0752,  0.0857, -0.0091,  0.0491, -0.0349, -0.0175,  0.0204],\n",
            "        [ 0.1198,  0.1007,  0.1170, -0.0097,  0.0625, -0.0421, -0.0172,  0.0293],\n",
            "        [ 0.0599,  0.0405,  0.0452, -0.0073,  0.0304, -0.0284, -0.0109,  0.0221]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01950109750032425 tensor([[ 0.1721,  0.1351,  0.1500, -0.0222,  0.0854, -0.0629, -0.0291,  0.0357],\n",
            "        [ 0.1020,  0.0685,  0.0801, -0.0100,  0.0494, -0.0303, -0.0179,  0.0102],\n",
            "        [ 0.1303,  0.1051,  0.1246, -0.0108,  0.0684, -0.0404, -0.0177,  0.0221],\n",
            "        [ 0.0703,  0.0442,  0.0498, -0.0089,  0.0355, -0.0326, -0.0127,  0.0234]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019419971853494644 tensor([[ 0.1809,  0.1343,  0.1500, -0.0256,  0.0893, -0.0614, -0.0304,  0.0263],\n",
            "        [ 0.1056,  0.0637,  0.0765, -0.0109,  0.0506, -0.0267, -0.0187,  0.0011],\n",
            "        [ 0.1385,  0.1070,  0.1296, -0.0118,  0.0730, -0.0376, -0.0178,  0.0137],\n",
            "        [ 0.0787,  0.0461,  0.0524, -0.0105,  0.0395, -0.0357, -0.0142,  0.0235]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, 5.9605e-08, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022922132164239883 tensor([[ 0.0918,  0.0906,  0.0988, -0.0073,  0.0455, -0.0386, -0.0153,  0.0336],\n",
            "        [ 0.1012,  0.0958,  0.1071, -0.0087,  0.0519, -0.0445, -0.0164,  0.0404],\n",
            "        [ 0.0631,  0.0543,  0.0592, -0.0061,  0.0323, -0.0281, -0.0108,  0.0277],\n",
            "        [ 0.0247,  0.0187,  0.0206, -0.0020,  0.0121, -0.0108, -0.0041,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018266433849930763 tensor([[ 0.0966,  0.0933,  0.1022, -0.0089,  0.0464, -0.0336, -0.0154,  0.0213],\n",
            "        [ 0.1122,  0.1014,  0.1172, -0.0116,  0.0572, -0.0420, -0.0166,  0.0299],\n",
            "        [ 0.0799,  0.0614,  0.0678, -0.0095,  0.0413, -0.0319, -0.0140,  0.0288],\n",
            "        [ 0.0391,  0.0263,  0.0297, -0.0028,  0.0188, -0.0153, -0.0062,  0.0122]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01813744381070137 tensor([[ 0.0994,  0.0940,  0.1034, -0.0104,  0.0464, -0.0276, -0.0151,  0.0083],\n",
            "        [ 0.1170,  0.1006,  0.1204, -0.0141,  0.0595, -0.0364, -0.0157,  0.0163],\n",
            "        [ 0.0891,  0.0611,  0.0683, -0.0124,  0.0465, -0.0318, -0.0159,  0.0258],\n",
            "        [ 0.0500,  0.0306,  0.0351, -0.0034,  0.0237, -0.0178, -0.0077,  0.0127]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018060781061649323 tensor([[ 0.1031,  0.0956,  0.1055, -0.0118,  0.0468, -0.0222, -0.0151, -0.0041],\n",
            "        [ 0.1219,  0.0999,  0.1237, -0.0165,  0.0617, -0.0309, -0.0149,  0.0029],\n",
            "        [ 0.0964,  0.0589,  0.0667, -0.0152,  0.0506, -0.0307, -0.0174,  0.0217],\n",
            "        [ 0.0589,  0.0328,  0.0383, -0.0039,  0.0275, -0.0191, -0.0089,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017995070666074753 tensor([[ 0.1071,  0.0975,  0.1080, -0.0133,  0.0474, -0.0169, -0.0150, -0.0162],\n",
            "        [ 0.1269,  0.0993,  0.1270, -0.0188,  0.0640, -0.0256, -0.0140, -0.0102],\n",
            "        [ 0.1029,  0.0559,  0.0643, -0.0179,  0.0543, -0.0293, -0.0189,  0.0173],\n",
            "        [ 0.0665,  0.0338,  0.0402, -0.0043,  0.0307, -0.0197, -0.0098,  0.0103]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025051143020391464 tensor([[ 0.1251,  0.1176,  0.1289, -0.0093,  0.0629, -0.0563, -0.0209,  0.0527],\n",
            "        [ 0.0946,  0.0845,  0.0921, -0.0074,  0.0471, -0.0430, -0.0158,  0.0398],\n",
            "        [ 0.0722,  0.0674,  0.0740, -0.0061,  0.0365, -0.0330, -0.0120,  0.0315],\n",
            "        [ 0.0282,  0.0241,  0.0270, -0.0034,  0.0145, -0.0137, -0.0052,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017187677323818207 tensor([[ 0.1703,  0.1569,  0.1744, -0.0139,  0.0852, -0.0713, -0.0278,  0.0599],\n",
            "        [ 0.1343,  0.1155,  0.1269, -0.0114,  0.0671, -0.0566, -0.0226,  0.0468],\n",
            "        [ 0.1084,  0.1003,  0.1100, -0.0094,  0.0550, -0.0472, -0.0179,  0.0421],\n",
            "        [ 0.0459,  0.0365,  0.0419, -0.0060,  0.0233, -0.0218, -0.0083,  0.0178]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016200875863432884 tensor([[ 0.1792,  0.1588,  0.1791, -0.0161,  0.0889, -0.0682, -0.0282,  0.0484],\n",
            "        [ 0.1455,  0.1173,  0.1304, -0.0135,  0.0724, -0.0551, -0.0245,  0.0377],\n",
            "        [ 0.1227,  0.1115,  0.1224, -0.0111,  0.0621, -0.0500, -0.0199,  0.0403],\n",
            "        [ 0.0579,  0.0431,  0.0508, -0.0081,  0.0288, -0.0267, -0.0102,  0.0201]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01608242467045784 tensor([[ 0.1848,  0.1573,  0.1800, -0.0179,  0.0910, -0.0636, -0.0281,  0.0354],\n",
            "        [ 0.1528,  0.1150,  0.1294, -0.0153,  0.0758, -0.0516, -0.0257,  0.0267],\n",
            "        [ 0.1310,  0.1167,  0.1285, -0.0125,  0.0662, -0.0498, -0.0210,  0.0353],\n",
            "        [ 0.0669,  0.0468,  0.0567, -0.0100,  0.0327, -0.0298, -0.0115,  0.0205]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01601058430969715 tensor([[ 0.1907,  0.1561,  0.1812, -0.0198,  0.0932, -0.0591, -0.0281,  0.0227],\n",
            "        [ 0.1596,  0.1123,  0.1279, -0.0170,  0.0789, -0.0479, -0.0269,  0.0156],\n",
            "        [ 0.1371,  0.1197,  0.1320, -0.0137,  0.0691, -0.0485, -0.0216,  0.0291],\n",
            "        [ 0.0739,  0.0488,  0.0607, -0.0118,  0.0357, -0.0320, -0.0125,  0.0197]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025666330009698868 tensor([[ 0.1068,  0.1051,  0.1152, -0.0088,  0.0552, -0.0456, -0.0190,  0.0409],\n",
            "        [ 0.0896,  0.0819,  0.0909, -0.0078,  0.0461, -0.0414, -0.0154,  0.0360],\n",
            "        [ 0.0638,  0.0549,  0.0603, -0.0051,  0.0327, -0.0282, -0.0114,  0.0268],\n",
            "        [ 0.0285,  0.0224,  0.0262, -0.0030,  0.0143, -0.0119, -0.0042,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020291123539209366 tensor([[ 0.1308,  0.1271,  0.1405, -0.0124,  0.0677, -0.0494, -0.0237,  0.0377],\n",
            "        [ 0.0997,  0.0844,  0.0962, -0.0105,  0.0515, -0.0421, -0.0172,  0.0276],\n",
            "        [ 0.0903,  0.0718,  0.0799, -0.0075,  0.0470, -0.0359, -0.0167,  0.0310],\n",
            "        [ 0.0456,  0.0330,  0.0400, -0.0048,  0.0227, -0.0171, -0.0062,  0.0117]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019962463527917862 tensor([[ 0.1399,  0.1337,  0.1488, -0.0150,  0.0725, -0.0458, -0.0257,  0.0267],\n",
            "        [ 0.1000,  0.0770,  0.0908, -0.0125,  0.0518, -0.0379, -0.0172,  0.0143],\n",
            "        [ 0.1056,  0.0775,  0.0873, -0.0092,  0.0555, -0.0376, -0.0199,  0.0288],\n",
            "        [ 0.0579,  0.0390,  0.0489, -0.0063,  0.0285, -0.0197, -0.0072,  0.0108]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01984497159719467 tensor([[ 0.1461,  0.1372,  0.1538, -0.0173,  0.0758, -0.0407, -0.0272,  0.0143],\n",
            "        [ 0.1025,  0.0719,  0.0879, -0.0146,  0.0533, -0.0349, -0.0176,  0.0026],\n",
            "        [ 0.1170,  0.0794,  0.0907, -0.0106,  0.0620, -0.0375, -0.0225,  0.0247],\n",
            "        [ 0.0676,  0.0424,  0.0550, -0.0077,  0.0330, -0.0207, -0.0077,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019762132316827774 tensor([[ 0.1511,  0.1396,  0.1576, -0.0197,  0.0785, -0.0350, -0.0285,  0.0012],\n",
            "        [ 0.1063,  0.0681,  0.0865, -0.0167,  0.0555, -0.0326, -0.0182, -0.0082],\n",
            "        [ 0.1263,  0.0793,  0.0917, -0.0118,  0.0674, -0.0363, -0.0247,  0.0194],\n",
            "        [ 0.0756,  0.0442,  0.0594, -0.0089,  0.0365, -0.0208, -0.0079,  0.0045]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.027692480012774467 tensor([[ 0.1046,  0.0960,  0.1041, -0.0086,  0.0529, -0.0475, -0.0183,  0.0447],\n",
            "        [ 0.0782,  0.0706,  0.0778, -0.0061,  0.0389, -0.0356, -0.0137,  0.0328],\n",
            "        [ 0.0619,  0.0615,  0.0676, -0.0053,  0.0316, -0.0274, -0.0103,  0.0250],\n",
            "        [ 0.0165,  0.0130,  0.0164, -0.0030,  0.0083, -0.0079, -0.0027,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021873202174901962 tensor([[ 0.1673,  0.1525,  0.1661, -0.0149,  0.0845, -0.0727, -0.0292,  0.0649],\n",
            "        [ 0.1175,  0.1033,  0.1146, -0.0099,  0.0584, -0.0508, -0.0210,  0.0429],\n",
            "        [ 0.0865,  0.0856,  0.0939, -0.0078,  0.0436, -0.0356, -0.0140,  0.0286],\n",
            "        [ 0.0245,  0.0165,  0.0235, -0.0052,  0.0118, -0.0108, -0.0036,  0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020447323098778725 tensor([[ 0.1994,  0.1781,  0.1945, -0.0192,  0.1004, -0.0823, -0.0349,  0.0686],\n",
            "        [ 0.1333,  0.1123,  0.1259, -0.0121,  0.0659, -0.0536, -0.0241,  0.0396],\n",
            "        [ 0.0915,  0.0900,  0.0988, -0.0089,  0.0455, -0.0337, -0.0142,  0.0215],\n",
            "        [ 0.0293,  0.0165,  0.0272, -0.0070,  0.0133, -0.0115, -0.0036,  0.0088]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02017771080136299 tensor([[ 0.2175,  0.1892,  0.2071, -0.0226,  0.1089, -0.0848, -0.0382,  0.0650],\n",
            "        [ 0.1411,  0.1131,  0.1284, -0.0138,  0.0691, -0.0522, -0.0258,  0.0318],\n",
            "        [ 0.0921,  0.0899,  0.0989, -0.0096,  0.0450, -0.0296, -0.0137,  0.0119],\n",
            "        [ 0.0329,  0.0155,  0.0298, -0.0088,  0.0142, -0.0116, -0.0034,  0.0081]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020093686878681183 tensor([[ 0.2297,  0.1941,  0.2130, -0.0255,  0.1144, -0.0843, -0.0404,  0.0582],\n",
            "        [ 0.1466,  0.1116,  0.1285, -0.0153,  0.0712, -0.0496, -0.0272,  0.0227],\n",
            "        [ 0.0925,  0.0895,  0.0988, -0.0103,  0.0445, -0.0254, -0.0131,  0.0024],\n",
            "        [ 0.0361,  0.0140,  0.0319, -0.0105,  0.0148, -0.0114, -0.0031,  0.0070]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02414262294769287 tensor([[ 0.1229,  0.1149,  0.1248, -0.0092,  0.0629, -0.0551, -0.0225,  0.0506],\n",
            "        [ 0.0999,  0.0924,  0.1010, -0.0076,  0.0498, -0.0452, -0.0155,  0.0411],\n",
            "        [ 0.0663,  0.0585,  0.0645, -0.0063,  0.0340, -0.0309, -0.0118,  0.0295],\n",
            "        [ 0.0291,  0.0259,  0.0290, -0.0031,  0.0149, -0.0140, -0.0046,  0.0122]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01712895929813385 tensor([[ 0.1514,  0.1352,  0.1481, -0.0126,  0.0775, -0.0617, -0.0280,  0.0483],\n",
            "        [ 0.1301,  0.1168,  0.1287, -0.0111,  0.0647, -0.0534, -0.0193,  0.0413],\n",
            "        [ 0.0951,  0.0794,  0.0881, -0.0099,  0.0491, -0.0423, -0.0174,  0.0375],\n",
            "        [ 0.0455,  0.0389,  0.0442, -0.0052,  0.0232, -0.0208, -0.0068,  0.0165]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016641810536384583 tensor([[ 0.1597,  0.1344,  0.1485, -0.0147,  0.0817, -0.0583, -0.0298,  0.0357],\n",
            "        [ 0.1379,  0.1184,  0.1318, -0.0131,  0.0682, -0.0499, -0.0193,  0.0290],\n",
            "        [ 0.1084,  0.0847,  0.0950, -0.0124,  0.0563, -0.0455, -0.0203,  0.0366],\n",
            "        [ 0.0561,  0.0463,  0.0535, -0.0068,  0.0286, -0.0244, -0.0079,  0.0172]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016538303345441818 tensor([[ 0.1670,  0.1326,  0.1478, -0.0166,  0.0855, -0.0545, -0.0314,  0.0227],\n",
            "        [ 0.1434,  0.1176,  0.1323, -0.0149,  0.0705, -0.0453, -0.0189,  0.0158],\n",
            "        [ 0.1174,  0.0857,  0.0971, -0.0146,  0.0612, -0.0465, -0.0224,  0.0332],\n",
            "        [ 0.0638,  0.0509,  0.0598, -0.0083,  0.0324, -0.0263, -0.0085,  0.0160]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016465693712234497 tensor([[ 0.1747,  0.1312,  0.1475, -0.0186,  0.0894, -0.0509, -0.0330,  0.0102],\n",
            "        [ 0.1488,  0.1167,  0.1327, -0.0167,  0.0728, -0.0408, -0.0185,  0.0028],\n",
            "        [ 0.1246,  0.0849,  0.0973, -0.0167,  0.0651, -0.0466, -0.0243,  0.0290],\n",
            "        [ 0.0698,  0.0538,  0.0643, -0.0097,  0.0354, -0.0273, -0.0088,  0.0138]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.03017398528754711 tensor([[ 0.1173,  0.1098,  0.1192, -0.0093,  0.0588, -0.0523, -0.0197,  0.0481],\n",
            "        [ 0.0781,  0.0659,  0.0747, -0.0070,  0.0393, -0.0350, -0.0135,  0.0336],\n",
            "        [ 0.0615,  0.0578,  0.0644, -0.0056,  0.0317, -0.0273, -0.0103,  0.0243],\n",
            "        [ 0.0221,  0.0184,  0.0208, -0.0029,  0.0118, -0.0099, -0.0038,  0.0065]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02453663945198059 tensor([[ 0.1522,  0.1383,  0.1502, -0.0136,  0.0756, -0.0626, -0.0254,  0.0510],\n",
            "        [ 0.1044,  0.0803,  0.0945, -0.0106,  0.0528, -0.0426, -0.0185,  0.0375],\n",
            "        [ 0.0822,  0.0754,  0.0851, -0.0084,  0.0426, -0.0335, -0.0135,  0.0253],\n",
            "        [ 0.0329,  0.0246,  0.0295, -0.0052,  0.0178, -0.0129, -0.0052,  0.0054]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.024139173328876495 tensor([[ 0.1611,  0.1401,  0.1520, -0.0160,  0.0792, -0.0600, -0.0268,  0.0405],\n",
            "        [ 0.1137,  0.0772,  0.0955, -0.0130,  0.0576, -0.0412, -0.0206,  0.0319],\n",
            "        [ 0.0888,  0.0788,  0.0907, -0.0103,  0.0462, -0.0323, -0.0141,  0.0185],\n",
            "        [ 0.0398,  0.0269,  0.0341, -0.0072,  0.0218, -0.0135, -0.0058,  0.0017]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.024079587310552597 tensor([[ 0.1679,  0.1399,  0.1515, -0.0184,  0.0817, -0.0566, -0.0278,  0.0291],\n",
            "        [ 0.1215,  0.0725,  0.0948, -0.0152,  0.0616, -0.0390, -0.0224,  0.0255],\n",
            "        [ 0.0928,  0.0799,  0.0936, -0.0120,  0.0485, -0.0298, -0.0143,  0.0103],\n",
            "        [ 0.0453,  0.0278,  0.0371, -0.0091,  0.0250, -0.0132, -0.0062, -0.0028]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.024027260020375252 tensor([[ 0.1747,  0.1396,  0.1510, -0.0207,  0.0842, -0.0531, -0.0288,  0.0179],\n",
            "        [ 0.1291,  0.0676,  0.0939, -0.0174,  0.0654, -0.0368, -0.0241,  0.0191],\n",
            "        [ 0.0962,  0.0802,  0.0957, -0.0137,  0.0504, -0.0271, -0.0144,  0.0019],\n",
            "        [ 0.0498,  0.0278,  0.0393, -0.0110,  0.0278, -0.0125, -0.0064, -0.0078]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.032662130892276764 tensor([[ 0.1083,  0.1001,  0.1097, -0.0078,  0.0546, -0.0481, -0.0190,  0.0455],\n",
            "        [ 0.1122,  0.1057,  0.1159, -0.0091,  0.0568, -0.0505, -0.0180,  0.0452],\n",
            "        [ 0.0692,  0.0579,  0.0639, -0.0054,  0.0353, -0.0313, -0.0124,  0.0314],\n",
            "        [ 0.0281,  0.0246,  0.0277, -0.0031,  0.0149, -0.0134, -0.0047,  0.0115]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02576911635696888 tensor([[ 0.1379,  0.1224,  0.1356, -0.0106,  0.0691, -0.0562, -0.0242,  0.0479],\n",
            "        [ 0.1356,  0.1233,  0.1369, -0.0134,  0.0683, -0.0544, -0.0210,  0.0389],\n",
            "        [ 0.1011,  0.0778,  0.0867, -0.0075,  0.0519, -0.0424, -0.0187,  0.0405],\n",
            "        [ 0.0432,  0.0362,  0.0420, -0.0051,  0.0235, -0.0196, -0.0070,  0.0151]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.025320909917354584 tensor([[ 0.1484,  0.1251,  0.1399, -0.0121,  0.0739, -0.0549, -0.0259,  0.0407],\n",
            "        [ 0.1368,  0.1181,  0.1334, -0.0161,  0.0684, -0.0468, -0.0201,  0.0205],\n",
            "        [ 0.1184,  0.0826,  0.0932, -0.0086,  0.0607, -0.0456, -0.0223,  0.0412],\n",
            "        [ 0.0530,  0.0426,  0.0511, -0.0067,  0.0295, -0.0226, -0.0083,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.025196995586156845 tensor([[ 0.1570,  0.1258,  0.1420, -0.0135,  0.0776, -0.0526, -0.0273,  0.0325],\n",
            "        [ 0.1402,  0.1150,  0.1323, -0.0190,  0.0698, -0.0404, -0.0197,  0.0036],\n",
            "        [ 0.1317,  0.0835,  0.0955, -0.0094,  0.0674, -0.0467, -0.0251,  0.0397],\n",
            "        [ 0.0601,  0.0465,  0.0575, -0.0081,  0.0341, -0.0242, -0.0091,  0.0138]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.025111831724643707 tensor([[ 0.1647,  0.1255,  0.1432, -0.0148,  0.0808, -0.0499, -0.0285,  0.0238],\n",
            "        [ 0.1448,  0.1132,  0.1325, -0.0219,  0.0717, -0.0347, -0.0194, -0.0124],\n",
            "        [ 0.1427,  0.0822,  0.0953, -0.0101,  0.0730, -0.0467, -0.0276,  0.0371],\n",
            "        [ 0.0656,  0.0488,  0.0621, -0.0095,  0.0378, -0.0248, -0.0096,  0.0113]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[5, 6, 6, 11, 1, 8, 9, 8, 6, 4, 4, 1, 4, 6, 4, 11, 11, 4, 1, 8, 5, 2, 4, 2, 5, 2, 4, 6, 5, 1, 6, 4, 4, 4, 6, 4, 4, 1, 1, 11, 4, 11, 10, 13, 4, 4, 1, 6, 4, 8, 2, 8, 6, 6, 2, 2, 4, 7, 9, 7, 6, 4, 11, 6, 1, 5, 6, 5, 2, 8, 6, 4, 6, 4, 4, 6, 1, 2, 6, 4, 6, 8, 6, 3, 5, 4, 4, 6, 6, 11, 11, 8, 4, 1, 8, 11, 4, 14, 11, 2, 6, 5, 2, 6, 4, 6, 8, 4, 4, 6, 5, 5, 4, 11, 11, 8, 13, 11, 8, 11, 4, 4, 1, 8, 4]\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02933303453028202 tensor([[ 0.1258,  0.1203,  0.1329, -0.0104,  0.0642, -0.0557, -0.0213,  0.0513],\n",
            "        [ 0.0955,  0.0838,  0.0923, -0.0080,  0.0480, -0.0435, -0.0165,  0.0413],\n",
            "        [ 0.0726,  0.0666,  0.0734, -0.0057,  0.0365, -0.0324, -0.0118,  0.0294],\n",
            "        [ 0.0272,  0.0217,  0.0245, -0.0033,  0.0144, -0.0122, -0.0048,  0.0104]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.022315319627523422 tensor([[ 0.1441,  0.1329,  0.1496, -0.0143,  0.0734, -0.0570, -0.0239,  0.0439],\n",
            "        [ 0.1246,  0.1010,  0.1131, -0.0121,  0.0626, -0.0517, -0.0222,  0.0439],\n",
            "        [ 0.1051,  0.0940,  0.1040, -0.0084,  0.0523, -0.0436, -0.0166,  0.0355],\n",
            "        [ 0.0416,  0.0296,  0.0347, -0.0057,  0.0222, -0.0171, -0.0072,  0.0127]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02190004661679268 tensor([[ 0.1463,  0.1289,  0.1480, -0.0170,  0.0745, -0.0506, -0.0236,  0.0287],\n",
            "        [ 0.1350,  0.0989,  0.1131, -0.0151,  0.0676, -0.0501, -0.0246,  0.0364],\n",
            "        [ 0.1191,  0.1028,  0.1146, -0.0098,  0.0584, -0.0453, -0.0182,  0.0311],\n",
            "        [ 0.0509,  0.0323,  0.0396, -0.0077,  0.0274, -0.0190, -0.0086,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.021802853792905807 tensor([[ 0.1505,  0.1269,  0.1487, -0.0198,  0.0766, -0.0452, -0.0238,  0.0149],\n",
            "        [ 0.1436,  0.0952,  0.1114, -0.0178,  0.0717, -0.0478, -0.0268,  0.0282],\n",
            "        [ 0.1278,  0.1064,  0.1193, -0.0108,  0.0617, -0.0443, -0.0188,  0.0238],\n",
            "        [ 0.0582,  0.0331,  0.0423, -0.0096,  0.0314, -0.0196, -0.0096,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.021734140813350677 tensor([[ 0.1556,  0.1259,  0.1504, -0.0226,  0.0793, -0.0403, -0.0240,  0.0018],\n",
            "        [ 0.1519,  0.0913,  0.1094, -0.0205,  0.0757, -0.0454, -0.0289,  0.0200],\n",
            "        [ 0.1344,  0.1078,  0.1217, -0.0117,  0.0639, -0.0422, -0.0191,  0.0154],\n",
            "        [ 0.0643,  0.0327,  0.0438, -0.0115,  0.0349, -0.0197, -0.0103,  0.0062]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025150425732135773 tensor([[ 0.1204,  0.1155,  0.1257, -0.0091,  0.0613, -0.0539, -0.0208,  0.0499],\n",
            "        [ 0.1034,  0.0942,  0.1035, -0.0079,  0.0524, -0.0468, -0.0178,  0.0444],\n",
            "        [ 0.0723,  0.0658,  0.0732, -0.0064,  0.0368, -0.0325, -0.0123,  0.0299],\n",
            "        [ 0.0278,  0.0256,  0.0276, -0.0032,  0.0147, -0.0139, -0.0055,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017066791653633118 tensor([[ 0.1564,  0.1474,  0.1612, -0.0130,  0.0794, -0.0644, -0.0269,  0.0524],\n",
            "        [ 0.1497,  0.1330,  0.1479, -0.0126,  0.0761, -0.0623, -0.0262,  0.0541],\n",
            "        [ 0.1130,  0.1007,  0.1135, -0.0106,  0.0579, -0.0477, -0.0195,  0.0403],\n",
            "        [ 0.0447,  0.0392,  0.0422, -0.0054,  0.0238, -0.0220, -0.0090,  0.0191]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016013525426387787 tensor([[ 0.1603,  0.1459,  0.1600, -0.0146,  0.0811, -0.0590, -0.0275,  0.0390],\n",
            "        [ 0.1627,  0.1373,  0.1553, -0.0150,  0.0826, -0.0603, -0.0287,  0.0451],\n",
            "        [ 0.1325,  0.1144,  0.1313, -0.0135,  0.0680, -0.0515, -0.0228,  0.0384],\n",
            "        [ 0.0552,  0.0465,  0.0503, -0.0069,  0.0295, -0.0265, -0.0115,  0.0215]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015851717442274094 tensor([[ 0.1634,  0.1436,  0.1580, -0.0160,  0.0824, -0.0535, -0.0279,  0.0256],\n",
            "        [ 0.1706,  0.1364,  0.1571, -0.0171,  0.0864, -0.0558, -0.0303,  0.0337],\n",
            "        [ 0.1447,  0.1208,  0.1412, -0.0159,  0.0743, -0.0514, -0.0249,  0.0323],\n",
            "        [ 0.0627,  0.0509,  0.0552, -0.0083,  0.0337, -0.0293, -0.0134,  0.0219]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015762917697429657 tensor([[ 0.1677,  0.1426,  0.1574, -0.0176,  0.0844, -0.0486, -0.0286,  0.0131],\n",
            "        [ 0.1776,  0.1346,  0.1577, -0.0191,  0.0898, -0.0509, -0.0317,  0.0220],\n",
            "        [ 0.1535,  0.1238,  0.1475, -0.0180,  0.0789, -0.0496, -0.0264,  0.0243],\n",
            "        [ 0.0683,  0.0535,  0.0582, -0.0095,  0.0370, -0.0311, -0.0151,  0.0212]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026092108339071274 tensor([[ 0.1010,  0.0974,  0.1083, -0.0085,  0.0505, -0.0428, -0.0170,  0.0383],\n",
            "        [ 0.1032,  0.0974,  0.1078, -0.0083,  0.0527, -0.0444, -0.0168,  0.0402],\n",
            "        [ 0.0711,  0.0621,  0.0689, -0.0058,  0.0364, -0.0324, -0.0122,  0.0312],\n",
            "        [ 0.0291,  0.0251,  0.0268, -0.0027,  0.0154, -0.0145, -0.0058,  0.0137]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02067108452320099 tensor([[ 0.1069,  0.0982,  0.1119, -0.0108,  0.0521, -0.0380, -0.0172,  0.0266],\n",
            "        [ 0.1073,  0.0946,  0.1075, -0.0101,  0.0550, -0.0373, -0.0170,  0.0242],\n",
            "        [ 0.0986,  0.0797,  0.0909, -0.0086,  0.0506, -0.0406, -0.0168,  0.0363],\n",
            "        [ 0.0453,  0.0377,  0.0401, -0.0043,  0.0245, -0.0221, -0.0092,  0.0201]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020409952849149704 tensor([[ 0.1119,  0.0981,  0.1146, -0.0130,  0.0534, -0.0329, -0.0171,  0.0148],\n",
            "        [ 0.1098,  0.0903,  0.1053, -0.0117,  0.0565, -0.0294, -0.0168,  0.0078],\n",
            "        [ 0.1149,  0.0860,  0.1007, -0.0106,  0.0590, -0.0429, -0.0195,  0.0348],\n",
            "        [ 0.0563,  0.0454,  0.0482, -0.0056,  0.0309, -0.0268, -0.0118,  0.0233]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020287375897169113 tensor([[ 0.1174,  0.0984,  0.1178, -0.0153,  0.0549, -0.0281, -0.0172,  0.0033],\n",
            "        [ 0.1138,  0.0875,  0.1048, -0.0133,  0.0589, -0.0225, -0.0170, -0.0073],\n",
            "        [ 0.1264,  0.0875,  0.1053, -0.0124,  0.0649, -0.0427, -0.0213,  0.0306],\n",
            "        [ 0.0643,  0.0503,  0.0533, -0.0068,  0.0358, -0.0299, -0.0139,  0.0246]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020204097032546997 tensor([[ 0.1229,  0.0988,  0.1209, -0.0175,  0.0564, -0.0233, -0.0173, -0.0080],\n",
            "        [ 0.1187,  0.0857,  0.1053, -0.0148,  0.0618, -0.0162, -0.0174, -0.0216],\n",
            "        [ 0.1355,  0.0866,  0.1073, -0.0140,  0.0694, -0.0412, -0.0226,  0.0251],\n",
            "        [ 0.0704,  0.0533,  0.0563, -0.0078,  0.0397, -0.0318, -0.0156,  0.0247]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024835098534822464 tensor([[ 0.1161,  0.1056,  0.1163, -0.0089,  0.0575, -0.0509, -0.0187,  0.0463],\n",
            "        [ 0.0944,  0.0880,  0.0971, -0.0080,  0.0479, -0.0422, -0.0159,  0.0389],\n",
            "        [ 0.0668,  0.0632,  0.0696, -0.0061,  0.0346, -0.0307, -0.0115,  0.0278],\n",
            "        [ 0.0277,  0.0227,  0.0248, -0.0032,  0.0148, -0.0120, -0.0054,  0.0103]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0176764614880085 tensor([[ 0.1713,  0.1525,  0.1695, -0.0142,  0.0841, -0.0705, -0.0268,  0.0586],\n",
            "        [ 0.1320,  0.1207,  0.1348, -0.0126,  0.0670, -0.0546, -0.0220,  0.0449],\n",
            "        [ 0.0913,  0.0840,  0.0930, -0.0092,  0.0478, -0.0395, -0.0159,  0.0313],\n",
            "        [ 0.0424,  0.0314,  0.0346, -0.0053,  0.0229, -0.0162, -0.0085,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01666235737502575 tensor([[ 0.1924,  0.1646,  0.1847, -0.0171,  0.0932, -0.0733, -0.0290,  0.0534],\n",
            "        [ 0.1396,  0.1229,  0.1397, -0.0152,  0.0704, -0.0514, -0.0227,  0.0343],\n",
            "        [ 0.0974,  0.0863,  0.0966, -0.0111,  0.0515, -0.0387, -0.0170,  0.0244],\n",
            "        [ 0.0518,  0.0348,  0.0389, -0.0069,  0.0282, -0.0172, -0.0106,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016543153673410416 tensor([[ 0.2054,  0.1684,  0.1906, -0.0194,  0.0982, -0.0721, -0.0299,  0.0441],\n",
            "        [ 0.1430,  0.1208,  0.1400, -0.0174,  0.0716, -0.0461, -0.0227,  0.0213],\n",
            "        [ 0.1008,  0.0858,  0.0973, -0.0127,  0.0538, -0.0365, -0.0177,  0.0161],\n",
            "        [ 0.0595,  0.0363,  0.0413, -0.0084,  0.0325, -0.0172, -0.0123,  0.0074]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016472546383738518 tensor([[ 0.2159,  0.1695,  0.1937, -0.0215,  0.1019, -0.0696, -0.0303,  0.0335],\n",
            "        [ 0.1465,  0.1188,  0.1405, -0.0196,  0.0729, -0.0408, -0.0226,  0.0087],\n",
            "        [ 0.1039,  0.0850,  0.0977, -0.0143,  0.0560, -0.0342, -0.0183,  0.0078],\n",
            "        [ 0.0662,  0.0370,  0.0427, -0.0098,  0.0363, -0.0167, -0.0139,  0.0040]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023033948615193367 tensor([[ 0.1079,  0.1006,  0.1109, -0.0087,  0.0537, -0.0487, -0.0171,  0.0444],\n",
            "        [ 0.0845,  0.0761,  0.0848, -0.0077,  0.0431, -0.0382, -0.0149,  0.0365],\n",
            "        [ 0.0694,  0.0663,  0.0735, -0.0059,  0.0357, -0.0302, -0.0115,  0.0266],\n",
            "        [ 0.0243,  0.0206,  0.0217, -0.0029,  0.0136, -0.0135, -0.0054,  0.0121]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017110435292124748 tensor([[ 0.1471,  0.1339,  0.1488, -0.0132,  0.0725, -0.0624, -0.0224,  0.0511],\n",
            "        [ 0.1095,  0.0938,  0.1068, -0.0118,  0.0564, -0.0455, -0.0198,  0.0394],\n",
            "        [ 0.0952,  0.0888,  0.0998, -0.0090,  0.0491, -0.0371, -0.0157,  0.0277],\n",
            "        [ 0.0371,  0.0293,  0.0306, -0.0047,  0.0213, -0.0208, -0.0087,  0.0174]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01654975116252899 tensor([[ 0.1606,  0.1407,  0.1577, -0.0159,  0.0781, -0.0634, -0.0232,  0.0446],\n",
            "        [ 0.1140,  0.0904,  0.1063, -0.0146,  0.0593, -0.0420, -0.0212,  0.0308],\n",
            "        [ 0.1049,  0.0949,  0.1085, -0.0110,  0.0542, -0.0353, -0.0171,  0.0197],\n",
            "        [ 0.0450,  0.0331,  0.0343, -0.0062,  0.0263, -0.0253, -0.0111,  0.0197]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016464684158563614 tensor([[ 0.1692,  0.1425,  0.1611, -0.0182,  0.0811, -0.0619, -0.0233,  0.0356],\n",
            "        [ 0.1168,  0.0854,  0.1041, -0.0171,  0.0614, -0.0377, -0.0222,  0.0214],\n",
            "        [ 0.1112,  0.0976,  0.1135, -0.0127,  0.0576, -0.0319, -0.0179,  0.0098],\n",
            "        [ 0.0507,  0.0348,  0.0358, -0.0075,  0.0302, -0.0286, -0.0132,  0.0206]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016407083719968796 tensor([[ 0.1765,  0.1431,  0.1630, -0.0205,  0.0835, -0.0599, -0.0231,  0.0260],\n",
            "        [ 0.1204,  0.0810,  0.1026, -0.0198,  0.0639, -0.0338, -0.0234,  0.0126],\n",
            "        [ 0.1163,  0.0992,  0.1173, -0.0144,  0.0603, -0.0279, -0.0186, -0.0005],\n",
            "        [ 0.0551,  0.0353,  0.0360, -0.0088,  0.0334, -0.0312, -0.0150,  0.0208]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024701837450265884 tensor([[ 0.1142,  0.1020,  0.1126, -0.0085,  0.0566, -0.0503, -0.0186,  0.0462],\n",
            "        [ 0.0927,  0.0844,  0.0922, -0.0072,  0.0471, -0.0421, -0.0160,  0.0402],\n",
            "        [ 0.0663,  0.0611,  0.0678, -0.0060,  0.0342, -0.0301, -0.0117,  0.0279],\n",
            "        [ 0.0291,  0.0249,  0.0266, -0.0026,  0.0147, -0.0136, -0.0050,  0.0117]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01781512051820755 tensor([[ 0.1629,  0.1402,  0.1572, -0.0132,  0.0798, -0.0664, -0.0254,  0.0547],\n",
            "        [ 0.1379,  0.1237,  0.1355, -0.0113,  0.0708, -0.0589, -0.0241,  0.0523],\n",
            "        [ 0.0968,  0.0875,  0.0984, -0.0099,  0.0508, -0.0414, -0.0174,  0.0351],\n",
            "        [ 0.0464,  0.0374,  0.0394, -0.0041,  0.0230, -0.0205, -0.0076,  0.0157]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016737284138798714 tensor([[ 0.1771,  0.1428,  0.1631, -0.0154,  0.0853, -0.0652, -0.0262,  0.0449],\n",
            "        [ 0.1533,  0.1328,  0.1462, -0.0134,  0.0794, -0.0600, -0.0272,  0.0476],\n",
            "        [ 0.1079,  0.0948,  0.1086, -0.0125,  0.0574, -0.0424, -0.0197,  0.0312],\n",
            "        [ 0.0576,  0.0439,  0.0458, -0.0050,  0.0282, -0.0239, -0.0091,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01661742478609085 tensor([[ 0.1855,  0.1395,  0.1626, -0.0172,  0.0879, -0.0612, -0.0259,  0.0325],\n",
            "        [ 0.1619,  0.1348,  0.1494, -0.0151,  0.0844, -0.0576, -0.0291,  0.0393],\n",
            "        [ 0.1136,  0.0967,  0.1131, -0.0146,  0.0612, -0.0406, -0.0209,  0.0243],\n",
            "        [ 0.0661,  0.0478,  0.0495, -0.0057,  0.0320, -0.0258, -0.0101,  0.0144]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016550365835428238 tensor([[ 0.1936,  0.1360,  0.1619, -0.0190,  0.0904, -0.0571, -0.0256,  0.0200],\n",
            "        [ 0.1689,  0.1353,  0.1508, -0.0166,  0.0886, -0.0545, -0.0308,  0.0302],\n",
            "        [ 0.1175,  0.0970,  0.1158, -0.0167,  0.0641, -0.0379, -0.0218,  0.0165],\n",
            "        [ 0.0731,  0.0503,  0.0516, -0.0062,  0.0350, -0.0269, -0.0108,  0.0120]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.027045074850320816 tensor([[ 0.1028,  0.0973,  0.1075, -0.0089,  0.0521, -0.0458, -0.0176,  0.0415],\n",
            "        [ 0.0880,  0.0844,  0.0941, -0.0083,  0.0462, -0.0381, -0.0158,  0.0364],\n",
            "        [ 0.0673,  0.0581,  0.0636, -0.0049,  0.0328, -0.0301, -0.0108,  0.0262],\n",
            "        [ 0.0314,  0.0266,  0.0294, -0.0034,  0.0170, -0.0150, -0.0061,  0.0135]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021568700671195984 tensor([[ 0.1412,  0.1309,  0.1463, -0.0137,  0.0713, -0.0590, -0.0239,  0.0485],\n",
            "        [ 0.0940,  0.0864,  0.0986, -0.0116,  0.0505, -0.0342, -0.0178,  0.0268],\n",
            "        [ 0.0901,  0.0710,  0.0784, -0.0064,  0.0429, -0.0360, -0.0138,  0.0264],\n",
            "        [ 0.0484,  0.0387,  0.0437, -0.0058,  0.0267, -0.0216, -0.0096,  0.0178]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02107175812125206 tensor([[ 0.1601,  0.1447,  0.1634, -0.0173,  0.0803, -0.0625, -0.0268,  0.0453],\n",
            "        [ 0.0904,  0.0786,  0.0927, -0.0142,  0.0499, -0.0253, -0.0182,  0.0120],\n",
            "        [ 0.1021,  0.0730,  0.0815, -0.0070,  0.0473, -0.0364, -0.0149,  0.0206],\n",
            "        [ 0.0598,  0.0454,  0.0524, -0.0078,  0.0335, -0.0250, -0.0121,  0.0186]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020910650491714478 tensor([[ 0.1724,  0.1516,  0.1730, -0.0205,  0.0859, -0.0627, -0.0285,  0.0385],\n",
            "        [ 0.0903,  0.0742,  0.0907, -0.0171,  0.0510, -0.0180, -0.0190, -0.0009],\n",
            "        [ 0.1119,  0.0729,  0.0821, -0.0075,  0.0506, -0.0358, -0.0155,  0.0139],\n",
            "        [ 0.0683,  0.0492,  0.0580, -0.0097,  0.0388, -0.0268, -0.0140,  0.0175]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020827066153287888 tensor([[ 0.1812,  0.1549,  0.1788, -0.0234,  0.0897, -0.0612, -0.0297,  0.0298],\n",
            "        [ 0.0921,  0.0718,  0.0909, -0.0200,  0.0532, -0.0118, -0.0202, -0.0126],\n",
            "        [ 0.1206,  0.0717,  0.0815, -0.0079,  0.0533, -0.0348, -0.0159,  0.0067],\n",
            "        [ 0.0751,  0.0513,  0.0617, -0.0115,  0.0432, -0.0275, -0.0156,  0.0153]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026417139917612076 tensor([[ 0.1167,  0.1118,  0.1218, -0.0085,  0.0589, -0.0515, -0.0192,  0.0470],\n",
            "        [ 0.1052,  0.0972,  0.1077, -0.0088,  0.0537, -0.0470, -0.0182,  0.0440],\n",
            "        [ 0.0681,  0.0599,  0.0670, -0.0059,  0.0348, -0.0309, -0.0124,  0.0296],\n",
            "        [ 0.0304,  0.0268,  0.0299, -0.0028,  0.0157, -0.0140, -0.0050,  0.0114]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018827561289072037 tensor([[ 0.1513,  0.1423,  0.1558, -0.0118,  0.0755, -0.0611, -0.0242,  0.0489],\n",
            "        [ 0.1452,  0.1312,  0.1477, -0.0139,  0.0743, -0.0588, -0.0251,  0.0494],\n",
            "        [ 0.1019,  0.0853,  0.0973, -0.0092,  0.0527, -0.0436, -0.0191,  0.0392],\n",
            "        [ 0.0484,  0.0403,  0.0459, -0.0045,  0.0250, -0.0209, -0.0076,  0.0148]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018037552013993263 tensor([[ 0.1588,  0.1446,  0.1591, -0.0133,  0.0783, -0.0573, -0.0245,  0.0374],\n",
            "        [ 0.1535,  0.1329,  0.1527, -0.0168,  0.0786, -0.0543, -0.0265,  0.0376],\n",
            "        [ 0.1171,  0.0919,  0.1075, -0.0112,  0.0610, -0.0464, -0.0227,  0.0382],\n",
            "        [ 0.0600,  0.0476,  0.0556, -0.0057,  0.0309, -0.0240, -0.0090,  0.0140]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017922740429639816 tensor([[ 0.1642,  0.1449,  0.1600, -0.0145,  0.0801, -0.0525, -0.0245,  0.0251],\n",
            "        [ 0.1586,  0.1311,  0.1539, -0.0195,  0.0811, -0.0483, -0.0272,  0.0242],\n",
            "        [ 0.1270,  0.0934,  0.1121, -0.0129,  0.0666, -0.0464, -0.0253,  0.0342],\n",
            "        [ 0.0686,  0.0518,  0.0620, -0.0067,  0.0353, -0.0254, -0.0098,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01784682646393776 tensor([[ 0.1697,  0.1453,  0.1611, -0.0158,  0.0820, -0.0479, -0.0245,  0.0130],\n",
            "        [ 0.1636,  0.1293,  0.1551, -0.0221,  0.0837, -0.0423, -0.0280,  0.0111],\n",
            "        [ 0.1348,  0.0927,  0.1144, -0.0144,  0.0710, -0.0454, -0.0275,  0.0291],\n",
            "        [ 0.0753,  0.0543,  0.0665, -0.0075,  0.0387, -0.0257, -0.0102,  0.0074]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02606860175728798 tensor([[ 0.1016,  0.0978,  0.1054, -0.0074,  0.0516, -0.0452, -0.0178,  0.0422],\n",
            "        [ 0.0845,  0.0794,  0.0893, -0.0073,  0.0432, -0.0382, -0.0143,  0.0365],\n",
            "        [ 0.0599,  0.0584,  0.0644, -0.0058,  0.0319, -0.0256, -0.0117,  0.0250],\n",
            "        [ 0.0264,  0.0206,  0.0233, -0.0022,  0.0132, -0.0140, -0.0037,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020553486421704292 tensor([[ 0.1406,  0.1333,  0.1436, -0.0111,  0.0710, -0.0581, -0.0244,  0.0493],\n",
            "        [ 0.1126,  0.1039,  0.1198, -0.0113,  0.0584, -0.0467, -0.0191,  0.0408],\n",
            "        [ 0.0779,  0.0750,  0.0835, -0.0090,  0.0426, -0.0294, -0.0164,  0.0266],\n",
            "        [ 0.0426,  0.0303,  0.0355, -0.0034,  0.0206, -0.0224, -0.0050,  0.0123]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019955938681960106 tensor([[ 0.1552,  0.1433,  0.1541, -0.0132,  0.0778, -0.0585, -0.0267,  0.0436],\n",
            "        [ 0.1194,  0.1066,  0.1271, -0.0138,  0.0628, -0.0441, -0.0202,  0.0333],\n",
            "        [ 0.0822,  0.0777,  0.0878, -0.0113,  0.0462, -0.0261, -0.0187,  0.0205],\n",
            "        [ 0.0538,  0.0352,  0.0427, -0.0042,  0.0252, -0.0279, -0.0054,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019867662340402603 tensor([[ 0.1637,  0.1471,  0.1577, -0.0149,  0.0814, -0.0560, -0.0280,  0.0349],\n",
            "        [ 0.1224,  0.1055,  0.1302, -0.0160,  0.0652, -0.0396, -0.0207,  0.0237],\n",
            "        [ 0.0842,  0.0781,  0.0896, -0.0134,  0.0486, -0.0216, -0.0205,  0.0133],\n",
            "        [ 0.0623,  0.0374,  0.0470, -0.0048,  0.0284, -0.0320, -0.0051,  0.0100]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01981370523571968 tensor([[ 0.1704,  0.1491,  0.1593, -0.0165,  0.0841, -0.0527, -0.0289,  0.0253],\n",
            "        [ 0.1251,  0.1040,  0.1330, -0.0181,  0.0675, -0.0348, -0.0211,  0.0139],\n",
            "        [ 0.0859,  0.0781,  0.0910, -0.0154,  0.0509, -0.0170, -0.0223,  0.0061],\n",
            "        [ 0.0692,  0.0381,  0.0497, -0.0053,  0.0308, -0.0352, -0.0046,  0.0070]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020158469676971436 tensor([[ 0.0917,  0.0863,  0.0948, -0.0075,  0.0460, -0.0390, -0.0155,  0.0346],\n",
            "        [ 0.0810,  0.0736,  0.0822, -0.0065,  0.0422, -0.0355, -0.0156,  0.0344],\n",
            "        [ 0.0603,  0.0520,  0.0584, -0.0051,  0.0296, -0.0256, -0.0093,  0.0226],\n",
            "        [ 0.0251,  0.0206,  0.0216, -0.0021,  0.0123, -0.0115, -0.0040,  0.0104]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016306737437844276 tensor([[ 0.0952,  0.0834,  0.0925, -0.0091,  0.0467, -0.0337, -0.0155,  0.0222],\n",
            "        [ 0.0964,  0.0817,  0.0943, -0.0088,  0.0513, -0.0362, -0.0199,  0.0309],\n",
            "        [ 0.0854,  0.0695,  0.0803, -0.0081,  0.0416, -0.0321, -0.0125,  0.0242],\n",
            "        [ 0.0399,  0.0306,  0.0316, -0.0030,  0.0191, -0.0173, -0.0059,  0.0146]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016082612797617912 tensor([[ 0.0981,  0.0798,  0.0895, -0.0105,  0.0471, -0.0283, -0.0155,  0.0102],\n",
            "        [ 0.1032,  0.0810,  0.0967, -0.0105,  0.0560, -0.0325, -0.0226,  0.0228],\n",
            "        [ 0.0997,  0.0763,  0.0909, -0.0104,  0.0481, -0.0328, -0.0138,  0.0195],\n",
            "        [ 0.0503,  0.0364,  0.0369, -0.0036,  0.0236, -0.0208, -0.0070,  0.0160]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01599394902586937 tensor([[ 0.1030,  0.0782,  0.0886, -0.0120,  0.0486, -0.0240, -0.0157, -0.0005],\n",
            "        [ 0.1084,  0.0789,  0.0976, -0.0120,  0.0600, -0.0281, -0.0251,  0.0142],\n",
            "        [ 0.1097,  0.0789,  0.0969, -0.0125,  0.0523, -0.0313, -0.0144,  0.0124],\n",
            "        [ 0.0582,  0.0398,  0.0397, -0.0041,  0.0269, -0.0228, -0.0077,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01593414880335331 tensor([[ 0.1086,  0.0775,  0.0886, -0.0135,  0.0505, -0.0201, -0.0161, -0.0107],\n",
            "        [ 0.1133,  0.0765,  0.0980, -0.0135,  0.0638, -0.0237, -0.0275,  0.0056],\n",
            "        [ 0.1176,  0.0794,  0.1005, -0.0145,  0.0554, -0.0287, -0.0146,  0.0041],\n",
            "        [ 0.0646,  0.0417,  0.0409, -0.0045,  0.0294, -0.0240, -0.0081,  0.0149]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.027662599459290504 tensor([[ 0.0818,  0.0737,  0.0843, -0.0072,  0.0423, -0.0356, -0.0149,  0.0352],\n",
            "        [ 0.0918,  0.0902,  0.0980, -0.0074,  0.0466, -0.0406, -0.0156,  0.0369],\n",
            "        [ 0.0574,  0.0490,  0.0532, -0.0046,  0.0282, -0.0276, -0.0101,  0.0248],\n",
            "        [ 0.0235,  0.0202,  0.0205, -0.0028,  0.0120, -0.0104, -0.0045,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.023594405502080917 tensor([[ 0.1045,  0.0887,  0.1052, -0.0107,  0.0545, -0.0414, -0.0194,  0.0385],\n",
            "        [ 0.1015,  0.0990,  0.1073, -0.0096,  0.0512, -0.0395, -0.0169,  0.0289],\n",
            "        [ 0.0758,  0.0586,  0.0642, -0.0067,  0.0362, -0.0340, -0.0134,  0.0268],\n",
            "        [ 0.0347,  0.0272,  0.0272, -0.0047,  0.0179, -0.0135, -0.0070,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02340177446603775 tensor([[ 0.1161,  0.0924,  0.1138, -0.0134,  0.0610, -0.0416, -0.0220,  0.0361],\n",
            "        [ 0.1017,  0.0983,  0.1063, -0.0112,  0.0508, -0.0337, -0.0165,  0.0160],\n",
            "        [ 0.0856,  0.0595,  0.0660, -0.0082,  0.0396, -0.0359, -0.0151,  0.0240],\n",
            "        [ 0.0421,  0.0303,  0.0301, -0.0065,  0.0220, -0.0144, -0.0089,  0.0118]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.023338690400123596 tensor([[ 0.1247,  0.0930,  0.1189, -0.0160,  0.0660, -0.0402, -0.0240,  0.0319],\n",
            "        [ 0.1024,  0.0981,  0.1059, -0.0127,  0.0507, -0.0281, -0.0161,  0.0035],\n",
            "        [ 0.0932,  0.0583,  0.0654, -0.0096,  0.0419, -0.0366, -0.0164,  0.0200],\n",
            "        [ 0.0479,  0.0318,  0.0311, -0.0081,  0.0253, -0.0145, -0.0105,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02329333871603012 tensor([[ 0.1316,  0.0919,  0.1223, -0.0184,  0.0701, -0.0380, -0.0257,  0.0269],\n",
            "        [ 0.1040,  0.0989,  0.1065, -0.0142,  0.0510, -0.0231, -0.0158, -0.0084],\n",
            "        [ 0.1000,  0.0563,  0.0638, -0.0109,  0.0438, -0.0370, -0.0175,  0.0156],\n",
            "        [ 0.0528,  0.0324,  0.0312, -0.0097,  0.0280, -0.0140, -0.0119,  0.0088]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02454625815153122 tensor([[ 0.0973,  0.0908,  0.1024, -0.0080,  0.0494, -0.0420, -0.0168,  0.0379],\n",
            "        [ 0.0914,  0.0824,  0.0928, -0.0075,  0.0460, -0.0393, -0.0149,  0.0346],\n",
            "        [ 0.0713,  0.0632,  0.0699, -0.0059,  0.0366, -0.0317, -0.0121,  0.0296],\n",
            "        [ 0.0281,  0.0236,  0.0261, -0.0031,  0.0145, -0.0134, -0.0054,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020043112337589264 tensor([[ 0.0930,  0.0793,  0.0926, -0.0088,  0.0471, -0.0341, -0.0165,  0.0241],\n",
            "        [ 0.0939,  0.0749,  0.0897, -0.0092,  0.0470, -0.0316, -0.0145,  0.0182],\n",
            "        [ 0.1024,  0.0873,  0.0982, -0.0092,  0.0533, -0.0414, -0.0172,  0.0353],\n",
            "        [ 0.0441,  0.0349,  0.0395, -0.0051,  0.0228, -0.0200, -0.0086,  0.0164]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01969321072101593 tensor([[ 0.0967,  0.0761,  0.0919, -0.0101,  0.0490, -0.0302, -0.0175,  0.0146],\n",
            "        [ 0.0983,  0.0696,  0.0887, -0.0109,  0.0491, -0.0253, -0.0145,  0.0037],\n",
            "        [ 0.1223,  0.1003,  0.1144, -0.0119,  0.0641, -0.0453, -0.0204,  0.0346],\n",
            "        [ 0.0549,  0.0412,  0.0476, -0.0069,  0.0285, -0.0236, -0.0109,  0.0174]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019547972828149796 tensor([[ 0.1014,  0.0739,  0.0923, -0.0115,  0.0514, -0.0268, -0.0186,  0.0059],\n",
            "        [ 0.1028,  0.0644,  0.0878, -0.0124,  0.0512, -0.0191, -0.0144, -0.0102],\n",
            "        [ 0.1355,  0.1065,  0.1233, -0.0141,  0.0714, -0.0458, -0.0223,  0.0301],\n",
            "        [ 0.0626,  0.0444,  0.0525, -0.0084,  0.0325, -0.0255, -0.0126,  0.0165]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019466467201709747 tensor([[ 0.1065,  0.0723,  0.0933, -0.0129,  0.0541, -0.0237, -0.0199, -0.0025],\n",
            "        [ 0.1080,  0.0600,  0.0877, -0.0140,  0.0537, -0.0135, -0.0146, -0.0233],\n",
            "        [ 0.1452,  0.1093,  0.1283, -0.0160,  0.0769, -0.0445, -0.0236,  0.0238],\n",
            "        [ 0.0685,  0.0460,  0.0555, -0.0098,  0.0355, -0.0263, -0.0140,  0.0144]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.0219716876745224 tensor([[ 0.0809,  0.0795,  0.0874, -0.0071,  0.0402, -0.0369, -0.0134,  0.0298],\n",
            "        [ 0.0789,  0.0665,  0.0745, -0.0060,  0.0399, -0.0339, -0.0135,  0.0324],\n",
            "        [ 0.0614,  0.0572,  0.0623, -0.0053,  0.0309, -0.0275, -0.0105,  0.0257],\n",
            "        [ 0.0195,  0.0155,  0.0174, -0.0027,  0.0102, -0.0100, -0.0040,  0.0097]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018719496205449104 tensor([[ 0.0724,  0.0686,  0.0760, -0.0083,  0.0349, -0.0293, -0.0114,  0.0130],\n",
            "        [ 0.0976,  0.0713,  0.0833, -0.0081,  0.0493, -0.0354, -0.0163,  0.0298],\n",
            "        [ 0.0805,  0.0728,  0.0794, -0.0074,  0.0400, -0.0333, -0.0135,  0.0277],\n",
            "        [ 0.0306,  0.0223,  0.0261, -0.0047,  0.0157, -0.0151, -0.0063,  0.0145]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018522152677178383 tensor([[ 0.0724,  0.0663,  0.0742, -0.0098,  0.0340, -0.0261, -0.0109,  0.0014],\n",
            "        [ 0.1103,  0.0706,  0.0860, -0.0097,  0.0557, -0.0342, -0.0181,  0.0244],\n",
            "        [ 0.0911,  0.0799,  0.0872, -0.0090,  0.0447, -0.0346, -0.0150,  0.0248],\n",
            "        [ 0.0382,  0.0259,  0.0312, -0.0063,  0.0194, -0.0182, -0.0080,  0.0171]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018453944474458694 tensor([[ 0.0743,  0.0658,  0.0742, -0.0114,  0.0341, -0.0239, -0.0106, -0.0089],\n",
            "        [ 0.1205,  0.0672,  0.0857, -0.0112,  0.0608, -0.0318, -0.0194,  0.0177],\n",
            "        [ 0.0979,  0.0832,  0.0910, -0.0104,  0.0475, -0.0340, -0.0159,  0.0198],\n",
            "        [ 0.0437,  0.0275,  0.0342, -0.0079,  0.0220, -0.0202, -0.0092,  0.0184]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018407270312309265 tensor([[ 0.0769,  0.0662,  0.0752, -0.0130,  0.0346, -0.0221, -0.0105, -0.0186],\n",
            "        [ 0.1296,  0.0630,  0.0844, -0.0126,  0.0653, -0.0289, -0.0206,  0.0106],\n",
            "        [ 0.1030,  0.0849,  0.0929, -0.0116,  0.0494, -0.0326, -0.0164,  0.0140],\n",
            "        [ 0.0479,  0.0278,  0.0359, -0.0094,  0.0240, -0.0214, -0.0103,  0.0189]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025214970111846924 tensor([[ 0.0976,  0.0897,  0.1002, -0.0083,  0.0484, -0.0415, -0.0159,  0.0381],\n",
            "        [ 0.0957,  0.0939,  0.1038, -0.0063,  0.0489, -0.0419, -0.0165,  0.0386],\n",
            "        [ 0.0696,  0.0609,  0.0681, -0.0072,  0.0359, -0.0306, -0.0121,  0.0280],\n",
            "        [ 0.0323,  0.0265,  0.0284, -0.0030,  0.0166, -0.0161, -0.0055,  0.0145]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020463500171899796 tensor([[ 0.1100,  0.0941,  0.1080, -0.0110,  0.0536, -0.0406, -0.0169,  0.0318],\n",
            "        [ 0.0896,  0.0859,  0.0970, -0.0057,  0.0455, -0.0321, -0.0151,  0.0213],\n",
            "        [ 0.0898,  0.0703,  0.0818, -0.0112,  0.0466, -0.0337, -0.0160,  0.0262],\n",
            "        [ 0.0514,  0.0400,  0.0430, -0.0050,  0.0267, -0.0251, -0.0084,  0.0212]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020243708044290543 tensor([[ 0.1194,  0.0954,  0.1124, -0.0135,  0.0571, -0.0382, -0.0174,  0.0238],\n",
            "        [ 0.0884,  0.0829,  0.0957, -0.0053,  0.0448, -0.0247, -0.0145,  0.0071],\n",
            "        [ 0.1027,  0.0726,  0.0876, -0.0148,  0.0535, -0.0331, -0.0187,  0.0205],\n",
            "        [ 0.0649,  0.0482,  0.0519, -0.0066,  0.0339, -0.0310, -0.0103,  0.0244]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02013714239001274 tensor([[ 0.1272,  0.0951,  0.1152, -0.0159,  0.0599, -0.0351, -0.0176,  0.0150],\n",
            "        [ 0.0894,  0.0821,  0.0968, -0.0049,  0.0452, -0.0185, -0.0143, -0.0057],\n",
            "        [ 0.1123,  0.0717,  0.0898, -0.0180,  0.0587, -0.0309, -0.0208,  0.0130],\n",
            "        [ 0.0750,  0.0530,  0.0572, -0.0081,  0.0393, -0.0350, -0.0116,  0.0254]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02006368711590767 tensor([[ 0.1343,  0.0942,  0.1172, -0.0183,  0.0623, -0.0316, -0.0177,  0.0059],\n",
            "        [ 0.0914,  0.0824,  0.0990, -0.0046,  0.0461, -0.0129, -0.0143, -0.0176],\n",
            "        [ 0.1204,  0.0694,  0.0904, -0.0212,  0.0632, -0.0279, -0.0226,  0.0050],\n",
            "        [ 0.0829,  0.0557,  0.0602, -0.0094,  0.0435, -0.0378, -0.0125,  0.0251]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022143546491861343 tensor([[ 0.1022,  0.0946,  0.1044, -0.0074,  0.0509, -0.0452, -0.0175,  0.0419],\n",
            "        [ 0.1006,  0.0959,  0.1050, -0.0082,  0.0505, -0.0468, -0.0163,  0.0435],\n",
            "        [ 0.0505,  0.0475,  0.0535, -0.0051,  0.0272, -0.0214, -0.0105,  0.0201],\n",
            "        [ 0.0273,  0.0220,  0.0240, -0.0024,  0.0126, -0.0137, -0.0036,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01671864464879036 tensor([[ 0.1305,  0.1156,  0.1295, -0.0102,  0.0641, -0.0526, -0.0221,  0.0438],\n",
            "        [ 0.1232,  0.1143,  0.1258, -0.0111,  0.0611, -0.0533, -0.0193,  0.0439],\n",
            "        [ 0.0574,  0.0504,  0.0595, -0.0073,  0.0321, -0.0197, -0.0137,  0.0152],\n",
            "        [ 0.0443,  0.0339,  0.0371, -0.0038,  0.0189, -0.0219, -0.0048,  0.0147]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016430689021945 tensor([[ 0.1424,  0.1197,  0.1362, -0.0119,  0.0688, -0.0519, -0.0238,  0.0374],\n",
            "        [ 0.1280,  0.1145,  0.1270, -0.0129,  0.0624, -0.0507, -0.0192,  0.0346],\n",
            "        [ 0.0585,  0.0471,  0.0589, -0.0090,  0.0340, -0.0148, -0.0158,  0.0072],\n",
            "        [ 0.0562,  0.0410,  0.0450, -0.0049,  0.0226, -0.0273, -0.0050,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016357485204935074 tensor([[ 0.1512,  0.1207,  0.1393, -0.0134,  0.0718, -0.0497, -0.0250,  0.0294],\n",
            "        [ 0.1318,  0.1138,  0.1273, -0.0145,  0.0633, -0.0476, -0.0190,  0.0249],\n",
            "        [ 0.0601,  0.0442,  0.0589, -0.0106,  0.0361, -0.0101, -0.0181, -0.0005],\n",
            "        [ 0.0653,  0.0453,  0.0498, -0.0059,  0.0248, -0.0311, -0.0047,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016306009143590927 tensor([[ 0.1588,  0.1205,  0.1412, -0.0148,  0.0743, -0.0469, -0.0260,  0.0208],\n",
            "        [ 0.1358,  0.1133,  0.1278, -0.0162,  0.0642, -0.0447, -0.0187,  0.0153],\n",
            "        [ 0.0622,  0.0419,  0.0594, -0.0123,  0.0385, -0.0058, -0.0204, -0.0076],\n",
            "        [ 0.0724,  0.0477,  0.0527, -0.0067,  0.0260, -0.0339, -0.0040,  0.0135]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.9802e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02576393075287342 tensor([[ 0.1047,  0.1011,  0.1134, -0.0088,  0.0552, -0.0462, -0.0201,  0.0434],\n",
            "        [ 0.0806,  0.0707,  0.0778, -0.0066,  0.0404, -0.0380, -0.0137,  0.0337],\n",
            "        [ 0.0546,  0.0489,  0.0553, -0.0041,  0.0280, -0.0235, -0.0087,  0.0212],\n",
            "        [ 0.0246,  0.0196,  0.0202, -0.0020,  0.0123, -0.0121, -0.0043,  0.0115]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02134806104004383 tensor([[ 0.1154,  0.1072,  0.1237, -0.0119,  0.0623, -0.0444, -0.0240,  0.0359],\n",
            "        [ 0.0973,  0.0776,  0.0872, -0.0089,  0.0487, -0.0425, -0.0164,  0.0309],\n",
            "        [ 0.0716,  0.0604,  0.0706, -0.0055,  0.0370, -0.0271, -0.0108,  0.0207],\n",
            "        [ 0.0396,  0.0296,  0.0300, -0.0029,  0.0194, -0.0189, -0.0064,  0.0172]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02118605002760887 tensor([[ 0.1182,  0.1051,  0.1251, -0.0145,  0.0654, -0.0389, -0.0266,  0.0245],\n",
            "        [ 0.1047,  0.0751,  0.0865, -0.0106,  0.0524, -0.0422, -0.0175,  0.0230],\n",
            "        [ 0.0802,  0.0634,  0.0769, -0.0063,  0.0416, -0.0261, -0.0114,  0.0153],\n",
            "        [ 0.0504,  0.0356,  0.0355, -0.0035,  0.0244, -0.0233, -0.0077,  0.0203]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.021118059754371643 tensor([[ 0.1215,  0.1036,  0.1271, -0.0170,  0.0688, -0.0337, -0.0292,  0.0136],\n",
            "        [ 0.1114,  0.0719,  0.0850, -0.0122,  0.0556, -0.0416, -0.0184,  0.0148],\n",
            "        [ 0.0863,  0.0640,  0.0805, -0.0069,  0.0450, -0.0239, -0.0116,  0.0086],\n",
            "        [ 0.0586,  0.0392,  0.0383, -0.0040,  0.0280, -0.0264, -0.0085,  0.0218]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.021064065396785736 tensor([[ 0.1252,  0.1025,  0.1295, -0.0196,  0.0723, -0.0287, -0.0319,  0.0031],\n",
            "        [ 0.1180,  0.0687,  0.0833, -0.0138,  0.0589, -0.0410, -0.0193,  0.0067],\n",
            "        [ 0.0914,  0.0635,  0.0829, -0.0075,  0.0478, -0.0211, -0.0116,  0.0013],\n",
            "        [ 0.0651,  0.0412,  0.0395, -0.0043,  0.0308, -0.0285, -0.0090,  0.0223]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, 1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025093141943216324 tensor([[ 0.0790,  0.0697,  0.0777, -0.0070,  0.0398, -0.0355, -0.0147,  0.0320],\n",
            "        [ 0.0860,  0.0862,  0.0944, -0.0077,  0.0438, -0.0396, -0.0138,  0.0347],\n",
            "        [ 0.0524,  0.0467,  0.0541, -0.0052,  0.0269, -0.0216, -0.0086,  0.0196],\n",
            "        [ 0.0233,  0.0121,  0.0147, -0.0023,  0.0125, -0.0105, -0.0046,  0.0092]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021747540682554245 tensor([[ 0.1040,  0.0855,  0.0972, -0.0107,  0.0520, -0.0432, -0.0197,  0.0348],\n",
            "        [ 0.0907,  0.0912,  0.1002, -0.0100,  0.0464, -0.0384, -0.0138,  0.0251],\n",
            "        [ 0.0501,  0.0393,  0.0497, -0.0068,  0.0254, -0.0151, -0.0073,  0.0084],\n",
            "        [ 0.0364,  0.0125,  0.0174, -0.0038,  0.0198, -0.0149, -0.0074,  0.0122]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.021572530269622803 tensor([[ 0.1194,  0.0917,  0.1062, -0.0137,  0.0592, -0.0461, -0.0230,  0.0325],\n",
            "        [ 0.0910,  0.0919,  0.1014, -0.0121,  0.0468, -0.0348, -0.0130,  0.0130],\n",
            "        [ 0.0518,  0.0358,  0.0496, -0.0085,  0.0259, -0.0107, -0.0065, -0.0003],\n",
            "        [ 0.0478,  0.0113,  0.0182, -0.0052,  0.0262, -0.0183, -0.0098,  0.0141]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.021487686783075333 tensor([[ 0.1307,  0.0937,  0.1106, -0.0165,  0.0643, -0.0470, -0.0257,  0.0280],\n",
            "        [ 0.0911,  0.0923,  0.1025, -0.0140,  0.0471, -0.0311, -0.0122,  0.0007],\n",
            "        [ 0.0548,  0.0337,  0.0510, -0.0103,  0.0271, -0.0070, -0.0060, -0.0080],\n",
            "        [ 0.0578,  0.0088,  0.0177, -0.0065,  0.0318, -0.0210, -0.0120,  0.0152]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.021428342908620834 tensor([[ 0.1400,  0.0937,  0.1128, -0.0191,  0.0683, -0.0468, -0.0279,  0.0224],\n",
            "        [ 0.0916,  0.0933,  0.1042, -0.0160,  0.0476, -0.0276, -0.0115, -0.0112],\n",
            "        [ 0.0582,  0.0320,  0.0528, -0.0121,  0.0284, -0.0036, -0.0055, -0.0154],\n",
            "        [ 0.0667,  0.0053,  0.0161, -0.0077,  0.0369, -0.0231, -0.0140,  0.0156]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02299884334206581 tensor([[ 0.1082,  0.1003,  0.1097, -0.0087,  0.0544, -0.0489, -0.0188,  0.0448],\n",
            "        [ 0.0814,  0.0741,  0.0810, -0.0064,  0.0406, -0.0370, -0.0138,  0.0362],\n",
            "        [ 0.0444,  0.0409,  0.0450, -0.0029,  0.0219, -0.0189, -0.0066,  0.0156],\n",
            "        [ 0.0230,  0.0202,  0.0224, -0.0019,  0.0130, -0.0116, -0.0040,  0.0104]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017796291038393974 tensor([[ 0.1585,  0.1447,  0.1598, -0.0142,  0.0795, -0.0675, -0.0274,  0.0565],\n",
            "        [ 0.1124,  0.0989,  0.1085, -0.0098,  0.0562, -0.0480, -0.0192,  0.0437],\n",
            "        [ 0.0498,  0.0422,  0.0466, -0.0029,  0.0236, -0.0179, -0.0065,  0.0098],\n",
            "        [ 0.0339,  0.0292,  0.0330, -0.0026,  0.0195, -0.0163, -0.0053,  0.0136]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017026320099830627 tensor([[ 0.1802,  0.1599,  0.1780, -0.0178,  0.0900, -0.0714, -0.0309,  0.0526],\n",
            "        [ 0.1222,  0.1024,  0.1128, -0.0117,  0.0609, -0.0478, -0.0209,  0.0390],\n",
            "        [ 0.0484,  0.0366,  0.0409, -0.0023,  0.0218, -0.0134, -0.0051,  0.0005],\n",
            "        [ 0.0403,  0.0341,  0.0393, -0.0030,  0.0238, -0.0186, -0.0056,  0.0143]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016895994544029236 tensor([[ 0.1926,  0.1655,  0.1858, -0.0209,  0.0956, -0.0707, -0.0329,  0.0439],\n",
            "        [ 0.1274,  0.1013,  0.1123, -0.0133,  0.0633, -0.0452, -0.0219,  0.0317],\n",
            "        [ 0.0483,  0.0324,  0.0367, -0.0018,  0.0207, -0.0097, -0.0039, -0.0077],\n",
            "        [ 0.0449,  0.0370,  0.0435, -0.0032,  0.0270, -0.0200, -0.0057,  0.0139]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016837812960147858 tensor([[ 0.2017,  0.1678,  0.1899, -0.0237,  0.0996, -0.0684, -0.0342,  0.0334],\n",
            "        [ 0.1320,  0.0996,  0.1111, -0.0148,  0.0654, -0.0423, -0.0228,  0.0241],\n",
            "        [ 0.0498,  0.0298,  0.0343, -0.0013,  0.0205, -0.0068, -0.0029, -0.0148],\n",
            "        [ 0.0483,  0.0389,  0.0464, -0.0034,  0.0298, -0.0207, -0.0055,  0.0128]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023478824645280838 tensor([[ 0.0887,  0.0846,  0.0953, -0.0088,  0.0448, -0.0369, -0.0149,  0.0326],\n",
            "        [ 0.0766,  0.0714,  0.0777, -0.0070,  0.0403, -0.0357, -0.0143,  0.0325],\n",
            "        [ 0.0597,  0.0511,  0.0563, -0.0047,  0.0300, -0.0268, -0.0093,  0.0258],\n",
            "        [ 0.0244,  0.0187,  0.0206, -0.0023,  0.0124, -0.0113, -0.0045,  0.0110]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02018294669687748 tensor([[ 0.0902,  0.0805,  0.0939, -0.0118,  0.0449, -0.0301, -0.0148,  0.0189],\n",
            "        [ 0.0800,  0.0696,  0.0768, -0.0094,  0.0436, -0.0331, -0.0158,  0.0235],\n",
            "        [ 0.0767,  0.0590,  0.0662, -0.0067,  0.0384, -0.0314, -0.0112,  0.0277],\n",
            "        [ 0.0385,  0.0272,  0.0306, -0.0038,  0.0194, -0.0164, -0.0069,  0.0152]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02006721682846546 tensor([[ 0.0942,  0.0792,  0.0953, -0.0149,  0.0464, -0.0247, -0.0151,  0.0068],\n",
            "        [ 0.0831,  0.0675,  0.0756, -0.0116,  0.0468, -0.0304, -0.0172,  0.0147],\n",
            "        [ 0.0872,  0.0603,  0.0690, -0.0082,  0.0434, -0.0326, -0.0120,  0.0259],\n",
            "        [ 0.0486,  0.0317,  0.0364, -0.0050,  0.0243, -0.0192, -0.0087,  0.0169]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020006809383630753 tensor([[ 0.0991,  0.0786,  0.0977, -0.0180,  0.0483, -0.0198, -0.0155, -0.0046],\n",
            "        [ 0.0865,  0.0657,  0.0747, -0.0139,  0.0501, -0.0280, -0.0187,  0.0063],\n",
            "        [ 0.0951,  0.0590,  0.0689, -0.0096,  0.0471, -0.0324, -0.0124,  0.0227],\n",
            "        [ 0.0563,  0.0340,  0.0397, -0.0060,  0.0281, -0.0207, -0.0100,  0.0171]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01995900645852089 tensor([[ 0.1041,  0.0782,  0.1003, -0.0212,  0.0502, -0.0151, -0.0160, -0.0158],\n",
            "        [ 0.0901,  0.0641,  0.0740, -0.0162,  0.0535, -0.0257, -0.0202, -0.0019],\n",
            "        [ 0.1018,  0.0567,  0.0676, -0.0109,  0.0502, -0.0317, -0.0125,  0.0189],\n",
            "        [ 0.0626,  0.0349,  0.0415, -0.0070,  0.0311, -0.0214, -0.0110,  0.0164]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, 1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021123137325048447 tensor([[ 0.0924,  0.0867,  0.0958, -0.0071,  0.0459, -0.0404, -0.0150,  0.0351],\n",
            "        [ 0.0932,  0.0883,  0.0968, -0.0076,  0.0478, -0.0433, -0.0160,  0.0406],\n",
            "        [ 0.0713,  0.0654,  0.0723, -0.0069,  0.0365, -0.0322, -0.0120,  0.0311],\n",
            "        [ 0.0223,  0.0161,  0.0180, -0.0023,  0.0125, -0.0094, -0.0050,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01641896739602089 tensor([[ 0.1080,  0.0967,  0.1089, -0.0091,  0.0528, -0.0419, -0.0165,  0.0292],\n",
            "        [ 0.1038,  0.0938,  0.1039, -0.0098,  0.0535, -0.0445, -0.0177,  0.0358],\n",
            "        [ 0.0853,  0.0738,  0.0832, -0.0098,  0.0435, -0.0343, -0.0140,  0.0294],\n",
            "        [ 0.0336,  0.0199,  0.0233, -0.0036,  0.0195, -0.0123, -0.0082,  0.0135]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016308767721056938 tensor([[ 0.1154,  0.0983,  0.1129, -0.0106,  0.0556, -0.0394, -0.0165,  0.0192],\n",
            "        [ 0.1072,  0.0918,  0.1030, -0.0116,  0.0554, -0.0419, -0.0180,  0.0272],\n",
            "        [ 0.0908,  0.0735,  0.0848, -0.0122,  0.0461, -0.0321, -0.0146,  0.0230],\n",
            "        [ 0.0422,  0.0209,  0.0256, -0.0048,  0.0251, -0.0135, -0.0108,  0.0150]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01626066118478775 tensor([[ 0.1216,  0.0988,  0.1156, -0.0120,  0.0578, -0.0364, -0.0163,  0.0086],\n",
            "        [ 0.1106,  0.0899,  0.1022, -0.0133,  0.0574, -0.0395, -0.0184,  0.0187],\n",
            "        [ 0.0953,  0.0724,  0.0854, -0.0145,  0.0483, -0.0294, -0.0149,  0.0162],\n",
            "        [ 0.0494,  0.0206,  0.0264, -0.0059,  0.0300, -0.0140, -0.0132,  0.0156]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016217205673456192 tensor([[ 0.1274,  0.0988,  0.1178, -0.0133,  0.0597, -0.0332, -0.0160, -0.0020],\n",
            "        [ 0.1145,  0.0885,  0.1020, -0.0151,  0.0597, -0.0372, -0.0189,  0.0106],\n",
            "        [ 0.0996,  0.0711,  0.0859, -0.0168,  0.0503, -0.0267, -0.0153,  0.0095],\n",
            "        [ 0.0556,  0.0194,  0.0263, -0.0069,  0.0344, -0.0139, -0.0155,  0.0157]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02000214159488678 tensor([[ 0.0922,  0.0896,  0.0985, -0.0079,  0.0457, -0.0415, -0.0152,  0.0344],\n",
            "        [ 0.0852,  0.0776,  0.0867, -0.0074,  0.0433, -0.0385, -0.0145,  0.0338],\n",
            "        [ 0.0674,  0.0596,  0.0655, -0.0062,  0.0348, -0.0309, -0.0118,  0.0288],\n",
            "        [ 0.0266,  0.0214,  0.0226, -0.0023,  0.0128, -0.0129, -0.0046,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01579374447464943 tensor([[ 0.0962,  0.0906,  0.1007, -0.0100,  0.0463, -0.0390, -0.0151,  0.0221],\n",
            "        [ 0.0951,  0.0793,  0.0920, -0.0101,  0.0483, -0.0372, -0.0158,  0.0248],\n",
            "        [ 0.0918,  0.0755,  0.0843, -0.0095,  0.0479, -0.0386, -0.0163,  0.0322],\n",
            "        [ 0.0417,  0.0312,  0.0327, -0.0033,  0.0198, -0.0192, -0.0069,  0.0166]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015608436428010464 tensor([[ 0.0990,  0.0902,  0.1013, -0.0121,  0.0464, -0.0358, -0.0147,  0.0096],\n",
            "        [ 0.0994,  0.0754,  0.0911, -0.0125,  0.0505, -0.0333, -0.0161,  0.0132],\n",
            "        [ 0.1045,  0.0797,  0.0904, -0.0120,  0.0551, -0.0403, -0.0188,  0.0290],\n",
            "        [ 0.0522,  0.0367,  0.0382, -0.0041,  0.0243, -0.0229, -0.0083,  0.0183]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015530748292803764 tensor([[ 0.1028,  0.0908,  0.1031, -0.0142,  0.0470, -0.0333, -0.0146, -0.0022],\n",
            "        [ 0.1043,  0.0720,  0.0908, -0.0148,  0.0530, -0.0297, -0.0165,  0.0022],\n",
            "        [ 0.1132,  0.0800,  0.0921, -0.0143,  0.0602, -0.0400, -0.0206,  0.0237],\n",
            "        [ 0.0603,  0.0398,  0.0410, -0.0047,  0.0277, -0.0252, -0.0094,  0.0185]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015473411418497562 tensor([[ 0.1068,  0.0918,  0.1052, -0.0163,  0.0478, -0.0309, -0.0144, -0.0137],\n",
            "        [ 0.1096,  0.0691,  0.0909, -0.0171,  0.0557, -0.0264, -0.0170, -0.0084],\n",
            "        [ 0.1203,  0.0787,  0.0921, -0.0164,  0.0644, -0.0388, -0.0222,  0.0175],\n",
            "        [ 0.0668,  0.0414,  0.0423, -0.0052,  0.0302, -0.0267, -0.0101,  0.0177]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020491383969783783 tensor([[ 0.1089,  0.1039,  0.1149, -0.0091,  0.0553, -0.0487, -0.0189,  0.0441],\n",
            "        [ 0.0911,  0.0832,  0.0916, -0.0072,  0.0457, -0.0409, -0.0153,  0.0371],\n",
            "        [ 0.0694,  0.0653,  0.0709, -0.0062,  0.0358, -0.0326, -0.0122,  0.0314],\n",
            "        [ 0.0261,  0.0210,  0.0239, -0.0032,  0.0136, -0.0122, -0.0046,  0.0109]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014644252136349678 tensor([[ 0.1291,  0.1191,  0.1340, -0.0126,  0.0653, -0.0524, -0.0225,  0.0404],\n",
            "        [ 0.1147,  0.0992,  0.1109, -0.0098,  0.0570, -0.0462, -0.0186,  0.0349],\n",
            "        [ 0.0922,  0.0844,  0.0916, -0.0091,  0.0483, -0.0409, -0.0165,  0.0360],\n",
            "        [ 0.0398,  0.0284,  0.0339, -0.0054,  0.0208, -0.0169, -0.0070,  0.0137]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.014365551993250847 tensor([[ 0.1334,  0.1180,  0.1350, -0.0149,  0.0671, -0.0483, -0.0234,  0.0287],\n",
            "        [ 0.1219,  0.0981,  0.1119, -0.0113,  0.0598, -0.0430, -0.0189,  0.0238],\n",
            "        [ 0.1000,  0.0885,  0.0961, -0.0111,  0.0531, -0.0415, -0.0182,  0.0323],\n",
            "        [ 0.0490,  0.0313,  0.0392, -0.0072,  0.0258, -0.0190, -0.0086,  0.0136]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014302397146821022 tensor([[ 0.1379,  0.1171,  0.1364, -0.0172,  0.0692, -0.0444, -0.0242,  0.0174],\n",
            "        [ 0.1283,  0.0963,  0.1120, -0.0127,  0.0623, -0.0395, -0.0192,  0.0124],\n",
            "        [ 0.1048,  0.0896,  0.0974, -0.0128,  0.0564, -0.0406, -0.0194,  0.0270],\n",
            "        [ 0.0563,  0.0323,  0.0425, -0.0090,  0.0298, -0.0200, -0.0099,  0.0123]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01425102911889553 tensor([[ 0.1427,  0.1165,  0.1381, -0.0195,  0.0714, -0.0406, -0.0252,  0.0064],\n",
            "        [ 0.1348,  0.0946,  0.1122, -0.0141,  0.0647, -0.0361, -0.0194,  0.0013],\n",
            "        [ 0.1085,  0.0897,  0.0975, -0.0144,  0.0592, -0.0392, -0.0204,  0.0211],\n",
            "        [ 0.0624,  0.0322,  0.0446, -0.0106,  0.0332, -0.0203, -0.0109,  0.0103]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02565959095954895 tensor([[ 0.1041,  0.1035,  0.1145, -0.0104,  0.0529, -0.0434, -0.0174,  0.0397],\n",
            "        [ 0.0914,  0.0773,  0.0839, -0.0067,  0.0454, -0.0431, -0.0158,  0.0404],\n",
            "        [ 0.0720,  0.0687,  0.0752, -0.0058,  0.0361, -0.0313, -0.0115,  0.0291],\n",
            "        [ 0.0282,  0.0227,  0.0257, -0.0030,  0.0147, -0.0140, -0.0050,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019918665289878845 tensor([[ 0.1169,  0.1156,  0.1307, -0.0152,  0.0593, -0.0402, -0.0190,  0.0295],\n",
            "        [ 0.1258,  0.0988,  0.1077, -0.0096,  0.0618, -0.0565, -0.0214,  0.0481],\n",
            "        [ 0.0981,  0.0924,  0.1015, -0.0084,  0.0489, -0.0385, -0.0151,  0.0320],\n",
            "        [ 0.0443,  0.0328,  0.0381, -0.0049,  0.0227, -0.0214, -0.0075,  0.0167]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019492965191602707 tensor([[ 0.1164,  0.1139,  0.1316, -0.0189,  0.0589, -0.0305, -0.0184,  0.0130],\n",
            "        [ 0.1422,  0.1021,  0.1119, -0.0114,  0.0690, -0.0605, -0.0236,  0.0457],\n",
            "        [ 0.1073,  0.0991,  0.1094, -0.0100,  0.0531, -0.0368, -0.0157,  0.0255],\n",
            "        [ 0.0551,  0.0379,  0.0452, -0.0064,  0.0278, -0.0258, -0.0089,  0.0181]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019383281469345093 tensor([[ 0.1170,  0.1134,  0.1338, -0.0226,  0.0591, -0.0215, -0.0179, -0.0024],\n",
            "        [ 0.1546,  0.1015,  0.1118, -0.0129,  0.0742, -0.0625, -0.0251,  0.0413],\n",
            "        [ 0.1125,  0.1016,  0.1130, -0.0113,  0.0553, -0.0331, -0.0157,  0.0168],\n",
            "        [ 0.0632,  0.0404,  0.0495, -0.0077,  0.0314, -0.0287, -0.0098,  0.0178]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019305942580103874 tensor([[ 0.1187,  0.1140,  0.1371, -0.0263,  0.0600, -0.0131, -0.0177, -0.0168],\n",
            "        [ 0.1655,  0.0994,  0.1100, -0.0142,  0.0786, -0.0637, -0.0264,  0.0361],\n",
            "        [ 0.1164,  0.1028,  0.1150, -0.0126,  0.0567, -0.0287, -0.0155,  0.0074],\n",
            "        [ 0.0697,  0.0413,  0.0521, -0.0089,  0.0342, -0.0307, -0.0104,  0.0165]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024063531309366226 tensor([[ 0.0766,  0.0784,  0.0846, -0.0067,  0.0383, -0.0333, -0.0129,  0.0257],\n",
            "        [ 0.0994,  0.0842,  0.0935, -0.0077,  0.0501, -0.0429, -0.0169,  0.0404],\n",
            "        [ 0.0702,  0.0607,  0.0677, -0.0067,  0.0358, -0.0327, -0.0122,  0.0309],\n",
            "        [ 0.0289,  0.0242,  0.0270, -0.0034,  0.0148, -0.0134, -0.0053,  0.0121]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01980290189385414 tensor([[ 0.0569,  0.0595,  0.0630, -0.0066,  0.0275, -0.0186, -0.0095,  0.0016],\n",
            "        [ 0.1245,  0.0929,  0.1065, -0.0109,  0.0620, -0.0465, -0.0204,  0.0379],\n",
            "        [ 0.1051,  0.0874,  0.0990, -0.0108,  0.0540, -0.0465, -0.0183,  0.0409],\n",
            "        [ 0.0464,  0.0364,  0.0415, -0.0058,  0.0239, -0.0201, -0.0087,  0.0169]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019240930676460266 tensor([[ 0.0538,  0.0575,  0.0597, -0.0073,  0.0255, -0.0126, -0.0088, -0.0125],\n",
            "        [ 0.1410,  0.0935,  0.1107, -0.0135,  0.0695, -0.0459, -0.0224,  0.0312],\n",
            "        [ 0.1255,  0.1000,  0.1148, -0.0140,  0.0647, -0.0528, -0.0220,  0.0427],\n",
            "        [ 0.0585,  0.0431,  0.0502, -0.0079,  0.0301, -0.0237, -0.0111,  0.0181]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019079091027379036 tensor([[ 0.0532,  0.0580,  0.0591, -0.0081,  0.0248, -0.0080, -0.0085, -0.0246],\n",
            "        [ 0.1529,  0.0895,  0.1099, -0.0158,  0.0746, -0.0431, -0.0235,  0.0222],\n",
            "        [ 0.1381,  0.1046,  0.1219, -0.0166,  0.0714, -0.0550, -0.0242,  0.0402],\n",
            "        [ 0.0673,  0.0467,  0.0556, -0.0098,  0.0348, -0.0255, -0.0130,  0.0172]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018999001011252403 tensor([[ 0.0543,  0.0602,  0.0604, -0.0090,  0.0250, -0.0044, -0.0085, -0.0356],\n",
            "        [ 0.1636,  0.0845,  0.1078, -0.0180,  0.0791, -0.0398, -0.0244,  0.0128],\n",
            "        [ 0.1471,  0.1056,  0.1251, -0.0189,  0.0762, -0.0554, -0.0259,  0.0356],\n",
            "        [ 0.0743,  0.0483,  0.0590, -0.0116,  0.0384, -0.0262, -0.0145,  0.0152]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02476748637855053 tensor([[ 0.1008,  0.0932,  0.1013, -0.0075,  0.0500, -0.0443, -0.0170,  0.0404],\n",
            "        [ 0.1001,  0.0965,  0.1064, -0.0081,  0.0503, -0.0449, -0.0161,  0.0415],\n",
            "        [ 0.0664,  0.0594,  0.0651, -0.0057,  0.0342, -0.0300, -0.0120,  0.0283],\n",
            "        [ 0.0243,  0.0208,  0.0237, -0.0028,  0.0129, -0.0107, -0.0045,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018284175544977188 tensor([[ 0.1477,  0.1343,  0.1469, -0.0117,  0.0727, -0.0606, -0.0244,  0.0505],\n",
            "        [ 0.1334,  0.1278,  0.1422, -0.0122,  0.0667, -0.0551, -0.0210,  0.0449],\n",
            "        [ 0.0960,  0.0820,  0.0902, -0.0088,  0.0499, -0.0408, -0.0178,  0.0355],\n",
            "        [ 0.0368,  0.0289,  0.0346, -0.0049,  0.0196, -0.0145, -0.0067,  0.0110]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017415933310985565 tensor([[ 0.1691,  0.1492,  0.1640, -0.0142,  0.0822, -0.0641, -0.0273,  0.0475],\n",
            "        [ 0.1364,  0.1283,  0.1445, -0.0143,  0.0676, -0.0498, -0.0207,  0.0318],\n",
            "        [ 0.1079,  0.0864,  0.0957, -0.0106,  0.0564, -0.0422, -0.0204,  0.0327],\n",
            "        [ 0.0443,  0.0320,  0.0405, -0.0065,  0.0236, -0.0152, -0.0079,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017276469618082047 tensor([[ 0.1831,  0.1566,  0.1728, -0.0163,  0.0879, -0.0640, -0.0289,  0.0407],\n",
            "        [ 0.1365,  0.1259,  0.1437, -0.0161,  0.0669, -0.0430, -0.0199,  0.0172],\n",
            "        [ 0.1159,  0.0870,  0.0971, -0.0121,  0.0609, -0.0417, -0.0222,  0.0278],\n",
            "        [ 0.0498,  0.0333,  0.0444, -0.0080,  0.0266, -0.0148, -0.0088,  0.0067]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01719498448073864 tensor([[ 0.1940,  0.1607,  0.1782, -0.0181,  0.0919, -0.0623, -0.0299,  0.0322],\n",
            "        [ 0.1374,  0.1244,  0.1440, -0.0179,  0.0667, -0.0366, -0.0192,  0.0032],\n",
            "        [ 0.1227,  0.0864,  0.0973, -0.0135,  0.0648, -0.0405, -0.0239,  0.0224],\n",
            "        [ 0.0543,  0.0335,  0.0472, -0.0095,  0.0290, -0.0139, -0.0095,  0.0033]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024988077580928802 tensor([[ 0.0680,  0.0660,  0.0747, -0.0071,  0.0339, -0.0290, -0.0096,  0.0225],\n",
            "        [ 0.0694,  0.0628,  0.0714, -0.0061,  0.0354, -0.0262, -0.0125,  0.0240],\n",
            "        [ 0.0619,  0.0522,  0.0562, -0.0052,  0.0298, -0.0301, -0.0098,  0.0277],\n",
            "        [ 0.0252,  0.0202,  0.0206, -0.0021,  0.0119, -0.0117, -0.0047,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.022460129112005234 tensor([[ 0.0648,  0.0601,  0.0711, -0.0093,  0.0316, -0.0227, -0.0068,  0.0077],\n",
            "        [ 0.0680,  0.0540,  0.0660, -0.0079,  0.0350, -0.0158, -0.0130,  0.0082],\n",
            "        [ 0.0882,  0.0688,  0.0739, -0.0080,  0.0412, -0.0416, -0.0133,  0.0346],\n",
            "        [ 0.0397,  0.0298,  0.0298, -0.0034,  0.0184, -0.0174, -0.0073,  0.0194]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02223118767142296 tensor([[ 0.0674,  0.0601,  0.0739, -0.0119,  0.0323, -0.0194, -0.0050, -0.0037],\n",
            "        [ 0.0712,  0.0497,  0.0654, -0.0098,  0.0368, -0.0078, -0.0142, -0.0045],\n",
            "        [ 0.1046,  0.0757,  0.0810, -0.0102,  0.0476, -0.0479, -0.0151,  0.0359],\n",
            "        [ 0.0496,  0.0349,  0.0344, -0.0045,  0.0226, -0.0204, -0.0092,  0.0231]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.022139057517051697 tensor([[ 0.0705,  0.0606,  0.0773, -0.0145,  0.0333, -0.0163, -0.0033, -0.0147],\n",
            "        [ 0.0754,  0.0463,  0.0656, -0.0118,  0.0392, -0.0004, -0.0156, -0.0164],\n",
            "        [ 0.1159,  0.0775,  0.0825, -0.0121,  0.0513, -0.0516, -0.0160,  0.0344],\n",
            "        [ 0.0568,  0.0374,  0.0362, -0.0055,  0.0254, -0.0220, -0.0107,  0.0251]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02207823283970356 tensor([[ 0.0737,  0.0613,  0.0808, -0.0171,  0.0343, -0.0134, -0.0016, -0.0255],\n",
            "        [ 0.0802,  0.0437,  0.0667, -0.0137,  0.0420,  0.0065, -0.0172, -0.0275],\n",
            "        [ 0.1248,  0.0769,  0.0814, -0.0137,  0.0538, -0.0541, -0.0164,  0.0315],\n",
            "        [ 0.0625,  0.0385,  0.0364, -0.0064,  0.0275, -0.0227, -0.0118,  0.0262]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020831551402807236 tensor([[ 0.0939,  0.0882,  0.0985, -0.0080,  0.0470, -0.0411, -0.0159,  0.0385],\n",
            "        [ 0.0825,  0.0762,  0.0844, -0.0054,  0.0414, -0.0356, -0.0136,  0.0322],\n",
            "        [ 0.0550,  0.0502,  0.0554, -0.0051,  0.0286, -0.0248, -0.0092,  0.0208],\n",
            "        [ 0.0270,  0.0187,  0.0218, -0.0027,  0.0146, -0.0129, -0.0050,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017105907201766968 tensor([[ 0.1030,  0.0914,  0.1051, -0.0107,  0.0510, -0.0395, -0.0168,  0.0314],\n",
            "        [ 0.0942,  0.0818,  0.0920, -0.0060,  0.0466, -0.0348, -0.0148,  0.0254],\n",
            "        [ 0.0593,  0.0494,  0.0561, -0.0066,  0.0313, -0.0228, -0.0098,  0.0129],\n",
            "        [ 0.0413,  0.0240,  0.0296, -0.0044,  0.0227, -0.0184, -0.0077,  0.0178]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01701899990439415 tensor([[ 0.1079,  0.0902,  0.1069, -0.0131,  0.0527, -0.0357, -0.0170,  0.0223],\n",
            "        [ 0.1011,  0.0824,  0.0944, -0.0062,  0.0493, -0.0315, -0.0152,  0.0161],\n",
            "        [ 0.0618,  0.0468,  0.0549, -0.0080,  0.0330, -0.0200, -0.0101,  0.0040],\n",
            "        [ 0.0525,  0.0262,  0.0340, -0.0059,  0.0292, -0.0222, -0.0097,  0.0208]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016960373148322105 tensor([[ 0.1125,  0.0889,  0.1085, -0.0154,  0.0544, -0.0320, -0.0172,  0.0132],\n",
            "        [ 0.1073,  0.0824,  0.0960, -0.0064,  0.0516, -0.0279, -0.0154,  0.0065],\n",
            "        [ 0.0648,  0.0448,  0.0544, -0.0094,  0.0351, -0.0174, -0.0104, -0.0043],\n",
            "        [ 0.0617,  0.0265,  0.0365, -0.0073,  0.0348, -0.0249, -0.0115,  0.0227]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01690925471484661 tensor([[ 0.1171,  0.0876,  0.1102, -0.0178,  0.0561, -0.0283, -0.0174,  0.0043],\n",
            "        [ 0.1132,  0.0822,  0.0973, -0.0065,  0.0538, -0.0242, -0.0157, -0.0031],\n",
            "        [ 0.0681,  0.0431,  0.0541, -0.0108,  0.0373, -0.0150, -0.0108, -0.0124],\n",
            "        [ 0.0697,  0.0256,  0.0375, -0.0086,  0.0396, -0.0269, -0.0130,  0.0238]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02288629114627838 tensor([[ 0.0874,  0.0774,  0.0848, -0.0070,  0.0425, -0.0392, -0.0140,  0.0345],\n",
            "        [ 0.0777,  0.0719,  0.0797, -0.0067,  0.0394, -0.0339, -0.0130,  0.0326],\n",
            "        [ 0.0456,  0.0412,  0.0459, -0.0034,  0.0230, -0.0206, -0.0083,  0.0200],\n",
            "        [ 0.0108,  0.0090,  0.0125, -0.0021,  0.0051, -0.0023, -0.0009, -0.0016]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019138023257255554 tensor([[ 0.1209,  0.1020,  0.1123, -0.0105,  0.0575, -0.0509, -0.0188,  0.0396],\n",
            "        [ 0.1062,  0.0956,  0.1078, -0.0107,  0.0543, -0.0417, -0.0176,  0.0371],\n",
            "        [ 0.0594,  0.0515,  0.0576, -0.0042,  0.0297, -0.0254, -0.0108,  0.0229],\n",
            "        [ 0.0150,  0.0115,  0.0188, -0.0040,  0.0065, -0.0005, -0.0003, -0.0075]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01876038685441017 tensor([[ 0.1338,  0.1059,  0.1170, -0.0125,  0.0621, -0.0524, -0.0201,  0.0343],\n",
            "        [ 0.1153,  0.0995,  0.1147, -0.0134,  0.0592, -0.0394, -0.0188,  0.0307],\n",
            "        [ 0.0632,  0.0521,  0.0587, -0.0042,  0.0312, -0.0250, -0.0115,  0.0205],\n",
            "        [ 0.0176,  0.0125,  0.0238, -0.0058,  0.0069,  0.0023,  0.0007, -0.0143]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01871003769338131 tensor([[ 0.1425,  0.1054,  0.1169, -0.0142,  0.0646, -0.0519, -0.0207,  0.0268],\n",
            "        [ 0.1206,  0.0995,  0.1174, -0.0159,  0.0622, -0.0351, -0.0194,  0.0223],\n",
            "        [ 0.0655,  0.0512,  0.0582, -0.0040,  0.0320, -0.0240, -0.0118,  0.0174],\n",
            "        [ 0.0198,  0.0132,  0.0284, -0.0076,  0.0070,  0.0053,  0.0018, -0.0214]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018671073019504547 tensor([[ 0.1502,  0.1040,  0.1157, -0.0158,  0.0666, -0.0509, -0.0212,  0.0190],\n",
            "        [ 0.1252,  0.0988,  0.1193, -0.0183,  0.0648, -0.0305, -0.0198,  0.0136],\n",
            "        [ 0.0676,  0.0503,  0.0577, -0.0038,  0.0326, -0.0229, -0.0121,  0.0143],\n",
            "        [ 0.0218,  0.0137,  0.0329, -0.0093,  0.0070,  0.0084,  0.0029, -0.0284]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024346332997083664 tensor([[ 0.1077,  0.1028,  0.1117, -0.0081,  0.0548, -0.0487, -0.0186,  0.0470],\n",
            "        [ 0.0793,  0.0707,  0.0790, -0.0071,  0.0408, -0.0353, -0.0149,  0.0340],\n",
            "        [ 0.0637,  0.0590,  0.0645, -0.0044,  0.0312, -0.0280, -0.0095,  0.0249],\n",
            "        [ 0.0286,  0.0181,  0.0210, -0.0027,  0.0139, -0.0134, -0.0044,  0.0110]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020043451339006424 tensor([[ 0.1155,  0.1041,  0.1135, -0.0100,  0.0588, -0.0468, -0.0199,  0.0401],\n",
            "        [ 0.0846,  0.0666,  0.0778, -0.0094,  0.0441, -0.0318, -0.0172,  0.0265],\n",
            "        [ 0.0732,  0.0649,  0.0716, -0.0050,  0.0345, -0.0284, -0.0093,  0.0201],\n",
            "        [ 0.0477,  0.0259,  0.0311, -0.0044,  0.0222, -0.0211, -0.0066,  0.0158]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01994233764708042 tensor([[ 0.1213,  0.1034,  0.1132, -0.0117,  0.0618, -0.0441, -0.0208,  0.0324],\n",
            "        [ 0.0892,  0.0618,  0.0758, -0.0116,  0.0470, -0.0281, -0.0194,  0.0188],\n",
            "        [ 0.0788,  0.0668,  0.0745, -0.0052,  0.0358, -0.0267, -0.0084,  0.0131],\n",
            "        [ 0.0627,  0.0297,  0.0369, -0.0058,  0.0284, -0.0265, -0.0080,  0.0180]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01987685263156891 tensor([[ 0.1271,  0.1026,  0.1129, -0.0134,  0.0648, -0.0413, -0.0217,  0.0247],\n",
            "        [ 0.0944,  0.0577,  0.0746, -0.0139,  0.0503, -0.0247, -0.0217,  0.0117],\n",
            "        [ 0.0833,  0.0677,  0.0762, -0.0055,  0.0366, -0.0245, -0.0073,  0.0057],\n",
            "        [ 0.0750,  0.0309,  0.0399, -0.0071,  0.0332, -0.0305, -0.0090,  0.0185]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019825637340545654 tensor([[ 0.1328,  0.1019,  0.1125, -0.0151,  0.0678, -0.0386, -0.0226,  0.0172],\n",
            "        [ 0.1000,  0.0540,  0.0738, -0.0161,  0.0537, -0.0216, -0.0241,  0.0049],\n",
            "        [ 0.0875,  0.0681,  0.0774, -0.0056,  0.0371, -0.0221, -0.0062, -0.0019],\n",
            "        [ 0.0855,  0.0304,  0.0410, -0.0082,  0.0371, -0.0334, -0.0096,  0.0180]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.9802e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02417532354593277 tensor([[ 0.1100,  0.1041,  0.1151, -0.0090,  0.0555, -0.0476, -0.0183,  0.0429],\n",
            "        [ 0.1025,  0.0928,  0.1010, -0.0080,  0.0516, -0.0476, -0.0179,  0.0455],\n",
            "        [ 0.0688,  0.0632,  0.0703, -0.0061,  0.0353, -0.0312, -0.0117,  0.0289],\n",
            "        [ 0.0241,  0.0206,  0.0244, -0.0035,  0.0130, -0.0110, -0.0042,  0.0094]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017087416723370552 tensor([[ 0.1421,  0.1311,  0.1477, -0.0135,  0.0711, -0.0548, -0.0231,  0.0424],\n",
            "        [ 0.1484,  0.1310,  0.1427, -0.0125,  0.0750, -0.0650, -0.0263,  0.0576],\n",
            "        [ 0.1017,  0.0912,  0.1024, -0.0094,  0.0525, -0.0436, -0.0171,  0.0369],\n",
            "        [ 0.0360,  0.0282,  0.0358, -0.0063,  0.0200, -0.0151, -0.0060,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016279295086860657 tensor([[ 0.1450,  0.1281,  0.1475, -0.0158,  0.0719, -0.0476, -0.0229,  0.0275],\n",
            "        [ 0.1635,  0.1373,  0.1502, -0.0150,  0.0826, -0.0663, -0.0294,  0.0527],\n",
            "        [ 0.1139,  0.0986,  0.1122, -0.0111,  0.0589, -0.0452, -0.0189,  0.0331],\n",
            "        [ 0.0429,  0.0307,  0.0422, -0.0088,  0.0244, -0.0160, -0.0068,  0.0100]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01617610827088356 tensor([[ 0.1476,  0.1248,  0.1468, -0.0181,  0.0727, -0.0405, -0.0226,  0.0131],\n",
            "        [ 0.1736,  0.1386,  0.1520, -0.0170,  0.0876, -0.0651, -0.0316,  0.0451],\n",
            "        [ 0.1212,  0.1010,  0.1167, -0.0125,  0.0628, -0.0442, -0.0197,  0.0266],\n",
            "        [ 0.0479,  0.0313,  0.0467, -0.0111,  0.0277, -0.0159, -0.0073,  0.0075]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016110070049762726 tensor([[ 0.1516,  0.1228,  0.1476, -0.0203,  0.0741, -0.0341, -0.0226, -0.0005],\n",
            "        [ 0.1822,  0.1383,  0.1521, -0.0190,  0.0919, -0.0632, -0.0335,  0.0370],\n",
            "        [ 0.1267,  0.1015,  0.1191, -0.0138,  0.0657, -0.0422, -0.0203,  0.0191],\n",
            "        [ 0.0518,  0.0310,  0.0501, -0.0134,  0.0306, -0.0152, -0.0076,  0.0044]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02760181948542595 tensor([[ 0.1130,  0.1028,  0.1117, -0.0085,  0.0565, -0.0503, -0.0198,  0.0474],\n",
            "        [ 0.0966,  0.0912,  0.1014, -0.0077,  0.0496, -0.0428, -0.0170,  0.0398],\n",
            "        [ 0.0729,  0.0669,  0.0742, -0.0065,  0.0375, -0.0324, -0.0120,  0.0296],\n",
            "        [ 0.0317,  0.0248,  0.0282, -0.0030,  0.0163, -0.0145, -0.0053,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020375650376081467 tensor([[ 0.1648,  0.1463,  0.1595, -0.0134,  0.0815, -0.0688, -0.0287,  0.0604],\n",
            "        [ 0.1298,  0.1201,  0.1359, -0.0112,  0.0672, -0.0524, -0.0232,  0.0430],\n",
            "        [ 0.1019,  0.0899,  0.1010, -0.0103,  0.0529, -0.0413, -0.0168,  0.0331],\n",
            "        [ 0.0506,  0.0352,  0.0416, -0.0048,  0.0261, -0.0215, -0.0083,  0.0155]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019431788474321365 tensor([[ 0.1867,  0.1593,  0.1741, -0.0163,  0.0913, -0.0725, -0.0324,  0.0581],\n",
            "        [ 0.1343,  0.1199,  0.1388, -0.0127,  0.0699, -0.0472, -0.0242,  0.0306],\n",
            "        [ 0.1099,  0.0915,  0.1050, -0.0126,  0.0577, -0.0392, -0.0180,  0.0246],\n",
            "        [ 0.0638,  0.0398,  0.0490, -0.0061,  0.0330, -0.0251, -0.0101,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01928846724331379 tensor([[ 0.2011,  0.1643,  0.1799, -0.0186,  0.0970, -0.0724, -0.0347,  0.0518],\n",
            "        [ 0.1362,  0.1169,  0.1389, -0.0139,  0.0712, -0.0407, -0.0247,  0.0168],\n",
            "        [ 0.1150,  0.0903,  0.1060, -0.0147,  0.0610, -0.0356, -0.0187,  0.0147],\n",
            "        [ 0.0744,  0.0419,  0.0537, -0.0072,  0.0385, -0.0273, -0.0115,  0.0136]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019200431182980537 tensor([[ 0.2125,  0.1663,  0.1824, -0.0207,  0.1012, -0.0707, -0.0364,  0.0439],\n",
            "        [ 0.1388,  0.1146,  0.1398, -0.0151,  0.0729, -0.0345, -0.0253,  0.0034],\n",
            "        [ 0.1198,  0.0888,  0.1067, -0.0168,  0.0642, -0.0319, -0.0194,  0.0048],\n",
            "        [ 0.0835,  0.0425,  0.0569, -0.0083,  0.0432, -0.0286, -0.0126,  0.0110]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02479151077568531 tensor([[ 0.0933,  0.0922,  0.0997, -0.0075,  0.0460, -0.0412, -0.0154,  0.0368],\n",
            "        [ 0.1002,  0.0934,  0.1031, -0.0075,  0.0504, -0.0440, -0.0170,  0.0404],\n",
            "        [ 0.0737,  0.0683,  0.0754, -0.0065,  0.0376, -0.0337, -0.0121,  0.0298],\n",
            "        [ 0.0262,  0.0197,  0.0226, -0.0027,  0.0151, -0.0122, -0.0063,  0.0130]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01998043991625309 tensor([[ 0.0953,  0.0927,  0.0996, -0.0088,  0.0457, -0.0368, -0.0151,  0.0260],\n",
            "        [ 0.1089,  0.0946,  0.1068, -0.0090,  0.0536, -0.0407, -0.0176,  0.0300],\n",
            "        [ 0.0869,  0.0761,  0.0847, -0.0091,  0.0429, -0.0359, -0.0133,  0.0252],\n",
            "        [ 0.0386,  0.0240,  0.0291, -0.0043,  0.0236, -0.0163, -0.0105,  0.0177]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01988864690065384 tensor([[ 0.0974,  0.0933,  0.0997, -0.0101,  0.0455, -0.0327, -0.0148,  0.0156],\n",
            "        [ 0.1154,  0.0934,  0.1079, -0.0102,  0.0556, -0.0364, -0.0177,  0.0187],\n",
            "        [ 0.0930,  0.0769,  0.0863, -0.0112,  0.0446, -0.0345, -0.0134,  0.0169],\n",
            "        [ 0.0479,  0.0252,  0.0323, -0.0057,  0.0304, -0.0185, -0.0142,  0.0205]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01983076147735119 tensor([[ 0.1004,  0.0948,  0.1007, -0.0114,  0.0457, -0.0290, -0.0147,  0.0058],\n",
            "        [ 0.1219,  0.0924,  0.1091, -0.0115,  0.0578, -0.0321, -0.0179,  0.0076],\n",
            "        [ 0.0979,  0.0765,  0.0865, -0.0132,  0.0458, -0.0326, -0.0132,  0.0081],\n",
            "        [ 0.0555,  0.0247,  0.0337, -0.0071,  0.0364, -0.0198, -0.0176,  0.0221]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019776346161961555 tensor([[ 0.1036,  0.0965,  0.1020, -0.0128,  0.0461, -0.0254, -0.0146, -0.0038],\n",
            "        [ 0.1285,  0.0915,  0.1104, -0.0127,  0.0599, -0.0280, -0.0181, -0.0033],\n",
            "        [ 0.1026,  0.0758,  0.0865, -0.0152,  0.0467, -0.0306, -0.0131, -0.0007],\n",
            "        [ 0.0621,  0.0232,  0.0340, -0.0084,  0.0418, -0.0205, -0.0208,  0.0231]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026569774374365807 tensor([[ 0.1210,  0.1126,  0.1246, -0.0094,  0.0603, -0.0521, -0.0203,  0.0494],\n",
            "        [ 0.0965,  0.0874,  0.0956, -0.0074,  0.0478, -0.0438, -0.0157,  0.0392],\n",
            "        [ 0.0681,  0.0622,  0.0699, -0.0058,  0.0347, -0.0311, -0.0109,  0.0291],\n",
            "        [ 0.0307,  0.0262,  0.0293, -0.0033,  0.0156, -0.0152, -0.0053,  0.0131]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019270211458206177 tensor([[ 0.1645,  0.1497,  0.1678, -0.0143,  0.0815, -0.0646, -0.0274,  0.0556],\n",
            "        [ 0.1307,  0.1138,  0.1251, -0.0106,  0.0640, -0.0542, -0.0208,  0.0416],\n",
            "        [ 0.0938,  0.0833,  0.0952, -0.0088,  0.0481, -0.0403, -0.0145,  0.0346],\n",
            "        [ 0.0498,  0.0399,  0.0457, -0.0059,  0.0245, -0.0241, -0.0082,  0.0196]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018547194078564644 tensor([[ 0.1753,  0.1532,  0.1741, -0.0169,  0.0861, -0.0610, -0.0289,  0.0451],\n",
            "        [ 0.1389,  0.1134,  0.1259, -0.0120,  0.0669, -0.0510, -0.0214,  0.0295],\n",
            "        [ 0.1019,  0.0869,  0.1018, -0.0107,  0.0523, -0.0403, -0.0149,  0.0302],\n",
            "        [ 0.0629,  0.0479,  0.0562, -0.0082,  0.0302, -0.0297, -0.0098,  0.0224]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01844421960413456 tensor([[ 0.1827,  0.1533,  0.1767, -0.0192,  0.0889, -0.0558, -0.0298,  0.0330],\n",
            "        [ 0.1453,  0.1110,  0.1245, -0.0131,  0.0688, -0.0467, -0.0218,  0.0165],\n",
            "        [ 0.1072,  0.0875,  0.1051, -0.0123,  0.0551, -0.0388, -0.0148,  0.0244],\n",
            "        [ 0.0728,  0.0527,  0.0633, -0.0103,  0.0341, -0.0336, -0.0108,  0.0233]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018370505422353745 tensor([[ 0.1897,  0.1530,  0.1788, -0.0215,  0.0916, -0.0505, -0.0307,  0.0208],\n",
            "        [ 0.1518,  0.1090,  0.1235, -0.0143,  0.0709, -0.0427, -0.0221,  0.0039],\n",
            "        [ 0.1114,  0.0872,  0.1074, -0.0138,  0.0573, -0.0369, -0.0146,  0.0180],\n",
            "        [ 0.0806,  0.0554,  0.0681, -0.0122,  0.0370, -0.0362, -0.0114,  0.0229]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024800164625048637 tensor([[ 0.1053,  0.0974,  0.1056, -0.0078,  0.0531, -0.0459, -0.0174,  0.0422],\n",
            "        [ 0.0658,  0.0592,  0.0656, -0.0062,  0.0340, -0.0303, -0.0126,  0.0281],\n",
            "        [ 0.0377,  0.0344,  0.0385, -0.0036,  0.0189, -0.0152, -0.0058,  0.0126],\n",
            "        [ 0.0168,  0.0158,  0.0184, -0.0019,  0.0089, -0.0065, -0.0027,  0.0056]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020394425839185715 tensor([[ 0.1595,  0.1458,  0.1583, -0.0125,  0.0800, -0.0653, -0.0259,  0.0553],\n",
            "        [ 0.0928,  0.0801,  0.0898, -0.0099,  0.0484, -0.0407, -0.0187,  0.0345],\n",
            "        [ 0.0466,  0.0403,  0.0464, -0.0052,  0.0235, -0.0153, -0.0066,  0.0089],\n",
            "        [ 0.0236,  0.0216,  0.0264, -0.0032,  0.0127, -0.0071, -0.0036,  0.0052]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019574541598558426 tensor([[ 0.1845,  0.1644,  0.1785, -0.0153,  0.0918, -0.0699, -0.0294,  0.0528],\n",
            "        [ 0.1022,  0.0835,  0.0950, -0.0124,  0.0537, -0.0421, -0.0217,  0.0311],\n",
            "        [ 0.0478,  0.0386,  0.0464, -0.0063,  0.0242, -0.0112, -0.0059,  0.0009],\n",
            "        [ 0.0272,  0.0242,  0.0311, -0.0043,  0.0148, -0.0059, -0.0039,  0.0029]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01942932978272438 tensor([[ 1.9885e-01,  1.7192e-01,  1.8668e-01, -1.7449e-02,  9.8114e-02,\n",
            "         -6.9261e-02, -3.1028e-02,  4.4684e-02],\n",
            "        [ 1.0697e-01,  8.2148e-02,  9.5246e-02, -1.4563e-02,  5.6539e-02,\n",
            "         -4.1105e-02, -2.3856e-02,  2.5239e-02],\n",
            "        [ 4.8507e-02,  3.6619e-02,  4.6129e-02, -7.3797e-03,  2.4719e-02,\n",
            "         -6.8480e-03, -5.0241e-03, -7.1430e-03],\n",
            "        [ 2.9745e-02,  2.5840e-02,  3.4685e-02, -5.3650e-03,  1.6344e-02,\n",
            "         -4.1795e-03, -3.9804e-03, -1.6689e-05]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01937156915664673 tensor([[ 0.2092,  0.1753,  0.1902, -0.0193,  0.1023, -0.0666, -0.0320,  0.0344],\n",
            "        [ 0.1107,  0.0798,  0.0944, -0.0167,  0.0588, -0.0396, -0.0258,  0.0189],\n",
            "        [ 0.0498,  0.0353,  0.0466, -0.0084,  0.0256, -0.0029, -0.0043, -0.0147],\n",
            "        [ 0.0318,  0.0270,  0.0377, -0.0063,  0.0176, -0.0022, -0.0040, -0.0031]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025128351524472237 tensor([[ 0.1099,  0.1011,  0.1115, -0.0094,  0.0551, -0.0500, -0.0194,  0.0457],\n",
            "        [ 0.1000,  0.0896,  0.0977, -0.0084,  0.0502, -0.0458, -0.0168,  0.0425],\n",
            "        [ 0.0571,  0.0480,  0.0546, -0.0051,  0.0297, -0.0261, -0.0100,  0.0240],\n",
            "        [ 0.0280,  0.0234,  0.0254, -0.0030,  0.0143, -0.0127, -0.0047,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020272191613912582 tensor([[ 0.1130,  0.0949,  0.1064, -0.0117,  0.0557, -0.0465, -0.0202,  0.0349],\n",
            "        [ 0.1116,  0.0899,  0.0993, -0.0115,  0.0558, -0.0461, -0.0187,  0.0358],\n",
            "        [ 0.0750,  0.0563,  0.0671, -0.0072,  0.0395, -0.0310, -0.0132,  0.0250],\n",
            "        [ 0.0436,  0.0349,  0.0383, -0.0050,  0.0227, -0.0182, -0.0071,  0.0151]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020126156508922577 tensor([[ 0.1178,  0.0905,  0.1033, -0.0140,  0.0572, -0.0439, -0.0213,  0.0255],\n",
            "        [ 0.1196,  0.0868,  0.0969, -0.0142,  0.0596, -0.0446, -0.0199,  0.0275],\n",
            "        [ 0.0859,  0.0577,  0.0722, -0.0088,  0.0456, -0.0322, -0.0152,  0.0218],\n",
            "        [ 0.0548,  0.0422,  0.0466, -0.0069,  0.0289, -0.0214, -0.0087,  0.0162]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020050253719091415 tensor([[ 0.1240,  0.0875,  0.1018, -0.0164,  0.0595, -0.0420, -0.0227,  0.0170],\n",
            "        [ 0.1273,  0.0834,  0.0943, -0.0169,  0.0633, -0.0430, -0.0211,  0.0192],\n",
            "        [ 0.0940,  0.0563,  0.0743, -0.0102,  0.0503, -0.0319, -0.0167,  0.0172],\n",
            "        [ 0.0634,  0.0468,  0.0521, -0.0086,  0.0337, -0.0230, -0.0098,  0.0156]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01999492570757866 tensor([[ 0.1306,  0.0851,  0.1007, -0.0188,  0.0620, -0.0404, -0.0240,  0.0088],\n",
            "        [ 0.1349,  0.0801,  0.0916, -0.0196,  0.0669, -0.0415, -0.0222,  0.0110],\n",
            "        [ 0.1008,  0.0537,  0.0749, -0.0115,  0.0543, -0.0310, -0.0180,  0.0118],\n",
            "        [ 0.0702,  0.0498,  0.0557, -0.0102,  0.0376, -0.0236, -0.0105,  0.0139]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.1921e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[6, 5, 4, 1, 4, 4, 6, 2, 4, 11, 6, 6, 11, 4, 2, 1, 11, 1, 5, 5, 8, 10, 6, 5, 1, 11, 6, 4, 4, 6, 6, 6, 4, 8, 1, 9, 6, 6, 6, 1, 6, 4, 6, 1, 5, 6, 4, 4, 4, 2, 1, 8, 4, 6, 2, 4, 4, 1, 6, 6, 6, 1, 10, 11, 5, 3, 11, 6, 6, 11, 5, 1, 4, 1, 11, 11, 1, 5, 11, 8, 4, 2, 11, 5, 2, 2, 11, 11, 4, 9, 5, 4, 4, 4, 11, 8, 4, 2, 6, 4, 1, 0, 2, 6, 4, 1, 5, 8, 5, 6, 6, 4, 4, 5, 4, 2, 4, 8, 1, 2, 4, 6, 5, 6, 4, 4, 5, 6, 9, 4, 4, 6, 1, 6, 2, 6, 5, 11, 2, 8, 4, 4, 4]\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.0250031016767025 tensor([[ 0.1036,  0.0942,  0.1028, -0.0085,  0.0515, -0.0470, -0.0163,  0.0409],\n",
            "        [ 0.0905,  0.0864,  0.0954, -0.0076,  0.0467, -0.0407, -0.0164,  0.0399],\n",
            "        [ 0.0598,  0.0563,  0.0618, -0.0055,  0.0307, -0.0257, -0.0105,  0.0228],\n",
            "        [ 0.0245,  0.0178,  0.0194, -0.0020,  0.0119, -0.0118, -0.0034,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019375817850232124 tensor([[ 0.1432,  0.1259,  0.1381, -0.0129,  0.0704, -0.0612, -0.0217,  0.0463],\n",
            "        [ 0.1182,  0.1116,  0.1249, -0.0116,  0.0620, -0.0490, -0.0223,  0.0444],\n",
            "        [ 0.0774,  0.0711,  0.0788, -0.0083,  0.0400, -0.0289, -0.0138,  0.0211],\n",
            "        [ 0.0385,  0.0245,  0.0268, -0.0030,  0.0181, -0.0179, -0.0045,  0.0127]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018864978104829788 tensor([[ 0.1576,  0.1319,  0.1452, -0.0155,  0.0766, -0.0628, -0.0228,  0.0390],\n",
            "        [ 0.1224,  0.1128,  0.1285, -0.0141,  0.0654, -0.0450, -0.0242,  0.0357],\n",
            "        [ 0.0818,  0.0726,  0.0818, -0.0102,  0.0423, -0.0252, -0.0148,  0.0119],\n",
            "        [ 0.0484,  0.0273,  0.0300, -0.0037,  0.0220, -0.0218, -0.0048,  0.0136]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018791725859045982 tensor([[ 0.1674,  0.1332,  0.1472, -0.0177,  0.0803, -0.0622, -0.0232,  0.0293],\n",
            "        [ 0.1243,  0.1115,  0.1295, -0.0163,  0.0676, -0.0399, -0.0257,  0.0258],\n",
            "        [ 0.0848,  0.0729,  0.0834, -0.0120,  0.0440, -0.0207, -0.0155,  0.0022],\n",
            "        [ 0.0565,  0.0282,  0.0312, -0.0043,  0.0249, -0.0247, -0.0047,  0.0134]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018736133351922035 tensor([[ 0.1759,  0.1333,  0.1478, -0.0198,  0.0834, -0.0609, -0.0234,  0.0190],\n",
            "        [ 0.1264,  0.1106,  0.1308, -0.0186,  0.0700, -0.0349, -0.0272,  0.0162],\n",
            "        [ 0.0878,  0.0732,  0.0851, -0.0138,  0.0457, -0.0164, -0.0162, -0.0073],\n",
            "        [ 0.0634,  0.0280,  0.0312, -0.0048,  0.0273, -0.0269, -0.0044,  0.0124]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02477201819419861 tensor([[ 0.1054,  0.0954,  0.1057, -0.0085,  0.0535, -0.0469, -0.0187,  0.0431],\n",
            "        [ 0.0978,  0.0931,  0.1020, -0.0083,  0.0498, -0.0446, -0.0167,  0.0420],\n",
            "        [ 0.0620,  0.0525,  0.0588, -0.0055,  0.0315, -0.0279, -0.0107,  0.0260],\n",
            "        [ 0.0260,  0.0232,  0.0262, -0.0032,  0.0139, -0.0118, -0.0048,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018600787967443466 tensor([[ 0.1496,  0.1307,  0.1468, -0.0133,  0.0757, -0.0622, -0.0266,  0.0524],\n",
            "        [ 0.1251,  0.1169,  0.1287, -0.0121,  0.0638, -0.0528, -0.0218,  0.0442],\n",
            "        [ 0.0867,  0.0678,  0.0779, -0.0080,  0.0443, -0.0360, -0.0147,  0.0305],\n",
            "        [ 0.0393,  0.0336,  0.0395, -0.0057,  0.0214, -0.0164, -0.0072,  0.0112]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017963308840990067 tensor([[ 0.1688,  0.1404,  0.1599, -0.0163,  0.0851, -0.0651, -0.0301,  0.0488],\n",
            "        [ 0.1260,  0.1141,  0.1263, -0.0141,  0.0645, -0.0475, -0.0224,  0.0319],\n",
            "        [ 0.0970,  0.0685,  0.0815, -0.0095,  0.0496, -0.0364, -0.0161,  0.0269],\n",
            "        [ 0.0477,  0.0393,  0.0480, -0.0080,  0.0263, -0.0181, -0.0087,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017848439514636993 tensor([[ 0.1820,  0.1440,  0.1663, -0.0189,  0.0915, -0.0649, -0.0325,  0.0420],\n",
            "        [ 0.1265,  0.1107,  0.1236, -0.0160,  0.0649, -0.0419, -0.0229,  0.0195],\n",
            "        [ 0.1050,  0.0670,  0.0826, -0.0107,  0.0537, -0.0356, -0.0170,  0.0221],\n",
            "        [ 0.0538,  0.0429,  0.0541, -0.0101,  0.0301, -0.0186, -0.0099,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017778582870960236 tensor([[ 0.1927,  0.1449,  0.1698, -0.0213,  0.0964, -0.0635, -0.0345,  0.0339],\n",
            "        [ 0.1282,  0.1087,  0.1223, -0.0180,  0.0661, -0.0370, -0.0236,  0.0079],\n",
            "        [ 0.1122,  0.0647,  0.0829, -0.0119,  0.0573, -0.0345, -0.0178,  0.0169],\n",
            "        [ 0.0585,  0.0452,  0.0588, -0.0121,  0.0331, -0.0184, -0.0107,  0.0036]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02716992050409317 tensor([[ 0.1168,  0.1080,  0.1186, -0.0093,  0.0586, -0.0522, -0.0197,  0.0475],\n",
            "        [ 0.0972,  0.0915,  0.0999, -0.0071,  0.0492, -0.0442, -0.0171,  0.0418],\n",
            "        [ 0.0716,  0.0678,  0.0757, -0.0065,  0.0373, -0.0324, -0.0117,  0.0286],\n",
            "        [ 0.0300,  0.0254,  0.0286, -0.0032,  0.0160, -0.0149, -0.0057,  0.0140]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019477708265185356 tensor([[ 0.1698,  0.1540,  0.1708, -0.0150,  0.0845, -0.0711, -0.0281,  0.0583],\n",
            "        [ 0.1376,  0.1273,  0.1392, -0.0106,  0.0699, -0.0583, -0.0248,  0.0505],\n",
            "        [ 0.0997,  0.0921,  0.1048, -0.0102,  0.0527, -0.0416, -0.0160,  0.0312],\n",
            "        [ 0.0465,  0.0362,  0.0421, -0.0054,  0.0251, -0.0221, -0.0091,  0.0199]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018412552773952484 tensor([[ 0.1872,  0.1636,  0.1832, -0.0182,  0.0922, -0.0723, -0.0302,  0.0508],\n",
            "        [ 0.1462,  0.1302,  0.1430, -0.0120,  0.0741, -0.0560, -0.0270,  0.0415],\n",
            "        [ 0.1064,  0.0947,  0.1107, -0.0124,  0.0571, -0.0394, -0.0166,  0.0218],\n",
            "        [ 0.0568,  0.0406,  0.0492, -0.0073,  0.0310, -0.0257, -0.0113,  0.0221]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018300559371709824 tensor([[ 0.1976,  0.1659,  0.1876, -0.0210,  0.0963, -0.0701, -0.0311,  0.0397],\n",
            "        [ 0.1507,  0.1287,  0.1421, -0.0131,  0.0763, -0.0516, -0.0285,  0.0304],\n",
            "        [ 0.1100,  0.0942,  0.1133, -0.0144,  0.0599, -0.0357, -0.0167,  0.0107],\n",
            "        [ 0.0645,  0.0425,  0.0537, -0.0091,  0.0355, -0.0278, -0.0130,  0.0228]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018231065943837166 tensor([[ 0.2063,  0.1664,  0.1900, -0.0236,  0.0995, -0.0670, -0.0318,  0.0278],\n",
            "        [ 0.1552,  0.1274,  0.1413, -0.0141,  0.0785, -0.0472, -0.0299,  0.0195],\n",
            "        [ 0.1133,  0.0934,  0.1156, -0.0163,  0.0626, -0.0319, -0.0168, -0.0004],\n",
            "        [ 0.0706,  0.0430,  0.0566, -0.0107,  0.0393, -0.0291, -0.0144,  0.0225]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026888806372880936 tensor([[ 0.1175,  0.1093,  0.1184, -0.0084,  0.0588, -0.0530, -0.0205,  0.0481],\n",
            "        [ 0.0981,  0.0893,  0.0980, -0.0078,  0.0496, -0.0435, -0.0162,  0.0408],\n",
            "        [ 0.0687,  0.0620,  0.0692, -0.0064,  0.0351, -0.0307, -0.0114,  0.0288],\n",
            "        [ 0.0298,  0.0259,  0.0278, -0.0032,  0.0154, -0.0143, -0.0054,  0.0132]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019390355795621872 tensor([[ 0.1640,  0.1485,  0.1615, -0.0126,  0.0810, -0.0692, -0.0284,  0.0556],\n",
            "        [ 0.1441,  0.1287,  0.1426, -0.0127,  0.0738, -0.0587, -0.0239,  0.0501],\n",
            "        [ 0.1032,  0.0901,  0.1024, -0.0107,  0.0534, -0.0432, -0.0173,  0.0370],\n",
            "        [ 0.0483,  0.0396,  0.0422, -0.0052,  0.0249, -0.0224, -0.0088,  0.0194]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018284542486071587 tensor([[ 0.1762,  0.1521,  0.1657, -0.0144,  0.0855, -0.0681, -0.0301,  0.0455],\n",
            "        [ 0.1576,  0.1348,  0.1516, -0.0157,  0.0812, -0.0568, -0.0261,  0.0410],\n",
            "        [ 0.1178,  0.0983,  0.1145, -0.0136,  0.0615, -0.0451, -0.0196,  0.0336],\n",
            "        [ 0.0606,  0.0471,  0.0499, -0.0068,  0.0312, -0.0270, -0.0110,  0.0217]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018153585493564606 tensor([[ 0.1837,  0.1509,  0.1647, -0.0158,  0.0876, -0.0649, -0.0311,  0.0332],\n",
            "        [ 0.1650,  0.1347,  0.1538, -0.0181,  0.0855, -0.0519, -0.0271,  0.0287],\n",
            "        [ 0.1265,  0.1005,  0.1201, -0.0162,  0.0665, -0.0437, -0.0210,  0.0268],\n",
            "        [ 0.0699,  0.0517,  0.0545, -0.0081,  0.0359, -0.0299, -0.0127,  0.0221]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018070844933390617 tensor([[ 0.1912,  0.1495,  0.1635, -0.0172,  0.0897, -0.0616, -0.0320,  0.0211],\n",
            "        [ 0.1714,  0.1336,  0.1550, -0.0205,  0.0894, -0.0466, -0.0280,  0.0162],\n",
            "        [ 0.1330,  0.1006,  0.1234, -0.0186,  0.0703, -0.0412, -0.0219,  0.0189],\n",
            "        [ 0.0775,  0.0546,  0.0572, -0.0093,  0.0397, -0.0318, -0.0141,  0.0214]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02594958245754242 tensor([[ 0.1212,  0.1154,  0.1266, -0.0101,  0.0616, -0.0536, -0.0202,  0.0474],\n",
            "        [ 0.1042,  0.0969,  0.1060, -0.0077,  0.0526, -0.0472, -0.0177,  0.0441],\n",
            "        [ 0.0739,  0.0680,  0.0757, -0.0069,  0.0382, -0.0337, -0.0126,  0.0311],\n",
            "        [ 0.0293,  0.0253,  0.0277, -0.0032,  0.0158, -0.0147, -0.0057,  0.0134]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017664358019828796 tensor([[ 0.1643,  0.1543,  0.1714, -0.0157,  0.0831, -0.0669, -0.0267,  0.0514],\n",
            "        [ 0.1441,  0.1311,  0.1437, -0.0110,  0.0726, -0.0604, -0.0244,  0.0504],\n",
            "        [ 0.1107,  0.0990,  0.1116, -0.0109,  0.0574, -0.0473, -0.0188,  0.0396],\n",
            "        [ 0.0462,  0.0371,  0.0409, -0.0052,  0.0254, -0.0225, -0.0093,  0.0192]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01670125685632229 tensor([[ 0.1740,  0.1588,  0.1786, -0.0189,  0.0876, -0.0637, -0.0274,  0.0386],\n",
            "        [ 0.1522,  0.1325,  0.1461, -0.0121,  0.0761, -0.0571, -0.0255,  0.0392],\n",
            "        [ 0.1246,  0.1067,  0.1226, -0.0133,  0.0645, -0.0487, -0.0209,  0.0349],\n",
            "        [ 0.0567,  0.0427,  0.0476, -0.0066,  0.0318, -0.0267, -0.0117,  0.0210]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016587860882282257 tensor([[ 0.1801,  0.1597,  0.1819, -0.0219,  0.0903, -0.0589, -0.0275,  0.0242],\n",
            "        [ 0.1573,  0.1308,  0.1450, -0.0130,  0.0780, -0.0522, -0.0260,  0.0265],\n",
            "        [ 0.1329,  0.1087,  0.1274, -0.0153,  0.0686, -0.0472, -0.0219,  0.0271],\n",
            "        [ 0.0647,  0.0457,  0.0516, -0.0079,  0.0368, -0.0295, -0.0137,  0.0212]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016507938504219055 tensor([[ 0.1860,  0.1603,  0.1848, -0.0249,  0.0929, -0.0540, -0.0276,  0.0099],\n",
            "        [ 0.1625,  0.1293,  0.1441, -0.0138,  0.0800, -0.0474, -0.0267,  0.0141],\n",
            "        [ 0.1393,  0.1088,  0.1300, -0.0172,  0.0718, -0.0448, -0.0227,  0.0184],\n",
            "        [ 0.0711,  0.0472,  0.0540, -0.0091,  0.0411, -0.0314, -0.0153,  0.0204]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.027872281149029732 tensor([[ 0.1145,  0.1095,  0.1210, -0.0099,  0.0589, -0.0485, -0.0204,  0.0443],\n",
            "        [ 0.1049,  0.0985,  0.1067, -0.0081,  0.0529, -0.0491, -0.0178,  0.0441],\n",
            "        [ 0.0705,  0.0623,  0.0689, -0.0063,  0.0360, -0.0315, -0.0120,  0.0294],\n",
            "        [ 0.0307,  0.0272,  0.0295, -0.0035,  0.0162, -0.0142, -0.0058,  0.0129]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020081013441085815 tensor([[ 0.1571,  0.1483,  0.1664, -0.0157,  0.0806, -0.0597, -0.0281,  0.0481],\n",
            "        [ 0.1462,  0.1353,  0.1466, -0.0123,  0.0739, -0.0645, -0.0250,  0.0509],\n",
            "        [ 0.1061,  0.0893,  0.0997, -0.0101,  0.0544, -0.0441, -0.0183,  0.0375],\n",
            "        [ 0.0492,  0.0417,  0.0450, -0.0060,  0.0260, -0.0217, -0.0094,  0.0183]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01908797398209572 tensor([[ 0.1679,  0.1541,  0.1759, -0.0192,  0.0860, -0.0551, -0.0302,  0.0359],\n",
            "        [ 0.1538,  0.1376,  0.1494, -0.0143,  0.0776, -0.0627, -0.0265,  0.0393],\n",
            "        [ 0.1228,  0.0972,  0.1100, -0.0125,  0.0632, -0.0465, -0.0213,  0.0344],\n",
            "        [ 0.0614,  0.0501,  0.0538, -0.0079,  0.0325, -0.0255, -0.0118,  0.0197]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018950816243886948 tensor([[ 0.1743,  0.1555,  0.1803, -0.0225,  0.0891, -0.0485, -0.0315,  0.0217],\n",
            "        [ 0.1576,  0.1360,  0.1479, -0.0161,  0.0794, -0.0588, -0.0273,  0.0257],\n",
            "        [ 0.1342,  0.0997,  0.1146, -0.0146,  0.0692, -0.0460, -0.0233,  0.0283],\n",
            "        [ 0.0706,  0.0554,  0.0595, -0.0097,  0.0374, -0.0276, -0.0136,  0.0191]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01885949820280075 tensor([[ 0.1802,  0.1563,  0.1842, -0.0256,  0.0920, -0.0416, -0.0327,  0.0074],\n",
            "        [ 0.1616,  0.1346,  0.1466, -0.0178,  0.0812, -0.0552, -0.0282,  0.0126],\n",
            "        [ 0.1434,  0.1001,  0.1168, -0.0165,  0.0740, -0.0444, -0.0249,  0.0211],\n",
            "        [ 0.0779,  0.0590,  0.0632, -0.0114,  0.0414, -0.0286, -0.0151,  0.0174]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025047605857253075 tensor([[ 0.1065,  0.1003,  0.1118, -0.0091,  0.0539, -0.0472, -0.0177,  0.0437],\n",
            "        [ 0.0902,  0.0850,  0.0922, -0.0083,  0.0463, -0.0415, -0.0160,  0.0384],\n",
            "        [ 0.0685,  0.0616,  0.0673, -0.0056,  0.0352, -0.0303, -0.0120,  0.0290],\n",
            "        [ 0.0250,  0.0214,  0.0232, -0.0027,  0.0130, -0.0119, -0.0045,  0.0103]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019158581271767616 tensor([[ 0.1335,  0.1214,  0.1378, -0.0131,  0.0671, -0.0545, -0.0217,  0.0449],\n",
            "        [ 0.1152,  0.1053,  0.1147, -0.0129,  0.0600, -0.0483, -0.0212,  0.0389],\n",
            "        [ 0.0971,  0.0830,  0.0912, -0.0085,  0.0499, -0.0394, -0.0171,  0.0349],\n",
            "        [ 0.0380,  0.0307,  0.0333, -0.0044,  0.0199, -0.0171, -0.0067,  0.0133]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018760859966278076 tensor([[ 0.1396,  0.1210,  0.1402, -0.0156,  0.0697, -0.0517, -0.0221,  0.0359],\n",
            "        [ 0.1199,  0.1048,  0.1147, -0.0160,  0.0633, -0.0443, -0.0231,  0.0283],\n",
            "        [ 0.1101,  0.0887,  0.0981, -0.0103,  0.0566, -0.0402, -0.0195,  0.0318],\n",
            "        [ 0.0464,  0.0356,  0.0387, -0.0057,  0.0244, -0.0197, -0.0081,  0.0134]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018688742071390152 tensor([[ 0.1448,  0.1196,  0.1414, -0.0180,  0.0718, -0.0485, -0.0223,  0.0264],\n",
            "        [ 0.1234,  0.1031,  0.1133, -0.0190,  0.0660, -0.0399, -0.0247,  0.0172],\n",
            "        [ 0.1194,  0.0906,  0.1010, -0.0118,  0.0613, -0.0391, -0.0211,  0.0267],\n",
            "        [ 0.0527,  0.0385,  0.0420, -0.0068,  0.0278, -0.0211, -0.0091,  0.0122]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018634993582963943 tensor([[ 0.1500,  0.1184,  0.1428, -0.0204,  0.0739, -0.0453, -0.0225,  0.0171],\n",
            "        [ 0.1272,  0.1017,  0.1123, -0.0220,  0.0688, -0.0356, -0.0263,  0.0064],\n",
            "        [ 0.1271,  0.0910,  0.1021, -0.0133,  0.0651, -0.0371, -0.0225,  0.0207],\n",
            "        [ 0.0578,  0.0402,  0.0441, -0.0079,  0.0306, -0.0218, -0.0098,  0.0103]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02310863323509693 tensor([[ 0.1068,  0.1010,  0.1117, -0.0085,  0.0546, -0.0470, -0.0184,  0.0437],\n",
            "        [ 0.0797,  0.0725,  0.0793, -0.0064,  0.0402, -0.0369, -0.0137,  0.0331],\n",
            "        [ 0.0667,  0.0634,  0.0689, -0.0058,  0.0329, -0.0294, -0.0106,  0.0275],\n",
            "        [ 0.0157,  0.0136,  0.0144, -0.0028,  0.0081, -0.0080, -0.0039,  0.0087]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017643075436353683 tensor([[ 0.1423,  0.1311,  0.1472, -0.0126,  0.0726, -0.0576, -0.0244,  0.0484],\n",
            "        [ 0.1018,  0.0887,  0.0980, -0.0089,  0.0517, -0.0433, -0.0176,  0.0329],\n",
            "        [ 0.0946,  0.0890,  0.0965, -0.0090,  0.0463, -0.0383, -0.0146,  0.0322],\n",
            "        [ 0.0239,  0.0193,  0.0198, -0.0048,  0.0119, -0.0122, -0.0064,  0.0134]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017145823687314987 tensor([[ 0.1544,  0.1371,  0.1565, -0.0151,  0.0788, -0.0566, -0.0263,  0.0410],\n",
            "        [ 0.1048,  0.0856,  0.0960, -0.0099,  0.0533, -0.0398, -0.0181,  0.0219],\n",
            "        [ 0.1062,  0.0981,  0.1066, -0.0110,  0.0513, -0.0385, -0.0159,  0.0277],\n",
            "        [ 0.0291,  0.0220,  0.0220, -0.0065,  0.0139, -0.0147, -0.0083,  0.0164]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01706199161708355 tensor([[ 0.1622,  0.1386,  0.1608, -0.0173,  0.0827, -0.0534, -0.0274,  0.0314],\n",
            "        [ 0.1066,  0.0814,  0.0927, -0.0108,  0.0545, -0.0356, -0.0184,  0.0106],\n",
            "        [ 0.1138,  0.1033,  0.1123, -0.0128,  0.0543, -0.0367, -0.0164,  0.0211],\n",
            "        [ 0.0328,  0.0234,  0.0226, -0.0082,  0.0150, -0.0165, -0.0098,  0.0185]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01700615882873535 tensor([[ 1.6878e-01,  1.3893e-01,  1.6388e-01, -1.9467e-02,  8.5986e-02,\n",
            "         -4.9722e-02, -2.8315e-02,  2.1262e-02],\n",
            "        [ 1.0934e-01,  7.8099e-02,  9.0494e-02, -1.1704e-02,  5.6130e-02,\n",
            "         -3.2041e-02, -1.8859e-02, -6.6757e-05],\n",
            "        [ 1.1988e-01,  1.0683e-01,  1.1628e-01, -1.4479e-02,  5.6427e-02,\n",
            "         -3.4230e-02, -1.6695e-02,  1.3731e-02],\n",
            "        [ 3.5594e-02,  2.3991e-02,  2.2424e-02, -9.7764e-03,  1.5734e-02,\n",
            "         -1.7852e-02, -1.1264e-02,  2.0133e-02]], device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024084221571683884 tensor([[ 0.1057,  0.0977,  0.1075, -0.0082,  0.0536, -0.0477, -0.0189,  0.0439],\n",
            "        [ 0.0781,  0.0746,  0.0820, -0.0073,  0.0404, -0.0359, -0.0138,  0.0327],\n",
            "        [ 0.0569,  0.0501,  0.0562, -0.0050,  0.0290, -0.0264, -0.0098,  0.0257],\n",
            "        [ 0.0120,  0.0107,  0.0090, -0.0015,  0.0045, -0.0076, -0.0030,  0.0067]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019425006583333015 tensor([[ 0.1391,  0.1233,  0.1371, -0.0119,  0.0704, -0.0583, -0.0248,  0.0479],\n",
            "        [ 0.0903,  0.0842,  0.0943, -0.0104,  0.0478, -0.0374, -0.0165,  0.0280],\n",
            "        [ 0.0739,  0.0603,  0.0694, -0.0072,  0.0377, -0.0323, -0.0126,  0.0294],\n",
            "        [ 0.0194,  0.0172,  0.0131, -0.0025,  0.0054, -0.0128, -0.0046,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01912667602300644 tensor([[ 0.1548,  0.1304,  0.1467, -0.0143,  0.0780, -0.0599, -0.0275,  0.0424],\n",
            "        [ 0.0904,  0.0815,  0.0934, -0.0128,  0.0490, -0.0324, -0.0172,  0.0166],\n",
            "        [ 0.0824,  0.0620,  0.0734, -0.0088,  0.0419, -0.0338, -0.0139,  0.0284],\n",
            "        [ 0.0245,  0.0215,  0.0151, -0.0033,  0.0049, -0.0166, -0.0056,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019052021205425262 tensor([[ 0.1657,  0.1327,  0.1509, -0.0165,  0.0831, -0.0591, -0.0293,  0.0343],\n",
            "        [ 0.0903,  0.0786,  0.0923, -0.0151,  0.0502, -0.0273, -0.0178,  0.0052],\n",
            "        [ 0.0890,  0.0617,  0.0754, -0.0102,  0.0452, -0.0343, -0.0149,  0.0264],\n",
            "        [ 0.0281,  0.0246,  0.0156, -0.0040,  0.0036, -0.0196, -0.0064,  0.0144]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019004253670573235 tensor([[ 0.1746,  0.1330,  0.1530, -0.0185,  0.0873, -0.0574, -0.0308,  0.0252],\n",
            "        [ 0.0912,  0.0767,  0.0925, -0.0174,  0.0520, -0.0228, -0.0185, -0.0054],\n",
            "        [ 0.0948,  0.0606,  0.0764, -0.0116,  0.0480, -0.0344, -0.0157,  0.0240],\n",
            "        [ 0.0308,  0.0268,  0.0152, -0.0047,  0.0019, -0.0222, -0.0070,  0.0153]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023498205468058586 tensor([[ 0.0867,  0.0718,  0.0799, -0.0063,  0.0449, -0.0362, -0.0162,  0.0333],\n",
            "        [ 0.0960,  0.0912,  0.0998, -0.0072,  0.0475, -0.0423, -0.0151,  0.0392],\n",
            "        [ 0.0617,  0.0526,  0.0583, -0.0056,  0.0312, -0.0286, -0.0108,  0.0269],\n",
            "        [ 0.0227,  0.0195,  0.0211, -0.0025,  0.0110, -0.0106, -0.0034,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01871984452009201 tensor([[ 0.1109,  0.0814,  0.0934, -0.0087,  0.0579, -0.0398, -0.0210,  0.0313],\n",
            "        [ 0.1286,  0.1211,  0.1337, -0.0106,  0.0633, -0.0513, -0.0194,  0.0422],\n",
            "        [ 0.0883,  0.0704,  0.0792, -0.0085,  0.0447, -0.0393, -0.0156,  0.0337],\n",
            "        [ 0.0348,  0.0286,  0.0307, -0.0041,  0.0164, -0.0153, -0.0046,  0.0106]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018285900354385376 tensor([[ 0.1204,  0.0759,  0.0906, -0.0100,  0.0636, -0.0361, -0.0231,  0.0219],\n",
            "        [ 0.1385,  0.1279,  0.1426, -0.0125,  0.0674, -0.0488, -0.0198,  0.0330],\n",
            "        [ 0.1008,  0.0742,  0.0848, -0.0104,  0.0508, -0.0425, -0.0177,  0.0323],\n",
            "        [ 0.0425,  0.0335,  0.0361, -0.0053,  0.0194, -0.0176, -0.0049,  0.0098]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01819860190153122 tensor([[ 0.1289,  0.0697,  0.0868, -0.0113,  0.0688, -0.0320, -0.0250,  0.0122],\n",
            "        [ 0.1443,  0.1304,  0.1468, -0.0141,  0.0694, -0.0443, -0.0195,  0.0217],\n",
            "        [ 0.1092,  0.0739,  0.0861, -0.0119,  0.0549, -0.0436, -0.0191,  0.0286],\n",
            "        [ 0.0482,  0.0365,  0.0393, -0.0065,  0.0214, -0.0187, -0.0048,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018135903403162956 tensor([[ 0.1380,  0.0641,  0.0838, -0.0125,  0.0744, -0.0283, -0.0270,  0.0031],\n",
            "        [ 0.1491,  0.1319,  0.1499, -0.0156,  0.0710, -0.0393, -0.0191,  0.0102],\n",
            "        [ 0.1161,  0.0721,  0.0858, -0.0134,  0.0581, -0.0438, -0.0203,  0.0240],\n",
            "        [ 0.0527,  0.0384,  0.0414, -0.0075,  0.0228, -0.0192, -0.0046,  0.0051]],\n",
            "       device='cuda:0')\n",
            "c tensor([3.5763e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019544728100299835 tensor([[ 0.0907,  0.0899,  0.1010, -0.0088,  0.0455, -0.0384, -0.0155,  0.0345],\n",
            "        [ 0.0742,  0.0668,  0.0727, -0.0059,  0.0386, -0.0332, -0.0138,  0.0300],\n",
            "        [ 0.0657,  0.0563,  0.0615, -0.0049,  0.0333, -0.0295, -0.0106,  0.0278],\n",
            "        [ 0.0265,  0.0192,  0.0221, -0.0027,  0.0133, -0.0126, -0.0046,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016264427453279495 tensor([[ 0.0835,  0.0800,  0.0930, -0.0110,  0.0410, -0.0283, -0.0140,  0.0183],\n",
            "        [ 0.0693,  0.0527,  0.0589, -0.0067,  0.0370, -0.0245, -0.0140,  0.0152],\n",
            "        [ 0.0892,  0.0707,  0.0779, -0.0067,  0.0455, -0.0366, -0.0138,  0.0312],\n",
            "        [ 0.0431,  0.0285,  0.0339, -0.0045,  0.0216, -0.0196, -0.0074,  0.0177]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016019493341445923 tensor([[ 0.0864,  0.0806,  0.0965, -0.0140,  0.0416, -0.0233, -0.0143,  0.0076],\n",
            "        [ 0.0729,  0.0473,  0.0543, -0.0080,  0.0397, -0.0203, -0.0156,  0.0057],\n",
            "        [ 0.1052,  0.0776,  0.0862, -0.0081,  0.0537, -0.0399, -0.0157,  0.0304],\n",
            "        [ 0.0555,  0.0338,  0.0414, -0.0060,  0.0278, -0.0243, -0.0094,  0.0209]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015922170132398605 tensor([[ 0.0893,  0.0812,  0.1001, -0.0169,  0.0423, -0.0183, -0.0146, -0.0029],\n",
            "        [ 0.0773,  0.0427,  0.0506, -0.0092,  0.0429, -0.0166, -0.0173, -0.0030],\n",
            "        [ 0.1162,  0.0796,  0.0891, -0.0091,  0.0593, -0.0406, -0.0168,  0.0269],\n",
            "        [ 0.0651,  0.0363,  0.0459, -0.0073,  0.0326, -0.0275, -0.0108,  0.0223]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015861326828598976 tensor([[ 0.0924,  0.0821,  0.1038, -0.0198,  0.0431, -0.0136, -0.0149, -0.0132],\n",
            "        [ 0.0825,  0.0391,  0.0479, -0.0105,  0.0466, -0.0135, -0.0192, -0.0110],\n",
            "        [ 0.1248,  0.0793,  0.0894, -0.0101,  0.0637, -0.0402, -0.0175,  0.0222],\n",
            "        [ 0.0729,  0.0372,  0.0486, -0.0085,  0.0364, -0.0296, -0.0120,  0.0225]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020980354398489 tensor([[ 0.1008,  0.0962,  0.1061, -0.0086,  0.0492, -0.0448, -0.0156,  0.0405],\n",
            "        [ 0.0844,  0.0760,  0.0832, -0.0075,  0.0427, -0.0376, -0.0146,  0.0337],\n",
            "        [ 0.0540,  0.0489,  0.0557, -0.0052,  0.0276, -0.0235, -0.0095,  0.0230],\n",
            "        [ 0.0274,  0.0222,  0.0227, -0.0024,  0.0136, -0.0129, -0.0050,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01670631766319275 tensor([[ 0.1090,  0.0996,  0.1112, -0.0109,  0.0515, -0.0437, -0.0155,  0.0325],\n",
            "        [ 0.1015,  0.0849,  0.0939, -0.0104,  0.0515, -0.0400, -0.0174,  0.0290],\n",
            "        [ 0.0683,  0.0584,  0.0694, -0.0079,  0.0352, -0.0258, -0.0122,  0.0237],\n",
            "        [ 0.0440,  0.0336,  0.0336, -0.0038,  0.0213, -0.0197, -0.0079,  0.0171]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016555748879909515 tensor([[ 0.1116,  0.0973,  0.1100, -0.0128,  0.0510, -0.0399, -0.0146,  0.0221],\n",
            "        [ 0.1098,  0.0848,  0.0949, -0.0127,  0.0557, -0.0378, -0.0187,  0.0196],\n",
            "        [ 0.0747,  0.0599,  0.0748, -0.0101,  0.0387, -0.0240, -0.0135,  0.0200],\n",
            "        [ 0.0559,  0.0405,  0.0396, -0.0049,  0.0266, -0.0239, -0.0099,  0.0194]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01649206317961216 tensor([[ 0.1155,  0.0963,  0.1101, -0.0148,  0.0512, -0.0367, -0.0138,  0.0125],\n",
            "        [ 0.1171,  0.0838,  0.0948, -0.0149,  0.0594, -0.0352, -0.0198,  0.0098],\n",
            "        [ 0.0790,  0.0594,  0.0778, -0.0122,  0.0412, -0.0212, -0.0144,  0.0151],\n",
            "        [ 0.0651,  0.0447,  0.0427, -0.0059,  0.0305, -0.0265, -0.0114,  0.0200]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016443127766251564 tensor([[ 1.1989e-01,  9.5855e-02,  1.1089e-01, -1.6723e-02,  5.1666e-02,\n",
            "         -3.3857e-02, -1.3141e-02,  3.2663e-03],\n",
            "        [ 1.2396e-01,  8.2412e-02,  9.4335e-02, -1.7034e-02,  6.2878e-02,\n",
            "         -3.2494e-02, -2.0895e-02,  4.2915e-05],\n",
            "        [ 8.2440e-02,  5.8036e-02,  7.9950e-02, -1.4187e-02,  4.3274e-02,\n",
            "         -1.7871e-02, -1.5201e-02,  9.9140e-03],\n",
            "        [ 7.2536e-02,  4.7178e-02,  4.3977e-02, -6.7103e-03,  3.3505e-02,\n",
            "         -2.8242e-02, -1.2581e-02,  1.9524e-02]], device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021025145426392555 tensor([[ 0.1100,  0.1021,  0.1127, -0.0092,  0.0554, -0.0490, -0.0181,  0.0439],\n",
            "        [ 0.0864,  0.0783,  0.0866, -0.0069,  0.0434, -0.0401, -0.0155,  0.0384],\n",
            "        [ 0.0624,  0.0597,  0.0668, -0.0055,  0.0315, -0.0266, -0.0100,  0.0229],\n",
            "        [ 0.0240,  0.0186,  0.0209, -0.0026,  0.0129, -0.0116, -0.0046,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01529655046761036 tensor([[ 0.1452,  0.1304,  0.1458, -0.0138,  0.0727, -0.0595, -0.0232,  0.0467],\n",
            "        [ 0.1155,  0.1011,  0.1134, -0.0102,  0.0586, -0.0505, -0.0214,  0.0449],\n",
            "        [ 0.0778,  0.0730,  0.0835, -0.0075,  0.0388, -0.0287, -0.0118,  0.0192],\n",
            "        [ 0.0364,  0.0252,  0.0294, -0.0042,  0.0196, -0.0168, -0.0070,  0.0129]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01485422346740961 tensor([[ 0.1554,  0.1331,  0.1507, -0.0165,  0.0773, -0.0576, -0.0240,  0.0367],\n",
            "        [ 0.1252,  0.1040,  0.1188, -0.0120,  0.0638, -0.0508, -0.0240,  0.0405],\n",
            "        [ 0.0808,  0.0739,  0.0871, -0.0086,  0.0397, -0.0243, -0.0113,  0.0087],\n",
            "        [ 0.0447,  0.0279,  0.0337, -0.0055,  0.0240, -0.0196, -0.0085,  0.0135]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014789009466767311 tensor([[ 0.1622,  0.1323,  0.1518, -0.0189,  0.0802, -0.0541, -0.0243,  0.0252],\n",
            "        [ 0.1316,  0.1037,  0.1207, -0.0137,  0.0673, -0.0494, -0.0259,  0.0344],\n",
            "        [ 0.0829,  0.0739,  0.0897, -0.0096,  0.0401, -0.0195, -0.0107, -0.0022],\n",
            "        [ 0.0511,  0.0287,  0.0360, -0.0067,  0.0275, -0.0215, -0.0097,  0.0128]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014736167155206203 tensor([[ 0.1686,  0.1310,  0.1524, -0.0212,  0.0828, -0.0504, -0.0245,  0.0136],\n",
            "        [ 0.1372,  0.1026,  0.1218, -0.0152,  0.0704, -0.0476, -0.0277,  0.0279],\n",
            "        [ 0.0851,  0.0741,  0.0925, -0.0106,  0.0406, -0.0149, -0.0101, -0.0129],\n",
            "        [ 0.0563,  0.0285,  0.0372, -0.0078,  0.0303, -0.0227, -0.0107,  0.0115]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02292357012629509 tensor([[ 0.0832,  0.0748,  0.0819, -0.0060,  0.0402, -0.0364, -0.0133,  0.0313],\n",
            "        [ 0.0881,  0.0868,  0.0944, -0.0075,  0.0446, -0.0391, -0.0155,  0.0373],\n",
            "        [ 0.0624,  0.0559,  0.0631, -0.0058,  0.0321, -0.0275, -0.0107,  0.0259],\n",
            "        [ 0.0187,  0.0110,  0.0143, -0.0021,  0.0097, -0.0084, -0.0024,  0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019003398716449738 tensor([[ 0.1035,  0.0871,  0.0961, -0.0079,  0.0488, -0.0410, -0.0158,  0.0293],\n",
            "        [ 0.0987,  0.0950,  0.1040, -0.0103,  0.0497, -0.0383, -0.0177,  0.0322],\n",
            "        [ 0.0747,  0.0608,  0.0714, -0.0079,  0.0383, -0.0288, -0.0127,  0.0235],\n",
            "        [ 0.0280,  0.0119,  0.0185, -0.0034,  0.0141, -0.0112, -0.0027,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018889829516410828 tensor([[ 0.1143,  0.0900,  0.0999, -0.0091,  0.0525, -0.0409, -0.0167,  0.0224],\n",
            "        [ 0.1005,  0.0940,  0.1039, -0.0126,  0.0503, -0.0329, -0.0184,  0.0224],\n",
            "        [ 0.0806,  0.0593,  0.0726, -0.0095,  0.0410, -0.0267, -0.0136,  0.0176],\n",
            "        [ 0.0352,  0.0109,  0.0206, -0.0046,  0.0175, -0.0128, -0.0026,  0.0076]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01884254813194275 tensor([[ 0.1232,  0.0909,  0.1016, -0.0101,  0.0553, -0.0399, -0.0173,  0.0146],\n",
            "        [ 0.1024,  0.0931,  0.1040, -0.0149,  0.0510, -0.0276, -0.0191,  0.0128],\n",
            "        [ 0.0858,  0.0571,  0.0732, -0.0111,  0.0435, -0.0244, -0.0143,  0.0114],\n",
            "        [ 0.0414,  0.0089,  0.0216, -0.0057,  0.0204, -0.0139, -0.0024,  0.0067]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01880078949034214 tensor([[ 0.1312,  0.0910,  0.1024, -0.0111,  0.0577, -0.0385, -0.0177,  0.0064],\n",
            "        [ 0.1048,  0.0928,  0.1046, -0.0172,  0.0520, -0.0227, -0.0199,  0.0036],\n",
            "        [ 0.0908,  0.0549,  0.0737, -0.0127,  0.0458, -0.0220, -0.0150,  0.0053],\n",
            "        [ 0.0470,  0.0063,  0.0220, -0.0067,  0.0229, -0.0147, -0.0020,  0.0056]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021843846887350082 tensor([[ 0.0891,  0.0855,  0.0954, -0.0072,  0.0419, -0.0370, -0.0128,  0.0310],\n",
            "        [ 0.0786,  0.0734,  0.0816, -0.0066,  0.0405, -0.0335, -0.0141,  0.0308],\n",
            "        [ 0.0650,  0.0579,  0.0633, -0.0062,  0.0330, -0.0311, -0.0111,  0.0276],\n",
            "        [ 0.0290,  0.0195,  0.0214, -0.0023,  0.0152, -0.0137, -0.0054,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01876945234835148 tensor([[ 0.0812,  0.0734,  0.0835, -0.0074,  0.0347, -0.0263, -0.0094,  0.0128],\n",
            "        [ 0.0712,  0.0594,  0.0695, -0.0076,  0.0375, -0.0231, -0.0132,  0.0148],\n",
            "        [ 0.0733,  0.0599,  0.0662, -0.0086,  0.0367, -0.0332, -0.0120,  0.0239],\n",
            "        [ 0.0447,  0.0247,  0.0281, -0.0036,  0.0238, -0.0193, -0.0086,  0.0152]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01862119697034359 tensor([[ 0.0872,  0.0756,  0.0876, -0.0086,  0.0347, -0.0225, -0.0083,  0.0019],\n",
            "        [ 0.0744,  0.0563,  0.0691, -0.0092,  0.0399, -0.0184, -0.0143,  0.0051],\n",
            "        [ 0.0800,  0.0602,  0.0671, -0.0109,  0.0395, -0.0345, -0.0128,  0.0195],\n",
            "        [ 0.0574,  0.0269,  0.0315, -0.0047,  0.0308, -0.0231, -0.0113,  0.0165]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01855935901403427 tensor([[ 0.0924,  0.0768,  0.0906, -0.0098,  0.0342, -0.0182, -0.0070, -0.0094],\n",
            "        [ 0.0781,  0.0537,  0.0692, -0.0108,  0.0426, -0.0140, -0.0154, -0.0040],\n",
            "        [ 0.0846,  0.0585,  0.0656, -0.0130,  0.0414, -0.0348, -0.0133,  0.0142],\n",
            "        [ 0.0678,  0.0269,  0.0327, -0.0057,  0.0367, -0.0258, -0.0136,  0.0163]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01850610226392746 tensor([[ 0.0974,  0.0780,  0.0935, -0.0109,  0.0337, -0.0140, -0.0057, -0.0206],\n",
            "        [ 0.0823,  0.0517,  0.0700, -0.0125,  0.0456, -0.0100, -0.0166, -0.0127],\n",
            "        [ 0.0890,  0.0565,  0.0639, -0.0151,  0.0430, -0.0351, -0.0137,  0.0089],\n",
            "        [ 0.0770,  0.0257,  0.0324, -0.0066,  0.0419, -0.0277, -0.0156,  0.0153]],\n",
            "       device='cuda:0')\n",
            "c tensor([4.1723e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020548518747091293 tensor([[ 0.0954,  0.0883,  0.0980, -0.0078,  0.0477, -0.0438, -0.0161,  0.0394],\n",
            "        [ 0.0886,  0.0846,  0.0936, -0.0070,  0.0455, -0.0388, -0.0158,  0.0367],\n",
            "        [ 0.0556,  0.0521,  0.0574, -0.0051,  0.0285, -0.0253, -0.0105,  0.0230],\n",
            "        [ 0.0216,  0.0156,  0.0180, -0.0023,  0.0108, -0.0093, -0.0034,  0.0063]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015997137874364853 tensor([[ 0.1171,  0.1038,  0.1167, -0.0109,  0.0579, -0.0504, -0.0194,  0.0395],\n",
            "        [ 0.1108,  0.1029,  0.1159, -0.0099,  0.0574, -0.0430, -0.0201,  0.0361],\n",
            "        [ 0.0626,  0.0557,  0.0626, -0.0070,  0.0323, -0.0253, -0.0128,  0.0185],\n",
            "        [ 0.0326,  0.0194,  0.0240, -0.0038,  0.0159, -0.0117, -0.0048,  0.0053]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01580134406685829 tensor([[ 0.1240,  0.1041,  0.1189, -0.0129,  0.0606, -0.0497, -0.0202,  0.0321],\n",
            "        [ 0.1185,  0.1061,  0.1220, -0.0118,  0.0616, -0.0397, -0.0218,  0.0274],\n",
            "        [ 0.0638,  0.0537,  0.0618, -0.0085,  0.0332, -0.0223, -0.0140,  0.0108],\n",
            "        [ 0.0408,  0.0205,  0.0273, -0.0050,  0.0195, -0.0125, -0.0057,  0.0027]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015753060579299927 tensor([[ 0.1292,  0.1027,  0.1192, -0.0147,  0.0624, -0.0482, -0.0207,  0.0240],\n",
            "        [ 0.1239,  0.1071,  0.1256, -0.0135,  0.0647, -0.0352, -0.0232,  0.0175],\n",
            "        [ 0.0655,  0.0521,  0.0616, -0.0101,  0.0342, -0.0195, -0.0153,  0.0034],\n",
            "        [ 0.0479,  0.0205,  0.0293, -0.0062,  0.0224, -0.0126, -0.0064, -0.0007]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015710797160863876 tensor([[ 0.1342,  0.1012,  0.1193, -0.0165,  0.0641, -0.0467, -0.0212,  0.0160],\n",
            "        [ 0.1287,  0.1074,  0.1285, -0.0152,  0.0675, -0.0305, -0.0244,  0.0073],\n",
            "        [ 0.0677,  0.0510,  0.0619, -0.0116,  0.0356, -0.0170, -0.0166, -0.0036],\n",
            "        [ 0.0541,  0.0198,  0.0305, -0.0073,  0.0250, -0.0123, -0.0070, -0.0044]],\n",
            "       device='cuda:0')\n",
            "c tensor([6.5565e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02379961498081684 tensor([[ 0.1010,  0.0939,  0.1019, -0.0071,  0.0508, -0.0449, -0.0179,  0.0420],\n",
            "        [ 0.0833,  0.0742,  0.0815, -0.0063,  0.0417, -0.0382, -0.0144,  0.0365],\n",
            "        [ 0.0652,  0.0623,  0.0674, -0.0058,  0.0327, -0.0289, -0.0107,  0.0262],\n",
            "        [ 0.0260,  0.0183,  0.0205, -0.0022,  0.0131, -0.0118, -0.0047,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018725138157606125 tensor([[ 0.1343,  0.1205,  0.1317, -0.0103,  0.0669, -0.0548, -0.0238,  0.0465],\n",
            "        [ 0.1089,  0.0917,  0.1019, -0.0089,  0.0546, -0.0465, -0.0191,  0.0402],\n",
            "        [ 0.0811,  0.0759,  0.0821, -0.0083,  0.0405, -0.0322, -0.0130,  0.0245],\n",
            "        [ 0.0406,  0.0240,  0.0276, -0.0034,  0.0202, -0.0167, -0.0074,  0.0157]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018355753272771835 tensor([[ 0.1465,  0.1254,  0.1378, -0.0120,  0.0723, -0.0541, -0.0259,  0.0400],\n",
            "        [ 0.1177,  0.0920,  0.1040, -0.0104,  0.0591, -0.0459, -0.0209,  0.0345],\n",
            "        [ 0.0843,  0.0770,  0.0832, -0.0100,  0.0417, -0.0289, -0.0131,  0.0156],\n",
            "        [ 0.0513,  0.0258,  0.0307, -0.0043,  0.0254, -0.0194, -0.0093,  0.0174]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018293818458914757 tensor([[ 0.1545,  0.1261,  0.1394, -0.0135,  0.0755, -0.0513, -0.0274,  0.0314],\n",
            "        [ 0.1242,  0.0900,  0.1035, -0.0117,  0.0624, -0.0442, -0.0223,  0.0276],\n",
            "        [ 0.0865,  0.0771,  0.0834, -0.0115,  0.0425, -0.0250, -0.0129,  0.0062],\n",
            "        [ 0.0604,  0.0260,  0.0320, -0.0050,  0.0297, -0.0211, -0.0110,  0.0181]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018245749175548553 tensor([[ 0.1616,  0.1258,  0.1398, -0.0149,  0.0782, -0.0482, -0.0286,  0.0224],\n",
            "        [ 0.1303,  0.0877,  0.1027, -0.0130,  0.0654, -0.0423, -0.0236,  0.0205],\n",
            "        [ 0.0890,  0.0774,  0.0838, -0.0131,  0.0433, -0.0213, -0.0129, -0.0029],\n",
            "        [ 0.0684,  0.0252,  0.0322, -0.0057,  0.0334, -0.0223, -0.0124,  0.0182]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021106649190187454 tensor([[ 0.0981,  0.0938,  0.1031, -0.0084,  0.0501, -0.0444, -0.0171,  0.0400],\n",
            "        [ 0.0702,  0.0652,  0.0723, -0.0057,  0.0362, -0.0310, -0.0125,  0.0291],\n",
            "        [ 0.0463,  0.0431,  0.0485, -0.0039,  0.0244, -0.0204, -0.0084,  0.0184],\n",
            "        [ 0.0242,  0.0222,  0.0227, -0.0023,  0.0124, -0.0121, -0.0042,  0.0097]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017085175961256027 tensor([[ 0.1303,  0.1221,  0.1356, -0.0129,  0.0663, -0.0549, -0.0226,  0.0439],\n",
            "        [ 0.0831,  0.0734,  0.0832, -0.0077,  0.0437, -0.0324, -0.0153,  0.0262],\n",
            "        [ 0.0559,  0.0495,  0.0578, -0.0052,  0.0302, -0.0214, -0.0104,  0.0157],\n",
            "        [ 0.0367,  0.0333,  0.0337, -0.0039,  0.0189, -0.0176, -0.0060,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016782045364379883 tensor([[ 0.1436,  0.1310,  0.1468, -0.0161,  0.0728, -0.0558, -0.0248,  0.0377],\n",
            "        [ 0.0851,  0.0705,  0.0824, -0.0091,  0.0458, -0.0282, -0.0164,  0.0173],\n",
            "        [ 0.0587,  0.0491,  0.0599, -0.0061,  0.0326, -0.0188, -0.0112,  0.0093],\n",
            "        [ 0.0448,  0.0401,  0.0402, -0.0051,  0.0231, -0.0207, -0.0071,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016709063202142715 tensor([[ 0.1516,  0.1346,  0.1523, -0.0190,  0.0766, -0.0541, -0.0261,  0.0287],\n",
            "        [ 0.0870,  0.0675,  0.0815, -0.0105,  0.0478, -0.0239, -0.0174,  0.0084],\n",
            "        [ 0.0609,  0.0481,  0.0612, -0.0070,  0.0346, -0.0159, -0.0118,  0.0026],\n",
            "        [ 0.0506,  0.0447,  0.0442, -0.0062,  0.0260, -0.0225, -0.0077,  0.0098]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016665104776620865 tensor([[ 1.5778e-01,  1.3624e-01,  1.5560e-01, -2.1708e-02,  7.9355e-02,\n",
            "         -5.1452e-02, -2.7128e-02,  1.8671e-02],\n",
            "        [ 8.9554e-02,  6.5222e-02,  8.1418e-02, -1.1886e-02,  5.0286e-02,\n",
            "         -1.9989e-02, -1.8623e-02,  1.4305e-05],\n",
            "        [ 6.3078e-02,  4.7098e-02,  6.2617e-02, -7.7736e-03,  3.6625e-02,\n",
            "         -1.3082e-02, -1.2512e-02, -3.8910e-03],\n",
            "        [ 5.4963e-02,  4.7863e-02,  4.6692e-02, -7.2753e-03,  2.8234e-02,\n",
            "         -2.3512e-02, -8.0776e-03,  7.1204e-03]], device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[5, 6, 4, 4, 2, 8, 4, 6, 6, 4, 5, 6, 6, 1, 8, 11, 1, 2, 2, 2, 4, 6, 1, 1, 11, 2, 1, 13, 2, 2, 8, 5, 6, 1, 2, 2, 5, 2, 8, 11, 4, 1, 11, 2, 11, 10, 6, 1, 5, 6, 6, 6, 2, 4, 5, 5, 5, 6, 5, 4, 4, 6, 2, 8, 6, 2, 5, 2, 4, 2, 2, 8, 1, 5]\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021735195070505142 tensor([[ 0.0807,  0.0756,  0.0831, -0.0060,  0.0405, -0.0354, -0.0135,  0.0287],\n",
            "        [ 0.0784,  0.0730,  0.0811, -0.0073,  0.0415, -0.0359, -0.0140,  0.0332],\n",
            "        [ 0.0491,  0.0416,  0.0480, -0.0043,  0.0244, -0.0215, -0.0082,  0.0197],\n",
            "        [ 0.0133,  0.0145,  0.0150, -0.0017,  0.0059, -0.0071, -0.0018,  0.0035]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018892798572778702 tensor([[ 0.0871,  0.0755,  0.0837, -0.0071,  0.0427, -0.0331, -0.0138,  0.0176],\n",
            "        [ 0.0831,  0.0726,  0.0833, -0.0097,  0.0460, -0.0335, -0.0151,  0.0253],\n",
            "        [ 0.0594,  0.0444,  0.0546, -0.0060,  0.0290, -0.0225, -0.0095,  0.0174],\n",
            "        [ 0.0202,  0.0228,  0.0236, -0.0029,  0.0078, -0.0103, -0.0022,  0.0028]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01883077062666416 tensor([[ 0.0929,  0.0748,  0.0837, -0.0081,  0.0446, -0.0307, -0.0141,  0.0065],\n",
            "        [ 0.0860,  0.0704,  0.0837, -0.0119,  0.0496, -0.0303, -0.0160,  0.0166],\n",
            "        [ 0.0666,  0.0441,  0.0579, -0.0075,  0.0320, -0.0220, -0.0102,  0.0135],\n",
            "        [ 0.0243,  0.0285,  0.0293, -0.0039,  0.0082, -0.0121, -0.0020,  0.0004]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018786251544952393 tensor([[ 0.0990,  0.0743,  0.0840, -0.0090,  0.0467, -0.0284, -0.0143, -0.0043],\n",
            "        [ 0.0891,  0.0685,  0.0842, -0.0142,  0.0533, -0.0272, -0.0169,  0.0082],\n",
            "        [ 0.0727,  0.0427,  0.0600, -0.0089,  0.0344, -0.0209, -0.0108,  0.0089],\n",
            "        [ 0.0269,  0.0327,  0.0334, -0.0048,  0.0079, -0.0131, -0.0016, -0.0029]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018746182322502136 tensor([[ 1.0507e-01,  7.3997e-02,  8.4385e-02, -1.0012e-02,  4.8776e-02,\n",
            "         -2.6187e-02, -1.4613e-02, -1.4944e-02],\n",
            "        [ 9.2335e-02,  6.6704e-02,  8.4908e-02, -1.6391e-02,  5.6982e-02,\n",
            "         -2.4291e-02, -1.7774e-02, -3.3379e-05],\n",
            "        [ 7.8115e-02,  4.0668e-02,  6.1347e-02, -1.0210e-02,  3.6397e-02,\n",
            "         -1.9453e-02, -1.1185e-02,  3.9828e-03],\n",
            "        [ 2.8580e-02,  3.6027e-02,  3.6438e-02, -5.6863e-03,  7.0363e-03,\n",
            "         -1.3518e-02, -1.0276e-03, -6.7657e-03]], device='cuda:0')\n",
            "c tensor([2.3842e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023198578506708145 tensor([[ 0.0981,  0.0883,  0.0964, -0.0077,  0.0497, -0.0434, -0.0177,  0.0396],\n",
            "        [ 0.0941,  0.0888,  0.0988, -0.0076,  0.0468, -0.0403, -0.0150,  0.0354],\n",
            "        [ 0.0797,  0.0723,  0.0792, -0.0064,  0.0405, -0.0368, -0.0138,  0.0355],\n",
            "        [ 0.0256,  0.0189,  0.0218, -0.0033,  0.0131, -0.0123, -0.0047,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017756108194589615 tensor([[ 0.1154,  0.0957,  0.1052, -0.0101,  0.0584, -0.0460, -0.0210,  0.0353],\n",
            "        [ 0.1066,  0.0968,  0.1110, -0.0101,  0.0520, -0.0377, -0.0156,  0.0244],\n",
            "        [ 0.1124,  0.0970,  0.1070, -0.0097,  0.0572, -0.0484, -0.0198,  0.0430],\n",
            "        [ 0.0396,  0.0252,  0.0306, -0.0054,  0.0201, -0.0178, -0.0071,  0.0151]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.0174449123442173 tensor([[ 0.1239,  0.0939,  0.1041, -0.0119,  0.0626, -0.0443, -0.0227,  0.0266],\n",
            "        [ 0.1079,  0.0934,  0.1108, -0.0117,  0.0516, -0.0296, -0.0141,  0.0079],\n",
            "        [ 0.1280,  0.1045,  0.1162, -0.0118,  0.0651, -0.0511, -0.0229,  0.0408],\n",
            "        [ 0.0492,  0.0273,  0.0350, -0.0073,  0.0249, -0.0207, -0.0087,  0.0162]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017342930659651756 tensor([[ 0.1322,  0.0921,  0.1029, -0.0136,  0.0667, -0.0426, -0.0244,  0.0180],\n",
            "        [ 0.1103,  0.0911,  0.1118, -0.0133,  0.0519, -0.0222, -0.0129, -0.0075],\n",
            "        [ 0.1385,  0.1068,  0.1197, -0.0136,  0.0703, -0.0513, -0.0251,  0.0359],\n",
            "        [ 0.0567,  0.0275,  0.0373, -0.0090,  0.0285, -0.0225, -0.0099,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017270132899284363 tensor([[ 0.1404,  0.0903,  0.1016, -0.0153,  0.0708, -0.0408, -0.0261,  0.0095],\n",
            "        [ 0.1139,  0.0899,  0.1140, -0.0149,  0.0527, -0.0156, -0.0119, -0.0219],\n",
            "        [ 0.1467,  0.1068,  0.1206, -0.0152,  0.0743, -0.0503, -0.0269,  0.0298],\n",
            "        [ 0.0631,  0.0265,  0.0384, -0.0106,  0.0316, -0.0236, -0.0109,  0.0149]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02463340014219284 tensor([[ 0.0967,  0.0875,  0.0957, -0.0085,  0.0476, -0.0443, -0.0168,  0.0401],\n",
            "        [ 0.0952,  0.0910,  0.0984, -0.0069,  0.0483, -0.0414, -0.0157,  0.0383],\n",
            "        [ 0.0711,  0.0660,  0.0743, -0.0058,  0.0368, -0.0310, -0.0126,  0.0303],\n",
            "        [ 0.0267,  0.0200,  0.0218, -0.0027,  0.0134, -0.0135, -0.0046,  0.0110]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019080042839050293 tensor([[ 0.1199,  0.1017,  0.1118, -0.0122,  0.0578, -0.0513, -0.0208,  0.0406],\n",
            "        [ 0.1183,  0.1101,  0.1199, -0.0094,  0.0607, -0.0445, -0.0193,  0.0350],\n",
            "        [ 0.0971,  0.0875,  0.1009, -0.0087,  0.0510, -0.0380, -0.0175,  0.0340],\n",
            "        [ 0.0421,  0.0279,  0.0309, -0.0042,  0.0210, -0.0207, -0.0069,  0.0148]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018745271489024162 tensor([[ 0.1266,  0.0993,  0.1096, -0.0146,  0.0597, -0.0505, -0.0220,  0.0331],\n",
            "        [ 0.1230,  0.1102,  0.1210, -0.0107,  0.0637, -0.0381, -0.0197,  0.0216],\n",
            "        [ 0.1067,  0.0923,  0.1096, -0.0104,  0.0566, -0.0362, -0.0196,  0.0283],\n",
            "        [ 0.0530,  0.0313,  0.0353, -0.0054,  0.0261, -0.0252, -0.0083,  0.0156]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018668780103325844 tensor([[ 0.1335,  0.0970,  0.1074, -0.0171,  0.0616, -0.0497, -0.0232,  0.0258],\n",
            "        [ 0.1275,  0.1099,  0.1217, -0.0120,  0.0666, -0.0316, -0.0201,  0.0084],\n",
            "        [ 0.1128,  0.0936,  0.1144, -0.0120,  0.0604, -0.0327, -0.0211,  0.0208],\n",
            "        [ 0.0617,  0.0327,  0.0374, -0.0064,  0.0302, -0.0284, -0.0094,  0.0151]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018607832491397858 tensor([[ 0.1405,  0.0950,  0.1055, -0.0195,  0.0637, -0.0490, -0.0244,  0.0187],\n",
            "        [ 0.1323,  0.1100,  0.1228, -0.0133,  0.0696, -0.0254, -0.0205, -0.0045],\n",
            "        [ 0.1175,  0.0935,  0.1176, -0.0134,  0.0635, -0.0285, -0.0224,  0.0126],\n",
            "        [ 0.0690,  0.0328,  0.0382, -0.0072,  0.0335, -0.0310, -0.0102,  0.0137]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.0244367104023695 tensor([[ 0.1183,  0.1178,  0.1310, -0.0095,  0.0600, -0.0490, -0.0197,  0.0426],\n",
            "        [ 0.1004,  0.0879,  0.0955, -0.0085,  0.0505, -0.0472, -0.0172,  0.0442],\n",
            "        [ 0.0576,  0.0494,  0.0546, -0.0050,  0.0294, -0.0264, -0.0106,  0.0249],\n",
            "        [ 0.0237,  0.0220,  0.0240, -0.0028,  0.0118, -0.0105, -0.0038,  0.0081]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018045732751488686 tensor([[ 0.1321,  0.1304,  0.1487, -0.0125,  0.0664, -0.0449, -0.0213,  0.0282],\n",
            "        [ 0.1348,  0.1111,  0.1218, -0.0128,  0.0682, -0.0590, -0.0234,  0.0493],\n",
            "        [ 0.0832,  0.0673,  0.0749, -0.0074,  0.0431, -0.0364, -0.0159,  0.0321],\n",
            "        [ 0.0367,  0.0333,  0.0365, -0.0047,  0.0178, -0.0149, -0.0052,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017633941024541855 tensor([[ 0.1308,  0.1272,  0.1490, -0.0144,  0.0650, -0.0336, -0.0203,  0.0071],\n",
            "        [ 0.1476,  0.1121,  0.1241, -0.0157,  0.0748, -0.0595, -0.0258,  0.0423],\n",
            "        [ 0.0971,  0.0736,  0.0826, -0.0089,  0.0508, -0.0401, -0.0192,  0.0324],\n",
            "        [ 0.0450,  0.0403,  0.0443, -0.0063,  0.0214, -0.0166, -0.0058,  0.0076]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017505254596471786 tensor([[ 0.1328,  0.1273,  0.1529, -0.0163,  0.0655, -0.0241, -0.0200, -0.0119],\n",
            "        [ 0.1574,  0.1103,  0.1233, -0.0183,  0.0799, -0.0586, -0.0277,  0.0339],\n",
            "        [ 0.1070,  0.0761,  0.0863, -0.0102,  0.0565, -0.0417, -0.0217,  0.0304],\n",
            "        [ 0.0510,  0.0450,  0.0497, -0.0077,  0.0238, -0.0170, -0.0060,  0.0044]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017415042966604233 tensor([[ 0.1360,  0.1287,  0.1582, -0.0183,  0.0665, -0.0153, -0.0198, -0.0298],\n",
            "        [ 0.1659,  0.1073,  0.1211, -0.0208,  0.0843, -0.0571, -0.0294,  0.0251],\n",
            "        [ 0.1149,  0.0765,  0.0876, -0.0113,  0.0611, -0.0422, -0.0239,  0.0273],\n",
            "        [ 0.0554,  0.0484,  0.0535, -0.0090,  0.0254, -0.0166, -0.0059,  0.0003]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022383669391274452 tensor([[ 0.0854,  0.0749,  0.0833, -0.0075,  0.0441, -0.0390, -0.0158,  0.0356],\n",
            "        [ 0.0602,  0.0533,  0.0600, -0.0048,  0.0304, -0.0270, -0.0105,  0.0252],\n",
            "        [ 0.0757,  0.0760,  0.0850, -0.0062,  0.0382, -0.0320, -0.0115,  0.0289],\n",
            "        [ 0.0148,  0.0125,  0.0129, -0.0022,  0.0087, -0.0067, -0.0038,  0.0057]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0185749102383852 tensor([[ 0.1217,  0.1013,  0.1143, -0.0119,  0.0631, -0.0527, -0.0229,  0.0448],\n",
            "        [ 0.0657,  0.0522,  0.0609, -0.0058,  0.0330, -0.0263, -0.0115,  0.0208],\n",
            "        [ 0.0900,  0.0908,  0.1037, -0.0081,  0.0444, -0.0321, -0.0120,  0.0235],\n",
            "        [ 0.0192,  0.0136,  0.0141, -0.0036,  0.0119, -0.0069, -0.0058,  0.0041]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01823834702372551 tensor([[ 0.1422,  0.1117,  0.1277, -0.0152,  0.0739, -0.0585, -0.0273,  0.0455],\n",
            "        [ 0.0638,  0.0439,  0.0540, -0.0062,  0.0318, -0.0219, -0.0111,  0.0126],\n",
            "        [ 0.0933,  0.0947,  0.1105, -0.0093,  0.0449, -0.0267, -0.0104,  0.0123],\n",
            "        [ 0.0220,  0.0131,  0.0138, -0.0049,  0.0143, -0.0062, -0.0074,  0.0017]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018122658133506775 tensor([[ 0.1565,  0.1159,  0.1343, -0.0180,  0.0814, -0.0612, -0.0307,  0.0429],\n",
            "        [ 0.0633,  0.0369,  0.0488, -0.0066,  0.0313, -0.0181, -0.0108,  0.0051],\n",
            "        [ 0.0958,  0.0976,  0.1162, -0.0103,  0.0450, -0.0209, -0.0088,  0.0009],\n",
            "        [ 0.0246,  0.0123,  0.0132, -0.0062,  0.0165, -0.0052, -0.0090, -0.0010]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018054930493235588 tensor([[ 0.1676,  0.1168,  0.1373, -0.0206,  0.0873, -0.0623, -0.0335,  0.0386],\n",
            "        [ 0.0645,  0.0317,  0.0455, -0.0071,  0.0317, -0.0153, -0.0109, -0.0013],\n",
            "        [ 0.0982,  0.1004,  0.1218, -0.0113,  0.0450, -0.0152, -0.0072, -0.0104],\n",
            "        [ 0.0270,  0.0114,  0.0125, -0.0074,  0.0186, -0.0043, -0.0106, -0.0037]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, 3.5167e-06], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02441878616809845 tensor([[ 0.1044,  0.0966,  0.1064, -0.0076,  0.0517, -0.0462, -0.0173,  0.0412],\n",
            "        [ 0.0927,  0.0810,  0.0890, -0.0075,  0.0457, -0.0437, -0.0155,  0.0410],\n",
            "        [ 0.0534,  0.0479,  0.0534, -0.0047,  0.0269, -0.0235, -0.0087,  0.0205],\n",
            "        [ 0.0256,  0.0215,  0.0228, -0.0031,  0.0133, -0.0109, -0.0048,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01943671703338623 tensor([[ 0.1250,  0.1092,  0.1230, -0.0100,  0.0606, -0.0490, -0.0199,  0.0366],\n",
            "        [ 0.1217,  0.0994,  0.1100, -0.0107,  0.0590, -0.0550, -0.0202,  0.0463],\n",
            "        [ 0.0731,  0.0629,  0.0714, -0.0066,  0.0363, -0.0291, -0.0111,  0.0215],\n",
            "        [ 0.0385,  0.0304,  0.0322, -0.0055,  0.0202, -0.0147, -0.0071,  0.0117]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01914876513183117 tensor([[ 0.1313,  0.1072,  0.1236, -0.0113,  0.0623, -0.0447, -0.0201,  0.0250],\n",
            "        [ 0.1339,  0.1007,  0.1124, -0.0128,  0.0637, -0.0575, -0.0218,  0.0422],\n",
            "        [ 0.0817,  0.0671,  0.0780, -0.0078,  0.0400, -0.0289, -0.0114,  0.0162],\n",
            "        [ 0.0470,  0.0350,  0.0372, -0.0077,  0.0249, -0.0160, -0.0087,  0.0112]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019083647057414055 tensor([[ 0.1371,  0.1046,  0.1235, -0.0125,  0.0637, -0.0403, -0.0202,  0.0134],\n",
            "        [ 0.1430,  0.0988,  0.1114, -0.0146,  0.0668, -0.0586, -0.0230,  0.0365],\n",
            "        [ 0.0873,  0.0683,  0.0812, -0.0088,  0.0420, -0.0271, -0.0112,  0.0092],\n",
            "        [ 0.0534,  0.0375,  0.0399, -0.0098,  0.0285, -0.0161, -0.0099,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019032590091228485 tensor([[ 0.1433,  0.1025,  0.1239, -0.0137,  0.0654, -0.0362, -0.0204,  0.0023],\n",
            "        [ 0.1512,  0.0961,  0.1095, -0.0163,  0.0695, -0.0592, -0.0240,  0.0304],\n",
            "        [ 0.0917,  0.0684,  0.0833, -0.0097,  0.0435, -0.0248, -0.0108,  0.0016],\n",
            "        [ 0.0586,  0.0389,  0.0415, -0.0118,  0.0316, -0.0156, -0.0109,  0.0068]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025593679398298264 tensor([[ 0.0982,  0.0917,  0.1027, -0.0074,  0.0489, -0.0432, -0.0157,  0.0379],\n",
            "        [ 0.0818,  0.0750,  0.0829, -0.0058,  0.0421, -0.0376, -0.0146,  0.0356],\n",
            "        [ 0.0652,  0.0597,  0.0663, -0.0052,  0.0331, -0.0289, -0.0102,  0.0263],\n",
            "        [ 0.0276,  0.0208,  0.0232, -0.0018,  0.0139, -0.0126, -0.0045,  0.0118]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02107016183435917 tensor([[ 0.1131,  0.0995,  0.1143, -0.0095,  0.0550, -0.0440, -0.0172,  0.0309],\n",
            "        [ 0.0993,  0.0866,  0.0978, -0.0071,  0.0521, -0.0413, -0.0180,  0.0349],\n",
            "        [ 0.0820,  0.0713,  0.0808, -0.0073,  0.0416, -0.0327, -0.0120,  0.0247],\n",
            "        [ 0.0425,  0.0287,  0.0330, -0.0027,  0.0212, -0.0176, -0.0065,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02090386301279068 tensor([[ 0.1184,  0.0976,  0.1152, -0.0109,  0.0563, -0.0402, -0.0170,  0.0192],\n",
            "        [ 0.1053,  0.0865,  0.0999, -0.0075,  0.0561, -0.0391, -0.0194,  0.0280],\n",
            "        [ 0.0882,  0.0721,  0.0838, -0.0088,  0.0446, -0.0310, -0.0120,  0.0172],\n",
            "        [ 0.0531,  0.0324,  0.0384, -0.0033,  0.0263, -0.0201, -0.0076,  0.0161]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020846012979745865 tensor([[ 0.1240,  0.0959,  0.1164, -0.0122,  0.0577, -0.0366, -0.0169,  0.0079],\n",
            "        [ 0.1102,  0.0854,  0.1008, -0.0079,  0.0596, -0.0365, -0.0206,  0.0207],\n",
            "        [ 0.0928,  0.0713,  0.0851, -0.0102,  0.0468, -0.0285, -0.0117,  0.0088],\n",
            "        [ 0.0615,  0.0340,  0.0416, -0.0038,  0.0303, -0.0214, -0.0084,  0.0155]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02079913392663002 tensor([[ 0.1298,  0.0946,  0.1178, -0.0136,  0.0593, -0.0332, -0.0168, -0.0031],\n",
            "        [ 0.1148,  0.0840,  0.1014, -0.0083,  0.0630, -0.0337, -0.0218,  0.0133],\n",
            "        [ 0.0970,  0.0702,  0.0859, -0.0115,  0.0487, -0.0258, -0.0114,  0.0003],\n",
            "        [ 0.0685,  0.0344,  0.0434, -0.0042,  0.0336, -0.0219, -0.0089,  0.0142]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02336292341351509 tensor([[ 0.1175,  0.1120,  0.1240, -0.0100,  0.0585, -0.0508, -0.0190,  0.0463],\n",
            "        [ 0.0896,  0.0845,  0.0939, -0.0069,  0.0449, -0.0393, -0.0148,  0.0367],\n",
            "        [ 0.0620,  0.0539,  0.0602, -0.0051,  0.0319, -0.0283, -0.0097,  0.0238],\n",
            "        [ 0.0310,  0.0246,  0.0278, -0.0037,  0.0165, -0.0144, -0.0056,  0.0131]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018230602145195007 tensor([[ 0.1351,  0.1234,  0.1386, -0.0139,  0.0661, -0.0522, -0.0208,  0.0403],\n",
            "        [ 0.0823,  0.0712,  0.0815, -0.0072,  0.0403, -0.0287, -0.0131,  0.0201],\n",
            "        [ 0.0756,  0.0585,  0.0676, -0.0066,  0.0386, -0.0310, -0.0106,  0.0197],\n",
            "        [ 0.0488,  0.0355,  0.0417, -0.0064,  0.0265, -0.0208, -0.0088,  0.0177]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018013454973697662 tensor([[ 0.1462,  0.1282,  0.1460, -0.0174,  0.0703, -0.0504, -0.0215,  0.0308],\n",
            "        [ 0.0823,  0.0652,  0.0772, -0.0079,  0.0395, -0.0218, -0.0126,  0.0076],\n",
            "        [ 0.0851,  0.0589,  0.0704, -0.0078,  0.0432, -0.0317, -0.0107,  0.0135],\n",
            "        [ 0.0620,  0.0420,  0.0507, -0.0088,  0.0342, -0.0246, -0.0112,  0.0193]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.0179128535091877 tensor([[ 0.1543,  0.1300,  0.1501, -0.0207,  0.0730, -0.0472, -0.0218,  0.0197],\n",
            "        [ 0.0852,  0.0622,  0.0762, -0.0087,  0.0402, -0.0163, -0.0125, -0.0030],\n",
            "        [ 0.0925,  0.0574,  0.0711, -0.0088,  0.0467, -0.0315, -0.0105,  0.0063],\n",
            "        [ 0.0722,  0.0455,  0.0565, -0.0111,  0.0404, -0.0268, -0.0130,  0.0191]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01784564182162285 tensor([[ 0.1612,  0.1306,  0.1529, -0.0239,  0.0751, -0.0434, -0.0218,  0.0080],\n",
            "        [ 0.0894,  0.0604,  0.0766, -0.0096,  0.0416, -0.0116, -0.0127, -0.0128],\n",
            "        [ 0.0990,  0.0550,  0.0707, -0.0098,  0.0498, -0.0308, -0.0101, -0.0011],\n",
            "        [ 0.0805,  0.0470,  0.0602, -0.0133,  0.0455, -0.0279, -0.0145,  0.0177]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026178091764450073 tensor([[ 0.0983,  0.0877,  0.0957, -0.0077,  0.0491, -0.0443, -0.0173,  0.0402],\n",
            "        [ 0.0974,  0.0950,  0.1054, -0.0083,  0.0489, -0.0429, -0.0149,  0.0379],\n",
            "        [ 0.0607,  0.0560,  0.0601, -0.0057,  0.0321, -0.0298, -0.0118,  0.0280],\n",
            "        [ 0.0217,  0.0167,  0.0198, -0.0029,  0.0118, -0.0084, -0.0040,  0.0077]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020302599295973778 tensor([[ 0.1416,  0.1219,  0.1339, -0.0120,  0.0701, -0.0600, -0.0249,  0.0495],\n",
            "        [ 0.1303,  0.1263,  0.1424, -0.0131,  0.0653, -0.0516, -0.0190,  0.0382],\n",
            "        [ 0.0825,  0.0729,  0.0770, -0.0084,  0.0446, -0.0396, -0.0173,  0.0341],\n",
            "        [ 0.0319,  0.0218,  0.0277, -0.0051,  0.0177, -0.0101, -0.0059,  0.0080]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019634589552879333 tensor([[ 0.1603,  0.1312,  0.1448, -0.0147,  0.0786, -0.0635, -0.0282,  0.0461],\n",
            "        [ 0.1343,  0.1280,  0.1475, -0.0160,  0.0668, -0.0452, -0.0181,  0.0225],\n",
            "        [ 0.0891,  0.0743,  0.0771, -0.0100,  0.0494, -0.0416, -0.0202,  0.0317],\n",
            "        [ 0.0381,  0.0232,  0.0317, -0.0069,  0.0217, -0.0096, -0.0069,  0.0058]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.0195323396474123 tensor([[ 0.1733,  0.1346,  0.1492, -0.0170,  0.0841, -0.0641, -0.0305,  0.0397],\n",
            "        [ 0.1357,  0.1269,  0.1497, -0.0187,  0.0671, -0.0375, -0.0169,  0.0053],\n",
            "        [ 0.0937,  0.0736,  0.0750, -0.0114,  0.0530, -0.0424, -0.0228,  0.0282],\n",
            "        [ 0.0431,  0.0233,  0.0344, -0.0087,  0.0249, -0.0083, -0.0076,  0.0029]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01945667155086994 tensor([[ 0.1841,  0.1357,  0.1512, -0.0191,  0.0884, -0.0636, -0.0325,  0.0321],\n",
            "        [ 0.1377,  0.1264,  0.1526, -0.0214,  0.0677, -0.0300, -0.0157, -0.0114],\n",
            "        [ 0.0979,  0.0726,  0.0726, -0.0127,  0.0565, -0.0430, -0.0253,  0.0245],\n",
            "        [ 0.0475,  0.0227,  0.0363, -0.0104,  0.0278, -0.0067, -0.0083, -0.0003]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02485675737261772 tensor([[ 0.1025,  0.0980,  0.1066, -0.0074,  0.0506, -0.0466, -0.0172,  0.0418],\n",
            "        [ 0.0959,  0.0909,  0.1015, -0.0091,  0.0488, -0.0420, -0.0157,  0.0364],\n",
            "        [ 0.0633,  0.0544,  0.0598, -0.0054,  0.0338, -0.0291, -0.0119,  0.0287],\n",
            "        [ 0.0287,  0.0246,  0.0273, -0.0030,  0.0148, -0.0134, -0.0049,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020111069083213806 tensor([[ 0.1149,  0.1048,  0.1139, -0.0087,  0.0550, -0.0483, -0.0185,  0.0368],\n",
            "        [ 0.0930,  0.0822,  0.0953, -0.0120,  0.0466, -0.0330, -0.0141,  0.0176],\n",
            "        [ 0.0824,  0.0635,  0.0713, -0.0079,  0.0459, -0.0345, -0.0165,  0.0319],\n",
            "        [ 0.0446,  0.0369,  0.0417, -0.0050,  0.0232, -0.0196, -0.0073,  0.0162]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01991180330514908 tensor([[ 0.1235,  0.1076,  0.1170, -0.0097,  0.0575, -0.0482, -0.0190,  0.0297],\n",
            "        [ 0.0943,  0.0777,  0.0938, -0.0150,  0.0467, -0.0262, -0.0133,  0.0014],\n",
            "        [ 0.0952,  0.0665,  0.0760, -0.0100,  0.0547, -0.0365, -0.0199,  0.0316],\n",
            "        [ 0.0556,  0.0445,  0.0510, -0.0067,  0.0290, -0.0231, -0.0088,  0.0174]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019815197214484215 tensor([[ 0.1304,  0.1088,  0.1183, -0.0106,  0.0590, -0.0471, -0.0193,  0.0218],\n",
            "        [ 0.0977,  0.0753,  0.0945, -0.0181,  0.0478, -0.0205, -0.0128, -0.0133],\n",
            "        [ 0.1046,  0.0662,  0.0771, -0.0118,  0.0617, -0.0369, -0.0228,  0.0296],\n",
            "        [ 0.0636,  0.0492,  0.0572, -0.0082,  0.0332, -0.0248, -0.0096,  0.0166]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019747894257307053 tensor([[ 0.1365,  0.1092,  0.1188, -0.0115,  0.0602, -0.0458, -0.0194,  0.0135],\n",
            "        [ 0.1020,  0.0738,  0.0962, -0.0212,  0.0494, -0.0153, -0.0125, -0.0272],\n",
            "        [ 0.1123,  0.0641,  0.0762, -0.0135,  0.0678, -0.0365, -0.0254,  0.0267],\n",
            "        [ 0.0697,  0.0520,  0.0614, -0.0097,  0.0365, -0.0255, -0.0102,  0.0146]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019454166293144226 tensor([[ 0.1033,  0.0957,  0.1047, -0.0080,  0.0517, -0.0469, -0.0176,  0.0429],\n",
            "        [ 0.0763,  0.0697,  0.0764, -0.0062,  0.0388, -0.0345, -0.0138,  0.0330],\n",
            "        [ 0.0626,  0.0610,  0.0671, -0.0053,  0.0313, -0.0276, -0.0097,  0.0255],\n",
            "        [ 0.0245,  0.0183,  0.0207, -0.0025,  0.0130, -0.0114, -0.0047,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01443688478320837 tensor([[ 0.1417,  0.1270,  0.1401, -0.0120,  0.0698, -0.0601, -0.0237,  0.0496],\n",
            "        [ 0.1001,  0.0878,  0.0972, -0.0092,  0.0515, -0.0416, -0.0188,  0.0362],\n",
            "        [ 0.0794,  0.0764,  0.0845, -0.0076,  0.0392, -0.0314, -0.0115,  0.0252],\n",
            "        [ 0.0380,  0.0250,  0.0288, -0.0039,  0.0200, -0.0164, -0.0073,  0.0133]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013980654068291187 tensor([[ 0.1570,  0.1344,  0.1496, -0.0144,  0.0761, -0.0616, -0.0257,  0.0440],\n",
            "        [ 0.1075,  0.0891,  0.1000, -0.0110,  0.0559, -0.0400, -0.0209,  0.0301],\n",
            "        [ 0.0832,  0.0786,  0.0878, -0.0090,  0.0404, -0.0284, -0.0111,  0.0177],\n",
            "        [ 0.0474,  0.0276,  0.0327, -0.0050,  0.0248, -0.0190, -0.0092,  0.0139]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01391017995774746 tensor([[ 0.1666,  0.1361,  0.1527, -0.0165,  0.0794, -0.0602, -0.0268,  0.0355],\n",
            "        [ 0.1122,  0.0877,  0.0999, -0.0126,  0.0589, -0.0370, -0.0226,  0.0226],\n",
            "        [ 0.0852,  0.0791,  0.0893, -0.0102,  0.0407, -0.0245, -0.0105,  0.0094],\n",
            "        [ 0.0550,  0.0284,  0.0346, -0.0060,  0.0287, -0.0206, -0.0106,  0.0133]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01386443804949522 tensor([[ 0.1745,  0.1360,  0.1540, -0.0184,  0.0818, -0.0580, -0.0275,  0.0262],\n",
            "        [ 0.1166,  0.0860,  0.0995, -0.0142,  0.0617, -0.0339, -0.0242,  0.0149],\n",
            "        [ 0.0873,  0.0795,  0.0908, -0.0114,  0.0410, -0.0207, -0.0098,  0.0012],\n",
            "        [ 0.0616,  0.0281,  0.0354, -0.0069,  0.0320, -0.0217, -0.0119,  0.0121]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.027863875031471252 tensor([[ 0.0967,  0.0849,  0.0934, -0.0080,  0.0488, -0.0449, -0.0169,  0.0428],\n",
            "        [ 0.0696,  0.0634,  0.0698, -0.0058,  0.0347, -0.0320, -0.0116,  0.0293],\n",
            "        [ 0.0545,  0.0526,  0.0599, -0.0053,  0.0283, -0.0214, -0.0103,  0.0201],\n",
            "        [ 0.0170,  0.0087,  0.0109, -0.0021,  0.0086, -0.0068, -0.0025,  0.0037]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.023746779188513756 tensor([[ 0.1453,  0.1235,  0.1371, -0.0132,  0.0733, -0.0646, -0.0252,  0.0586],\n",
            "        [ 0.0890,  0.0785,  0.0870, -0.0083,  0.0444, -0.0392, -0.0147,  0.0322],\n",
            "        [ 0.0583,  0.0543,  0.0642, -0.0074,  0.0297, -0.0172, -0.0115,  0.0122],\n",
            "        [ 0.0252,  0.0069,  0.0113, -0.0035,  0.0125, -0.0076, -0.0033,  0.0012]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.023143820464611053 tensor([[ 0.1717,  0.1395,  0.1565, -0.0168,  0.0864, -0.0730, -0.0298,  0.0624],\n",
            "        [ 0.0935,  0.0790,  0.0883, -0.0096,  0.0466, -0.0388, -0.0152,  0.0270],\n",
            "        [ 0.0573,  0.0516,  0.0638, -0.0092,  0.0284, -0.0107, -0.0116,  0.0018],\n",
            "        [ 0.0323,  0.0040,  0.0106, -0.0048,  0.0158, -0.0077, -0.0039, -0.0019]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.022992722690105438 tensor([[ 0.1887,  0.1460,  0.1652, -0.0198,  0.0947, -0.0767, -0.0327,  0.0610],\n",
            "        [ 0.0946,  0.0763,  0.0863, -0.0106,  0.0471, -0.0368, -0.0151,  0.0202],\n",
            "        [ 0.0580,  0.0507,  0.0653, -0.0111,  0.0279, -0.0050, -0.0119, -0.0075],\n",
            "        [ 0.0392,  0.0010,  0.0096, -0.0061,  0.0189, -0.0077, -0.0044, -0.0050]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.022922206670045853 tensor([[ 2.0133e-01,  1.4789e-01,  1.6909e-01, -2.2466e-02,  1.0067e-01,\n",
            "         -7.8081e-02, -3.4850e-02,  5.7164e-02],\n",
            "        [ 9.5406e-02,  7.3238e-02,  8.3947e-02, -1.1594e-02,  4.7432e-02,\n",
            "         -3.4607e-02, -1.5026e-02,  1.3230e-02],\n",
            "        [ 5.9664e-02,  5.0859e-02,  6.8095e-02, -1.3108e-02,  2.7873e-02,\n",
            "          2.6226e-05, -1.2335e-02, -1.6117e-02],\n",
            "        [ 4.5824e-02, -2.2960e-03,  8.3905e-03, -7.3957e-03,  2.1944e-02,\n",
            "         -7.5740e-03, -4.8900e-03, -8.2397e-03]], device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022012906149029732 tensor([[ 0.1086,  0.0984,  0.1059, -0.0083,  0.0551, -0.0509, -0.0197,  0.0477],\n",
            "        [ 0.0751,  0.0639,  0.0711, -0.0062,  0.0370, -0.0342, -0.0130,  0.0329],\n",
            "        [ 0.0601,  0.0579,  0.0642, -0.0050,  0.0301, -0.0269, -0.0093,  0.0237],\n",
            "        [ 0.0114,  0.0108,  0.0142, -0.0020,  0.0068, -0.0039, -0.0020,  0.0022]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016953980550169945 tensor([[ 0.1511,  0.1315,  0.1418, -0.0126,  0.0764, -0.0671, -0.0278,  0.0579],\n",
            "        [ 0.1010,  0.0791,  0.0896, -0.0094,  0.0492, -0.0428, -0.0177,  0.0379],\n",
            "        [ 0.0819,  0.0781,  0.0871, -0.0069,  0.0402, -0.0340, -0.0117,  0.0260],\n",
            "        [ 0.0133,  0.0119,  0.0186, -0.0034,  0.0086, -0.0021, -0.0023, -0.0017]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016432391479611397 tensor([[ 0.1680,  0.1383,  0.1492, -0.0152,  0.0846, -0.0704, -0.0313,  0.0546],\n",
            "        [ 0.1097,  0.0769,  0.0893, -0.0113,  0.0525, -0.0424, -0.0193,  0.0333],\n",
            "        [ 0.0889,  0.0837,  0.0940, -0.0077,  0.0426, -0.0334, -0.0114,  0.0201],\n",
            "        [ 0.0133,  0.0111,  0.0211, -0.0047,  0.0093,  0.0010, -0.0021, -0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016364920884370804 tensor([[ 0.1789,  0.1390,  0.1499, -0.0173,  0.0898, -0.0707, -0.0337,  0.0480],\n",
            "        [ 0.1159,  0.0721,  0.0863, -0.0131,  0.0545, -0.0407, -0.0204,  0.0273],\n",
            "        [ 0.0928,  0.0863,  0.0975, -0.0082,  0.0435, -0.0314, -0.0106,  0.0126],\n",
            "        [ 0.0133,  0.0103,  0.0236, -0.0060,  0.0100,  0.0041, -0.0019, -0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016321435570716858 tensor([[ 0.1881,  0.1378,  0.1486, -0.0193,  0.0940, -0.0700, -0.0359,  0.0406],\n",
            "        [ 0.1221,  0.0674,  0.0833, -0.0148,  0.0565, -0.0391, -0.0216,  0.0214],\n",
            "        [ 0.0960,  0.0881,  0.1003, -0.0087,  0.0440, -0.0290, -0.0097,  0.0048],\n",
            "        [ 0.0134,  0.0096,  0.0263, -0.0073,  0.0108,  0.0071, -0.0017, -0.0168]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026152705773711205 tensor([[ 0.1196,  0.1090,  0.1176, -0.0094,  0.0602, -0.0558, -0.0212,  0.0526],\n",
            "        [ 0.0966,  0.0848,  0.0951, -0.0088,  0.0484, -0.0430, -0.0161,  0.0392],\n",
            "        [ 0.0679,  0.0638,  0.0706, -0.0057,  0.0348, -0.0299, -0.0113,  0.0279],\n",
            "        [ 0.0229,  0.0180,  0.0203, -0.0028,  0.0121, -0.0106, -0.0041,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01999271661043167 tensor([[ 0.1355,  0.1145,  0.1223, -0.0117,  0.0680, -0.0598, -0.0250,  0.0502],\n",
            "        [ 0.1246,  0.1013,  0.1180, -0.0134,  0.0620, -0.0489, -0.0200,  0.0382],\n",
            "        [ 0.0865,  0.0790,  0.0889, -0.0078,  0.0447, -0.0340, -0.0140,  0.0282],\n",
            "        [ 0.0338,  0.0234,  0.0277, -0.0049,  0.0177, -0.0142, -0.0058,  0.0115]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.0197654590010643 tensor([[ 0.1390,  0.1073,  0.1130, -0.0130,  0.0697, -0.0577, -0.0267,  0.0418],\n",
            "        [ 0.1372,  0.1020,  0.1239, -0.0170,  0.0676, -0.0468, -0.0210,  0.0286],\n",
            "        [ 0.0931,  0.0821,  0.0942, -0.0089,  0.0485, -0.0318, -0.0147,  0.0218],\n",
            "        [ 0.0410,  0.0254,  0.0315, -0.0067,  0.0213, -0.0157, -0.0067,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019691137596964836 tensor([[ 0.1450,  0.1028,  0.1068, -0.0145,  0.0727, -0.0570, -0.0288,  0.0350],\n",
            "        [ 0.1479,  0.1010,  0.1279, -0.0204,  0.0723, -0.0440, -0.0217,  0.0183],\n",
            "        [ 0.0975,  0.0831,  0.0971, -0.0099,  0.0511, -0.0286, -0.0150,  0.0142],\n",
            "        [ 0.0468,  0.0259,  0.0338, -0.0084,  0.0241, -0.0163, -0.0073,  0.0101]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01963418908417225 tensor([[ 0.1518,  0.0993,  0.1015, -0.0159,  0.0761, -0.0567, -0.0311,  0.0288],\n",
            "        [ 0.1576,  0.0989,  0.1306, -0.0237,  0.0764, -0.0406, -0.0223,  0.0074],\n",
            "        [ 0.1010,  0.0831,  0.0990, -0.0108,  0.0533, -0.0248, -0.0151,  0.0062],\n",
            "        [ 0.0516,  0.0255,  0.0351, -0.0101,  0.0265, -0.0164, -0.0077,  0.0084]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022077949717640877 tensor([[ 0.0800,  0.0754,  0.0827, -0.0068,  0.0418, -0.0346, -0.0151,  0.0308],\n",
            "        [ 0.0797,  0.0740,  0.0807, -0.0059,  0.0392, -0.0365, -0.0118,  0.0324],\n",
            "        [ 0.0499,  0.0470,  0.0536, -0.0055,  0.0260, -0.0209, -0.0093,  0.0209],\n",
            "        [ 0.0133,  0.0084,  0.0112, -0.0014,  0.0064, -0.0049, -0.0017,  0.0035]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018772397190332413 tensor([[ 0.1001,  0.0906,  0.1006, -0.0098,  0.0529, -0.0385, -0.0195,  0.0293],\n",
            "        [ 0.0967,  0.0861,  0.0943, -0.0078,  0.0467, -0.0409, -0.0128,  0.0300],\n",
            "        [ 0.0569,  0.0505,  0.0601, -0.0084,  0.0301, -0.0198, -0.0113,  0.0180],\n",
            "        [ 0.0191,  0.0094,  0.0150, -0.0020,  0.0085, -0.0050, -0.0016,  0.0023]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01864003948867321 tensor([[ 0.1088,  0.0941,  0.1058, -0.0120,  0.0582, -0.0366, -0.0218,  0.0219],\n",
            "        [ 0.1031,  0.0875,  0.0963, -0.0088,  0.0488, -0.0398, -0.0120,  0.0218],\n",
            "        [ 0.0588,  0.0485,  0.0608, -0.0109,  0.0314, -0.0159, -0.0124,  0.0123],\n",
            "        [ 0.0234,  0.0092,  0.0175, -0.0025,  0.0099, -0.0043, -0.0012,  0.0003]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01859864965081215 tensor([[ 0.1149,  0.0950,  0.1082, -0.0140,  0.0622, -0.0335, -0.0237,  0.0132],\n",
            "        [ 0.1080,  0.0875,  0.0969, -0.0098,  0.0502, -0.0380, -0.0109,  0.0128],\n",
            "        [ 0.0606,  0.0466,  0.0616, -0.0134,  0.0328, -0.0121, -0.0135,  0.0066],\n",
            "        [ 0.0273,  0.0084,  0.0194, -0.0030,  0.0109, -0.0033, -0.0007, -0.0021]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018563253805041313 tensor([[ 0.1201,  0.0951,  0.1096, -0.0160,  0.0657, -0.0300, -0.0254,  0.0041],\n",
            "        [ 0.1128,  0.0874,  0.0973, -0.0107,  0.0515, -0.0362, -0.0098,  0.0039],\n",
            "        [ 0.0627,  0.0450,  0.0626, -0.0160,  0.0342, -0.0084, -0.0147,  0.0012],\n",
            "        [ 0.0308,  0.0073,  0.0210, -0.0034,  0.0118, -0.0022, -0.0002, -0.0046]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021615708246827126 tensor([[ 0.0776,  0.0767,  0.0831, -0.0071,  0.0397, -0.0331, -0.0143,  0.0284],\n",
            "        [ 0.0689,  0.0610,  0.0679, -0.0052,  0.0342, -0.0307, -0.0107,  0.0261],\n",
            "        [ 0.0584,  0.0480,  0.0542, -0.0044,  0.0294, -0.0268, -0.0093,  0.0259],\n",
            "        [ 0.0176,  0.0133,  0.0152, -0.0018,  0.0093, -0.0067, -0.0032,  0.0066]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01905057206749916 tensor([[ 0.0749,  0.0717,  0.0771, -0.0090,  0.0385, -0.0260, -0.0146,  0.0146],\n",
            "        [ 0.0726,  0.0564,  0.0657, -0.0061,  0.0357, -0.0274, -0.0102,  0.0160],\n",
            "        [ 0.0781,  0.0571,  0.0667, -0.0060,  0.0389, -0.0336, -0.0115,  0.0299],\n",
            "        [ 0.0261,  0.0177,  0.0216, -0.0028,  0.0141, -0.0076, -0.0045,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01893959939479828 tensor([[ 0.0774,  0.0720,  0.0768, -0.0113,  0.0400, -0.0216, -0.0159,  0.0040],\n",
            "        [ 0.0772,  0.0528,  0.0643, -0.0071,  0.0375, -0.0247, -0.0099,  0.0068],\n",
            "        [ 0.0902,  0.0586,  0.0709, -0.0071,  0.0444, -0.0365, -0.0125,  0.0295],\n",
            "        [ 0.0321,  0.0195,  0.0253, -0.0036,  0.0175, -0.0071, -0.0052,  0.0059]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018889615312218666 tensor([[ 0.0808,  0.0733,  0.0776, -0.0135,  0.0419, -0.0177, -0.0173, -0.0060],\n",
            "        [ 0.0820,  0.0495,  0.0632, -0.0079,  0.0396, -0.0223, -0.0097, -0.0018],\n",
            "        [ 0.0990,  0.0568,  0.0715, -0.0080,  0.0483, -0.0377, -0.0129,  0.0274],\n",
            "        [ 0.0367,  0.0201,  0.0276, -0.0043,  0.0203, -0.0057, -0.0058,  0.0038]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018851086497306824 tensor([[ 0.0843,  0.0747,  0.0786, -0.0158,  0.0440, -0.0140, -0.0187, -0.0157],\n",
            "        [ 0.0873,  0.0466,  0.0625, -0.0088,  0.0418, -0.0201, -0.0095, -0.0101],\n",
            "        [ 0.1064,  0.0537,  0.0707, -0.0088,  0.0515, -0.0383, -0.0131,  0.0245],\n",
            "        [ 0.0405,  0.0199,  0.0292, -0.0050,  0.0226, -0.0040, -0.0062,  0.0013]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[4, 1, 1, 2, 4, 6, 2, 11, 8, 6, 2, 11, 6, 12, 6, 11, 5, 2, 6, 11, 6, 2, 6, 4, 4, 6, 6, 6, 12, 11, 1, 8, 5, 11, 11, 11, 4, 8, 11, 4, 4, 2, 2, 5, 2, 6, 6, 2, 4, 2, 11, 3, 11, 4, 2, 1, 4, 8, 11, 1, 4, 2, 8, 11, 4, 2, 11]\n",
            "12 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.020129088312387466 0.03106689453125 4.44466495513916 0.33295130729675293 1.780259370803833 1.9658203125\n",
            "repr, std, cov, clossl, z, norm 0.020717283710837364 0.029205322265625 4.487087249755859 0.47275829315185547 1.6808284521102905 1.8056640625\n",
            "repr, std, cov, clossl, z, norm 0.02103697694838047 0.0323486328125 4.289904594421387 0.32777318358421326 1.6726171970367432 2.841796875\n",
            "repr, std, cov, clossl, z, norm 0.02122535929083824 0.028533935546875 4.782835006713867 0.3465415835380554 1.7023687362670898 1.51953125\n",
            "repr, std, cov, clossl, z, norm 0.021680597215890884 0.027740478515625 4.580655097961426 0.33627626299858093 1.6947616338729858 2.314453125\n",
            "repr, std, cov, clossl, z, norm 0.022119978442788124 0.0276336669921875 4.687117099761963 0.5579097867012024 1.3336561918258667 0.85205078125\n",
            "repr, std, cov, clossl, z, norm 0.019582640379667282 0.032379150390625 4.4731879234313965 0.48731064796447754 2.0209648609161377 2.0703125\n",
            "repr, std, cov, clossl, z, norm 0.01849924586713314 0.023040771484375 5.172623157501221 0.32976263761520386 1.3314441442489624 2.15625\n",
            "repr, std, cov, clossl, z, norm 0.018420787528157234 0.02569580078125 4.769752502441406 0.32215502858161926 1.605021595954895 1.958984375\n",
            "repr, std, cov, clossl, z, norm 0.017621316015720367 0.02801513671875 4.6124773025512695 0.3387588858604431 1.796324372291565 1.548828125\n",
            "repr, std, cov, clossl, z, norm 0.01849677786231041 0.02777099609375 4.62224817276001 0.3199102282524109 1.4348846673965454 1.9248046875\n",
            "repr, std, cov, clossl, z, norm 0.017663832753896713 0.0242919921875 4.911128044128418 0.32377731800079346 1.5906718969345093 2.072265625\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023139024153351784 tensor([[ 0.1128,  0.1070,  0.1167, -0.0088,  0.0571, -0.0513, -0.0192,  0.0465],\n",
            "        [ 0.0771,  0.0724,  0.0772, -0.0078,  0.0399, -0.0359, -0.0149,  0.0326],\n",
            "        [ 0.0615,  0.0531,  0.0599, -0.0057,  0.0316, -0.0261, -0.0103,  0.0234],\n",
            "        [ 0.0271,  0.0231,  0.0230, -0.0023,  0.0139, -0.0144, -0.0057,  0.0132]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01861654594540596 tensor([[ 0.1293,  0.1165,  0.1281, -0.0106,  0.0648, -0.0539, -0.0207,  0.0413],\n",
            "        [ 0.0724,  0.0633,  0.0665, -0.0098,  0.0378, -0.0294, -0.0143,  0.0188],\n",
            "        [ 0.0799,  0.0621,  0.0733, -0.0085,  0.0414, -0.0282, -0.0124,  0.0211],\n",
            "        [ 0.0444,  0.0363,  0.0351, -0.0035,  0.0224, -0.0233, -0.0091,  0.0202]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018403859809041023 tensor([[ 0.1395,  0.1194,  0.1323, -0.0119,  0.0692, -0.0534, -0.0210,  0.0327],\n",
            "        [ 0.0720,  0.0586,  0.0606, -0.0121,  0.0378, -0.0250, -0.0145,  0.0075],\n",
            "        [ 0.0928,  0.0657,  0.0810, -0.0109,  0.0482, -0.0275, -0.0135,  0.0158],\n",
            "        [ 0.0569,  0.0449,  0.0421, -0.0043,  0.0284, -0.0296, -0.0117,  0.0243]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01830364018678665 tensor([[ 0.1474,  0.1201,  0.1341, -0.0131,  0.0724, -0.0517, -0.0210,  0.0228],\n",
            "        [ 0.0740,  0.0564,  0.0575, -0.0144,  0.0391, -0.0218, -0.0150, -0.0021],\n",
            "        [ 0.1029,  0.0665,  0.0855, -0.0131,  0.0535, -0.0253, -0.0140,  0.0089],\n",
            "        [ 0.0663,  0.0504,  0.0457, -0.0048,  0.0328, -0.0342, -0.0136,  0.0265]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018240246921777725 tensor([[ 0.1544,  0.1200,  0.1349, -0.0142,  0.0752, -0.0497, -0.0207,  0.0126],\n",
            "        [ 0.0771,  0.0554,  0.0556, -0.0168,  0.0410, -0.0193, -0.0158, -0.0110],\n",
            "        [ 0.1114,  0.0658,  0.0883, -0.0151,  0.0580, -0.0224, -0.0142,  0.0013],\n",
            "        [ 0.0736,  0.0539,  0.0472, -0.0053,  0.0360, -0.0377, -0.0152,  0.0273]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025427237153053284 tensor([[ 0.1165,  0.1099,  0.1221, -0.0114,  0.0598, -0.0507, -0.0210,  0.0455],\n",
            "        [ 0.1008,  0.0932,  0.1019, -0.0083,  0.0504, -0.0464, -0.0177,  0.0428],\n",
            "        [ 0.0605,  0.0568,  0.0638, -0.0056,  0.0328, -0.0265, -0.0121,  0.0257],\n",
            "        [ 0.0276,  0.0243,  0.0265, -0.0032,  0.0147, -0.0138, -0.0053,  0.0109]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018701544031500816 tensor([[ 0.1416,  0.1290,  0.1467, -0.0166,  0.0723, -0.0537, -0.0247,  0.0390],\n",
            "        [ 0.1381,  0.1234,  0.1359, -0.0121,  0.0676, -0.0591, -0.0232,  0.0483],\n",
            "        [ 0.0819,  0.0737,  0.0850, -0.0083,  0.0459, -0.0320, -0.0172,  0.0282],\n",
            "        [ 0.0431,  0.0356,  0.0400, -0.0056,  0.0231, -0.0205, -0.0082,  0.0138]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018207672983407974 tensor([[ 0.1469,  0.1275,  0.1491, -0.0203,  0.0747, -0.0468, -0.0247,  0.0225],\n",
            "        [ 0.1505,  0.1278,  0.1421, -0.0141,  0.0716, -0.0588, -0.0239,  0.0399],\n",
            "        [ 0.0886,  0.0755,  0.0902, -0.0099,  0.0513, -0.0296, -0.0196,  0.0222],\n",
            "        [ 0.0532,  0.0417,  0.0481, -0.0075,  0.0287, -0.0241, -0.0100,  0.0134]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018107982352375984 tensor([[ 0.1520,  0.1259,  0.1512, -0.0238,  0.0770, -0.0400, -0.0247,  0.0062],\n",
            "        [ 0.1591,  0.1284,  0.1441, -0.0159,  0.0736, -0.0566, -0.0239,  0.0296],\n",
            "        [ 0.0926,  0.0746,  0.0923, -0.0113,  0.0553, -0.0258, -0.0215,  0.0147],\n",
            "        [ 0.0607,  0.0452,  0.0534, -0.0093,  0.0330, -0.0262, -0.0112,  0.0112]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018030699342489243 tensor([[ 0.1578,  0.1249,  0.1540, -0.0274,  0.0797, -0.0335, -0.0248, -0.0095],\n",
            "        [ 0.1669,  0.1281,  0.1452, -0.0175,  0.0752, -0.0541, -0.0238,  0.0190],\n",
            "        [ 0.0960,  0.0729,  0.0936, -0.0126,  0.0589, -0.0217, -0.0233,  0.0069],\n",
            "        [ 0.0666,  0.0471,  0.0572, -0.0109,  0.0364, -0.0274, -0.0122,  0.0081]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02024942636489868 tensor([[ 0.1141,  0.1131,  0.1246, -0.0097,  0.0575, -0.0494, -0.0191,  0.0429],\n",
            "        [ 0.0940,  0.0867,  0.0944, -0.0084,  0.0490, -0.0433, -0.0183,  0.0405],\n",
            "        [ 0.0581,  0.0508,  0.0560, -0.0054,  0.0302, -0.0270, -0.0110,  0.0250],\n",
            "        [ 0.0216,  0.0207,  0.0222, -0.0022,  0.0111, -0.0096, -0.0037,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014760352671146393 tensor([[ 0.1202,  0.1163,  0.1303, -0.0118,  0.0589, -0.0438, -0.0180,  0.0272],\n",
            "        [ 0.1130,  0.0974,  0.1075, -0.0119,  0.0600, -0.0462, -0.0233,  0.0368],\n",
            "        [ 0.0792,  0.0644,  0.0722, -0.0080,  0.0417, -0.0346, -0.0151,  0.0284],\n",
            "        [ 0.0323,  0.0308,  0.0332, -0.0034,  0.0164, -0.0126, -0.0051,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01456121914088726 tensor([[ 0.1224,  0.1153,  0.1314, -0.0134,  0.0583, -0.0364, -0.0162,  0.0101],\n",
            "        [ 0.1205,  0.0963,  0.1077, -0.0145,  0.0650, -0.0430, -0.0262,  0.0268],\n",
            "        [ 0.0907,  0.0683,  0.0780, -0.0098,  0.0482, -0.0371, -0.0175,  0.0262],\n",
            "        [ 0.0391,  0.0371,  0.0403, -0.0043,  0.0196, -0.0134, -0.0057,  0.0045]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014473401010036469 tensor([[ 0.1265,  0.1164,  0.1347, -0.0152,  0.0588, -0.0301, -0.0147, -0.0056],\n",
            "        [ 0.1269,  0.0940,  0.1067, -0.0170,  0.0694, -0.0395, -0.0288,  0.0165],\n",
            "        [ 0.0989,  0.0689,  0.0804, -0.0114,  0.0530, -0.0379, -0.0192,  0.0222],\n",
            "        [ 0.0438,  0.0414,  0.0452, -0.0050,  0.0218, -0.0130, -0.0059,  0.0007]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014401762746274471 tensor([[ 0.1312,  0.1180,  0.1386, -0.0170,  0.0595, -0.0241, -0.0134, -0.0208],\n",
            "        [ 0.1331,  0.0917,  0.1056, -0.0195,  0.0737, -0.0358, -0.0314,  0.0063],\n",
            "        [ 0.1056,  0.0680,  0.0811, -0.0129,  0.0570, -0.0378, -0.0207,  0.0173],\n",
            "        [ 0.0473,  0.0445,  0.0489, -0.0056,  0.0233, -0.0120, -0.0059, -0.0040]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025040408596396446 tensor([[ 0.0982,  0.0919,  0.0997, -0.0085,  0.0502, -0.0449, -0.0190,  0.0409],\n",
            "        [ 0.0870,  0.0845,  0.0945, -0.0091,  0.0447, -0.0398, -0.0164,  0.0360],\n",
            "        [ 0.0745,  0.0722,  0.0790, -0.0065,  0.0377, -0.0330, -0.0129,  0.0285],\n",
            "        [ 0.0238,  0.0207,  0.0222, -0.0034,  0.0132, -0.0134, -0.0054,  0.0114]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019636079668998718 tensor([[ 0.1298,  0.1171,  0.1272, -0.0122,  0.0657, -0.0556, -0.0251,  0.0452],\n",
            "        [ 0.0954,  0.0902,  0.1041, -0.0130,  0.0491, -0.0386, -0.0181,  0.0275],\n",
            "        [ 0.0970,  0.0914,  0.1011, -0.0095,  0.0484, -0.0380, -0.0161,  0.0260],\n",
            "        [ 0.0369,  0.0297,  0.0324, -0.0060,  0.0204, -0.0209, -0.0087,  0.0162]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01927822455763817 tensor([[ 0.1457,  0.1258,  0.1368, -0.0147,  0.0730, -0.0582, -0.0283,  0.0412],\n",
            "        [ 0.0937,  0.0854,  0.1027, -0.0162,  0.0481, -0.0319, -0.0178,  0.0135],\n",
            "        [ 0.1060,  0.0969,  0.1082, -0.0115,  0.0522, -0.0360, -0.0168,  0.0162],\n",
            "        [ 0.0452,  0.0340,  0.0375, -0.0083,  0.0250, -0.0256, -0.0110,  0.0179]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019169768318533897 tensor([[ 0.1568,  0.1297,  0.1411, -0.0168,  0.0778, -0.0585, -0.0305,  0.0345],\n",
            "        [ 0.0937,  0.0824,  0.1033, -0.0195,  0.0481, -0.0261, -0.0177,  0.0005],\n",
            "        [ 0.1124,  0.0995,  0.1122, -0.0132,  0.0545, -0.0327, -0.0170,  0.0050],\n",
            "        [ 0.0509,  0.0358,  0.0400, -0.0105,  0.0282, -0.0290, -0.0128,  0.0180]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019102459773421288 tensor([[ 0.1656,  0.1312,  0.1428, -0.0188,  0.0813, -0.0576, -0.0323,  0.0266],\n",
            "        [ 0.0952,  0.0809,  0.1055, -0.0228,  0.0489, -0.0210, -0.0179, -0.0115],\n",
            "        [ 0.1178,  0.1011,  0.1151, -0.0148,  0.0563, -0.0291, -0.0170, -0.0064],\n",
            "        [ 0.0552,  0.0362,  0.0409, -0.0126,  0.0307, -0.0315, -0.0144,  0.0172]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025634655728936195 tensor([[ 0.0754,  0.0698,  0.0791, -0.0055,  0.0366, -0.0292, -0.0120,  0.0244],\n",
            "        [ 0.0965,  0.0946,  0.1051, -0.0094,  0.0495, -0.0417, -0.0170,  0.0361],\n",
            "        [ 0.0532,  0.0465,  0.0523, -0.0052,  0.0281, -0.0251, -0.0104,  0.0244],\n",
            "        [ 0.0208,  0.0190,  0.0210, -0.0027,  0.0117, -0.0109, -0.0039,  0.0083]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02226841449737549 tensor([[ 0.0666,  0.0536,  0.0650, -0.0044,  0.0300, -0.0157, -0.0082,  0.0041],\n",
            "        [ 0.1057,  0.1004,  0.1149, -0.0128,  0.0539, -0.0374, -0.0178,  0.0228],\n",
            "        [ 0.0603,  0.0455,  0.0544, -0.0068,  0.0324, -0.0257, -0.0122,  0.0222],\n",
            "        [ 0.0312,  0.0281,  0.0323, -0.0049,  0.0176, -0.0158, -0.0055,  0.0103]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.022108128294348717 tensor([[ 0.0702,  0.0503,  0.0649, -0.0042,  0.0298, -0.0087, -0.0067, -0.0089],\n",
            "        [ 0.1140,  0.1053,  0.1235, -0.0161,  0.0578, -0.0330, -0.0185,  0.0095],\n",
            "        [ 0.0656,  0.0428,  0.0545, -0.0083,  0.0358, -0.0254, -0.0136,  0.0189],\n",
            "        [ 0.0377,  0.0334,  0.0393, -0.0068,  0.0216, -0.0186, -0.0063,  0.0098]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.022029709070920944 tensor([[ 0.0753,  0.0486,  0.0665, -0.0041,  0.0305, -0.0026, -0.0055, -0.0208],\n",
            "        [ 0.1198,  0.1076,  0.1291, -0.0192,  0.0604, -0.0274, -0.0187, -0.0050],\n",
            "        [ 0.0700,  0.0391,  0.0536, -0.0097,  0.0386, -0.0245, -0.0149,  0.0152],\n",
            "        [ 0.0418,  0.0364,  0.0440, -0.0085,  0.0243, -0.0200, -0.0066,  0.0079]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.021964415907859802 tensor([[ 0.0811,  0.0477,  0.0689, -0.0040,  0.0316,  0.0031, -0.0046, -0.0321],\n",
            "        [ 0.1247,  0.1091,  0.1339, -0.0222,  0.0626, -0.0215, -0.0188, -0.0198],\n",
            "        [ 0.0743,  0.0353,  0.0525, -0.0110,  0.0414, -0.0236, -0.0161,  0.0114],\n",
            "        [ 0.0446,  0.0381,  0.0472, -0.0102,  0.0263, -0.0207, -0.0067,  0.0052]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02561153471469879 tensor([[ 0.0772,  0.0702,  0.0763, -0.0052,  0.0381, -0.0337, -0.0139,  0.0304],\n",
            "        [ 0.0617,  0.0563,  0.0624, -0.0054,  0.0314, -0.0272, -0.0108,  0.0234],\n",
            "        [ 0.0311,  0.0268,  0.0326, -0.0043,  0.0160, -0.0123, -0.0063,  0.0114],\n",
            "        [ 0.0177,  0.0127,  0.0122, -0.0011,  0.0076, -0.0074, -0.0019,  0.0039]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.023556236177682877 tensor([[ 0.0883,  0.0732,  0.0798, -0.0055,  0.0421, -0.0335, -0.0154,  0.0249],\n",
            "        [ 0.0718,  0.0611,  0.0696, -0.0071,  0.0366, -0.0275, -0.0120,  0.0177],\n",
            "        [ 0.0319,  0.0232,  0.0324, -0.0066,  0.0158, -0.0091, -0.0067,  0.0061],\n",
            "        [ 0.0283,  0.0184,  0.0171, -0.0015,  0.0112, -0.0105, -0.0021,  0.0033]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.023498736321926117 tensor([[ 0.0951,  0.0718,  0.0786, -0.0055,  0.0439, -0.0311, -0.0162,  0.0173],\n",
            "        [ 0.0780,  0.0621,  0.0725, -0.0084,  0.0397, -0.0258, -0.0125,  0.0098],\n",
            "        [ 0.0331,  0.0200,  0.0327, -0.0089,  0.0157, -0.0061, -0.0072,  0.0011],\n",
            "        [ 0.0365,  0.0217,  0.0194, -0.0017,  0.0136, -0.0123, -0.0018,  0.0011]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02346060611307621 tensor([[ 0.1017,  0.0703,  0.0771, -0.0055,  0.0455, -0.0287, -0.0169,  0.0096],\n",
            "        [ 0.0833,  0.0622,  0.0745, -0.0097,  0.0424, -0.0236, -0.0129,  0.0016],\n",
            "        [ 0.0350,  0.0175,  0.0338, -0.0112,  0.0159, -0.0035, -0.0077, -0.0035],\n",
            "        [ 0.0433,  0.0236,  0.0201, -0.0019,  0.0152, -0.0133, -0.0012, -0.0018]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.023427467793226242 tensor([[ 0.1083,  0.0688,  0.0757, -0.0055,  0.0473, -0.0264, -0.0176,  0.0021],\n",
            "        [ 0.0882,  0.0619,  0.0761, -0.0109,  0.0448, -0.0213, -0.0131, -0.0068],\n",
            "        [ 0.0372,  0.0154,  0.0352, -0.0136,  0.0163, -0.0011, -0.0084, -0.0079],\n",
            "        [ 0.0491,  0.0246,  0.0200, -0.0020,  0.0163, -0.0139, -0.0005, -0.0053]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019380709156394005 tensor([[ 0.1115,  0.1052,  0.1144, -0.0092,  0.0566, -0.0509, -0.0202,  0.0474],\n",
            "        [ 0.0822,  0.0737,  0.0815, -0.0068,  0.0419, -0.0370, -0.0151,  0.0356],\n",
            "        [ 0.0508,  0.0456,  0.0487, -0.0041,  0.0256, -0.0230, -0.0089,  0.0212],\n",
            "        [ 0.0274,  0.0211,  0.0250, -0.0035,  0.0145, -0.0126, -0.0047,  0.0104]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015032189898192883 tensor([[ 0.1160,  0.1019,  0.1107, -0.0106,  0.0584, -0.0478, -0.0208,  0.0372],\n",
            "        [ 0.0942,  0.0762,  0.0879, -0.0089,  0.0482, -0.0361, -0.0175,  0.0304],\n",
            "        [ 0.0635,  0.0537,  0.0569, -0.0051,  0.0321, -0.0258, -0.0108,  0.0206],\n",
            "        [ 0.0433,  0.0307,  0.0384, -0.0063,  0.0230, -0.0184, -0.0069,  0.0135]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.014915194362401962 tensor([[ 0.1210,  0.0992,  0.1076, -0.0120,  0.0605, -0.0450, -0.0215,  0.0276],\n",
            "        [ 0.1018,  0.0743,  0.0895, -0.0107,  0.0522, -0.0330, -0.0190,  0.0229],\n",
            "        [ 0.0703,  0.0560,  0.0588, -0.0056,  0.0355, -0.0256, -0.0116,  0.0166],\n",
            "        [ 0.0549,  0.0361,  0.0473, -0.0088,  0.0293, -0.0218, -0.0083,  0.0139]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014852295629680157 tensor([[ 0.1271,  0.0977,  0.1057, -0.0134,  0.0632, -0.0427, -0.0224,  0.0188],\n",
            "        [ 0.1086,  0.0717,  0.0902, -0.0124,  0.0557, -0.0295, -0.0204,  0.0151],\n",
            "        [ 0.0753,  0.0564,  0.0586, -0.0060,  0.0379, -0.0243, -0.0121,  0.0116],\n",
            "        [ 0.0638,  0.0388,  0.0534, -0.0111,  0.0341, -0.0236, -0.0091,  0.0125]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014804482460021973 tensor([[ 0.1334,  0.0965,  0.1043, -0.0148,  0.0660, -0.0407, -0.0234,  0.0102],\n",
            "        [ 0.1150,  0.0688,  0.0907, -0.0140,  0.0591, -0.0260, -0.0217,  0.0073],\n",
            "        [ 0.0796,  0.0562,  0.0578, -0.0063,  0.0400, -0.0227, -0.0124,  0.0062],\n",
            "        [ 0.0709,  0.0399,  0.0577, -0.0133,  0.0380, -0.0245, -0.0097,  0.0102]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024142201989889145 tensor([[ 0.1164,  0.1110,  0.1201, -0.0091,  0.0587, -0.0533, -0.0211,  0.0500],\n",
            "        [ 0.0851,  0.0795,  0.0870, -0.0087,  0.0437, -0.0387, -0.0162,  0.0360],\n",
            "        [ 0.0634,  0.0585,  0.0648, -0.0051,  0.0322, -0.0282, -0.0113,  0.0256],\n",
            "        [ 0.0179,  0.0160,  0.0184, -0.0024,  0.0093, -0.0074, -0.0029,  0.0051]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01809750683605671 tensor([[ 0.1591,  0.1493,  0.1616, -0.0130,  0.0793, -0.0688, -0.0285,  0.0596],\n",
            "        [ 0.1008,  0.0893,  0.0992, -0.0129,  0.0520, -0.0408, -0.0197,  0.0324],\n",
            "        [ 0.0826,  0.0727,  0.0817, -0.0065,  0.0415, -0.0330, -0.0142,  0.0260],\n",
            "        [ 0.0258,  0.0220,  0.0266, -0.0038,  0.0129, -0.0089, -0.0033,  0.0038]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017608409747481346 tensor([[ 0.1761,  0.1612,  0.1742, -0.0149,  0.0865, -0.0716, -0.0311,  0.0556],\n",
            "        [ 0.1018,  0.0838,  0.0951, -0.0161,  0.0526, -0.0351, -0.0205,  0.0206],\n",
            "        [ 0.0896,  0.0748,  0.0856, -0.0070,  0.0444, -0.0315, -0.0147,  0.0197],\n",
            "        [ 0.0305,  0.0251,  0.0318, -0.0050,  0.0147, -0.0085, -0.0029,  0.0005]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01751759648323059 tensor([[ 0.1865,  0.1662,  0.1794, -0.0164,  0.0903, -0.0709, -0.0324,  0.0481],\n",
            "        [ 0.1039,  0.0794,  0.0924, -0.0193,  0.0538, -0.0299, -0.0216,  0.0096],\n",
            "        [ 0.0950,  0.0753,  0.0877, -0.0074,  0.0464, -0.0292, -0.0150,  0.0127],\n",
            "        [ 0.0339,  0.0270,  0.0356, -0.0061,  0.0159, -0.0075, -0.0023, -0.0035]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017463907599449158 tensor([[ 0.1944,  0.1685,  0.1816, -0.0177,  0.0927, -0.0691, -0.0333,  0.0391],\n",
            "        [ 0.1074,  0.0765,  0.0913, -0.0226,  0.0558, -0.0254, -0.0229, -0.0004],\n",
            "        [ 0.0998,  0.0753,  0.0892, -0.0078,  0.0482, -0.0267, -0.0151,  0.0055],\n",
            "        [ 0.0367,  0.0283,  0.0388, -0.0072,  0.0167, -0.0060, -0.0015, -0.0080]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02941279672086239 tensor([[ 0.1192,  0.1129,  0.1225, -0.0098,  0.0613, -0.0547, -0.0224,  0.0517],\n",
            "        [ 0.0735,  0.0651,  0.0725, -0.0059,  0.0373, -0.0324, -0.0146,  0.0316],\n",
            "        [ 0.0742,  0.0721,  0.0798, -0.0069,  0.0376, -0.0332, -0.0123,  0.0280],\n",
            "        [ 0.0199,  0.0162,  0.0184, -0.0029,  0.0107, -0.0103, -0.0041,  0.0072]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.023524483665823936 tensor([[ 0.1540,  0.1413,  0.1540, -0.0139,  0.0790, -0.0655, -0.0290,  0.0562],\n",
            "        [ 0.0832,  0.0657,  0.0755, -0.0072,  0.0416, -0.0319, -0.0172,  0.0279],\n",
            "        [ 0.0958,  0.0922,  0.1038, -0.0097,  0.0475, -0.0390, -0.0140,  0.0261],\n",
            "        [ 0.0297,  0.0208,  0.0250, -0.0052,  0.0156, -0.0145, -0.0063,  0.0077]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.023171402513980865 tensor([[ 0.1670,  0.1470,  0.1609, -0.0164,  0.0856, -0.0652, -0.0316,  0.0491],\n",
            "        [ 0.0839,  0.0569,  0.0685, -0.0077,  0.0412, -0.0270, -0.0182,  0.0196],\n",
            "        [ 0.1028,  0.0977,  0.1117, -0.0114,  0.0497, -0.0374, -0.0130,  0.0163],\n",
            "        [ 0.0363,  0.0218,  0.0280, -0.0073,  0.0186, -0.0167, -0.0079,  0.0062]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02309161052107811 tensor([[ 0.1760,  0.1487,  0.1634, -0.0185,  0.0900, -0.0629, -0.0334,  0.0399],\n",
            "        [ 0.0864,  0.0500,  0.0635, -0.0083,  0.0417, -0.0230, -0.0195,  0.0124],\n",
            "        [ 0.1075,  0.1006,  0.1170, -0.0129,  0.0506, -0.0347, -0.0116,  0.0054],\n",
            "        [ 0.0415,  0.0214,  0.0296, -0.0094,  0.0208, -0.0181, -0.0092,  0.0037]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.0230342335999012 tensor([[ 0.1837,  0.1490,  0.1644, -0.0206,  0.0937, -0.0599, -0.0350,  0.0300],\n",
            "        [ 0.0906,  0.0447,  0.0603, -0.0090,  0.0432, -0.0199, -0.0211,  0.0064],\n",
            "        [ 0.1115,  0.1028,  0.1212, -0.0143,  0.0512, -0.0317, -0.0100, -0.0057],\n",
            "        [ 0.0458,  0.0203,  0.0303, -0.0113,  0.0226, -0.0190, -0.0104,  0.0007]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023428449407219887 tensor([[ 0.1080,  0.1013,  0.1108, -0.0084,  0.0545, -0.0476, -0.0187,  0.0435],\n",
            "        [ 0.0979,  0.0940,  0.1020, -0.0081,  0.0494, -0.0443, -0.0179,  0.0411],\n",
            "        [ 0.0703,  0.0668,  0.0737, -0.0064,  0.0365, -0.0320, -0.0126,  0.0289],\n",
            "        [ 0.0213,  0.0178,  0.0200, -0.0028,  0.0117, -0.0105, -0.0045,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017283737659454346 tensor([[ 0.1353,  0.1216,  0.1347, -0.0112,  0.0677, -0.0534, -0.0221,  0.0417],\n",
            "        [ 0.1183,  0.1114,  0.1216, -0.0111,  0.0593, -0.0479, -0.0213,  0.0379],\n",
            "        [ 0.0915,  0.0833,  0.0936, -0.0090,  0.0475, -0.0375, -0.0160,  0.0284],\n",
            "        [ 0.0312,  0.0239,  0.0282, -0.0048,  0.0174, -0.0143, -0.0068,  0.0132]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01698892191052437 tensor([[ 0.1453,  0.1238,  0.1390, -0.0126,  0.0719, -0.0503, -0.0222,  0.0308],\n",
            "        [ 0.1218,  0.1115,  0.1225, -0.0129,  0.0605, -0.0428, -0.0215,  0.0254],\n",
            "        [ 0.0995,  0.0860,  0.0991, -0.0106,  0.0516, -0.0359, -0.0170,  0.0203],\n",
            "        [ 0.0375,  0.0264,  0.0327, -0.0066,  0.0212, -0.0160, -0.0083,  0.0142]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016918376088142395 tensor([[ 0.1534,  0.1240,  0.1411, -0.0139,  0.0751, -0.0464, -0.0220,  0.0190],\n",
            "        [ 0.1249,  0.1113,  0.1231, -0.0147,  0.0615, -0.0376, -0.0216,  0.0130],\n",
            "        [ 0.1054,  0.0866,  0.1021, -0.0121,  0.0546, -0.0332, -0.0175,  0.0111],\n",
            "        [ 0.0420,  0.0272,  0.0355, -0.0083,  0.0240, -0.0167, -0.0094,  0.0142]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016857817769050598 tensor([[ 0.1610,  0.1238,  0.1428, -0.0152,  0.0781, -0.0422, -0.0217,  0.0071],\n",
            "        [ 0.1286,  0.1117,  0.1243, -0.0164,  0.0628, -0.0327, -0.0219,  0.0010],\n",
            "        [ 0.1106,  0.0865,  0.1043, -0.0134,  0.0572, -0.0302, -0.0180,  0.0015],\n",
            "        [ 0.0455,  0.0271,  0.0372, -0.0100,  0.0264, -0.0168, -0.0104,  0.0136]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020267777144908905 tensor([[ 0.0878,  0.0845,  0.0933, -0.0072,  0.0437, -0.0379, -0.0142,  0.0324],\n",
            "        [ 0.0939,  0.0898,  0.0986, -0.0083,  0.0465, -0.0432, -0.0164,  0.0393],\n",
            "        [ 0.0559,  0.0491,  0.0549, -0.0056,  0.0290, -0.0246, -0.0106,  0.0218],\n",
            "        [ 0.0175,  0.0118,  0.0140, -0.0015,  0.0088, -0.0061, -0.0023,  0.0033]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01655544713139534 tensor([[ 0.0953,  0.0877,  0.0989, -0.0083,  0.0462, -0.0351, -0.0134,  0.0219],\n",
            "        [ 0.0968,  0.0881,  0.0982, -0.0099,  0.0455, -0.0394, -0.0153,  0.0282],\n",
            "        [ 0.0666,  0.0522,  0.0604, -0.0082,  0.0336, -0.0255, -0.0122,  0.0176],\n",
            "        [ 0.0263,  0.0142,  0.0189, -0.0023,  0.0134, -0.0062, -0.0029,  0.0007]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01649044081568718 tensor([[ 0.1012,  0.0893,  0.1027, -0.0093,  0.0479, -0.0317, -0.0124,  0.0107],\n",
            "        [ 0.1001,  0.0869,  0.0982, -0.0116,  0.0448, -0.0360, -0.0144,  0.0177],\n",
            "        [ 0.0736,  0.0516,  0.0619, -0.0105,  0.0363, -0.0245, -0.0132,  0.0114],\n",
            "        [ 0.0332,  0.0146,  0.0218, -0.0029,  0.0169, -0.0053, -0.0032, -0.0031]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01644105091691017 tensor([[ 0.1065,  0.0905,  0.1060, -0.0103,  0.0493, -0.0279, -0.0112, -0.0007],\n",
            "        [ 0.1041,  0.0865,  0.0991, -0.0132,  0.0445, -0.0329, -0.0136,  0.0077],\n",
            "        [ 0.0796,  0.0500,  0.0623, -0.0127,  0.0385, -0.0230, -0.0140,  0.0047],\n",
            "        [ 0.0392,  0.0142,  0.0237, -0.0035,  0.0200, -0.0039, -0.0033, -0.0074]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016392288729548454 tensor([[ 0.1116,  0.0913,  0.1089, -0.0113,  0.0506, -0.0241, -0.0101, -0.0121],\n",
            "        [ 0.1084,  0.0864,  0.1003, -0.0148,  0.0443, -0.0300, -0.0129, -0.0020],\n",
            "        [ 0.0851,  0.0480,  0.0623, -0.0148,  0.0404, -0.0213, -0.0147, -0.0021],\n",
            "        [ 0.0446,  0.0132,  0.0250, -0.0040,  0.0228, -0.0022, -0.0033, -0.0120]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02586198039352894 tensor([[ 0.0600,  0.0610,  0.0666, -0.0066,  0.0313, -0.0265, -0.0128,  0.0248],\n",
            "        [ 0.0871,  0.0809,  0.0894, -0.0072,  0.0438, -0.0404, -0.0151,  0.0336],\n",
            "        [ 0.0650,  0.0609,  0.0665, -0.0057,  0.0339, -0.0311, -0.0116,  0.0276],\n",
            "        [ 0.0253,  0.0165,  0.0196, -0.0026,  0.0118, -0.0124, -0.0038,  0.0104]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.022869717329740524 tensor([[ 0.0521,  0.0524,  0.0576, -0.0082,  0.0278, -0.0185, -0.0132,  0.0139],\n",
            "        [ 0.0955,  0.0817,  0.0930, -0.0089,  0.0471, -0.0394, -0.0152,  0.0230],\n",
            "        [ 0.0730,  0.0643,  0.0714, -0.0073,  0.0389, -0.0323, -0.0130,  0.0224],\n",
            "        [ 0.0401,  0.0217,  0.0278, -0.0041,  0.0177, -0.0183, -0.0051,  0.0136]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02277465909719467 tensor([[ 0.0507,  0.0507,  0.0560, -0.0101,  0.0278, -0.0139, -0.0147,  0.0070],\n",
            "        [ 0.1034,  0.0820,  0.0958, -0.0105,  0.0502, -0.0384, -0.0153,  0.0125],\n",
            "        [ 0.0777,  0.0645,  0.0726, -0.0087,  0.0423, -0.0317, -0.0139,  0.0154],\n",
            "        [ 0.0513,  0.0234,  0.0323, -0.0054,  0.0217, -0.0223, -0.0056,  0.0146]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02272314392030239 tensor([[ 0.0510,  0.0507,  0.0563, -0.0121,  0.0287, -0.0102, -0.0166,  0.0012],\n",
            "        [ 0.1104,  0.0814,  0.0977, -0.0120,  0.0529, -0.0370, -0.0153,  0.0017],\n",
            "        [ 0.0812,  0.0635,  0.0726, -0.0099,  0.0451, -0.0306, -0.0145,  0.0078],\n",
            "        [ 0.0605,  0.0231,  0.0345, -0.0065,  0.0246, -0.0250, -0.0058,  0.0144]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.022681530565023422 tensor([[ 0.0519,  0.0512,  0.0572, -0.0142,  0.0298, -0.0068, -0.0185, -0.0043],\n",
            "        [ 0.1170,  0.0805,  0.0991, -0.0135,  0.0553, -0.0354, -0.0151, -0.0092],\n",
            "        [ 0.0846,  0.0624,  0.0724, -0.0111,  0.0478, -0.0294, -0.0151,  0.0003],\n",
            "        [ 0.0683,  0.0215,  0.0354, -0.0076,  0.0268, -0.0271, -0.0056,  0.0133]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022143837064504623 tensor([[ 0.0756,  0.0750,  0.0819, -0.0070,  0.0385, -0.0319, -0.0143,  0.0274],\n",
            "        [ 0.0706,  0.0667,  0.0742, -0.0074,  0.0363, -0.0318, -0.0125,  0.0264],\n",
            "        [ 0.0508,  0.0407,  0.0451, -0.0039,  0.0254, -0.0224, -0.0086,  0.0205],\n",
            "        [ 0.0156,  0.0107,  0.0125, -0.0024,  0.0076, -0.0068, -0.0023,  0.0055]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0198417529463768 tensor([[ 0.0782,  0.0754,  0.0829, -0.0087,  0.0392, -0.0269, -0.0150,  0.0158],\n",
            "        [ 0.0644,  0.0560,  0.0650, -0.0096,  0.0329, -0.0237, -0.0107,  0.0099],\n",
            "        [ 0.0623,  0.0409,  0.0470, -0.0045,  0.0301, -0.0238, -0.0095,  0.0183],\n",
            "        [ 0.0235,  0.0139,  0.0173, -0.0044,  0.0108, -0.0089, -0.0028,  0.0062]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01975966989994049 tensor([[ 0.0832,  0.0783,  0.0866, -0.0105,  0.0412, -0.0229, -0.0161,  0.0053],\n",
            "        [ 0.0666,  0.0539,  0.0652, -0.0123,  0.0339, -0.0200, -0.0105, -0.0017],\n",
            "        [ 0.0722,  0.0396,  0.0471, -0.0050,  0.0340, -0.0246, -0.0101,  0.0152],\n",
            "        [ 0.0294,  0.0152,  0.0201, -0.0062,  0.0130, -0.0100, -0.0029,  0.0057]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01970742642879486 tensor([[ 0.0873,  0.0804,  0.0894, -0.0123,  0.0428, -0.0187, -0.0171, -0.0055],\n",
            "        [ 0.0698,  0.0527,  0.0663, -0.0151,  0.0354, -0.0169, -0.0105, -0.0125],\n",
            "        [ 0.0805,  0.0367,  0.0455, -0.0055,  0.0370, -0.0245, -0.0105,  0.0113],\n",
            "        [ 0.0342,  0.0154,  0.0216, -0.0080,  0.0146, -0.0104, -0.0028,  0.0045]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019663147628307343 tensor([[ 0.0912,  0.0821,  0.0918, -0.0141,  0.0442, -0.0143, -0.0180, -0.0163],\n",
            "        [ 0.0734,  0.0520,  0.0680, -0.0179,  0.0372, -0.0140, -0.0105, -0.0229],\n",
            "        [ 0.0882,  0.0334,  0.0433, -0.0058,  0.0398, -0.0242, -0.0108,  0.0072],\n",
            "        [ 0.0382,  0.0149,  0.0225, -0.0098,  0.0158, -0.0104, -0.0026,  0.0028]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024716731160879135 tensor([[ 0.1160,  0.1088,  0.1190, -0.0094,  0.0592, -0.0522, -0.0212,  0.0493],\n",
            "        [ 0.0900,  0.0850,  0.0935, -0.0083,  0.0458, -0.0409, -0.0164,  0.0376],\n",
            "        [ 0.0658,  0.0599,  0.0642, -0.0052,  0.0328, -0.0306, -0.0117,  0.0284],\n",
            "        [ 0.0198,  0.0168,  0.0203, -0.0033,  0.0112, -0.0082, -0.0046,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018442567437887192 tensor([[ 0.1567,  0.1433,  0.1579, -0.0137,  0.0798, -0.0656, -0.0285,  0.0567],\n",
            "        [ 0.1104,  0.1013,  0.1137, -0.0122,  0.0562, -0.0447, -0.0198,  0.0349],\n",
            "        [ 0.0867,  0.0748,  0.0789, -0.0065,  0.0423, -0.0377, -0.0151,  0.0309],\n",
            "        [ 0.0285,  0.0214,  0.0285, -0.0060,  0.0166, -0.0098, -0.0070,  0.0087]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01796860620379448 tensor([[ 0.1726,  0.1519,  0.1684, -0.0162,  0.0875, -0.0664, -0.0310,  0.0508],\n",
            "        [ 0.1128,  0.0992,  0.1144, -0.0148,  0.0574, -0.0391, -0.0199,  0.0222],\n",
            "        [ 0.0944,  0.0765,  0.0793, -0.0066,  0.0450, -0.0381, -0.0161,  0.0262],\n",
            "        [ 0.0338,  0.0227,  0.0333, -0.0084,  0.0202, -0.0094, -0.0088,  0.0075]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017888205125927925 tensor([[ 0.1829,  0.1548,  0.1727, -0.0182,  0.0922, -0.0643, -0.0325,  0.0419],\n",
            "        [ 0.1148,  0.0968,  0.1147, -0.0174,  0.0583, -0.0331, -0.0198,  0.0094],\n",
            "        [ 0.1003,  0.0764,  0.0778, -0.0067,  0.0467, -0.0375, -0.0167,  0.0206],\n",
            "        [ 0.0379,  0.0227,  0.0368, -0.0108,  0.0231, -0.0083, -0.0103,  0.0056]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017832046374678612 tensor([[ 0.1914,  0.1557,  0.1749, -0.0202,  0.0960, -0.0614, -0.0337,  0.0320],\n",
            "        [ 0.1177,  0.0952,  0.1161, -0.0200,  0.0597, -0.0277, -0.0199, -0.0028],\n",
            "        [ 0.1059,  0.0760,  0.0759, -0.0067,  0.0483, -0.0369, -0.0173,  0.0150],\n",
            "        [ 0.0413,  0.0221,  0.0396, -0.0131,  0.0257, -0.0068, -0.0117,  0.0032]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021939679980278015 tensor([[ 0.1158,  0.1148,  0.1257, -0.0097,  0.0585, -0.0530, -0.0201,  0.0473],\n",
            "        [ 0.0872,  0.0816,  0.0921, -0.0081,  0.0452, -0.0383, -0.0162,  0.0361],\n",
            "        [ 0.0710,  0.0633,  0.0679, -0.0062,  0.0358, -0.0328, -0.0127,  0.0278],\n",
            "        [ 0.0237,  0.0179,  0.0208, -0.0028,  0.0122, -0.0103, -0.0042,  0.0091]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01654651388525963 tensor([[ 0.1263,  0.1225,  0.1350, -0.0118,  0.0625, -0.0530, -0.0207,  0.0387],\n",
            "        [ 0.0845,  0.0715,  0.0861, -0.0100,  0.0441, -0.0293, -0.0159,  0.0209],\n",
            "        [ 0.0965,  0.0817,  0.0874, -0.0088,  0.0480, -0.0415, -0.0166,  0.0290],\n",
            "        [ 0.0356,  0.0238,  0.0291, -0.0048,  0.0182, -0.0139, -0.0060,  0.0110]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01633002609014511 tensor([[ 0.1323,  0.1255,  0.1390, -0.0137,  0.0641, -0.0506, -0.0205,  0.0278],\n",
            "        [ 0.0846,  0.0644,  0.0833, -0.0119,  0.0446, -0.0218, -0.0162,  0.0077],\n",
            "        [ 0.1113,  0.0893,  0.0949, -0.0107,  0.0546, -0.0446, -0.0184,  0.0241],\n",
            "        [ 0.0438,  0.0262,  0.0337, -0.0066,  0.0224, -0.0153, -0.0070,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016243038699030876 tensor([[ 0.1372,  0.1275,  0.1420, -0.0154,  0.0652, -0.0478, -0.0202,  0.0164],\n",
            "        [ 0.0872,  0.0598,  0.0832, -0.0140,  0.0465, -0.0156, -0.0169, -0.0037],\n",
            "        [ 0.1216,  0.0924,  0.0977, -0.0123,  0.0588, -0.0455, -0.0194,  0.0168],\n",
            "        [ 0.0501,  0.0267,  0.0364, -0.0083,  0.0255, -0.0156, -0.0077,  0.0088]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016179725527763367 tensor([[ 0.1418,  0.1291,  0.1444, -0.0171,  0.0661, -0.0449, -0.0197,  0.0048],\n",
            "        [ 0.0911,  0.0565,  0.0846, -0.0161,  0.0489, -0.0102, -0.0180, -0.0143],\n",
            "        [ 0.1299,  0.0934,  0.0981, -0.0136,  0.0620, -0.0454, -0.0201,  0.0085],\n",
            "        [ 0.0555,  0.0263,  0.0381, -0.0099,  0.0281, -0.0154, -0.0082,  0.0064]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023774925619363785 tensor([[ 0.1035,  0.0999,  0.1086, -0.0090,  0.0530, -0.0477, -0.0192,  0.0415],\n",
            "        [ 0.0838,  0.0794,  0.0874, -0.0078,  0.0436, -0.0380, -0.0165,  0.0364],\n",
            "        [ 0.0632,  0.0579,  0.0622, -0.0054,  0.0317, -0.0298, -0.0106,  0.0269],\n",
            "        [ 0.0176,  0.0146,  0.0182, -0.0032,  0.0102, -0.0064, -0.0046,  0.0073]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018648751080036163 tensor([[ 0.1260,  0.1184,  0.1285, -0.0123,  0.0641, -0.0541, -0.0235,  0.0389],\n",
            "        [ 0.1032,  0.0939,  0.1055, -0.0115,  0.0546, -0.0412, -0.0213,  0.0346],\n",
            "        [ 0.0850,  0.0750,  0.0798, -0.0074,  0.0424, -0.0379, -0.0134,  0.0296],\n",
            "        [ 0.0256,  0.0187,  0.0261, -0.0060,  0.0153, -0.0073, -0.0072,  0.0089]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01840796321630478 tensor([[ 0.1315,  0.1191,  0.1289, -0.0141,  0.0663, -0.0519, -0.0247,  0.0277],\n",
            "        [ 0.1074,  0.0925,  0.1066, -0.0142,  0.0577, -0.0363, -0.0233,  0.0242],\n",
            "        [ 0.0940,  0.0792,  0.0836, -0.0083,  0.0464, -0.0392, -0.0140,  0.0250],\n",
            "        [ 0.0308,  0.0199,  0.0309, -0.0087,  0.0188, -0.0066, -0.0092,  0.0088]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018355002626776695 tensor([[ 0.1367,  0.1195,  0.1290, -0.0159,  0.0684, -0.0497, -0.0258,  0.0165],\n",
            "        [ 0.1113,  0.0907,  0.1074, -0.0169,  0.0606, -0.0312, -0.0252,  0.0137],\n",
            "        [ 0.1004,  0.0808,  0.0845, -0.0090,  0.0491, -0.0391, -0.0140,  0.0191],\n",
            "        [ 0.0346,  0.0198,  0.0345, -0.0112,  0.0216, -0.0051, -0.0109,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01830902323126793 tensor([[ 0.1420,  0.1201,  0.1292, -0.0177,  0.0706, -0.0475, -0.0269,  0.0055],\n",
            "        [ 0.1154,  0.0892,  0.1085, -0.0195,  0.0636, -0.0264, -0.0272,  0.0036],\n",
            "        [ 0.1058,  0.0814,  0.0843, -0.0097,  0.0513, -0.0386, -0.0139,  0.0127],\n",
            "        [ 0.0377,  0.0189,  0.0372, -0.0137,  0.0240, -0.0032, -0.0125,  0.0063]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023826315999031067 tensor([[ 0.0777,  0.0764,  0.0826, -0.0079,  0.0396, -0.0358, -0.0149,  0.0285],\n",
            "        [ 0.0798,  0.0733,  0.0793, -0.0064,  0.0407, -0.0354, -0.0148,  0.0307],\n",
            "        [ 0.0541,  0.0475,  0.0525, -0.0051,  0.0272, -0.0248, -0.0098,  0.0222],\n",
            "        [ 0.0138,  0.0110,  0.0138, -0.0016,  0.0064, -0.0056, -0.0019,  0.0042]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02109750360250473 tensor([[ 0.0664,  0.0628,  0.0662, -0.0087,  0.0338, -0.0274, -0.0138,  0.0111],\n",
            "        [ 0.0851,  0.0698,  0.0770, -0.0075,  0.0433, -0.0309, -0.0156,  0.0183],\n",
            "        [ 0.0712,  0.0581,  0.0654, -0.0076,  0.0352, -0.0302, -0.0126,  0.0233],\n",
            "        [ 0.0198,  0.0147,  0.0205, -0.0026,  0.0086, -0.0063, -0.0019,  0.0035]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02095687948167324 tensor([[ 0.0687,  0.0631,  0.0651, -0.0105,  0.0349, -0.0258, -0.0151,  0.0010],\n",
            "        [ 0.0937,  0.0701,  0.0784, -0.0087,  0.0476, -0.0285, -0.0170,  0.0084],\n",
            "        [ 0.0824,  0.0627,  0.0717, -0.0097,  0.0402, -0.0325, -0.0143,  0.0209],\n",
            "        [ 0.0236,  0.0163,  0.0250, -0.0035,  0.0097, -0.0058, -0.0015,  0.0013]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020904136821627617 tensor([[ 0.0711,  0.0635,  0.0643, -0.0123,  0.0362, -0.0243, -0.0164, -0.0086],\n",
            "        [ 0.1006,  0.0687,  0.0779, -0.0097,  0.0510, -0.0253, -0.0182, -0.0022],\n",
            "        [ 0.0899,  0.0636,  0.0740, -0.0116,  0.0432, -0.0328, -0.0153,  0.0164],\n",
            "        [ 0.0264,  0.0168,  0.0284, -0.0043,  0.0102, -0.0047, -0.0008, -0.0015]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02086363360285759 tensor([[ 7.4139e-02,  6.4592e-02,  6.4118e-02, -1.4140e-02,  3.7802e-02,\n",
            "         -2.3209e-02, -1.7911e-02, -1.7858e-02],\n",
            "        [ 1.0734e-01,  6.7250e-02,  7.7304e-02, -1.0760e-02,  5.4297e-02,\n",
            "         -2.2132e-02, -1.9279e-02, -1.2665e-02],\n",
            "        [ 9.6011e-02,  6.3121e-02,  7.4787e-02, -1.3320e-02,  4.5516e-02,\n",
            "         -3.2443e-02, -1.6124e-02,  1.1149e-02],\n",
            "        [ 2.8548e-02,  1.6856e-02,  3.1338e-02, -5.0050e-03,  1.0423e-02,\n",
            "         -3.2282e-03, -7.8678e-05, -4.6694e-03]], device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024908168241381645 tensor([[ 0.1017,  0.0916,  0.1006, -0.0089,  0.0511, -0.0460, -0.0187,  0.0433],\n",
            "        [ 0.0996,  0.0954,  0.1045, -0.0086,  0.0496, -0.0441, -0.0160,  0.0393],\n",
            "        [ 0.0506,  0.0479,  0.0542, -0.0049,  0.0275, -0.0221, -0.0111,  0.0223],\n",
            "        [ 0.0253,  0.0218,  0.0231, -0.0031,  0.0141, -0.0128, -0.0054,  0.0098]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019715160131454468 tensor([[ 0.1247,  0.1042,  0.1164, -0.0122,  0.0618, -0.0512, -0.0223,  0.0424],\n",
            "        [ 0.1252,  0.1181,  0.1307, -0.0124,  0.0612, -0.0494, -0.0182,  0.0367],\n",
            "        [ 0.0567,  0.0490,  0.0581, -0.0066,  0.0315, -0.0210, -0.0140,  0.0194],\n",
            "        [ 0.0375,  0.0297,  0.0323, -0.0052,  0.0215, -0.0174, -0.0082,  0.0110]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01947910711169243 tensor([[ 0.1338,  0.1026,  0.1166, -0.0145,  0.0655, -0.0494, -0.0232,  0.0342],\n",
            "        [ 0.1340,  0.1235,  0.1381, -0.0149,  0.0641, -0.0460, -0.0171,  0.0248],\n",
            "        [ 0.0576,  0.0444,  0.0562, -0.0080,  0.0326, -0.0170, -0.0159,  0.0135],\n",
            "        [ 0.0458,  0.0339,  0.0374, -0.0070,  0.0268, -0.0197, -0.0103,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019410237669944763 tensor([[ 0.1417,  0.0997,  0.1155, -0.0167,  0.0685, -0.0470, -0.0239,  0.0255],\n",
            "        [ 0.1402,  0.1264,  0.1428, -0.0173,  0.0655, -0.0414, -0.0157,  0.0116],\n",
            "        [ 0.0593,  0.0407,  0.0551, -0.0095,  0.0342, -0.0135, -0.0180,  0.0081],\n",
            "        [ 0.0522,  0.0362,  0.0406, -0.0086,  0.0311, -0.0210, -0.0120,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019350271672010422 tensor([[ 0.1494,  0.0967,  0.1143, -0.0188,  0.0715, -0.0446, -0.0247,  0.0169],\n",
            "        [ 0.1456,  0.1285,  0.1466, -0.0196,  0.0666, -0.0364, -0.0141, -0.0019],\n",
            "        [ 0.0617,  0.0376,  0.0547, -0.0110,  0.0360, -0.0104, -0.0202,  0.0033],\n",
            "        [ 0.0574,  0.0374,  0.0425, -0.0102,  0.0347, -0.0216, -0.0134,  0.0039]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021813245490193367 tensor([[ 0.1084,  0.1013,  0.1112, -0.0092,  0.0549, -0.0487, -0.0197,  0.0440],\n",
            "        [ 0.0907,  0.0845,  0.0916, -0.0074,  0.0461, -0.0412, -0.0167,  0.0378],\n",
            "        [ 0.0613,  0.0563,  0.0631, -0.0062,  0.0318, -0.0277, -0.0108,  0.0256],\n",
            "        [ 0.0276,  0.0232,  0.0255, -0.0027,  0.0141, -0.0123, -0.0050,  0.0106]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016235195100307465 tensor([[ 0.1312,  0.1164,  0.1292, -0.0124,  0.0654, -0.0536, -0.0234,  0.0412],\n",
            "        [ 0.1165,  0.1038,  0.1135, -0.0102,  0.0590, -0.0475, -0.0215,  0.0371],\n",
            "        [ 0.0825,  0.0725,  0.0836, -0.0098,  0.0435, -0.0342, -0.0143,  0.0277],\n",
            "        [ 0.0435,  0.0341,  0.0380, -0.0042,  0.0220, -0.0175, -0.0075,  0.0136]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015923090279102325 tensor([[ 0.1377,  0.1146,  0.1286, -0.0143,  0.0675, -0.0504, -0.0241,  0.0303],\n",
            "        [ 0.1249,  0.1052,  0.1160, -0.0118,  0.0627, -0.0446, -0.0230,  0.0265],\n",
            "        [ 0.0905,  0.0756,  0.0901, -0.0125,  0.0483, -0.0337, -0.0153,  0.0224],\n",
            "        [ 0.0545,  0.0400,  0.0454, -0.0054,  0.0272, -0.0198, -0.0091,  0.0133]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.0158458910882473 tensor([[ 0.1440,  0.1127,  0.1279, -0.0161,  0.0696, -0.0472, -0.0247,  0.0195],\n",
            "        [ 0.1316,  0.1047,  0.1165, -0.0133,  0.0656, -0.0409, -0.0242,  0.0151],\n",
            "        [ 0.0954,  0.0757,  0.0932, -0.0149,  0.0516, -0.0317, -0.0158,  0.0154],\n",
            "        [ 0.0629,  0.0434,  0.0501, -0.0065,  0.0311, -0.0207, -0.0101,  0.0114]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01578768715262413 tensor([[ 0.1507,  0.1112,  0.1278, -0.0179,  0.0718, -0.0442, -0.0255,  0.0091],\n",
            "        [ 0.1381,  0.1041,  0.1169, -0.0147,  0.0684, -0.0371, -0.0253,  0.0039],\n",
            "        [ 0.0994,  0.0747,  0.0953, -0.0172,  0.0544, -0.0291, -0.0161,  0.0080],\n",
            "        [ 0.0697,  0.0453,  0.0531, -0.0074,  0.0342, -0.0207, -0.0109,  0.0086]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023439517244696617 tensor([[ 0.0905,  0.0854,  0.0929, -0.0075,  0.0457, -0.0413, -0.0162,  0.0380],\n",
            "        [ 0.0834,  0.0830,  0.0928, -0.0084,  0.0433, -0.0369, -0.0156,  0.0345],\n",
            "        [ 0.0747,  0.0730,  0.0800, -0.0061,  0.0386, -0.0322, -0.0126,  0.0259],\n",
            "        [ 0.0339,  0.0246,  0.0259, -0.0040,  0.0181, -0.0174, -0.0077,  0.0163]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018727486953139305 tensor([[ 0.1155,  0.1050,  0.1146, -0.0103,  0.0576, -0.0492, -0.0201,  0.0406],\n",
            "        [ 0.0800,  0.0787,  0.0912, -0.0110,  0.0420, -0.0293, -0.0153,  0.0212],\n",
            "        [ 0.0897,  0.0863,  0.0960, -0.0080,  0.0459, -0.0324, -0.0137,  0.0171],\n",
            "        [ 0.0541,  0.0342,  0.0362, -0.0070,  0.0294, -0.0268, -0.0129,  0.0243]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01843627542257309 tensor([[ 0.1295,  0.1131,  0.1238, -0.0124,  0.0638, -0.0515, -0.0219,  0.0372],\n",
            "        [ 0.0756,  0.0733,  0.0886, -0.0136,  0.0403, -0.0211, -0.0148,  0.0073],\n",
            "        [ 0.0954,  0.0902,  0.1016, -0.0090,  0.0484, -0.0280, -0.0131,  0.0035],\n",
            "        [ 0.0693,  0.0387,  0.0412, -0.0097,  0.0382, -0.0333, -0.0171,  0.0289]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018306642770767212 tensor([[ 0.1392,  0.1170,  0.1283, -0.0141,  0.0677, -0.0517, -0.0230,  0.0316],\n",
            "        [ 0.0741,  0.0711,  0.0895, -0.0163,  0.0401, -0.0144, -0.0149, -0.0046],\n",
            "        [ 0.0999,  0.0927,  0.1057, -0.0100,  0.0501, -0.0231, -0.0124, -0.0105],\n",
            "        [ 0.0815,  0.0403,  0.0430, -0.0122,  0.0453, -0.0381, -0.0208,  0.0317]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018222257494926453 tensor([[ 0.1469,  0.1187,  0.1304, -0.0157,  0.0705, -0.0509, -0.0236,  0.0247],\n",
            "        [ 0.0743,  0.0705,  0.0923, -0.0192,  0.0408, -0.0085, -0.0152, -0.0156],\n",
            "        [ 0.1039,  0.0948,  0.1091, -0.0108,  0.0517, -0.0182, -0.0115, -0.0244],\n",
            "        [ 0.0917,  0.0400,  0.0427, -0.0146,  0.0514, -0.0418, -0.0240,  0.0332]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024068443104624748 tensor([[ 0.0994,  0.0906,  0.1006, -0.0088,  0.0502, -0.0454, -0.0172,  0.0397],\n",
            "        [ 0.0865,  0.0793,  0.0861, -0.0080,  0.0443, -0.0401, -0.0160,  0.0375],\n",
            "        [ 0.0472,  0.0431,  0.0470, -0.0050,  0.0240, -0.0225, -0.0086,  0.0196],\n",
            "        [ 0.0094,  0.0096,  0.0134, -0.0020,  0.0059, -0.0012, -0.0018,  0.0004]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019041135907173157 tensor([[ 0.1411,  0.1245,  0.1406, -0.0137,  0.0706, -0.0605, -0.0238,  0.0469],\n",
            "        [ 0.1261,  0.1131,  0.1228, -0.0128,  0.0649, -0.0552, -0.0235,  0.0470],\n",
            "        [ 0.0639,  0.0555,  0.0605, -0.0077,  0.0322, -0.0293, -0.0115,  0.0219],\n",
            "        [ 0.0105,  0.0112,  0.0185, -0.0034,  0.0075,  0.0027, -0.0018, -0.0045]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018356457352638245 tensor([[ 0.1553,  0.1301,  0.1496, -0.0165,  0.0767, -0.0616, -0.0252,  0.0395],\n",
            "        [ 0.1397,  0.1203,  0.1308, -0.0157,  0.0719, -0.0566, -0.0261,  0.0416],\n",
            "        [ 0.0688,  0.0561,  0.0613, -0.0096,  0.0342, -0.0297, -0.0121,  0.0175],\n",
            "        [ 0.0099,  0.0114,  0.0224, -0.0048,  0.0081,  0.0076, -0.0013, -0.0101]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018280958756804466 tensor([[ 0.1635,  0.1296,  0.1521, -0.0189,  0.0798, -0.0598, -0.0256,  0.0292],\n",
            "        [ 0.1474,  0.1215,  0.1323, -0.0181,  0.0758, -0.0548, -0.0277,  0.0329],\n",
            "        [ 0.0720,  0.0549,  0.0603, -0.0113,  0.0352, -0.0292, -0.0124,  0.0120],\n",
            "        [ 0.0094,  0.0116,  0.0263, -0.0061,  0.0088,  0.0125, -0.0009, -0.0157]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018234681338071823 tensor([[ 0.1706,  0.1279,  0.1532, -0.0211,  0.0822, -0.0575, -0.0257,  0.0185],\n",
            "        [ 0.1538,  0.1214,  0.1323, -0.0203,  0.0790, -0.0525, -0.0290,  0.0235],\n",
            "        [ 0.0751,  0.0537,  0.0593, -0.0130,  0.0363, -0.0287, -0.0127,  0.0067],\n",
            "        [ 0.0090,  0.0121,  0.0305, -0.0075,  0.0096,  0.0172, -0.0005, -0.0211]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025767192244529724 tensor([[ 0.0967,  0.0886,  0.0978, -0.0088,  0.0489, -0.0452, -0.0182,  0.0413],\n",
            "        [ 0.0838,  0.0776,  0.0853, -0.0070,  0.0420, -0.0375, -0.0138,  0.0326],\n",
            "        [ 0.0561,  0.0526,  0.0585, -0.0059,  0.0291, -0.0251, -0.0105,  0.0235],\n",
            "        [ 0.0130,  0.0080,  0.0080, -0.0016,  0.0050, -0.0058, -0.0019,  0.0035]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021366633474826813 tensor([[ 0.1220,  0.1050,  0.1175, -0.0124,  0.0611, -0.0535, -0.0232,  0.0428],\n",
            "        [ 0.1050,  0.0935,  0.1042, -0.0095,  0.0520, -0.0416, -0.0157,  0.0285],\n",
            "        [ 0.0704,  0.0637,  0.0722, -0.0089,  0.0364, -0.0285, -0.0130,  0.0234],\n",
            "        [ 0.0202,  0.0099,  0.0097, -0.0028,  0.0060, -0.0080, -0.0019,  0.0032]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.021158499643206596 tensor([[ 0.1312,  0.1048,  0.1189, -0.0148,  0.0650, -0.0537, -0.0252,  0.0360],\n",
            "        [ 0.1113,  0.0941,  0.1068, -0.0108,  0.0543, -0.0380, -0.0149,  0.0160],\n",
            "        [ 0.0750,  0.0650,  0.0754, -0.0113,  0.0385, -0.0268, -0.0137,  0.0180],\n",
            "        [ 0.0259,  0.0104,  0.0099, -0.0038,  0.0060, -0.0093, -0.0014,  0.0019]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.021112892776727676 tensor([[ 1.3874e-01,  1.0280e-01,  1.1837e-01, -1.7099e-02,  6.7935e-02,\n",
            "         -5.3046e-02, -2.6850e-02,  2.8307e-02],\n",
            "        [ 1.1672e-01,  9.3684e-02,  1.0839e-01, -1.2028e-02,  5.6150e-02,\n",
            "         -3.3858e-02, -1.3815e-02,  3.1662e-03],\n",
            "        [ 7.8394e-02,  6.5053e-02,  7.7268e-02, -1.3494e-02,  4.0048e-02,\n",
            "         -2.4469e-02, -1.4197e-02,  1.2046e-02],\n",
            "        [ 3.0870e-02,  1.0214e-02,  9.4652e-03, -4.6974e-03,  5.6303e-03,\n",
            "         -1.0243e-02, -8.3268e-04,  1.3232e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02106422930955887 tensor([[ 0.1459,  0.1005,  0.1175, -0.0193,  0.0707, -0.0522, -0.0285,  0.0205],\n",
            "        [ 0.1222,  0.0934,  0.1101, -0.0132,  0.0581, -0.0299, -0.0128, -0.0095],\n",
            "        [ 0.0814,  0.0648,  0.0788, -0.0157,  0.0414, -0.0220, -0.0146,  0.0060],\n",
            "        [ 0.0354,  0.0097,  0.0086, -0.0056,  0.0050, -0.0109, -0.0002, -0.0018]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02803168073296547 tensor([[ 0.1002,  0.0935,  0.1025, -0.0090,  0.0526, -0.0446, -0.0195,  0.0420],\n",
            "        [ 0.0798,  0.0728,  0.0811, -0.0073,  0.0398, -0.0364, -0.0141,  0.0323],\n",
            "        [ 0.0663,  0.0601,  0.0646, -0.0063,  0.0343, -0.0287, -0.0125,  0.0257],\n",
            "        [ 0.0257,  0.0234,  0.0250, -0.0028,  0.0133, -0.0129, -0.0050,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.023901555687189102 tensor([[ 0.1089,  0.0942,  0.1044, -0.0113,  0.0580, -0.0428, -0.0216,  0.0343],\n",
            "        [ 0.0806,  0.0669,  0.0779, -0.0088,  0.0384, -0.0312, -0.0129,  0.0197],\n",
            "        [ 0.0894,  0.0760,  0.0822, -0.0095,  0.0468, -0.0337, -0.0172,  0.0256],\n",
            "        [ 0.0398,  0.0356,  0.0379, -0.0047,  0.0206, -0.0194, -0.0077,  0.0156]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.023711849004030228 tensor([[ 0.1157,  0.0929,  0.1041, -0.0136,  0.0625, -0.0400, -0.0234,  0.0256],\n",
            "        [ 0.0827,  0.0622,  0.0762, -0.0104,  0.0378, -0.0268, -0.0119,  0.0083],\n",
            "        [ 0.1034,  0.0828,  0.0899, -0.0121,  0.0546, -0.0340, -0.0201,  0.0203],\n",
            "        [ 0.0490,  0.0430,  0.0456, -0.0064,  0.0254, -0.0231, -0.0094,  0.0167]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.023623812943696976 tensor([[ 0.1222,  0.0913,  0.1035, -0.0158,  0.0667, -0.0372, -0.0251,  0.0169],\n",
            "        [ 0.0864,  0.0592,  0.0760, -0.0120,  0.0380, -0.0232, -0.0112, -0.0019],\n",
            "        [ 0.1134,  0.0856,  0.0931, -0.0144,  0.0602, -0.0322, -0.0223,  0.0129],\n",
            "        [ 0.0555,  0.0476,  0.0504, -0.0078,  0.0287, -0.0252, -0.0106,  0.0160]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.023564163595438004 tensor([[ 0.1285,  0.0896,  0.1029, -0.0179,  0.0710, -0.0343, -0.0268,  0.0082],\n",
            "        [ 0.0908,  0.0569,  0.0768, -0.0136,  0.0385, -0.0202, -0.0106, -0.0113],\n",
            "        [ 0.1215,  0.0864,  0.0942, -0.0165,  0.0647, -0.0295, -0.0241,  0.0044],\n",
            "        [ 0.0601,  0.0505,  0.0534, -0.0092,  0.0310, -0.0263, -0.0114,  0.0142]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02367393672466278 tensor([[ 0.1069,  0.0977,  0.1077, -0.0096,  0.0532, -0.0495, -0.0184,  0.0442],\n",
            "        [ 0.0958,  0.0868,  0.0948, -0.0078,  0.0484, -0.0428, -0.0174,  0.0407],\n",
            "        [ 0.0663,  0.0614,  0.0674, -0.0063,  0.0332, -0.0308, -0.0119,  0.0278],\n",
            "        [ 0.0248,  0.0202,  0.0227, -0.0026,  0.0128, -0.0113, -0.0041,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01821601204574108 tensor([[ 0.1146,  0.0961,  0.1077, -0.0120,  0.0556, -0.0483, -0.0186,  0.0341],\n",
            "        [ 0.1231,  0.1060,  0.1175, -0.0111,  0.0622, -0.0486, -0.0222,  0.0411],\n",
            "        [ 0.0889,  0.0796,  0.0879, -0.0093,  0.0432, -0.0388, -0.0153,  0.0303],\n",
            "        [ 0.0381,  0.0282,  0.0325, -0.0041,  0.0193, -0.0159, -0.0057,  0.0100]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017944207414984703 tensor([[ 0.1163,  0.0885,  0.1012, -0.0137,  0.0551, -0.0442, -0.0179,  0.0215],\n",
            "        [ 0.1365,  0.1107,  0.1247, -0.0134,  0.0688, -0.0472, -0.0243,  0.0339],\n",
            "        [ 0.0976,  0.0838,  0.0931, -0.0113,  0.0458, -0.0396, -0.0161,  0.0248],\n",
            "        [ 0.0473,  0.0320,  0.0380, -0.0054,  0.0235, -0.0181, -0.0065,  0.0088]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01786070689558983 tensor([[ 0.1210,  0.0840,  0.0981, -0.0156,  0.0562, -0.0418, -0.0176,  0.0109],\n",
            "        [ 0.1471,  0.1127,  0.1289, -0.0155,  0.0738, -0.0445, -0.0259,  0.0253],\n",
            "        [ 0.1030,  0.0846,  0.0947, -0.0131,  0.0467, -0.0386, -0.0162,  0.0174],\n",
            "        [ 0.0544,  0.0338,  0.0413, -0.0065,  0.0267, -0.0192, -0.0069,  0.0064]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017802296206355095 tensor([[ 0.1268,  0.0808,  0.0964, -0.0175,  0.0580, -0.0399, -0.0176,  0.0011],\n",
            "        [ 0.1561,  0.1132,  0.1313, -0.0174,  0.0780, -0.0411, -0.0271,  0.0160],\n",
            "        [ 0.1073,  0.0844,  0.0952, -0.0148,  0.0470, -0.0371, -0.0161,  0.0096],\n",
            "        [ 0.0604,  0.0345,  0.0434, -0.0075,  0.0293, -0.0196, -0.0070,  0.0032]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020854607224464417 tensor([[ 0.0877,  0.0800,  0.0874, -0.0077,  0.0456, -0.0398, -0.0175,  0.0374],\n",
            "        [ 0.0734,  0.0723,  0.0803, -0.0067,  0.0374, -0.0322, -0.0136,  0.0294],\n",
            "        [ 0.0555,  0.0542,  0.0589, -0.0053,  0.0290, -0.0265, -0.0097,  0.0230],\n",
            "        [ 0.0180,  0.0143,  0.0169, -0.0024,  0.0089, -0.0088, -0.0032,  0.0066]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01682906411588192 tensor([[ 0.1302,  0.1157,  0.1271, -0.0124,  0.0679, -0.0560, -0.0263,  0.0493],\n",
            "        [ 0.0884,  0.0861,  0.0976, -0.0094,  0.0450, -0.0337, -0.0165,  0.0255],\n",
            "        [ 0.0655,  0.0624,  0.0682, -0.0071,  0.0345, -0.0292, -0.0109,  0.0199],\n",
            "        [ 0.0270,  0.0195,  0.0244, -0.0044,  0.0124, -0.0124, -0.0042,  0.0077]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01633131504058838 tensor([[ 0.1536,  0.1319,  0.1455, -0.0157,  0.0802, -0.0624, -0.0314,  0.0509],\n",
            "        [ 0.0891,  0.0853,  0.0993, -0.0112,  0.0452, -0.0274, -0.0168,  0.0133],\n",
            "        [ 0.0675,  0.0625,  0.0690, -0.0084,  0.0360, -0.0277, -0.0106,  0.0123],\n",
            "        [ 0.0329,  0.0216,  0.0288, -0.0061,  0.0142, -0.0143, -0.0045,  0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016188235953450203 tensor([[ 0.1690,  0.1398,  0.1550, -0.0184,  0.0884, -0.0648, -0.0350,  0.0481],\n",
            "        [ 0.0886,  0.0834,  0.0999, -0.0129,  0.0448, -0.0205, -0.0168,  0.0004],\n",
            "        [ 0.0692,  0.0623,  0.0694, -0.0095,  0.0372, -0.0260, -0.0103,  0.0048],\n",
            "        [ 0.0373,  0.0223,  0.0316, -0.0077,  0.0152, -0.0154, -0.0046,  0.0051]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01611408032476902 tensor([[ 0.1804,  0.1436,  0.1599, -0.0209,  0.0944, -0.0651, -0.0379,  0.0430],\n",
            "        [ 0.0889,  0.0823,  0.1015, -0.0146,  0.0448, -0.0139, -0.0170, -0.0121],\n",
            "        [ 0.0713,  0.0624,  0.0701, -0.0107,  0.0386, -0.0245, -0.0099, -0.0024],\n",
            "        [ 0.0409,  0.0222,  0.0335, -0.0092,  0.0157, -0.0161, -0.0044,  0.0030]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023488789796829224 tensor([[ 0.0686,  0.0637,  0.0679, -0.0055,  0.0347, -0.0300, -0.0136,  0.0265],\n",
            "        [ 0.0904,  0.0915,  0.1016, -0.0088,  0.0449, -0.0385, -0.0146,  0.0337],\n",
            "        [ 0.0716,  0.0639,  0.0695, -0.0061,  0.0371, -0.0338, -0.0135,  0.0313],\n",
            "        [ 0.0257,  0.0199,  0.0221, -0.0033,  0.0142, -0.0137, -0.0059,  0.0127]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02031524106860161 tensor([[ 0.0719,  0.0611,  0.0640, -0.0060,  0.0360, -0.0273, -0.0147,  0.0188],\n",
            "        [ 0.0725,  0.0729,  0.0841, -0.0100,  0.0336, -0.0214, -0.0087,  0.0077],\n",
            "        [ 0.0868,  0.0695,  0.0765, -0.0083,  0.0449, -0.0372, -0.0164,  0.0298],\n",
            "        [ 0.0393,  0.0275,  0.0316, -0.0057,  0.0224, -0.0202, -0.0096,  0.0178]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.0201091431081295 tensor([[ 0.0794,  0.0630,  0.0650, -0.0069,  0.0395, -0.0265, -0.0167,  0.0131],\n",
            "        [ 0.0742,  0.0744,  0.0885, -0.0125,  0.0324, -0.0144, -0.0065, -0.0067],\n",
            "        [ 0.0981,  0.0714,  0.0792, -0.0102,  0.0506, -0.0387, -0.0186,  0.0263],\n",
            "        [ 0.0489,  0.0313,  0.0370, -0.0079,  0.0285, -0.0246, -0.0125,  0.0204]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020035434514284134 tensor([[ 0.0855,  0.0635,  0.0645, -0.0076,  0.0423, -0.0251, -0.0183,  0.0067],\n",
            "        [ 0.0759,  0.0758,  0.0927, -0.0149,  0.0313, -0.0075, -0.0042, -0.0209],\n",
            "        [ 0.1060,  0.0698,  0.0781, -0.0118,  0.0545, -0.0385, -0.0201,  0.0209],\n",
            "        [ 0.0560,  0.0327,  0.0397, -0.0099,  0.0334, -0.0275, -0.0150,  0.0214]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019976802170276642 tensor([[ 0.0912,  0.0635,  0.0636, -0.0084,  0.0448, -0.0235, -0.0199,  0.0001],\n",
            "        [ 0.0782,  0.0778,  0.0977, -0.0173,  0.0305, -0.0010, -0.0021, -0.0345],\n",
            "        [ 0.1128,  0.0673,  0.0760, -0.0133,  0.0578, -0.0379, -0.0215,  0.0152],\n",
            "        [ 0.0617,  0.0326,  0.0409, -0.0118,  0.0374, -0.0296, -0.0172,  0.0214]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02093937061727047 tensor([[ 0.0835,  0.0754,  0.0837, -0.0071,  0.0412, -0.0356, -0.0138,  0.0313],\n",
            "        [ 0.0595,  0.0563,  0.0610, -0.0052,  0.0296, -0.0272, -0.0111,  0.0250],\n",
            "        [ 0.0682,  0.0626,  0.0707, -0.0075,  0.0347, -0.0289, -0.0112,  0.0245],\n",
            "        [ 0.0183,  0.0122,  0.0139, -0.0019,  0.0099, -0.0067, -0.0044,  0.0076]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017910558730363846 tensor([[ 0.1062,  0.0901,  0.1016, -0.0099,  0.0512, -0.0405, -0.0165,  0.0304],\n",
            "        [ 0.0543,  0.0478,  0.0516, -0.0054,  0.0260, -0.0213, -0.0102,  0.0150],\n",
            "        [ 0.0828,  0.0712,  0.0836, -0.0111,  0.0413, -0.0294, -0.0120,  0.0183],\n",
            "        [ 0.0275,  0.0142,  0.0174, -0.0031,  0.0150, -0.0076, -0.0070,  0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017736323177814484 tensor([[ 0.1203,  0.0961,  0.1100, -0.0121,  0.0568, -0.0412, -0.0176,  0.0248],\n",
            "        [ 0.0516,  0.0421,  0.0453, -0.0058,  0.0238, -0.0166, -0.0096,  0.0066],\n",
            "        [ 0.0903,  0.0728,  0.0889, -0.0141,  0.0442, -0.0266, -0.0115,  0.0085],\n",
            "        [ 0.0345,  0.0140,  0.0185, -0.0042,  0.0189, -0.0072, -0.0092,  0.0089]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01766277477145195 tensor([[ 1.3085e-01,  9.8475e-02,  1.1445e-01, -1.4089e-02,  6.0556e-02,\n",
            "         -4.0075e-02, -1.8084e-02,  1.7265e-02],\n",
            "        [ 5.1860e-02,  3.9363e-02,  4.2391e-02, -6.3413e-03,  2.3025e-02,\n",
            "         -1.3416e-02, -9.4652e-03, -1.0014e-04],\n",
            "        [ 9.6369e-02,  7.2928e-02,  9.2399e-02, -1.7076e-02,  4.6306e-02,\n",
            "         -2.3066e-02, -1.0771e-02, -1.9598e-03],\n",
            "        [ 4.0464e-02,  1.2716e-02,  1.8585e-02, -5.2047e-03,  2.2285e-02,\n",
            "         -6.2633e-03, -1.1175e-02,  8.1939e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017613422125577927 tensor([[ 0.1395,  0.0990,  0.1168, -0.0159,  0.0633, -0.0381, -0.0183,  0.0088],\n",
            "        [ 0.0535,  0.0382,  0.0412, -0.0070,  0.0230, -0.0110, -0.0096, -0.0059],\n",
            "        [ 0.1018,  0.0725,  0.0953, -0.0199,  0.0481, -0.0194, -0.0100, -0.0125],\n",
            "        [ 0.0458,  0.0109,  0.0180, -0.0062,  0.0253, -0.0050, -0.0131,  0.0071]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026500588282942772 tensor([[ 0.1034,  0.1001,  0.1083, -0.0089,  0.0521, -0.0473, -0.0190,  0.0404],\n",
            "        [ 0.0928,  0.0861,  0.0955, -0.0077,  0.0458, -0.0381, -0.0144,  0.0311],\n",
            "        [ 0.0774,  0.0695,  0.0756, -0.0073,  0.0397, -0.0369, -0.0150,  0.0360],\n",
            "        [ 0.0211,  0.0162,  0.0182, -0.0031,  0.0108, -0.0101, -0.0044,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021085890009999275 tensor([[ 0.1073,  0.0996,  0.1075, -0.0103,  0.0534, -0.0444, -0.0195,  0.0275],\n",
            "        [ 0.1051,  0.0908,  0.1049, -0.0103,  0.0506, -0.0325, -0.0141,  0.0146],\n",
            "        [ 0.1136,  0.0986,  0.1071, -0.0114,  0.0584, -0.0525, -0.0224,  0.0486],\n",
            "        [ 0.0333,  0.0223,  0.0261, -0.0055,  0.0165, -0.0150, -0.0068,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020693428814411163 tensor([[ 0.1096,  0.0975,  0.1048, -0.0115,  0.0539, -0.0410, -0.0197,  0.0145],\n",
            "        [ 0.1100,  0.0879,  0.1059, -0.0123,  0.0516, -0.0232, -0.0124, -0.0052],\n",
            "        [ 0.1337,  0.1115,  0.1211, -0.0144,  0.0687, -0.0596, -0.0268,  0.0520],\n",
            "        [ 0.0417,  0.0246,  0.0301, -0.0076,  0.0199, -0.0176, -0.0083,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020538505166769028 tensor([[ 0.1140,  0.0976,  0.1045, -0.0128,  0.0555, -0.0386, -0.0203,  0.0028],\n",
            "        [ 0.1152,  0.0853,  0.1072, -0.0141,  0.0529, -0.0143, -0.0108, -0.0242],\n",
            "        [ 0.1471,  0.1174,  0.1275, -0.0169,  0.0755, -0.0633, -0.0299,  0.0516],\n",
            "        [ 0.0482,  0.0250,  0.0321, -0.0096,  0.0223, -0.0191, -0.0094,  0.0111]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020440788939595222 tensor([[ 0.1189,  0.0983,  0.1048, -0.0142,  0.0574, -0.0365, -0.0210, -0.0084],\n",
            "        [ 0.1210,  0.0833,  0.1091, -0.0159,  0.0545, -0.0059, -0.0094, -0.0424],\n",
            "        [ 0.1568,  0.1198,  0.1300, -0.0191,  0.0803, -0.0651, -0.0323,  0.0492],\n",
            "        [ 0.0536,  0.0243,  0.0330, -0.0115,  0.0240, -0.0200, -0.0103,  0.0095]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020857401192188263 tensor([[ 0.0779,  0.0739,  0.0816, -0.0063,  0.0406, -0.0339, -0.0141,  0.0320],\n",
            "        [ 0.0502,  0.0499,  0.0553, -0.0052,  0.0258, -0.0209, -0.0099,  0.0175],\n",
            "        [ 0.0355,  0.0261,  0.0312, -0.0038,  0.0175, -0.0152, -0.0057,  0.0108],\n",
            "        [ 0.0170,  0.0152,  0.0163, -0.0018,  0.0082, -0.0060, -0.0023,  0.0025]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018744919449090958 tensor([[ 0.1040,  0.0958,  0.1071, -0.0090,  0.0543, -0.0414, -0.0185,  0.0359],\n",
            "        [ 0.0427,  0.0413,  0.0472, -0.0064,  0.0218, -0.0121, -0.0095,  0.0032],\n",
            "        [ 0.0443,  0.0248,  0.0333, -0.0056,  0.0208, -0.0159, -0.0057,  0.0059],\n",
            "        [ 0.0265,  0.0234,  0.0252, -0.0031,  0.0122, -0.0074, -0.0029,  0.0002]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018513984978199005 tensor([[ 0.1187,  0.1060,  0.1199, -0.0109,  0.0620, -0.0431, -0.0207,  0.0335],\n",
            "        [ 0.0397,  0.0374,  0.0445, -0.0080,  0.0203, -0.0055, -0.0098, -0.0086],\n",
            "        [ 0.0520,  0.0225,  0.0343, -0.0072,  0.0235, -0.0162, -0.0055,  0.0007],\n",
            "        [ 0.0329,  0.0286,  0.0309, -0.0043,  0.0146, -0.0073, -0.0030, -0.0038]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018428616225719452 tensor([[ 0.1280,  0.1106,  0.1265, -0.0125,  0.0668, -0.0420, -0.0220,  0.0281],\n",
            "        [ 0.0393,  0.0362,  0.0449, -0.0098,  0.0202, -0.0002, -0.0106, -0.0189],\n",
            "        [ 0.0592,  0.0197,  0.0348, -0.0088,  0.0259, -0.0162, -0.0052, -0.0047],\n",
            "        [ 0.0377,  0.0321,  0.0348, -0.0053,  0.0161, -0.0062, -0.0027, -0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01837976835668087 tensor([[ 0.1347,  0.1126,  0.1303, -0.0139,  0.0702, -0.0396, -0.0227,  0.0212],\n",
            "        [ 0.0401,  0.0362,  0.0466, -0.0117,  0.0207,  0.0046, -0.0116, -0.0285],\n",
            "        [ 0.0659,  0.0165,  0.0349, -0.0103,  0.0281, -0.0160, -0.0048, -0.0101],\n",
            "        [ 0.0415,  0.0347,  0.0376, -0.0062,  0.0172, -0.0046, -0.0023, -0.0146]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022760191932320595 tensor([[ 0.0692,  0.0638,  0.0711, -0.0067,  0.0355, -0.0326, -0.0135,  0.0279],\n",
            "        [ 0.0792,  0.0769,  0.0826, -0.0066,  0.0395, -0.0351, -0.0139,  0.0323],\n",
            "        [ 0.0463,  0.0412,  0.0471, -0.0060,  0.0242, -0.0207, -0.0090,  0.0189],\n",
            "        [ 0.0242,  0.0194,  0.0192, -0.0015,  0.0110, -0.0126, -0.0036,  0.0103]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020321954041719437 tensor([[ 0.0682,  0.0565,  0.0646, -0.0080,  0.0348, -0.0297, -0.0138,  0.0187],\n",
            "        [ 0.0782,  0.0715,  0.0771, -0.0078,  0.0380, -0.0282, -0.0131,  0.0189],\n",
            "        [ 0.0540,  0.0433,  0.0531, -0.0094,  0.0286, -0.0204, -0.0106,  0.0151],\n",
            "        [ 0.0385,  0.0291,  0.0282, -0.0021,  0.0165, -0.0195, -0.0049,  0.0143]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020235365256667137 tensor([[ 0.0725,  0.0547,  0.0643, -0.0097,  0.0369, -0.0295, -0.0151,  0.0124],\n",
            "        [ 0.0829,  0.0719,  0.0777, -0.0094,  0.0394, -0.0243, -0.0133,  0.0089],\n",
            "        [ 0.0596,  0.0432,  0.0567, -0.0126,  0.0318, -0.0191, -0.0118,  0.0102],\n",
            "        [ 0.0487,  0.0347,  0.0328, -0.0025,  0.0199, -0.0242, -0.0055,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020189102739095688 tensor([[ 0.0768,  0.0529,  0.0639, -0.0114,  0.0391, -0.0294, -0.0164,  0.0061],\n",
            "        [ 0.0874,  0.0722,  0.0782, -0.0110,  0.0408, -0.0203, -0.0136, -0.0009],\n",
            "        [ 0.0637,  0.0417,  0.0587, -0.0157,  0.0343, -0.0169, -0.0127,  0.0045],\n",
            "        [ 0.0564,  0.0380,  0.0348, -0.0028,  0.0220, -0.0276, -0.0055,  0.0158]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020153608173131943 tensor([[ 8.1133e-02,  5.1147e-02,  6.3649e-02, -1.3036e-02,  4.1205e-02,\n",
            "         -2.9234e-02, -1.7721e-02,  4.7684e-06],\n",
            "        [ 9.2081e-02,  7.2639e-02,  7.8887e-02, -1.2516e-02,  4.2224e-02,\n",
            "         -1.6537e-02, -1.3812e-02, -1.0581e-02],\n",
            "        [ 6.7279e-02,  3.9710e-02,  6.0188e-02, -1.8756e-02,  3.6542e-02,\n",
            "         -1.4609e-02, -1.3598e-02, -1.3542e-03],\n",
            "        [ 6.2575e-02,  3.9722e-02,  3.5136e-02, -2.9081e-03,  2.3347e-02,\n",
            "         -3.0072e-02, -5.3191e-03,  1.4908e-02]], device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[11, 11, 1, 11, 4, 6, 5, 6, 4, 2, 5, 8, 4, 2, 11, 1, 4, 6, 11, 6, 4, 4, 10, 12, 11, 6, 12, 5, 6, 6, 6, 4, 6, 4, 13, 6, 4, 4, 10, 6, 4, 6, 4, 11, 6, 6, 11, 11, 6, 5, 11, 4, 6, 4, 2, 6, 6, 2, 6, 4, 2, 2, 11, 6, 5, 4, 6, 6, 4, 4, 10, 4, 6, 6, 5, 4, 8, 2, 4, 4, 5, 11, 8, 9, 4, 4, 4, 1, 6, 4, 9, 9, 5, 4, 2, 4, 4, 2, 1, 10, 6, 4, 10, 1, 11, 11, 6, 10, 4, 9, 8, 1, 4, 11, 9, 8, 5, 12, 2, 11, 6, 5, 8]\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018082933500409126 tensor([[ 0.0764,  0.0752,  0.0837, -0.0061,  0.0385, -0.0327, -0.0122,  0.0285],\n",
            "        [ 0.0716,  0.0660,  0.0729, -0.0065,  0.0365, -0.0323, -0.0140,  0.0317],\n",
            "        [ 0.0282,  0.0212,  0.0236, -0.0033,  0.0147, -0.0121, -0.0055,  0.0091],\n",
            "        [ 0.0159,  0.0128,  0.0145, -0.0012,  0.0078, -0.0070, -0.0015,  0.0038]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01586872711777687 tensor([[ 0.0800,  0.0765,  0.0877, -0.0071,  0.0395, -0.0282, -0.0106,  0.0172],\n",
            "        [ 0.0804,  0.0695,  0.0779, -0.0086,  0.0404, -0.0326, -0.0159,  0.0288],\n",
            "        [ 0.0283,  0.0132,  0.0169, -0.0045,  0.0151, -0.0089, -0.0057,  0.0023],\n",
            "        [ 0.0243,  0.0189,  0.0222, -0.0016,  0.0116, -0.0098, -0.0012,  0.0032]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015808582305908203 tensor([[ 0.0829,  0.0771,  0.0908, -0.0080,  0.0402, -0.0234, -0.0090,  0.0057],\n",
            "        [ 0.0862,  0.0700,  0.0797, -0.0104,  0.0428, -0.0313, -0.0173,  0.0242],\n",
            "        [ 0.0309,  0.0079,  0.0130, -0.0058,  0.0169, -0.0071, -0.0064, -0.0031],\n",
            "        [ 0.0304,  0.0226,  0.0273, -0.0019,  0.0142, -0.0114, -0.0005,  0.0012]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015767719596624374 tensor([[ 0.0859,  0.0778,  0.0940, -0.0089,  0.0410, -0.0188, -0.0074, -0.0056],\n",
            "        [ 0.0909,  0.0695,  0.0804, -0.0122,  0.0446, -0.0295, -0.0185,  0.0191],\n",
            "        [ 0.0347,  0.0038,  0.0106, -0.0072,  0.0193, -0.0059, -0.0073, -0.0076],\n",
            "        [ 0.0350,  0.0249,  0.0307, -0.0021,  0.0160, -0.0122,  0.0005, -0.0017]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01573427952826023 tensor([[ 0.0890,  0.0787,  0.0974, -0.0098,  0.0418, -0.0142, -0.0059, -0.0166],\n",
            "        [ 0.0953,  0.0686,  0.0806, -0.0139,  0.0462, -0.0276, -0.0197,  0.0139],\n",
            "        [ 0.0391,  0.0003,  0.0088, -0.0087,  0.0220, -0.0050, -0.0083, -0.0117],\n",
            "        [ 0.0386,  0.0262,  0.0331, -0.0023,  0.0174, -0.0126,  0.0017, -0.0051]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01902579516172409 tensor([[ 0.1065,  0.0990,  0.1101, -0.0105,  0.0540, -0.0478, -0.0202,  0.0434],\n",
            "        [ 0.0943,  0.0886,  0.0965, -0.0078,  0.0481, -0.0440, -0.0170,  0.0406],\n",
            "        [ 0.0626,  0.0578,  0.0640, -0.0061,  0.0324, -0.0284, -0.0110,  0.0257],\n",
            "        [ 0.0239,  0.0194,  0.0221, -0.0028,  0.0129, -0.0116, -0.0045,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014054582454264164 tensor([[ 0.1188,  0.1031,  0.1178, -0.0142,  0.0596, -0.0473, -0.0226,  0.0359],\n",
            "        [ 0.1044,  0.0925,  0.1017, -0.0096,  0.0530, -0.0442, -0.0185,  0.0334],\n",
            "        [ 0.0759,  0.0661,  0.0749, -0.0085,  0.0392, -0.0311, -0.0124,  0.0230],\n",
            "        [ 0.0367,  0.0271,  0.0328, -0.0046,  0.0201, -0.0162, -0.0067,  0.0139]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01394613366574049 tensor([[ 0.1257,  0.1015,  0.1192, -0.0174,  0.0623, -0.0441, -0.0240,  0.0258],\n",
            "        [ 0.1092,  0.0907,  0.1008, -0.0110,  0.0552, -0.0415, -0.0189,  0.0233],\n",
            "        [ 0.0822,  0.0673,  0.0782, -0.0104,  0.0424, -0.0301, -0.0126,  0.0164],\n",
            "        [ 0.0454,  0.0307,  0.0393, -0.0061,  0.0253, -0.0185, -0.0081,  0.0147]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.013888568617403507 tensor([[ 0.1323,  0.0998,  0.1205, -0.0206,  0.0649, -0.0409, -0.0254,  0.0156],\n",
            "        [ 0.1142,  0.0893,  0.1002, -0.0124,  0.0574, -0.0391, -0.0194,  0.0135],\n",
            "        [ 0.0870,  0.0669,  0.0798, -0.0121,  0.0448, -0.0283, -0.0124,  0.0090],\n",
            "        [ 0.0519,  0.0322,  0.0434, -0.0074,  0.0293, -0.0196, -0.0091,  0.0141]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013840659521520138 tensor([[ 0.1388,  0.0980,  0.1216, -0.0237,  0.0675, -0.0377, -0.0268,  0.0056],\n",
            "        [ 0.1194,  0.0882,  0.0999, -0.0137,  0.0598, -0.0368, -0.0200,  0.0040],\n",
            "        [ 0.0912,  0.0660,  0.0808, -0.0138,  0.0469, -0.0263, -0.0122,  0.0015],\n",
            "        [ 0.0570,  0.0323,  0.0461, -0.0087,  0.0326, -0.0199, -0.0098,  0.0127]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.9802e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02497238665819168 tensor([[ 0.1004,  0.0984,  0.1052, -0.0082,  0.0497, -0.0446, -0.0175,  0.0382],\n",
            "        [ 0.0909,  0.0816,  0.0915, -0.0085,  0.0467, -0.0409, -0.0171,  0.0387],\n",
            "        [ 0.0568,  0.0522,  0.0579, -0.0051,  0.0288, -0.0254, -0.0095,  0.0219],\n",
            "        [ 0.0203,  0.0155,  0.0193, -0.0030,  0.0111, -0.0085, -0.0036,  0.0081]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020404748618602753 tensor([[ 0.1056,  0.1007,  0.1060, -0.0095,  0.0504, -0.0407, -0.0174,  0.0248],\n",
            "        [ 0.1112,  0.0918,  0.1070, -0.0122,  0.0572, -0.0443, -0.0209,  0.0364],\n",
            "        [ 0.0758,  0.0671,  0.0762, -0.0073,  0.0384, -0.0303, -0.0120,  0.0217],\n",
            "        [ 0.0292,  0.0195,  0.0269, -0.0056,  0.0167, -0.0107, -0.0050,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020232688635587692 tensor([[ 0.1077,  0.0998,  0.1032, -0.0103,  0.0496, -0.0354, -0.0169,  0.0104],\n",
            "        [ 0.1203,  0.0905,  0.1100, -0.0150,  0.0619, -0.0418, -0.0226,  0.0280],\n",
            "        [ 0.0844,  0.0717,  0.0834, -0.0087,  0.0427, -0.0297, -0.0125,  0.0155],\n",
            "        [ 0.0350,  0.0202,  0.0311, -0.0079,  0.0206, -0.0110, -0.0057,  0.0091]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020166758447885513 tensor([[ 0.1118,  0.1009,  0.1028, -0.0113,  0.0498, -0.0311, -0.0167, -0.0026],\n",
            "        [ 0.1280,  0.0878,  0.1113, -0.0177,  0.0658, -0.0387, -0.0240,  0.0190],\n",
            "        [ 0.0901,  0.0733,  0.0873, -0.0099,  0.0454, -0.0276, -0.0125,  0.0076],\n",
            "        [ 0.0393,  0.0195,  0.0340, -0.0103,  0.0238, -0.0106, -0.0062,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02011064812541008 tensor([[ 0.1165,  0.1028,  0.1030, -0.0122,  0.0505, -0.0273, -0.0166, -0.0150],\n",
            "        [ 0.1351,  0.0846,  0.1121, -0.0203,  0.0694, -0.0354, -0.0253,  0.0098],\n",
            "        [ 0.0946,  0.0737,  0.0900, -0.0110,  0.0474, -0.0248, -0.0123, -0.0008],\n",
            "        [ 0.0428,  0.0181,  0.0360, -0.0125,  0.0266, -0.0097, -0.0066,  0.0060]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021559536457061768 tensor([[ 0.0925,  0.0864,  0.0952, -0.0077,  0.0457, -0.0408, -0.0170,  0.0368],\n",
            "        [ 0.0934,  0.0903,  0.0977, -0.0075,  0.0473, -0.0422, -0.0165,  0.0367],\n",
            "        [ 0.0614,  0.0520,  0.0592, -0.0066,  0.0326, -0.0270, -0.0113,  0.0256],\n",
            "        [ 0.0212,  0.0167,  0.0177, -0.0018,  0.0105, -0.0104, -0.0037,  0.0085]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018014339730143547 tensor([[ 0.0913,  0.0774,  0.0862, -0.0082,  0.0432, -0.0345, -0.0165,  0.0245],\n",
            "        [ 0.0790,  0.0711,  0.0770, -0.0071,  0.0390, -0.0282, -0.0129,  0.0130],\n",
            "        [ 0.0790,  0.0589,  0.0713, -0.0101,  0.0429, -0.0298, -0.0146,  0.0251],\n",
            "        [ 0.0333,  0.0242,  0.0257, -0.0025,  0.0164, -0.0156, -0.0055,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017807332798838615 tensor([[ 0.0994,  0.0780,  0.0880, -0.0095,  0.0455, -0.0329, -0.0178,  0.0169],\n",
            "        [ 0.0838,  0.0717,  0.0777, -0.0079,  0.0407, -0.0243, -0.0128,  0.0006],\n",
            "        [ 0.0922,  0.0618,  0.0787, -0.0134,  0.0507, -0.0305, -0.0170,  0.0222],\n",
            "        [ 0.0423,  0.0287,  0.0304, -0.0031,  0.0206, -0.0190, -0.0067,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017733853310346603 tensor([[ 0.1058,  0.0771,  0.0879, -0.0106,  0.0469, -0.0305, -0.0187,  0.0087],\n",
            "        [ 0.0877,  0.0713,  0.0773, -0.0085,  0.0418, -0.0199, -0.0126, -0.0120],\n",
            "        [ 0.1010,  0.0603,  0.0812, -0.0164,  0.0563, -0.0290, -0.0186,  0.0168],\n",
            "        [ 0.0491,  0.0311,  0.0330, -0.0036,  0.0237, -0.0212, -0.0076,  0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01768222451210022 tensor([[ 0.1122,  0.0762,  0.0879, -0.0118,  0.0484, -0.0281, -0.0197,  0.0005],\n",
            "        [ 0.0924,  0.0718,  0.0778, -0.0092,  0.0435, -0.0161, -0.0126, -0.0239],\n",
            "        [ 0.1083,  0.0573,  0.0821, -0.0192,  0.0611, -0.0268, -0.0199,  0.0107],\n",
            "        [ 0.0547,  0.0323,  0.0343, -0.0040,  0.0262, -0.0227, -0.0082,  0.0099]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020820695906877518 tensor([[ 0.0609,  0.0575,  0.0631, -0.0060,  0.0299, -0.0271, -0.0111,  0.0232],\n",
            "        [ 0.0838,  0.0798,  0.0890, -0.0070,  0.0428, -0.0344, -0.0155,  0.0302],\n",
            "        [ 0.0658,  0.0590,  0.0631, -0.0065,  0.0329, -0.0303, -0.0116,  0.0289],\n",
            "        [ 0.0237,  0.0197,  0.0219, -0.0027,  0.0125, -0.0111, -0.0051,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017829015851020813 tensor([[ 0.0543,  0.0471,  0.0520, -0.0067,  0.0254, -0.0207, -0.0099,  0.0114],\n",
            "        [ 0.0896,  0.0805,  0.0932, -0.0085,  0.0449, -0.0273, -0.0157,  0.0156],\n",
            "        [ 0.0851,  0.0705,  0.0757, -0.0100,  0.0426, -0.0361, -0.0148,  0.0305],\n",
            "        [ 0.0367,  0.0286,  0.0328, -0.0045,  0.0194, -0.0159, -0.0081,  0.0137]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01766808144748211 tensor([[ 0.0550,  0.0440,  0.0489, -0.0079,  0.0247, -0.0179, -0.0099,  0.0038],\n",
            "        [ 0.0954,  0.0811,  0.0972, -0.0098,  0.0470, -0.0204, -0.0160,  0.0015],\n",
            "        [ 0.0963,  0.0738,  0.0794, -0.0129,  0.0480, -0.0376, -0.0167,  0.0275],\n",
            "        [ 0.0456,  0.0334,  0.0395, -0.0061,  0.0243, -0.0183, -0.0103,  0.0145]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017598984763026237 tensor([[ 0.0576,  0.0429,  0.0481, -0.0092,  0.0251, -0.0162, -0.0104, -0.0026],\n",
            "        [ 0.1009,  0.0812,  0.1006, -0.0111,  0.0490, -0.0135, -0.0162, -0.0124],\n",
            "        [ 0.1041,  0.0738,  0.0794, -0.0156,  0.0516, -0.0375, -0.0179,  0.0227],\n",
            "        [ 0.0521,  0.0358,  0.0436, -0.0075,  0.0279, -0.0193, -0.0120,  0.0138]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017547667026519775 tensor([[ 0.0609,  0.0425,  0.0480, -0.0105,  0.0258, -0.0148, -0.0109, -0.0085],\n",
            "        [ 0.1063,  0.0814,  0.1040, -0.0124,  0.0509, -0.0066, -0.0165, -0.0260],\n",
            "        [ 0.1107,  0.0724,  0.0779, -0.0182,  0.0546, -0.0367, -0.0188,  0.0173],\n",
            "        [ 0.0571,  0.0369,  0.0462, -0.0089,  0.0307, -0.0194, -0.0135,  0.0122]],\n",
            "       device='cuda:0')\n",
            "c tensor([7.1526e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02135363779962063 tensor([[ 0.0563,  0.0568,  0.0620, -0.0067,  0.0290, -0.0220, -0.0113,  0.0187],\n",
            "        [ 0.0754,  0.0644,  0.0710, -0.0056,  0.0383, -0.0345, -0.0135,  0.0315],\n",
            "        [ 0.0642,  0.0592,  0.0652, -0.0056,  0.0305, -0.0284, -0.0097,  0.0263],\n",
            "        [ 0.0206,  0.0168,  0.0180, -0.0025,  0.0113, -0.0092, -0.0053,  0.0098]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018855741247534752 tensor([[ 0.0445,  0.0437,  0.0479, -0.0079,  0.0232, -0.0095, -0.0102,  0.0016],\n",
            "        [ 0.0889,  0.0661,  0.0754, -0.0066,  0.0450, -0.0365, -0.0150,  0.0276],\n",
            "        [ 0.0840,  0.0745,  0.0830, -0.0078,  0.0378, -0.0335, -0.0108,  0.0272],\n",
            "        [ 0.0316,  0.0236,  0.0258, -0.0041,  0.0174, -0.0126, -0.0087,  0.0137]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01867169886827469 tensor([[ 0.0444,  0.0427,  0.0470, -0.0099,  0.0234, -0.0032, -0.0112, -0.0084],\n",
            "        [ 0.0996,  0.0651,  0.0767, -0.0074,  0.0502, -0.0372, -0.0159,  0.0225],\n",
            "        [ 0.0943,  0.0801,  0.0904, -0.0095,  0.0401, -0.0336, -0.0102,  0.0227],\n",
            "        [ 0.0389,  0.0269,  0.0297, -0.0055,  0.0215, -0.0139, -0.0114,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01861359179019928 tensor([[ 0.0459,  0.0433,  0.0478, -0.0119,  0.0245,  0.0023, -0.0124, -0.0173],\n",
            "        [ 0.1084,  0.0622,  0.0759, -0.0080,  0.0544, -0.0370, -0.0164,  0.0165],\n",
            "        [ 0.1008,  0.0820,  0.0937, -0.0108,  0.0405, -0.0318, -0.0088,  0.0161],\n",
            "        [ 0.0443,  0.0282,  0.0316, -0.0068,  0.0245, -0.0141, -0.0137,  0.0156]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018572958186268806 tensor([[ 0.0479,  0.0444,  0.0492, -0.0140,  0.0260,  0.0074, -0.0138, -0.0257],\n",
            "        [ 0.1166,  0.0587,  0.0744, -0.0086,  0.0583, -0.0365, -0.0168,  0.0102],\n",
            "        [ 0.1061,  0.0825,  0.0955, -0.0120,  0.0402, -0.0294, -0.0073,  0.0089],\n",
            "        [ 0.0485,  0.0285,  0.0324, -0.0080,  0.0270, -0.0137, -0.0157,  0.0152]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021470852196216583 tensor([[ 0.0550,  0.0572,  0.0642, -0.0055,  0.0291, -0.0228, -0.0104,  0.0198],\n",
            "        [ 0.0425,  0.0308,  0.0340, -0.0036,  0.0199, -0.0182, -0.0074,  0.0163],\n",
            "        [ 0.0531,  0.0521,  0.0607, -0.0058,  0.0269, -0.0212, -0.0079,  0.0176],\n",
            "        [ 0.0233,  0.0124,  0.0139, -0.0010,  0.0110, -0.0096, -0.0033,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02003202587366104 tensor([[ 0.0538,  0.0571,  0.0663, -0.0069,  0.0295, -0.0171, -0.0104,  0.0096],\n",
            "        [ 0.0440,  0.0196,  0.0231, -0.0043,  0.0188, -0.0151, -0.0071,  0.0102],\n",
            "        [ 0.0553,  0.0539,  0.0672, -0.0079,  0.0275, -0.0160, -0.0062,  0.0067],\n",
            "        [ 0.0376,  0.0155,  0.0182, -0.0011,  0.0174, -0.0139, -0.0047,  0.0107]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.019949067384004593 tensor([[ 0.0549,  0.0595,  0.0709, -0.0085,  0.0309, -0.0126, -0.0109,  0.0008],\n",
            "        [ 0.0501,  0.0133,  0.0174, -0.0053,  0.0202, -0.0145, -0.0078,  0.0069],\n",
            "        [ 0.0576,  0.0557,  0.0736, -0.0099,  0.0282, -0.0110, -0.0045, -0.0039],\n",
            "        [ 0.0492,  0.0159,  0.0195, -0.0011,  0.0223, -0.0167, -0.0055,  0.0115]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019889982417225838 tensor([[ 0.0556,  0.0614,  0.0751, -0.0100,  0.0322, -0.0081, -0.0113, -0.0082],\n",
            "        [ 0.0575,  0.0083,  0.0132, -0.0063,  0.0222, -0.0145, -0.0086,  0.0043],\n",
            "        [ 0.0593,  0.0569,  0.0792, -0.0118,  0.0286, -0.0057, -0.0028, -0.0146],\n",
            "        [ 0.0590,  0.0146,  0.0190, -0.0010,  0.0263, -0.0185, -0.0060,  0.0111]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019836798310279846 tensor([[ 0.0562,  0.0633,  0.0792, -0.0115,  0.0334, -0.0035, -0.0117, -0.0171],\n",
            "        [ 0.0652,  0.0037,  0.0095, -0.0073,  0.0244, -0.0147, -0.0095,  0.0020],\n",
            "        [ 0.0609,  0.0579,  0.0846, -0.0137,  0.0289, -0.0005, -0.0011, -0.0253],\n",
            "        [ 0.0678,  0.0124,  0.0175, -0.0007,  0.0297, -0.0198, -0.0063,  0.0101]],\n",
            "       device='cuda:0')\n",
            "c tensor([3.6955e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02891862764954567 tensor([[ 0.0399,  0.0387,  0.0457, -0.0060,  0.0214, -0.0147, -0.0095,  0.0140],\n",
            "        [ 0.0529,  0.0443,  0.0476, -0.0032,  0.0256, -0.0233, -0.0077,  0.0186],\n",
            "        [ 0.0517,  0.0420,  0.0479, -0.0061,  0.0262, -0.0223, -0.0094,  0.0211],\n",
            "        [ 0.0100,  0.0089,  0.0101, -0.0002,  0.0045, -0.0015, -0.0016,  0.0013]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02767949551343918 tensor([[ 3.6774e-02,  3.4063e-02,  4.4050e-02, -8.5711e-03,  2.0867e-02,\n",
            "         -7.7724e-03, -1.1184e-02,  5.7364e-03],\n",
            "        [ 5.3875e-02,  3.5625e-02,  3.8438e-02, -2.6256e-03,  2.4432e-02,\n",
            "         -1.9355e-02, -5.3334e-03,  8.0681e-03],\n",
            "        [ 7.1926e-02,  5.2958e-02,  6.2437e-02, -9.9230e-03,  3.6364e-02,\n",
            "         -2.7671e-02, -1.2956e-02,  2.3808e-02],\n",
            "        [ 1.3959e-02,  1.1797e-02,  1.3943e-02,  5.4836e-05,  5.8186e-03,\n",
            "          6.3896e-04, -2.0188e-03, -1.4341e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.027571827173233032 tensor([[ 0.0377,  0.0336,  0.0469, -0.0115,  0.0224, -0.0029, -0.0136, -0.0003],\n",
            "        [ 0.0595,  0.0318,  0.0345, -0.0023,  0.0257, -0.0180, -0.0039,  0.0006],\n",
            "        [ 0.0846,  0.0563,  0.0687, -0.0132,  0.0425, -0.0291, -0.0151,  0.0222],\n",
            "        [ 0.0165,  0.0132,  0.0163,  0.0004,  0.0064,  0.0037, -0.0021, -0.0053]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.027525749057531357 tensor([[ 0.0389,  0.0333,  0.0499, -0.0144,  0.0240,  0.0017, -0.0160, -0.0061],\n",
            "        [ 0.0658,  0.0288,  0.0312, -0.0020,  0.0274, -0.0170, -0.0026, -0.0062],\n",
            "        [ 0.0937,  0.0561,  0.0710, -0.0162,  0.0468, -0.0287, -0.0166,  0.0186],\n",
            "        [ 0.0185,  0.0140,  0.0180,  0.0007,  0.0067,  0.0070, -0.0022, -0.0094]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.027491504326462746 tensor([[ 0.0400,  0.0331,  0.0530, -0.0172,  0.0257,  0.0064, -0.0184, -0.0118],\n",
            "        [ 0.0725,  0.0262,  0.0285, -0.0017,  0.0293, -0.0163, -0.0014, -0.0127],\n",
            "        [ 0.1011,  0.0542,  0.0715, -0.0191,  0.0502, -0.0275, -0.0177,  0.0142],\n",
            "        [ 0.0203,  0.0147,  0.0197,  0.0011,  0.0069,  0.0104, -0.0022, -0.0137]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        1.1921e-07, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021625960245728493 tensor([[ 5.1537e-02,  4.8027e-02,  5.2032e-02, -5.8556e-03,  2.5978e-02,\n",
            "         -2.5463e-02, -9.7418e-03,  2.3994e-02],\n",
            "        [-5.4979e-03, -7.0190e-03, -5.5647e-03, -9.1493e-04, -3.4785e-03,\n",
            "          6.5994e-03, -7.1526e-05, -6.4754e-03],\n",
            "        [ 1.1368e-02,  1.0576e-02,  1.0386e-02,  8.6010e-04,  5.2977e-03,\n",
            "         -4.0746e-03, -9.1791e-05, -5.5671e-04],\n",
            "        [ 9.4557e-03, -4.2582e-03,  1.4353e-03, -2.7895e-03,  4.2367e-03,\n",
            "          1.3208e-03, -1.9681e-03,  3.2663e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02091914787888527 tensor([[ 0.0708,  0.0636,  0.0687, -0.0094,  0.0354, -0.0346, -0.0136,  0.0303],\n",
            "        [-0.0133, -0.0162, -0.0129, -0.0015, -0.0083,  0.0146,  0.0006, -0.0139],\n",
            "        [ 0.0124,  0.0111,  0.0099,  0.0025,  0.0053, -0.0030,  0.0017, -0.0065],\n",
            "        [ 0.0174, -0.0102,  0.0012, -0.0055,  0.0076,  0.0036, -0.0036, -0.0004]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020796772092580795 tensor([[ 0.0804,  0.0693,  0.0747, -0.0124,  0.0397, -0.0387, -0.0157,  0.0309],\n",
            "        [-0.0167, -0.0207, -0.0150, -0.0024, -0.0108,  0.0203,  0.0006, -0.0184],\n",
            "        [ 0.0147,  0.0127,  0.0106,  0.0041,  0.0059, -0.0027,  0.0033, -0.0115],\n",
            "        [ 0.0249, -0.0165,  0.0005, -0.0081,  0.0107,  0.0062, -0.0051, -0.0013]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020757094025611877 tensor([[ 0.0860,  0.0711,  0.0763, -0.0150,  0.0420, -0.0408, -0.0171,  0.0294],\n",
            "        [-0.0177, -0.0230, -0.0146, -0.0034, -0.0121,  0.0248,  0.0002, -0.0215],\n",
            "        [ 0.0168,  0.0141,  0.0111,  0.0056,  0.0064, -0.0024,  0.0048, -0.0165],\n",
            "        [ 0.0321, -0.0230, -0.0003, -0.0108,  0.0138,  0.0088, -0.0066, -0.0024]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020733904093503952 tensor([[ 0.0900,  0.0713,  0.0763, -0.0176,  0.0435, -0.0420, -0.0182,  0.0269],\n",
            "        [-0.0177, -0.0241, -0.0130, -0.0045, -0.0128,  0.0287, -0.0004, -0.0239],\n",
            "        [ 0.0188,  0.0154,  0.0115,  0.0072,  0.0069, -0.0020,  0.0063, -0.0215],\n",
            "        [ 0.0393, -0.0294, -0.0011, -0.0134,  0.0168,  0.0114, -0.0081, -0.0034]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023697281256318092 tensor([[ 0.0486,  0.0481,  0.0520, -0.0050,  0.0231, -0.0218, -0.0082,  0.0177],\n",
            "        [ 0.0595,  0.0497,  0.0550, -0.0058,  0.0288, -0.0267, -0.0113,  0.0253],\n",
            "        [ 0.0275,  0.0305,  0.0306, -0.0036,  0.0132, -0.0117, -0.0048,  0.0099],\n",
            "        [ 0.0107,  0.0017,  0.0067, -0.0012,  0.0060, -0.0039, -0.0012,  0.0030]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.022583231329917908 tensor([[ 0.0414,  0.0395,  0.0421, -0.0056,  0.0178, -0.0155, -0.0062,  0.0058],\n",
            "        [ 0.0719,  0.0513,  0.0586, -0.0082,  0.0335, -0.0287, -0.0140,  0.0240],\n",
            "        [ 0.0250,  0.0309,  0.0290, -0.0053,  0.0106, -0.0082, -0.0037,  0.0035],\n",
            "        [ 0.0164, -0.0018,  0.0085, -0.0019,  0.0094, -0.0044, -0.0014,  0.0030]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.022523483261466026 tensor([[ 0.0420,  0.0387,  0.0407, -0.0067,  0.0165, -0.0131, -0.0057, -0.0016],\n",
            "        [ 0.0834,  0.0522,  0.0613, -0.0106,  0.0377, -0.0302, -0.0164,  0.0222],\n",
            "        [ 0.0242,  0.0329,  0.0293, -0.0072,  0.0089, -0.0055, -0.0029, -0.0019],\n",
            "        [ 0.0215, -0.0057,  0.0097, -0.0026,  0.0126, -0.0046, -0.0014,  0.0026]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.022489072754979134 tensor([[ 0.0434,  0.0387,  0.0402, -0.0078,  0.0156, -0.0112, -0.0053, -0.0084],\n",
            "        [ 0.0927,  0.0511,  0.0618, -0.0128,  0.0408, -0.0307, -0.0185,  0.0193],\n",
            "        [ 0.0236,  0.0352,  0.0299, -0.0090,  0.0073, -0.0031, -0.0020, -0.0070],\n",
            "        [ 0.0262, -0.0100,  0.0105, -0.0033,  0.0156, -0.0047, -0.0014,  0.0021]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.022463982924818993 tensor([[ 0.0453,  0.0392,  0.0402, -0.0089,  0.0149, -0.0097, -0.0050, -0.0147],\n",
            "        [ 0.1009,  0.0490,  0.0611, -0.0149,  0.0434, -0.0306, -0.0204,  0.0157],\n",
            "        [ 0.0233,  0.0378,  0.0309, -0.0109,  0.0058, -0.0008, -0.0012, -0.0120],\n",
            "        [ 0.0307, -0.0145,  0.0111, -0.0040,  0.0184, -0.0046, -0.0014,  0.0014]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.5392e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.012882297858595848 tensor([[ 0.0656,  0.0610,  0.0651, -0.0059,  0.0325, -0.0317, -0.0122,  0.0278],\n",
            "        [ 0.0644,  0.0602,  0.0666, -0.0061,  0.0315, -0.0280, -0.0111,  0.0263],\n",
            "        [ 0.0452,  0.0410,  0.0459, -0.0047,  0.0235, -0.0196, -0.0089,  0.0182],\n",
            "        [ 0.0134,  0.0109,  0.0115, -0.0014,  0.0068, -0.0064, -0.0021,  0.0057]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01100964192301035 tensor([[ 0.0750,  0.0652,  0.0686, -0.0075,  0.0364, -0.0347, -0.0140,  0.0257],\n",
            "        [ 0.0618,  0.0528,  0.0602, -0.0072,  0.0288, -0.0214, -0.0096,  0.0156],\n",
            "        [ 0.0517,  0.0428,  0.0499, -0.0066,  0.0268, -0.0185, -0.0102,  0.0143],\n",
            "        [ 0.0195,  0.0149,  0.0162, -0.0022,  0.0099, -0.0081, -0.0026,  0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.010959873907268047 tensor([[ 0.0822,  0.0673,  0.0697, -0.0090,  0.0392, -0.0366, -0.0153,  0.0222],\n",
            "        [ 0.0633,  0.0497,  0.0586, -0.0087,  0.0283, -0.0171, -0.0088,  0.0073],\n",
            "        [ 0.0569,  0.0434,  0.0526, -0.0083,  0.0295, -0.0169, -0.0113,  0.0097],\n",
            "        [ 0.0235,  0.0169,  0.0187, -0.0030,  0.0119, -0.0087, -0.0027,  0.0066]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.010933032259345055 tensor([[ 8.7867e-02,  6.8040e-02,  6.9309e-02, -1.0403e-02,  4.1245e-02,\n",
            "         -3.7730e-02, -1.6390e-02,  1.7911e-02],\n",
            "        [ 6.6253e-02,  4.7998e-02,  5.8554e-02, -1.0166e-02,  2.8447e-02,\n",
            "         -1.3421e-02, -8.3256e-03,  9.5367e-06],\n",
            "        [ 6.1305e-02,  4.3064e-02,  5.4270e-02, -1.0027e-02,  3.1630e-02,\n",
            "         -1.4871e-02, -1.2162e-02,  4.6778e-03],\n",
            "        [ 2.6474e-02,  1.7769e-02,  2.0155e-02, -3.6341e-03,  1.3423e-02,\n",
            "         -8.7559e-03, -2.6274e-03,  5.7697e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01091217901557684 tensor([[ 0.0928,  0.0680,  0.0681, -0.0117,  0.0429, -0.0385, -0.0173,  0.0133],\n",
            "        [ 0.0697,  0.0469,  0.0591, -0.0117,  0.0289, -0.0101, -0.0079, -0.0069],\n",
            "        [ 0.0652,  0.0423,  0.0555, -0.0117,  0.0336, -0.0126, -0.0130, -0.0005],\n",
            "        [ 0.0288,  0.0181,  0.0209, -0.0043,  0.0146, -0.0085, -0.0024,  0.0046]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.3246e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.017308928072452545 tensor([[ 0.0590,  0.0502,  0.0573, -0.0056,  0.0301, -0.0254, -0.0113,  0.0243],\n",
            "        [ 0.0646,  0.0627,  0.0689, -0.0057,  0.0319, -0.0290, -0.0112,  0.0263],\n",
            "        [ 0.0269,  0.0222,  0.0264, -0.0038,  0.0140, -0.0108, -0.0044,  0.0078],\n",
            "        [ 0.0181,  0.0120,  0.0135, -0.0023,  0.0087, -0.0071, -0.0033,  0.0051]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015783246606588364 tensor([[ 0.0673,  0.0493,  0.0595, -0.0073,  0.0344, -0.0249, -0.0131,  0.0214],\n",
            "        [ 0.0696,  0.0649,  0.0728, -0.0071,  0.0326, -0.0273, -0.0107,  0.0195],\n",
            "        [ 0.0241,  0.0143,  0.0208, -0.0054,  0.0123, -0.0057, -0.0031, -0.0014],\n",
            "        [ 0.0292,  0.0170,  0.0199, -0.0042,  0.0135, -0.0099, -0.0051,  0.0058]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015728339552879333 tensor([[ 0.0738,  0.0467,  0.0596, -0.0089,  0.0377, -0.0236, -0.0146,  0.0174],\n",
            "        [ 0.0734,  0.0661,  0.0755, -0.0084,  0.0326, -0.0249, -0.0101,  0.0121],\n",
            "        [ 0.0256,  0.0109,  0.0201, -0.0074,  0.0130, -0.0029, -0.0026, -0.0080],\n",
            "        [ 0.0379,  0.0195,  0.0236, -0.0059,  0.0170, -0.0114, -0.0065,  0.0050]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015695076435804367 tensor([[ 0.0798,  0.0435,  0.0592, -0.0105,  0.0407, -0.0219, -0.0160,  0.0132],\n",
            "        [ 0.0768,  0.0668,  0.0777, -0.0097,  0.0325, -0.0224, -0.0094,  0.0045],\n",
            "        [ 0.0282,  0.0086,  0.0207, -0.0094,  0.0141, -0.0007, -0.0023, -0.0138],\n",
            "        [ 0.0449,  0.0205,  0.0255, -0.0075,  0.0196, -0.0120, -0.0075,  0.0032]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015667632222175598 tensor([[ 0.0856,  0.0402,  0.0587, -0.0120,  0.0437, -0.0202, -0.0173,  0.0090],\n",
            "        [ 0.0801,  0.0675,  0.0798, -0.0109,  0.0323, -0.0199, -0.0086, -0.0031],\n",
            "        [ 0.0311,  0.0066,  0.0216, -0.0113,  0.0155,  0.0013, -0.0021, -0.0194],\n",
            "        [ 0.0508,  0.0204,  0.0263, -0.0091,  0.0217, -0.0120, -0.0083,  0.0007]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020680148154497147 tensor([[ 0.0710,  0.0676,  0.0741, -0.0055,  0.0356, -0.0329, -0.0120,  0.0296],\n",
            "        [ 0.0756,  0.0700,  0.0766, -0.0068,  0.0379, -0.0349, -0.0138,  0.0315],\n",
            "        [ 0.0456,  0.0382,  0.0415, -0.0051,  0.0227, -0.0216, -0.0087,  0.0206],\n",
            "        [-0.0049, -0.0016, -0.0004, -0.0018, -0.0026,  0.0053, -0.0003, -0.0034]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018317505717277527 tensor([[ 7.0940e-02,  6.3195e-02,  6.9759e-02, -5.6040e-03,  3.4836e-02,\n",
            "         -3.0088e-02, -1.0989e-02,  2.1963e-02],\n",
            "        [ 8.1873e-02,  6.9984e-02,  7.8233e-02, -8.6784e-03,  4.0277e-02,\n",
            "         -3.3197e-02, -1.4586e-02,  2.3479e-02],\n",
            "        [ 5.8784e-02,  4.3173e-02,  4.7679e-02, -7.8583e-03,  2.9192e-02,\n",
            "         -2.6140e-02, -1.1733e-02,  2.2829e-02],\n",
            "        [-1.0562e-02, -4.0114e-03, -9.4712e-04, -3.6669e-03, -6.3467e-03,\n",
            "          1.1535e-02,  3.6955e-05, -7.6509e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018261093646287918 tensor([[ 7.3389e-02,  6.1464e-02,  6.8272e-02, -5.8454e-03,  3.5458e-02,\n",
            "         -2.8620e-02, -1.0477e-02,  1.5941e-02],\n",
            "        [ 8.6586e-02,  6.8456e-02,  7.8164e-02, -1.0369e-02,  4.1898e-02,\n",
            "         -3.0804e-02, -1.5061e-02,  1.4868e-02],\n",
            "        [ 6.8302e-02,  4.4572e-02,  4.9876e-02, -1.0357e-02,  3.3784e-02,\n",
            "         -2.8861e-02, -1.4149e-02,  2.3026e-02],\n",
            "        [-1.4844e-02, -4.9859e-03,  8.2254e-05, -5.5802e-03, -9.4080e-03,\n",
            "          1.7056e-02,  1.2577e-04, -1.1070e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01822575554251671 tensor([[ 7.6619e-02,  6.0579e-02,  6.7706e-02, -6.1285e-03,  3.6501e-02,\n",
            "         -2.7583e-02, -1.0115e-02,  1.0467e-02],\n",
            "        [ 9.1136e-02,  6.6799e-02,  7.7924e-02, -1.2020e-02,  4.3446e-02,\n",
            "         -2.8394e-02, -1.5514e-02,  6.3610e-03],\n",
            "        [ 7.5936e-02,  4.4140e-02,  5.0055e-02, -1.2709e-02,  3.7398e-02,\n",
            "         -3.0636e-02, -1.6227e-02,  2.2212e-02],\n",
            "        [-1.8170e-02, -4.9806e-03,  2.1994e-03, -7.5483e-03, -1.2004e-02,\n",
            "          2.2073e-02,  6.6757e-05, -1.3902e-02]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018197055906057358 tensor([[ 8.0105e-02,  5.9990e-02,  6.7459e-02, -6.4194e-03,  3.7686e-02,\n",
            "         -2.6705e-02, -9.8056e-03,  5.2214e-03],\n",
            "        [ 9.5780e-02,  6.5289e-02,  7.7821e-02, -1.3652e-02,  4.5055e-02,\n",
            "         -2.6090e-02, -1.5996e-02, -1.9360e-03],\n",
            "        [ 8.2517e-02,  4.2717e-02,  4.9140e-02, -1.4971e-02,  4.0469e-02,\n",
            "         -3.1886e-02, -1.8114e-02,  2.0856e-02],\n",
            "        [-2.0869e-02, -4.3404e-03,  5.0104e-03, -9.5475e-03, -1.4294e-02,\n",
            "          2.6741e-02, -9.1791e-05, -1.6336e-02]], device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02152145653963089 tensor([[ 0.0988,  0.0899,  0.0990, -0.0088,  0.0502, -0.0463, -0.0178,  0.0408],\n",
            "        [ 0.0650,  0.0592,  0.0642, -0.0057,  0.0324, -0.0309, -0.0117,  0.0277],\n",
            "        [ 0.0464,  0.0440,  0.0481, -0.0045,  0.0230, -0.0206, -0.0086,  0.0193],\n",
            "        [ 0.0041,  0.0057,  0.0077, -0.0016,  0.0024, -0.0024, -0.0009,  0.0003]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01835199072957039 tensor([[ 0.1143,  0.0955,  0.1067, -0.0116,  0.0578, -0.0500, -0.0199,  0.0357],\n",
            "        [ 0.0763,  0.0641,  0.0705, -0.0076,  0.0378, -0.0337, -0.0133,  0.0251],\n",
            "        [ 0.0549,  0.0498,  0.0551, -0.0062,  0.0264, -0.0212, -0.0099,  0.0175],\n",
            "        [ 0.0037,  0.0076,  0.0119, -0.0029,  0.0019, -0.0021, -0.0004, -0.0020]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018289364874362946 tensor([[ 0.1224,  0.0934,  0.1060, -0.0138,  0.0615, -0.0498, -0.0206,  0.0267],\n",
            "        [ 0.0821,  0.0634,  0.0708, -0.0090,  0.0403, -0.0336, -0.0138,  0.0195],\n",
            "        [ 0.0593,  0.0514,  0.0578, -0.0077,  0.0276, -0.0196, -0.0104,  0.0134],\n",
            "        [ 0.0026,  0.0090,  0.0156, -0.0041,  0.0011, -0.0014,  0.0002, -0.0045]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018257059156894684 tensor([[ 1.2983e-01,  9.0778e-02,  1.0467e-01, -1.5873e-02,  6.4974e-02,\n",
            "         -4.9394e-02, -2.1183e-02,  1.7529e-02],\n",
            "        [ 8.7113e-02,  6.1949e-02,  7.0232e-02, -1.0293e-02,  4.2346e-02,\n",
            "         -3.3147e-02, -1.4271e-02,  1.3542e-02],\n",
            "        [ 6.2625e-02,  5.2102e-02,  5.9401e-02, -9.0647e-03,  2.8287e-02,\n",
            "         -1.7567e-02, -1.0732e-02,  8.8263e-03],\n",
            "        [ 1.3316e-03,  1.0164e-02,  1.9090e-02, -5.2553e-03,  6.9141e-05,\n",
            "         -6.8247e-04,  9.4533e-04, -7.1973e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018227603286504745 tensor([[ 1.3719e-01,  8.8143e-02,  1.0331e-01, -1.7958e-02,  6.8393e-02,\n",
            "         -4.8972e-02, -2.1787e-02,  8.4162e-03],\n",
            "        [ 9.1920e-02,  6.0333e-02,  6.9495e-02, -1.1597e-02,  4.4352e-02,\n",
            "         -3.2611e-02, -1.4672e-02,  7.5674e-03],\n",
            "        [ 6.5663e-02,  5.2458e-02,  6.0643e-02, -1.0407e-02,  2.8809e-02,\n",
            "         -1.5351e-02, -1.0988e-02,  4.1008e-03],\n",
            "        [-8.5831e-05,  1.1248e-02,  2.2454e-02, -6.4099e-03, -9.9242e-04,\n",
            "          1.2219e-04,  1.6659e-03, -9.9081e-03]], device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02217753417789936 tensor([[ 0.0737,  0.0661,  0.0736, -0.0070,  0.0373, -0.0307, -0.0135,  0.0279],\n",
            "        [ 0.0767,  0.0779,  0.0853, -0.0054,  0.0373, -0.0341, -0.0122,  0.0340],\n",
            "        [ 0.0476,  0.0402,  0.0430, -0.0051,  0.0250, -0.0216, -0.0111,  0.0222],\n",
            "        [ 0.0217,  0.0191,  0.0192, -0.0019,  0.0107, -0.0116, -0.0038,  0.0098]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019767191261053085 tensor([[ 0.0780,  0.0610,  0.0703, -0.0092,  0.0391, -0.0258, -0.0139,  0.0179],\n",
            "        [ 0.0664,  0.0682,  0.0761, -0.0046,  0.0303, -0.0230, -0.0085,  0.0197],\n",
            "        [ 0.0602,  0.0443,  0.0473, -0.0075,  0.0319, -0.0246, -0.0153,  0.0243],\n",
            "        [ 0.0338,  0.0291,  0.0289, -0.0028,  0.0164, -0.0177, -0.0056,  0.0137]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.0196499265730381 tensor([[ 0.0859,  0.0598,  0.0712, -0.0117,  0.0427, -0.0227, -0.0149,  0.0098],\n",
            "        [ 0.0666,  0.0691,  0.0783, -0.0045,  0.0287, -0.0175, -0.0066,  0.0115],\n",
            "        [ 0.0698,  0.0456,  0.0485, -0.0098,  0.0372, -0.0259, -0.0190,  0.0246],\n",
            "        [ 0.0419,  0.0354,  0.0345, -0.0036,  0.0201, -0.0217, -0.0067,  0.0152]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019596077501773834 tensor([[ 0.0933,  0.0580,  0.0715, -0.0141,  0.0460, -0.0194, -0.0158,  0.0014],\n",
            "        [ 0.0671,  0.0703,  0.0809, -0.0044,  0.0272, -0.0122, -0.0049,  0.0037],\n",
            "        [ 0.0770,  0.0445,  0.0470, -0.0118,  0.0412, -0.0260, -0.0222,  0.0235],\n",
            "        [ 0.0477,  0.0392,  0.0376, -0.0042,  0.0226, -0.0243, -0.0073,  0.0151]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019557883962988853 tensor([[ 0.1004,  0.0562,  0.0717, -0.0165,  0.0493, -0.0160, -0.0167, -0.0069],\n",
            "        [ 0.0681,  0.0720,  0.0839, -0.0043,  0.0260, -0.0072, -0.0032, -0.0037],\n",
            "        [ 0.0832,  0.0423,  0.0445, -0.0137,  0.0447, -0.0255, -0.0252,  0.0218],\n",
            "        [ 0.0520,  0.0417,  0.0392, -0.0047,  0.0243, -0.0261, -0.0076,  0.0141]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023262903094291687 tensor([[ 0.0636,  0.0572,  0.0653, -0.0058,  0.0316, -0.0269, -0.0115,  0.0270],\n",
            "        [ 0.0532,  0.0483,  0.0538, -0.0037,  0.0265, -0.0241, -0.0098,  0.0246],\n",
            "        [ 0.0206,  0.0239,  0.0259, -0.0044,  0.0110, -0.0066, -0.0036,  0.0034],\n",
            "        [ 0.0129,  0.0062,  0.0067, -0.0004,  0.0043, -0.0063, -0.0008,  0.0059]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02177608385682106 tensor([[ 0.0766,  0.0633,  0.0752, -0.0081,  0.0372, -0.0275, -0.0136,  0.0257],\n",
            "        [ 0.0633,  0.0535,  0.0613, -0.0044,  0.0308, -0.0260, -0.0114,  0.0252],\n",
            "        [ 0.0191,  0.0260,  0.0289, -0.0068,  0.0103, -0.0010, -0.0025, -0.0057],\n",
            "        [ 0.0210,  0.0077,  0.0086, -0.0006,  0.0057, -0.0100, -0.0004,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02171514555811882 tensor([[ 0.0835,  0.0629,  0.0782, -0.0099,  0.0396, -0.0249, -0.0146,  0.0211],\n",
            "        [ 0.0683,  0.0537,  0.0635, -0.0047,  0.0325, -0.0252, -0.0120,  0.0229],\n",
            "        [ 0.0171,  0.0278,  0.0315, -0.0092,  0.0093,  0.0049, -0.0013, -0.0149],\n",
            "        [ 0.0274,  0.0073,  0.0086, -0.0006,  0.0062, -0.0127,  0.0003,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.021684296429157257 tensor([[ 0.0889,  0.0611,  0.0796, -0.0116,  0.0413, -0.0217, -0.0153,  0.0158],\n",
            "        [ 0.0723,  0.0528,  0.0644, -0.0049,  0.0337, -0.0239, -0.0124,  0.0201],\n",
            "        [ 0.0157,  0.0301,  0.0348, -0.0115,  0.0086,  0.0103, -0.0002, -0.0235],\n",
            "        [ 0.0327,  0.0061,  0.0076, -0.0006,  0.0062, -0.0148,  0.0012,  0.0133]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02165885828435421 tensor([[ 0.0940,  0.0591,  0.0807, -0.0133,  0.0429, -0.0183, -0.0159,  0.0103],\n",
            "        [ 0.0759,  0.0516,  0.0651, -0.0051,  0.0346, -0.0225, -0.0128,  0.0171],\n",
            "        [ 0.0148,  0.0329,  0.0385, -0.0139,  0.0082,  0.0155,  0.0008, -0.0317],\n",
            "        [ 0.0375,  0.0043,  0.0061, -0.0005,  0.0059, -0.0167,  0.0023,  0.0146]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021366097033023834 tensor([[-0.0030, -0.0055, -0.0018, -0.0009, -0.0028,  0.0055,  0.0019, -0.0083],\n",
            "        [ 0.0444,  0.0444,  0.0486, -0.0031,  0.0224, -0.0182, -0.0064,  0.0133],\n",
            "        [ 0.0188,  0.0048,  0.0047, -0.0025,  0.0085, -0.0097, -0.0049,  0.0096],\n",
            "        [ 0.0144,  0.0100,  0.0116, -0.0008,  0.0060, -0.0050, -0.0008,  0.0031]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020942145958542824 tensor([[-0.0073, -0.0128, -0.0058, -0.0013, -0.0062,  0.0114,  0.0041, -0.0159],\n",
            "        [ 0.0416,  0.0408,  0.0448, -0.0026,  0.0209, -0.0126, -0.0043,  0.0008],\n",
            "        [ 0.0257, -0.0027, -0.0038, -0.0042,  0.0111, -0.0129, -0.0075,  0.0122],\n",
            "        [ 0.0239,  0.0154,  0.0187, -0.0012,  0.0095, -0.0072, -0.0005,  0.0032]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02087796851992607 tensor([[-0.0060, -0.0144, -0.0036, -0.0022, -0.0068,  0.0143,  0.0052, -0.0203],\n",
            "        [ 0.0456,  0.0438,  0.0483, -0.0027,  0.0227, -0.0106, -0.0036, -0.0078],\n",
            "        [ 0.0335, -0.0091, -0.0110, -0.0060,  0.0141, -0.0166, -0.0103,  0.0153],\n",
            "        [ 0.0310,  0.0184,  0.0230, -0.0015,  0.0118, -0.0082,  0.0002,  0.0018]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02083554118871689 tensor([[-0.0045, -0.0156, -0.0011, -0.0030, -0.0072,  0.0171,  0.0063, -0.0243],\n",
            "        [ 0.0481,  0.0454,  0.0501, -0.0027,  0.0237, -0.0079, -0.0025, -0.0171],\n",
            "        [ 0.0409, -0.0158, -0.0186, -0.0077,  0.0169, -0.0202, -0.0131,  0.0182],\n",
            "        [ 0.0365,  0.0200,  0.0258, -0.0016,  0.0133, -0.0083,  0.0011, -0.0005]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020796753466129303 tensor([[-0.0026, -0.0165,  0.0017, -0.0038, -0.0075,  0.0196,  0.0072, -0.0282],\n",
            "        [ 0.0507,  0.0470,  0.0519, -0.0026,  0.0248, -0.0053, -0.0015, -0.0262],\n",
            "        [ 0.0484, -0.0222, -0.0259, -0.0094,  0.0198, -0.0237, -0.0158,  0.0211],\n",
            "        [ 0.0411,  0.0207,  0.0276, -0.0017,  0.0144, -0.0079,  0.0022, -0.0034]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0.0000e+00, -0.0000e+00, 1.9670e-06, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02356603369116783 tensor([[ 0.0529,  0.0497,  0.0583, -0.0071,  0.0258, -0.0215, -0.0074,  0.0177],\n",
            "        [ 0.0589,  0.0525,  0.0561, -0.0040,  0.0304, -0.0280, -0.0116,  0.0274],\n",
            "        [ 0.0397,  0.0364,  0.0433, -0.0050,  0.0203, -0.0161, -0.0061,  0.0117],\n",
            "        [ 0.0134,  0.0094,  0.0096, -0.0013,  0.0067, -0.0058, -0.0022,  0.0052]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02219299040734768 tensor([[ 0.0453,  0.0385,  0.0500, -0.0094,  0.0209, -0.0124, -0.0040,  0.0035],\n",
            "        [ 0.0658,  0.0517,  0.0550, -0.0044,  0.0342, -0.0286, -0.0133,  0.0253],\n",
            "        [ 0.0482,  0.0420,  0.0538, -0.0076,  0.0242, -0.0158, -0.0058,  0.0056],\n",
            "        [ 0.0198,  0.0117,  0.0122, -0.0020,  0.0098, -0.0074, -0.0030,  0.0061]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.022118782624602318 tensor([[ 0.0468,  0.0366,  0.0517, -0.0123,  0.0207, -0.0080, -0.0023, -0.0054],\n",
            "        [ 0.0730,  0.0513,  0.0542, -0.0048,  0.0380, -0.0294, -0.0150,  0.0234],\n",
            "        [ 0.0533,  0.0441,  0.0605, -0.0100,  0.0263, -0.0136, -0.0050, -0.0026],\n",
            "        [ 0.0243,  0.0123,  0.0129, -0.0027,  0.0120, -0.0079, -0.0034,  0.0059]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.022081555798649788 tensor([[ 0.0491,  0.0354,  0.0543, -0.0152,  0.0209, -0.0041, -0.0007, -0.0137],\n",
            "        [ 0.0790,  0.0497,  0.0522, -0.0051,  0.0413, -0.0296, -0.0165,  0.0209],\n",
            "        [ 0.0567,  0.0446,  0.0654, -0.0122,  0.0275, -0.0107, -0.0039, -0.0116],\n",
            "        [ 0.0280,  0.0120,  0.0128, -0.0033,  0.0137, -0.0079, -0.0037,  0.0052]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02205129899084568 tensor([[ 0.0517,  0.0347,  0.0573, -0.0181,  0.0213, -0.0005,  0.0008, -0.0216],\n",
            "        [ 0.0845,  0.0478,  0.0497, -0.0054,  0.0443, -0.0295, -0.0179,  0.0182],\n",
            "        [ 0.0595,  0.0445,  0.0696, -0.0144,  0.0285, -0.0074, -0.0026, -0.0208],\n",
            "        [ 0.0311,  0.0113,  0.0122, -0.0039,  0.0152, -0.0077, -0.0038,  0.0042]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02093980461359024 tensor([[ 0.1063,  0.0943,  0.1031, -0.0091,  0.0530, -0.0490, -0.0197,  0.0451],\n",
            "        [ 0.0937,  0.0876,  0.0967, -0.0087,  0.0470, -0.0426, -0.0167,  0.0392],\n",
            "        [ 0.0615,  0.0584,  0.0640, -0.0062,  0.0321, -0.0279, -0.0113,  0.0259],\n",
            "        [ 0.0153,  0.0130,  0.0146, -0.0019,  0.0079, -0.0075, -0.0025,  0.0061]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016015203669667244 tensor([[ 0.1291,  0.1053,  0.1168, -0.0124,  0.0634, -0.0549, -0.0235,  0.0441],\n",
            "        [ 0.1085,  0.0959,  0.1079, -0.0123,  0.0532, -0.0442, -0.0187,  0.0343],\n",
            "        [ 0.0721,  0.0651,  0.0724, -0.0086,  0.0377, -0.0292, -0.0130,  0.0226],\n",
            "        [ 0.0216,  0.0167,  0.0198, -0.0029,  0.0108, -0.0095, -0.0029,  0.0065]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015887660905718803 tensor([[ 0.1415,  0.1055,  0.1187, -0.0149,  0.0683, -0.0555, -0.0252,  0.0375],\n",
            "        [ 0.1142,  0.0947,  0.1091, -0.0152,  0.0546, -0.0411, -0.0189,  0.0244],\n",
            "        [ 0.0764,  0.0652,  0.0737, -0.0105,  0.0399, -0.0271, -0.0134,  0.0157],\n",
            "        [ 0.0259,  0.0184,  0.0229, -0.0037,  0.0127, -0.0102, -0.0028,  0.0056]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01583978906273842 tensor([[ 0.1517,  0.1036,  0.1183, -0.0172,  0.0722, -0.0550, -0.0265,  0.0298],\n",
            "        [ 0.1194,  0.0931,  0.1097, -0.0181,  0.0557, -0.0378, -0.0191,  0.0142],\n",
            "        [ 0.0800,  0.0647,  0.0744, -0.0124,  0.0418, -0.0247, -0.0138,  0.0086],\n",
            "        [ 0.0292,  0.0191,  0.0249, -0.0045,  0.0141, -0.0104, -0.0026,  0.0041]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015796683728694916 tensor([[ 0.1611,  0.1010,  0.1171, -0.0195,  0.0757, -0.0541, -0.0277,  0.0217],\n",
            "        [ 0.1246,  0.0916,  0.1105, -0.0209,  0.0569, -0.0345, -0.0193,  0.0043],\n",
            "        [ 0.0835,  0.0641,  0.0750, -0.0142,  0.0437, -0.0223, -0.0141,  0.0015],\n",
            "        [ 0.0319,  0.0193,  0.0264, -0.0053,  0.0151, -0.0103, -0.0023,  0.0023]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "dided\n",
            "time\n",
            "[8, 11, 11, 11, 4, 2, 11, 2, 4, 6, 6, 4, 5, 5, 12, 2, 3, 6, 11, 6, 4, 6, 2, 4, 1, 11, 6, 1, 4, 6, 6, 11, 4, 4, 11, 6, 6, 10, 4, 6, 5, 11, 4, 2, 4, 4, 11, 6, 5, 6, 5, 11, 4, 4, 4, 4, 6, 8, 4, 6, 6, 1, 4, 4, 6, 11, 5, 2, 5, 4, 11, 4, 4, 5, 11, 8, 4, 11]\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019253231585025787 tensor([[ 0.0943,  0.0907,  0.0979, -0.0076,  0.0460, -0.0416, -0.0161,  0.0374],\n",
            "        [ 0.0736,  0.0679,  0.0744, -0.0064,  0.0381, -0.0336, -0.0142,  0.0323],\n",
            "        [ 0.0572,  0.0578,  0.0622, -0.0057,  0.0286, -0.0256, -0.0100,  0.0229],\n",
            "        [ 0.0150,  0.0098,  0.0104, -0.0021,  0.0079, -0.0066, -0.0039,  0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015582410618662834 tensor([[ 0.1097,  0.1018,  0.1097, -0.0095,  0.0515, -0.0428, -0.0174,  0.0318],\n",
            "        [ 0.0848,  0.0727,  0.0817, -0.0086,  0.0447, -0.0342, -0.0168,  0.0293],\n",
            "        [ 0.0635,  0.0655,  0.0702, -0.0079,  0.0309, -0.0249, -0.0105,  0.0173],\n",
            "        [ 0.0223,  0.0115,  0.0125, -0.0035,  0.0113, -0.0083, -0.0062,  0.0087]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015502400696277618 tensor([[ 0.1162,  0.1036,  0.1114, -0.0106,  0.0524, -0.0394, -0.0170,  0.0216],\n",
            "        [ 0.0901,  0.0713,  0.0824, -0.0103,  0.0483, -0.0316, -0.0184,  0.0229],\n",
            "        [ 0.0652,  0.0687,  0.0734, -0.0097,  0.0308, -0.0217, -0.0101,  0.0091],\n",
            "        [ 0.0278,  0.0115,  0.0129, -0.0048,  0.0138, -0.0090, -0.0081,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015462677925825119 tensor([[ 0.1219,  0.1047,  0.1124, -0.0117,  0.0529, -0.0358, -0.0165,  0.0111],\n",
            "        [ 0.0949,  0.0694,  0.0827, -0.0120,  0.0516, -0.0287, -0.0198,  0.0163],\n",
            "        [ 0.0664,  0.0714,  0.0760, -0.0114,  0.0304, -0.0183, -0.0097,  0.0008],\n",
            "        [ 0.0323,  0.0107,  0.0124, -0.0061,  0.0159, -0.0092, -0.0099,  0.0099]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015424424782395363 tensor([[ 0.1275,  0.1057,  0.1132, -0.0127,  0.0533, -0.0322, -0.0160,  0.0007],\n",
            "        [ 0.0996,  0.0676,  0.0829, -0.0136,  0.0549, -0.0259, -0.0212,  0.0098],\n",
            "        [ 0.0674,  0.0739,  0.0785, -0.0131,  0.0300, -0.0149, -0.0092, -0.0075],\n",
            "        [ 0.0364,  0.0094,  0.0114, -0.0073,  0.0177, -0.0092, -0.0115,  0.0099]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023406662046909332 tensor([[ 0.1119,  0.1012,  0.1115, -0.0096,  0.0553, -0.0494, -0.0188,  0.0427],\n",
            "        [ 0.0896,  0.0839,  0.0922, -0.0078,  0.0457, -0.0407, -0.0162,  0.0393],\n",
            "        [ 0.0545,  0.0490,  0.0546, -0.0050,  0.0282, -0.0245, -0.0108,  0.0231],\n",
            "        [ 0.0217,  0.0148,  0.0176, -0.0020,  0.0106, -0.0088, -0.0027,  0.0054]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018038537353277206 tensor([[ 0.1431,  0.1224,  0.1369, -0.0139,  0.0692, -0.0568, -0.0226,  0.0399],\n",
            "        [ 0.1157,  0.1052,  0.1172, -0.0112,  0.0592, -0.0480, -0.0206,  0.0423],\n",
            "        [ 0.0659,  0.0547,  0.0627, -0.0066,  0.0344, -0.0269, -0.0136,  0.0225],\n",
            "        [ 0.0339,  0.0196,  0.0253, -0.0031,  0.0158, -0.0119, -0.0029,  0.0043]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017731767147779465 tensor([[ 0.1544,  0.1230,  0.1400, -0.0166,  0.0730, -0.0541, -0.0226,  0.0267],\n",
            "        [ 0.1241,  0.1081,  0.1225, -0.0133,  0.0634, -0.0459, -0.0218,  0.0353],\n",
            "        [ 0.0694,  0.0525,  0.0624, -0.0076,  0.0363, -0.0252, -0.0148,  0.0176],\n",
            "        [ 0.0431,  0.0215,  0.0299, -0.0039,  0.0195, -0.0133, -0.0025,  0.0015]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01766505464911461 tensor([[ 0.1635,  0.1214,  0.1407, -0.0190,  0.0757, -0.0503, -0.0223,  0.0125],\n",
            "        [ 0.1297,  0.1083,  0.1249, -0.0152,  0.0662, -0.0425, -0.0224,  0.0269],\n",
            "        [ 0.0724,  0.0498,  0.0617, -0.0085,  0.0380, -0.0232, -0.0159,  0.0126],\n",
            "        [ 0.0508,  0.0219,  0.0330, -0.0045,  0.0223, -0.0138, -0.0018, -0.0023]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017607174813747406 tensor([[ 0.1724,  0.1198,  0.1412, -0.0215,  0.0783, -0.0466, -0.0219, -0.0015],\n",
            "        [ 0.1348,  0.1080,  0.1267, -0.0171,  0.0688, -0.0388, -0.0230,  0.0182],\n",
            "        [ 0.0757,  0.0475,  0.0612, -0.0094,  0.0398, -0.0213, -0.0171,  0.0078],\n",
            "        [ 0.0576,  0.0215,  0.0352, -0.0051,  0.0247, -0.0139, -0.0009, -0.0065]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023228906095027924 tensor([[ 0.1091,  0.1094,  0.1196, -0.0103,  0.0566, -0.0475, -0.0208,  0.0432],\n",
            "        [ 0.1014,  0.0934,  0.1034, -0.0086,  0.0505, -0.0455, -0.0175,  0.0418],\n",
            "        [ 0.0577,  0.0555,  0.0603, -0.0057,  0.0307, -0.0257, -0.0111,  0.0233],\n",
            "        [ 0.0271,  0.0208,  0.0233, -0.0031,  0.0149, -0.0135, -0.0056,  0.0121]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017698101699352264 tensor([[ 0.1200,  0.1200,  0.1321, -0.0137,  0.0629, -0.0451, -0.0236,  0.0335],\n",
            "        [ 0.1164,  0.0992,  0.1128, -0.0111,  0.0562, -0.0456, -0.0186,  0.0342],\n",
            "        [ 0.0645,  0.0587,  0.0648, -0.0075,  0.0350, -0.0239, -0.0128,  0.0170],\n",
            "        [ 0.0417,  0.0284,  0.0334, -0.0051,  0.0237, -0.0197, -0.0087,  0.0167]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017568077892065048 tensor([[ 0.1231,  0.1224,  0.1357, -0.0164,  0.0651, -0.0387, -0.0249,  0.0201],\n",
            "        [ 0.1240,  0.0974,  0.1138, -0.0129,  0.0580, -0.0418, -0.0182,  0.0226],\n",
            "        [ 0.0673,  0.0579,  0.0650, -0.0090,  0.0374, -0.0200, -0.0137,  0.0086],\n",
            "        [ 0.0525,  0.0323,  0.0396, -0.0068,  0.0304, -0.0238, -0.0111,  0.0188]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017488589510321617 tensor([[ 0.1261,  0.1248,  0.1392, -0.0191,  0.0672, -0.0324, -0.0263,  0.0066],\n",
            "        [ 0.1316,  0.0955,  0.1149, -0.0148,  0.0598, -0.0381, -0.0179,  0.0110],\n",
            "        [ 0.0702,  0.0571,  0.0653, -0.0105,  0.0397, -0.0161, -0.0147,  0.0004],\n",
            "        [ 0.0611,  0.0341,  0.0434, -0.0084,  0.0360, -0.0267, -0.0131,  0.0196]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017418870702385902 tensor([[ 0.1290,  0.1271,  0.1427, -0.0218,  0.0694, -0.0261, -0.0276, -0.0067],\n",
            "        [ 0.1390,  0.0937,  0.1160, -0.0166,  0.0615, -0.0343, -0.0175, -0.0004],\n",
            "        [ 0.0732,  0.0564,  0.0657, -0.0120,  0.0421, -0.0124, -0.0156, -0.0077],\n",
            "        [ 0.0682,  0.0343,  0.0455, -0.0099,  0.0408, -0.0287, -0.0147,  0.0195]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018107477575540543 tensor([[ 0.0819,  0.0772,  0.0850, -0.0069,  0.0401, -0.0357, -0.0133,  0.0307],\n",
            "        [ 0.0688,  0.0645,  0.0715, -0.0054,  0.0352, -0.0298, -0.0120,  0.0268],\n",
            "        [ 0.0378,  0.0368,  0.0409, -0.0040,  0.0194, -0.0171, -0.0064,  0.0145],\n",
            "        [ 0.0130,  0.0112,  0.0133, -0.0019,  0.0072, -0.0050, -0.0024,  0.0043]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015408557839691639 tensor([[ 0.1001,  0.0909,  0.1008, -0.0093,  0.0475, -0.0391, -0.0150,  0.0275],\n",
            "        [ 0.0809,  0.0719,  0.0817, -0.0066,  0.0416, -0.0299, -0.0137,  0.0212],\n",
            "        [ 0.0352,  0.0328,  0.0385, -0.0051,  0.0180, -0.0131, -0.0051,  0.0065],\n",
            "        [ 0.0177,  0.0145,  0.0187, -0.0030,  0.0096, -0.0049, -0.0028,  0.0034]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015310011804103851 tensor([[ 0.1075,  0.0934,  0.1044, -0.0109,  0.0494, -0.0371, -0.0148,  0.0187],\n",
            "        [ 0.0862,  0.0723,  0.0845, -0.0072,  0.0447, -0.0263, -0.0141,  0.0118],\n",
            "        [ 0.0336,  0.0301,  0.0377, -0.0063,  0.0172, -0.0098, -0.0039, -0.0006],\n",
            "        [ 0.0208,  0.0164,  0.0226, -0.0040,  0.0112, -0.0040, -0.0030,  0.0016]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015269136056303978 tensor([[ 0.1132,  0.0943,  0.1062, -0.0123,  0.0504, -0.0342, -0.0143,  0.0090],\n",
            "        [ 0.0908,  0.0720,  0.0865, -0.0078,  0.0473, -0.0224, -0.0143,  0.0020],\n",
            "        [ 0.0336,  0.0291,  0.0386, -0.0076,  0.0173, -0.0072, -0.0031, -0.0068],\n",
            "        [ 0.0231,  0.0174,  0.0255, -0.0050,  0.0124, -0.0027, -0.0029, -0.0006]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015233300626277924 tensor([[ 0.1185,  0.0947,  0.1075, -0.0137,  0.0512, -0.0313, -0.0138, -0.0007],\n",
            "        [ 0.0953,  0.0717,  0.0884, -0.0083,  0.0499, -0.0185, -0.0146, -0.0078],\n",
            "        [ 0.0343,  0.0287,  0.0402, -0.0088,  0.0176, -0.0051, -0.0024, -0.0124],\n",
            "        [ 0.0249,  0.0180,  0.0280, -0.0059,  0.0134, -0.0012, -0.0028, -0.0032]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.0235728919506073 tensor([[ 0.1088,  0.0972,  0.1053, -0.0085,  0.0539, -0.0489, -0.0196,  0.0441],\n",
            "        [ 0.0965,  0.0929,  0.1022, -0.0088,  0.0489, -0.0433, -0.0170,  0.0408],\n",
            "        [ 0.0698,  0.0636,  0.0693, -0.0062,  0.0360, -0.0330, -0.0127,  0.0306],\n",
            "        [ 0.0205,  0.0161,  0.0192, -0.0023,  0.0112, -0.0087, -0.0041,  0.0079]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017861679196357727 tensor([[ 0.1354,  0.1124,  0.1217, -0.0112,  0.0657, -0.0559, -0.0237,  0.0434],\n",
            "        [ 0.1095,  0.1021,  0.1147, -0.0122,  0.0551, -0.0424, -0.0186,  0.0341],\n",
            "        [ 0.0933,  0.0806,  0.0885, -0.0089,  0.0480, -0.0416, -0.0168,  0.0344],\n",
            "        [ 0.0305,  0.0209,  0.0273, -0.0036,  0.0169, -0.0113, -0.0061,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01762213557958603 tensor([[ 0.1483,  0.1135,  0.1226, -0.0128,  0.0705, -0.0560, -0.0254,  0.0356],\n",
            "        [ 0.1107,  0.0991,  0.1140, -0.0148,  0.0552, -0.0355, -0.0180,  0.0211],\n",
            "        [ 0.1038,  0.0846,  0.0936, -0.0108,  0.0533, -0.0433, -0.0184,  0.0309],\n",
            "        [ 0.0372,  0.0226,  0.0321, -0.0048,  0.0209, -0.0121, -0.0074,  0.0092]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.0175507552921772 tensor([[ 0.1593,  0.1127,  0.1215, -0.0142,  0.0743, -0.0552, -0.0267,  0.0270],\n",
            "        [ 0.1130,  0.0971,  0.1145, -0.0173,  0.0558, -0.0291, -0.0176,  0.0089],\n",
            "        [ 0.1112,  0.0855,  0.0953, -0.0123,  0.0569, -0.0435, -0.0195,  0.0258],\n",
            "        [ 0.0424,  0.0227,  0.0353, -0.0058,  0.0241, -0.0119, -0.0085,  0.0079]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017494503408670425 tensor([[ 0.1697,  0.1113,  0.1198, -0.0156,  0.0777, -0.0541, -0.0278,  0.0181],\n",
            "        [ 0.1161,  0.0960,  0.1159, -0.0199,  0.0569, -0.0233, -0.0174, -0.0027],\n",
            "        [ 0.1172,  0.0851,  0.0955, -0.0138,  0.0598, -0.0429, -0.0203,  0.0199],\n",
            "        [ 0.0467,  0.0220,  0.0375, -0.0068,  0.0269, -0.0113, -0.0094,  0.0060]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025052133947610855 tensor([[ 0.1108,  0.1069,  0.1161, -0.0094,  0.0558, -0.0503, -0.0199,  0.0466],\n",
            "        [ 0.0930,  0.0883,  0.0984, -0.0078,  0.0464, -0.0397, -0.0150,  0.0341],\n",
            "        [ 0.0596,  0.0511,  0.0561, -0.0060,  0.0308, -0.0285, -0.0111,  0.0267],\n",
            "        [ 0.0167,  0.0141,  0.0169, -0.0024,  0.0091, -0.0065, -0.0031,  0.0049]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.019716184586286545 tensor([[ 0.1311,  0.1227,  0.1337, -0.0123,  0.0654, -0.0546, -0.0229,  0.0438],\n",
            "        [ 0.1051,  0.0952,  0.1093, -0.0099,  0.0510, -0.0367, -0.0151,  0.0219],\n",
            "        [ 0.0767,  0.0596,  0.0664, -0.0088,  0.0400, -0.0353, -0.0142,  0.0302],\n",
            "        [ 0.0238,  0.0186,  0.0244, -0.0043,  0.0128, -0.0075, -0.0040,  0.0039]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.0195525661110878 tensor([[ 0.1390,  0.1258,  0.1373, -0.0142,  0.0685, -0.0527, -0.0236,  0.0346],\n",
            "        [ 0.1080,  0.0926,  0.1100, -0.0112,  0.0509, -0.0288, -0.0136,  0.0049],\n",
            "        [ 0.0855,  0.0598,  0.0677, -0.0110,  0.0448, -0.0377, -0.0158,  0.0291],\n",
            "        [ 0.0284,  0.0207,  0.0293, -0.0060,  0.0151, -0.0070, -0.0044,  0.0015]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019482119008898735 tensor([[ 0.1456,  0.1274,  0.1393, -0.0161,  0.0710, -0.0501, -0.0240,  0.0247],\n",
            "        [ 0.1122,  0.0913,  0.1121, -0.0126,  0.0515, -0.0218, -0.0123, -0.0111],\n",
            "        [ 0.0925,  0.0582,  0.0671, -0.0131,  0.0487, -0.0392, -0.0171,  0.0270],\n",
            "        [ 0.0318,  0.0217,  0.0330, -0.0077,  0.0169, -0.0060, -0.0046, -0.0017]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019420480355620384 tensor([[ 0.1515,  0.1284,  0.1406, -0.0179,  0.0731, -0.0472, -0.0244,  0.0145],\n",
            "        [ 0.1172,  0.0909,  0.1151, -0.0140,  0.0525, -0.0152, -0.0113, -0.0265],\n",
            "        [ 0.0986,  0.0557,  0.0655, -0.0150,  0.0522, -0.0403, -0.0182,  0.0244],\n",
            "        [ 0.0345,  0.0220,  0.0361, -0.0094,  0.0182, -0.0045, -0.0046, -0.0053]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023264646530151367 tensor([[ 0.1100,  0.1036,  0.1135, -0.0091,  0.0553, -0.0484, -0.0198,  0.0431],\n",
            "        [ 0.0958,  0.0929,  0.1023, -0.0082,  0.0482, -0.0429, -0.0168,  0.0383],\n",
            "        [ 0.0611,  0.0536,  0.0591, -0.0063,  0.0319, -0.0290, -0.0119,  0.0273],\n",
            "        [ 0.0224,  0.0169,  0.0193, -0.0015,  0.0107, -0.0084, -0.0030,  0.0058]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0180350374430418 tensor([[ 0.1293,  0.1163,  0.1284, -0.0117,  0.0642, -0.0509, -0.0230,  0.0379],\n",
            "        [ 0.0992,  0.0923,  0.1040, -0.0097,  0.0487, -0.0373, -0.0163,  0.0240],\n",
            "        [ 0.0755,  0.0595,  0.0671, -0.0092,  0.0393, -0.0333, -0.0149,  0.0277],\n",
            "        [ 0.0353,  0.0241,  0.0285, -0.0019,  0.0162, -0.0112, -0.0038,  0.0062]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01788971573114395 tensor([[ 0.1392,  0.1195,  0.1327, -0.0136,  0.0683, -0.0488, -0.0244,  0.0278],\n",
            "        [ 0.1001,  0.0889,  0.1029, -0.0110,  0.0479, -0.0304, -0.0154,  0.0084],\n",
            "        [ 0.0835,  0.0590,  0.0683, -0.0115,  0.0434, -0.0343, -0.0167,  0.0246],\n",
            "        [ 0.0450,  0.0281,  0.0342, -0.0020,  0.0200, -0.0122, -0.0039,  0.0045]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017812199890613556 tensor([[ 0.1474,  0.1209,  0.1351, -0.0154,  0.0715, -0.0458, -0.0256,  0.0168],\n",
            "        [ 0.1031,  0.0877,  0.1041, -0.0124,  0.0482, -0.0245, -0.0149, -0.0057],\n",
            "        [ 0.0900,  0.0571,  0.0679, -0.0138,  0.0466, -0.0347, -0.0182,  0.0207],\n",
            "        [ 0.0528,  0.0303,  0.0380, -0.0020,  0.0228, -0.0121, -0.0036,  0.0016]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017749961465597153 tensor([[ 0.1546,  0.1214,  0.1366, -0.0172,  0.0742, -0.0425, -0.0266,  0.0054],\n",
            "        [ 0.1070,  0.0874,  0.1063, -0.0138,  0.0491, -0.0192, -0.0146, -0.0192],\n",
            "        [ 0.0959,  0.0546,  0.0667, -0.0159,  0.0496, -0.0346, -0.0197,  0.0166],\n",
            "        [ 0.0593,  0.0313,  0.0405, -0.0020,  0.0249, -0.0114, -0.0031, -0.0020]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.022369593381881714 tensor([[ 0.1082,  0.1057,  0.1158, -0.0105,  0.0551, -0.0502, -0.0196,  0.0484],\n",
            "        [ 0.0641,  0.0630,  0.0699, -0.0061,  0.0330, -0.0264, -0.0128,  0.0239],\n",
            "        [ 0.0655,  0.0597,  0.0667, -0.0053,  0.0337, -0.0296, -0.0114,  0.0268],\n",
            "        [ 0.0167,  0.0092,  0.0107, -0.0028,  0.0084, -0.0068, -0.0030,  0.0051]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01820451207458973 tensor([[ 0.1307,  0.1254,  0.1382, -0.0148,  0.0665, -0.0575, -0.0239,  0.0518],\n",
            "        [ 0.0540,  0.0508,  0.0585, -0.0065,  0.0275, -0.0143, -0.0116,  0.0069],\n",
            "        [ 0.0739,  0.0620,  0.0717, -0.0061,  0.0374, -0.0297, -0.0113,  0.0217],\n",
            "        [ 0.0255,  0.0095,  0.0128, -0.0050,  0.0124, -0.0084, -0.0044,  0.0046]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01799951307475567 tensor([[ 0.1417,  0.1334,  0.1478, -0.0184,  0.0720, -0.0590, -0.0261,  0.0488],\n",
            "        [ 0.0504,  0.0452,  0.0545, -0.0071,  0.0253, -0.0058, -0.0115, -0.0059],\n",
            "        [ 0.0802,  0.0620,  0.0743, -0.0067,  0.0399, -0.0289, -0.0110,  0.0156],\n",
            "        [ 0.0329,  0.0082,  0.0132, -0.0071,  0.0156, -0.0091, -0.0054,  0.0031]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017932027578353882 tensor([[ 0.1481,  0.1367,  0.1523, -0.0218,  0.0751, -0.0581, -0.0275,  0.0433],\n",
            "        [ 0.0504,  0.0434,  0.0546, -0.0080,  0.0251,  0.0007, -0.0121, -0.0163],\n",
            "        [ 0.0856,  0.0613,  0.0760, -0.0073,  0.0421, -0.0276, -0.0105,  0.0092],\n",
            "        [ 0.0393,  0.0061,  0.0128, -0.0092,  0.0183, -0.0093, -0.0063,  0.0011]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017892222851514816 tensor([[ 0.1527,  0.1380,  0.1546, -0.0249,  0.0772, -0.0563, -0.0285,  0.0367],\n",
            "        [ 0.0519,  0.0431,  0.0563, -0.0089,  0.0256,  0.0063, -0.0130, -0.0256],\n",
            "        [ 0.0905,  0.0601,  0.0771, -0.0078,  0.0440, -0.0262, -0.0099,  0.0027],\n",
            "        [ 0.0453,  0.0036,  0.0119, -0.0112,  0.0209, -0.0093, -0.0071, -0.0011]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01844814233481884 tensor([[ 0.0917,  0.0914,  0.1020, -0.0076,  0.0471, -0.0397, -0.0165,  0.0363],\n",
            "        [ 0.0945,  0.0877,  0.0951, -0.0081,  0.0470, -0.0423, -0.0164,  0.0371],\n",
            "        [ 0.0679,  0.0618,  0.0681, -0.0066,  0.0347, -0.0312, -0.0123,  0.0302],\n",
            "        [ 0.0211,  0.0173,  0.0194, -0.0022,  0.0114, -0.0101, -0.0045,  0.0090]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01406730618327856 tensor([[ 0.0905,  0.0885,  0.1019, -0.0082,  0.0465, -0.0319, -0.0156,  0.0227],\n",
            "        [ 0.1034,  0.0887,  0.0977, -0.0105,  0.0505, -0.0397, -0.0171,  0.0254],\n",
            "        [ 0.0828,  0.0702,  0.0792, -0.0094,  0.0421, -0.0343, -0.0147,  0.0301],\n",
            "        [ 0.0316,  0.0239,  0.0276, -0.0034,  0.0170, -0.0141, -0.0068,  0.0115]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.013962522149085999 tensor([[ 0.0918,  0.0881,  0.1045, -0.0090,  0.0472, -0.0255, -0.0152,  0.0108],\n",
            "        [ 0.1102,  0.0875,  0.0978, -0.0126,  0.0528, -0.0360, -0.0173,  0.0128],\n",
            "        [ 0.0910,  0.0718,  0.0829, -0.0117,  0.0461, -0.0338, -0.0158,  0.0262],\n",
            "        [ 0.0388,  0.0271,  0.0325, -0.0044,  0.0209, -0.0162, -0.0085,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.0139012411236763 tensor([[ 0.0941,  0.0889,  0.1083, -0.0098,  0.0485, -0.0197, -0.0151, -0.0003],\n",
            "        [ 0.1169,  0.0863,  0.0979, -0.0147,  0.0551, -0.0324, -0.0176,  0.0004],\n",
            "        [ 0.0970,  0.0713,  0.0841, -0.0138,  0.0488, -0.0322, -0.0166,  0.0211],\n",
            "        [ 0.0441,  0.0286,  0.0353, -0.0052,  0.0238, -0.0172, -0.0098,  0.0110]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.013849230483174324 tensor([[ 0.0967,  0.0899,  0.1123, -0.0106,  0.0499, -0.0141, -0.0150, -0.0111],\n",
            "        [ 0.1236,  0.0853,  0.0980, -0.0168,  0.0575, -0.0290, -0.0179, -0.0117],\n",
            "        [ 0.1022,  0.0699,  0.0845, -0.0159,  0.0511, -0.0303, -0.0173,  0.0157],\n",
            "        [ 0.0483,  0.0289,  0.0370, -0.0060,  0.0261, -0.0176, -0.0109,  0.0095]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020977288484573364 tensor([[ 0.0900,  0.0852,  0.0954, -0.0076,  0.0461, -0.0394, -0.0169,  0.0358],\n",
            "        [ 0.0900,  0.0852,  0.0924, -0.0076,  0.0452, -0.0413, -0.0159,  0.0364],\n",
            "        [ 0.0578,  0.0539,  0.0603, -0.0055,  0.0291, -0.0257, -0.0096,  0.0228],\n",
            "        [ 0.0240,  0.0194,  0.0206, -0.0023,  0.0130, -0.0115, -0.0048,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016638105735182762 tensor([[ 0.1037,  0.0934,  0.1078, -0.0097,  0.0529, -0.0392, -0.0196,  0.0292],\n",
            "        [ 0.1097,  0.1005,  0.1098, -0.0104,  0.0548, -0.0457, -0.0188,  0.0325],\n",
            "        [ 0.0676,  0.0596,  0.0687, -0.0076,  0.0333, -0.0261, -0.0102,  0.0181],\n",
            "        [ 0.0358,  0.0258,  0.0279, -0.0035,  0.0196, -0.0155, -0.0070,  0.0130]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01649368926882744 tensor([[ 0.1078,  0.0916,  0.1093, -0.0111,  0.0548, -0.0341, -0.0204,  0.0179],\n",
            "        [ 0.1176,  0.1036,  0.1141, -0.0125,  0.0583, -0.0438, -0.0195,  0.0219],\n",
            "        [ 0.0706,  0.0584,  0.0698, -0.0091,  0.0339, -0.0229, -0.0094,  0.0097],\n",
            "        [ 0.0440,  0.0288,  0.0315, -0.0044,  0.0243, -0.0174, -0.0085,  0.0133]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01643529161810875 tensor([[ 0.1120,  0.0901,  0.1110, -0.0125,  0.0568, -0.0293, -0.0213,  0.0068],\n",
            "        [ 0.1239,  0.1052,  0.1166, -0.0144,  0.0610, -0.0412, -0.0199,  0.0106],\n",
            "        [ 0.0736,  0.0572,  0.0707, -0.0106,  0.0344, -0.0197, -0.0087,  0.0014],\n",
            "        [ 0.0505,  0.0301,  0.0333, -0.0053,  0.0281, -0.0184, -0.0097,  0.0126]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016382094472646713 tensor([[ 0.1166,  0.0889,  0.1131, -0.0138,  0.0589, -0.0246, -0.0223, -0.0039],\n",
            "        [ 0.1296,  0.1062,  0.1186, -0.0162,  0.0634, -0.0384, -0.0202, -0.0009],\n",
            "        [ 0.0767,  0.0561,  0.0719, -0.0121,  0.0350, -0.0166, -0.0079, -0.0068],\n",
            "        [ 0.0559,  0.0304,  0.0340, -0.0061,  0.0313, -0.0189, -0.0106,  0.0112]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02342439815402031 tensor([[ 0.1141,  0.1059,  0.1152, -0.0096,  0.0583, -0.0518, -0.0215,  0.0481],\n",
            "        [ 0.1023,  0.0967,  0.1076, -0.0086,  0.0523, -0.0458, -0.0182,  0.0431],\n",
            "        [ 0.0708,  0.0655,  0.0711, -0.0066,  0.0366, -0.0314, -0.0127,  0.0282],\n",
            "        [ 0.0302,  0.0249,  0.0272, -0.0033,  0.0163, -0.0146, -0.0060,  0.0132]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016687285155057907 tensor([[ 0.1400,  0.1229,  0.1345, -0.0129,  0.0709, -0.0584, -0.0264,  0.0477],\n",
            "        [ 0.1242,  0.1139,  0.1300, -0.0113,  0.0633, -0.0491, -0.0213,  0.0404],\n",
            "        [ 0.0980,  0.0868,  0.0951, -0.0099,  0.0504, -0.0392, -0.0172,  0.0299],\n",
            "        [ 0.0474,  0.0358,  0.0399, -0.0057,  0.0262, -0.0215, -0.0096,  0.0182]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016319992020726204 tensor([[ 0.1500,  0.1235,  0.1357, -0.0149,  0.0752, -0.0570, -0.0282,  0.0394],\n",
            "        [ 0.1271,  0.1115,  0.1312, -0.0126,  0.0643, -0.0427, -0.0208,  0.0276],\n",
            "        [ 0.1089,  0.0916,  0.1014, -0.0120,  0.0558, -0.0383, -0.0186,  0.0223],\n",
            "        [ 0.0593,  0.0413,  0.0471, -0.0077,  0.0334, -0.0251, -0.0122,  0.0196]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016228634864091873 tensor([[ 0.1586,  0.1226,  0.1355, -0.0169,  0.0789, -0.0551, -0.0298,  0.0305],\n",
            "        [ 0.1302,  0.1093,  0.1327, -0.0138,  0.0655, -0.0364, -0.0203,  0.0151],\n",
            "        [ 0.1164,  0.0929,  0.1039, -0.0138,  0.0592, -0.0357, -0.0194,  0.0128],\n",
            "        [ 0.0685,  0.0442,  0.0515, -0.0096,  0.0392, -0.0272, -0.0143,  0.0194]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016159139573574066 tensor([[ 0.1668,  0.1215,  0.1349, -0.0188,  0.0823, -0.0529, -0.0314,  0.0214],\n",
            "        [ 0.1340,  0.1080,  0.1350, -0.0151,  0.0670, -0.0306, -0.0200,  0.0032],\n",
            "        [ 0.1227,  0.0931,  0.1050, -0.0155,  0.0621, -0.0324, -0.0200,  0.0028],\n",
            "        [ 0.0761,  0.0454,  0.0541, -0.0114,  0.0441, -0.0284, -0.0162,  0.0181]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02171734720468521 tensor([[ 0.0732,  0.0726,  0.0781, -0.0066,  0.0360, -0.0314, -0.0124,  0.0261],\n",
            "        [ 0.0781,  0.0748,  0.0818, -0.0062,  0.0388, -0.0340, -0.0138,  0.0324],\n",
            "        [ 0.0563,  0.0527,  0.0591, -0.0055,  0.0302, -0.0263, -0.0107,  0.0245],\n",
            "        [ 0.0202,  0.0128,  0.0159, -0.0024,  0.0102, -0.0082, -0.0032,  0.0058]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01870887354016304 tensor([[ 0.0905,  0.0891,  0.0955, -0.0093,  0.0432, -0.0340, -0.0142,  0.0223],\n",
            "        [ 0.0827,  0.0743,  0.0822, -0.0072,  0.0397, -0.0303, -0.0140,  0.0247],\n",
            "        [ 0.0614,  0.0539,  0.0627, -0.0067,  0.0336, -0.0256, -0.0114,  0.0200],\n",
            "        [ 0.0311,  0.0155,  0.0219, -0.0041,  0.0153, -0.0105, -0.0044,  0.0054]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01860218308866024 tensor([[ 0.1000,  0.0976,  0.1041, -0.0114,  0.0463, -0.0326, -0.0146,  0.0141],\n",
            "        [ 0.0851,  0.0714,  0.0801, -0.0081,  0.0394, -0.0253, -0.0138,  0.0158],\n",
            "        [ 0.0640,  0.0525,  0.0635, -0.0077,  0.0356, -0.0237, -0.0115,  0.0143],\n",
            "        [ 0.0397,  0.0159,  0.0254, -0.0057,  0.0192, -0.0115, -0.0051,  0.0036]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018547363579273224 tensor([[ 0.1065,  0.1031,  0.1096, -0.0133,  0.0478, -0.0297, -0.0145,  0.0044],\n",
            "        [ 0.0883,  0.0694,  0.0791, -0.0090,  0.0396, -0.0208, -0.0138,  0.0075],\n",
            "        [ 0.0665,  0.0511,  0.0643, -0.0087,  0.0377, -0.0219, -0.0117,  0.0087],\n",
            "        [ 0.0469,  0.0151,  0.0275, -0.0072,  0.0224, -0.0117, -0.0056,  0.0010]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018504146486520767 tensor([[ 0.1116,  0.1072,  0.1134, -0.0151,  0.0486, -0.0261, -0.0141, -0.0061],\n",
            "        [ 0.0922,  0.0681,  0.0788, -0.0100,  0.0402, -0.0167, -0.0139, -0.0004],\n",
            "        [ 0.0691,  0.0497,  0.0651, -0.0096,  0.0397, -0.0201, -0.0119,  0.0033],\n",
            "        [ 0.0534,  0.0135,  0.0288, -0.0086,  0.0252, -0.0115, -0.0059, -0.0021]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02095850184559822 tensor([[ 0.0708,  0.0729,  0.0798, -0.0074,  0.0358, -0.0287, -0.0124,  0.0242],\n",
            "        [ 0.0669,  0.0586,  0.0642, -0.0053,  0.0341, -0.0298, -0.0128,  0.0298],\n",
            "        [ 0.0422,  0.0365,  0.0413, -0.0035,  0.0214, -0.0180, -0.0067,  0.0152],\n",
            "        [ 0.0199,  0.0141,  0.0132, -0.0016,  0.0092, -0.0088, -0.0032,  0.0071]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018967658281326294 tensor([[ 0.0632,  0.0666,  0.0730, -0.0090,  0.0316, -0.0182, -0.0108,  0.0073],\n",
            "        [ 0.0704,  0.0523,  0.0590, -0.0058,  0.0356, -0.0264, -0.0136,  0.0237],\n",
            "        [ 0.0514,  0.0401,  0.0479, -0.0043,  0.0258, -0.0181, -0.0069,  0.0110],\n",
            "        [ 0.0320,  0.0205,  0.0185, -0.0025,  0.0144, -0.0133, -0.0046,  0.0096]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01887783594429493 tensor([[ 0.0646,  0.0693,  0.0761, -0.0111,  0.0321, -0.0122, -0.0107, -0.0046],\n",
            "        [ 0.0772,  0.0496,  0.0573, -0.0066,  0.0388, -0.0248, -0.0150,  0.0197],\n",
            "        [ 0.0581,  0.0411,  0.0517, -0.0050,  0.0289, -0.0169, -0.0067,  0.0053],\n",
            "        [ 0.0414,  0.0241,  0.0209, -0.0033,  0.0182, -0.0162, -0.0055,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018828053027391434 tensor([[ 0.0662,  0.0722,  0.0795, -0.0133,  0.0326, -0.0063, -0.0107, -0.0162],\n",
            "        [ 0.0834,  0.0463,  0.0551, -0.0074,  0.0418, -0.0229, -0.0163,  0.0156],\n",
            "        [ 0.0632,  0.0406,  0.0537, -0.0055,  0.0311, -0.0148, -0.0062, -0.0013],\n",
            "        [ 0.0489,  0.0260,  0.0213, -0.0039,  0.0210, -0.0181, -0.0061,  0.0098]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018786070868372917 tensor([[ 0.0679,  0.0752,  0.0829, -0.0154,  0.0333, -0.0006, -0.0107, -0.0276],\n",
            "        [ 0.0896,  0.0431,  0.0529, -0.0081,  0.0447, -0.0211, -0.0176,  0.0115],\n",
            "        [ 0.0678,  0.0395,  0.0552, -0.0060,  0.0330, -0.0124, -0.0057, -0.0081],\n",
            "        [ 0.0553,  0.0269,  0.0206, -0.0045,  0.0232, -0.0195, -0.0065,  0.0087]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01748601347208023 tensor([[ 0.0687,  0.0619,  0.0699, -0.0068,  0.0347, -0.0308, -0.0126,  0.0280],\n",
            "        [ 0.0700,  0.0663,  0.0741, -0.0056,  0.0352, -0.0300, -0.0115,  0.0262],\n",
            "        [ 0.0386,  0.0370,  0.0412, -0.0048,  0.0200, -0.0167, -0.0075,  0.0170],\n",
            "        [ 0.0136,  0.0103,  0.0117, -0.0014,  0.0064, -0.0047, -0.0015,  0.0018]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01538962870836258 tensor([[ 0.0696,  0.0554,  0.0656, -0.0087,  0.0349, -0.0274, -0.0128,  0.0197],\n",
            "        [ 0.0819,  0.0738,  0.0850, -0.0069,  0.0404, -0.0291, -0.0119,  0.0191],\n",
            "        [ 0.0369,  0.0339,  0.0398, -0.0066,  0.0194, -0.0123, -0.0076,  0.0113],\n",
            "        [ 0.0205,  0.0137,  0.0168, -0.0022,  0.0090, -0.0051, -0.0013, -0.0006]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015325499698519707 tensor([[ 0.0725,  0.0509,  0.0635, -0.0106,  0.0361, -0.0249, -0.0134,  0.0127],\n",
            "        [ 0.0902,  0.0776,  0.0919, -0.0079,  0.0438, -0.0264, -0.0116,  0.0102],\n",
            "        [ 0.0362,  0.0320,  0.0395, -0.0085,  0.0193, -0.0083, -0.0077,  0.0062],\n",
            "        [ 0.0259,  0.0155,  0.0202, -0.0029,  0.0108, -0.0048, -0.0008, -0.0039]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015285917557775974 tensor([[ 0.0764,  0.0475,  0.0625, -0.0126,  0.0378, -0.0230, -0.0142,  0.0064],\n",
            "        [ 0.0966,  0.0795,  0.0967, -0.0088,  0.0462, -0.0230, -0.0111,  0.0004],\n",
            "        [ 0.0362,  0.0308,  0.0402, -0.0104,  0.0197, -0.0047, -0.0080,  0.0015],\n",
            "        [ 0.0304,  0.0164,  0.0226, -0.0035,  0.0121, -0.0039, -0.0002, -0.0077]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015251465141773224 tensor([[ 0.0808,  0.0445,  0.0620, -0.0147,  0.0398, -0.0214, -0.0150,  0.0005],\n",
            "        [ 0.1021,  0.0805,  0.1005, -0.0096,  0.0481, -0.0191, -0.0103, -0.0098],\n",
            "        [ 0.0368,  0.0301,  0.0414, -0.0123,  0.0202, -0.0014, -0.0084, -0.0028],\n",
            "        [ 0.0343,  0.0168,  0.0244, -0.0041,  0.0131, -0.0028,  0.0005, -0.0119]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.0234703179448843 tensor([[ 0.0955,  0.0855,  0.0933, -0.0086,  0.0486, -0.0441, -0.0178,  0.0412],\n",
            "        [ 0.0961,  0.0924,  0.1006, -0.0073,  0.0478, -0.0433, -0.0166,  0.0400],\n",
            "        [ 0.0610,  0.0579,  0.0643, -0.0055,  0.0316, -0.0258, -0.0103,  0.0213],\n",
            "        [ 0.0237,  0.0164,  0.0177, -0.0027,  0.0130, -0.0117, -0.0051,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018702074885368347 tensor([[ 0.1224,  0.1023,  0.1130, -0.0127,  0.0622, -0.0526, -0.0225,  0.0447],\n",
            "        [ 0.1101,  0.1028,  0.1120, -0.0085,  0.0533, -0.0444, -0.0180,  0.0341],\n",
            "        [ 0.0683,  0.0612,  0.0703, -0.0068,  0.0352, -0.0228, -0.0103,  0.0112],\n",
            "        [ 0.0363,  0.0209,  0.0231, -0.0044,  0.0202, -0.0168, -0.0080,  0.0162]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01850537583231926 tensor([[ 0.1364,  0.1059,  0.1184, -0.0158,  0.0690, -0.0545, -0.0247,  0.0412],\n",
            "        [ 0.1141,  0.1031,  0.1125, -0.0089,  0.0538, -0.0403, -0.0174,  0.0225],\n",
            "        [ 0.0714,  0.0601,  0.0718, -0.0077,  0.0367, -0.0175, -0.0095, -0.0012],\n",
            "        [ 0.0458,  0.0225,  0.0254, -0.0059,  0.0257, -0.0201, -0.0103,  0.0189]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018428245559334755 tensor([[ 0.1468,  0.1058,  0.1197, -0.0187,  0.0740, -0.0546, -0.0263,  0.0357],\n",
            "        [ 0.1179,  0.1033,  0.1129, -0.0094,  0.0541, -0.0360, -0.0168,  0.0108],\n",
            "        [ 0.0750,  0.0596,  0.0739, -0.0086,  0.0383, -0.0126, -0.0088, -0.0131],\n",
            "        [ 0.0537,  0.0224,  0.0258, -0.0072,  0.0304, -0.0226, -0.0122,  0.0206]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018367024138569832 tensor([[ 0.1557,  0.1042,  0.1195, -0.0214,  0.0782, -0.0540, -0.0277,  0.0295],\n",
            "        [ 0.1220,  0.1038,  0.1138, -0.0098,  0.0547, -0.0320, -0.0163, -0.0005],\n",
            "        [ 0.0789,  0.0593,  0.0763, -0.0095,  0.0402, -0.0079, -0.0082, -0.0245],\n",
            "        [ 0.0605,  0.0213,  0.0252, -0.0085,  0.0345, -0.0244, -0.0139,  0.0217]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.017172425985336304 tensor([[ 0.0666,  0.0609,  0.0669, -0.0067,  0.0340, -0.0299, -0.0126,  0.0276],\n",
            "        [ 0.0506,  0.0498,  0.0554, -0.0054,  0.0253, -0.0215, -0.0081,  0.0195],\n",
            "        [ 0.0385,  0.0348,  0.0382, -0.0029,  0.0201, -0.0174, -0.0086,  0.0179],\n",
            "        [ 0.0015,  0.0006,  0.0015, -0.0018,  0.0002,  0.0015, -0.0001, -0.0030]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015468467958271503 tensor([[ 0.0851,  0.0735,  0.0813, -0.0100,  0.0432, -0.0355, -0.0163,  0.0295],\n",
            "        [ 0.0515,  0.0495,  0.0569, -0.0074,  0.0252, -0.0169, -0.0069,  0.0114],\n",
            "        [ 0.0425,  0.0345,  0.0389, -0.0031,  0.0223, -0.0168, -0.0107,  0.0165],\n",
            "        [ 0.0006, -0.0010,  0.0010, -0.0032, -0.0013,  0.0048,  0.0005, -0.0074]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015390638262033463 tensor([[ 0.0955,  0.0780,  0.0868, -0.0127,  0.0484, -0.0372, -0.0185,  0.0272],\n",
            "        [ 0.0508,  0.0475,  0.0569, -0.0092,  0.0242, -0.0112, -0.0054,  0.0025],\n",
            "        [ 0.0453,  0.0332,  0.0384, -0.0033,  0.0239, -0.0156, -0.0126,  0.0145],\n",
            "        [ 0.0006, -0.0019,  0.0014, -0.0047, -0.0023,  0.0076,  0.0011, -0.0113]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015357306227087975 tensor([[ 0.1031,  0.0795,  0.0890, -0.0152,  0.0519, -0.0373, -0.0203,  0.0233],\n",
            "        [ 0.0511,  0.0466,  0.0580, -0.0110,  0.0238, -0.0061, -0.0041, -0.0058],\n",
            "        [ 0.0482,  0.0319,  0.0380, -0.0035,  0.0255, -0.0145, -0.0145,  0.0127],\n",
            "        [ 0.0010, -0.0022,  0.0023, -0.0061, -0.0031,  0.0101,  0.0015, -0.0148]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015333572402596474 tensor([[ 0.1091,  0.0797,  0.0896, -0.0176,  0.0548, -0.0368, -0.0217,  0.0186],\n",
            "        [ 0.0521,  0.0463,  0.0599, -0.0129,  0.0237, -0.0013, -0.0029, -0.0137],\n",
            "        [ 0.0510,  0.0307,  0.0376, -0.0037,  0.0271, -0.0134, -0.0164,  0.0108],\n",
            "        [ 0.0017, -0.0022,  0.0035, -0.0076, -0.0037,  0.0124,  0.0019, -0.0181]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02323261648416519 tensor([[ 0.1068,  0.0993,  0.1100, -0.0082,  0.0538, -0.0490, -0.0189,  0.0467],\n",
            "        [ 0.0677,  0.0682,  0.0726, -0.0060,  0.0331, -0.0308, -0.0118,  0.0263],\n",
            "        [ 0.0491,  0.0396,  0.0438, -0.0039,  0.0258, -0.0230, -0.0103,  0.0229],\n",
            "        [ 0.0157,  0.0154,  0.0161, -0.0026,  0.0070, -0.0083, -0.0023,  0.0056]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01974618248641491 tensor([[ 0.1260,  0.1095,  0.1236, -0.0101,  0.0627, -0.0539, -0.0212,  0.0465],\n",
            "        [ 0.0564,  0.0583,  0.0604, -0.0059,  0.0252, -0.0207, -0.0085,  0.0093],\n",
            "        [ 0.0607,  0.0405,  0.0464, -0.0047,  0.0326, -0.0266, -0.0135,  0.0254],\n",
            "        [ 0.0240,  0.0239,  0.0252, -0.0046,  0.0095, -0.0126, -0.0026,  0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01958698034286499 tensor([[ 0.1392,  0.1135,  0.1306, -0.0116,  0.0684, -0.0556, -0.0223,  0.0425],\n",
            "        [ 0.0544,  0.0579,  0.0586, -0.0064,  0.0220, -0.0156, -0.0068, -0.0021],\n",
            "        [ 0.0701,  0.0395,  0.0468, -0.0054,  0.0381, -0.0289, -0.0163,  0.0267],\n",
            "        [ 0.0290,  0.0293,  0.0308, -0.0064,  0.0103, -0.0151, -0.0024,  0.0058]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019525965675711632 tensor([[ 0.1489,  0.1139,  0.1337, -0.0129,  0.0723, -0.0555, -0.0228,  0.0365],\n",
            "        [ 0.0544,  0.0596,  0.0590, -0.0070,  0.0199, -0.0115, -0.0056, -0.0121],\n",
            "        [ 0.0777,  0.0366,  0.0453, -0.0060,  0.0426, -0.0303, -0.0187,  0.0269],\n",
            "        [ 0.0323,  0.0328,  0.0345, -0.0082,  0.0102, -0.0166, -0.0018,  0.0038]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019486311823129654 tensor([[ 0.1569,  0.1127,  0.1351, -0.0140,  0.0754, -0.0545, -0.0229,  0.0297],\n",
            "        [ 0.0553,  0.0621,  0.0604, -0.0076,  0.0182, -0.0080, -0.0045, -0.0213],\n",
            "        [ 0.0843,  0.0329,  0.0428, -0.0064,  0.0467, -0.0312, -0.0210,  0.0266],\n",
            "        [ 0.0344,  0.0354,  0.0372, -0.0098,  0.0095, -0.0175, -0.0009,  0.0011]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019523901864886284 tensor([[ 0.0685,  0.0637,  0.0721, -0.0050,  0.0325, -0.0279, -0.0105,  0.0220],\n",
            "        [ 0.0527,  0.0434,  0.0479, -0.0055,  0.0263, -0.0237, -0.0101,  0.0211],\n",
            "        [ 0.0534,  0.0499,  0.0568, -0.0059,  0.0282, -0.0202, -0.0097,  0.0199],\n",
            "        [ 0.0218,  0.0177,  0.0177, -0.0018,  0.0106, -0.0109, -0.0039,  0.0095]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017657624557614326 tensor([[ 0.0709,  0.0596,  0.0708, -0.0052,  0.0309, -0.0223, -0.0085,  0.0084],\n",
            "        [ 0.0504,  0.0310,  0.0359, -0.0070,  0.0243, -0.0188, -0.0098,  0.0111],\n",
            "        [ 0.0687,  0.0607,  0.0722, -0.0092,  0.0369, -0.0197, -0.0124,  0.0173],\n",
            "        [ 0.0345,  0.0265,  0.0262, -0.0029,  0.0163, -0.0165, -0.0058,  0.0130]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017517728731036186 tensor([[ 0.0768,  0.0592,  0.0735, -0.0055,  0.0312, -0.0185, -0.0071, -0.0031],\n",
            "        [ 0.0542,  0.0249,  0.0307, -0.0090,  0.0256, -0.0171, -0.0107,  0.0049],\n",
            "        [ 0.0782,  0.0657,  0.0812, -0.0121,  0.0426, -0.0165, -0.0140,  0.0116],\n",
            "        [ 0.0431,  0.0315,  0.0304, -0.0038,  0.0199, -0.0197, -0.0070,  0.0140]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017450565472245216 tensor([[ 0.0827,  0.0589,  0.0763, -0.0059,  0.0315, -0.0148, -0.0058, -0.0144],\n",
            "        [ 0.0593,  0.0202,  0.0271, -0.0109,  0.0275, -0.0161, -0.0117, -0.0004],\n",
            "        [ 0.0846,  0.0675,  0.0866, -0.0148,  0.0466, -0.0117, -0.0150,  0.0042],\n",
            "        [ 0.0494,  0.0342,  0.0322, -0.0045,  0.0223, -0.0217, -0.0077,  0.0135]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01740146614611149 tensor([[ 0.0886,  0.0586,  0.0790, -0.0062,  0.0318, -0.0112, -0.0045, -0.0256],\n",
            "        [ 0.0652,  0.0163,  0.0243, -0.0129,  0.0298, -0.0155, -0.0130, -0.0051],\n",
            "        [ 0.0897,  0.0680,  0.0904, -0.0173,  0.0500, -0.0062, -0.0158, -0.0038],\n",
            "        [ 0.0544,  0.0357,  0.0327, -0.0051,  0.0240, -0.0229, -0.0081,  0.0122]],\n",
            "       device='cuda:0')\n",
            "c tensor([0.0028, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "        -0.0000, -0.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.021019859239459038 tensor([[ 0.1050,  0.0993,  0.1102, -0.0091,  0.0536, -0.0465, -0.0193,  0.0422],\n",
            "        [ 0.0797,  0.0685,  0.0753, -0.0078,  0.0401, -0.0366, -0.0149,  0.0345],\n",
            "        [ 0.0490,  0.0439,  0.0485, -0.0045,  0.0241, -0.0215, -0.0078,  0.0186],\n",
            "        [ 0.0129,  0.0094,  0.0119, -0.0015,  0.0062, -0.0044, -0.0010,  0.0022]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0169648639857769 tensor([[ 0.1154,  0.1031,  0.1173, -0.0115,  0.0583, -0.0445, -0.0208,  0.0322],\n",
            "        [ 0.1003,  0.0778,  0.0876, -0.0115,  0.0502, -0.0415, -0.0191,  0.0345],\n",
            "        [ 0.0654,  0.0553,  0.0621, -0.0064,  0.0315, -0.0260, -0.0093,  0.0185],\n",
            "        [ 0.0187,  0.0116,  0.0164, -0.0022,  0.0084, -0.0044, -0.0003, -0.0001]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016830846667289734 tensor([[ 0.1193,  0.1001,  0.1169, -0.0133,  0.0597, -0.0392, -0.0211,  0.0191],\n",
            "        [ 0.1110,  0.0770,  0.0890, -0.0145,  0.0553, -0.0412, -0.0215,  0.0288],\n",
            "        [ 0.0740,  0.0590,  0.0673, -0.0077,  0.0348, -0.0264, -0.0095,  0.0140],\n",
            "        [ 0.0226,  0.0121,  0.0191, -0.0028,  0.0096, -0.0033,  0.0009, -0.0036]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016772910952568054 tensor([[ 0.1243,  0.0982,  0.1178, -0.0152,  0.0616, -0.0345, -0.0216,  0.0068],\n",
            "        [ 0.1198,  0.0744,  0.0883, -0.0173,  0.0593, -0.0400, -0.0235,  0.0223],\n",
            "        [ 0.0802,  0.0603,  0.0699, -0.0089,  0.0368, -0.0255, -0.0091,  0.0082],\n",
            "        [ 0.0259,  0.0121,  0.0213, -0.0032,  0.0105, -0.0019,  0.0022, -0.0075]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016725514084100723 tensor([[ 0.1298,  0.0969,  0.1193, -0.0170,  0.0639, -0.0302, -0.0222, -0.0049],\n",
            "        [ 0.1279,  0.0712,  0.0870, -0.0201,  0.0631, -0.0385, -0.0253,  0.0155],\n",
            "        [ 0.0853,  0.0605,  0.0714, -0.0100,  0.0383, -0.0241, -0.0086,  0.0018],\n",
            "        [ 0.0289,  0.0117,  0.0231, -0.0037,  0.0113, -0.0003,  0.0036, -0.0115]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.018343748524785042 tensor([[ 0.0938,  0.0906,  0.0987, -0.0094,  0.0482, -0.0428, -0.0182,  0.0401],\n",
            "        [ 0.0691,  0.0624,  0.0697, -0.0064,  0.0357, -0.0315, -0.0129,  0.0301],\n",
            "        [ 0.0487,  0.0471,  0.0524, -0.0041,  0.0260, -0.0215, -0.0085,  0.0182],\n",
            "        [ 0.0148,  0.0118,  0.0126, -0.0016,  0.0081, -0.0080, -0.0028,  0.0062]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015174850821495056 tensor([[ 0.1093,  0.1016,  0.1113, -0.0135,  0.0560, -0.0457, -0.0216,  0.0379],\n",
            "        [ 0.0709,  0.0564,  0.0658, -0.0079,  0.0367, -0.0279, -0.0133,  0.0226],\n",
            "        [ 0.0536,  0.0498,  0.0578, -0.0049,  0.0296, -0.0198, -0.0087,  0.0117],\n",
            "        [ 0.0213,  0.0159,  0.0174, -0.0023,  0.0116, -0.0114, -0.0037,  0.0072]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015098555013537407 tensor([[ 0.1177,  0.1054,  0.1160, -0.0170,  0.0601, -0.0449, -0.0236,  0.0317],\n",
            "        [ 0.0735,  0.0512,  0.0629, -0.0095,  0.0381, -0.0246, -0.0139,  0.0156],\n",
            "        [ 0.0566,  0.0507,  0.0612, -0.0056,  0.0322, -0.0171, -0.0086,  0.0043],\n",
            "        [ 0.0256,  0.0180,  0.0200, -0.0030,  0.0141, -0.0135, -0.0041,  0.0070]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015059060417115688 tensor([[ 0.1237,  0.1068,  0.1180, -0.0204,  0.0629, -0.0430, -0.0252,  0.0242],\n",
            "        [ 0.0774,  0.0473,  0.0616, -0.0112,  0.0403, -0.0219, -0.0148,  0.0095],\n",
            "        [ 0.0592,  0.0511,  0.0641, -0.0062,  0.0346, -0.0142, -0.0084, -0.0032],\n",
            "        [ 0.0288,  0.0190,  0.0214, -0.0035,  0.0159, -0.0150, -0.0043,  0.0061]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01502910628914833 tensor([[ 0.1288,  0.1073,  0.1190, -0.0237,  0.0653, -0.0407, -0.0266,  0.0163],\n",
            "        [ 0.0820,  0.0442,  0.0610, -0.0129,  0.0428, -0.0197, -0.0157,  0.0039],\n",
            "        [ 0.0616,  0.0512,  0.0667, -0.0068,  0.0369, -0.0113, -0.0081, -0.0108],\n",
            "        [ 0.0313,  0.0193,  0.0220, -0.0040,  0.0173, -0.0162, -0.0044,  0.0047]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01886015012860298 tensor([[ 0.0668,  0.0645,  0.0709, -0.0067,  0.0330, -0.0289, -0.0125,  0.0261],\n",
            "        [ 0.0574,  0.0475,  0.0555, -0.0061,  0.0284, -0.0239, -0.0097,  0.0198],\n",
            "        [ 0.0628,  0.0558,  0.0609, -0.0051,  0.0317, -0.0279, -0.0096,  0.0242],\n",
            "        [ 0.0290,  0.0204,  0.0228, -0.0027,  0.0142, -0.0142, -0.0053,  0.0132]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01676749810576439 tensor([[ 0.0628,  0.0568,  0.0634, -0.0082,  0.0295, -0.0217, -0.0119,  0.0142],\n",
            "        [ 0.0566,  0.0358,  0.0478, -0.0080,  0.0265, -0.0172, -0.0081,  0.0068],\n",
            "        [ 0.0757,  0.0612,  0.0679, -0.0067,  0.0379, -0.0299, -0.0097,  0.0202],\n",
            "        [ 0.0476,  0.0305,  0.0347, -0.0043,  0.0226, -0.0222, -0.0082,  0.0195]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01662437804043293 tensor([[ 0.0665,  0.0570,  0.0645, -0.0102,  0.0300, -0.0186, -0.0128,  0.0067],\n",
            "        [ 0.0619,  0.0305,  0.0468, -0.0104,  0.0279, -0.0138, -0.0077, -0.0024],\n",
            "        [ 0.0847,  0.0629,  0.0707, -0.0080,  0.0421, -0.0299, -0.0092,  0.0142],\n",
            "        [ 0.0612,  0.0357,  0.0414, -0.0055,  0.0284, -0.0274, -0.0102,  0.0225]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016560496762394905 tensor([[ 0.0703,  0.0574,  0.0658, -0.0122,  0.0305, -0.0155, -0.0136, -0.0006],\n",
            "        [ 0.0680,  0.0261,  0.0468, -0.0127,  0.0297, -0.0109, -0.0075, -0.0108],\n",
            "        [ 0.0915,  0.0624,  0.0710, -0.0091,  0.0451, -0.0288, -0.0083,  0.0070],\n",
            "        [ 0.0718,  0.0378,  0.0448, -0.0065,  0.0326, -0.0308, -0.0115,  0.0236]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016511745750904083 tensor([[ 0.0740,  0.0577,  0.0670, -0.0142,  0.0311, -0.0125, -0.0145, -0.0079],\n",
            "        [ 0.0746,  0.0222,  0.0473, -0.0150,  0.0318, -0.0083, -0.0074, -0.0189],\n",
            "        [ 0.0975,  0.0611,  0.0704, -0.0102,  0.0477, -0.0274, -0.0073, -0.0004],\n",
            "        [ 0.0804,  0.0382,  0.0463, -0.0074,  0.0359, -0.0332, -0.0124,  0.0235]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025538675487041473 tensor([[ 0.1051,  0.1012,  0.1099, -0.0097,  0.0541, -0.0475, -0.0194,  0.0425],\n",
            "        [ 0.0847,  0.0763,  0.0859, -0.0084,  0.0436, -0.0366, -0.0148,  0.0336],\n",
            "        [ 0.0683,  0.0632,  0.0673, -0.0051,  0.0348, -0.0314, -0.0119,  0.0286],\n",
            "        [ 0.0261,  0.0210,  0.0240, -0.0032,  0.0137, -0.0130, -0.0050,  0.0121]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.0203421488404274 tensor([[ 0.1214,  0.1127,  0.1226, -0.0131,  0.0621, -0.0494, -0.0223,  0.0355],\n",
            "        [ 0.1026,  0.0860,  0.1015, -0.0130,  0.0532, -0.0369, -0.0175,  0.0275],\n",
            "        [ 0.0964,  0.0866,  0.0913, -0.0068,  0.0492, -0.0414, -0.0164,  0.0336],\n",
            "        [ 0.0408,  0.0300,  0.0359, -0.0056,  0.0215, -0.0195, -0.0078,  0.0172]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020042190328240395 tensor([[ 0.1263,  0.1126,  0.1225, -0.0156,  0.0645, -0.0457, -0.0229,  0.0228],\n",
            "        [ 0.1076,  0.0823,  0.1028, -0.0166,  0.0562, -0.0303, -0.0177,  0.0145],\n",
            "        [ 0.1100,  0.0953,  0.0994, -0.0075,  0.0560, -0.0439, -0.0182,  0.0303],\n",
            "        [ 0.0507,  0.0342,  0.0428, -0.0078,  0.0267, -0.0230, -0.0096,  0.0191]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.019943561404943466 tensor([[ 0.1316,  0.1128,  0.1228, -0.0180,  0.0669, -0.0422, -0.0237,  0.0105],\n",
            "        [ 0.1122,  0.0783,  0.1037, -0.0201,  0.0590, -0.0238, -0.0179,  0.0016],\n",
            "        [ 0.1191,  0.0994,  0.1026, -0.0079,  0.0605, -0.0440, -0.0191,  0.0244],\n",
            "        [ 0.0580,  0.0360,  0.0472, -0.0098,  0.0307, -0.0252, -0.0110,  0.0194]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.019873404875397682 tensor([[ 0.1371,  0.1133,  0.1234, -0.0205,  0.0695, -0.0389, -0.0245, -0.0015],\n",
            "        [ 0.1174,  0.0749,  0.1052, -0.0236,  0.0621, -0.0175, -0.0182, -0.0108],\n",
            "        [ 0.1262,  0.1015,  0.1035, -0.0081,  0.0639, -0.0431, -0.0197,  0.0175],\n",
            "        [ 0.0638,  0.0362,  0.0500, -0.0117,  0.0338, -0.0264, -0.0120,  0.0187]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020258013159036636 tensor([[ 0.0872,  0.0835,  0.0925, -0.0079,  0.0440, -0.0376, -0.0152,  0.0332],\n",
            "        [ 0.0718,  0.0677,  0.0725, -0.0062,  0.0371, -0.0330, -0.0141,  0.0305],\n",
            "        [ 0.0440,  0.0397,  0.0470, -0.0044,  0.0224, -0.0171, -0.0067,  0.0136],\n",
            "        [ 0.0265,  0.0226,  0.0240, -0.0033,  0.0145, -0.0140, -0.0055,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01714913733303547 tensor([[ 0.1015,  0.0932,  0.1052, -0.0105,  0.0503, -0.0381, -0.0170,  0.0277],\n",
            "        [ 0.0797,  0.0709,  0.0752, -0.0077,  0.0416, -0.0332, -0.0164,  0.0257],\n",
            "        [ 0.0469,  0.0375,  0.0496, -0.0056,  0.0232, -0.0121, -0.0055,  0.0035],\n",
            "        [ 0.0411,  0.0336,  0.0358, -0.0059,  0.0229, -0.0212, -0.0085,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01703302375972271 tensor([[ 0.1083,  0.0952,  0.1093, -0.0125,  0.0527, -0.0347, -0.0174,  0.0181],\n",
            "        [ 0.0837,  0.0701,  0.0736, -0.0089,  0.0442, -0.0314, -0.0181,  0.0187],\n",
            "        [ 0.0490,  0.0345,  0.0513, -0.0068,  0.0237, -0.0067, -0.0041, -0.0068],\n",
            "        [ 0.0511,  0.0400,  0.0427, -0.0081,  0.0287, -0.0258, -0.0106,  0.0173]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01697218418121338 tensor([[ 0.1137,  0.0958,  0.1119, -0.0144,  0.0544, -0.0308, -0.0176,  0.0080],\n",
            "        [ 0.0878,  0.0695,  0.0722, -0.0101,  0.0468, -0.0296, -0.0198,  0.0119],\n",
            "        [ 0.0518,  0.0323,  0.0538, -0.0079,  0.0245, -0.0017, -0.0028, -0.0165],\n",
            "        [ 0.0584,  0.0437,  0.0467, -0.0102,  0.0331, -0.0289, -0.0122,  0.0170]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01692393608391285 tensor([[ 0.1188,  0.0960,  0.1141, -0.0163,  0.0558, -0.0267, -0.0178, -0.0023],\n",
            "        [ 0.0921,  0.0690,  0.0710, -0.0113,  0.0496, -0.0280, -0.0215,  0.0053],\n",
            "        [ 0.0550,  0.0305,  0.0567, -0.0091,  0.0255,  0.0030, -0.0016, -0.0258],\n",
            "        [ 0.0638,  0.0457,  0.0488, -0.0121,  0.0366, -0.0311, -0.0135,  0.0155]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019618581980466843 tensor([[ 0.0713,  0.0658,  0.0720, -0.0056,  0.0361, -0.0300, -0.0129,  0.0274],\n",
            "        [ 0.0697,  0.0667,  0.0743, -0.0057,  0.0353, -0.0315, -0.0116,  0.0285],\n",
            "        [ 0.0131,  0.0060,  0.0078, -0.0014,  0.0049, -0.0034, -0.0014,  0.0002],\n",
            "        [ 0.0158,  0.0167,  0.0160, -0.0007,  0.0081, -0.0075, -0.0027,  0.0060]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017455555498600006 tensor([[ 0.0866,  0.0747,  0.0827, -0.0075,  0.0431, -0.0311, -0.0152,  0.0240],\n",
            "        [ 0.0832,  0.0782,  0.0888, -0.0072,  0.0418, -0.0340, -0.0126,  0.0260],\n",
            "        [ 0.0089, -0.0061, -0.0036, -0.0013,  0.0003,  0.0027,  0.0009, -0.0096],\n",
            "        [ 0.0240,  0.0267,  0.0251, -0.0006,  0.0121, -0.0109, -0.0036,  0.0075]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017319554463028908 tensor([[ 0.0946,  0.0761,  0.0854, -0.0088,  0.0463, -0.0285, -0.0161,  0.0167],\n",
            "        [ 0.0886,  0.0816,  0.0944, -0.0080,  0.0440, -0.0324, -0.0120,  0.0190],\n",
            "        [ 0.0090, -0.0140, -0.0102, -0.0014, -0.0022,  0.0065,  0.0023, -0.0168],\n",
            "        [ 0.0289,  0.0333,  0.0305, -0.0004,  0.0144, -0.0125, -0.0040,  0.0072]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017270013689994812 tensor([[ 0.1011,  0.0759,  0.0863, -0.0100,  0.0488, -0.0251, -0.0168,  0.0086],\n",
            "        [ 0.0922,  0.0832,  0.0981, -0.0087,  0.0453, -0.0299, -0.0112,  0.0111],\n",
            "        [ 0.0120, -0.0189, -0.0137, -0.0018, -0.0031,  0.0088,  0.0033, -0.0221],\n",
            "        [ 0.0320,  0.0380,  0.0338, -0.0001,  0.0158, -0.0131, -0.0040,  0.0058]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017237529158592224 tensor([[ 0.1071,  0.0753,  0.0868, -0.0111,  0.0510, -0.0215, -0.0174,  0.0004],\n",
            "        [ 0.0951,  0.0842,  0.1011, -0.0094,  0.0462, -0.0271, -0.0102,  0.0028],\n",
            "        [ 0.0164, -0.0224, -0.0156, -0.0022, -0.0033,  0.0103,  0.0040, -0.0266],\n",
            "        [ 0.0340,  0.0416,  0.0358,  0.0003,  0.0166, -0.0132, -0.0039,  0.0037]],\n",
            "       device='cuda:0')\n",
            "c tensor([6.5565e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01932332292199135 tensor([[ 0.0890,  0.0806,  0.0906, -0.0090,  0.0441, -0.0369, -0.0157,  0.0321],\n",
            "        [ 0.0721,  0.0701,  0.0786, -0.0072,  0.0371, -0.0324, -0.0132,  0.0300],\n",
            "        [ 0.0550,  0.0480,  0.0529, -0.0042,  0.0285, -0.0255, -0.0098,  0.0242],\n",
            "        [ 0.0184,  0.0161,  0.0164, -0.0023,  0.0091, -0.0092, -0.0037,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016525860875844955 tensor([[ 0.0963,  0.0781,  0.0909, -0.0118,  0.0463, -0.0327, -0.0161,  0.0207],\n",
            "        [ 0.0625,  0.0571,  0.0683, -0.0088,  0.0322, -0.0219, -0.0112,  0.0143],\n",
            "        [ 0.0664,  0.0517,  0.0586, -0.0049,  0.0346, -0.0278, -0.0114,  0.0235],\n",
            "        [ 0.0285,  0.0248,  0.0252, -0.0039,  0.0135, -0.0137, -0.0053,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01641112007200718 tensor([[ 0.1054,  0.0775,  0.0934, -0.0148,  0.0494, -0.0294, -0.0168,  0.0103],\n",
            "        [ 0.0628,  0.0541,  0.0688, -0.0111,  0.0324, -0.0165, -0.0110,  0.0044],\n",
            "        [ 0.0751,  0.0528,  0.0614, -0.0055,  0.0393, -0.0288, -0.0125,  0.0214],\n",
            "        [ 0.0351,  0.0301,  0.0303, -0.0053,  0.0162, -0.0162, -0.0063,  0.0127]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016356002539396286 tensor([[ 0.1135,  0.0761,  0.0950, -0.0178,  0.0520, -0.0256, -0.0174, -0.0006],\n",
            "        [ 0.0644,  0.0525,  0.0709, -0.0134,  0.0333, -0.0120, -0.0111, -0.0043],\n",
            "        [ 0.0819,  0.0518,  0.0618, -0.0060,  0.0429, -0.0288, -0.0132,  0.0182],\n",
            "        [ 0.0396,  0.0334,  0.0333, -0.0066,  0.0177, -0.0175, -0.0069,  0.0126]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016312478110194206 tensor([[ 0.1214,  0.0745,  0.0962, -0.0206,  0.0545, -0.0218, -0.0179, -0.0115],\n",
            "        [ 0.0666,  0.0515,  0.0736, -0.0157,  0.0345, -0.0077, -0.0113, -0.0127],\n",
            "        [ 0.0877,  0.0499,  0.0613, -0.0063,  0.0461, -0.0282, -0.0138,  0.0144],\n",
            "        [ 0.0428,  0.0355,  0.0349, -0.0078,  0.0186, -0.0181, -0.0072,  0.0116]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02182876318693161 tensor([[ 0.0909,  0.0843,  0.0931, -0.0071,  0.0454, -0.0394, -0.0162,  0.0358],\n",
            "        [ 0.0851,  0.0794,  0.0869, -0.0064,  0.0417, -0.0390, -0.0141,  0.0350],\n",
            "        [ 0.0458,  0.0430,  0.0488, -0.0041,  0.0248, -0.0191, -0.0099,  0.0172],\n",
            "        [ 0.0218,  0.0152,  0.0167, -0.0028,  0.0108, -0.0087, -0.0034,  0.0051]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018348418176174164 tensor([[ 0.1032,  0.0897,  0.1012, -0.0088,  0.0506, -0.0386, -0.0176,  0.0287],\n",
            "        [ 0.0958,  0.0848,  0.0934, -0.0071,  0.0447, -0.0399, -0.0142,  0.0289],\n",
            "        [ 0.0501,  0.0441,  0.0528, -0.0047,  0.0275, -0.0160, -0.0118,  0.0101],\n",
            "        [ 0.0328,  0.0191,  0.0217, -0.0049,  0.0159, -0.0107, -0.0044,  0.0033]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018277110531926155 tensor([[ 1.1024e-01,  8.9757e-02,  1.0363e-01, -1.0120e-02,  5.3148e-02,\n",
            "         -3.5126e-02, -1.8113e-02,  1.8911e-02],\n",
            "        [ 1.0157e-01,  8.5203e-02,  9.4635e-02, -7.4840e-03,  4.5290e-02,\n",
            "         -3.8336e-02, -1.3225e-02,  2.0270e-02],\n",
            "        [ 5.2329e-02,  4.3011e-02,  5.4491e-02, -5.0145e-03,  2.9011e-02,\n",
            "         -1.1663e-02, -1.3244e-02,  1.9026e-03],\n",
            "        [ 4.1447e-02,  2.0684e-02,  2.4278e-02, -6.8200e-03,  1.9886e-02,\n",
            "         -1.1511e-02, -5.0616e-03,  1.0252e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018229076638817787 tensor([[ 0.1165,  0.0891,  0.1052, -0.0113,  0.0553, -0.0313, -0.0185,  0.0088],\n",
            "        [ 0.1068,  0.0852,  0.0953, -0.0078,  0.0456, -0.0365, -0.0122,  0.0114],\n",
            "        [ 0.0547,  0.0422,  0.0564, -0.0054,  0.0306, -0.0075, -0.0148, -0.0061],\n",
            "        [ 0.0489,  0.0211,  0.0256, -0.0087,  0.0232, -0.0116, -0.0054, -0.0038]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01818414404988289 tensor([[ 0.1226,  0.0882,  0.1065, -0.0125,  0.0574, -0.0275, -0.0188, -0.0012],\n",
            "        [ 0.1120,  0.0851,  0.0960, -0.0081,  0.0458, -0.0347, -0.0112,  0.0027],\n",
            "        [ 0.0572,  0.0415,  0.0586, -0.0057,  0.0324, -0.0035, -0.0163, -0.0139],\n",
            "        [ 0.0556,  0.0208,  0.0261, -0.0105,  0.0261, -0.0113, -0.0057, -0.0082]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026983089745044708 tensor([[ 0.0941,  0.0864,  0.0944, -0.0096,  0.0470, -0.0445, -0.0173,  0.0394],\n",
            "        [ 0.1035,  0.0981,  0.1086, -0.0079,  0.0525, -0.0446, -0.0177,  0.0402],\n",
            "        [ 0.0694,  0.0629,  0.0686, -0.0066,  0.0359, -0.0334, -0.0135,  0.0310],\n",
            "        [ 0.0238,  0.0172,  0.0203, -0.0024,  0.0121, -0.0104, -0.0039,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.021874312311410904 tensor([[ 0.0932,  0.0770,  0.0841, -0.0120,  0.0455, -0.0411, -0.0177,  0.0282],\n",
            "        [ 0.1224,  0.1109,  0.1260, -0.0097,  0.0615, -0.0439, -0.0194,  0.0307],\n",
            "        [ 0.0885,  0.0759,  0.0837, -0.0095,  0.0462, -0.0406, -0.0178,  0.0332],\n",
            "        [ 0.0361,  0.0221,  0.0283, -0.0038,  0.0182, -0.0141, -0.0053,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.021680371835827827 tensor([[ 0.0948,  0.0703,  0.0768, -0.0145,  0.0455, -0.0391, -0.0185,  0.0191],\n",
            "        [ 0.1322,  0.1143,  0.1330, -0.0108,  0.0659, -0.0387, -0.0195,  0.0166],\n",
            "        [ 0.0966,  0.0778,  0.0870, -0.0117,  0.0509, -0.0421, -0.0201,  0.0291],\n",
            "        [ 0.0451,  0.0238,  0.0330, -0.0049,  0.0226, -0.0159, -0.0060,  0.0097]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.021602874621748924 tensor([[ 0.0993,  0.0668,  0.0729, -0.0171,  0.0470, -0.0386, -0.0199,  0.0118],\n",
            "        [ 0.1404,  0.1160,  0.1382, -0.0117,  0.0694, -0.0329, -0.0192,  0.0020],\n",
            "        [ 0.1018,  0.0770,  0.0872, -0.0137,  0.0540, -0.0421, -0.0218,  0.0233],\n",
            "        [ 0.0523,  0.0238,  0.0358, -0.0059,  0.0261, -0.0167, -0.0064,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.021541714668273926 tensor([[ 0.1047,  0.0643,  0.0702, -0.0198,  0.0491, -0.0387, -0.0214,  0.0052],\n",
            "        [ 0.1477,  0.1168,  0.1423, -0.0125,  0.0725, -0.0267, -0.0189, -0.0129],\n",
            "        [ 0.1062,  0.0752,  0.0864, -0.0156,  0.0567, -0.0416, -0.0233,  0.0171],\n",
            "        [ 0.0585,  0.0228,  0.0376, -0.0069,  0.0291, -0.0169, -0.0066,  0.0060]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024414071813225746 tensor([[ 0.1022,  0.0977,  0.1066, -0.0094,  0.0510, -0.0460, -0.0182,  0.0404],\n",
            "        [ 0.1026,  0.0967,  0.1048, -0.0081,  0.0521, -0.0471, -0.0186,  0.0454],\n",
            "        [ 0.0678,  0.0642,  0.0712, -0.0073,  0.0352, -0.0304, -0.0122,  0.0270],\n",
            "        [ 0.0273,  0.0222,  0.0230, -0.0028,  0.0145, -0.0145, -0.0060,  0.0134]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018703829497098923 tensor([[ 0.1123,  0.1024,  0.1127, -0.0123,  0.0549, -0.0449, -0.0193,  0.0309],\n",
            "        [ 0.1203,  0.1076,  0.1177, -0.0103,  0.0612, -0.0495, -0.0218,  0.0422],\n",
            "        [ 0.0874,  0.0790,  0.0902, -0.0114,  0.0454, -0.0344, -0.0154,  0.0250],\n",
            "        [ 0.0432,  0.0325,  0.0334, -0.0044,  0.0232, -0.0225, -0.0099,  0.0200]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018509086221456528 tensor([[ 0.1154,  0.0999,  0.1109, -0.0147,  0.0551, -0.0403, -0.0191,  0.0181],\n",
            "        [ 0.1265,  0.1065,  0.1176, -0.0115,  0.0644, -0.0459, -0.0227,  0.0327],\n",
            "        [ 0.0959,  0.0824,  0.0969, -0.0148,  0.0498, -0.0325, -0.0166,  0.0166],\n",
            "        [ 0.0543,  0.0382,  0.0389, -0.0058,  0.0294, -0.0277, -0.0128,  0.0236]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018429217860102654 tensor([[ 0.1202,  0.0992,  0.1111, -0.0171,  0.0564, -0.0366, -0.0193,  0.0065],\n",
            "        [ 0.1326,  0.1053,  0.1173, -0.0128,  0.0675, -0.0424, -0.0236,  0.0235],\n",
            "        [ 0.1018,  0.0832,  0.1008, -0.0179,  0.0529, -0.0294, -0.0173,  0.0069],\n",
            "        [ 0.0628,  0.0413,  0.0416, -0.0069,  0.0342, -0.0314, -0.0152,  0.0255]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01836705394089222 tensor([[ 0.1255,  0.0989,  0.1118, -0.0195,  0.0578, -0.0333, -0.0195, -0.0048],\n",
            "        [ 0.1386,  0.1042,  0.1170, -0.0140,  0.0705, -0.0390, -0.0246,  0.0144],\n",
            "        [ 0.1067,  0.0829,  0.1035, -0.0209,  0.0554, -0.0257, -0.0178, -0.0034],\n",
            "        [ 0.0696,  0.0427,  0.0425, -0.0080,  0.0382, -0.0342, -0.0172,  0.0263]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019138522446155548 tensor([[ 0.0746,  0.0747,  0.0828, -0.0073,  0.0369, -0.0311, -0.0125,  0.0252],\n",
            "        [ 0.0847,  0.0775,  0.0847, -0.0066,  0.0428, -0.0392, -0.0154,  0.0359],\n",
            "        [ 0.0715,  0.0674,  0.0753, -0.0079,  0.0373, -0.0321, -0.0132,  0.0296],\n",
            "        [ 0.0216,  0.0172,  0.0178, -0.0023,  0.0117, -0.0114, -0.0048,  0.0101]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.015768755227327347 tensor([[ 0.0673,  0.0659,  0.0750, -0.0083,  0.0317, -0.0209, -0.0099,  0.0079],\n",
            "        [ 0.0857,  0.0701,  0.0777, -0.0071,  0.0425, -0.0348, -0.0147,  0.0243],\n",
            "        [ 0.0879,  0.0792,  0.0915, -0.0121,  0.0460, -0.0345, -0.0161,  0.0273],\n",
            "        [ 0.0329,  0.0237,  0.0245, -0.0037,  0.0182, -0.0169, -0.0074,  0.0141]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.015618707984685898 tensor([[ 0.0689,  0.0664,  0.0773, -0.0100,  0.0312, -0.0153, -0.0089, -0.0044],\n",
            "        [ 0.0909,  0.0671,  0.0754, -0.0078,  0.0445, -0.0327, -0.0149,  0.0155],\n",
            "        [ 0.0973,  0.0837,  0.0996, -0.0159,  0.0510, -0.0333, -0.0178,  0.0209],\n",
            "        [ 0.0408,  0.0270,  0.0278, -0.0048,  0.0230, -0.0206, -0.0094,  0.0159]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015555230900645256 tensor([[ 0.0716,  0.0679,  0.0807, -0.0117,  0.0313, -0.0103, -0.0081, -0.0160],\n",
            "        [ 0.0963,  0.0644,  0.0734, -0.0085,  0.0466, -0.0308, -0.0152,  0.0071],\n",
            "        [ 0.1033,  0.0848,  0.1041, -0.0194,  0.0542, -0.0305, -0.0188,  0.0128],\n",
            "        [ 0.0467,  0.0283,  0.0291, -0.0059,  0.0267, -0.0231, -0.0110,  0.0165]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015504937618970871 tensor([[ 0.0745,  0.0697,  0.0843, -0.0134,  0.0314, -0.0055, -0.0074, -0.0273],\n",
            "        [ 0.1022,  0.0621,  0.0718, -0.0092,  0.0489, -0.0292, -0.0155, -0.0009],\n",
            "        [ 0.1080,  0.0847,  0.1071, -0.0227,  0.0569, -0.0270, -0.0196,  0.0041],\n",
            "        [ 0.0514,  0.0285,  0.0292, -0.0069,  0.0299, -0.0250, -0.0124,  0.0164]],\n",
            "       device='cuda:0')\n",
            "c tensor([2.4140e-05, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02032754383981228 tensor([[ 0.0970,  0.0894,  0.1003, -0.0094,  0.0480, -0.0402, -0.0164,  0.0352],\n",
            "        [ 0.0902,  0.0873,  0.0943, -0.0069,  0.0462, -0.0411, -0.0165,  0.0385],\n",
            "        [ 0.0571,  0.0541,  0.0592, -0.0059,  0.0301, -0.0269, -0.0114,  0.0244],\n",
            "        [ 0.0203,  0.0172,  0.0175, -0.0026,  0.0101, -0.0104, -0.0039,  0.0067]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01585942693054676 tensor([[ 0.1137,  0.0978,  0.1135, -0.0132,  0.0546, -0.0387, -0.0176,  0.0260],\n",
            "        [ 0.1062,  0.0997,  0.1082, -0.0085,  0.0543, -0.0431, -0.0192,  0.0349],\n",
            "        [ 0.0679,  0.0619,  0.0690, -0.0084,  0.0364, -0.0293, -0.0143,  0.0222],\n",
            "        [ 0.0303,  0.0244,  0.0244, -0.0043,  0.0145, -0.0149, -0.0057,  0.0070]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01572134718298912 tensor([[ 0.1213,  0.0968,  0.1165, -0.0163,  0.0566, -0.0327, -0.0171,  0.0121],\n",
            "        [ 0.1123,  0.1016,  0.1108, -0.0093,  0.0572, -0.0398, -0.0199,  0.0256],\n",
            "        [ 0.0719,  0.0630,  0.0715, -0.0103,  0.0392, -0.0281, -0.0160,  0.0162],\n",
            "        [ 0.0371,  0.0286,  0.0282, -0.0059,  0.0171, -0.0175, -0.0068,  0.0053]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01566089689731598 tensor([[ 0.1285,  0.0955,  0.1191, -0.0193,  0.0585, -0.0267, -0.0166, -0.0017],\n",
            "        [ 0.1172,  0.1024,  0.1122, -0.0100,  0.0595, -0.0360, -0.0204,  0.0159],\n",
            "        [ 0.0747,  0.0629,  0.0727, -0.0121,  0.0414, -0.0263, -0.0174,  0.0097],\n",
            "        [ 0.0422,  0.0311,  0.0301, -0.0074,  0.0189, -0.0192, -0.0075,  0.0025]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.015604312531650066 tensor([[ 0.1358,  0.0943,  0.1218, -0.0224,  0.0604, -0.0207, -0.0161, -0.0153],\n",
            "        [ 0.1218,  0.1030,  0.1133, -0.0108,  0.0617, -0.0322, -0.0208,  0.0062],\n",
            "        [ 0.0772,  0.0625,  0.0736, -0.0138,  0.0434, -0.0244, -0.0187,  0.0030],\n",
            "        [ 0.0463,  0.0327,  0.0311, -0.0087,  0.0202, -0.0203, -0.0081, -0.0009]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.020237550139427185 tensor([[ 0.0925,  0.0806,  0.0893, -0.0078,  0.0460, -0.0404, -0.0166,  0.0368],\n",
            "        [ 0.0808,  0.0781,  0.0846, -0.0065,  0.0408, -0.0377, -0.0152,  0.0357],\n",
            "        [ 0.0591,  0.0545,  0.0607, -0.0063,  0.0300, -0.0269, -0.0099,  0.0243],\n",
            "        [ 0.0072,  0.0077,  0.0087, -0.0012,  0.0045, -0.0012, -0.0025,  0.0011]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016136374324560165 tensor([[ 0.1234,  0.1003,  0.1134, -0.0114,  0.0606, -0.0486, -0.0214,  0.0391],\n",
            "        [ 0.0977,  0.0925,  0.1000, -0.0083,  0.0485, -0.0424, -0.0181,  0.0359],\n",
            "        [ 0.0727,  0.0630,  0.0723, -0.0091,  0.0364, -0.0298, -0.0112,  0.0224],\n",
            "        [ 0.0081,  0.0089,  0.0109, -0.0020,  0.0053,  0.0017, -0.0035, -0.0020]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01590590365231037 tensor([[ 0.1390,  0.1042,  0.1203, -0.0138,  0.0671, -0.0491, -0.0234,  0.0331],\n",
            "        [ 0.1017,  0.0936,  0.1013, -0.0092,  0.0494, -0.0404, -0.0185,  0.0288],\n",
            "        [ 0.0780,  0.0630,  0.0750, -0.0113,  0.0383, -0.0282, -0.0108,  0.0158],\n",
            "        [ 0.0084,  0.0096,  0.0127, -0.0027,  0.0057,  0.0051, -0.0044, -0.0054]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.015853365883231163 tensor([[ 0.1505,  0.1040,  0.1227, -0.0159,  0.0716, -0.0475, -0.0246,  0.0250],\n",
            "        [ 0.1041,  0.0933,  0.1009, -0.0099,  0.0496, -0.0377, -0.0186,  0.0210],\n",
            "        [ 0.0823,  0.0621,  0.0766, -0.0134,  0.0397, -0.0261, -0.0103,  0.0086],\n",
            "        [ 0.0087,  0.0103,  0.0145, -0.0034,  0.0060,  0.0084, -0.0052, -0.0087]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01581195555627346 tensor([[ 0.1605,  0.1023,  0.1236, -0.0180,  0.0753, -0.0452, -0.0255,  0.0161],\n",
            "        [ 0.1067,  0.0932,  0.1007, -0.0106,  0.0498, -0.0350, -0.0187,  0.0134],\n",
            "        [ 0.0865,  0.0610,  0.0780, -0.0155,  0.0410, -0.0240, -0.0097,  0.0015],\n",
            "        [ 0.0091,  0.0111,  0.0164, -0.0041,  0.0065,  0.0116, -0.0061, -0.0120]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02175278775393963 tensor([[ 0.0952,  0.0919,  0.1015, -0.0097,  0.0484, -0.0420, -0.0169,  0.0373],\n",
            "        [ 0.0671,  0.0615,  0.0681, -0.0064,  0.0340, -0.0298, -0.0130,  0.0286],\n",
            "        [ 0.0646,  0.0609,  0.0654, -0.0045,  0.0321, -0.0293, -0.0111,  0.0231],\n",
            "        [ 0.0243,  0.0137,  0.0167, -0.0027,  0.0130, -0.0105, -0.0049,  0.0101]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.018429279327392578 tensor([[ 0.1031,  0.0954,  0.1073, -0.0130,  0.0523, -0.0396, -0.0176,  0.0277],\n",
            "        [ 0.0635,  0.0515,  0.0592, -0.0079,  0.0315, -0.0232, -0.0126,  0.0189],\n",
            "        [ 0.0720,  0.0652,  0.0695, -0.0046,  0.0343, -0.0288, -0.0110,  0.0140],\n",
            "        [ 0.0382,  0.0157,  0.0218, -0.0046,  0.0209, -0.0146, -0.0079,  0.0138]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018333975225687027 tensor([[ 0.1091,  0.0970,  0.1111, -0.0162,  0.0551, -0.0363, -0.0179,  0.0169],\n",
            "        [ 0.0645,  0.0461,  0.0555, -0.0096,  0.0315, -0.0191, -0.0132,  0.0120],\n",
            "        [ 0.0777,  0.0677,  0.0715, -0.0045,  0.0356, -0.0276, -0.0107,  0.0042],\n",
            "        [ 0.0495,  0.0151,  0.0242, -0.0064,  0.0275, -0.0172, -0.0103,  0.0158]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.018274564296007156 tensor([[ 0.1142,  0.0978,  0.1139, -0.0194,  0.0575, -0.0326, -0.0182,  0.0058],\n",
            "        [ 0.0675,  0.0428,  0.0539, -0.0114,  0.0325, -0.0161, -0.0141,  0.0064],\n",
            "        [ 0.0826,  0.0693,  0.0726, -0.0044,  0.0365, -0.0261, -0.0102, -0.0059],\n",
            "        [ 0.0590,  0.0128,  0.0247, -0.0080,  0.0332, -0.0188, -0.0124,  0.0167]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018221762031316757 tensor([[ 0.1189,  0.0983,  0.1164, -0.0224,  0.0597, -0.0288, -0.0184, -0.0055],\n",
            "        [ 0.0712,  0.0403,  0.0533, -0.0132,  0.0339, -0.0135, -0.0151,  0.0013],\n",
            "        [ 0.0872,  0.0707,  0.0733, -0.0042,  0.0372, -0.0244, -0.0098, -0.0160],\n",
            "        [ 0.0675,  0.0095,  0.0241, -0.0096,  0.0382, -0.0198, -0.0143,  0.0169]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01878952607512474 tensor([[ 0.0579,  0.0542,  0.0595, -0.0052,  0.0290, -0.0258, -0.0112,  0.0241],\n",
            "        [ 0.0468,  0.0471,  0.0485, -0.0041,  0.0222, -0.0222, -0.0085,  0.0202],\n",
            "        [ 0.0072,  0.0057,  0.0084, -0.0027,  0.0047,  0.0011, -0.0022, -0.0021],\n",
            "        [ 0.0163,  0.0057,  0.0077, -0.0011,  0.0080, -0.0069, -0.0027,  0.0061]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017469186335802078 tensor([[ 0.0810,  0.0737,  0.0815, -0.0080,  0.0400, -0.0338, -0.0158,  0.0291],\n",
            "        [ 0.0547,  0.0551,  0.0542, -0.0055,  0.0239, -0.0246, -0.0096,  0.0188],\n",
            "        [ 0.0018, -0.0013,  0.0034, -0.0044,  0.0027,  0.0088, -0.0017, -0.0109],\n",
            "        [ 0.0280,  0.0066,  0.0104, -0.0018,  0.0136, -0.0110, -0.0044,  0.0094]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017298433929681778 tensor([[ 0.0940,  0.0828,  0.0923, -0.0101,  0.0457, -0.0365, -0.0185,  0.0285],\n",
            "        [ 0.0574,  0.0580,  0.0545, -0.0064,  0.0229, -0.0242, -0.0096,  0.0142],\n",
            "        [ 0.0009, -0.0038,  0.0033, -0.0064,  0.0029,  0.0141, -0.0019, -0.0168],\n",
            "        [ 0.0379,  0.0058,  0.0112, -0.0024,  0.0182, -0.0142, -0.0058,  0.0116]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017242981120944023 tensor([[ 0.1025,  0.0875,  0.0982, -0.0119,  0.0491, -0.0369, -0.0203,  0.0254],\n",
            "        [ 0.0589,  0.0598,  0.0536, -0.0073,  0.0211, -0.0231, -0.0094,  0.0088],\n",
            "        [ 0.0014, -0.0050,  0.0047, -0.0085,  0.0037,  0.0186, -0.0023, -0.0217],\n",
            "        [ 0.0464,  0.0036,  0.0104, -0.0029,  0.0220, -0.0167, -0.0069,  0.0130]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.01721053011715412 tensor([[ 0.1088,  0.0899,  0.1017, -0.0136,  0.0513, -0.0361, -0.0218,  0.0211],\n",
            "        [ 0.0603,  0.0616,  0.0526, -0.0081,  0.0193, -0.0220, -0.0091,  0.0035],\n",
            "        [ 0.0024, -0.0058,  0.0065, -0.0105,  0.0048,  0.0228, -0.0029, -0.0263],\n",
            "        [ 0.0540,  0.0006,  0.0087, -0.0033,  0.0254, -0.0187, -0.0078,  0.0138]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01757693849503994 tensor([[ 0.0720,  0.0671,  0.0766, -0.0072,  0.0353, -0.0299, -0.0117,  0.0235],\n",
            "        [ 0.0896,  0.0826,  0.0879, -0.0077,  0.0451, -0.0410, -0.0167,  0.0379],\n",
            "        [ 0.0620,  0.0563,  0.0632, -0.0059,  0.0310, -0.0271, -0.0104,  0.0260],\n",
            "        [ 0.0210,  0.0181,  0.0192, -0.0024,  0.0109, -0.0099, -0.0045,  0.0101]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014391103759407997 tensor([[ 0.0631,  0.0522,  0.0641, -0.0079,  0.0293, -0.0191, -0.0084,  0.0050],\n",
            "        [ 0.0984,  0.0833,  0.0879, -0.0096,  0.0492, -0.0398, -0.0184,  0.0297],\n",
            "        [ 0.0773,  0.0664,  0.0773, -0.0085,  0.0382, -0.0292, -0.0120,  0.0252],\n",
            "        [ 0.0314,  0.0257,  0.0273, -0.0041,  0.0161, -0.0138, -0.0069,  0.0139]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01424193475395441 tensor([[ 0.0657,  0.0490,  0.0645, -0.0095,  0.0294, -0.0142, -0.0072, -0.0068],\n",
            "        [ 0.1071,  0.0840,  0.0877, -0.0114,  0.0533, -0.0387, -0.0201,  0.0220],\n",
            "        [ 0.0862,  0.0699,  0.0843, -0.0106,  0.0420, -0.0280, -0.0123,  0.0206],\n",
            "        [ 0.0384,  0.0299,  0.0317, -0.0056,  0.0196, -0.0157, -0.0087,  0.0154]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014185120351612568 tensor([[ 0.0697,  0.0473,  0.0664, -0.0111,  0.0302, -0.0101, -0.0063, -0.0176],\n",
            "        [ 0.1141,  0.0832,  0.0858, -0.0130,  0.0566, -0.0369, -0.0216,  0.0137],\n",
            "        [ 0.0920,  0.0703,  0.0879, -0.0125,  0.0442, -0.0252, -0.0121,  0.0142],\n",
            "        [ 0.0434,  0.0321,  0.0341, -0.0070,  0.0219, -0.0165, -0.0100,  0.0157]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014142356812953949 tensor([[ 0.0742,  0.0462,  0.0690, -0.0127,  0.0313, -0.0064, -0.0055, -0.0279],\n",
            "        [ 0.1209,  0.0822,  0.0836, -0.0146,  0.0597, -0.0350, -0.0230,  0.0053],\n",
            "        [ 0.0967,  0.0696,  0.0903, -0.0143,  0.0459, -0.0218, -0.0118,  0.0074],\n",
            "        [ 0.0472,  0.0332,  0.0354, -0.0083,  0.0237, -0.0167, -0.0112,  0.0152]],\n",
            "       device='cuda:0')\n",
            "c tensor([1.7881e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.017788007855415344 tensor([[ 0.0544,  0.0570,  0.0637, -0.0051,  0.0268, -0.0219, -0.0094,  0.0183],\n",
            "        [ 0.0455,  0.0386,  0.0426, -0.0037,  0.0233, -0.0217, -0.0081,  0.0174],\n",
            "        [ 0.0311,  0.0256,  0.0310, -0.0047,  0.0160, -0.0112, -0.0058,  0.0112],\n",
            "        [ 0.0158,  0.0113,  0.0144, -0.0012,  0.0076, -0.0064, -0.0015,  0.0061]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01675477623939514 tensor([[ 0.0490,  0.0528,  0.0603, -0.0057,  0.0228, -0.0138, -0.0080,  0.0055],\n",
            "        [ 0.0457,  0.0310,  0.0357, -0.0039,  0.0234, -0.0196, -0.0078,  0.0094],\n",
            "        [ 0.0360,  0.0249,  0.0340, -0.0076,  0.0182, -0.0086, -0.0063,  0.0077],\n",
            "        [ 0.0248,  0.0162,  0.0224, -0.0019,  0.0117, -0.0087, -0.0015,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01669829711318016 tensor([[ 0.0502,  0.0554,  0.0644, -0.0068,  0.0223, -0.0092, -0.0078, -0.0037],\n",
            "        [ 0.0505,  0.0282,  0.0338, -0.0043,  0.0258, -0.0199, -0.0084,  0.0041],\n",
            "        [ 0.0408,  0.0240,  0.0367, -0.0104,  0.0203, -0.0059, -0.0069,  0.0042],\n",
            "        [ 0.0309,  0.0183,  0.0274, -0.0025,  0.0142, -0.0094, -0.0009,  0.0079]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016665106639266014 tensor([[ 5.1137e-02,  5.7862e-02,  6.8307e-02, -7.7957e-03,  2.1678e-02,\n",
            "         -4.4298e-03, -7.5412e-03, -1.2832e-02],\n",
            "        [ 5.5173e-02,  2.5387e-02,  3.1911e-02, -4.7797e-03,  2.8234e-02,\n",
            "         -2.0210e-02, -8.9401e-03, -1.0824e-03],\n",
            "        [ 4.4751e-02,  2.2277e-02,  3.8519e-02, -1.3092e-02,  2.2035e-02,\n",
            "         -2.8157e-03, -7.2497e-03,  2.0266e-04],\n",
            "        [ 3.5319e-02,  1.8767e-02,  3.0627e-02, -2.9224e-03,  1.5966e-02,\n",
            "         -9.2798e-03, -8.4043e-05,  6.8712e-03]], device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016636593267321587 tensor([[ 0.0522,  0.0603,  0.0722, -0.0088,  0.0211,  0.0002, -0.0073, -0.0219],\n",
            "        [ 0.0600,  0.0228,  0.0302, -0.0052,  0.0307, -0.0206, -0.0095, -0.0061],\n",
            "        [ 0.0485,  0.0203,  0.0401, -0.0158,  0.0236,  0.0004, -0.0076, -0.0039],\n",
            "        [ 0.0389,  0.0184,  0.0329, -0.0033,  0.0172, -0.0087,  0.0009,  0.0053]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01849270984530449 tensor([[ 0.0344,  0.0342,  0.0377, -0.0037,  0.0162, -0.0146, -0.0066,  0.0095],\n",
            "        [ 0.0620,  0.0541,  0.0620, -0.0053,  0.0316, -0.0267, -0.0097,  0.0249],\n",
            "        [ 0.0557,  0.0522,  0.0566, -0.0051,  0.0276, -0.0266, -0.0105,  0.0257],\n",
            "        [ 0.0122,  0.0116,  0.0111, -0.0018,  0.0069, -0.0068, -0.0037,  0.0069]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016953017562627792 tensor([[ 0.0248,  0.0235,  0.0261, -0.0039,  0.0099, -0.0070, -0.0048, -0.0040],\n",
            "        [ 0.0670,  0.0498,  0.0617, -0.0066,  0.0343, -0.0233, -0.0088,  0.0176],\n",
            "        [ 0.0727,  0.0656,  0.0717, -0.0074,  0.0350, -0.0328, -0.0137,  0.0292],\n",
            "        [ 0.0172,  0.0166,  0.0155, -0.0028,  0.0101, -0.0094, -0.0060,  0.0093]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.016839589923620224 tensor([[ 0.0262,  0.0238,  0.0266, -0.0048,  0.0093, -0.0050, -0.0051, -0.0111],\n",
            "        [ 0.0731,  0.0468,  0.0626, -0.0080,  0.0375, -0.0207, -0.0082,  0.0114],\n",
            "        [ 0.0821,  0.0715,  0.0785, -0.0092,  0.0386, -0.0349, -0.0154,  0.0283],\n",
            "        [ 0.0197,  0.0191,  0.0175, -0.0035,  0.0122, -0.0104, -0.0078,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016805926337838173 tensor([[ 0.0282,  0.0246,  0.0276, -0.0058,  0.0089, -0.0033, -0.0054, -0.0178],\n",
            "        [ 0.0785,  0.0430,  0.0625, -0.0093,  0.0404, -0.0178, -0.0074,  0.0048],\n",
            "        [ 0.0879,  0.0737,  0.0813, -0.0107,  0.0404, -0.0353, -0.0164,  0.0253],\n",
            "        [ 0.0210,  0.0205,  0.0182, -0.0041,  0.0136, -0.0109, -0.0093,  0.0102]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016784200444817543 tensor([[ 0.0303,  0.0256,  0.0290, -0.0067,  0.0087, -0.0018, -0.0058, -0.0242],\n",
            "        [ 0.0839,  0.0393,  0.0625, -0.0106,  0.0432, -0.0149, -0.0067, -0.0015],\n",
            "        [ 0.0923,  0.0745,  0.0826, -0.0121,  0.0414, -0.0348, -0.0172,  0.0216],\n",
            "        [ 0.0217,  0.0213,  0.0184, -0.0047,  0.0147, -0.0109, -0.0108,  0.0100]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.019189076498150826 tensor([[ 0.0401,  0.0335,  0.0374, -0.0038,  0.0181, -0.0135, -0.0073,  0.0133],\n",
            "        [ 0.0167,  0.0132,  0.0149, -0.0026,  0.0086, -0.0081, -0.0028,  0.0071],\n",
            "        [ 0.0070,  0.0085,  0.0130, -0.0010,  0.0037,  0.0005, -0.0019,  0.0007],\n",
            "        [-0.0035, -0.0038, -0.0062,  0.0011, -0.0041,  0.0012,  0.0029, -0.0036]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.01880602166056633 tensor([[ 0.0549,  0.0415,  0.0471, -0.0058,  0.0231, -0.0139, -0.0098,  0.0122],\n",
            "        [ 0.0194,  0.0123,  0.0151, -0.0043,  0.0101, -0.0086, -0.0031,  0.0063],\n",
            "        [ 0.0063,  0.0094,  0.0179, -0.0015,  0.0033,  0.0052, -0.0022, -0.0029],\n",
            "        [-0.0048, -0.0053, -0.0097,  0.0020, -0.0072,  0.0013,  0.0057, -0.0058]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.018764788284897804 tensor([[ 0.0638,  0.0435,  0.0503, -0.0073,  0.0250, -0.0113, -0.0112,  0.0078],\n",
            "        [ 0.0207,  0.0100,  0.0139, -0.0059,  0.0110, -0.0084, -0.0030,  0.0048],\n",
            "        [ 0.0050,  0.0098,  0.0224, -0.0020,  0.0026,  0.0100, -0.0024, -0.0066],\n",
            "        [-0.0049, -0.0056, -0.0119,  0.0029, -0.0097,  0.0008,  0.0082, -0.0072]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.01874503493309021 tensor([[ 0.0708,  0.0436,  0.0514, -0.0088,  0.0260, -0.0077, -0.0123,  0.0024],\n",
            "        [ 0.0221,  0.0078,  0.0129, -0.0076,  0.0120, -0.0083, -0.0030,  0.0035],\n",
            "        [ 0.0039,  0.0103,  0.0270, -0.0024,  0.0020,  0.0148, -0.0026, -0.0102],\n",
            "        [-0.0044, -0.0053, -0.0134,  0.0037, -0.0119, -0.0001,  0.0106, -0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.018725905567407608 tensor([[ 0.0772,  0.0432,  0.0518, -0.0101,  0.0267, -0.0038, -0.0133, -0.0033],\n",
            "        [ 0.0236,  0.0058,  0.0122, -0.0092,  0.0130, -0.0083, -0.0030,  0.0023],\n",
            "        [ 0.0029,  0.0110,  0.0317, -0.0029,  0.0014,  0.0194, -0.0028, -0.0137],\n",
            "        [-0.0036, -0.0046, -0.0146,  0.0046, -0.0139, -0.0012,  0.0129, -0.0089]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02829933539032936 tensor([[-1.5049e-02, -2.0447e-02, -1.6050e-02,  1.4305e-05, -7.7915e-03,\n",
            "          1.1387e-02,  3.3331e-03, -1.7872e-02],\n",
            "        [ 5.7640e-02,  3.6240e-02,  4.0245e-02, -5.5790e-03,  2.8915e-02,\n",
            "         -2.3670e-02, -9.9277e-03,  2.0828e-02],\n",
            "        [ 5.2185e-02,  4.2915e-02,  4.7226e-02, -4.7708e-03,  2.5997e-02,\n",
            "         -2.5196e-02, -8.9216e-03,  2.4281e-02],\n",
            "        [ 2.8968e-03,  6.1560e-03,  6.9284e-03, -1.5342e-03,  1.5390e-03,\n",
            "         -2.3305e-04, -1.5450e-03,  1.8656e-04]], device='cuda:0')\n",
            "update_h0 loss, lz 1 0.027102015912532806 tensor([[-0.0130, -0.0242, -0.0141, -0.0007, -0.0068,  0.0141,  0.0037, -0.0244],\n",
            "        [ 0.0754,  0.0336,  0.0384, -0.0084,  0.0376, -0.0269, -0.0124,  0.0195],\n",
            "        [ 0.0781,  0.0602,  0.0667, -0.0077,  0.0389, -0.0366, -0.0132,  0.0333],\n",
            "        [ 0.0016,  0.0085,  0.0103, -0.0029,  0.0009,  0.0022, -0.0021, -0.0028]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.026922771707177162 tensor([[-0.0112, -0.0283, -0.0126, -0.0013, -0.0060,  0.0168,  0.0042, -0.0305],\n",
            "        [ 0.0862,  0.0240,  0.0291, -0.0106,  0.0428, -0.0266, -0.0136,  0.0146],\n",
            "        [ 0.0935,  0.0670,  0.0748, -0.0098,  0.0463, -0.0426, -0.0155,  0.0363],\n",
            "        [-0.0005,  0.0102,  0.0130, -0.0041, -0.0003,  0.0053, -0.0025, -0.0065]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02683991752564907 tensor([[-0.0075, -0.0303, -0.0090, -0.0020, -0.0042,  0.0186,  0.0043, -0.0354],\n",
            "        [ 0.0973,  0.0150,  0.0203, -0.0129,  0.0482, -0.0264, -0.0149,  0.0100],\n",
            "        [ 0.1046,  0.0694,  0.0783, -0.0116,  0.0515, -0.0462, -0.0171,  0.0368],\n",
            "        [-0.0026,  0.0120,  0.0159, -0.0054, -0.0015,  0.0084, -0.0028, -0.0104]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.026779983192682266 tensor([[-0.0045, -0.0331, -0.0061, -0.0026, -0.0027,  0.0206,  0.0045, -0.0406],\n",
            "        [ 0.1085,  0.0063,  0.0119, -0.0151,  0.0537, -0.0264, -0.0162,  0.0058],\n",
            "        [ 0.1134,  0.0696,  0.0792, -0.0132,  0.0554, -0.0487, -0.0182,  0.0360],\n",
            "        [-0.0047,  0.0138,  0.0189, -0.0066, -0.0027,  0.0115, -0.0032, -0.0142]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.025577478110790253 tensor([[ 0.0450,  0.0396,  0.0439, -0.0066,  0.0202, -0.0175, -0.0074,  0.0145],\n",
            "        [ 0.0527,  0.0468,  0.0509, -0.0023,  0.0258, -0.0235, -0.0077,  0.0185],\n",
            "        [ 0.0259,  0.0222,  0.0253, -0.0024,  0.0138, -0.0103, -0.0057,  0.0115],\n",
            "        [ 0.0104,  0.0027,  0.0046, -0.0010,  0.0034, -0.0038, -0.0002,  0.0014]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.024699952453374863 tensor([[ 0.0466,  0.0352,  0.0400, -0.0099,  0.0185, -0.0131, -0.0067,  0.0059],\n",
            "        [ 0.0593,  0.0466,  0.0513, -0.0012,  0.0279, -0.0229, -0.0067,  0.0111],\n",
            "        [ 0.0279,  0.0203,  0.0250, -0.0029,  0.0153, -0.0077, -0.0068,  0.0095],\n",
            "        [ 0.0181,  0.0030,  0.0073, -0.0018,  0.0050, -0.0060,  0.0005,  0.0013]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.024668406695127487 tensor([[ 0.0509,  0.0336,  0.0392, -0.0133,  0.0181, -0.0102, -0.0066, -0.0009],\n",
            "        [ 0.0659,  0.0464,  0.0517, -0.0001,  0.0300, -0.0223, -0.0058,  0.0039],\n",
            "        [ 0.0301,  0.0186,  0.0250, -0.0035,  0.0170, -0.0052, -0.0080,  0.0077],\n",
            "        [ 0.0244,  0.0020,  0.0085, -0.0024,  0.0059, -0.0074,  0.0016,  0.0002]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02464235946536064 tensor([[ 0.0558,  0.0326,  0.0389, -0.0168,  0.0180, -0.0075, -0.0065, -0.0074],\n",
            "        [ 0.0721,  0.0459,  0.0516,  0.0010,  0.0319, -0.0215, -0.0048, -0.0035],\n",
            "        [ 0.0323,  0.0169,  0.0250, -0.0040,  0.0187, -0.0028, -0.0092,  0.0059],\n",
            "        [ 0.0299,  0.0003,  0.0089, -0.0030,  0.0064, -0.0085,  0.0027, -0.0012]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.024619027972221375 tensor([[ 0.0608,  0.0317,  0.0388, -0.0202,  0.0181, -0.0050, -0.0065, -0.0136],\n",
            "        [ 0.0779,  0.0451,  0.0512,  0.0021,  0.0337, -0.0206, -0.0037, -0.0110],\n",
            "        [ 0.0345,  0.0152,  0.0251, -0.0045,  0.0203, -0.0003, -0.0103,  0.0042],\n",
            "        [ 0.0349, -0.0018,  0.0089, -0.0035,  0.0067, -0.0092,  0.0040, -0.0030]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.0204831101000309 tensor([[ 0.0089,  0.0069,  0.0095, -0.0030,  0.0031, -0.0009, -0.0001, -0.0020],\n",
            "        [ 0.0181,  0.0176,  0.0222, -0.0018,  0.0099, -0.0031, -0.0032,  0.0025],\n",
            "        [ 0.0192,  0.0072,  0.0091, -0.0015,  0.0088, -0.0068, -0.0037,  0.0064],\n",
            "        [ 0.0197,  0.0157,  0.0172, -0.0021,  0.0086, -0.0099, -0.0030,  0.0082]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.020285971462726593 tensor([[ 0.0090,  0.0047,  0.0090, -0.0052,  0.0018,  0.0025,  0.0013, -0.0081],\n",
            "        [ 0.0140,  0.0126,  0.0197, -0.0020,  0.0085,  0.0051, -0.0024, -0.0069],\n",
            "        [ 0.0277,  0.0033,  0.0063, -0.0022,  0.0120, -0.0078, -0.0054,  0.0065],\n",
            "        [ 0.0324,  0.0247,  0.0274, -0.0037,  0.0135, -0.0159, -0.0047,  0.0120]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020222902297973633 tensor([[ 0.0117,  0.0052,  0.0114, -0.0076,  0.0017,  0.0047,  0.0023, -0.0129],\n",
            "        [ 0.0163,  0.0141,  0.0243, -0.0025,  0.0103,  0.0099, -0.0028, -0.0126],\n",
            "        [ 0.0360, -0.0007,  0.0034, -0.0029,  0.0152, -0.0087, -0.0070,  0.0065],\n",
            "        [ 0.0411,  0.0298,  0.0333, -0.0051,  0.0164, -0.0196, -0.0055,  0.0132]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02018500119447708 tensor([[ 0.0138,  0.0051,  0.0132, -0.0100,  0.0015,  0.0070,  0.0034, -0.0179],\n",
            "        [ 0.0171,  0.0140,  0.0272, -0.0030,  0.0114,  0.0154, -0.0030, -0.0189],\n",
            "        [ 0.0433, -0.0056, -0.0005, -0.0035,  0.0178, -0.0091, -0.0084,  0.0060],\n",
            "        [ 0.0474,  0.0325,  0.0367, -0.0064,  0.0180, -0.0220, -0.0059,  0.0129]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020157037302851677 tensor([[ 0.0160,  0.0052,  0.0151, -0.0123,  0.0012,  0.0092,  0.0045, -0.0228],\n",
            "        [ 0.0184,  0.0144,  0.0305, -0.0035,  0.0128,  0.0206, -0.0032, -0.0249],\n",
            "        [ 0.0504, -0.0106, -0.0045, -0.0041,  0.0204, -0.0094, -0.0098,  0.0055],\n",
            "        [ 0.0522,  0.0338,  0.0386, -0.0075,  0.0189, -0.0237, -0.0060,  0.0118]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.01592252030968666 tensor([[ 0.0530,  0.0544,  0.0612, -0.0049,  0.0284, -0.0220, -0.0105,  0.0190],\n",
            "        [ 0.0640,  0.0512,  0.0560, -0.0058,  0.0306, -0.0291, -0.0104,  0.0256],\n",
            "        [ 0.0444,  0.0384,  0.0436, -0.0045,  0.0218, -0.0182, -0.0077,  0.0168],\n",
            "        [ 0.0084,  0.0096,  0.0102, -0.0011,  0.0041, -0.0035, -0.0015,  0.0018]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.014440013095736504 tensor([[ 0.0386,  0.0397,  0.0474, -0.0047,  0.0223, -0.0099, -0.0085,  0.0025],\n",
            "        [ 0.0739,  0.0475,  0.0534, -0.0077,  0.0337, -0.0299, -0.0107,  0.0202],\n",
            "        [ 0.0598,  0.0482,  0.0566, -0.0069,  0.0286, -0.0207, -0.0097,  0.0169],\n",
            "        [ 0.0102,  0.0129,  0.0142, -0.0018,  0.0048, -0.0029, -0.0017, -0.0008]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01431779284030199 tensor([[ 0.0398,  0.0409,  0.0509, -0.0055,  0.0242, -0.0057, -0.0093, -0.0054],\n",
            "        [ 0.0844,  0.0446,  0.0516, -0.0097,  0.0371, -0.0312, -0.0112,  0.0155],\n",
            "        [ 0.0702,  0.0531,  0.0640, -0.0090,  0.0328, -0.0206, -0.0108,  0.0143],\n",
            "        [ 0.0106,  0.0148,  0.0168, -0.0023,  0.0048, -0.0014, -0.0017, -0.0044]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.014271430671215057 tensor([[ 0.0402,  0.0412,  0.0535, -0.0063,  0.0257, -0.0011, -0.0099, -0.0135],\n",
            "        [ 0.0929,  0.0400,  0.0477, -0.0115,  0.0396, -0.0315, -0.0113,  0.0099],\n",
            "        [ 0.0774,  0.0547,  0.0681, -0.0109,  0.0353, -0.0189, -0.0114,  0.0098],\n",
            "        [ 0.0106,  0.0162,  0.0190, -0.0029,  0.0045,  0.0004, -0.0015, -0.0084]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.014235427603125572 tensor([[ 0.0411,  0.0421,  0.0566, -0.0071,  0.0274,  0.0031, -0.0107, -0.0213],\n",
            "        [ 0.1014,  0.0354,  0.0439, -0.0132,  0.0420, -0.0319, -0.0114,  0.0043],\n",
            "        [ 0.0833,  0.0550,  0.0707, -0.0127,  0.0371, -0.0165, -0.0117,  0.0047],\n",
            "        [ 0.0103,  0.0175,  0.0211, -0.0034,  0.0042,  0.0023, -0.0014, -0.0124]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02057509869337082 tensor([[ 0.0926,  0.0828,  0.0914, -0.0087,  0.0470, -0.0411, -0.0176,  0.0368],\n",
            "        [ 0.0927,  0.0900,  0.0980, -0.0081,  0.0476, -0.0423, -0.0169,  0.0403],\n",
            "        [ 0.0458,  0.0401,  0.0445, -0.0041,  0.0232, -0.0203, -0.0083,  0.0171],\n",
            "        [ 0.0232,  0.0137,  0.0153, -0.0018,  0.0112, -0.0094, -0.0035,  0.0068]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.016968023031949997 tensor([[ 0.1023,  0.0821,  0.0926, -0.0113,  0.0516, -0.0395, -0.0194,  0.0283],\n",
            "        [ 0.0993,  0.0932,  0.1028, -0.0106,  0.0513, -0.0402, -0.0180,  0.0330],\n",
            "        [ 0.0455,  0.0344,  0.0399, -0.0046,  0.0221, -0.0165, -0.0077,  0.0081],\n",
            "        [ 0.0375,  0.0176,  0.0205, -0.0026,  0.0177, -0.0132, -0.0051,  0.0078]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.01689537987112999 tensor([[ 0.1102,  0.0796,  0.0919, -0.0137,  0.0553, -0.0371, -0.0208,  0.0190],\n",
            "        [ 0.1039,  0.0942,  0.1054, -0.0129,  0.0538, -0.0370, -0.0187,  0.0245],\n",
            "        [ 0.0481,  0.0316,  0.0384, -0.0052,  0.0225, -0.0142, -0.0076,  0.0008],\n",
            "        [ 0.0494,  0.0192,  0.0233, -0.0033,  0.0229, -0.0157, -0.0061,  0.0074]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.016840843483805656 tensor([[ 0.1179,  0.0770,  0.0910, -0.0160,  0.0589, -0.0347, -0.0222,  0.0097],\n",
            "        [ 0.1078,  0.0947,  0.1073, -0.0152,  0.0561, -0.0336, -0.0193,  0.0158],\n",
            "        [ 0.0516,  0.0297,  0.0379, -0.0059,  0.0233, -0.0123, -0.0076, -0.0058],\n",
            "        [ 0.0597,  0.0192,  0.0242, -0.0039,  0.0272, -0.0173, -0.0069,  0.0060]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.016791902482509613 tensor([[ 0.1255,  0.0744,  0.0901, -0.0184,  0.0624, -0.0322, -0.0237,  0.0005],\n",
            "        [ 0.1115,  0.0950,  0.1089, -0.0174,  0.0582, -0.0301, -0.0199,  0.0071],\n",
            "        [ 0.0554,  0.0282,  0.0378, -0.0066,  0.0243, -0.0106, -0.0077, -0.0122],\n",
            "        [ 0.0688,  0.0181,  0.0240, -0.0044,  0.0309, -0.0183, -0.0075,  0.0039]],\n",
            "       device='cuda:0')\n",
            "c tensor([5.9605e-08, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.0248960480093956 tensor([[ 0.0854,  0.0909,  0.0995, -0.0088,  0.0452, -0.0370, -0.0187,  0.0359],\n",
            "        [ 0.0824,  0.0698,  0.0783, -0.0072,  0.0406, -0.0377, -0.0132,  0.0335],\n",
            "        [ 0.0510,  0.0431,  0.0475, -0.0042,  0.0253, -0.0237, -0.0100,  0.0233],\n",
            "        [ 0.0125,  0.0124,  0.0142, -0.0015,  0.0057, -0.0055, -0.0016,  0.0024]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02168884128332138 tensor([[ 0.0736,  0.0832,  0.0915, -0.0104,  0.0408, -0.0251, -0.0199,  0.0203],\n",
            "        [ 0.0970,  0.0710,  0.0841, -0.0096,  0.0466, -0.0387, -0.0135,  0.0276],\n",
            "        [ 0.0658,  0.0504,  0.0561, -0.0054,  0.0319, -0.0293, -0.0130,  0.0272],\n",
            "        [ 0.0189,  0.0191,  0.0231, -0.0023,  0.0076, -0.0074, -0.0014,  0.0011]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02156434766948223 tensor([[ 0.0705,  0.0844,  0.0930, -0.0126,  0.0408, -0.0177, -0.0226,  0.0097],\n",
            "        [ 0.1085,  0.0693,  0.0866, -0.0118,  0.0511, -0.0383, -0.0133,  0.0203],\n",
            "        [ 0.0758,  0.0527,  0.0593, -0.0063,  0.0359, -0.0322, -0.0152,  0.0283],\n",
            "        [ 0.0229,  0.0234,  0.0295, -0.0029,  0.0081, -0.0078, -0.0005, -0.0018]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.021504797041416168 tensor([[ 0.0686,  0.0867,  0.0959, -0.0148,  0.0414, -0.0109, -0.0256,  0.0002],\n",
            "        [ 0.1179,  0.0656,  0.0868, -0.0138,  0.0544, -0.0369, -0.0128,  0.0119],\n",
            "        [ 0.0832,  0.0525,  0.0598, -0.0070,  0.0386, -0.0337, -0.0168,  0.0279],\n",
            "        [ 0.0253,  0.0264,  0.0344, -0.0033,  0.0078, -0.0074,  0.0006, -0.0055]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.021458150818943977 tensor([[ 0.0673,  0.0897,  0.0995, -0.0170,  0.0424, -0.0046, -0.0286, -0.0089],\n",
            "        [ 0.1267,  0.0613,  0.0864, -0.0157,  0.0575, -0.0353, -0.0121,  0.0034],\n",
            "        [ 0.0894,  0.0512,  0.0591, -0.0076,  0.0407, -0.0347, -0.0182,  0.0269],\n",
            "        [ 0.0270,  0.0286,  0.0385, -0.0037,  0.0071, -0.0065,  0.0018, -0.0098]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.03042658418416977 tensor([[ 0.1002,  0.0938,  0.1020, -0.0087,  0.0510, -0.0439, -0.0180,  0.0420],\n",
            "        [ 0.0673,  0.0607,  0.0695, -0.0055,  0.0336, -0.0273, -0.0121,  0.0255],\n",
            "        [ 0.0713,  0.0704,  0.0745, -0.0069,  0.0369, -0.0332, -0.0133,  0.0315],\n",
            "        [ 0.0245,  0.0167,  0.0192, -0.0021,  0.0124, -0.0113, -0.0042,  0.0108]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.026404771953821182 tensor([[ 0.1149,  0.1012,  0.1108, -0.0115,  0.0584, -0.0443, -0.0202,  0.0369],\n",
            "        [ 0.0711,  0.0573,  0.0712, -0.0067,  0.0347, -0.0214, -0.0121,  0.0156],\n",
            "        [ 0.0806,  0.0784,  0.0817, -0.0091,  0.0420, -0.0343, -0.0150,  0.0281],\n",
            "        [ 0.0364,  0.0195,  0.0240, -0.0033,  0.0183, -0.0149, -0.0059,  0.0138]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.0263203252106905 tensor([[ 0.1227,  0.1017,  0.1119, -0.0137,  0.0623, -0.0413, -0.0211,  0.0282],\n",
            "        [ 0.0736,  0.0526,  0.0715, -0.0077,  0.0353, -0.0148, -0.0119,  0.0053],\n",
            "        [ 0.0839,  0.0804,  0.0823, -0.0109,  0.0441, -0.0324, -0.0156,  0.0217],\n",
            "        [ 0.0455,  0.0195,  0.0259, -0.0042,  0.0228, -0.0168, -0.0071,  0.0151]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02626946195960045 tensor([[ 0.1298,  0.1014,  0.1122, -0.0159,  0.0658, -0.0379, -0.0219,  0.0190],\n",
            "        [ 0.0774,  0.0491,  0.0732, -0.0088,  0.0365, -0.0090, -0.0120, -0.0040],\n",
            "        [ 0.0867,  0.0818,  0.0823, -0.0127,  0.0458, -0.0302, -0.0161,  0.0150],\n",
            "        [ 0.0533,  0.0182,  0.0264, -0.0051,  0.0265, -0.0180, -0.0080,  0.0155]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.026223182678222656 tensor([[ 0.1365,  0.1008,  0.1121, -0.0181,  0.0691, -0.0343, -0.0227,  0.0098],\n",
            "        [ 0.0818,  0.0464,  0.0756, -0.0100,  0.0381, -0.0036, -0.0122, -0.0128],\n",
            "        [ 0.0893,  0.0830,  0.0820, -0.0143,  0.0475, -0.0281, -0.0165,  0.0084],\n",
            "        [ 0.0602,  0.0161,  0.0260, -0.0060,  0.0297, -0.0187, -0.0087,  0.0153]],\n",
            "       device='cuda:0')\n",
            "c tensor([9.5367e-07, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "        -0.0000e+00, -0.0000e+00], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.024682000279426575 tensor([[ 0.0640,  0.0586,  0.0641, -0.0065,  0.0325, -0.0298, -0.0123,  0.0242],\n",
            "        [ 0.0593,  0.0521,  0.0571, -0.0043,  0.0295, -0.0261, -0.0106,  0.0254],\n",
            "        [ 0.0480,  0.0496,  0.0536, -0.0041,  0.0237, -0.0218, -0.0082,  0.0203],\n",
            "        [ 0.0116,  0.0059,  0.0062, -0.0030,  0.0056, -0.0057, -0.0029,  0.0059]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.022779278457164764 tensor([[ 0.0671,  0.0557,  0.0612, -0.0083,  0.0337, -0.0284, -0.0134,  0.0155],\n",
            "        [ 0.0689,  0.0540,  0.0604, -0.0049,  0.0337, -0.0259, -0.0118,  0.0223],\n",
            "        [ 0.0546,  0.0584,  0.0630, -0.0053,  0.0257, -0.0224, -0.0087,  0.0176],\n",
            "        [ 0.0178,  0.0064,  0.0069, -0.0056,  0.0080, -0.0083, -0.0045,  0.0086]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02273513376712799 tensor([[ 0.0705,  0.0533,  0.0588, -0.0102,  0.0351, -0.0272, -0.0146,  0.0073],\n",
            "        [ 0.0757,  0.0530,  0.0606, -0.0052,  0.0365, -0.0242, -0.0124,  0.0178],\n",
            "        [ 0.0568,  0.0628,  0.0676, -0.0063,  0.0255, -0.0207, -0.0083,  0.0124],\n",
            "        [ 0.0227,  0.0056,  0.0062, -0.0082,  0.0097, -0.0101, -0.0059,  0.0104]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.022707119584083557 tensor([[ 0.0747,  0.0517,  0.0573, -0.0121,  0.0370, -0.0265, -0.0160, -0.0003],\n",
            "        [ 0.0819,  0.0513,  0.0601, -0.0055,  0.0390, -0.0223, -0.0130,  0.0130],\n",
            "        [ 0.0578,  0.0660,  0.0708, -0.0071,  0.0246, -0.0183, -0.0078,  0.0066],\n",
            "        [ 0.0268,  0.0041,  0.0048, -0.0108,  0.0109, -0.0115, -0.0071,  0.0119]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.02267959900200367 tensor([[ 0.0793,  0.0505,  0.0562, -0.0140,  0.0390, -0.0260, -0.0174, -0.0077],\n",
            "        [ 0.0878,  0.0495,  0.0594, -0.0057,  0.0414, -0.0202, -0.0135,  0.0081],\n",
            "        [ 0.0583,  0.0686,  0.0735, -0.0078,  0.0235, -0.0158, -0.0071,  0.0005],\n",
            "        [ 0.0305,  0.0023,  0.0030, -0.0133,  0.0120, -0.0127, -0.0082,  0.0131]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.026821328327059746 tensor([[ 0.1003,  0.0881,  0.0965, -0.0081,  0.0509, -0.0460, -0.0185,  0.0413],\n",
            "        [ 0.0795,  0.0760,  0.0822, -0.0065,  0.0395, -0.0363, -0.0142,  0.0353],\n",
            "        [ 0.0662,  0.0644,  0.0711, -0.0071,  0.0339, -0.0282, -0.0120,  0.0261],\n",
            "        [ 0.0243,  0.0177,  0.0199, -0.0020,  0.0121, -0.0116, -0.0041,  0.0100]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02230735868215561 tensor([[ 0.1295,  0.1049,  0.1164, -0.0112,  0.0653, -0.0550, -0.0232,  0.0426],\n",
            "        [ 0.0880,  0.0814,  0.0887, -0.0083,  0.0432, -0.0354, -0.0154,  0.0307],\n",
            "        [ 0.0810,  0.0770,  0.0865, -0.0109,  0.0410, -0.0291, -0.0143,  0.0227],\n",
            "        [ 0.0381,  0.0244,  0.0283, -0.0027,  0.0183, -0.0169, -0.0058,  0.0133]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.02209003083407879 tensor([[ 0.1452,  0.1078,  0.1212, -0.0133,  0.0727, -0.0570, -0.0254,  0.0364],\n",
            "        [ 0.0887,  0.0791,  0.0869, -0.0095,  0.0429, -0.0304, -0.0151,  0.0219],\n",
            "        [ 0.0864,  0.0800,  0.0914, -0.0139,  0.0432, -0.0251, -0.0149,  0.0140],\n",
            "        [ 0.0480,  0.0273,  0.0327, -0.0030,  0.0225, -0.0199, -0.0066,  0.0141]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.02201632596552372 tensor([[ 0.1572,  0.1069,  0.1219, -0.0152,  0.0781, -0.0572, -0.0269,  0.0283],\n",
            "        [ 0.0902,  0.0775,  0.0860, -0.0107,  0.0431, -0.0257, -0.0149,  0.0137],\n",
            "        [ 0.0903,  0.0815,  0.0947, -0.0168,  0.0446, -0.0205, -0.0152,  0.0047],\n",
            "        [ 0.0558,  0.0282,  0.0349, -0.0032,  0.0256, -0.0218, -0.0070,  0.0136]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.021961240097880363 tensor([[ 0.1678,  0.1047,  0.1211, -0.0169,  0.0828, -0.0566, -0.0281,  0.0194],\n",
            "        [ 0.0926,  0.0768,  0.0862, -0.0120,  0.0438, -0.0216, -0.0149,  0.0060],\n",
            "        [ 0.0937,  0.0825,  0.0974, -0.0196,  0.0458, -0.0157, -0.0154, -0.0047],\n",
            "        [ 0.0624,  0.0279,  0.0357, -0.0034,  0.0280, -0.0230, -0.0071,  0.0123]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.02568417228758335 tensor([[ 0.1016,  0.1020,  0.1110, -0.0107,  0.0521, -0.0441, -0.0189,  0.0369],\n",
            "        [ 0.0941,  0.0787,  0.0864, -0.0079,  0.0475, -0.0429, -0.0168,  0.0400],\n",
            "        [ 0.0661,  0.0595,  0.0661, -0.0062,  0.0332, -0.0299, -0.0121,  0.0288],\n",
            "        [ 0.0255,  0.0225,  0.0241, -0.0028,  0.0127, -0.0123, -0.0048,  0.0105]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.02098078466951847 tensor([[ 0.0751,  0.0737,  0.0805, -0.0118,  0.0385, -0.0230, -0.0143,  0.0038],\n",
            "        [ 0.1281,  0.0965,  0.1086, -0.0117,  0.0647, -0.0527, -0.0223,  0.0435],\n",
            "        [ 0.0975,  0.0854,  0.0966, -0.0100,  0.0490, -0.0414, -0.0179,  0.0373],\n",
            "        [ 0.0408,  0.0345,  0.0370, -0.0046,  0.0196, -0.0189, -0.0072,  0.0146]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.020277276635169983 tensor([[ 0.0777,  0.0755,  0.0827, -0.0148,  0.0400, -0.0170, -0.0151, -0.0120],\n",
            "        [ 0.1511,  0.1044,  0.1197, -0.0147,  0.0761, -0.0571, -0.0257,  0.0411],\n",
            "        [ 0.1151,  0.0976,  0.1121, -0.0129,  0.0576, -0.0456, -0.0211,  0.0376],\n",
            "        [ 0.0511,  0.0416,  0.0446, -0.0060,  0.0237, -0.0227, -0.0086,  0.0153]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 3 0.020099934190511703 tensor([[ 0.0772,  0.0742,  0.0814, -0.0173,  0.0402, -0.0097, -0.0153, -0.0286],\n",
            "        [ 0.1654,  0.1034,  0.1212, -0.0170,  0.0830, -0.0570, -0.0274,  0.0340],\n",
            "        [ 0.1248,  0.1018,  0.1192, -0.0153,  0.0621, -0.0455, -0.0229,  0.0334],\n",
            "        [ 0.0583,  0.0458,  0.0491, -0.0072,  0.0260, -0.0248, -0.0093,  0.0141]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.020011698827147484 tensor([[ 0.0797,  0.0759,  0.0835, -0.0200,  0.0419, -0.0040, -0.0161, -0.0432],\n",
            "        [ 0.1777,  0.1005,  0.1206, -0.0191,  0.0889, -0.0559, -0.0287,  0.0260],\n",
            "        [ 0.1316,  0.1031,  0.1233, -0.0174,  0.0651, -0.0440, -0.0240,  0.0277],\n",
            "        [ 0.0639,  0.0483,  0.0519, -0.0083,  0.0275, -0.0259, -0.0097,  0.0119]],\n",
            "       device='cuda:0')\n",
            "c tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "update_h0 loss, lz 0 0.023183343932032585 tensor([[ 0.1117,  0.1048,  0.1150, -0.0097,  0.0566, -0.0488, -0.0198,  0.0436],\n",
            "        [ 0.0824,  0.0776,  0.0845, -0.0079,  0.0418, -0.0374, -0.0151,  0.0348],\n",
            "        [ 0.0614,  0.0559,  0.0611, -0.0059,  0.0311, -0.0282, -0.0109,  0.0256],\n",
            "        [ 0.0163,  0.0142,  0.0182, -0.0024,  0.0091, -0.0055, -0.0031,  0.0049]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 1 0.017734624445438385 tensor([[ 0.1461,  0.1323,  0.1471, -0.0142,  0.0731, -0.0575, -0.0248,  0.0438],\n",
            "        [ 0.1005,  0.0915,  0.1005, -0.0111,  0.0510, -0.0407, -0.0184,  0.0326],\n",
            "        [ 0.0859,  0.0745,  0.0817, -0.0091,  0.0432, -0.0370, -0.0148,  0.0297],\n",
            "        [ 0.0226,  0.0182,  0.0261, -0.0039,  0.0130, -0.0047, -0.0041,  0.0034]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 2 0.017351500689983368 tensor([[ 1.5908e-01,  1.3761e-01,  1.5513e-01, -1.7059e-02,  7.8454e-02,\n",
            "         -5.5392e-02, -2.5874e-02,  3.2603e-02],\n",
            "        [ 1.0279e-01,  8.9320e-02,  9.9099e-02, -1.3158e-02,  5.2024e-02,\n",
            "         -3.5658e-02, -1.8631e-02,  2.1631e-02],\n",
            "        [ 9.7218e-02,  7.9589e-02,  8.7819e-02, -1.1375e-02,  4.8385e-02,\n",
            "         -3.8954e-02, -1.6338e-02,  2.6376e-02],\n",
            "        [ 2.6038e-02,  1.9363e-02,  3.1157e-02, -5.2059e-03,  1.5389e-02,\n",
            "         -2.2143e-03, -4.5377e-03,  7.3910e-05]], device='cuda:0')\n",
            "update_h0 loss, lz 3 0.017270024865865707 tensor([[ 0.1683,  0.1391,  0.1590, -0.0196,  0.0818, -0.0515, -0.0262,  0.0195],\n",
            "        [ 0.1050,  0.0870,  0.0977, -0.0151,  0.0530, -0.0306, -0.0189,  0.0108],\n",
            "        [ 0.1051,  0.0813,  0.0902, -0.0133,  0.0518, -0.0391, -0.0172,  0.0213],\n",
            "        [ 0.0285,  0.0196,  0.0352, -0.0064,  0.0173,  0.0009, -0.0048, -0.0039]],\n",
            "       device='cuda:0')\n",
            "update_h0 loss, lz 4 0.017211101949214935 tensor([[ 0.1764,  0.1394,  0.1616, -0.0221,  0.0847, -0.0470, -0.0264,  0.0060],\n",
            "        [ 0.1081,  0.0856,  0.0973, -0.0171,  0.0544, -0.0260, -0.0193,  0.0007],\n",
            "        [ 0.1117,  0.0817,  0.0911, -0.0152,  0.0545, -0.0387, -0.0178,  0.0155],\n",
            "        [ 0.0305,  0.0193,  0.0388, -0.0076,  0.0189,  0.0042, -0.0049, -0.0081]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(30):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    for _ in range(5):\n",
        "        buffer = simulate(agent, buffer)\n",
        "\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    c_loader = make_weighted(buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    # train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "    checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    torch.save(checkpoint, folder+'agentoptimargm.pkl')\n",
        "\n",
        "    buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "    with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# print(optim.param_groups[0][\"lr\"])\n",
        "optim.param_groups[0][\"lr\"] = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d342af7f92c04cd1812d5f2af52d1d63",
            "3a32d8c27bbd4c4190d3223fb11fb424",
            "6161ef63f81d47fa9514bef5354eba8c",
            "759457001f0d4d87952ab6ce357016f4",
            "d266c1f1011c43c5b45be7b1aac8aa04",
            "3644071176a64744871b0fb4582f26bc",
            "f5d37034b5724d9faa9533afd15ba36b",
            "1c409ca121644fa7b40d5a9bd2492ad1"
          ]
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "857f770a-edf4-4b67-ff92-ee701516bb6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  self.hub = sentry_sdk.Hub(client)\n",
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111280885555617, max=1.0)…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d342af7f92c04cd1812d5f2af52d1d63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240916_010951-xw6krj4q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/xw6krj4q' target=\"_blank\">northern-surf-49</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/xw6krj4q' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/xw6krj4q</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        _,T,_ = sx.shape\n",
        "        batch = 1\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            # print(i,x.data, z.squeeze(), loss.item())\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbad0ffc-e8e9-4a4d-bd3c-d6d9ddaf6e21",
        "cellView": "form",
        "id": "2mY7BITKjSKC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2336301\n",
            "1278976\n",
            "399360\n",
            "1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-e20d23bca149>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=3, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # e = d_model**-0.5\n",
        "        # self.h0 = torch.empty((self.jepa.pred.num_layers, 1, d_model), device=device).uniform_(-e, e) # [num_layers, batch, d_model]\n",
        "        # self.h0 = torch.normal(mean=0, std=e, size=(self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # torch.nn.init.xavier_uniform_(self.h0)\n",
        "        # torch.nn.init.kaiming_normal_(self.h0)\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "        state = torch.zeros((1, 3,64,64), device=device)\n",
        "        self.sx = self.jepa.enc(state)\n",
        "\n",
        "    # def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        self.update_h0(lstate, laction)\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "            # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.h0=lh0[k].unsqueeze(1) # [num_layers, 1, d_model]\n",
        "        # self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        self.lx, self.lz = lx[k:], lz[k:] # [T, dim_a], [T, dim_z]\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "\n",
        "                # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "                # print(torch.cat(lstate, dim=0).shape)\n",
        "                # lsx = self.jepa.enc(torch.stack(lstate, dim=0))#.unsqueeze(0)\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx-torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                # batch, seq_len, _ = lstate.shape\n",
        "                # seq_len, _ = lstate.shape\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    try: la = self.emb(self.la[:seq_len])\n",
        "                    except:\n",
        "                        print(\"err self.la\")\n",
        "                        # la = self.emb([0]*seq_len)\n",
        "                        la = self.emb(torch.zeros(seq_len, dtype=int, device=device))\n",
        "\n",
        "        # lz = nn.Parameter(torch.zeros((batch, seq_len, self.dim_z),device=device))\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        # optim_z = torch.optim.SGD([lz], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([lz], 1e0, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "\n",
        "        for i in range(20): # num epochs\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                # print(sxaz.shape, self.h0.shape)\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                # sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                loss = F.mse_loss(out_, out)\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            print(lz.data)\n",
        "            with torch.no_grad(): lz.clamp_(min=-1, max=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1]\n",
        "        self.la = la[k:]\n",
        "        return h0\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                # x[:,:self.lx.shape[1]], z[:,:self.lz.shape[1]] = self.lx[:,:T], self.lz[:,:T]\n",
        "                x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].repeat(batch,1,1), self.lz[:T].repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            # print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            # print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i, \"loss\", loss.squeeze().data)\n",
        "            # print(x.shape,torch.argmax(-dist,dim=-1).shape,z.shape,loss.shape) # [16, 6, 3], [16, 6], [16, 6, 1], [16, 1]\n",
        "            # print(i, torch.cat([x,torch.argmax(-dist,dim=-1),z],dim=-1).squeeze().data)\n",
        "            print(i, \"x act z\", torch.cat([x[0],torch.argmax(-dist,dim=-1)[0].unsqueeze(-1),z[0]],dim=-1).squeeze().data)\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, t, dim_a], [batch, t, dim_z]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    # imshow(state[0].cpu())\n",
        "                    # print(\"norm\", torch.norm(sy[0]-sy_[0], dim=-1))\n",
        "                    # # if torch.norm(sy[0]-sy_[0], dim=-1) > 1:\n",
        "                    # print(i, reward[0])\n",
        "                    # print(sy)\n",
        "                    # print(sy_)\n",
        "                    # print(sy[0]-sy_[0])\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "\n",
        "                    # cost loss\n",
        "                    # syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = 100*clossl\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "\n",
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def argm(self, sx, lr=3e3): # 3e3\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch_size, T, _ = sx.shape\n",
        "        batch = 16\n",
        "        # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "        z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "        optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "        sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "        # sx = sx.detach()\n",
        "        for i in range(20): # 10\n",
        "            # print(sx.shape,z.shape)\n",
        "            sxz = torch.cat([sx, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                cost = model(sxz)\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "            # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "            # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        # return z.detach()\n",
        "        # print(\"argm z\",z.squeeze().data)\n",
        "        # print(\"cost\",cost.squeeze())\n",
        "        idx = torch.argmax(loss)\n",
        "        # return z[idx].detach().unsqueeze(0)\n",
        "        return z[:,idx].detach()\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=1\n",
        "        T=1\n",
        "        x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "\n",
        "        lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "        # ratio = 6e0\n",
        "        lr = 1e-1 # adamw 1e-1\n",
        "        ratio = 4\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        # optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze())\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "        for i in range(50):\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = argm(x)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "            # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "            # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                x.clamp_(min=-1, max=1)\n",
        "            # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "        idx = torch.argmax(loss)\n",
        "        print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HcOidvtW9KAH"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jx0k_ndHOEMe",
        "outputId": "efe5ca4c-7764-463c-b443-e817ea29a565",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 128, 3, 3])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3000x7800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2TGs69fnrZo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b4ea134c-eaee-4269-8c04-6c5e1620458e",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcost.1.weight torch.Size([2, 1024])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRU(262, 256, num_layers=3, batch_first=True, dropout=0.2)\n"
          ]
        }
      ],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "# for name, param in agent.emb.named_parameters():\n",
        "for name, param in agent.tcost.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(vars(agent.jepa.pred.))\n",
        "# print(vars(agent.tcost.state_dict()))\n",
        "# print(agent.jepa.pred._parameters.keys())\n",
        "# print(agent.jepa.pred._parameters['weight_ih_l0'])\n",
        "# print(agent.jepa.pred._parameters['weight_hh_l2']) # weight_hh_l0, weight_hh_l2\n",
        "# print(agent.tcost.state_dict().keys())\n",
        "print(agent.tcost.state_dict()['tcost.1.weight']) # tcost.2.bias, tcost.4.bias\n",
        "# print(agent.tcost.named_parameters())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjEWGq2WGi9a",
        "outputId": "649e3612-f156-496e-d8d5-fc576110e2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0015, -0.0132,  0.0280,  ...,  0.0297,  0.0289,  0.0152],\n",
            "        [ 0.0168,  0.0031, -0.0288,  ..., -0.0064, -0.0137, -0.0085]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title transfer weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 10  # size of the first dimension\n",
        "a = 5   # size of the extra nodes to omit\n",
        "m = 8   # output dimension\n",
        "\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "target_layer = nn.Linear(d, m)\n",
        "# source_layer = nn.Linear(d, m)\n",
        "# target_layer = nn.Linear(d+a, m)\n",
        "\n",
        "def transfer(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt.weight[:, :src.weight.shape[1]].copy_(src.weight[:, :tgt.weight.shape[1]])\n",
        "        tgt.bias.copy_(src.bias)\n",
        "    return tgt,src\n",
        "\n",
        "target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "src_sd = source_layer.state_dict()\n",
        "tgt_sd = target_layer.state_dict()\n",
        "\n",
        "def transfersd(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt['weight'][:, :src['weight'].shape[1]].copy_(src['weight'][:, :tgt['weight'].shape[1]])\n",
        "        tgt['bias'].copy_(src['bias'])\n",
        "    return tgt\n",
        "\n",
        "tgt_sd = transfersd(tgt_sd, src_sd)\n",
        "target_layer.load_state_dict(tgt_sd)\n",
        "\n",
        "\n",
        "agent_src = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "\n",
        "# agent.tcost = TCost((1+agent.jepa.pred.num_layers)*agent.d_model) # replace tcost\n",
        "\n",
        "agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# agent.jepa.pred\n",
        "# target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(vars(agent.jepa.pred))\n",
        "# gru = agent.jepa.pred\n",
        "# gru = agent_src.jepa.pred\n",
        "# for wht_name in gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, gru._parameters[wht_name].shape)\n",
        "\n",
        "# weight_ih_l0 dim_z=3: [768, 262] , dim_z=1: [768, 260]\n",
        "# weight_hh_l0 torch.Size([768, 256])\n",
        "# bias_ih_l0 torch.Size([768])\n",
        "# bias_hh_l0 torch.Size([768])\n",
        "\n",
        "# tgt_gru = agent.jepa.pred\n",
        "# src_gru = agent_src.jepa.pred\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "\n",
        "tgt_gru[]\n",
        "def transfer_gru(tgt_gru, src_gru): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(len(tgt_gru._all_weights), len(src_gru._all_weights))):\n",
        "        # for lyr in tgt_gru._all_weights:\n",
        "            lyr = tgt_gru._all_weights[i]\n",
        "            for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "                # print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "                tgt_wht, src_wht = tgt_gru._parameters[wht_name], src_gru._parameters[wht_name]\n",
        "                if len(tgt_wht.shape)==2:\n",
        "                    tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "                elif len(tgt_wht.shape)==1:\n",
        "                    tgt_gru._parameters[wht_name] = src_wht\n",
        "    return tgt_gru\n",
        "tgt_gru = transfer_gru(tgt_gru, src_gru)\n",
        "\n",
        "# for wht_name in tgt_gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d_model=256; dim_a=3; dim_z=1; dim_v=512\n",
        "\n",
        "pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "# pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "print(pred._all_weights)\n",
        "for lyr in pred._all_weights:\n",
        "    for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "        print(wht_name, pred._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(pred.state_dict().keys())\n",
        "\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "print(tgt_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "print(src_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "\n",
        "print(tgt_gru.state_dict()['bias_ih_l0'][:10])\n",
        "print(src_gru.state_dict()['bias_ih_l0'][:10])\n",
        "tgt_gru.state_dict().keys()\n",
        "src_gru.state_dict().keys()\n",
        "\n",
        "# tgt_gru\n",
        "# src_gru\n",
        "for wht_name in tgt_gru.state_dict().keys():\n",
        "    if not wht_name in src_gru.state_dict().keys(): continue\n",
        "    print(wht_name)\n",
        "    # print(tgt_gru.state_dict()[wht_name])\n",
        "    # tgt_gru.state_dict()[wht_name].copy_(src_gru.state_dict()[wht_name])\n",
        "\n",
        "tgt_sd = tgt_gru.state_dict()\n",
        "src_sd = src_gru.state_dict()\n",
        "def transfer_sd(tgt_sd, src_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            # print(wht_name)\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            if len(tgt_wht.shape)==2:\n",
        "                tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "            elif len(tgt_wht.shape)==1:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "    return tgt_sd\n",
        "tgt_sd = transfer_sd(tgt_sd, src_sd)\n",
        "print(tgt_sd['weight_ih_l0'][0][:10])\n",
        "print(tgt_sd['bias_ih_l0'][:10])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S_xnBFjXVxgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_UlGz6Xoq3"
      },
      "source": [
        "## plot 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VX5IExbRriwm"
      },
      "outputs": [],
      "source": [
        "# @title sklearn RBF\n",
        "# https://gist.github.com/eljost/2c4e1af652ef02b2989da341c5569af7\n",
        "# from nn_plot.ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import scipy.stats as st\n",
        "\n",
        "# np.random.seed(1)\n",
        "def func(x):\n",
        "    # print(x.shape)\n",
        "    # x= np.sum(x**2, axis=-1)\n",
        "    x=np.random.rand(x.shape[0])\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "res = 50\n",
        "num_pts=15\n",
        "X=np.random.rand(num_pts,2)*res\n",
        "# Y = func(X)\n",
        "Y=np.random.rand(num_pts)\n",
        "# print(X);print(Y)\n",
        "\n",
        "lim = 1\n",
        "# lin = np.linspace(-lim, lim, res)\n",
        "lin = np.linspace(0, res, res)\n",
        "x1, x2 = np.meshgrid(lin, lin)\n",
        "xx = np.vstack((x1.flatten(), x2.flatten())).T\n",
        "\n",
        "kernel = RBF()\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gp.fit(X, Y)\n",
        "# print(\"Learned kernel\", gp.kernel_)\n",
        "y_mean, y_cov = gp.predict(xx, return_cov=True)\n",
        "\n",
        "posteriors = st.multivariate_normal.rvs(mean=y_mean, cov=y_cov, size=1)\n",
        "\n",
        "ax = plt.figure().add_subplot(projection='3d')\n",
        "Z=posteriors.reshape(-1, res)\n",
        "# ax.plot_surface(x1, x2, Z)\n",
        "ax.plot_surface(x1, x2, Z, cmap='rainbow', alpha=0.7)\n",
        "\n",
        "# ax.plot_surface(x1, x2, posteriors.reshape(-1, res))\n",
        "ax.contour(x1, x2, Z, zdir='z', offset=-1, cmap='coolwarm') # https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html#sphx-glr-gallery-mplot3d-contour3d-3-py\n",
        "# ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-0.4, 0.5))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-1, 2))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, c=zdata, cmap='Greens');\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, cmap='Greens');\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SusX7gpzxFNL",
        "outputId": "9f14a9da-e188-49ba-f5f5-70192ff33134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-2.6321], grad_fn=<AddBackward0>)\n",
            "tensor([2.7358]) tensor([-4.7358])\n"
          ]
        }
      ],
      "source": [
        "# @title chatgpt RBFKernelLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBFKernelLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        self.centres = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        dists = torch.cdist(x, self.centres, p=2) ** 2\n",
        "        return torch.exp(-dists / (2 * self.sigma ** 2))\n",
        "\n",
        "class SaddlePointNetwork(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(SaddlePointNetwork, self).__init__()\n",
        "        self.rbf_layer = RBFKernelLayer(in_features, out_features, sigma)\n",
        "        self.linear = nn.Linear(out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rbf_output = self.rbf_layer(x)\n",
        "        # Introduce a saddle point structure\n",
        "        linear_output = self.linear(rbf_output)\n",
        "        # Example saddle function: x^2 - y^2\n",
        "        saddle_output = torch.sum(linear_output[:, :1]**2 - linear_output[:, 1:]**2, dim=1, keepdim=True)\n",
        "        return saddle_output\n",
        "\n",
        "# sin(ax)sin(bx)\n",
        "# (x^2 - y^2)\n",
        "import torch\n",
        "\n",
        "def rbf_saddle(x, y, gamma=1.0, a=1.0, b=1.0):\n",
        "    # RBF-like term\n",
        "    rbf_term = torch.exp(-gamma * torch.norm(x - y, p=2)**2)\n",
        "    # Saddle point term\n",
        "    saddle_term = (a * x)**2 - (b * y)**2\n",
        "    return rbf_term + saddle_term\n",
        "\n",
        "# Example usage\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "output = rbf_saddle(x, y)\n",
        "print(output)\n",
        "\n",
        "# Compute gradients\n",
        "output.backward()\n",
        "print(x.grad, y.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rTmCo7pm0NxL"
      },
      "outputs": [],
      "source": [
        "# @title plot 3d\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "y = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "X, Y = torch.meshgrid(x, y)\n",
        "Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fWZaQTDFg1",
        "outputId": "4c5ced88-54f1-436e-89f9-66f1c8396373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000, -0.7231,  0.3792,  0.0000]]) tensor([0.3362])\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title shape NN\n",
        "num_pts=1\n",
        "\n",
        "# X=torch.rand(num_pts,4)*2-1\n",
        "# X=torch.cat([torch.tensor([0,0]).unsqueeze(0),torch.rand(num_pts,2)*2-1], dim=-1)\n",
        "X=torch.cat([torch.zeros(1,1),torch.rand(num_pts,2)*2-1,torch.zeros(1,1)], dim=-1)\n",
        "Y=torch.rand(num_pts)\n",
        "print(X,Y)\n",
        "optim = torch.optim.SGD(model.parameters(), 1e-1)\n",
        "\n",
        "# model.train()\n",
        "pred = model(X)\n",
        "# print(Y.shape,pred.shape)\n",
        "# loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "loss = F.mse_loss(Y, pred.squeeze(-1))\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "boDd__PE2sGy"
      },
      "outputs": [],
      "source": [
        "# @title plot NN\n",
        "\n",
        "xx = torch.linspace(-1, 1, 100)\n",
        "yy = torch.linspace(-1, 1, 100)\n",
        "X, Y = torch.meshgrid(xx, yy) # [100,100]\n",
        "xy = torch.cat([X.unsqueeze(-1), torch.zeros(X.shape+(2,)), Y.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "with torch.no_grad(): Z = model(xy).squeeze(-1)\n",
        "# print(Z)\n",
        "# print(Z.shape)\n",
        "\n",
        "# Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW6BYoXsX57o",
        "outputId": "f6dc0e5f-6def-42fd-f829-e227ec94833b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tensor([-0.1000, -0.1000,  0.1000]) tensor(0.4161) -0.0008160677389241755\n",
            "1 tensor([-0.1999, -0.1999,  0.1999]) tensor(0.8145) -0.000763896678108722\n",
            "2 tensor([-0.2997, -0.2997,  0.2997]) tensor(1.2112) -0.0007120760856196284\n",
            "3 tensor([-0.3994, -0.3994,  0.3994]) tensor(1.3960) -0.0007130901212804019\n",
            "4 tensor([-0.4989, -0.4990,  0.4990]) tensor(1.3960) -0.0007601650431752205\n",
            "5 tensor([-0.5885, -0.5989,  0.5981]) tensor(1.3965) -0.0008037001243792474\n",
            "6 tensor([-0.6703, -0.6989,  0.6969]) tensor(1.3969) -0.0008425545529462397\n",
            "7 tensor([-0.7460, -0.7991,  0.7955]) tensor(1.3972) -0.0008812308078631759\n",
            "8 tensor([-0.8166, -0.8994,  0.8938]) tensor(1.3974) -0.000919759797398001\n",
            "9 tensor([-0.8943, -1.0004,  0.9918]) tensor(1.4013) -0.0009680598159320652\n",
            "10 tensor([-0.9786, -1.1017,  1.0897]) tensor(1.4044) -0.0010272095678374171\n",
            "11 tensor([-1.0680, -1.1023,  1.0977]) tensor(1.4068) -0.0010494425659999251\n",
            "12 tensor([-1.0936, -1.1028,  1.0976]) tensor(1.4087) -0.0010546413250267506\n",
            "13 tensor([-1.0968, -1.1032,  1.0976]) tensor(1.4102) -0.0010546413250267506\n",
            "14 tensor([-1.0994, -1.1035,  1.0975]) tensor(1.4113) -0.0010546413250267506\n",
            "15 tensor([-1.1015, -1.1037,  1.0975]) tensor(1.4123) -0.0010546413250267506\n",
            "16 tensor([-1.1032, -1.1039,  1.0975]) tensor(1.4130) -0.0010546413250267506\n",
            "17 tensor([-1.1045, -1.1040,  1.0975]) tensor(1.4135) -0.0010546413250267506\n",
            "18 tensor([-1.1057, -1.1041,  1.0975]) tensor(1.4139) -0.0010546413250267506\n",
            "19 tensor([-1.1066, -1.1041,  1.0975]) tensor(1.4142) -0.0010546413250267506\n",
            "20 tensor([-1.1073, -1.1042,  1.0975]) tensor(1.4144) -0.0010546413250267506\n",
            "21 tensor([-1.1079, -1.1042,  1.0975]) tensor(1.4145) -0.0010546413250267506\n",
            "22 tensor([-1.1084, -1.1042,  1.0975]) tensor(1.4146) -0.0010546413250267506\n",
            "23 tensor([-1.1088, -1.1042,  1.0975]) tensor(1.4146) -0.0010546413250267506\n",
            "24 tensor([-1.1091, -1.1042,  1.0975]) tensor(1.4145) -0.0010546413250267506\n",
            "25 tensor([-1.1093, -1.1041,  1.0975]) tensor(1.4144) -0.0010546413250267506\n",
            "26 tensor([-1.1094, -1.1041,  1.0976]) tensor(1.4143) -0.0010546413250267506\n",
            "27 tensor([-1.1096, -1.1040,  1.0976]) tensor(1.4141) -0.0010546413250267506\n",
            "28 tensor([-1.1096, -1.1040,  1.0976]) tensor(1.4140) -0.0010546413250267506\n",
            "29 tensor([-1.1097, -1.1039,  1.0976]) tensor(1.4138) -0.0010546413250267506\n",
            "30 tensor([-1.1097, -1.1039,  1.0976]) tensor(1.4135) -0.0010546413250267506\n",
            "31 tensor([-1.1096, -1.1038,  1.0976]) tensor(1.4133) -0.0010546413250267506\n",
            "32 tensor([-1.1096, -1.1037,  1.0977]) tensor(1.4131) -0.0010546413250267506\n",
            "33 tensor([-1.1095, -1.1037,  1.0977]) tensor(1.4128) -0.0010546413250267506\n",
            "34 tensor([-1.1094, -1.1036,  1.0977]) tensor(1.4126) -0.0010546413250267506\n",
            "35 tensor([-1.1093, -1.1035,  1.0977]) tensor(1.4123) -0.0010546413250267506\n",
            "36 tensor([-1.1092, -1.1034,  1.0978]) tensor(1.4121) -0.0010546413250267506\n",
            "37 tensor([-1.1091, -1.1034,  1.0978]) tensor(1.4118) -0.0010546413250267506\n",
            "38 tensor([-1.1090, -1.1033,  1.0978]) tensor(1.4116) -0.0010546413250267506\n",
            "39 tensor([-1.1089, -1.1032,  1.0978]) tensor(1.4113) -0.0010546413250267506\n",
            "40 tensor([-1.1087, -1.1032,  1.0978]) tensor(1.4111) -0.0010546413250267506\n",
            "41 tensor([-1.1086, -1.1031,  1.0979]) tensor(1.4108) -0.0010546413250267506\n",
            "42 tensor([-1.1085, -1.1030,  1.0979]) tensor(1.4106) -0.0010546413250267506\n",
            "43 tensor([-1.1083, -1.1030,  1.0979]) tensor(1.4103) -0.0010546413250267506\n",
            "44 tensor([-1.1082, -1.1029,  1.0979]) tensor(1.4101) -0.0010546413250267506\n",
            "45 tensor([-1.1080, -1.1028,  1.0979]) tensor(1.4098) -0.0010546413250267506\n",
            "46 tensor([-1.1079, -1.1027,  1.0979]) tensor(1.4096) -0.0010546413250267506\n",
            "47 tensor([-1.1077, -1.1027,  1.0980]) tensor(1.4094) -0.0010546413250267506\n",
            "48 tensor([-1.1076, -1.1026,  1.0980]) tensor(1.4091) -0.0010546413250267506\n",
            "49 tensor([-1.1075, -1.1026,  1.0980]) tensor(1.4089) -0.0010546413250267506\n",
            "tensor([-0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011,\n",
            "        -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011, -0.0011],\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "tensor([[-1., -1.,  1.]]) tensor([[1.]]) -0.0010546413250267506\n",
            "tensor([[-1., -1.,  1.]]) tensor([[1.]]) -0.0010546413250267506\n",
            "tensor([[-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011],\n",
            "        [-1.0000, -1.0000,  1.0000,  1.0000, -0.0011]])\n"
          ]
        }
      ],
      "source": [
        "# @title test optim saddle same time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def f(x, y):\n",
        "    return x ** 2 - y ** 2 + x * y\n",
        "# (x-y)(x+y)+xy\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "dim_x, dim_z = 3, 1\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# xx = torch.empty((1, T, dim_x))\n",
        "# torch.nn.init.xavier_uniform_(xx)\n",
        "# x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "\n",
        "# tensor([[0.6478, 0.0531, 0.0861]]) tensor([[-1.,  1.]]) 0.2974517047405243\n",
        "# tensor([-0.9419, -1.0000,  0.4416, -1.0000,  1.0000,  0.2963])\n",
        "\n",
        "# x = nn.Parameter(torch.tensor([[0.6478, 0.0531, 0.0861]]))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "z = nn.Parameter(torch.empty((batch, T, dim_z)))\n",
        "# z = torch.empty((1, T, 1))\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# z = nn.Parameter(z.repeat(batch,1,1))\n",
        "# z = nn.Parameter(torch.tensor([[-1.,  1.]]))\n",
        "# optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "# optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.95), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "# .95,1e-1,3e-1\n",
        "# .99,\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50): # num epochs\n",
        "    # loss = f(x,z)\n",
        "    # loss = f(x.sum(-1),z)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step(); optim_z.step()\n",
        "    optim_x.zero_grad(); optim_z.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x.squeeze()[0].data, z[0].squeeze().data, loss[0].squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "        z.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "# idx = torch.argmax(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJdFpDr2wIMT",
        "outputId": "ff06bd6b-2bec-4392-c749-77cd3f1688ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3976, -1.0000, -1.0000]]) tensor([[-1.,  1.]]) 0.29717573523521423\n"
          ]
        }
      ],
      "source": [
        "# @title test optim saddle argm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def argm(sx, lr=3e3): # 3e3\n",
        "    # batch=sx.size(dim=0)\n",
        "    batch_size, T, _ = sx.shape\n",
        "    batch = 16\n",
        "    # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "    # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "    z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "    torch.nn.init.xavier_uniform_(z)\n",
        "    # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "    # sx = sx.detach()\n",
        "    for i in range(20): # 10\n",
        "        # print(sx.shape,z.shape)\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            cost = model(sxz)\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "        # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    # return z.detach()\n",
        "    idx = torch.argmax(cost.squeeze(), dim=1)\n",
        "    return z[torch.arange(z.shape[0]),idx].detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(xx.clone())\n",
        "x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50):\n",
        "    z = argm(x)\n",
        "    # print(x.shape,z.shape)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "# print(\"z\",z)\n",
        "# print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjIJP6RlEv2",
        "outputId": "447fdefd-452b-437d-c228-1847492b36f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = torch.randn(16, 16)\n",
        "# print((b==torch.max(b)).nonzero())\n",
        "x = torch.randn(10, 3)\n",
        "idx = torch.randint(3,(10,))\n",
        "# print(x[:,idx].shape)\n",
        "print(x[torch.arange(x.shape[0]),idx].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title transfer_optim bad?\n",
        "\n",
        "import torch\n",
        "\n",
        "def transfer_optim(src_optim, tgt_optim, param_mapping):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    tgt_sd = tgt_optim.state_dict()\n",
        "\n",
        "    # Iterate over each parameter in the target optimizer\n",
        "    for (tgt_idx, target_param) in enumerate(tgt_optim.param_groups[0]['params']):\n",
        "        target_id = id(target_param)\n",
        "\n",
        "        # Find the corresponding source parameter using param_mapping\n",
        "        if target_id in param_mapping:\n",
        "            source_param = param_mapping[target_id]\n",
        "            source_id = id(source_param)\n",
        "\n",
        "            # If there's an existing state for the source parameter, transfer it\n",
        "            if source_id in src_sd['state']:\n",
        "                source_state = src_sd['state'][source_id]\n",
        "                target_state = {}\n",
        "\n",
        "                # Handle momentum/first and second moments (e.g., `exp_avg`, `exp_avg_sq` in Adam)\n",
        "                for key in source_state.keys():\n",
        "                    if source_state[key].shape == target_param.shape: target_state[key] = source_state[key].clone()\n",
        "                    # If size doesn't match, either copy what you can or initialise new values\n",
        "                    elif key in ['exp_avg', 'exp_avg_sq']:  # Momentums (specific to Adam-like optimizers)\n",
        "                        target_state[key] = torch.zeros_like(target_param)\n",
        "                        target_state[key][:source_param.numel()] = source_state[key].flatten()[:target_param.numel()]\n",
        "                    else: target_state[key] = torch.zeros_like(target_param) # init\n",
        "                tgt_sd['state'][target_id] = target_state\n",
        "\n",
        "    # Load the updated state dict back into the target optimizer\n",
        "    tgt_optim.load_state_dict(tgt_sd)\n",
        "    return tgt_optim\n",
        "# {'state': {0: {'step': tensor(1.), 'exp_avg': tensor, 'exp_avg_sq': tensor}, 1: }}\n",
        "\n",
        "\n",
        "\n",
        "model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "source_optimizer = optim.AdamW(model_src.parameters())\n",
        "target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "dummy_input = torch.randn(3, 10)\n",
        "dummy_target = torch.randn(3, 5)\n",
        "criterion = torch.nn.MSELoss()\n",
        "output = model_src(dummy_input)\n",
        "loss = criterion(output, dummy_target)\n",
        "loss.backward()\n",
        "source_optimizer.step()\n",
        "\n",
        "param_mapping = {id(tgt_param): src_param for src_param, tgt_param in zip(model_src.parameters(), model_tgt.parameters())}\n",
        "target_optimizer = transfer_optim(source_optimizer, target_optimizer, param_mapping)\n",
        "\n",
        "print(source_optimizer.state_dict())\n",
        "print(target_optimizer.state_dict())\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title transfer_optim bad? 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    opt_state_dict = optimizer.state_dict()\n",
        "    for group in opt_state_dict['param_groups']:\n",
        "        # For each parameter index (p in param group refers to the layer parameters)\n",
        "        for param_idx, p in enumerate(group['params']):\n",
        "            print(p,source_layer.weight)\n",
        "            if p == source_layer.weight:\n",
        "                # Find the corresponding target layer parameter (in this case, target_layer.weight)\n",
        "                target_param = target_layer.weight\n",
        "                source_state = optimizer.state[p]  # Get the state for the source parameter\n",
        "\n",
        "                # If the parameter is found in the optimizer's state dict\n",
        "                if 'exp_avg' in source_state and 'exp_avg_sq' in source_state:\n",
        "                    exp_avg = source_state['exp_avg']  # First moment (momentum)\n",
        "                    exp_avg_sq = source_state['exp_avg_sq']  # Second moment (variance)\n",
        "\n",
        "                    # Handle input dimension mismatch (copy/truncate or pad)\n",
        "                    source_in_dim = source_layer.weight.shape[1]\n",
        "                    target_in_dim = target_layer.weight.shape[1]\n",
        "\n",
        "                    # Copy optimizer state (exp_avg and exp_avg_sq) accordingly\n",
        "                    with torch.no_grad():\n",
        "                        # Copy the available part and initialize new dimensions to zero\n",
        "                        new_exp_avg = torch.zeros_like(target_param)\n",
        "                        new_exp_avg_sq = torch.zeros_like(target_param)\n",
        "                        # new_exp_avg[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        # new_exp_avg_sq[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        new_exp_avg[:, :source_in_dim] = exp_avg[:, :target_in_dim]\n",
        "                        new_exp_avg_sq[:, :source_in_dim] = exp_avg_sq[:, :target_in_dim]\n",
        "\n",
        "                    # Update the target layer's optimizer state\n",
        "                    optimizer.state[target_param] = {\n",
        "                        'exp_avg': new_exp_avg,\n",
        "                        'exp_avg_sq': new_exp_avg_sq,\n",
        "                        'step': source_state['step']  # Keep the same step count\n",
        "                    }\n",
        "\n",
        "                # Handle the bias (if it exists)\n",
        "                if hasattr(source_layer, 'bias') and hasattr(target_layer, 'bias'):\n",
        "                    source_bias = optimizer.state[source_layer.bias]\n",
        "                    target_bias = target_layer.bias\n",
        "\n",
        "                    optimizer.state[target_bias] = source_bias\n",
        "    return optimizer\n",
        "\n",
        "# Example usage:\n",
        "d = 10  # Input dimension of the source layer\n",
        "a = 5   # Extra nodes to be omitted or added in the target layer\n",
        "m = 8   # Output dimension (same for both)\n",
        "\n",
        "# Source layer (input dimension d+a)\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "\n",
        "# Target layer (input dimension d, or d+a, or arbitrary)\n",
        "target_layer = nn.Linear(d, m)\n",
        "\n",
        "# Optimizer (using AdamW in this case)\n",
        "optimizer = torch.optim.AdamW(source_layer.parameters())\n",
        "\n",
        "# Perform weight transfer (from d+a to d or vice versa) here (assumed done already)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "# Transfer optimizer states\n",
        "optimizer = transfer_optimizer_state(source_layer, target_layer, optimizer)\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    state_dict = optimizer.state_dict()\n",
        "    for old_param, new_param in zip(source_layer.parameters(), target_layer.parameters()):\n",
        "        # If old_param exists in optimizer state\n",
        "        if old_param in state_dict['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = state_dict['state'][old_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                if key in ['exp_avg', 'exp_avg_sq']:  # for Adam or AdamW momentum estimates\n",
        "                    # Handle the shape adjustment (copy, shrink, or randomly initialise the extra nodes)\n",
        "                    new_state[key] = torch.zeros_like(new_param)  # Initialise with zeros\n",
        "                    new_state[key][:old_param.shape[0]] = value[:new_param.shape[0]]  # Copy old values\n",
        "                    # else:\n",
        "                    #     new_state[key] = value.clone()  # Copy directly if shapes match\n",
        "                else:\n",
        "                    new_state[key] = value  # Copy other states directly if they exist\n",
        "\n",
        "            # Set the new parameter in optimizer state\n",
        "            state_dict['state'][new_param] = new_state\n",
        "            # Remove the old parameter from the optimizer state\n",
        "            del state_dict['state'][old_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(state_dict)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optim(src_model, tgt_model, src_optim, tgt_optim):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    for src_param, tgt_param in zip(src_model.parameters(), tgt_model.parameters()):\n",
        "        # If src_param exists in optimizer state\n",
        "        if src_param in src_sd['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = src_sd['state'][src_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                new_state[key] = torch.zeros_like(tgt_param)  # Initialise with zeros\n",
        "                new_state[key][:src_param.shape[0]] = value[:tgt_param.shape[0]]  # Copy old values\n",
        "\n",
        "            src_sd['state'][tgt_param] = new_state\n",
        "            del src_sd['state'][src_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(src_sd)\n",
        "    return optimizer\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cX71EprCMSNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rename wht_name\n",
        "# wht_name='jepa.enc.cnn.0.weight'\n",
        "wht_name='jepa.pred.weight_ih_l0'\n",
        "# wht_name='emb.weight'\n",
        "# print(o.isnumeric())\n",
        "# mask = [x.isnumeric() for x in o]\n",
        "# print(o[mask])\n",
        "na_=''\n",
        "# j=0\n",
        "\n",
        "for wht_name in agent.state_dict().keys():\n",
        "    o=wht_name.split('.')\n",
        "    # print(o)\n",
        "    name=wht_name\n",
        "    print(\"####\", wht_name)\n",
        "    for i in range(len(o)):\n",
        "        c = o[i]\n",
        "        if c.isnumeric():\n",
        "            na = '.'.join(o[:i])\n",
        "            me = '.'.join(o[i+1:])\n",
        "            # print(c_,c, c_<c, )\n",
        "            c=int(c)\n",
        "            if na!=na_: # param name diff\n",
        "                j=0 # reset num\n",
        "                c_=c # track wht_name num\n",
        "                na_=na # track param name\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('1', name)\n",
        "            elif c_<c: # same param name, diff num\n",
        "                j+=1\n",
        "                c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('2', name)\n",
        "            else: # same param name, same num\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('3', name)\n",
        "    print('4', name)\n"
      ],
      "metadata": {
        "id": "LKUSzmYLLuRh",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "07ca4b89-257b-4205-c5c8-6a96474ae82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-186620617543>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# j=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mwht_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwht_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(o)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mhkK_9AQm8_q",
        "Jt_UlGz6Xoq3",
        "wUhKd009Qvk3"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d342af7f92c04cd1812d5f2af52d1d63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a32d8c27bbd4c4190d3223fb11fb424",
              "IPY_MODEL_6161ef63f81d47fa9514bef5354eba8c"
            ],
            "layout": "IPY_MODEL_759457001f0d4d87952ab6ce357016f4"
          }
        },
        "3a32d8c27bbd4c4190d3223fb11fb424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d266c1f1011c43c5b45be7b1aac8aa04",
            "placeholder": "​",
            "style": "IPY_MODEL_3644071176a64744871b0fb4582f26bc",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "6161ef63f81d47fa9514bef5354eba8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5d37034b5724d9faa9533afd15ba36b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c409ca121644fa7b40d5a9bd2492ad1",
            "value": 1
          }
        },
        "759457001f0d4d87952ab6ce357016f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d266c1f1011c43c5b45be7b1aac8aa04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3644071176a64744871b0fb4582f26bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5d37034b5724d9faa9533afd15ba36b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c409ca121644fa7b40d5a9bd2492ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}