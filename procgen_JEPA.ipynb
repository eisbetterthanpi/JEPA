{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WkwnVjJTrW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93fda439-332f-4671-d388-8a110d6edcb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq procgen faiss-cpu vector-quantize-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(1,3,64,64)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DTSlle0RaQY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "import faiss\n",
        "import torch\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z5-_pfGZTsip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf1a5f0-54be-422f-8a23-083c240fce32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuA25qQknUAX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        # self.enc = nn.Sequential(nn.Linear(in_dim, d_model), nn.ReLU(),)\n",
        "        self.enc = get_res(d_model)\n",
        "\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        # self.pred = gru(emb_dim, rnn_units, num_layers)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25.0 # 25.0 # λ\n",
        "        self.std_coeff=1.0 # 25.0 # µ\n",
        "        self.cov_coeff=25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy):\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.rand((batch,self.dim_z),device=device)*2 -1)#*self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "        optim = torch.optim.SGD([z], lr=3e3)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        num_steps = 10\n",
        "        for i in range(num_steps):\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            sy_ = self.pred(sxaz)\n",
        "            # print(\"y_, y\",y_.shape, y.shape)\n",
        "            loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "        if loss.item()>0.1: print(\"argm\",loss.item(), z[0].item())\n",
        "        return z#.detach()\n",
        "\n",
        "    def loss(self, x, y, a, z=None):\n",
        "        sx, sy = self.enc(x), self.enc(y)\n",
        "        z = self.argm(sx, a, sy)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "        # v_c_loss = self.v_creg(self.exp(sx))\n",
        "        vx, vy = self.exp(sx), self.exp(sy)\n",
        "        v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "        return repr_loss + v_c_loss\n",
        "\n",
        "    def forward(self, sx, a): # state, ctrl\n",
        "        batch=sx.size(dim=0)\n",
        "        z=torch.zeros((batch,self.dim_z),device=device)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # c,c_= 0,0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                a = self.quantizer.indices_to_codes(action)\n",
        "                z = self.jepa.argm(sx_, a, sy)\n",
        "                sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                # sy_ = self.jepa.pred(sxaz)\n",
        "                sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                jloss = repr_loss + std_loss + cov_loss\n",
        "                loss = loss + jloss\n",
        "                c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(c_)\n",
        "                    print(c)\n",
        "                    closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    loss = loss + 100*closs\n",
        "                    loss.backward()\n",
        "                    optim.step()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    # c,c_= 0,0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "89752174-e96e-44ac-abe2-77b4c33205bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "import pickle\n",
        "# def save(folder=''):\n",
        "#     agent.save(folder)\n",
        "#     with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# def load(folder=''):\n",
        "#     agent.load(folder)\n",
        "#     with open(folder+'buffer.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "\n",
        "def save(folder, name='agent.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    agent.mem.save(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# buffer = load(folder)\n",
        "save('/content/')\n",
        "# buffer = load('/content/')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name='agent.pth'\n",
        "print(folder+name)\n",
        "torch.load(folder+name, map_location='o')"
      ],
      "metadata": {
        "id": "ubGk_uT3FL-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = self.data_process(buffer)\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    # def data_process(self, data): # str 10780437\n",
        "    #     return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        # state = list(state)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(\"__getitem__\",state)\n",
        "        return state, action, reward\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "    def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "        lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "        with torch.no_grad():\n",
        "            imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "            data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "            # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "            data=data.flatten(start_dim=-3)\n",
        "            data=lin(data) # random projection\n",
        "            data = F.normalize(data, dim=-1)\n",
        "            idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "            sample = data[idx]\n",
        "            index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "            # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "            index.add(data)\n",
        "            D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "            priority = (2**-D).sum(-1) # L2\n",
        "            # priority = -D.sum(-1) # IP\n",
        "            topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "            index_list = idx[topk.values] # most clustered\n",
        "            for i in reversed(index_list): data.pop(i)\n",
        "        return data\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 16 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PraFUAPB3j7v"
      },
      "outputs": [],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    # out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    out = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cm6KjvBrnNO",
        "outputId": "efeb01d0-1ada-4beb-b4c0-32a92876cd27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        -575.4270, -744.4379, -591.8696, -672.5833, -633.5300, -633.1868,\n",
            "        -596.1379, -575.8553, -753.5652, -632.8671, -633.8352, -683.1360,\n",
            "        -594.5530, -683.0232, -633.1744, -672.5233, -608.9763, -755.1338,\n",
            "        -592.1741, -575.5234, -673.2512, -754.5297, -589.9937, -754.2086,\n",
            "        -608.8657, -681.0463, -633.1831, -754.6017, -596.1569, -635.3656,\n",
            "        -829.8069, -595.9781, -746.0023, -636.1009, -636.3204, -575.7976,\n",
            "        -596.1102, -632.5848, -636.2004, -623.5195, -636.1671, -679.6396,\n",
            "        -633.1494, -683.0366, -594.7137, -576.2501, -829.5593, -596.1072,\n",
            "        -673.1666, -635.4657, -596.0515, -637.9117, -673.3334, -754.4291,\n",
            "        -767.3058, -673.4365, -622.3802, -633.4039, -633.7919, -757.4591,\n",
            "        -594.1539, -594.2184, -754.8317, -594.7693, -742.5779, -682.2382,\n",
            "        -636.1519, -682.7720, -678.5778, -633.6044, -672.0035, -635.2549,\n",
            "        -682.8419, -633.8014, -633.3984, -745.8668, -594.2958, -583.1685,\n",
            "        -826.5571, -673.4073, -575.9292, -754.3039, -635.7396, -673.3892,\n",
            "        -673.3309, -633.7542, -594.0616, -743.5674, -672.7199, -681.0521,\n",
            "        -633.4360, -635.4821, -672.3629, -635.5482, -828.9731, -769.0911,\n",
            "        -574.9325, -596.7510, -635.3843, -766.0277, -681.0574, -633.6906,\n",
            "        -635.4128, -633.1309, -622.6180, -755.2511, -754.5982, -583.3734,\n",
            "        -596.1340, -576.9191, -751.3638, -635.3668, -623.5443, -583.1078,\n",
            "        -635.4372, -591.3562, -633.4462, -753.7170, -767.2742, -682.6896,\n",
            "        -637.9614, -633.7491, -633.4955, -766.0347, -636.1303, -608.6779,\n",
            "        -828.6849, -595.9262, -608.8948, -673.1057, -596.5386, -595.7521,\n",
            "        -673.1320, -633.0905, -594.1608, -633.6959, -673.2972, -753.4034,\n",
            "        -633.5643, -577.6213, -673.2709, -576.0723, -633.5740, -594.8856,\n",
            "        -745.5414, -766.3317, -595.9222, -764.6168, -633.6322, -754.0514,\n",
            "        -591.4260, -575.6801, -654.3465, -633.4467, -754.1384, -635.5761,\n",
            "        -594.3803, -633.1187, -828.2883, -672.5099, -655.7437, -673.0819,\n",
            "        -596.3640, -635.3497, -596.6003, -754.2400, -672.0403, -577.7504,\n",
            "        -672.7080, -754.3661, -754.5903, -575.9778, -595.5860, -637.9366,\n",
            "        -829.7982, -673.3459, -608.8581, -582.8531, -766.5474, -638.0385,\n",
            "        -596.5090, -633.1907, -594.6537, -583.1658, -594.5438, -754.2838,\n",
            "        -633.6268, -582.4789, -592.6592, -576.2496, -681.0674, -635.3926,\n",
            "        -633.3917, -673.1425, -594.1395, -745.7178, -673.2737, -632.6275,\n",
            "        -636.2288, -759.6961, -745.4937, -754.9077, -633.4097, -633.1287,\n",
            "        -591.4376, -672.6265, -829.8328, -672.7755, -657.5163, -637.9221,\n",
            "        -594.1083, -575.9201, -673.4956, -677.0683, -767.2315, -635.4946,\n",
            "        -576.7958, -681.2570, -754.7381, -633.1582, -592.8303, -637.9118,\n",
            "        -828.6858, -635.5030, -757.4806, -745.3467, -594.8488, -758.4372,\n",
            "        -595.0970, -633.1379, -671.9634, -657.3406, -769.2250, -753.3447,\n",
            "        -751.5289, -638.0327, -591.4843, -576.8116, -830.0120, -673.2402,\n",
            "        -638.0496, -635.4876, -635.5012, -575.3685, -596.3083, -754.2823,\n",
            "        -594.5018, -744.9626, -583.1573, -754.2507, -633.6780, -608.0984,\n",
            "        -672.0870, -608.9531, -829.8265, -765.9758, -656.9560, -583.1443,\n",
            "        -635.5098, -638.0410, -594.7728, -633.5820, -672.3024, -766.5361,\n",
            "        -575.6730, -755.1664, -754.8571, -575.8022, -635.9540, -754.3032,\n",
            "        -826.6733, -673.4694, -595.0670, -582.3465, -592.1746, -754.8505,\n",
            "        -671.8150, -755.2579, -595.9036, -656.7109, -673.1452, -681.2451,\n",
            "        -753.5089, -754.2930, -761.9036, -765.1541, -829.8420, -672.0795,\n",
            "        -754.1090, -767.2321, -635.3608, -583.0345, -633.3885, -633.4736,\n",
            "        -594.6953, -758.9197, -583.1439, -633.7841, -633.5897, -575.9115,\n",
            "        -596.2729, -596.6370, -632.4257, -594.4625, -638.0303, -745.4224,\n",
            "        -636.2589, -608.6725, -595.3891, -754.2500, -594.6387, -575.4727,\n",
            "        -577.6739, -754.3416, -754.1843, -594.8687, -635.3662, -595.1485,\n",
            "        -829.9218, -672.2625, -673.2889, -745.5945, -595.5165, -577.8480,\n",
            "        -633.5888, -754.3035, -671.8275, -656.1810, -638.0419, -754.0883,\n",
            "        -754.5068, -754.4813, -592.6114, -575.0312, -829.1207, -672.1134,\n",
            "        -681.2060, -581.3299, -595.7538, -592.0522, -633.9205, -754.8005,\n",
            "        -594.1070, -635.4966, -741.6244, -681.0850, -676.4237, -577.5117,\n",
            "        -635.3996, -596.1644, -678.8751, -672.0309, -766.3038, -576.6044,\n",
            "        -594.4958, -582.7748, -595.0320, -633.6426, -595.1147, -657.1132,\n",
            "        -583.2256, -681.0344, -632.9676, -635.4120, -596.2985, -758.4486,\n",
            "        -754.3507, -596.1842, -622.3810, -576.9260, -594.2265, -635.8282,\n",
            "        -672.2380, -633.5777, -591.3906, -671.7979, -638.0286, -633.1436,\n",
            "        -754.5122, -672.0363, -673.2864, -575.6881, -828.6801, -635.2211,\n",
            "        -767.2669, -595.0849, -594.8243, -755.1008, -632.5630, -633.2232,\n",
            "        -636.0975, -595.9135, -637.9194, -633.2164, -632.4083, -681.7821,\n",
            "        -636.1653, -765.2101, -826.9954, -672.0105, -752.7323, -672.1774,\n",
            "        -635.2113, -624.7792, -596.0405, -754.2726, -636.1844, -638.0499,\n",
            "        -762.1288, -753.5037, -754.5909, -657.6092, -592.7656, -635.3193,\n",
            "        -633.3967, -595.8903, -633.4470, -595.7619, -596.0438, -656.1586,\n",
            "        -636.0646, -633.9131, -596.3528, -638.0399, -577.7645, -826.6780,\n",
            "        -754.6113, -633.1694, -596.3266, -577.8787, -828.8913, -596.1932,\n",
            "        -576.0084, -575.7969, -594.4391, -633.4331, -829.1311, -633.6769,\n",
            "        -594.7058, -633.5186, -608.9072, -633.6375, -633.5127, -590.3979,\n",
            "        -594.5120, -583.0707, -830.0190, -635.2838, -635.4693, -638.0345,\n",
            "        -590.0804, -764.8891, -635.3453, -632.6772, -673.4697, -608.2595,\n",
            "        -590.1906, -680.9261, -633.7965, -638.0461, -592.7762, -592.7241,\n",
            "        -680.2625, -591.3953], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.1369544118642807 0.4950000047683716 0.0 659.236083984375\n",
            "tensor([-759.3649, -593.7415, -768.7358, -660.9265, -608.8888, -653.3817,\n",
            "        -608.9641, -762.4931, -609.0264, -844.0953, -653.3113, -612.5988,\n",
            "        -609.9888, -653.5734, -842.6058, -610.3603, -762.1797, -769.7188,\n",
            "        -609.9805, -653.3486, -610.5388, -652.7432, -766.8124, -595.5436,\n",
            "        -627.2455, -772.8317, -653.3601, -766.8339, -685.2004, -593.6135,\n",
            "        -843.8577, -652.5176, -761.2911, -682.8450, -649.8466, -660.9306,\n",
            "        -685.1607, -772.3463, -610.4601, -594.1514, -594.7921, -696.1099,\n",
            "        -653.4675, -772.8088, -611.8812, -772.2490, -841.0176, -652.1122,\n",
            "        -598.3896, -651.6666, -608.7971, -610.7817, -653.3848, -653.2979,\n",
            "        -611.6902, -653.4783, -771.6567, -696.4543, -771.4404, -683.6184,\n",
            "        -652.2576, -762.2767, -841.8174, -684.2484, -626.9667, -653.4074,\n",
            "        -612.6060, -660.9634, -653.3907, -771.9269, -684.7300, -653.3630,\n",
            "        -660.9630, -653.3900, -769.6851, -627.2784, -613.1281, -593.6946,\n",
            "        -843.9678, -685.3778, -771.9332, -611.5674, -651.6201, -692.7748,\n",
            "        -652.5045, -842.5812, -608.7487, -761.0159, -662.2512, -772.6993,\n",
            "        -653.4028, -594.4715, -766.0223, -653.1178, -696.4490, -613.0674,\n",
            "        -595.3241, -652.0679, -613.1661, -766.8795, -612.5882, -653.3839,\n",
            "        -611.7658, -653.3804, -684.2719, -771.6876, -653.7307, -761.6070,\n",
            "        -651.3867, -693.6839, -769.6952, -611.7815, -683.7023, -767.7635,\n",
            "        -651.8660, -592.2815, -695.5875, -768.4907, -768.5388, -651.6246,\n",
            "        -593.9854, -653.3143, -653.3894, -595.2930, -683.7219, -758.7280,\n",
            "        -842.0241, -685.1979, -611.7422, -650.8134, -612.8511, -660.7732,\n",
            "        -611.7156, -771.7809, -683.4572, -668.5800, -772.8410, -652.7811,\n",
            "        -771.8713, -608.6937, -612.6063, -613.1320, -653.5866, -649.8411,\n",
            "        -595.2725, -761.8820, -649.8690, -760.8705, -613.1188, -768.6161,\n",
            "        -612.0524, -627.0594, -766.1183, -842.1977, -840.9867, -660.4514,\n",
            "        -650.3608, -626.7991, -842.5779, -613.1057, -771.6204, -693.8176,\n",
            "        -612.5315, -599.9397, -685.5554, -653.3885, -610.6998, -627.6302,\n",
            "        -668.3492, -683.5208, -773.2498, -594.1222, -612.5568, -660.4859,\n",
            "        -772.3890, -651.6909, -771.9644, -595.2794, -610.3613, -765.3079,\n",
            "        -685.6895, -652.7601, -612.9088, -627.0144, -647.0643, -613.1174,\n",
            "        -770.1888, -685.2525, -650.9935, -627.0014, -651.4374, -610.2527,\n",
            "        -652.4544, -690.0582, -613.0883, -627.0346, -653.4097, -627.2822,\n",
            "        -650.9676, -772.3387, -758.7322, -684.3006, -653.3423, -693.7603,\n",
            "        -611.7465, -647.1384, -842.5751, -611.7894, -594.2372, -653.3980,\n",
            "        -613.1735, -653.3979, -652.0684, -653.5157, -611.6828, -660.4941,\n",
            "        -651.7711, -683.8273, -653.3720, -766.3058, -607.6536, -693.0596,\n",
            "        -696.2551, -649.9713, -653.3417, -627.5291, -650.9184, -762.3719,\n",
            "        -768.4381, -653.3353, -612.6328, -651.0630, -658.8288, -613.3066,\n",
            "        -653.4976, -772.1963, -683.1093, -627.2108, -772.7198, -683.2889,\n",
            "        -627.2305, -627.1187, -652.4612, -761.9883, -607.7325, -653.3412,\n",
            "        -613.1389, -594.4188, -597.9948, -612.8251, -627.2129, -627.3306,\n",
            "        -683.3447, -647.4240, -840.9722, -610.5507, -693.0400, -647.2727,\n",
            "        -652.5246, -627.3085, -685.2916, -653.3300, -610.5150, -683.7518,\n",
            "        -652.6564, -612.5915, -660.8448, -660.2592, -767.7357, -653.3183,\n",
            "        -772.0236, -611.8247, -594.2568, -660.2471, -650.8522, -683.8324,\n",
            "        -613.0278, -651.7289, -685.2119, -693.6810, -760.4325, -683.1226,\n",
            "        -653.3323, -663.4193, -607.7228, -600.0368, -845.0264, -611.8191],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.1394762396812439 0.4950000047683716 0.0 674.4991455078125\n",
            "#### simulate ####\n",
            "search -4.1492564086889854e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -5.029406773369018e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -5.572344171198176e+23\n",
            "search -5.029406773369018e+23\n",
            "search -5.572344171198176e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -5.029406773369018e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -5.572344171198176e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -4.1492564086889854e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -7.530584949652607e+23\n",
            "search -5.029406773369018e+23\n",
            "search -5.572344171198176e+23\n",
            "search -5.572344171198176e+23\n",
            "search -7.530584949652607e+23\n",
            "ded\n",
            "time\n",
            "7 #### train ####\n",
            "tensor([-694.5107, -780.2303, -667.0437, -694.5697, -620.9363, -762.2972,\n",
            "        -661.7278, -623.5198, -693.4814, -611.4391, -694.5767, -622.7162,\n",
            "        -620.4647, -666.7872, -664.0436, -620.9525, -663.1362, -782.3033,\n",
            "        -606.6892, -611.0682, -622.7592, -663.9060, -623.2178, -663.9102,\n",
            "        -644.3083, -694.2315, -609.0084, -611.4804, -762.5667, -666.2169,\n",
            "        -663.6398, -623.8181, -624.0803, -663.2689, -666.8295, -611.6756,\n",
            "        -620.5965, -674.4153, -663.3145, -624.0240, -611.0256, -695.5473,\n",
            "        -781.8742, -644.4991, -623.6965, -666.5425, -664.2112, -620.8570,\n",
            "        -623.6070, -780.2740, -677.0387, -668.4365, -664.5410, -699.0295,\n",
            "        -624.2772, -661.7798, -764.7148, -608.1328, -699.8162, -611.0687,\n",
            "        -694.3245, -666.0582, -609.0173, -623.5903, -661.2248, -851.5365,\n",
            "        -666.6373, -780.1738, -622.6292, -643.9899, -619.2840, -623.8452,\n",
            "        -662.2453, -666.6414, -780.3978, -611.1169, -621.7493, -708.7783,\n",
            "        -610.8279, -763.3528, -663.8010, -848.8487, -666.2780, -609.1151,\n",
            "        -763.4417, -609.0929, -765.7498, -663.2643, -666.1470, -679.0349,\n",
            "        -698.6378, -618.6373, -623.9395, -708.0757, -611.4470, -623.8480,\n",
            "        -663.8202, -851.7303, -606.8743, -692.6473, -762.6888, -663.8940,\n",
            "        -623.6284, -663.5715, -766.6832, -694.2742, -763.7178, -611.6266,\n",
            "        -623.9767, -666.4641, -699.3363, -624.1099, -622.6976, -850.0062,\n",
            "        -780.4711, -666.1694, -663.9101, -782.7744, -624.4606, -619.1724,\n",
            "        -668.0381, -782.8699, -772.7820, -693.9495, -624.1464, -666.1741,\n",
            "        -693.4612, -623.2590, -623.2421, -780.4154, -644.3232, -767.9560,\n",
            "        -623.7858, -766.6971, -694.5026, -624.4068, -666.0739, -694.0157,\n",
            "        -663.0478, -663.1981, -624.0825, -705.4388, -611.4290, -621.2877,\n",
            "        -664.0010, -849.5071, -664.2238, -662.1716, -662.9781, -667.0115,\n",
            "        -694.5076, -621.0544, -666.0080, -782.5193, -611.0291, -622.2557,\n",
            "        -622.4392, -780.5532, -609.0380, -664.0537, -623.2031, -848.9261,\n",
            "        -677.7503, -780.4011, -663.1389, -671.7058, -622.9924, -623.6177,\n",
            "        -780.3031, -699.1671, -765.0600, -693.5605, -623.6241, -666.0493,\n",
            "        -611.7355, -622.6952, -619.0959, -663.3004, -662.6766, -606.8347,\n",
            "        -624.0866, -611.1167, -623.2932, -623.5258, -678.4662, -663.7985,\n",
            "        -606.8643, -666.9429, -692.7303, -776.0355, -644.4044, -662.5798,\n",
            "        -621.0148, -666.4955, -692.2512, -693.4874, -765.7556, -699.6132,\n",
            "        -767.2584, -766.5126, -624.0674, -644.4077, -662.9023, -644.4039,\n",
            "        -694.4296, -666.3554, -772.9552, -624.0889, -624.1160, -849.8481,\n",
            "        -622.6536, -779.9264, -623.9493, -663.8563, -623.7640, -624.2681,\n",
            "        -768.5048, -668.6923, -611.5432, -612.1865, -623.6647, -780.2316,\n",
            "        -776.0042, -624.0128, -620.9416, -776.9096, -767.1972, -699.5265,\n",
            "        -621.3174, -644.4154, -694.4545, -624.2538, -783.0191, -611.6656,\n",
            "        -610.9557, -764.8536, -621.3723, -780.4324, -765.0783, -661.6600,\n",
            "        -621.1548, -666.1042, -667.0606, -677.8608, -624.4716, -770.5762,\n",
            "        -624.0541, -624.1160, -663.4329, -694.4850, -666.5043, -674.0570,\n",
            "        -694.4806, -781.9316, -666.7408, -623.9678, -623.1653, -852.8522,\n",
            "        -663.8525, -782.1585, -620.9211, -666.2253, -623.4449, -766.6249,\n",
            "        -664.0160, -609.0430, -666.7261, -623.3019, -694.3945, -707.5966,\n",
            "        -612.0875, -663.8097, -623.2429, -781.9744, -610.8885, -619.3474,\n",
            "        -623.6720, -758.2418, -624.0829, -623.1844, -664.2550, -611.5303,\n",
            "        -679.1378, -622.7971, -663.4026, -780.2300, -661.9116, -663.8249,\n",
            "        -621.2607, -662.8034, -666.6296, -623.8365, -765.0601, -660.7535,\n",
            "        -621.5713, -623.8467, -678.6519, -611.0733, -644.4053, -666.6393,\n",
            "        -619.6718, -783.0156, -679.1454, -624.4193, -623.5238, -666.8567,\n",
            "        -699.1047, -619.6765, -621.2383, -678.5513, -758.1170, -663.9717,\n",
            "        -758.1575, -609.0850, -611.7724, -768.0135, -764.4747, -783.0937,\n",
            "        -611.4647, -620.9000, -620.6544, -666.0969, -610.6262, -665.8705,\n",
            "        -621.1548, -694.0559, -621.7944, -623.5170, -668.9534, -620.9168,\n",
            "        -782.6436, -644.3929, -663.0841, -705.4510, -666.7265, -623.1490,\n",
            "        -623.8875, -780.0121, -694.6648, -780.2971, -661.4723, -663.1831,\n",
            "        -624.1275, -623.2305, -694.0266, -781.6571, -611.4401, -769.3193,\n",
            "        -621.6314, -666.5789, -677.6310, -663.8126, -624.0875, -663.2746,\n",
            "        -678.9666, -698.1786, -663.6785, -661.9016, -664.3365, -623.5843,\n",
            "        -609.0713, -611.0402, -621.1577, -662.8948, -621.0757, -780.4124,\n",
            "        -644.3236, -623.6788, -623.6686, -666.0989, -677.0405, -771.7950,\n",
            "        -624.0445, -659.9448, -693.4556, -624.3992, -667.5878, -694.1488,\n",
            "        -781.9798, -609.1173, -663.2709, -776.5003, -606.8242, -663.4272,\n",
            "        -624.2843, -666.4860, -606.9036, -692.5959, -662.6681, -611.4139,\n",
            "        -694.1666, -623.3030, -772.8096, -667.7208, -608.8712, -663.8609,\n",
            "        -623.8829, -708.6586, -620.8187, -620.4667, -694.6548, -666.4283,\n",
            "        -666.7261, -697.8459, -663.8172, -665.9775, -624.0695, -622.5983,\n",
            "        -611.0224, -679.1544, -663.5739, -623.6130, -694.0596, -707.9440,\n",
            "        -777.6990, -623.5354, -623.9245, -775.4601, -767.0616, -611.6651,\n",
            "        -621.1031, -678.5256, -623.7339, -662.9871, -669.1448, -623.8068,\n",
            "        -694.5409, -670.2502, -621.5770, -666.1428, -700.1387, -623.6780,\n",
            "        -624.1813, -782.6083, -623.8119, -694.4181, -623.9994, -694.5721,\n",
            "        -623.2408, -664.4702, -699.4675, -611.3586, -699.0424, -694.3290,\n",
            "        -623.4097, -666.7932, -611.6166, -623.6158, -620.7590, -782.8768,\n",
            "        -773.1277, -609.0246, -624.3459, -764.8737, -623.8219, -663.1990,\n",
            "        -609.0446, -699.4955, -670.6228, -666.1770, -765.0997, -780.5024,\n",
            "        -663.2338, -624.4388, -762.8456, -776.5143, -668.0278, -782.2639,\n",
            "        -624.3204, -609.0587, -623.6182, -663.8436, -607.0309, -677.0424,\n",
            "        -666.5324, -694.5896, -624.0917, -848.8152, -663.7346, -663.0121,\n",
            "        -622.5521, -665.9561, -622.5988, -609.0244, -623.8782, -667.2178,\n",
            "        -624.0295, -661.4803, -677.7575, -764.7481, -623.9763, -608.7668,\n",
            "        -623.5345, -666.0909, -609.0470, -621.1114, -623.8549, -666.4628,\n",
            "        -609.0360, -664.2919, -623.9312, -766.6597, -623.9510, -663.6938,\n",
            "        -758.1055, -677.6422, -677.0397, -768.0129, -623.8553, -708.6715,\n",
            "        -608.8761, -664.4633], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.13199251890182495 0.4950000047683716 0.0 671.2352294921875\n",
            "tensor([-603.1074, -649.1340, -736.9726, -665.9720, -641.1227, -739.1182,\n",
            "        -643.4618, -641.1263, -668.5150, -648.3851, -606.8407, -660.9467,\n",
            "        -671.2310, -812.9027, -598.6725, -608.5649, -605.4411, -744.7324,\n",
            "        -748.8830, -720.9164, -609.0537, -598.7350, -642.6745, -606.5880,\n",
            "        -671.1725, -629.3133, -720.5571, -646.1065, -605.6075, -747.8136,\n",
            "        -598.2977, -605.2393, -607.0222, -747.8053, -649.0891, -739.2214,\n",
            "        -643.3486, -655.4645, -605.4457, -643.5491, -662.2907, -650.8519,\n",
            "        -748.8969, -671.2198, -641.3029, -686.0030, -630.0589, -642.0120,\n",
            "        -607.9171, -649.1669, -639.8091, -629.8945, -608.0717, -607.6039,\n",
            "        -671.1840, -607.2888, -720.1614, -597.8090, -671.3826, -655.3574,\n",
            "        -606.9370, -685.0483, -598.7405, -605.3254, -671.0926, -743.0911,\n",
            "        -636.3147, -598.2738, -605.2952, -670.9714, -606.3716, -607.9681,\n",
            "        -644.1073, -645.6673, -630.1725, -742.5837, -671.0562, -649.1357,\n",
            "        -718.3579, -608.9879, -608.9739, -648.3871, -658.4453, -645.8945,\n",
            "        -606.6427, -721.0717, -643.5367, -607.1500, -596.7214, -648.6625,\n",
            "        -595.0113, -592.7676, -644.1221, -747.4956, -671.1808, -604.3436,\n",
            "        -721.9285, -649.2212, -596.7377, -629.9047, -641.3895, -748.9965,\n",
            "        -608.0630, -608.1174, -654.9362, -596.5372, -720.8943, -596.9419,\n",
            "        -669.6431, -742.1726, -713.6544, -643.0750, -605.0775, -643.3693,\n",
            "        -655.3055, -597.8829, -607.9188, -667.1091, -605.4579, -606.3021,\n",
            "        -629.4183, -661.1210, -598.8647, -655.5002, -643.5176, -649.1224,\n",
            "        -595.4457, -643.3683, -607.1135, -802.0662, -594.9175, -637.0467,\n",
            "        -608.8083, -670.9368, -608.1694, -641.5462, -742.0120, -743.9733,\n",
            "        -603.2959, -595.4135, -605.1797, -648.3863, -629.9955, -608.1433,\n",
            "        -607.0168, -649.0255, -649.1158, -649.2437, -605.5699, -637.8815,\n",
            "        -607.4092, -606.8975, -649.1098, -750.1902, -661.0128, -652.2122,\n",
            "        -603.3056, -801.9387, -630.1089, -715.3881, -607.1535, -649.2272,\n",
            "        -596.7231, -594.4873, -604.1858, -651.4889, -605.3396, -643.3670,\n",
            "        -741.8969, -643.6213, -595.4084, -598.7130, -605.5922, -671.3172,\n",
            "        -641.5439, -607.3216, -607.9413, -649.1022, -630.0835, -747.0914,\n",
            "        -605.3663, -642.8275, -603.3509, -606.3056, -661.9711, -749.8314,\n",
            "        -669.5956, -629.8858, -670.4196, -607.8034, -670.9130, -643.7276,\n",
            "        -607.8132, -812.2315, -671.1071, -649.1700, -642.4543, -652.2173,\n",
            "        -643.1912, -643.4449, -750.1830, -733.6350, -665.6260, -655.4197,\n",
            "        -606.9510, -670.5939, -655.3558, -642.0005, -671.2120, -749.8696,\n",
            "        -740.8818, -595.3948, -604.1412, -661.8870, -608.8855, -643.4210,\n",
            "        -598.4604, -671.5857, -720.1520, -662.4064, -607.9272, -671.1938,\n",
            "        -592.7819, -602.9162, -608.1791, -649.2440, -742.4880, -745.6362,\n",
            "        -606.7047, -595.4001, -606.6339, -607.3248, -594.9084, -594.8468,\n",
            "        -750.1017, -642.9768, -607.8499, -606.9719, -649.1017, -641.3889,\n",
            "        -607.1012, -643.3484, -722.6621, -649.2333, -641.4193, -598.7047,\n",
            "        -606.8123, -608.0667, -736.1888, -667.2344, -629.8428, -653.4283,\n",
            "        -605.5398, -607.2921, -721.7793, -723.7722, -606.2393, -743.7091,\n",
            "        -606.0945, -629.4052, -643.3350, -597.8316, -607.9135, -644.1019,\n",
            "        -655.4281, -745.6364, -658.9803, -594.8135, -607.9127, -604.1247,\n",
            "        -741.4459, -608.2186, -603.3801, -806.9008, -658.1253, -661.0010,\n",
            "        -607.0668, -742.7376, -720.0497, -641.3785, -735.1274, -649.1765,\n",
            "        -635.7166, -603.5641, -641.1512, -670.9522, -630.0078, -643.6428],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.1298685222864151 0.4950000047683716 0.0 649.91748046875\n",
            "tensor([-719.4928, -615.9933, -761.6716, -710.9521, -681.3765, -675.7993,\n",
            "        -647.6497, -678.3626, -591.7149, -620.4959, -647.1064, -622.6452,\n",
            "        -629.2701, -581.9074, -589.2454, -589.1062, -631.0811, -647.3661,\n",
            "        -766.0185, -631.2067, -631.8431, -631.5959, -591.6833, -623.6516,\n",
            "        -589.9094, -590.2423, -589.6229, -668.7051, -637.1877, -720.9213,\n",
            "        -591.9322, -615.3762, -719.9873, -581.5964, -631.0616, -638.2950,\n",
            "        -581.7017, -615.6462, -647.6831, -638.3115, -591.9199, -587.4431,\n",
            "        -585.9345, -622.4266, -623.6759, -615.4188, -591.8461, -615.3596,\n",
            "        -630.9468, -585.1458, -721.6713, -576.4105, -615.4146, -721.0434,\n",
            "        -590.6696, -706.3876, -586.7612, -591.1765, -589.7891, -622.5330,\n",
            "        -714.4518, -719.8046, -590.0766, -639.6830, -714.9399, -720.9910,\n",
            "        -709.1184, -721.1870, -713.3859, -615.4593, -645.7957, -630.9323,\n",
            "        -647.6872, -591.8900, -588.3430, -589.6168, -719.5051, -584.9619,\n",
            "        -587.3419, -639.1428, -631.3293, -645.5701, -721.6339, -636.4333,\n",
            "        -706.5085, -620.5071, -586.2753, -647.6111, -586.3641, -591.7471,\n",
            "        -591.6820, -623.6239, -622.6678, -721.1887, -590.6095, -709.7036,\n",
            "        -709.3182, -647.3318, -769.2247, -645.7675, -639.0736, -630.6924,\n",
            "        -591.9753, -644.1181, -591.8884, -589.5051, -589.6657, -589.6478,\n",
            "        -620.4401, -630.8815, -591.7153, -614.8755, -713.9658, -705.7595,\n",
            "        -766.8842, -623.9707, -709.0054, -637.5699, -623.6364, -639.8765,\n",
            "        -591.6913, -623.5949, -586.2758, -622.3832, -625.6315, -620.4026,\n",
            "        -619.4896, -589.7500, -629.9996, -640.9789, -721.1525, -712.2300,\n",
            "        -631.1385, -647.5810, -591.8837, -581.9680, -647.3434, -586.3039,\n",
            "        -646.9287, -623.9235, -720.0784, -639.7659, -592.0286, -643.6627,\n",
            "        -631.3687, -623.6052, -631.0294, -630.9427, -647.2117, -638.6073,\n",
            "        -647.2344, -629.6498, -587.3100, -589.6580, -591.8083, -590.7155,\n",
            "        -592.0246, -581.8223, -623.6218, -589.6800, -719.5115, -647.6639,\n",
            "        -713.7445, -638.2164, -672.7595, -642.8148, -591.8568, -626.8242,\n",
            "        -590.4777, -679.0593, -623.9365, -591.8719, -581.9434, -676.7447,\n",
            "        -591.2303, -615.4200, -720.7968, -581.2205, -773.4047, -631.0340,\n",
            "        -592.0065, -645.7955, -623.7217, -674.8043, -591.0953, -591.2880,\n",
            "        -647.4626, -589.5063, -647.5868, -614.1024, -592.0098, -584.9185,\n",
            "        -721.2149, -667.4183, -718.7396, -584.8009, -636.5509, -622.2389,\n",
            "        -591.2569, -614.5925, -645.7405, -674.1834, -588.3135, -591.9785,\n",
            "        -682.2513, -634.3557, -591.7452, -675.5196, -630.9969, -579.3801,\n",
            "        -621.2687, -579.3500, -623.6290, -647.6056, -589.9290, -622.2207,\n",
            "        -589.9515, -591.7670, -589.5399, -591.7259, -647.3335, -640.3464,\n",
            "        -591.8702, -704.8734, -712.2164, -703.5685, -630.9727, -581.8348,\n",
            "        -640.0529, -591.9544, -588.5052, -674.4638, -589.4565, -591.7311,\n",
            "        -591.8400, -588.4703, -581.2314, -581.9609, -622.6860, -721.7212,\n",
            "        -647.3015, -679.0533, -755.9630, -673.8673, -636.4255, -714.5286,\n",
            "        -590.6296, -720.8096, -590.0692, -591.9783, -591.7144, -589.9287,\n",
            "        -647.6397, -590.9324, -590.2011, -710.8599, -709.7820, -679.0476,\n",
            "        -719.9738, -624.0092, -583.2881, -581.2484, -647.1994, -619.2900,\n",
            "        -680.4195, -623.6848, -590.9203, -591.8506, -642.2405, -647.8204,\n",
            "        -623.8506, -677.0907, -629.6517, -647.2530, -721.0084, -622.1896,\n",
            "        -591.9568, -676.4224, -590.6112, -622.3434, -591.8968, -591.6667,\n",
            "        -591.6343, -586.7252, -705.8127, -588.3984, -589.6192, -712.1121,\n",
            "        -711.3981, -623.6431, -631.1754, -710.9462, -581.8293, -581.9055,\n",
            "        -590.1772, -707.6746, -591.0417, -623.6854, -647.2662, -591.7572,\n",
            "        -623.6487, -617.9108, -591.9215, -623.0319, -647.8337, -636.9521,\n",
            "        -721.9279, -668.5888, -712.1886, -585.3326, -590.0778, -711.4651,\n",
            "        -675.3553, -621.7213, -622.2943, -623.6984, -581.8687, -619.2939,\n",
            "        -591.6337, -581.6758, -647.5695, -644.0471, -721.7353, -620.6275,\n",
            "        -621.2479, -647.5197, -591.8913, -591.6365, -589.5065, -590.6219,\n",
            "        -670.6238, -589.6404, -581.9360, -585.4629, -623.6417, -583.2947,\n",
            "        -630.9589, -639.4357, -711.3849, -682.8328, -615.4264, -581.9335,\n",
            "        -591.8457, -615.4177, -647.5704, -621.0566, -590.1863, -589.6197,\n",
            "        -674.5076, -623.9831, -623.6392, -622.4630, -719.1538, -585.0995,\n",
            "        -773.0381, -583.5247, -614.6672, -581.3348, -647.5042, -673.8220,\n",
            "        -623.9231, -623.6874, -622.9459, -589.4420, -705.0043, -647.0899,\n",
            "        -586.6014, -623.9824, -719.4900, -584.6510, -753.7447, -620.4305,\n",
            "        -585.5880, -640.3619, -623.9975, -635.0921, -647.5325, -622.2136,\n",
            "        -592.0073, -622.4282, -702.6547, -713.2910, -590.8823, -631.0413,\n",
            "        -590.5656, -591.9401, -772.5851, -621.2430, -636.9547, -704.4232,\n",
            "        -592.0245, -671.3276, -591.8884, -623.7270, -621.0732, -623.7550,\n",
            "        -581.8552, -623.5852, -590.0730, -674.3734, -631.2253, -623.6255,\n",
            "        -631.1240, -613.5760, -591.8401, -581.9295, -591.0745, -633.0682,\n",
            "        -590.9719, -622.7266, -588.3461, -590.9824, -576.4174, -643.2753,\n",
            "        -589.9309, -581.6577, -720.8582, -614.3495, -631.2413, -647.3640,\n",
            "        -613.4162, -643.9988, -647.1992, -632.1226, -591.9294, -588.4804,\n",
            "        -647.3846, -621.2456, -581.0542, -623.8257, -588.1387, -647.3856,\n",
            "        -587.6279, -615.4138, -770.1775, -589.4582, -592.0110, -644.6478,\n",
            "        -589.1773, -638.6104, -591.4437, -623.5760, -622.4461, -677.1004,\n",
            "        -591.8636, -674.8236, -589.6098, -629.6514, -647.2136, -644.4782,\n",
            "        -631.3157, -640.6389, -581.8436, -672.6757, -682.7951, -680.5336,\n",
            "        -591.1175, -589.9084, -590.4589, -623.6633, -645.5126, -709.1118,\n",
            "        -591.9550, -581.6997, -721.9474, -590.0106, -709.3361, -640.6036,\n",
            "        -623.8918, -580.7805, -589.7050, -615.4246, -622.5699, -623.9774,\n",
            "        -589.2571, -590.9160, -586.6917, -710.8646, -591.7328, -611.6973,\n",
            "        -713.9292, -590.1398, -772.1036, -631.0815, -612.6227, -615.0476,\n",
            "        -589.9125, -641.5891, -589.9976, -589.7531, -589.5070, -589.6071,\n",
            "        -585.0714, -639.1379, -623.7295, -589.7162, -711.9288, -630.8527,\n",
            "        -760.0330, -615.4552, -631.6960, -589.5941, -590.4865, -641.9106,\n",
            "        -590.6862, -590.1333, -591.9650, -591.9190, -641.3156, -647.8226,\n",
            "        -620.4885, -611.8010], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.11604032665491104 0.4950000047683716 0.0 631.683837890625\n",
            "tensor([-691.5084, -574.0151, -612.0984, -637.8040, -610.1981, -575.9104,\n",
            "        -573.3624, -612.7595, -622.1406, -573.7492, -574.3176, -573.7192,\n",
            "        -567.2885, -570.7832, -604.7914, -573.3017, -693.2449, -612.3152,\n",
            "        -688.4017, -572.7377, -598.7065, -600.9490, -622.8389, -622.5827,\n",
            "        -574.0526, -638.7747, -619.4767, -573.5909, -689.6282, -613.3658,\n",
            "        -604.6839, -688.9030, -676.4327, -627.3820, -611.9593, -567.8799,\n",
            "        -602.2498, -572.7849, -573.7148, -601.7079, -575.7721, -571.2971,\n",
            "        -573.9547, -629.3624, -621.3821, -612.4481, -573.8517, -614.4175,\n",
            "        -687.6522, -599.4780, -680.8499, -609.8361, -574.2175, -693.6711,\n",
            "        -604.7391, -621.0651, -602.3801, -602.0702, -604.7724, -601.7515,\n",
            "        -604.7859, -626.2299, -574.4173, -610.9957, -688.7515, -568.3406,\n",
            "        -612.0873, -612.1458, -680.3558, -626.8674, -573.8606, -689.2888,\n",
            "        -573.2280, -601.1191, -573.7194, -570.7457, -612.2663, -591.2794,\n",
            "        -571.5761, -598.2723, -692.7571, -593.6774, -612.1522, -688.2211,\n",
            "        -567.4023, -601.0930, -573.6436, -571.8202, -574.2360, -604.7407,\n",
            "        -645.5196, -604.7860, -573.4805, -574.0575, -573.7526, -691.5732,\n",
            "        -689.6925, -600.5462, -723.4208, -612.2653, -572.4584, -615.3384,\n",
            "        -575.7742, -682.7142, -645.3478, -601.1318, -603.5370, -573.6631,\n",
            "        -604.7530, -611.6407, -574.3112, -574.7545, -679.4013, -622.2610,\n",
            "        -693.2590, -603.6439, -603.5765, -565.6646, -602.0744, -621.8569,\n",
            "        -604.7728, -573.8408, -622.4434, -574.0355, -645.5022, -598.9863,\n",
            "        -637.0801, -567.6005, -676.3001, -600.4725, -706.1418, -601.1982,\n",
            "        -681.4333, -689.6424, -573.6083, -624.9059, -575.0469, -574.3716,\n",
            "        -570.8053, -604.8336, -603.5209, -602.2498, -573.9019, -570.6063,\n",
            "        -691.8127, -604.7314, -707.8799, -600.5224, -627.2743, -641.3839,\n",
            "        -573.5679, -622.4551, -573.8715, -573.7904, -602.2487, -603.4720,\n",
            "        -686.5604, -621.8371, -573.8048, -603.2729, -681.5605, -571.8437,\n",
            "        -612.1975, -600.6603, -572.5378, -597.2042, -570.7943, -603.2483,\n",
            "        -634.2794, -602.2498, -574.1377, -573.6722, -610.8766, -612.4876,\n",
            "        -604.7783, -573.6368, -689.6391, -601.1960, -730.8185, -600.5948,\n",
            "        -572.8073, -624.0825, -575.7280, -625.7574, -602.5573, -574.3035,\n",
            "        -602.9556, -641.4349, -572.4666, -626.3114, -573.7875, -563.7809,\n",
            "        -689.6277, -627.3387, -689.9247, -689.7408, -621.7278, -686.5623,\n",
            "        -636.9825, -566.7164, -573.8826, -571.4507, -573.4606, -573.5200,\n",
            "        -599.9054, -612.1505, -573.3191, -686.5936, -689.6341, -600.3846,\n",
            "        -721.0212, -623.2199, -624.6119, -612.6378, -622.3182, -627.2693,\n",
            "        -574.2930, -602.0753, -573.9755, -604.7382, -612.1675, -604.7893,\n",
            "        -575.1981, -604.7303, -691.5267, -566.7435, -718.6457, -604.7441,\n",
            "        -625.7126, -591.2717, -574.2211, -598.9105, -573.5682, -600.4444,\n",
            "        -613.7578, -573.8281, -600.4345, -639.3554, -575.1551, -621.8588,\n",
            "        -689.9156, -608.7609, -689.8434, -567.0423, -676.0261, -567.5792,\n",
            "        -574.1982, -610.7095, -641.5555, -602.3419, -622.0701, -573.8712,\n",
            "        -600.7013, -617.0555, -570.8710, -600.0198, -692.7394, -601.1108,\n",
            "        -611.7759, -622.4196, -611.5383, -612.2891, -603.5416, -572.8113,\n",
            "        -575.3751, -574.1293, -643.8087, -604.8338, -625.9705, -600.5090,\n",
            "        -604.0847, -597.5163, -691.5533, -612.3861, -689.7647, -607.6548,\n",
            "        -598.6771, -678.2703, -573.9271, -626.3126, -573.8804, -570.6742,\n",
            "        -573.8659, -575.3184, -623.1675, -634.3242, -575.1982, -604.8311],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.11032205075025558 0.4950000047683716 0.0 612.2761840820312\n",
            "tensor([-586.4095, -558.1949, -597.6333, -580.8665, -586.2701, -558.2045,\n",
            "        -558.3185, -559.1555, -603.3167, -557.2955, -558.4677, -558.1110,\n",
            "        -586.4708, -593.1630, -582.1487, -656.7419, -558.2374, -596.6628,\n",
            "        -596.2169, -558.2028, -556.2225, -607.5826, -558.1797, -598.1428,\n",
            "        -558.3165, -683.1179, -581.8733, -557.6807, -657.4646, -596.7251,\n",
            "        -558.1921, -685.2936, -586.5078, -586.4094, -577.5955, -592.0596,\n",
            "        -558.1623, -557.2150, -613.4977, -553.8398, -558.1247, -586.3841,\n",
            "        -558.1543, -558.2337, -583.9675, -593.0363, -558.2195, -592.9562,\n",
            "        -604.7111, -556.9707, -558.2227, -588.7322, -587.4300, -558.0883,\n",
            "        -557.2682, -580.3075, -558.2150, -558.1832, -558.3133, -599.5342,\n",
            "        -662.6265, -593.1917, -558.1349, -592.9583, -558.2405, -558.1995,\n",
            "        -559.6390, -596.3660, -556.0737, -558.1776, -558.1255, -609.5895,\n",
            "        -558.0634, -595.1690, -558.2982, -557.7713, -608.9295, -558.2848,\n",
            "        -586.5443, -670.4662, -584.8757, -586.4274, -642.7103, -557.5972,\n",
            "        -582.0839, -597.9291, -558.2711, -585.0206, -558.0424, -592.5558,\n",
            "        -596.7452, -556.0222, -592.6970, -623.0925, -558.1708, -593.6423,\n",
            "        -558.1950, -558.0613, -593.9457, -558.1973, -557.2058, -579.5865,\n",
            "        -586.2925, -554.6102, -596.6965, -597.3704, -558.3198, -558.3098,\n",
            "        -555.9767, -611.5036, -558.1423, -593.0005, -558.0651, -586.3940,\n",
            "        -553.3547, -585.8507, -558.2014, -558.1240, -558.2540, -585.1886,\n",
            "        -558.1894, -592.9726, -558.0916, -558.2139, -608.1774, -596.8159,\n",
            "        -558.2089, -687.2551, -582.9679, -596.2878, -609.7926, -556.0344,\n",
            "        -558.1417, -557.8868, -556.6812, -578.5015, -597.1773, -586.3303,\n",
            "        -558.2571, -558.1006, -579.6552, -589.4611, -556.0336, -659.6832,\n",
            "        -583.4344, -585.8441, -578.4061, -585.1345, -596.6768, -688.4282,\n",
            "        -581.9925, -588.4122, -612.0518, -558.1927, -558.2024, -558.0775,\n",
            "        -609.5620, -593.1064, -558.1172, -659.9879, -585.8512, -555.9993,\n",
            "        -604.8729, -558.1496, -557.7177, -596.3169, -556.0624, -592.9221,\n",
            "        -558.1063, -586.3151, -585.8265, -586.2862, -610.2742, -593.5261,\n",
            "        -556.0436, -662.9044, -558.1324, -558.2098, -637.4758, -558.1401,\n",
            "        -557.5018, -593.1588, -558.0157, -596.7169, -558.2872, -558.1808,\n",
            "        -597.8436, -558.1804, -556.0693, -592.9133, -558.1522, -656.9960,\n",
            "        -558.1339, -597.1730, -589.3048, -586.4999, -558.2619, -597.5425,\n",
            "        -558.1030, -583.9882, -557.2460, -591.2495, -556.2946, -579.6646,\n",
            "        -596.3243, -592.9165, -558.1539, -662.8344, -558.1592, -558.2834,\n",
            "        -657.0160, -558.2396, -558.1978, -586.4282, -584.4051, -659.2220,\n",
            "        -558.0671, -555.9689, -557.2330, -556.0132, -597.1874, -589.4790,\n",
            "        -558.0619, -662.7560, -558.1917, -558.1757, -586.2880, -558.1995,\n",
            "        -558.1172, -558.2449, -558.3445, -556.0915, -583.3892, -558.0877,\n",
            "        -556.1274, -558.1894, -559.4043, -558.0784, -584.8961, -659.9505,\n",
            "        -558.2593, -557.5663, -555.0694, -558.5184, -583.9200, -592.3438,\n",
            "        -583.3999, -585.2095, -558.2334, -558.4240, -586.4340, -579.5441,\n",
            "        -558.9601, -684.5509, -580.8253, -659.5671, -586.3862, -556.7455,\n",
            "        -576.2394, -558.1304, -607.5180, -590.6618, -558.1569, -581.3243,\n",
            "        -588.7741, -558.1710, -558.1239, -558.2468, -586.3887, -556.9048,\n",
            "        -584.9774, -655.3529, -558.1707, -558.3147, -607.0465, -558.2084,\n",
            "        -558.2338, -558.1744, -586.4594, -584.8860, -586.1588, -597.9749,\n",
            "        -597.7045, -558.2025, -660.3700, -558.1281, -609.3463, -682.0936,\n",
            "        -556.6700, -585.9106, -593.3931, -579.5696, -558.0984, -596.7156,\n",
            "        -558.1423, -556.6841, -558.0972, -558.0852, -558.2611, -579.7821,\n",
            "        -597.4191, -556.0379, -596.4387, -685.0425, -557.1348, -558.2674,\n",
            "        -584.8724, -558.2159, -558.1567, -558.1936, -586.2869, -575.5185,\n",
            "        -585.8137, -558.1880, -558.1009, -558.2028, -575.4349, -581.1888,\n",
            "        -597.4468, -585.8352, -557.2513, -558.2027, -553.7009, -558.0656,\n",
            "        -609.8116, -556.6160, -587.4877, -554.5486, -558.0826, -596.6959,\n",
            "        -558.1502, -558.0474, -573.8163, -558.2109, -596.3229, -690.6418,\n",
            "        -558.2419, -558.2155, -592.9979, -558.2646, -558.1244, -593.9459,\n",
            "        -558.0787, -604.6870, -579.4958, -558.2018, -604.6758, -581.9041,\n",
            "        -613.0624, -582.9902, -596.5742, -648.5993, -558.1608, -557.9758,\n",
            "        -597.1069, -558.2985, -585.9843, -558.1182, -596.8989, -554.6620,\n",
            "        -586.2974, -558.2006, -558.1304, -597.6628, -558.2386, -597.3593,\n",
            "        -592.9352, -592.7274, -558.2177, -557.2567, -553.6196, -558.3033,\n",
            "        -596.8839, -674.0127, -583.5437, -558.2350, -585.8449, -597.1921,\n",
            "        -597.3110, -586.4862, -588.3347, -620.3580, -596.6564, -655.8866,\n",
            "        -586.4932, -558.2746, -662.7630, -596.3187, -586.0010, -585.9893,\n",
            "        -558.4156, -643.4451, -581.1489, -585.9571, -558.2271, -583.4389,\n",
            "        -574.2547, -593.0209, -588.8962, -665.7407, -583.6160, -558.1884,\n",
            "        -557.6626, -580.7860, -558.1738, -619.2177, -558.2182, -587.6867,\n",
            "        -558.1320, -558.1426, -613.8044, -586.2826, -580.3641, -557.2833,\n",
            "        -593.3777, -640.9857, -558.1470, -558.1815, -578.5333, -558.1253,\n",
            "        -555.9714, -558.0980, -597.0236, -573.6746, -558.0770, -557.7742,\n",
            "        -558.1660, -586.4079, -605.0212, -558.2255, -596.7047, -695.1963,\n",
            "        -586.4160, -558.1215, -559.1876, -558.1535, -558.1585, -593.2108,\n",
            "        -558.1055, -553.7411, -596.8911, -588.5006, -558.1998, -558.2357,\n",
            "        -547.2347, -591.6489, -597.7012, -662.9412, -584.3771, -556.0216,\n",
            "        -587.4898, -586.4321, -558.1682, -585.8315, -556.0028, -558.2092,\n",
            "        -557.0145, -558.2040, -589.6237, -582.0192, -607.6525, -558.4461,\n",
            "        -597.1138, -657.9294, -558.1494, -586.4324, -558.2040, -597.0571,\n",
            "        -558.1766, -587.3970, -558.1213, -593.0017, -558.1430, -593.2423,\n",
            "        -558.0861, -609.8180, -558.1731, -558.1205, -596.3177, -591.1800,\n",
            "        -558.1079, -586.4782, -609.3418, -558.1809, -580.9401, -558.1235,\n",
            "        -557.8721, -571.3583, -586.4784, -558.0916, -558.2336, -556.0296,\n",
            "        -598.0778, -586.4453, -596.2211, -593.6415, -558.1498, -596.7203,\n",
            "        -593.2127, -596.4649, -558.1610, -558.1735, -584.9820, -553.4969,\n",
            "        -558.0994, -593.4246, -556.1290, -558.1000, -569.4071, -558.2002,\n",
            "        -593.1174, -691.7999], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.09894555062055588 0.4950000047683716 0.0 580.949951171875\n",
            "tensor([-543.7849, -542.2735, -587.7933, -544.4775, -572.8248, -544.1869,\n",
            "        -544.1776, -543.5312, -564.0502, -575.1620, -563.3183, -543.5645,\n",
            "        -545.5730, -544.1397, -558.6525, -574.4937, -544.0617, -575.7139,\n",
            "        -567.4869, -542.2322, -543.9930, -575.6843, -544.0899, -565.5261,\n",
            "        -572.4426, -544.3725, -543.3547, -544.1159, -541.9996, -574.5956,\n",
            "        -572.8162, -625.3503, -544.1661, -575.1135, -575.7946, -566.9087,\n",
            "        -544.2297, -542.9555, -544.1716, -565.5911, -576.0711, -544.2752,\n",
            "        -544.2382, -567.0596, -627.0017, -544.0856, -569.8854, -621.5069,\n",
            "        -542.6110, -544.4532, -542.3381, -581.0980, -542.6080, -542.0811,\n",
            "        -543.9004, -626.8247, -575.0337, -559.8216, -544.1448, -544.1423,\n",
            "        -633.0163, -544.2661, -544.0479, -640.7978, -566.8729, -544.0011,\n",
            "        -569.4521, -544.1053, -544.1082, -542.6177, -567.1226, -559.3111,\n",
            "        -568.3034, -563.4275, -544.1653, -543.9879, -578.6890, -575.5524,\n",
            "        -578.3233, -649.1337, -544.1107, -544.0505, -630.7053, -542.6420,\n",
            "        -544.1781, -562.5633, -581.9061, -542.2573, -563.7698, -575.4131,\n",
            "        -542.6637, -543.2679, -568.5975, -575.6261, -576.1946, -570.0214,\n",
            "        -565.5185, -566.9512, -615.6758, -543.5755, -563.3084, -574.4983,\n",
            "        -544.4308, -593.9187, -542.5797, -544.0552, -542.0935, -542.1029,\n",
            "        -540.8232, -544.3614, -558.8321, -648.5461, -559.8471, -542.2736,\n",
            "        -561.3373, -575.0013, -544.0361, -543.2827, -542.6238, -572.3422,\n",
            "        -566.1935, -602.8608, -574.6816, -544.2145, -544.2339, -544.4301,\n",
            "        -576.0862, -631.6416, -559.4857, -544.1450, -544.1923, -568.8096,\n",
            "        -544.3668, -542.3861, -542.1257, -618.8103, -567.6589, -544.0815,\n",
            "        -572.8317, -567.2330, -545.9460, -544.1473, -574.5261, -632.8999,\n",
            "        -544.4290, -565.3760, -565.4130, -543.3317, -543.3298, -544.1755,\n",
            "        -543.8967, -595.0013, -562.0446, -544.0637, -544.4014, -561.2894,\n",
            "        -628.3912, -563.3635, -568.4341, -565.4117, -543.9691, -543.5773,\n",
            "        -542.1937, -544.1646, -543.3361, -543.3422, -544.2281, -628.3770,\n",
            "        -566.9960, -543.5446, -567.7163, -544.1443, -545.4283, -573.6437,\n",
            "        -575.0856, -622.3591, -565.3998, -544.4617, -575.9515, -575.6205,\n",
            "        -543.5203, -559.4904, -544.2261, -593.8594, -578.7985, -544.2420,\n",
            "        -544.2326, -566.9670, -541.8694, -575.0944, -569.8992, -574.4276,\n",
            "        -559.9819, -575.1487, -540.1141, -544.0233, -544.2314, -574.4914,\n",
            "        -544.3911, -564.0864, -544.2109, -575.7197, -542.6193, -563.5035,\n",
            "        -548.5555, -542.6390, -571.2045, -575.1326, -558.6815, -543.8283,\n",
            "        -545.7200, -572.1035, -575.6104, -575.2823, -543.2941, -582.9894,\n",
            "        -566.7993, -648.4581, -544.4026, -544.1125, -633.1514, -544.2316,\n",
            "        -575.0383, -574.9461, -542.6218, -544.1590, -572.8239, -544.0318,\n",
            "        -565.6140, -575.1534, -561.6188, -546.3088, -572.7979, -575.1208,\n",
            "        -544.3964, -542.1887, -628.3078, -565.2715, -575.1729, -574.5911,\n",
            "        -544.1806, -544.3008, -542.3819, -544.1121, -544.1775, -567.4350,\n",
            "        -544.0641, -622.1416, -567.8468, -561.2150, -565.4639, -544.4218,\n",
            "        -580.3624, -575.7388, -569.9199, -575.8755, -567.1777, -565.6307,\n",
            "        -553.5117, -544.2859, -542.3912, -574.7155, -544.4068, -540.0737,\n",
            "        -544.2583, -600.0803, -544.0873, -544.4282, -573.5457, -602.6198,\n",
            "        -574.4388, -575.1416, -542.2186, -544.1974, -561.3051, -544.2966,\n",
            "        -575.3555, -561.4625, -575.6323, -571.1738, -575.1151, -596.6718,\n",
            "        -542.6205, -567.1948, -545.8359, -570.3020, -573.5334, -635.6020],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.09467092156410217 0.4950000047683716 0.0 563.1165161132812\n",
            "tensor([-530.3405, -530.3089, -529.2089, -529.1824, -582.2001, -532.7206,\n",
            "        -539.3340, -530.3132, -529.7244, -542.4692, -531.0077, -529.9973,\n",
            "        -584.6027, -536.2866, -550.3843, -528.8300, -547.6892, -530.1103,\n",
            "        -550.3420, -597.1099, -599.2747, -530.8766, -550.5042, -543.4730,\n",
            "        -529.1463, -531.0821, -528.7864, -529.0027, -543.2755, -529.2242,\n",
            "        -529.2097, -548.1165, -529.1382, -549.3845, -556.6876, -556.8008,\n",
            "        -582.8123, -530.8754, -547.1404, -586.2208, -529.0869, -530.8154,\n",
            "        -556.2698, -529.9810, -553.3964, -530.7845, -545.9252, -532.7657,\n",
            "        -532.6678, -530.6607, -529.0571, -606.1623, -593.8983, -552.2055,\n",
            "        -541.6653, -549.9290, -553.2364, -530.3305, -531.0355, -529.7501,\n",
            "        -557.2037, -530.1816, -541.1927, -531.0589, -550.3349, -553.4003,\n",
            "        -530.5161, -596.6434, -556.8217, -528.9493, -553.1424, -533.5012,\n",
            "        -553.4036, -529.1346, -531.1581, -529.5551, -557.3716, -531.0646,\n",
            "        -547.3691, -531.1423, -550.3970, -582.9777, -529.1651, -557.2648,\n",
            "        -560.4131, -529.1143, -553.4437, -533.4307, -529.0456, -529.0105,\n",
            "        -547.3665, -530.1075, -581.1860, -542.0267, -552.7498, -529.0713,\n",
            "        -530.9448, -539.1879, -530.1190, -550.6454, -583.0299, -529.1324,\n",
            "        -557.5860, -559.7368, -549.8145, -531.1016, -531.0978, -548.9385,\n",
            "        -549.9981, -529.0198, -529.8255, -542.1306, -545.9383, -539.7136,\n",
            "        -545.0040, -597.2659, -554.8344, -553.4440, -539.3315, -542.8932,\n",
            "        -582.0645, -530.2965, -550.4438, -531.0656, -577.7739, -531.0826,\n",
            "        -529.1647, -538.6252, -550.4000, -550.1071, -529.4924, -593.7563,\n",
            "        -600.1473, -540.8076, -553.1200, -551.6097, -552.7066, -530.8492,\n",
            "        -544.9233, -533.5323, -529.9509, -550.3582, -532.1187, -549.6998,\n",
            "        -556.8391, -530.8400, -550.3459, -597.2405, -608.4958, -550.1837,\n",
            "        -557.0654, -551.0134, -583.7755, -538.0213, -547.3648, -536.0825,\n",
            "        -551.8037, -527.1592, -528.9932, -530.8864, -554.0569, -530.7668,\n",
            "        -548.1884, -557.7435, -582.1489, -529.2011, -557.4611, -530.3184,\n",
            "        -558.0700, -550.0900, -550.4167, -550.8046, -533.4564, -541.2280,\n",
            "        -550.0245, -543.8696, -530.9250, -545.0331, -549.8521, -557.7715,\n",
            "        -557.5431, -529.7389, -557.0762, -530.9621, -552.1506, -543.7338,\n",
            "        -531.0305, -542.1346, -528.5472, -530.9218, -539.3281, -530.7639,\n",
            "        -531.0699, -530.8960, -557.7435, -545.0250, -607.6471, -546.0242,\n",
            "        -551.7075, -555.7709, -556.8272, -552.2794, -529.0245, -551.3251,\n",
            "        -529.8730, -550.1011, -530.0123, -529.1635, -529.5224, -552.8356,\n",
            "        -531.0416, -552.8373, -608.3235, -553.4444, -556.8199, -550.4410,\n",
            "        -531.0650, -529.2410, -530.9850, -560.1049, -526.7111, -530.9344,\n",
            "        -549.7441, -531.0449, -529.1166, -551.6460, -529.1536, -553.8105,\n",
            "        -605.2410, -549.7537, -557.0108, -577.7441, -529.8289, -543.5963,\n",
            "        -527.7060, -557.6027, -575.3203, -528.9968, -529.8450, -529.9181,\n",
            "        -542.5065, -541.1334, -582.9853, -586.7368, -580.2762, -549.8519,\n",
            "        -557.3289, -602.3465, -553.2805, -547.8693, -530.1826, -528.2310,\n",
            "        -599.8827, -532.6636, -552.2935, -529.8776, -529.0159, -530.9388,\n",
            "        -529.0796, -580.9659, -556.8550, -550.3839, -557.7273, -533.4178,\n",
            "        -552.3663, -530.7272, -530.8925, -533.5111, -529.6285, -550.4260,\n",
            "        -529.1429, -527.7272, -544.8316, -529.1182, -539.0290, -554.1281,\n",
            "        -606.4813, -553.7704, -553.4285, -531.0562, -529.0961, -549.6991,\n",
            "        -530.3853, -543.0605, -545.4199, -530.9580, -550.2203, -530.8668,\n",
            "        -528.4234, -529.1838, -547.9110, -557.1230, -611.7037, -532.6830,\n",
            "        -557.0682, -533.5364, -529.1569, -530.6986, -544.5846, -524.5594,\n",
            "        -557.0075, -528.7708, -545.0488, -530.5648, -556.8583, -540.7108,\n",
            "        -530.9998, -553.1500, -583.5064, -551.8375, -553.2066, -561.8629,\n",
            "        -529.9037, -539.7439, -550.3023, -553.7930, -536.9138, -530.0655,\n",
            "        -528.9519, -531.1138, -550.3029, -552.2667, -550.3095, -557.6735,\n",
            "        -620.5149, -550.4896, -556.6353, -604.2974, -553.1352, -531.1458,\n",
            "        -530.8987, -556.8478, -539.2719, -529.2196, -529.1138, -545.4308,\n",
            "        -554.1240, -553.4122, -551.6138, -553.7389, -620.4735, -530.3580,\n",
            "        -553.5918, -528.9346, -556.8030, -530.9265, -547.3768, -555.5891,\n",
            "        -571.9410, -553.2395, -541.1390, -530.1558, -598.5549, -530.9841,\n",
            "        -530.8344, -602.2524, -604.9553, -552.8076, -554.1490, -551.5999,\n",
            "        -531.1246, -543.0953, -548.6059, -543.4304, -570.0413, -530.8198,\n",
            "        -550.3214, -529.7873, -530.9987, -556.8447, -530.9496, -607.1639,\n",
            "        -597.5034, -553.2311, -553.0848, -520.9362, -557.3040, -545.2831,\n",
            "        -528.9027, -557.7963, -550.0226, -550.1832, -528.9256, -530.9515,\n",
            "        -544.2980, -530.8763, -550.4413, -556.6699, -582.3017, -542.0193,\n",
            "        -552.9496, -530.1237, -552.9548, -541.5991, -529.7203, -541.8732,\n",
            "        -550.9763, -555.4901, -530.8694, -528.9519, -531.0677, -529.0234,\n",
            "        -531.0678, -551.9073, -599.2760, -529.0429, -552.9317, -529.9346,\n",
            "        -530.3683, -550.0610, -530.7867, -540.8878, -552.2812, -550.3309,\n",
            "        -552.7691, -550.4297, -531.1019, -548.3554, -531.1169, -598.6006,\n",
            "        -589.6075, -549.8248, -538.2092, -545.8943, -552.2540, -528.6608,\n",
            "        -530.1607, -543.7885, -556.5709, -530.8323, -531.1042, -529.8874,\n",
            "        -529.2434, -531.0549, -617.3384, -553.2863, -586.4426, -553.3868,\n",
            "        -552.5154, -597.6010, -557.7051, -550.3309, -530.9791, -544.6348,\n",
            "        -545.6035, -530.8638, -530.9747, -529.0931, -550.3012, -529.9548,\n",
            "        -549.6719, -542.4999, -554.4334, -531.0801, -557.7698, -542.7951,\n",
            "        -528.9716, -528.9639, -540.6976, -553.3616, -551.2863, -553.8361,\n",
            "        -530.8792, -529.1320, -550.3357, -530.8315, -555.4693, -557.0653,\n",
            "        -605.8530, -531.0314, -558.2791, -535.9421, -550.4481, -551.7729,\n",
            "        -530.7981, -543.6446, -606.6226, -543.0298, -550.1532, -530.3434,\n",
            "        -529.9897, -553.3644, -530.8228, -557.2266, -618.6086, -547.2047,\n",
            "        -557.0020, -556.7869, -587.7198, -552.7957, -531.0623, -596.8408,\n",
            "        -533.6675, -531.0667, -547.4229, -530.5901, -531.0782, -531.0753,\n",
            "        -589.9772, -602.5828, -619.1147, -531.0017, -549.9001, -532.5067,\n",
            "        -547.2091, -549.8954, -531.0965, -602.3665, -530.1252, -530.9281,\n",
            "        -528.9936, -528.9066], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.08988650888204575 0.4950000047683716 0.0 547.6339111328125\n",
            "tensor([-540.6646, -518.3759, -529.7906, -540.6428, -593.0876, -515.7308,\n",
            "        -541.0895, -561.7255, -514.7271, -516.5224, -530.3790, -514.8959,\n",
            "        -518.1187, -516.4921, -515.6081, -516.4281, -533.8691, -533.8368,\n",
            "        -514.8156, -576.7929, -552.6711, -531.5023, -559.0011, -539.7413,\n",
            "        -515.4237, -515.2552, -516.5380, -553.7578, -520.4694, -525.7531,\n",
            "        -516.9568, -519.2722, -532.9581, -516.5589, -533.3972, -530.6406,\n",
            "        -551.4258, -517.9312, -507.1387, -532.2650, -520.0745, -506.7407,\n",
            "        -532.7622, -532.8736, -521.9448, -521.3978, -508.1722, -515.9329,\n",
            "        -524.0829, -515.4496, -540.5818, -525.5012, -574.7808, -532.3894,\n",
            "        -536.0866, -514.8073, -540.3154, -515.3709, -516.4053, -515.9822,\n",
            "        -522.1352, -530.5934, -516.6557, -516.0666, -533.4921, -540.4063,\n",
            "        -516.9112, -538.7627, -547.8801, -532.3837, -529.8359, -558.6000,\n",
            "        -539.9894, -515.2250, -516.9726, -540.2859, -517.6278, -515.3441,\n",
            "        -517.0020, -533.9545, -533.3802, -521.0835, -533.2741, -540.5914,\n",
            "        -544.6685, -514.8162, -532.5005, -519.6348, -535.1796, -516.8414,\n",
            "        -533.5701, -572.7598, -528.3676, -516.9856, -516.9323, -515.6940,\n",
            "        -517.1038, -516.5217, -528.4490, -540.4508, -569.0858, -514.7805,\n",
            "        -530.8099, -558.9355, -516.4227, -532.2941, -517.1014, -533.0792,\n",
            "        -560.7555, -531.6771, -516.9095, -514.7902, -535.1544, -539.3972,\n",
            "        -555.7241, -562.5893, -579.1822, -525.4178, -531.8611, -504.7476,\n",
            "        -547.4481, -515.7490, -533.4354, -576.9951, -530.7477, -516.9162,\n",
            "        -517.0554, -534.0471, -519.3786, -516.8624, -531.6733, -558.6403,\n",
            "        -576.3877, -534.7467, -540.5941, -542.8491, -516.9180, -517.0240,\n",
            "        -516.9219, -506.7135, -526.2291, -516.4302, -533.9167, -515.8250,\n",
            "        -516.4945, -528.5512, -529.0870, -539.4329, -579.5133, -517.0533,\n",
            "        -532.4914, -550.5768, -514.8889, -516.9180, -534.6705, -513.0457,\n",
            "        -534.9152, -533.5263, -519.1434, -517.0191, -540.9586, -516.5533,\n",
            "        -515.3565, -576.7929, -549.8545, -533.1889, -569.4567, -515.8106,\n",
            "        -532.9161, -530.7568, -517.0299, -527.0196, -506.6775, -514.8101,\n",
            "        -529.4232, -517.0238, -539.8424, -531.4633, -516.6775, -572.4218,\n",
            "        -549.0341, -516.9059, -547.4598, -532.2974, -532.8248, -517.0016,\n",
            "        -516.8919, -533.4670, -579.2048, -516.9843, -522.9240, -533.2521,\n",
            "        -516.8830, -517.0166, -539.2815, -540.1068, -544.5876, -516.9615,\n",
            "        -560.7249, -538.8852, -540.5180, -517.0106, -516.9927, -539.6220,\n",
            "        -528.6148, -533.6427, -516.9384, -534.8004, -531.8110, -515.6250,\n",
            "        -515.4851, -563.5618, -580.9443, -534.6903, -535.4417, -540.5576,\n",
            "        -529.9022, -516.8743, -514.8077, -533.9709, -520.3013, -516.9657,\n",
            "        -533.4311, -514.7821, -515.3738, -540.6217, -514.9757, -573.7879,\n",
            "        -545.1002, -532.9856, -540.6118, -580.2782, -526.8129, -515.3217,\n",
            "        -522.8690, -555.4593, -549.2101, -515.9001, -530.2778, -516.4240,\n",
            "        -514.7792, -516.9666, -515.8704, -542.6230, -589.8844, -516.9976,\n",
            "        -540.4673, -587.3749, -514.7947, -533.0969, -534.7186, -526.2985,\n",
            "        -517.9579, -527.0159, -529.3307, -515.1552, -516.7192, -540.6309,\n",
            "        -517.0012, -582.3028, -556.7611, -516.6005, -535.0659, -532.2864,\n",
            "        -529.8337, -516.9949, -529.6097, -573.7633, -525.3015, -516.9432,\n",
            "        -517.1129, -515.7086, -533.8670, -532.7555, -563.9006, -539.9512,\n",
            "        -561.8552, -514.8322, -539.8069, -525.4135, -514.8533, -516.5755,\n",
            "        -525.5939, -561.5994, -534.6487, -533.0558, -531.0967, -516.3453],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.08407013863325119 0.4950000047683716 0.0 531.5087890625\n",
            "tensor([-534.3017, -505.6889, -514.0584, -504.7301, -524.3082, -561.1662,\n",
            "        -524.3054, -509.7491, -515.3248, -502.2556, -548.8275, -487.6266,\n",
            "        -517.1418, -500.3706, -518.0676, -489.5515, -504.7829, -539.9874,\n",
            "        -499.9279, -522.8404, -505.6491, -514.4694, -505.5763, -511.0635,\n",
            "        -512.7979, -524.3425, -517.1216, -511.8183, -508.9919, -502.3484,\n",
            "        -501.6099, -501.2813, -555.9800, -511.1454, -517.0688, -486.4161,\n",
            "        -524.3727, -529.5248, -524.5276, -524.3210, -512.8475, -524.3254,\n",
            "        -529.9599, -523.4503, -502.3088, -501.2242, -500.7224, -483.3003,\n",
            "        -524.4633, -501.4126, -501.5479, -506.0003, -500.4765, -524.2164,\n",
            "        -501.7363, -499.1542, -500.9852, -500.4409, -518.7209, -504.7753,\n",
            "        -516.7430, -501.9734, -500.7129, -561.9921, -500.5881, -545.8904,\n",
            "        -500.3301, -504.3646, -517.2016, -526.2155, -503.5395, -489.7352,\n",
            "        -500.6521, -499.9521, -524.4456, -498.6182, -508.8349, -501.6288,\n",
            "        -515.7119, -517.1707, -505.6527, -526.5060, -501.8838, -503.6973,\n",
            "        -509.0984, -541.7780, -533.4295, -512.0628, -500.7459, -516.3044,\n",
            "        -523.8925, -504.3799, -502.1662, -500.7060, -517.2642, -496.0121,\n",
            "        -503.5985, -502.4720, -515.5115, -556.0618, -500.0591, -561.0157,\n",
            "        -516.1812, -499.0599, -502.5266, -515.0820, -524.2045, -498.9334,\n",
            "        -513.9642, -486.9911, -516.7684, -514.1121, -500.5868, -519.7124,\n",
            "        -517.9414, -519.2881, -514.4630, -518.4876, -518.6515, -516.0743,\n",
            "        -502.3994, -514.5210, -524.2869, -503.6724, -515.7204, -501.8124,\n",
            "        -500.8453, -512.7967, -505.0884, -556.0221, -516.4669, -501.5832,\n",
            "        -502.3104, -524.3523, -505.5648, -504.5504, -516.3104, -502.3474,\n",
            "        -524.0195, -505.0122, -502.4011, -515.5473, -502.4839, -523.8210,\n",
            "        -504.1968, -561.9073, -501.5178, -502.2019, -502.1171, -518.0613,\n",
            "        -505.8322, -503.9256, -502.3401, -518.6689, -545.3213, -519.3694,\n",
            "        -502.2040, -483.6480, -501.8471, -543.8138, -549.1516, -505.5591,\n",
            "        -501.2346, -500.6879, -512.0729, -534.3733, -546.0937, -500.8760,\n",
            "        -516.4553, -500.6835, -522.7594, -516.5671, -499.9193, -502.0706,\n",
            "        -500.7682, -516.5549, -551.9433, -504.7024, -512.4193, -505.7795,\n",
            "        -523.5498, -525.4160, -525.5270, -504.9886, -516.0992, -500.0240,\n",
            "        -524.3813, -488.9189, -501.5238, -502.2633, -501.9296, -493.1051,\n",
            "        -514.2505, -503.9525, -516.5726, -506.1834, -524.3699, -521.0781,\n",
            "        -512.6830, -505.1215, -502.4024, -523.5311, -547.4839, -484.5291,\n",
            "        -501.2859, -517.2476, -501.5027, -521.3391, -498.9094, -530.5615,\n",
            "        -502.4579, -505.6559, -523.2595, -516.3609, -490.8247, -500.7414,\n",
            "        -516.7618, -500.6608, -555.3812, -545.4183, -500.9263, -512.0363,\n",
            "        -500.3191, -505.6751, -507.0880, -489.6649, -504.7595, -502.4391,\n",
            "        -500.6203, -555.0306, -526.3995, -538.1570, -501.2349, -492.7370,\n",
            "        -524.2716, -508.5386, -515.7277, -502.2458, -502.1618, -516.1924,\n",
            "        -552.1642, -562.1047, -500.8551, -504.2104, -502.2632, -515.0930,\n",
            "        -524.4504, -514.6523, -514.0748, -510.2804, -524.5013, -524.3275,\n",
            "        -500.4965, -516.3910, -501.3506, -511.3048, -516.4635, -501.8945,\n",
            "        -504.1677, -504.8557, -502.3512, -554.6810, -498.0098, -505.2859,\n",
            "        -510.7858, -543.6385, -545.4865, -500.7601, -502.7897, -502.3439,\n",
            "        -502.4289, -550.2595, -512.6260, -506.2321, -517.0894, -543.1105,\n",
            "        -524.3331, -533.1345, -492.3798, -505.5645, -502.4040, -517.7288,\n",
            "        -545.4689, -517.5812, -491.4428, -502.1853, -501.9126, -506.5747,\n",
            "        -512.2972, -504.2844, -516.0640, -507.0791, -503.7263, -524.3320,\n",
            "        -513.2354, -516.3517, -502.4072, -507.6649, -523.5817, -502.1310,\n",
            "        -514.1364, -502.5082, -500.7122, -503.7509, -544.0645, -543.8338,\n",
            "        -495.8546, -494.1538, -508.7755, -546.2651, -511.0467, -521.4310,\n",
            "        -501.2966, -509.9555, -518.6436, -540.5989, -500.9171, -516.9912,\n",
            "        -502.1183, -516.4437, -493.1796, -524.3698, -501.8804, -515.2534,\n",
            "        -502.4633, -515.3326, -501.2569, -503.2938, -502.4645, -516.4082,\n",
            "        -553.6581, -503.9264, -515.6956, -505.6826, -501.4663, -502.1856,\n",
            "        -497.3899, -500.3778, -502.4251, -500.2520, -502.0154, -548.1490,\n",
            "        -545.8415, -485.5755, -511.2498, -499.9509, -522.8453, -488.6272,\n",
            "        -513.1225, -515.8643, -496.8528, -505.9183, -502.7131, -510.9448,\n",
            "        -501.8144, -515.2122, -495.8212, -509.8822, -543.8462, -538.4660,\n",
            "        -500.6650, -502.3323, -541.9582, -489.5725, -510.1042, -515.6732,\n",
            "        -510.6685, -505.0949, -503.7978, -509.7637, -512.0818, -509.9177,\n",
            "        -540.6482, -554.9479, -514.0046, -503.6047, -512.4216, -541.7501,\n",
            "        -558.6107, -503.2471, -516.5562, -500.7332, -502.6641, -505.6181,\n",
            "        -500.6017, -545.5394, -502.2415, -544.4387, -522.7935, -540.5085,\n",
            "        -487.4134, -497.7756, -516.8980, -515.3136, -541.2166, -555.4799,\n",
            "        -502.1044, -501.6944, -500.4461, -550.2612, -508.3428, -494.0874,\n",
            "        -516.8110, -532.4079, -516.0234, -557.4759, -506.1710, -505.4875,\n",
            "        -512.2870, -502.3855, -518.6790, -512.2831, -500.6407, -500.9462,\n",
            "        -502.3639, -518.0067, -501.4324, -506.2908, -501.2460, -525.7937,\n",
            "        -516.6157, -530.2589, -545.8989, -522.8701, -500.6600, -501.3639,\n",
            "        -557.6124, -499.9315, -507.7793, -515.1917, -502.3884, -517.5844,\n",
            "        -538.0963, -505.5930, -500.4956, -512.6293, -486.0376, -542.5574,\n",
            "        -509.9421, -534.3481, -510.0840, -502.4875, -522.8779, -536.2856,\n",
            "        -502.3748, -508.7996, -512.0947, -492.7675, -524.3890, -507.0580,\n",
            "        -517.1064, -504.5113, -524.4046, -550.7140, -552.1539, -530.2825,\n",
            "        -496.3716, -510.8333, -552.1095, -505.9487, -503.1985, -516.2324,\n",
            "        -501.3949, -540.4359, -511.9961, -501.4698, -500.7410, -516.8811,\n",
            "        -516.7012, -513.7394, -524.2987, -502.6244, -499.9389, -503.9734,\n",
            "        -550.6263, -532.5485, -516.5486, -500.9258, -495.1230, -499.7284,\n",
            "        -506.0460, -548.5188, -508.7223, -544.1091, -516.4967, -556.0889,\n",
            "        -501.1497, -551.9587, -500.7787, -524.3297, -523.9818, -546.1674,\n",
            "        -502.3239, -501.0309, -510.7278, -500.5173, -505.6234, -517.0123,\n",
            "        -516.6455, -506.1812, -501.0440, -561.3030, -507.0862, -510.4056,\n",
            "        -510.9459, -502.3181, -517.0396, -519.8257, -503.7173, -502.5075,\n",
            "        -502.4939, -493.5175], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.07730835676193237 0.4950000047683716 0.0 513.7940673828125\n",
            "tensor([-493.8312, -500.8350, -487.0907, -526.0865, -492.0819, -500.4184,\n",
            "        -509.8220, -501.7462, -485.6748, -517.7983, -509.9058, -491.0656,\n",
            "        -494.5139, -492.0217, -488.3329, -502.6515, -487.1495, -519.2092,\n",
            "        -500.2361, -493.7249, -501.5692, -517.6732, -489.1282, -487.6986,\n",
            "        -487.4228, -486.9755, -508.8253, -491.5185, -486.5727, -485.8678,\n",
            "        -494.8947, -512.8913, -491.0647, -484.2133, -486.9746, -486.7175,\n",
            "        -510.3127, -494.3832, -534.7705, -510.2324, -492.0341, -497.8234,\n",
            "        -508.3917, -510.3281, -486.6111, -487.1598, -487.7607, -491.6871,\n",
            "        -469.9388, -493.9865, -500.3761, -473.8046, -495.1815, -502.0782,\n",
            "        -491.6845, -500.2147, -486.6364, -487.6086, -531.8464, -483.4410,\n",
            "        -487.1875, -499.1959, -493.9629, -489.6942, -508.9983, -509.6298,\n",
            "        -487.9563, -500.1281, -510.3663, -506.7671, -490.1770, -524.7026,\n",
            "        -486.7931, -487.1361, -523.0987, -486.9435, -487.8742, -486.6781,\n",
            "        -478.1271, -476.5903, -509.9080, -497.7923, -486.7264, -478.8211,\n",
            "        -487.0522, -484.0669, -516.0210, -489.8258, -486.5518, -486.8157,\n",
            "        -503.3703, -473.7177, -498.5426, -489.8970, -486.6610, -501.1159,\n",
            "        -480.2019, -483.6726, -487.3011, -521.6010, -487.4113, -492.6320,\n",
            "        -492.5019, -501.8895, -486.8574, -499.7961, -510.2793, -492.8474,\n",
            "        -486.9656, -487.0909, -487.0862, -526.4320, -472.6462, -462.2411,\n",
            "        -486.8485, -507.5143, -487.9669, -490.3460, -500.8870, -528.5138,\n",
            "        -487.8751, -487.0978, -510.3035, -487.1484, -489.3304, -486.6788,\n",
            "        -487.7758, -484.9777, -493.6796, -478.3032, -487.2001, -492.5341,\n",
            "        -498.2426, -517.0635, -486.9860, -486.6877, -486.6881, -487.6839,\n",
            "        -533.0000, -494.1818, -497.7393, -486.7572, -500.1774, -519.4581,\n",
            "        -479.1676, -510.1637, -480.3362, -480.8724, -487.3121, -501.1759,\n",
            "        -491.1017, -471.9979, -490.7437, -487.9234, -517.2795, -484.0440,\n",
            "        -487.2123, -487.0741, -487.2819, -497.7917, -524.4669, -481.4455,\n",
            "        -486.6236, -463.1179, -497.7759, -525.8660, -492.4855, -504.4106,\n",
            "        -481.4083, -510.0327, -519.5805, -524.5522, -473.1833, -486.7036,\n",
            "        -486.9697, -492.8041, -487.0591, -497.4679, -481.3988, -509.0073,\n",
            "        -507.2538, -528.5612, -530.6700, -484.2172, -499.1044, -488.4086,\n",
            "        -528.5706, -529.7612, -485.5197, -486.5332, -499.6792, -521.7224,\n",
            "        -493.1111, -512.7458, -488.3818, -519.2333, -486.8783, -509.1240,\n",
            "        -500.0396, -510.2576, -492.0511, -510.1348, -510.3306, -494.5506,\n",
            "        -486.7324, -487.3011, -478.1093, -509.8842, -489.6086, -494.4717,\n",
            "        -471.3547, -522.1323, -509.6373, -514.1796, -479.9673, -485.6738,\n",
            "        -498.1131, -492.8817, -510.3271, -498.7029, -501.1365, -496.2636,\n",
            "        -487.8702, -528.5317, -492.7239, -485.7903, -499.8729, -484.2177,\n",
            "        -486.8833, -517.7576, -506.7042, -493.9813, -500.3641, -486.7021,\n",
            "        -510.3590, -490.1168, -492.2917, -497.2983, -494.2651, -507.9498,\n",
            "        -501.8190, -491.6917, -488.0016, -473.8018, -500.5001, -531.6988,\n",
            "        -499.9627, -503.5027, -485.2784, -508.3341, -496.4771, -500.1597,\n",
            "        -486.6221, -487.7620, -501.0343, -493.2224, -497.7169, -497.9097,\n",
            "        -486.6702, -487.5750, -509.6265, -522.5808, -493.3893, -486.7787,\n",
            "        -482.3605, -498.7225, -520.6085, -480.2467, -499.1151, -487.6278,\n",
            "        -485.6937, -499.8295, -510.1302, -486.7461, -498.9632, -494.1812,\n",
            "        -497.7325, -530.9804, -493.1638, -498.4288, -488.0826, -487.3000,\n",
            "        -499.3339, -478.1513, -490.2542, -486.8390, -485.6463, -512.5929],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.07301273941993713 0.4950000047683716 0.0 496.3512878417969\n",
            "tensor([-491.2939, -461.0680, -497.5627, -474.0912, -466.6288, -473.4693,\n",
            "        -503.4485, -473.2191, -477.9236, -485.4691, -474.0493, -494.5641,\n",
            "        -473.9955, -474.4415, -474.3555, -481.4473, -510.3260, -473.3258,\n",
            "        -492.6626, -482.6412, -476.1157, -485.8435, -504.9643, -483.8021,\n",
            "        -481.3534, -514.6073, -474.4449, -501.9243, -462.1207, -485.0145,\n",
            "        -483.0073, -458.4456, -503.6217, -463.8156, -496.2838, -474.4325,\n",
            "        -495.3743, -474.7190, -508.3823, -480.3416, -471.6458, -509.2387,\n",
            "        -483.8203, -490.8097, -465.9791, -496.1628, -474.2575, -484.2229,\n",
            "        -490.8935, -479.4633, -494.5788, -474.2358, -496.1504, -485.2594,\n",
            "        -481.9728, -509.2375, -495.7979, -468.1408, -472.9540, -450.6940,\n",
            "        -459.0788, -473.3877, -473.0822, -468.3496, -496.3196, -499.9648,\n",
            "        -505.6084, -482.7318, -489.4637, -481.8257, -505.5850, -474.4712,\n",
            "        -482.3935, -467.6128, -473.7603, -473.5700, -466.5782, -466.9224,\n",
            "        -474.5081, -480.5392, -461.8341, -455.4288, -495.8807, -470.9632,\n",
            "        -468.9955, -472.9590, -466.6081, -462.8760, -492.9235, -472.6675,\n",
            "        -474.4668, -470.9041, -461.7597, -494.4434, -473.7411, -498.1218,\n",
            "        -494.1025, -487.2132, -503.0648, -484.4321, -450.9181, -474.2428,\n",
            "        -491.7545, -448.4833, -478.1220, -474.4891, -474.9641, -490.1137,\n",
            "        -494.1108, -485.2272, -474.4370, -478.1083, -484.3177, -465.7755,\n",
            "        -496.3266, -473.7550, -494.4315, -474.3198, -475.3721, -498.9442,\n",
            "        -508.5443, -479.3606, -473.5263, -467.8802, -488.8460, -475.8877,\n",
            "        -474.1215, -478.6510, -486.0507, -477.8667, -487.2088, -485.0236,\n",
            "        -501.5349, -472.8363, -483.4559, -479.7908, -449.2791, -476.7908,\n",
            "        -476.7514, -474.8504, -494.5335, -474.8277, -474.7866, -462.9063,\n",
            "        -496.3562, -478.2844, -482.4129, -485.0802, -503.8607, -474.0811,\n",
            "        -511.3113, -506.8736, -446.1005, -472.4265, -474.7604, -473.0876,\n",
            "        -494.9033, -474.6006, -474.0876, -482.3116, -496.0571, -509.9283,\n",
            "        -503.6771, -481.6420, -484.4077, -474.0845, -503.1191, -481.3479,\n",
            "        -487.9515, -478.0143, -474.7459, -503.8954, -502.2862, -478.0840,\n",
            "        -485.9434, -505.4590, -484.2594, -472.9550, -492.7176, -460.9128,\n",
            "        -483.0120, -473.8008, -492.8748, -478.0399, -493.8402, -472.4337,\n",
            "        -474.0736, -482.9093, -499.1495, -474.4007, -477.7639, -498.6104,\n",
            "        -482.3277, -474.5465, -511.3691, -483.8151, -497.0914, -485.7909,\n",
            "        -509.2600, -498.4726, -479.7996, -493.9027, -485.1562, -463.3252,\n",
            "        -504.2091, -492.8795, -485.7831, -452.9104, -484.3340, -510.3283,\n",
            "        -492.1863, -471.7726, -490.3044, -474.3052, -486.8264, -481.0419,\n",
            "        -473.3095, -479.4286, -474.7465, -455.5446, -465.7027, -473.3980,\n",
            "        -474.0490, -474.2660, -495.8945, -470.6784, -496.2772, -472.4781,\n",
            "        -495.2538, -473.3921, -498.7067, -471.9716, -478.1432, -481.4066,\n",
            "        -474.4998, -461.9142, -458.8683, -479.7984, -473.5229, -506.0420,\n",
            "        -495.8884, -495.9691, -489.9436, -473.3622, -473.5848, -482.3833,\n",
            "        -509.9901, -484.2883, -482.8047, -481.7660, -474.2354, -501.9257,\n",
            "        -503.1894, -485.9583, -474.1924, -496.2081, -473.8561, -503.3558,\n",
            "        -503.3440, -463.5313, -466.3708, -473.7362, -511.3983, -464.6081,\n",
            "        -472.4087, -509.4089, -469.7404, -473.5951, -458.3326, -473.7422,\n",
            "        -474.5713, -467.0406, -489.3018, -478.1296, -487.9699, -474.7543,\n",
            "        -477.8206, -474.2629, -475.2431, -481.1189, -498.8743, -480.6042,\n",
            "        -474.0279, -485.5897, -474.6880, -495.8806, -483.3673, -495.3051,\n",
            "        -488.5721, -467.4254, -499.4568, -473.1525, -490.0214, -482.9980,\n",
            "        -510.3778, -492.3562, -478.0962, -495.1574, -474.4215, -503.1766,\n",
            "        -473.0233, -474.3859, -474.3430, -479.3150, -476.6010, -492.6775,\n",
            "        -495.8939, -474.7608, -483.1855, -474.3029, -474.5194, -491.3153,\n",
            "        -480.2104, -509.2776, -484.3123, -462.0793, -503.3260, -474.1621,\n",
            "        -474.2814, -484.4984, -482.3017, -498.8042, -506.2335, -453.6066,\n",
            "        -503.3307, -485.7528, -481.3387, -487.8640, -471.7373, -465.1034,\n",
            "        -484.9898, -506.4472, -461.8874, -467.5624, -474.2433, -455.8560,\n",
            "        -494.5727, -486.7559, -496.0796, -474.8057, -485.5605, -474.6256,\n",
            "        -501.9244, -460.1198, -481.3618, -474.7905, -473.1831, -495.6153,\n",
            "        -461.7232, -474.3162, -478.5982, -498.8205, -474.5624, -506.2049,\n",
            "        -497.9670, -474.4926, -506.4162, -476.4144, -474.0064, -479.4380,\n",
            "        -484.9287, -473.2628, -476.8937, -506.6566, -465.6831, -474.7589,\n",
            "        -474.5207, -510.5076, -495.8712, -481.7913, -493.5763, -484.2975,\n",
            "        -485.9396, -484.3245, -491.7760, -461.9796, -474.0455, -480.8141,\n",
            "        -474.6241, -503.2580, -472.7809, -496.1982, -468.1652, -507.5119,\n",
            "        -511.1819, -495.8654, -498.1307, -473.6902, -481.2609, -474.4228,\n",
            "        -507.5076, -473.9023, -479.8463, -473.3889, -473.7484, -496.3771,\n",
            "        -476.1889, -474.3009, -473.0494, -463.4113, -474.7519, -500.7218,\n",
            "        -496.3219, -457.6117, -485.9011, -473.2121, -495.4410, -507.1606,\n",
            "        -449.3098, -462.9806, -474.0639, -506.3081, -503.2572, -473.5285,\n",
            "        -474.0455, -495.8859, -485.4483, -492.2582, -495.9663, -472.9545,\n",
            "        -503.3434, -474.4989, -486.7436, -471.6400, -478.1321, -478.0923,\n",
            "        -473.4518, -496.2968, -477.7867, -473.7139, -472.6537, -496.3402,\n",
            "        -482.5099, -503.6566, -495.9834, -474.4762, -500.3297, -474.2024,\n",
            "        -505.3566, -458.1680, -480.9067, -457.3823, -485.0229, -505.5840,\n",
            "        -501.9109, -473.7277, -473.3908, -487.5402, -495.8024, -504.7887,\n",
            "        -481.9709, -470.9210, -495.8123, -473.3916, -480.1039, -459.2610,\n",
            "        -484.9806, -485.5481, -482.6234, -483.5602, -490.9431, -496.1751,\n",
            "        -474.3110, -499.4359, -491.7788, -504.9487, -496.1126, -474.4654,\n",
            "        -498.7686, -485.9084, -497.2993, -465.0852, -458.1845, -476.7627,\n",
            "        -474.0619, -501.0020, -510.3354, -496.3291, -483.6412, -479.1674,\n",
            "        -475.2344, -502.2072, -495.5873, -474.2218, -484.3319, -484.3203,\n",
            "        -503.6663, -484.2532, -501.8055, -472.6867, -483.6219, -503.7383,\n",
            "        -496.2537, -473.3701, -472.9574, -481.2918, -503.0712, -496.2512,\n",
            "        -495.8149, -474.2302, -504.2025, -478.3828, -478.5120, -482.4371,\n",
            "        -493.9191, -474.1043, -474.0480, -506.4082, -456.0980, -472.6649,\n",
            "        -474.0221, -503.6973], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.0701780915260315 0.4950000047683716 0.0 483.1168212890625\n",
            "tensor([-455.2307, -464.8095, -482.4994, -461.3897, -484.6682, -470.7497,\n",
            "        -480.2192, -462.7070, -467.3442, -482.2177, -462.1134, -479.6397,\n",
            "        -474.2020, -470.4217, -461.2196, -484.6683, -473.1752, -482.9821,\n",
            "        -477.5135, -445.8714, -442.4471, -462.6612, -474.7273, -467.0822,\n",
            "        -459.6863, -471.5673, -462.2830, -460.3698, -435.9984, -462.7148,\n",
            "        -462.4576, -470.6156, -486.8911, -485.0787, -468.7055, -462.1781,\n",
            "        -473.5721, -462.0627, -474.4928, -453.2409, -490.6073, -469.7524,\n",
            "        -470.8398, -480.6264, -445.5369, -482.5140, -462.1884, -462.1854,\n",
            "        -478.7280, -482.6639, -477.7881, -459.7018, -487.7388, -467.5511,\n",
            "        -485.2343, -437.6119, -468.0373, -463.9630, -469.0667, -485.4565,\n",
            "        -479.8497, -462.9907, -459.8099, -451.7006, -462.2325, -461.2460,\n",
            "        -478.6279, -461.4023, -478.3020, -467.9651, -453.2215, -491.3705,\n",
            "        -472.2858, -470.9644, -472.0650, -450.6565, -488.4605, -469.8300,\n",
            "        -461.6112, -467.0000, -479.2863, -487.3630, -441.0378, -460.4293,\n",
            "        -456.2969, -472.2520, -478.7742, -464.6101, -462.1988, -480.2068,\n",
            "        -450.7536, -452.1286, -459.1752, -462.2415, -466.3020, -462.0799,\n",
            "        -492.1565, -492.2427, -479.7105, -459.3411, -446.5916, -461.7634,\n",
            "        -462.4624, -468.7296, -441.1279, -442.9824, -462.1621, -436.8838,\n",
            "        -485.8113, -449.6039, -431.1704, -467.5092, -488.2878, -462.1865,\n",
            "        -481.7421, -462.7271, -456.1238, -459.8446, -490.5258, -444.9460,\n",
            "        -450.9139, -442.1076, -462.6284, -453.3622, -445.1193, -477.6289,\n",
            "        -446.6797, -476.2071, -462.1082, -481.5235, -483.2422, -470.3782,\n",
            "        -478.0078, -459.6553, -461.7145, -462.5023, -456.1876, -462.2196,\n",
            "        -463.1143, -449.3058, -470.0233, -460.9927, -462.2961, -482.6639,\n",
            "        -462.3447, -464.4709, -471.4012, -462.2061, -474.8756, -461.4043,\n",
            "        -457.6925, -460.4382, -462.2449, -460.8658, -462.1656, -434.9897,\n",
            "        -455.0077, -482.8906, -461.6445, -496.4157, -469.8245, -452.1547,\n",
            "        -482.3289, -460.7339, -466.0272, -462.2888, -476.4560, -471.7436,\n",
            "        -457.5216, -492.8160, -462.0842, -471.3939, -444.5296, -462.1239,\n",
            "        -462.1993, -454.2029, -468.6566, -480.2010, -482.1839, -470.6594,\n",
            "        -480.2077, -470.3241, -490.2907, -462.8278, -462.2345, -476.1898,\n",
            "        -459.1068, -446.0109, -445.6624, -454.5062, -461.9525, -486.5883,\n",
            "        -480.5468, -460.7500, -460.2829, -452.0848, -485.4621, -461.7575,\n",
            "        -487.7368, -467.1990, -455.2917, -459.8007, -462.2627, -459.4205,\n",
            "        -453.4966, -434.2657, -460.0592, -468.5219, -489.3790, -492.3017,\n",
            "        -478.9379, -462.6880, -474.1169, -462.6959, -490.0971, -460.9276,\n",
            "        -467.0825, -444.4904, -462.2324, -446.5773, -467.0970, -466.0496,\n",
            "        -458.1152, -437.3881, -482.5944, -491.0846, -462.2340, -461.1322,\n",
            "        -487.2297, -462.2339, -472.0656, -493.0721, -469.1549, -484.6802,\n",
            "        -461.1704, -474.2507, -488.5435, -458.9800, -441.7558, -485.6230,\n",
            "        -479.2280, -490.7911, -482.6763, -466.0186, -471.5124, -472.0254,\n",
            "        -460.1145, -461.2493, -491.0633, -452.5838, -472.0107, -468.7233,\n",
            "        -447.4518, -477.8582, -462.7778, -472.2879, -469.0938, -464.6420,\n",
            "        -482.7126, -461.7523, -482.7422, -456.8610, -476.3475, -459.9364,\n",
            "        -467.2794, -482.4396, -461.7555, -440.2012, -476.4143, -471.3287,\n",
            "        -462.1085, -480.9255, -461.2222, -483.3406, -480.6814, -460.7333,\n",
            "        -486.3981, -462.2116, -483.6574, -440.0970, -461.7503, -466.0565,\n",
            "        -462.2511, -464.9149, -479.3231, -482.7088, -461.3940, -483.2183],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, closs 0.06602902710437775 0.4950000047683716 0.0 467.22601318359375\n",
            "tensor([-469.2013, -449.9300, -449.1442, -469.2715, -455.0545, -429.7050,\n",
            "        -449.2025, -475.0677, -443.8339, -448.3614, -450.0174, -457.5032,\n",
            "        -449.2390, -475.1208, -442.6046, -450.3409, -422.5174, -449.7113,\n",
            "        -450.0989, -448.4880, -449.7324, -440.0248, -450.3957, -436.6839,\n",
            "        -460.0782, -439.2927, -449.2192, -447.6508, -450.3131, -448.1816,\n",
            "        -455.3249, -450.1996, -455.7990, -446.0031, -459.2265, -469.2325,\n",
            "        -433.2997, -455.0951, -450.4078, -468.3536, -447.5472, -450.2686,\n",
            "        -450.1501, -467.5956, -450.1250, -454.0405, -447.3232, -459.1075,\n",
            "        -430.9968, -451.1467, -448.3864, -435.3950, -449.7147, -441.6522,\n",
            "        -448.7946, -467.3542, -469.1852, -449.2612, -448.4023, -449.0179,\n",
            "        -450.1332, -454.2394, -474.9488, -448.3093, -452.1083, -449.6885,\n",
            "        -454.6753, -466.4131, -448.9449, -454.6294, -457.8881, -417.3157,\n",
            "        -469.9441, -442.1107, -456.6391, -448.0121, -455.8033, -454.4180,\n",
            "        -421.3222, -450.0949, -456.3473, -449.7520, -450.3690, -473.2020,\n",
            "        -444.9974, -427.4893, -453.8553, -423.0735, -459.2789, -448.9420,\n",
            "        -450.1586, -456.8179, -449.3218, -456.2019, -435.2135, -450.0471,\n",
            "        -461.0174, -449.1742, -450.3459, -475.2439, -416.7549, -454.6061,\n",
            "        -449.2429, -473.7075, -418.7916, -450.3550, -450.0656, -445.6324,\n",
            "        -449.2036, -437.7138, -442.9389, -449.1915, -447.4021, -448.3238,\n",
            "        -449.9769, -447.9772, -437.6411, -450.0577, -452.4395, -461.6714,\n",
            "        -464.4040, -449.1741, -450.0907, -448.2139, -450.4069, -450.0426,\n",
            "        -433.9098, -449.7777, -486.0066, -449.7054, -449.1784, -460.1387,\n",
            "        -449.7726, -434.8179, -450.1039, -459.9878, -469.9242, -450.0800,\n",
            "        -456.9843, -446.3683, -448.9962, -459.4621, -437.2545, -450.0392,\n",
            "        -460.8360, -440.7422, -447.2234, -469.1827, -451.1433, -448.3933,\n",
            "        -443.6484, -444.0216, -473.6974, -458.1385, -457.0374, -469.1301,\n",
            "        -448.8121, -450.0522, -437.3178, -459.3218, -455.8722, -459.1293,\n",
            "        -449.8352, -468.9472, -427.2612, -476.9272, -450.3163, -435.1980,\n",
            "        -459.8683, -450.1546, -449.1900, -455.5562, -449.2281, -449.2052,\n",
            "        -460.0432, -449.7296, -447.4573, -458.6317, -450.3555, -467.5006,\n",
            "        -450.3412, -468.9601, -450.3115, -452.0037, -449.6626, -449.0564,\n",
            "        -450.1012, -460.9073, -448.2098, -449.9166, -455.8712, -450.0234,\n",
            "        -437.9583, -449.0143, -449.0635, -427.7733, -449.7039, -457.1057,\n",
            "        -448.0135, -461.6718, -473.4868, -449.3276, -444.0013, -440.8911,\n",
            "        -441.4420, -443.6664, -457.4727, -450.1247, -437.8853, -450.4005,\n",
            "        -450.0816, -438.3538, -450.1472, -460.4917, -448.3905, -444.5768,\n",
            "        -461.1114, -450.3235, -444.9119, -455.1437, -449.2284, -474.5993,\n",
            "        -437.8086, -454.8986, -438.9881, -459.2095, -450.3416, -454.6711,\n",
            "        -449.5742, -454.2963, -456.2153, -472.3333, -457.4281, -437.6269,\n",
            "        -450.0308, -449.2346, -448.4294, -457.0128, -448.5770, -449.1826,\n",
            "        -455.1998, -454.4099, -449.1789, -470.6708, -456.1978, -457.0908,\n",
            "        -448.1799, -422.5452, -447.1117, -450.3746, -450.2155, -455.3343,\n",
            "        -454.0413, -477.3686, -434.7545, -444.2223, -471.6833, -458.4171,\n",
            "        -449.2415, -436.3381, -440.8778, -455.7610, -450.3645, -444.7069,\n",
            "        -450.9873, -450.1541, -455.0375, -455.8477, -450.1212, -458.3058,\n",
            "        -455.5771, -449.2023, -438.8929, -449.2691, -450.3464, -451.1606,\n",
            "        -440.2111, -459.1427, -449.7523, -454.3551, -454.4883, -449.6524,\n",
            "        -450.1275, -455.9411, -448.8144, -443.9637, -449.5665, -437.9146,\n",
            "        -457.1057, -449.0601, -445.9779, -455.4467, -450.1274, -460.7958,\n",
            "        -458.0709, -439.6069, -457.5210, -450.3453, -457.9163, -431.1813,\n",
            "        -457.8601, -448.0846, -420.6563, -450.3447, -469.9635, -450.6924,\n",
            "        -448.1495, -429.7998, -438.4297, -454.0140, -450.0959, -429.2604,\n",
            "        -467.3029, -449.7447, -450.1361, -467.4977, -449.2546, -453.0924,\n",
            "        -450.0484, -449.7373, -450.3867, -450.3181, -434.6687, -454.4279,\n",
            "        -450.0007, -459.3075, -450.3379, -443.6528, -449.7226, -450.3370,\n",
            "        -457.7247, -469.1368, -450.0656, -445.1136, -454.3936, -449.8041,\n",
            "        -473.7436, -450.0096, -448.8257, -455.1580, -450.1478, -427.2552,\n",
            "        -450.1324, -462.2894, -449.7522, -456.0585, -426.0188, -454.3716,\n",
            "        -455.0528, -452.2084, -471.4766, -437.3453, -459.4638, -417.8018,\n",
            "        -450.0953, -457.5006, -448.1673, -436.9008, -459.1350, -425.7794,\n",
            "        -440.9323, -449.2326, -442.0345, -450.3652, -449.7796, -441.8370,\n",
            "        -444.7325, -449.7021, -424.7082, -450.0231, -450.1084, -469.2939,\n",
            "        -453.8529, -456.7089, -449.5853, -454.0888, -450.2597, -448.5267,\n",
            "        -449.2941, -430.1866, -436.8676, -463.2234, -445.9095, -448.8122,\n",
            "        -443.8378, -448.8830, -443.1031, -467.6115, -450.3373, -451.0884,\n",
            "        -454.0599, -458.6803, -449.7038, -449.3069, -448.9229, -453.7979,\n",
            "        -434.8982, -458.0823, -419.0269, -449.9924, -472.1458, -448.0219,\n",
            "        -449.6167, -468.9675, -450.3429, -443.9365, -450.1312, -473.7463,\n",
            "        -449.3023, -439.2614, -444.9571, -460.9667, -450.0784, -461.1188,\n",
            "        -431.3352, -458.0359, -458.5434, -450.3447, -449.7030, -444.7553,\n",
            "        -449.7949, -454.2578, -449.8618, -422.8797, -447.9776, -450.3483,\n",
            "        -414.4265, -453.3483, -448.9863, -435.6986, -470.4280, -435.3175,\n",
            "        -450.6149, -449.1884, -457.8785, -469.3813, -439.8033, -450.1349,\n",
            "        -450.1408, -439.2462, -455.2125, -456.7241, -450.4156, -474.7640,\n",
            "        -449.2049, -449.4100, -472.3966, -448.4728, -449.3186, -450.0612,\n",
            "        -450.1467, -468.9072, -456.4842, -448.2720, -446.0233, -469.4263,\n",
            "        -450.0324, -447.9746, -450.3192, -455.1694, -449.9860, -450.3500,\n",
            "        -450.9639, -449.6936, -460.9695, -459.1908, -450.1971, -473.7004,\n",
            "        -448.8141, -446.6777, -446.5896, -415.2620, -449.0398, -416.8703,\n",
            "        -448.8600, -433.3138, -450.1244, -444.8114, -470.0857, -455.0530,\n",
            "        -450.6331, -450.1263, -448.4311, -449.3187, -450.0915, -455.4795,\n",
            "        -449.6029, -455.4301, -448.1771, -448.0371, -450.0578, -449.9249,\n",
            "        -449.9493, -459.1153, -443.3904, -416.8014, -456.7122, -450.0930,\n",
            "        -449.6420, -458.9649, -450.1264, -455.2064, -459.2176, -470.3193,\n",
            "        -449.6948, -449.6210, -449.5919, -447.4790, -455.1964, -434.6281,\n",
            "        -423.0482, -450.0887], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.06110428273677826 0.4950000047683716 0.0 450.58502197265625\n",
            "tensor([-433.7662, -439.0930, -440.5425, -448.8196, -439.3826, -408.0870,\n",
            "        -439.4261, -429.6095, -440.1703, -440.2363, -447.5508, -443.1595,\n",
            "        -435.8174, -460.5932, -430.8668, -440.1951, -453.6004, -436.8665,\n",
            "        -447.5510, -455.9443, -406.3834, -423.3867, -439.0468, -399.5791,\n",
            "        -438.9839, -440.1469, -444.3235, -434.5205, -428.5300, -449.2117,\n",
            "        -439.1312, -445.3893, -450.2439, -430.8164, -440.1730, -408.3302,\n",
            "        -440.4263, -427.6259, -434.6477, -424.4914, -438.8181, -442.7492,\n",
            "        -444.2094, -454.9787, -435.5457, -437.7957, -457.5602, -431.4318,\n",
            "        -409.3982, -404.7823, -438.8028, -448.4737, -439.4818, -416.7049,\n",
            "        -445.6992, -400.4350, -433.6037, -439.0567, -440.2088, -436.8649,\n",
            "        -439.9486, -459.6143, -450.9505, -439.3694, -423.2851, -439.9540,\n",
            "        -445.4395, -455.2208, -442.6833, -425.9548, -439.1834, -397.6882,\n",
            "        -438.9685, -440.1470, -439.8461, -422.8771, -447.4553, -433.4899,\n",
            "        -456.2605, -437.6769, -414.4990, -436.6076, -440.1908, -456.1872,\n",
            "        -440.1057, -445.1647, -438.8415, -458.6127, -440.0038, -439.3827,\n",
            "        -420.8745, -435.7021, -433.0954, -433.5224, -418.8846, -438.8801,\n",
            "        -416.5617, -438.8154, -441.9845, -455.1728, -439.3462, -440.2479,\n",
            "        -438.8800, -423.8553, -437.6854, -441.3007, -439.9432, -449.8913,\n",
            "        -440.1937, -432.3373, -427.1667, -439.3798, -455.2463, -440.2793,\n",
            "        -437.0379, -428.8030, -405.0446, -441.9851, -439.3615, -405.6440,\n",
            "        -440.1969, -444.2176, -439.9844, -426.4566, -436.8303, -447.8520,\n",
            "        -455.9392, -439.3275, -416.4088, -439.3348, -439.3292, -452.8366,\n",
            "        -445.7768, -440.1155, -435.5870, -456.0874, -447.7522, -439.1322,\n",
            "        -439.8218, -428.7609, -440.2029, -415.8673, -448.7505, -440.1934,\n",
            "        -407.8448, -439.9830, -439.3011, -437.0441, -439.2688, -432.7626,\n",
            "        -437.7526, -408.7657, -420.9543, -437.9731, -439.9670, -440.2852,\n",
            "        -438.8172, -433.4816, -455.5423, -440.4055, -418.1040, -438.1941,\n",
            "        -429.8350, -402.2101, -438.8677, -444.7134, -438.9657, -445.5524,\n",
            "        -433.7943, -439.3496, -440.2944, -445.7700, -439.9744, -434.5973,\n",
            "        -413.6513, -440.0782, -431.6387, -437.6477, -439.3055, -455.2189,\n",
            "        -438.8737, -449.0356, -438.8272, -455.7097, -439.3474, -445.6180,\n",
            "        -437.7043, -439.8472, -440.0999, -435.0650, -434.3709, -437.6451,\n",
            "        -441.5943, -439.3509, -440.3106, -440.0603, -444.3210, -447.3445,\n",
            "        -440.1539, -425.3537, -428.5854, -447.6017, -441.8709, -442.8252,\n",
            "        -443.9727, -440.1461, -451.7955, -439.4347, -415.6147, -439.3240,\n",
            "        -439.3843, -452.7130, -439.9648, -445.9125, -439.3713, -451.5190,\n",
            "        -438.8741, -447.4450, -439.3938, -435.1538, -446.0755, -460.5875,\n",
            "        -432.5739, -440.1428, -412.3127, -437.9695, -422.0703, -441.9400,\n",
            "        -438.0172, -443.2077, -422.3074, -455.8368, -447.4911, -447.5291,\n",
            "        -439.0914, -428.1223, -439.1260, -457.7715, -403.1954, -438.1963,\n",
            "        -455.5601, -400.9496, -435.0336, -452.2395, -440.0643, -447.6012,\n",
            "        -441.8658, -424.2398, -441.2821, -440.1375, -447.2747, -433.9012,\n",
            "        -447.6203, -450.4874, -442.1865, -440.0928, -429.0872, -437.3375,\n",
            "        -447.3789, -457.8465, -440.1975, -432.4545, -440.1933, -451.0385,\n",
            "        -439.2538, -440.2837, -440.4388, -437.9737, -439.2879, -421.6328,\n",
            "        -460.8088, -441.3228, -444.7524, -440.1331, -440.2915, -453.9446,\n",
            "        -445.8579, -455.1678, -439.3704, -404.2476, -434.9282, -440.2338,\n",
            "        -439.5438, -435.3539, -439.6808, -444.4394, -441.5529, -438.9165],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.058515869081020355 0.4950000047683716 0.0 437.8828430175781\n",
            "#### simulate ####\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.115567163784817e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.115567163784817e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -3.115567163784817e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -4.286743018689292e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.115567163784817e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.115567163784817e+23\n",
            "search -3.115567163784817e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.115567163784817e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.115567163784817e+23\n",
            "search -3.115567163784817e+23\n",
            "search -5.932598274318618e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.115567163784817e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.115567163784817e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -4.286743018689292e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.115567163784817e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -3.8642916826990725e+23\n",
            "search -4.286743018689292e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "search -5.932598274318618e+23\n",
            "ded\n",
            "time\n",
            "8 #### train ####\n",
            "tensor([-426.5141, -401.9376, -427.8431, -427.5237, -436.2326, -427.8564,\n",
            "        -432.0470, -434.5033, -422.7514, -429.0486, -401.1035, -442.9884,\n",
            "        -423.1085, -429.4467, -432.0191, -438.1558, -425.2267, -426.4993,\n",
            "        -428.8036, -436.4621, -423.2122, -428.7768, -419.5305, -426.1251,\n",
            "        -434.1770, -427.2597, -441.6532, -419.2484, -439.3938, -435.9138,\n",
            "        -389.2007, -385.6783, -440.8132, -421.8591, -412.2393, -427.2976,\n",
            "        -431.2942, -429.0699, -393.4390, -434.8652, -435.8481, -429.6592,\n",
            "        -446.6964, -443.4813, -403.8885, -409.2192, -408.2018, -441.2923,\n",
            "        -427.2247, -423.9302, -426.6385, -427.2499, -443.6345, -428.9961,\n",
            "        -421.3463, -428.8466, -416.3553, -429.3789, -404.9608, -440.8269,\n",
            "        -426.1038, -428.7447, -396.6532, -440.7659, -434.2636, -415.9276,\n",
            "        -422.1659, -429.0492, -429.3924, -428.9677, -436.0087, -385.8446,\n",
            "        -443.0230, -427.6293, -409.3754, -423.5082, -442.7235, -428.7751,\n",
            "        -419.3458, -399.2125, -416.8333, -422.1768, -437.0400, -428.8921,\n",
            "        -412.0076, -428.4923, -435.3504, -419.8986, -439.3891, -428.0184,\n",
            "        -417.0073, -424.0576, -429.9566, -425.2478, -445.7806, -441.4361,\n",
            "        -434.0968, -413.7525, -432.9806, -428.1966, -404.1401, -394.0380,\n",
            "        -439.5859, -428.7709, -433.9306, -428.3650, -390.1693, -404.5652,\n",
            "        -392.0873, -422.1412, -386.7264, -434.6150, -415.0149, -425.3018,\n",
            "        -423.4064, -428.9955, -415.0202, -428.9514, -431.1899, -440.3612,\n",
            "        -439.2569, -436.3066, -441.3086, -411.5726, -440.5260, -421.9320,\n",
            "        -436.0112, -395.4468, -435.7955, -424.8585, -428.8851, -428.8335,\n",
            "        -433.9918, -408.0560, -422.9876, -443.0180, -407.5902, -427.7205,\n",
            "        -415.4278, -423.2585, -418.9604, -436.3383, -421.0211, -420.7738,\n",
            "        -428.6570, -441.1333, -427.9817, -427.2672, -436.6562, -409.2491,\n",
            "        -407.0551, -443.5852, -436.0976, -436.3537, -446.6688, -433.7202,\n",
            "        -424.8547, -427.5021, -415.8760, -425.8280, -428.6939, -406.6035,\n",
            "        -429.0059, -410.6713, -430.3573, -427.5414, -427.8116, -436.1454,\n",
            "        -416.2373, -428.3100, -436.3326, -424.3448, -418.3650, -436.2716,\n",
            "        -402.3247, -410.2901, -445.7604, -384.6827, -432.4642, -416.4918,\n",
            "        -439.7272, -427.2494, -429.2894, -421.2440, -439.4044, -428.9771,\n",
            "        -436.0972, -416.9202, -433.0570, -426.0815, -411.8590, -400.3763,\n",
            "        -427.9000, -400.0583, -427.5257, -428.9437, -435.8981, -432.9741,\n",
            "        -420.6537, -413.0684, -415.5438, -429.1949, -437.0320, -383.4035,\n",
            "        -420.1242, -429.0920, -401.8302, -408.0889, -428.6431, -418.9665,\n",
            "        -433.9996, -427.3207, -433.8692, -402.8513, -420.2752, -443.0023,\n",
            "        -417.7502, -425.0094, -383.2917, -381.2250, -435.1368, -425.2508,\n",
            "        -383.7591, -426.4225, -429.0383, -444.5809, -441.3332, -428.9894,\n",
            "        -428.9517, -427.2244, -416.2204, -411.2840, -420.0769, -436.3518,\n",
            "        -445.4502, -426.5049, -434.0312, -428.7962, -401.1347, -397.5974,\n",
            "        -403.4280, -423.7963, -433.6204, -433.5409, -434.3319, -407.9873,\n",
            "        -421.7792, -435.4577, -445.9101, -424.6382, -413.0794, -408.2521,\n",
            "        -441.6514, -429.3641, -381.2475, -444.6781, -427.9383, -417.1255,\n",
            "        -429.3688, -428.6414, -419.4332, -426.4825, -431.6439, -418.3407,\n",
            "        -432.6495, -434.1634, -446.0952, -397.5719, -441.4394, -410.8814,\n",
            "        -422.1465, -406.1570, -402.7826, -440.8245, -428.1732, -428.1976,\n",
            "        -432.4052, -429.2893, -400.2088, -422.6327, -429.3876, -428.3812,\n",
            "        -421.4107, -411.4082, -432.7792, -427.3188, -425.5809, -423.5153,\n",
            "        -426.4494, -414.2006, -428.5538, -428.5206, -390.6772, -425.0016,\n",
            "        -434.7932, -430.4627, -429.6791, -429.0833, -433.9245, -402.0714,\n",
            "        -414.9693, -429.0555, -402.5593, -400.6850, -428.9906, -391.7009,\n",
            "        -431.8953, -429.2570, -442.1207, -435.6834, -416.1368, -435.0561,\n",
            "        -429.2698, -429.0364, -425.3547, -380.7695, -436.0605, -429.3682,\n",
            "        -408.0184, -436.3084, -429.2487, -398.8914, -430.3912, -427.6504,\n",
            "        -436.7224, -434.1855, -420.8521, -440.8431, -426.0829, -434.1953,\n",
            "        -408.5200, -411.8278, -403.1298, -433.9542, -401.3218, -406.3095,\n",
            "        -428.2075, -446.5688, -430.3745, -407.5972, -408.1496, -413.8376,\n",
            "        -434.5926, -425.2873, -401.4655, -423.1744, -408.7414, -441.3429,\n",
            "        -436.2799, -427.3032, -406.5197, -437.5964, -427.3152, -444.4271,\n",
            "        -425.8648, -428.7474, -428.9792, -429.1345, -419.7493, -429.4153,\n",
            "        -425.0905, -429.0334, -446.0882, -396.0720, -428.8676, -427.5396,\n",
            "        -416.0588, -435.0905, -428.7824, -385.5072, -434.5948, -421.4604,\n",
            "        -409.1422, -428.5474, -439.4265, -415.3673, -385.6548, -428.9499,\n",
            "        -436.3399, -391.6214, -437.6196, -427.8425, -421.4698, -404.6954,\n",
            "        -424.1705, -409.8156, -428.9461, -428.8436, -434.5318, -426.0689,\n",
            "        -429.3369, -450.1312, -421.8598, -428.7892, -443.1698, -422.4838,\n",
            "        -420.9903, -427.1172, -402.7855, -408.9780, -421.0500, -421.9934,\n",
            "        -427.2635, -407.9547, -439.3657, -429.2647, -434.5498, -409.3878,\n",
            "        -402.4040, -427.9268, -395.7232, -403.0445, -416.3800, -436.3357,\n",
            "        -446.1270, -418.0696, -429.3719, -444.8021, -421.4100, -429.0352,\n",
            "        -400.0448, -436.3134, -433.3908, -407.1291, -430.7877, -429.2965,\n",
            "        -437.1355, -411.4792, -429.1519, -425.1205, -443.2169, -390.8193,\n",
            "        -428.5389, -401.7374, -428.0460, -426.6562, -444.7546, -433.8723,\n",
            "        -443.0092, -429.2225, -414.6074, -428.7051, -400.1107, -395.7833,\n",
            "        -400.7982, -428.7646, -425.9491, -424.0074, -427.5460, -391.1678,\n",
            "        -429.1914, -420.5410, -434.5435, -433.8425, -407.6979, -430.1732,\n",
            "        -435.6047, -427.5235, -440.8315, -433.2155, -434.9922, -428.7398,\n",
            "        -424.0752, -421.5760, -440.8248, -441.2112, -428.5613, -427.5070,\n",
            "        -408.6416, -424.5718, -414.8081, -404.8328, -398.5966, -429.4403,\n",
            "        -381.6083, -388.7004, -424.8582, -425.8585, -385.4703, -429.6205,\n",
            "        -427.5231, -394.7064, -433.7096, -427.9644, -434.0654, -422.7612,\n",
            "        -441.7497, -422.2243, -438.6704, -414.5933, -441.2772, -409.6932,\n",
            "        -428.7328, -428.7344, -423.7650, -436.6133, -421.3838, -418.6553,\n",
            "        -426.2500, -428.7719, -394.2747, -428.5487, -402.5093, -436.2890,\n",
            "        -433.3970, -412.2713, -412.1333, -443.5693, -438.2635, -429.4000,\n",
            "        -421.8802, -421.4060], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.054663967341184616 0.4950000047683716 0.0 423.616943359375\n",
            "tensor([-433.5578, -395.4200, -414.2873, -427.9417, -408.0645, -415.7966,\n",
            "        -439.1213, -429.9502, -416.9100, -431.1193, -415.5141, -410.6183,\n",
            "        -425.6971, -424.8781, -381.9969, -437.5669, -436.9044, -441.0934,\n",
            "        -437.5473, -425.1599, -431.3918, -431.9490, -437.6060, -411.7570,\n",
            "        -413.9316, -420.7825, -398.2536, -406.5055, -385.4022, -418.7122,\n",
            "        -372.9349, -430.0519, -429.1204, -416.6105, -420.2280, -420.5729,\n",
            "        -436.7508, -425.4590, -443.2759, -429.7869, -430.9200, -424.9205,\n",
            "        -437.7668, -376.7905, -438.9886, -425.7426, -409.6684, -421.9839,\n",
            "        -425.5278, -439.9851, -425.2437, -425.0196, -423.1708, -431.1047,\n",
            "        -424.0786, -427.3463, -444.4463, -423.2585, -398.7645, -418.2518,\n",
            "        -419.4070, -424.7178, -437.5671, -388.8040, -424.5069, -434.2910,\n",
            "        -434.0858, -425.7408, -415.2797, -425.7319, -414.4807, -435.8732,\n",
            "        -430.2733, -425.7455, -437.6632, -393.3944, -413.3051, -433.7867,\n",
            "        -376.5141, -409.2847, -412.6238, -438.9153, -417.1808, -424.1616,\n",
            "        -414.6183, -433.7781, -418.3768, -406.3743, -431.2635, -423.7617,\n",
            "        -406.2631, -409.1581, -429.2184, -424.0031, -398.8486, -434.3103,\n",
            "        -435.9003, -408.3981, -423.2853, -423.9525, -409.8195, -423.2687,\n",
            "        -415.8061, -433.6750, -444.4117, -424.8727, -436.6221, -409.6647,\n",
            "        -406.8071, -425.5182, -434.2894, -408.3594, -433.8178, -438.9878,\n",
            "        -441.4955, -415.1815, -400.8123, -424.8399, -414.0873, -405.0740,\n",
            "        -432.7268, -403.2607, -403.4263, -403.2891, -432.9913, -418.1869,\n",
            "        -437.4992, -418.9005, -429.8929, -436.4245, -424.8741, -408.9055,\n",
            "        -393.9120, -422.6249, -404.1564, -422.7992, -397.8061, -424.0529,\n",
            "        -437.9336, -437.5622, -425.1985, -422.3901, -377.8442, -438.9442,\n",
            "        -425.4802, -437.5383, -392.2853, -424.9777, -428.1665, -423.9919,\n",
            "        -400.8315, -432.4374, -426.3508, -423.7708, -403.7145, -383.5173,\n",
            "        -431.9991, -425.6112, -436.1837, -428.7651, -425.5681, -402.6301,\n",
            "        -425.2845, -420.4301, -433.2296, -424.3806, -430.6935, -418.5726,\n",
            "        -436.8649, -433.7928, -409.4220, -395.1993, -437.0985, -425.2515,\n",
            "        -389.4020, -398.5969, -433.8719, -403.4410, -429.7639, -423.6065,\n",
            "        -452.5861, -423.9624, -413.9278, -413.1825, -436.8465, -424.2746,\n",
            "        -434.3005, -437.5805, -418.9844, -433.4682, -393.1951, -437.1894,\n",
            "        -433.8770, -413.9683, -425.1205, -424.8225, -397.4468, -425.7001,\n",
            "        -440.5863, -406.2622, -437.6506, -425.2766, -437.5781, -414.8286,\n",
            "        -416.3414, -411.5042, -437.0619, -437.0793, -424.7279, -437.5565,\n",
            "        -423.1682, -422.6314, -431.4358, -403.5921, -437.1904, -406.3106,\n",
            "        -436.6761, -423.0082, -425.0852, -441.6586, -393.8029, -424.7882,\n",
            "        -422.1159, -430.8263, -425.2508, -401.7354, -437.6167, -422.3228,\n",
            "        -408.6836, -414.7758, -427.2498, -429.7688, -399.4827, -424.9493,\n",
            "        -429.7153, -422.0828, -392.3821, -422.3951, -375.9157, -437.5656,\n",
            "        -408.6322, -409.5548, -420.5890, -432.3980, -423.6216, -424.8217,\n",
            "        -425.6740, -430.8683, -424.2924, -423.8354, -409.4969, -402.6637,\n",
            "        -444.3759, -423.1708, -388.5125, -437.6002, -425.1873, -416.6253,\n",
            "        -437.6396, -425.1535, -425.7440, -423.9377, -444.4898, -440.1844,\n",
            "        -414.1948, -414.1943, -418.4735, -426.9974, -414.2207, -421.0809,\n",
            "        -440.1790, -434.2744, -420.2724, -421.8478, -427.5135, -420.1850,\n",
            "        -435.8977, -425.2595, -417.6383, -425.5138, -420.1906, -420.9845,\n",
            "        -436.1920, -401.8497, -431.3911, -433.8171, -389.5531, -387.8459],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.05369376018643379 0.4950000047683716 0.0 421.4288024902344\n",
            "tensor([-419.8349, -404.4970, -418.8667, -418.5042, -421.1187, -420.5301,\n",
            "        -428.7741, -419.0155, -432.8221, -430.1141, -424.2898, -420.3968,\n",
            "        -420.7169, -421.4997, -408.4882, -421.2610, -374.6805, -416.7306,\n",
            "        -420.1357, -417.8957, -425.5142, -419.0855, -415.6176, -419.9688,\n",
            "        -396.7113, -409.7194, -421.0310, -418.5104, -425.6185, -428.9194,\n",
            "        -408.3007, -418.9588, -421.0231, -432.8732, -430.2179, -420.2406,\n",
            "        -416.1509, -419.1006, -402.4178, -382.3955, -408.8017, -409.1547,\n",
            "        -425.1001, -431.1857, -419.7227, -413.7101, -394.0514, -413.5980,\n",
            "        -420.7934, -432.0783, -407.8237, -419.7232, -430.1032, -423.8818,\n",
            "        -427.0744, -421.1479, -424.4928, -401.8041, -420.4240, -420.4373,\n",
            "        -412.8370, -427.3041, -422.5738, -419.8112, -420.3480, -398.8046,\n",
            "        -418.9899, -399.2881, -420.3855, -424.8725, -423.1439, -416.4972,\n",
            "        -421.7241, -429.0860, -420.2449, -420.7604, -420.6215, -427.3048,\n",
            "        -380.2915, -420.4172, -404.3669, -420.2343, -420.2405, -421.5227,\n",
            "        -411.1008, -418.5174, -427.2898, -421.4800, -424.4496, -429.1815,\n",
            "        -415.4325, -419.7935, -425.3588, -420.6864, -436.5265, -420.7380,\n",
            "        -421.2215, -394.4493, -421.2433, -421.5181, -430.1072, -421.5228,\n",
            "        -428.6552, -417.5545, -434.7069, -400.6040, -421.4910, -419.7173,\n",
            "        -417.2730, -417.8753, -408.6967, -414.5786, -418.0717, -418.9734,\n",
            "        -415.6455, -420.1264, -417.6543, -397.8163, -420.7167, -421.4903,\n",
            "        -428.8164, -421.2357, -427.1080, -406.4536, -417.5546, -421.3168,\n",
            "        -367.8343, -415.5461, -420.6208, -427.2695, -415.5334, -416.9994,\n",
            "        -417.4354, -421.1209, -421.7907, -404.0858, -379.1672, -424.2574,\n",
            "        -418.1300, -419.7627, -420.4043, -423.6077, -397.0695, -420.1107,\n",
            "        -365.3862, -402.5360, -430.1086, -419.0457, -420.3997, -376.1205,\n",
            "        -418.4228, -416.0569, -432.3500, -433.8582, -404.4504, -430.2123,\n",
            "        -418.9767, -418.9568, -413.8787, -418.6038, -404.6819, -426.4270,\n",
            "        -427.1053, -419.7825, -419.6809, -418.6897, -407.7725, -417.4970,\n",
            "        -433.5052, -412.5311, -418.4827, -430.2097, -419.4533, -407.3338,\n",
            "        -369.5036, -430.1088, -432.7085, -425.2779, -415.3187, -420.6573,\n",
            "        -420.3648, -419.8373, -411.3115, -406.8693, -404.9219, -404.8436,\n",
            "        -414.6434, -403.2036, -420.4331, -430.7643, -417.3548, -409.0331,\n",
            "        -425.7359, -427.3403, -418.9985, -412.8768, -400.5124, -417.4332,\n",
            "        -442.6260, -418.5013, -396.9744, -429.7417, -421.5179, -392.1677,\n",
            "        -418.1609, -424.6842, -382.0779, -398.1172, -403.0988, -418.6708,\n",
            "        -420.2900, -420.6870, -421.2016, -420.7398, -383.4750, -427.1073,\n",
            "        -376.3307, -431.0859, -421.4810, -421.3233, -418.9547, -427.3612,\n",
            "        -397.6542, -419.4276, -420.1303, -377.0824, -409.1521, -420.7474,\n",
            "        -420.4226, -404.5429, -427.6231, -418.0335, -406.2814, -383.7461,\n",
            "        -419.7303, -420.3089, -416.4113, -391.9016, -389.7360, -420.2584,\n",
            "        -418.4659, -437.6476, -427.1087, -421.1433, -416.4960, -374.0664,\n",
            "        -427.6005, -423.6765, -399.8455, -430.7701, -419.0131, -403.9144,\n",
            "        -419.7298, -397.0375, -385.3963, -416.4490, -433.4311, -417.8029,\n",
            "        -428.1417, -382.3215, -408.8832, -419.6989, -424.6444, -412.2182,\n",
            "        -408.7337, -411.7578, -424.8789, -421.5099, -416.1963, -419.0542,\n",
            "        -374.9145, -414.9707, -419.6790, -411.4387, -418.3360, -421.5264,\n",
            "        -420.5378, -420.1747, -408.5109, -420.6358, -403.2567, -412.2140,\n",
            "        -419.8234, -430.1473, -416.3570, -421.2186, -397.8984, -420.1725,\n",
            "        -425.4570, -420.0651, -418.9777, -419.6713, -420.2431, -411.2785,\n",
            "        -407.0374, -418.4996, -402.8545, -424.0027, -419.6696, -403.7895,\n",
            "        -418.6038, -410.8156, -432.2953, -420.5480, -419.0983, -419.5382,\n",
            "        -427.8201, -387.9654, -426.6583, -421.0663, -417.3817, -420.1673,\n",
            "        -423.2407, -421.7459, -415.7296, -421.4657, -424.8822, -381.7266,\n",
            "        -383.3104, -409.5915, -420.2785, -415.7929, -418.9932, -419.6431,\n",
            "        -417.4330, -419.0555, -433.2505, -418.1683, -393.8797, -407.4815,\n",
            "        -416.0574, -403.1794, -420.3318, -414.8542, -373.4750, -420.1681,\n",
            "        -414.5184, -397.1009, -421.0139, -425.3158, -417.4458, -419.7594,\n",
            "        -377.8125, -429.6257, -387.5070, -432.6245, -420.2070, -432.4146,\n",
            "        -419.5276, -391.8043, -370.6804, -424.9711, -420.9734, -448.6419,\n",
            "        -373.2115, -409.6320, -420.6939, -409.2852, -421.1039, -420.2895,\n",
            "        -387.9893, -426.6724, -425.3081, -418.3383, -407.8107, -411.4205,\n",
            "        -407.1631, -421.1806, -390.8914, -433.0478, -409.3028, -425.2055,\n",
            "        -418.9739, -420.4439, -430.7638, -403.7394, -433.4862, -434.0168,\n",
            "        -426.3191, -418.1700, -420.3356, -405.0051, -417.5791, -433.4632,\n",
            "        -429.6273, -389.9595, -418.4867, -421.5027, -421.3712, -413.8100,\n",
            "        -392.6773, -428.7244, -418.9895, -420.1855, -421.2131, -429.5220,\n",
            "        -421.3530, -414.3904, -435.2010, -424.4912, -390.0292, -433.8095,\n",
            "        -419.7657, -420.4271, -418.1658, -418.5010, -376.4585, -422.9181,\n",
            "        -433.4707, -389.6115, -416.7944, -420.6341, -421.4941, -421.2759,\n",
            "        -417.9968, -411.3746, -418.0364, -432.3413, -421.4799, -420.2867,\n",
            "        -427.1086, -419.6638, -440.5406, -420.1891, -409.7596, -424.0684,\n",
            "        -411.6411, -421.4771, -420.2426, -396.0044, -385.5807, -432.4670,\n",
            "        -407.9837, -434.1911, -422.9660, -420.3707, -430.1920, -420.7080,\n",
            "        -408.5752, -425.1202, -423.2492, -429.0917, -416.4714, -405.6590,\n",
            "        -418.9979, -399.3586, -367.1206, -420.7441, -413.7855, -401.8162,\n",
            "        -419.6494, -420.6610, -428.1404, -429.6637, -417.4182, -424.7089,\n",
            "        -432.3495, -430.1848, -376.4731, -424.6010, -419.8022, -432.7224,\n",
            "        -383.2472, -418.8940, -424.5156, -416.4622, -408.9536, -408.6916,\n",
            "        -404.3990, -430.2968, -426.3087, -421.2893, -406.5378, -409.3711,\n",
            "        -417.4288, -404.6063, -416.3551, -418.6906, -372.1527, -420.7981,\n",
            "        -421.4830, -434.9667, -418.4813, -419.6893, -430.1972, -420.1780,\n",
            "        -415.5616, -419.0501, -433.5139, -433.0378, -417.5111, -419.6958,\n",
            "        -425.1104, -404.9404, -412.8326, -420.2433, -418.0396, -408.1278,\n",
            "        -420.6755, -417.5206, -410.1966, -411.3405, -413.5860, -400.2713,\n",
            "        -404.3395, -433.3463, -420.6155, -399.0986, -420.3974, -408.3675,\n",
            "        -396.7838, -406.5160], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.05171438679099083 0.4950000047683716 0.0 415.81256103515625\n",
            "tensor([-414.0549, -404.0625, -415.4906, -419.3814, -425.7615, -414.0216,\n",
            "        -423.3354, -415.3164, -387.7201, -408.9749, -415.4660, -409.3567,\n",
            "        -415.5378, -416.6650, -386.8039, -392.7603, -403.8011, -402.6132,\n",
            "        -405.4963, -416.4351, -416.1191, -416.5568, -381.6550, -414.0669,\n",
            "        -424.0877, -423.2341, -413.9807, -413.6155, -415.1613, -394.7157,\n",
            "        -390.1097, -414.0445, -414.0016, -428.2785, -422.8600, -412.9315,\n",
            "        -404.0226, -425.3379, -419.2900, -385.8392, -400.5939, -434.1674,\n",
            "        -413.6171, -401.0872, -415.2602, -397.2987, -399.1555, -408.4169,\n",
            "        -414.0361, -406.7913, -391.2708, -425.6911, -415.4291, -413.5547,\n",
            "        -414.8400, -410.8225, -424.0130, -423.3546, -425.6911, -409.5009,\n",
            "        -415.4229, -430.2398, -419.2335, -408.4078, -415.4866, -383.5032,\n",
            "        -416.5757, -416.4162, -415.7652, -415.7323, -402.7703, -415.5266,\n",
            "        -427.4262, -376.9815, -415.7568, -416.1910, -416.2317, -421.9935,\n",
            "        -371.1689, -425.2181, -404.0630, -415.2180, -404.1773, -415.5492,\n",
            "        -413.0507, -412.2206, -381.9730, -411.6105, -405.5395, -396.8517,\n",
            "        -411.4449, -415.9786, -393.8091, -427.9773, -390.1786, -428.1642,\n",
            "        -424.7010, -407.8694, -413.0470, -374.4508, -416.4016, -411.2495,\n",
            "        -391.1670, -412.9986, -400.6652, -367.6871, -416.2332, -406.9129,\n",
            "        -415.3902, -416.3124, -406.3389, -414.1895, -416.4203, -397.1017,\n",
            "        -415.3944, -422.1433, -415.8696, -412.9890, -398.4279, -414.1145,\n",
            "        -386.6543, -401.2034, -425.6725, -365.5630, -416.0584, -442.1759,\n",
            "        -391.5907, -415.4508, -414.0627, -414.1326, -416.5022, -415.6523,\n",
            "        -424.2769, -407.8705, -397.2074, -415.4431, -428.7623, -397.5147,\n",
            "        -412.9308, -423.5826, -424.8264, -427.8926, -397.4747, -414.0029,\n",
            "        -415.5607, -384.4236, -412.8737, -415.7019, -415.4172, -411.2652,\n",
            "        -408.0088, -417.7631, -413.1769, -404.2477, -415.8482, -425.7582,\n",
            "        -415.8416, -437.9606, -400.2323, -416.1197, -415.8239, -434.6371,\n",
            "        -414.1144, -377.2649, -418.8932, -415.6452, -407.8376, -413.1628,\n",
            "        -396.8956, -398.3808, -415.5063, -415.1204, -416.2606, -423.2294,\n",
            "        -390.1337, -416.0638, -425.6823, -381.4179, -413.6191, -424.8188,\n",
            "        -413.0495, -416.3925, -409.3674, -421.1312, -424.4628, -413.5687,\n",
            "        -411.5245, -415.8551, -420.0181, -409.6725, -377.9260, -427.2968,\n",
            "        -428.4417, -428.1560, -415.8062, -415.8544, -399.2128, -413.6179,\n",
            "        -378.3457, -394.3945, -386.2410, -424.9718, -370.2884, -428.3105,\n",
            "        -415.1057, -412.8901, -401.0790, -420.3972, -424.1359, -412.9445,\n",
            "        -414.1109, -414.1219, -417.8729, -415.2502, -406.5308, -410.6219,\n",
            "        -428.2442, -429.0974, -419.3246, -415.7805, -415.4388, -430.6054,\n",
            "        -386.8234, -415.5732, -427.7304, -384.6028, -415.9041, -412.8741,\n",
            "        -415.0196, -425.8380, -423.6792, -422.1467, -396.7719, -427.2635,\n",
            "        -415.6760, -414.8865, -415.8554, -425.7592, -381.9758, -413.1130,\n",
            "        -422.8251, -428.6789, -401.4458, -393.1837, -410.8096, -414.2761,\n",
            "        -409.3709, -397.3474, -391.5996, -384.7737, -415.3007, -415.7417,\n",
            "        -402.6359, -441.3726, -396.1332, -419.0052, -401.1248, -408.5265,\n",
            "        -398.2789, -416.1151, -423.5865, -415.5385, -401.9295, -419.0203,\n",
            "        -385.5869, -422.1124, -406.0806, -413.2070, -402.8239, -429.0204,\n",
            "        -422.0727, -413.5485, -417.0842, -397.2976, -415.3198, -415.7411,\n",
            "        -404.1720, -415.6110, -428.7294, -415.5371, -406.6088, -400.3478,\n",
            "        -410.8251, -420.0835, -425.6724, -415.7820, -387.7388, -419.2901],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.05167815834283829 0.4950000047683716 0.0 411.1026306152344\n",
            "tensor([-387.0432, -405.9117, -397.5988, -376.4701, -413.3783, -411.3086,\n",
            "        -414.8706, -419.2507, -403.9152, -384.8174, -383.1387, -419.3965,\n",
            "        -422.4263, -398.6357, -410.1828, -416.1055, -421.8640, -401.8414,\n",
            "        -409.8888, -399.5332, -410.2173, -417.3124, -412.5929, -408.3171,\n",
            "        -395.7563, -423.7982, -407.1730, -420.1061, -409.1331, -407.4622,\n",
            "        -410.2528, -414.8563, -386.4303, -428.2913, -407.5316, -406.5803,\n",
            "        -408.7846, -408.5816, -414.3530, -411.9506, -417.7325, -365.4044,\n",
            "        -406.5420, -386.6967, -422.0652, -406.3588, -373.1175, -422.1891,\n",
            "        -420.6401, -411.2589, -410.3310, -414.0373, -410.2499, -408.6676,\n",
            "        -418.4893, -406.2034, -378.6847, -418.4458, -419.9240, -423.0622,\n",
            "        -408.0234, -403.8930, -407.8452, -372.1426, -416.2410, -419.2279,\n",
            "        -410.8286, -393.3246, -408.9548, -408.9587, -398.7437, -412.2200,\n",
            "        -399.4273, -420.5237, -374.1422, -396.8382, -397.8337, -408.0599,\n",
            "        -410.2288, -405.6866, -421.4059, -373.2415, -409.8039, -391.4120,\n",
            "        -410.1684, -407.5803, -421.3193, -410.2694, -405.3408, -411.3967,\n",
            "        -403.8677, -424.0703, -420.1217, -411.2864, -398.2744, -387.7780,\n",
            "        -421.5276, -390.2291, -384.7774, -424.3535, -410.2610, -408.0183,\n",
            "        -421.5707, -410.2737, -397.5192, -384.7523, -413.5194, -417.6639,\n",
            "        -410.9258, -411.2945, -406.2313, -417.9681, -398.0971, -416.3765,\n",
            "        -410.8913, -388.7220, -410.6435, -410.2067, -421.9885, -386.7728,\n",
            "        -437.1447, -427.6359, -406.5203, -410.8550, -413.6665, -400.4030,\n",
            "        -397.9249, -421.5563, -399.9188, -366.8369, -413.3818, -416.3749,\n",
            "        -412.6255, -392.0975, -406.4724, -410.2432, -423.0606, -368.0056,\n",
            "        -411.1773, -391.9027, -411.1607, -372.6663, -398.1446, -411.9810,\n",
            "        -422.5276, -374.9215, -389.8369, -421.3018, -410.2708, -411.0394,\n",
            "        -423.0478, -420.1584, -421.1345, -365.1701, -402.8650, -417.8795,\n",
            "        -410.9419, -401.4159, -399.4835, -419.2458, -366.7069, -376.3673,\n",
            "        -410.8170, -390.5001, -399.7377, -411.2890, -402.1811, -408.9762,\n",
            "        -405.0164, -422.5131, -414.0938, -421.5394, -413.4446, -410.2205,\n",
            "        -390.7815, -397.1242, -419.5600, -391.5146, -407.0691, -361.7205,\n",
            "        -399.8587, -397.8861, -423.0825, -411.4280, -406.2344, -415.0052,\n",
            "        -408.4683, -354.6039, -409.6473, -410.2101, -410.8781, -370.1136,\n",
            "        -380.7908, -405.9908, -410.7315, -415.3610, -410.1190, -409.9168,\n",
            "        -396.6739, -420.1978, -410.6996, -414.7833, -378.7239, -420.3253,\n",
            "        -421.0884, -407.1140, -406.5627, -415.8392, -403.7564, -355.8752,\n",
            "        -410.2576, -413.6927, -407.8353, -411.2122, -371.6389, -411.1499,\n",
            "        -354.6118, -410.3105, -423.8059, -376.3110, -423.0674, -413.3681,\n",
            "        -411.2920, -421.8603, -374.3766, -411.3310, -391.4688, -379.2517,\n",
            "        -411.1656, -413.3442, -403.6921, -408.2226, -373.0357, -417.8654,\n",
            "        -417.1169, -418.3032, -406.2256, -420.7114, -362.2058, -414.0593,\n",
            "        -414.6743, -417.9497, -409.8943, -416.1310, -410.3156, -413.5300,\n",
            "        -421.8910, -410.5816, -399.2437, -414.7734, -372.6867, -367.7792,\n",
            "        -411.2775, -413.6609, -410.2125, -405.1328, -370.5013, -415.4924,\n",
            "        -407.1054, -391.4565, -410.8669, -410.9680, -422.1314, -408.7820,\n",
            "        -417.4215, -424.3851, -372.5627, -413.9133, -410.7659, -402.3089,\n",
            "        -406.2019, -422.5922, -393.6129, -390.5974, -398.1492, -396.9735,\n",
            "        -407.6350, -398.3041, -417.5685, -411.3301, -410.3191, -373.0970,\n",
            "        -413.0618, -395.3432, -422.4099, -394.3660, -411.2841, -397.6774,\n",
            "        -377.5036, -423.0651, -410.1364, -411.1967, -395.8617, -410.1505,\n",
            "        -416.1697, -419.3488, -391.0793, -375.7670, -413.9291, -411.5477,\n",
            "        -367.1131, -377.4161, -392.6075, -435.8030, -414.7128, -414.6477,\n",
            "        -398.9052, -388.0825, -401.8798, -410.9331, -397.2767, -410.8911,\n",
            "        -391.5255, -407.0906, -412.3511, -403.2752, -419.3103, -384.4966,\n",
            "        -399.5953, -419.3949, -413.4570, -410.9795, -411.1929, -411.1904,\n",
            "        -378.3371, -410.2702, -390.8678, -411.1128, -387.9640, -431.3534,\n",
            "        -395.6127, -412.7334, -409.5930, -412.2943, -410.9018, -403.9785,\n",
            "        -418.5771, -418.4862, -420.1190, -428.5101, -406.9104, -410.8598,\n",
            "        -393.8409, -411.2901, -378.4134, -390.7676, -386.1145, -389.9557,\n",
            "        -410.1999, -410.2336, -420.1556, -398.8413, -421.6728, -401.4158,\n",
            "        -419.0833, -361.7591, -420.0904, -408.9081, -376.3986, -410.1330,\n",
            "        -424.3152, -413.7634, -401.2865, -408.4644, -407.1110, -413.7617,\n",
            "        -419.2569, -378.1936, -387.0429, -416.8519, -399.1781, -370.3277,\n",
            "        -411.1887, -389.8365, -392.3473, -411.3401, -424.5662, -409.8148,\n",
            "        -410.6729, -376.7267, -375.9378, -392.1093, -410.1285, -417.9158,\n",
            "        -408.5837, -379.6362, -419.4867, -361.7356, -394.7691, -411.2607,\n",
            "        -397.0525, -411.9820, -371.4320, -390.9515, -411.3389, -413.2986,\n",
            "        -422.4846, -384.8417, -410.1902, -417.9438, -416.7889, -410.0571,\n",
            "        -408.0283, -436.9283, -410.4777, -409.0037, -422.4605, -399.4961,\n",
            "        -408.4105, -368.1810, -394.6987, -377.4380, -394.4241, -410.2705,\n",
            "        -410.2402, -407.0788, -416.1113, -376.5610, -410.3138, -436.0848,\n",
            "        -413.8149, -406.2092, -419.3087, -410.1015, -434.4922, -408.2952,\n",
            "        -415.4158, -408.5787, -409.3138, -412.3665, -410.9038, -425.7499,\n",
            "        -410.2799, -402.2398, -414.0096, -422.7321, -419.9898, -410.2863,\n",
            "        -409.4960, -409.5636, -400.1605, -381.3228, -391.6977, -370.0109,\n",
            "        -390.8464, -409.8874, -409.9653, -377.4787, -414.9503, -401.0586,\n",
            "        -409.7607, -378.1079, -408.9774, -411.2943, -410.8199, -412.8318,\n",
            "        -423.4267, -414.3930, -407.2755, -414.0812, -419.8118, -409.3210,\n",
            "        -405.5844, -402.4384, -418.5098, -408.2118, -410.3423, -398.8075,\n",
            "        -409.7227, -410.1843, -423.0495, -413.7598, -391.3566, -406.9497,\n",
            "        -408.7533, -403.9363, -420.1512, -406.3444, -408.2149, -423.0603,\n",
            "        -393.1306, -436.9856, -401.3921, -429.3549, -411.2982, -411.0174,\n",
            "        -421.3015, -410.1758, -427.1956, -405.2341, -410.1214, -420.2034,\n",
            "        -407.1654, -395.3534, -411.2891, -419.0483, -386.5276, -422.9433,\n",
            "        -410.2460, -419.2704, -410.4559, -405.6392, -418.4592, -383.2919,\n",
            "        -391.9234, -397.4209, -403.7519, -389.4156, -408.2989, -408.0128,\n",
            "        -406.9790, -410.9110], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.049929141998291016 0.4950000047683716 0.0 405.4505920410156\n",
            "tensor([-365.8718, -419.7876, -412.3064, -415.0839, -404.3387, -403.7733,\n",
            "        -415.8699, -403.4140, -384.1349, -409.2005, -404.6697, -397.0087,\n",
            "        -399.6606, -405.0804, -382.8219, -423.9073, -407.7514, -363.5028,\n",
            "        -370.4854, -396.9557, -400.3870, -405.7008, -416.2665, -403.7759,\n",
            "        -405.8771, -414.1570, -374.9261, -400.8634, -397.5357, -403.5253,\n",
            "        -404.6224, -411.5140, -371.6519, -416.8909, -394.4083, -401.7662,\n",
            "        -404.5939, -404.5864, -390.8603, -383.3013, -403.1442, -401.8056,\n",
            "        -404.2128, -406.4727, -413.7455, -413.2330, -394.1409, -369.1928,\n",
            "        -403.3334, -403.1487, -388.1254, -377.3776, -391.3874, -391.5823,\n",
            "        -413.2863, -383.7384, -359.8776, -398.5002, -376.2078, -367.1257,\n",
            "        -395.5905, -408.8808, -404.9565, -413.0679, -399.2389, -419.8899,\n",
            "        -392.0337, -385.9639, -403.2516, -412.0133, -363.4494, -403.2455,\n",
            "        -412.6986, -403.2530, -412.4149, -400.6950, -396.3429, -408.8761,\n",
            "        -405.0939, -410.2156, -402.5641, -396.5327, -381.7914, -405.7637,\n",
            "        -351.2405, -407.0114, -373.2987, -405.3532, -412.2840, -404.8403,\n",
            "        -369.2806, -357.7217, -404.6639, -404.8565, -412.8339, -399.5086,\n",
            "        -413.8675, -390.0922, -405.1194, -351.1591, -394.6368, -372.6114,\n",
            "        -410.5181, -404.0193, -421.2365, -396.8334, -412.7753, -350.6088,\n",
            "        -401.7607, -405.0096, -403.5101, -415.7688, -411.6314, -367.1257,\n",
            "        -383.6133, -401.4844, -397.3569, -405.3775, -380.7120, -400.5183,\n",
            "        -423.5975, -403.3174, -379.4124, -394.2177, -405.4729, -404.9136,\n",
            "        -379.3807, -395.7610, -382.8028, -404.6536, -403.4319, -425.0608,\n",
            "        -404.2830, -401.3051, -369.0630, -404.8624, -407.6342, -421.6237,\n",
            "        -393.9567, -350.0702, -388.2694, -391.9134, -405.1363, -391.1069,\n",
            "        -365.6186, -395.4542, -412.8157, -404.4438, -413.5761, -401.5752,\n",
            "        -390.2419, -405.0832, -406.4606, -405.1008, -393.3081, -417.7817,\n",
            "        -405.1708, -402.1733, -413.5494, -407.1077, -361.8614, -412.2704,\n",
            "        -395.9718, -367.6104, -404.1769, -410.9518, -397.6475, -405.6977,\n",
            "        -403.4513, -413.5525, -414.4301, -410.5622, -408.8866, -393.4931,\n",
            "        -404.1648, -367.1804, -410.1314, -403.1547, -390.9538, -384.3300,\n",
            "        -393.7559, -404.6214, -395.7336, -393.5180, -384.0723, -401.5950,\n",
            "        -366.4972, -404.1651, -408.4051, -404.6944, -404.0011, -410.0152,\n",
            "        -403.3808, -411.7708, -402.8829, -412.4931, -404.6714, -394.1566,\n",
            "        -403.4088, -404.3244, -386.2699, -399.4212, -382.9176, -419.0202,\n",
            "        -405.4789, -402.9492, -405.0953, -382.7699, -403.6870, -413.0807,\n",
            "        -410.6924, -408.1737, -400.9080, -403.2450, -396.3210, -392.5487,\n",
            "        -398.8044, -406.9267, -393.9095, -403.1216, -416.8553, -385.3910,\n",
            "        -404.6174, -408.8896, -403.3443, -395.9374, -398.9895, -393.6058,\n",
            "        -377.0569, -403.2434, -415.9381, -404.7634, -418.2346, -425.6973,\n",
            "        -411.5234, -404.5363, -405.9195, -406.9958, -399.0114, -399.3993,\n",
            "        -378.3521, -385.7617, -413.0327, -375.5984, -407.4115, -392.7383,\n",
            "        -397.8735, -403.4498, -405.2869, -346.1662, -382.3861, -422.7691,\n",
            "        -392.3239, -404.5914, -413.5337, -379.3726, -390.0330, -391.1386,\n",
            "        -401.5810, -414.4875, -374.7273, -401.5449, -414.8038, -403.2520,\n",
            "        -369.2307, -399.1623, -406.2671, -408.5790, -410.3938, -404.9281,\n",
            "        -401.3440, -391.9667, -373.1684, -410.7086, -408.1521, -382.8530,\n",
            "        -404.1080, -404.7097, -416.5757, -404.6006, -363.9257, -406.2276,\n",
            "        -405.0885, -410.5142, -402.4612, -405.2276, -413.2870, -390.3018],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, closs 0.048923421651124954 0.4950000047683716 0.0 398.64990234375\n",
            "tensor([-376.7471, -397.9797, -399.7374, -394.5197, -399.7297, -389.6643,\n",
            "        -408.7998, -387.6950, -399.3003, -398.7336, -369.9477, -394.8118,\n",
            "        -405.5295, -399.6690, -399.3627, -399.4781, -399.2163, -360.0125,\n",
            "        -397.1490, -388.5501, -405.3059, -384.8857, -362.6897, -411.8528,\n",
            "        -377.9498, -382.1888, -358.7921, -369.4605, -350.0854, -385.5961,\n",
            "        -377.1327, -405.0949, -405.6592, -394.6578, -399.7727, -396.9856,\n",
            "        -406.5204, -404.3069, -391.6587, -408.1474, -377.8045, -399.8117,\n",
            "        -410.3785, -360.6481, -402.8029, -398.6807, -396.5728, -399.2154,\n",
            "        -401.0694, -406.2845, -396.3610, -399.5431, -399.2690, -370.1145,\n",
            "        -369.7795, -406.4212, -388.4821, -400.8132, -406.0061, -362.1170,\n",
            "        -356.0719, -382.9639, -395.3052, -383.2993, -399.6233, -399.7612,\n",
            "        -386.9420, -389.6648, -396.9664, -412.8258, -383.9371, -419.8572,\n",
            "        -364.1207, -398.6974, -376.3311, -373.7813, -405.9889, -402.5558,\n",
            "        -387.0240, -388.6087, -399.2665, -373.8596, -398.6509, -380.0297,\n",
            "        -388.9305, -379.8747, -351.8076, -378.1415, -380.2126, -399.7249,\n",
            "        -362.8676, -343.8725, -352.0560, -397.4637, -399.7734, -388.5959,\n",
            "        -374.5772, -399.7207, -397.1659, -397.3656, -404.4420, -399.5248,\n",
            "        -404.9925, -383.3177, -367.7158, -399.7433, -362.0855, -398.8053,\n",
            "        -406.2216, -398.6248, -398.5740, -398.8265, -404.3213, -384.4205,\n",
            "        -399.7419, -399.7149, -392.1501, -366.7712, -398.3976, -392.7296,\n",
            "        -404.6114, -398.6782, -358.3018, -364.9843, -401.3280, -399.8310,\n",
            "        -399.2531, -385.2151, -394.4897, -384.1615, -399.6573, -399.7204,\n",
            "        -399.3496, -382.5928, -356.2679, -400.0760, -406.5801, -398.7670,\n",
            "        -388.4151, -354.2036, -390.0452, -399.3576, -399.8832, -387.1855,\n",
            "        -386.9239, -399.7685, -399.8525, -404.8282, -381.9431, -399.7472,\n",
            "        -400.7121, -406.2299, -356.2167, -399.6397, -411.5737, -337.1117,\n",
            "        -395.1794, -399.2575, -405.7146, -406.2455, -399.7740, -399.2915,\n",
            "        -397.4346, -399.7186, -399.7419, -384.6453, -357.6288, -390.6266,\n",
            "        -392.1866, -398.1533, -404.6383, -347.5650, -373.8941, -398.8624,\n",
            "        -398.8512, -390.9065, -405.1316, -399.2808, -370.2058, -399.3429,\n",
            "        -396.9902, -366.3256, -383.1007, -389.7370, -389.8309, -391.4990,\n",
            "        -359.1090, -358.9646, -420.4510, -397.4189, -394.6359, -399.4152,\n",
            "        -392.6300, -398.6755, -391.2838, -399.4129, -408.0886, -405.3689,\n",
            "        -374.2067, -394.8227, -375.7212, -378.6627, -384.3996, -342.5859,\n",
            "        -399.1245, -395.6092, -399.5881, -397.4164, -399.4230, -399.3523,\n",
            "        -399.7652, -399.2419, -399.7751, -366.2100, -334.6827, -362.6877,\n",
            "        -367.7895, -399.2406, -334.7025, -410.3841, -392.1013, -398.5704,\n",
            "        -399.2502, -400.0643, -389.6679, -398.6653, -399.7451, -400.1121,\n",
            "        -367.7878, -382.5225, -367.7916, -396.6014, -402.4100, -398.5804,\n",
            "        -374.5745, -332.4483, -406.6711, -399.6701, -370.5820, -399.4388,\n",
            "        -397.0052, -397.4413, -400.5754, -399.2607, -372.7542, -401.1703,\n",
            "        -363.8982, -408.6136, -406.2126, -393.4250, -369.9740, -347.2372,\n",
            "        -397.5128, -396.0365, -399.7708, -399.1188, -392.5357, -397.1376,\n",
            "        -399.7223, -399.3356, -358.0475, -380.7700, -357.2577, -398.9977,\n",
            "        -402.6242, -386.9781, -408.1466, -363.2937, -392.6484, -399.2458,\n",
            "        -399.6205, -399.2996, -384.3499, -399.7527, -397.4792, -396.1397,\n",
            "        -383.7431, -358.6096, -396.2430, -367.8925, -406.6567, -402.7433,\n",
            "        -406.1762, -330.7137, -399.5544, -400.0616, -397.9771, -373.6483,\n",
            "        -392.4589, -399.3437, -385.2014, -397.6111, -391.4283, -400.3862,\n",
            "        -332.5771, -412.7169, -406.5007, -399.8214, -338.2996, -329.1039,\n",
            "        -376.7892, -405.6266, -395.5246, -385.9419, -379.2930, -388.6141,\n",
            "        -399.6664, -399.5666, -341.3427, -384.9351, -376.0150, -383.6151,\n",
            "        -398.4739, -352.8217, -383.6734, -355.0101, -392.8557, -399.2998,\n",
            "        -399.6679, -392.1207, -394.5220, -395.3113, -399.3438, -399.6484,\n",
            "        -401.0652, -358.1282, -367.8235, -398.6777, -406.2282, -382.7684,\n",
            "        -397.4244, -335.8173, -407.2433, -405.7832, -398.5624, -402.4621,\n",
            "        -399.7391, -398.4476, -394.4773, -395.6926, -399.3034, -408.1099,\n",
            "        -394.3199, -386.5429, -384.9348, -399.7581, -398.9872, -349.0600,\n",
            "        -388.5835, -399.2188, -399.7290, -403.9026, -398.7892, -399.2122,\n",
            "        -399.2679, -398.8490, -399.5099, -402.0345, -410.3619, -379.7235,\n",
            "        -350.1654, -399.7531, -380.7632, -411.2427, -380.3543, -402.8745,\n",
            "        -399.3000, -373.9756, -392.6467, -385.4309, -369.4810, -398.8614,\n",
            "        -394.1633, -397.2341, -367.8052, -400.0536, -380.3613, -377.0405,\n",
            "        -398.7451, -363.1908, -350.2170, -384.5838, -376.8684, -399.9178,\n",
            "        -399.6649, -376.5352, -371.0686, -400.1113, -383.1129, -358.0189,\n",
            "        -408.3984, -398.7525, -407.9599, -399.3638, -399.4995, -351.6548,\n",
            "        -373.8663, -399.6672, -399.5802, -397.3099, -389.1275, -397.7296,\n",
            "        -395.4835, -398.9717, -400.0146, -383.9077, -407.5924, -380.8100,\n",
            "        -372.4884, -378.7373, -399.7403, -396.8816, -395.8611, -399.6935,\n",
            "        -399.0691, -399.3506, -398.1524, -388.4640, -399.2284, -399.8837,\n",
            "        -399.7645, -411.8462, -402.9644, -371.6393, -354.7122, -399.2435,\n",
            "        -395.8286, -412.1437, -406.2445, -399.7220, -400.0650, -399.2860,\n",
            "        -399.3568, -380.2951, -399.2654, -399.2316, -399.1856, -399.5149,\n",
            "        -404.7989, -371.1818, -396.6244, -399.5545, -392.6461, -366.6010,\n",
            "        -357.0570, -399.8550, -380.2809, -399.5721, -399.1380, -405.1898,\n",
            "        -396.9672, -398.7423, -380.9590, -399.6246, -404.5168, -397.3005,\n",
            "        -393.3754, -376.2517, -397.8028, -369.1115, -348.0276, -395.2914,\n",
            "        -399.2517, -406.0857, -399.6468, -399.7186, -397.8159, -356.9958,\n",
            "        -376.7358, -365.9004, -411.3188, -406.5029, -403.9679, -399.2930,\n",
            "        -399.6453, -369.2440, -380.6424, -394.6502, -357.9204, -400.4230,\n",
            "        -405.6940, -399.5475, -399.7376, -399.7291, -406.7676, -406.3314,\n",
            "        -403.5314, -412.7966, -408.3326, -399.6607, -405.2154, -408.2513,\n",
            "        -392.1868, -384.5468, -399.8054, -398.6470, -401.8293, -397.1632,\n",
            "        -383.0130, -399.5771, -399.7264, -398.4447, -357.5867, -384.3999,\n",
            "        -346.5332, -399.9756, -398.4488, -332.7715, -400.0155, -405.6095,\n",
            "        -399.8402, -397.8051], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.04641304910182953 0.4950000047683716 0.0 389.95361328125\n",
            "tensor([-389.8924, -393.3569, -393.8863, -392.3131, -360.5730, -383.5479,\n",
            "        -352.6494, -341.9958, -389.7612, -391.3672, -391.5797, -390.9022,\n",
            "        -391.2466, -397.7184, -392.9693, -400.4104, -380.9311, -393.3723,\n",
            "        -383.2760, -393.3944, -392.4507, -378.4880, -379.4079, -357.3573,\n",
            "        -363.2617, -392.2465, -390.5016, -335.5619, -352.9222, -393.3874,\n",
            "        -392.5808, -380.2862, -392.3027, -376.3787, -388.3903, -393.3690,\n",
            "        -367.6824, -393.3597, -399.7112, -358.0318, -378.8076, -397.9241,\n",
            "        -392.8881, -347.0330, -381.6827, -393.4958, -398.3058, -382.8378,\n",
            "        -394.3272, -397.8774, -390.4818, -393.4837, -393.3598, -402.1303,\n",
            "        -368.1487, -399.5499, -383.7360, -392.5959, -360.8807, -355.1222,\n",
            "        -400.0552, -377.4773, -392.3397, -398.6570, -393.2126, -389.1673,\n",
            "        -379.3450, -379.0140, -393.2366, -399.5058, -335.0852, -408.1899,\n",
            "        -400.3680, -369.6707, -393.3062, -398.0387, -362.2333, -390.1042,\n",
            "        -393.3161, -387.8433, -392.3173, -369.1210, -382.3116, -393.2781,\n",
            "        -392.2759, -391.5840, -382.0775, -346.2573, -398.9796, -393.3428,\n",
            "        -390.5413, -344.2091, -390.7166, -381.6327, -392.2974, -396.9462,\n",
            "        -393.4123, -389.8343, -393.3712, -376.4026, -375.4928, -400.0652,\n",
            "        -370.5155, -399.9827, -388.7574, -364.7173, -390.4804, -403.4999,\n",
            "        -400.3381, -392.2360, -376.7291, -393.3125, -390.4458, -393.4407,\n",
            "        -389.5360, -391.5826, -390.2064, -393.2450, -368.7914, -391.8709,\n",
            "        -403.1924, -398.3015, -393.4908, -331.4359, -370.6711, -390.4508,\n",
            "        -387.5725, -369.0063, -389.9041, -397.1980, -389.7645, -392.3678,\n",
            "        -393.3961, -399.9381, -349.2324, -390.6977, -379.4809, -392.7843,\n",
            "        -372.0447, -353.4875, -398.1472, -393.3689, -386.9690, -388.3878,\n",
            "        -389.6248, -398.7202, -389.7627, -392.8167, -393.3369, -342.3721,\n",
            "        -350.2005, -389.9649, -398.9275, -392.4494, -333.8296, -319.8662,\n",
            "        -343.9042, -392.5480, -388.2925, -390.5368, -390.5965, -392.5720,\n",
            "        -391.5854, -387.0163, -386.8930, -390.9601, -385.0367, -356.7422,\n",
            "        -354.8274, -388.2144, -397.9241, -380.6331, -400.2745, -393.3909,\n",
            "        -390.7010, -400.3587, -383.9876, -342.2714, -393.4258, -393.0267,\n",
            "        -393.3503, -398.4722, -360.0365, -346.8549, -348.4604, -390.6599,\n",
            "        -391.1393, -327.6631, -404.4240, -394.4245, -362.0986, -395.8640,\n",
            "        -393.1871, -392.9307, -397.7251, -392.2290, -391.7275, -399.5388,\n",
            "        -361.8777, -376.4093, -385.7448, -393.3072, -370.6878, -355.1489,\n",
            "        -400.4743, -393.4543, -380.9810, -392.3109, -353.5058, -372.6644,\n",
            "        -379.7094, -384.5488, -398.2577, -397.3796, -382.9657, -404.5775,\n",
            "        -351.4697, -390.8232, -392.9614, -324.6752, -363.7595, -392.9890,\n",
            "        -398.5931, -390.6613, -378.9612, -390.7319, -392.2628, -393.3392,\n",
            "        -392.4201, -368.2110, -390.3932, -400.1628, -379.2653, -391.5859,\n",
            "        -390.8164, -370.8610, -355.6562, -393.3816, -373.5380, -393.2634,\n",
            "        -387.3890, -393.3392, -391.2101, -393.3230, -400.2501, -374.8262,\n",
            "        -376.0704, -380.2043, -403.2360, -390.4961, -373.4768, -395.5306,\n",
            "        -405.1225, -381.2463, -393.4206, -393.3252, -392.8829, -382.3304,\n",
            "        -389.5051, -389.8779, -380.6904, -399.5294, -364.5618, -371.9554,\n",
            "        -388.6486, -393.3959, -392.8115, -331.2807, -413.0971, -350.9661,\n",
            "        -392.5374, -390.7357, -359.0515, -393.3810, -393.3695, -392.8925,\n",
            "        -390.5436, -392.4413, -344.9449, -365.6220, -374.7133, -369.5187,\n",
            "        -376.9141, -348.7159, -353.7787, -393.1829, -393.3780, -389.8358],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.044208548963069916 0.4950000047683716 0.0 383.3917541503906\n",
            "tensor([-350.6802, -399.8734, -386.1422, -385.0267, -389.6580, -378.0874,\n",
            "        -378.9607, -389.7470, -384.5285, -321.7583, -342.7365, -378.5690,\n",
            "        -389.9571, -359.1075, -386.0300, -385.1546, -363.9612, -323.9231,\n",
            "        -386.0775, -378.3949, -386.2299, -361.3698, -350.7231, -386.3084,\n",
            "        -375.3596, -392.5710, -355.6766, -386.1031, -387.4644, -390.5810,\n",
            "        -386.0093, -384.8853, -398.7961, -403.4514, -366.7343, -367.3905,\n",
            "        -389.5881, -360.4665, -385.4294, -374.1679, -392.4724, -341.0631,\n",
            "        -396.7142, -372.1737, -385.1669, -387.2055, -382.6098, -386.3099,\n",
            "        -356.3099, -355.3643, -386.2955, -386.3251, -386.2189, -383.6150,\n",
            "        -365.9473, -329.1914, -376.9156, -377.1096, -375.3956, -386.2864,\n",
            "        -383.9640, -369.4130, -380.4278, -385.1842, -359.6422, -392.2696,\n",
            "        -386.0582, -385.0149, -385.0916, -365.5848, -332.7149, -384.9927,\n",
            "        -386.3458, -355.8987, -314.8890, -382.7545, -383.4038, -397.3232,\n",
            "        -384.9412, -380.7666, -398.1898, -398.3874, -385.3104, -386.3350,\n",
            "        -342.2778, -376.8136, -390.3015, -389.9370, -353.3585, -342.3152,\n",
            "        -350.4276, -386.1648, -324.7910, -400.2188, -383.0999, -382.3712,\n",
            "        -384.2377, -391.1216, -378.6578, -390.6083, -381.6026, -376.5994,\n",
            "        -386.3373, -382.6704, -338.5252, -374.8563, -407.4018, -385.4158,\n",
            "        -360.4088, -327.9927, -388.3114, -374.1632, -377.9950, -390.6906,\n",
            "        -385.7340, -385.2771, -382.6882, -337.5692, -398.3488, -384.8170,\n",
            "        -382.6720, -376.8357, -389.9946, -385.2431, -385.2474, -367.5594,\n",
            "        -379.6025, -385.2742, -358.9177, -350.1886, -364.5925, -385.3313,\n",
            "        -374.7953, -389.9222, -378.6464, -364.0750, -381.7164, -378.9091,\n",
            "        -358.6011, -386.3605, -365.9375, -365.8906, -381.9371, -386.0312,\n",
            "        -398.0827, -350.7484, -355.9380, -388.4568, -385.5766, -365.7967,\n",
            "        -348.1374, -390.6142, -362.1141, -389.4853, -363.8006, -385.7861,\n",
            "        -386.3195, -384.2889, -383.1885, -386.4681, -404.3021, -400.5026,\n",
            "        -386.3523, -382.6660, -385.0005, -395.9051, -367.9449, -364.1040,\n",
            "        -388.5352, -344.5544, -368.3179, -385.0104, -354.5796, -325.7187,\n",
            "        -385.1204, -386.1344, -390.9357, -405.0327, -386.2695, -386.5102,\n",
            "        -386.0580, -382.8708, -386.0838, -386.4732, -383.4834, -391.0933,\n",
            "        -386.1713, -383.3872, -387.2190, -384.8094, -352.3947, -386.4770,\n",
            "        -391.1411, -367.9030, -368.6407, -384.8932, -363.4267, -382.6867,\n",
            "        -385.2383, -386.4056, -385.1768, -383.1093, -368.8190, -382.6579,\n",
            "        -392.2861, -403.3450, -385.0517, -382.9213, -400.3702, -382.5968,\n",
            "        -385.0954, -388.3100, -386.3197, -356.0817, -369.5658, -385.6378,\n",
            "        -356.8475, -376.9709, -381.2988, -386.3335, -348.9142, -390.2220,\n",
            "        -368.9346, -386.0509, -375.2447, -342.3448, -386.1017, -386.1455,\n",
            "        -384.9940, -356.8771, -387.7547, -385.2617, -398.0335, -391.9313,\n",
            "        -334.8347, -386.2203, -340.3969, -381.7273, -380.7415, -385.8063,\n",
            "        -374.2815, -360.9115, -385.1872, -367.6436, -379.6547, -362.6508,\n",
            "        -392.5341, -378.5036, -348.6831, -342.6597, -382.3030, -371.9813,\n",
            "        -383.4817, -351.8426, -385.2807, -383.0007, -400.1509, -346.4843,\n",
            "        -329.5855, -384.8931, -386.1564, -347.6973, -374.2302, -390.3469,\n",
            "        -367.7854, -390.4626, -392.2235, -378.2467, -348.0234, -347.9005,\n",
            "        -390.4945, -385.0698, -356.5572, -362.9936, -385.2668, -368.8648,\n",
            "        -386.4612, -341.2953, -344.3479, -386.1696, -332.6029, -345.1258,\n",
            "        -379.7304, -362.4038, -398.0970, -350.6058, -388.3658, -386.3901,\n",
            "        -396.2218, -380.5795, -385.1903, -385.1537, -382.1478, -349.7890,\n",
            "        -377.0149, -386.3451, -389.0490, -346.5869, -391.8504, -366.8037,\n",
            "        -350.1038, -329.9236, -371.4535, -386.3711, -368.7168, -360.7819,\n",
            "        -390.4024, -380.0536, -365.4695, -392.6401, -384.1258, -382.1453,\n",
            "        -383.5110, -328.9964, -333.6575, -384.9039, -340.4447, -380.6455,\n",
            "        -379.8164, -379.6742, -341.0482, -346.5392, -390.5984, -349.1079,\n",
            "        -384.8730, -355.0761, -375.7995, -378.6736, -358.3823, -387.7771,\n",
            "        -388.4958, -355.2150, -366.6393, -400.6551, -381.9223, -381.5441,\n",
            "        -381.9015, -363.4613, -383.0067, -382.6637, -385.6095, -374.7632,\n",
            "        -330.7054, -385.2516, -359.0892, -361.0718, -400.6595, -382.8748,\n",
            "        -379.7201, -387.2013, -384.5258, -355.9657, -386.1015, -358.5108,\n",
            "        -385.1423, -385.0239, -382.4206, -374.1750, -386.1671, -381.2087,\n",
            "        -391.9066, -383.4652, -374.2966, -355.1965, -341.7444, -382.0226,\n",
            "        -385.2912, -385.1091, -392.6464, -327.6863, -386.3415, -388.4498,\n",
            "        -349.3767, -360.8350, -378.0348, -386.2142, -379.9370, -383.9480,\n",
            "        -371.6281, -380.5937, -356.9630, -392.4042, -360.2646, -383.8503,\n",
            "        -383.1830, -352.7573, -386.1538, -386.1770, -369.8156, -392.7480,\n",
            "        -371.9248, -359.4650, -379.0594, -366.7698, -377.7081, -383.9265,\n",
            "        -365.3629, -363.5552, -384.8106, -383.3928, -361.6154, -392.6602,\n",
            "        -386.3354, -385.5598, -385.1616, -338.7074, -384.2890, -357.1434,\n",
            "        -375.8043, -368.9895, -382.7442, -375.6267, -381.6371, -387.0236,\n",
            "        -385.2358, -379.8431, -392.5153, -392.6916, -382.1262, -367.9777,\n",
            "        -383.9975, -327.2311, -346.0099, -385.0798, -364.5723, -383.8046,\n",
            "        -377.8475, -385.2383, -386.5396, -358.5511, -386.4565, -386.1482,\n",
            "        -363.0915, -362.0623, -387.4491, -382.7574, -386.1071, -385.0258,\n",
            "        -394.9018, -383.7276, -322.3859, -386.0794, -384.8915, -386.2932,\n",
            "        -380.7138, -383.9737, -382.6917, -385.1686, -387.4641, -331.4537,\n",
            "        -385.3348, -386.2363, -386.3022, -334.5860, -381.2430, -364.6125,\n",
            "        -398.2292, -392.7892, -378.1774, -382.6847, -381.5270, -386.0522,\n",
            "        -385.7483, -384.8692, -396.8170, -370.7615, -385.1026, -385.3122,\n",
            "        -386.2147, -357.9442, -386.3206, -382.8585, -340.3584, -355.4796,\n",
            "        -389.3941, -382.0993, -343.2305, -401.4728, -383.1284, -323.8694,\n",
            "        -382.5395, -397.7089, -367.0902, -382.7034, -386.0042, -388.0663,\n",
            "        -357.2258, -386.1447, -375.9768, -391.0298, -341.8951, -385.8550,\n",
            "        -389.6179, -376.9032, -385.0172, -369.2590, -385.1622, -373.3210,\n",
            "        -379.6056, -382.8448, -384.8463, -378.4128, -368.8760, -380.1734,\n",
            "        -387.7394, -378.5208, -354.7334, -369.6989, -378.4762, -383.9215,\n",
            "        -389.8355, -385.2519], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.044439274817705154 0.4950000047683716 0.0 375.8057556152344\n",
            "tensor([-383.9575, -307.4395, -376.9584, -377.1730, -370.6838, -320.1949,\n",
            "        -362.7402, -378.4121, -377.8399, -339.1725, -350.3825, -381.7943,\n",
            "        -328.1429, -381.0926, -377.6836, -372.0594, -389.9183, -386.9191,\n",
            "        -377.0637, -378.3003, -358.4415, -381.1649, -384.1402, -378.3289,\n",
            "        -369.0385, -382.3269, -361.3120, -378.1326, -367.3924, -367.0965,\n",
            "        -376.9613, -369.5066, -338.2910, -378.3414, -361.7142, -378.2893,\n",
            "        -377.1767, -319.6280, -356.5530, -378.1520, -371.1450, -389.8817,\n",
            "        -355.3904, -368.7533, -375.1718, -377.6969, -357.7907, -374.8310,\n",
            "        -334.9966, -375.8780, -359.5705, -378.4211, -372.8210, -384.0126,\n",
            "        -358.3251, -378.4615, -378.3641, -384.0197, -375.7234, -377.1305,\n",
            "        -391.0860, -312.9862, -376.5449, -348.8297, -357.7051, -378.2665,\n",
            "        -380.0292, -381.6209, -372.1002, -378.5598, -335.1652, -362.5725,\n",
            "        -370.4415, -370.9889, -369.8277, -378.1405, -370.6860, -362.5978,\n",
            "        -369.2061, -367.3750, -383.9481, -372.0705, -351.5641, -311.8038,\n",
            "        -360.0582, -345.7238, -392.4573, -377.9099, -383.4655, -330.9665,\n",
            "        -389.9116, -375.5783, -376.2665, -370.3376, -375.7308, -373.6182,\n",
            "        -366.3883, -367.8345, -378.0963, -376.9333, -378.2460, -356.1098,\n",
            "        -372.5738, -381.9513, -362.2577, -379.0135, -382.8049, -365.8323,\n",
            "        -341.6230, -376.3701, -378.3449, -376.5414, -319.4663, -374.3174,\n",
            "        -377.9263, -378.2632, -378.6114, -373.5670, -378.3745, -355.1110,\n",
            "        -317.1727, -381.3131, -374.8656, -376.7250, -374.8222, -354.2005,\n",
            "        -378.5559, -378.0890, -377.8300, -374.3257, -378.5714, -378.1911,\n",
            "        -376.6154, -378.8171, -383.4672, -357.4424, -380.4845, -374.1929,\n",
            "        -362.8540, -366.3392, -372.7448, -375.3785, -377.8169, -350.5824,\n",
            "        -385.9778, -353.2749, -376.3540, -378.2474, -378.3325, -345.0178,\n",
            "        -363.6990, -373.6018, -384.0158, -395.6839, -337.3537, -377.8761,\n",
            "        -384.1287, -356.7194, -371.2312, -377.1613, -371.6537, -389.2138,\n",
            "        -374.5688, -378.2945, -376.3521, -325.4226, -376.2622, -378.4428,\n",
            "        -352.6320, -394.2339, -384.3670, -340.6695, -357.9344, -369.2838,\n",
            "        -377.3121, -376.6150, -361.5465, -378.6263, -362.5385, -377.0201,\n",
            "        -375.4503, -358.7979, -383.3304, -362.3846, -376.6559, -383.8421,\n",
            "        -382.4590, -377.6960, -361.3961, -368.2858, -378.1818, -379.8633,\n",
            "        -323.6682, -383.5727, -376.5976, -375.4589, -374.0827, -376.3526,\n",
            "        -334.9870, -381.1830, -384.0803, -338.3241, -370.4401, -377.0650,\n",
            "        -374.7614, -356.9265, -368.6469, -374.3910, -374.0135, -310.9480,\n",
            "        -377.1974, -369.2312, -378.6792, -324.0772, -339.5821, -362.8123,\n",
            "        -382.2662, -372.3816, -381.4412, -378.3620, -366.4008, -345.8172,\n",
            "        -368.7370, -378.3949, -360.3479, -368.4072, -376.3525, -376.0782,\n",
            "        -377.2408, -332.1144, -369.0055, -377.8406, -348.4228, -355.3676,\n",
            "        -397.7650, -376.5007, -351.5911, -317.5640, -333.0417, -339.2085,\n",
            "        -397.6787, -374.8967, -381.8551, -372.1172, -377.6736, -313.1241,\n",
            "        -331.7954, -377.0613, -349.3816, -389.2561, -318.6113, -377.8326,\n",
            "        -373.2467, -381.6154, -377.1691, -359.2758, -384.1600, -367.0933,\n",
            "        -378.4205, -378.2411, -377.0267, -358.3958, -362.0436, -360.0361,\n",
            "        -333.4079, -369.9378, -358.5214, -375.6045, -340.9536, -383.9836,\n",
            "        -378.4255, -376.1672, -383.4667, -385.3236, -373.5727, -350.3667,\n",
            "        -334.2338, -383.5008, -367.4105, -376.6013, -373.4063, -376.0383,\n",
            "        -378.2777, -375.7171, -378.5182, -368.0031, -378.5753, -350.2830],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.042909834533929825 0.4950000047683716 0.0 368.15081787109375\n",
            "tensor([-354.4788, -367.9000, -353.3650, -362.0294, -317.9318, -363.1529,\n",
            "        -361.2687, -348.8470, -347.9589, -367.0402, -367.3689, -361.2025,\n",
            "        -358.2719, -326.9518, -358.2943, -341.5277, -358.8641, -367.5003,\n",
            "        -366.9802, -365.7462, -368.2299, -369.0372, -367.5568, -361.5905,\n",
            "        -360.0936, -313.6772, -363.0568, -371.8631, -342.4961, -360.9063,\n",
            "        -307.8796, -372.6376, -356.7345, -345.5635, -369.2606, -359.4336,\n",
            "        -354.5980, -363.3531, -341.0381, -315.7373, -338.7711, -338.2669,\n",
            "        -362.0312, -360.4786, -323.1362, -362.4988, -349.9066, -319.5923,\n",
            "        -368.6537, -329.5274, -367.3892, -368.4144, -352.5919, -365.7565,\n",
            "        -369.4402, -368.6378, -346.0290, -299.3196, -369.3841, -367.3870,\n",
            "        -347.0656, -310.4552, -365.7403, -324.7997, -361.3499, -363.3183,\n",
            "        -369.2508, -363.4028, -374.4776, -365.1046, -361.6990, -366.9836,\n",
            "        -367.2745, -326.8356, -334.3120, -317.7323, -354.6924, -382.9920,\n",
            "        -361.9932, -380.8912, -312.1848, -365.2401, -372.5092, -369.3119,\n",
            "        -365.3402, -368.3640, -351.8226, -342.5650, -351.8591, -353.0619,\n",
            "        -362.9854, -363.2877, -361.7636, -355.8942, -375.5300, -378.5128,\n",
            "        -348.1205, -363.7801, -369.2898, -353.2433, -322.4993, -361.2479,\n",
            "        -369.2838, -341.0336, -344.2210, -339.2574, -368.7675, -352.9184,\n",
            "        -355.4544, -349.4529, -355.5999, -364.5444, -331.1323, -369.0732,\n",
            "        -368.8500, -371.8732, -368.1510, -371.1533, -370.6928, -355.9159,\n",
            "        -375.5188, -368.4844, -365.7853, -358.7160, -340.2788, -352.0910,\n",
            "        -375.5746, -334.2436, -377.3053, -368.4377, -368.2251, -369.1275,\n",
            "        -335.8411, -368.5795, -357.0126, -367.8874, -351.7907, -300.0313,\n",
            "        -363.0079, -353.0077, -354.6348, -367.5265, -372.4817, -324.2790,\n",
            "        -345.9318, -360.9596, -359.3335, -301.6175, -364.4168, -363.1169,\n",
            "        -342.7075, -367.0634, -366.5365, -316.5907, -371.2588, -368.8779,\n",
            "        -372.0294, -387.6844, -369.6690, -381.3242, -358.7356, -368.9485,\n",
            "        -369.6693, -369.6527, -333.4915, -302.1813, -359.5284, -326.7937,\n",
            "        -368.4546, -369.8736, -368.8701, -355.6002, -369.0337, -357.3839,\n",
            "        -351.6221, -350.9082, -381.3199, -368.9009, -358.0949, -367.5044,\n",
            "        -373.5773, -368.6163, -367.4571, -369.7214, -377.3000, -329.9813,\n",
            "        -364.7464, -361.4588, -373.2215, -367.4958, -358.7658, -362.2768,\n",
            "        -374.6236, -366.5381, -359.7010, -368.3112, -310.5615, -316.8821,\n",
            "        -364.0853, -368.5828, -329.6242, -361.7494, -365.7280, -369.9971,\n",
            "        -346.5565, -358.6834, -375.5960, -357.5650, -370.4980, -339.8406,\n",
            "        -367.3672, -368.8629, -383.9444, -369.7190, -365.2428, -357.5162,\n",
            "        -318.1876, -368.9018, -369.2746, -367.5023, -346.4927, -367.4832,\n",
            "        -369.0107, -370.1716, -331.4700, -367.1490, -369.1656, -369.2523,\n",
            "        -366.0357, -368.9773, -367.4958, -367.1761, -368.0972, -331.1547,\n",
            "        -369.3908, -346.7854, -360.0304, -363.3006, -369.2713, -385.8515,\n",
            "        -306.8866, -371.0981, -371.8197, -368.8455, -383.7502, -369.2858,\n",
            "        -369.2483, -370.2740, -375.1053, -369.9453, -369.1414, -370.6802,\n",
            "        -373.9391, -334.8475, -363.3177, -308.9891, -351.4868, -368.4538,\n",
            "        -312.5260, -370.2710, -359.0395, -348.9444, -364.7271, -343.0785,\n",
            "        -373.6390, -299.4902, -370.1707, -363.3177, -373.7430, -380.8283,\n",
            "        -368.8847, -375.5354, -372.6374, -325.8455, -369.0995, -363.5259,\n",
            "        -341.8484, -367.5049, -368.4487, -367.2916, -362.5509, -333.9002,\n",
            "        -371.4450, -342.8396, -352.9177, -348.0979, -354.6932, -331.6052,\n",
            "        -348.6873, -369.2830, -369.3521, -362.4576, -320.0717, -371.9787,\n",
            "        -360.3762, -367.4203, -374.4742, -335.9031, -368.9293, -369.8836,\n",
            "        -340.6812, -374.9510, -369.2893, -335.2091, -332.1894, -368.4632,\n",
            "        -368.6470, -369.0735, -320.0889, -353.4688, -363.3662, -359.2222,\n",
            "        -351.7917, -312.5558, -365.7457, -364.2634, -375.3333, -355.9721,\n",
            "        -366.9724, -383.1407, -334.6981, -368.7450, -309.3300, -364.0798,\n",
            "        -349.4057, -361.1979, -368.9194, -359.7195, -326.9089, -367.5842,\n",
            "        -361.5010, -333.1814, -372.1729, -333.5937, -346.2330, -365.5471,\n",
            "        -380.8495, -320.3156, -367.9488, -359.5534, -366.3296, -365.4633,\n",
            "        -368.7459, -367.7711, -349.5505, -369.9405, -363.0389, -369.4023,\n",
            "        -356.7809, -371.7354, -369.3049, -350.1135, -337.8686, -365.2477,\n",
            "        -350.8068, -368.6534, -297.4098, -365.5323, -369.3159, -369.1435,\n",
            "        -367.1948, -337.9544, -363.0088, -361.2876, -318.7952, -352.3690,\n",
            "        -368.3996, -358.0604, -335.8321, -368.6658, -364.0716, -352.9125,\n",
            "        -338.1587, -349.6638, -369.3360, -328.6123, -386.3686, -323.8972,\n",
            "        -370.3020, -361.3784, -366.8002, -363.3790, -364.5629, -346.7930,\n",
            "        -340.2306, -369.0430, -347.5445, -365.8150, -381.0640, -371.8156,\n",
            "        -369.2773, -363.3221, -383.7674, -369.9266, -369.2689, -365.4385,\n",
            "        -368.9525, -309.8727, -358.8893, -375.0855, -344.6292, -364.7950,\n",
            "        -369.2412, -371.1269, -297.3140, -371.1909, -369.1319, -366.5381,\n",
            "        -358.9813, -346.2078, -369.2588, -369.1623, -364.8893, -371.1074,\n",
            "        -366.9954, -355.4256, -313.1188, -328.5620, -369.1661, -361.4162,\n",
            "        -313.0123, -361.1796, -364.2890, -363.3444, -351.8957, -343.4924,\n",
            "        -363.2533, -361.4707, -364.9151, -376.8775, -369.3315, -375.3683,\n",
            "        -311.2133, -369.1540, -362.0002, -369.1510, -374.4786, -371.5367,\n",
            "        -368.9239, -365.5577, -321.9368, -330.3441, -366.9931, -368.4843,\n",
            "        -366.9279, -380.0094, -370.5722, -364.4913, -317.2014, -368.7346,\n",
            "        -369.2530, -364.3655, -363.0517, -367.7929, -367.7864, -370.6917,\n",
            "        -373.5728, -311.0313, -369.3335, -358.2045, -375.4549, -315.7262,\n",
            "        -369.2713, -346.6068, -356.1377, -338.8379, -368.0594, -338.5804,\n",
            "        -341.3109, -367.0416, -347.4896, -359.3982, -371.5371, -361.7174,\n",
            "        -368.8658, -375.6463, -373.4763, -357.8012, -375.6355, -367.7346,\n",
            "        -357.5996, -330.0124, -371.1507, -363.7713, -353.7866, -367.7773,\n",
            "        -365.3839, -369.3329, -342.5930, -355.4424, -361.4728, -361.2244,\n",
            "        -372.9369, -354.9033, -367.8051, -320.1869, -374.9709, -369.8335,\n",
            "        -367.1639, -369.4247, -368.7384, -366.9977, -355.9014, -329.6013,\n",
            "        -371.0252, -352.0144, -370.5149, -368.7926, -364.2758, -369.1670,\n",
            "        -347.2789, -348.6889], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.04130258038640022 0.4950000047683716 0.0 357.6943359375\n",
            "tensor([-371.5295, -359.1783, -320.7697, -361.2278, -377.0999, -354.1877,\n",
            "        -356.5512, -348.0737, -344.7633, -286.1059, -359.2031, -327.0041,\n",
            "        -338.0538, -359.1416, -363.4618, -322.2462, -357.7300, -352.9544,\n",
            "        -353.3630, -359.1663, -329.2701, -326.5248, -323.0354, -338.2471,\n",
            "        -365.4159, -341.7245, -348.1321, -358.2390, -306.4591, -376.1362,\n",
            "        -349.5445, -365.4026, -340.5668, -335.8268, -359.1689, -359.1338,\n",
            "        -343.4203, -351.8503, -349.3085, -359.1336, -356.5565, -319.3990,\n",
            "        -361.6938, -352.3419, -339.7241, -345.3153, -350.8185, -307.7846,\n",
            "        -351.9105, -359.1846, -349.5705, -353.6639, -358.3612, -362.1567,\n",
            "        -355.1375, -359.1346, -331.4402, -309.5997, -359.1931, -350.5283,\n",
            "        -342.6263, -299.6674, -354.1344, -356.0069, -356.0624, -350.5013,\n",
            "        -350.0261, -346.4815, -338.5803, -330.6520, -351.8816, -355.4099,\n",
            "        -361.5424, -290.0889, -359.2766, -361.5225, -340.9524, -366.0294,\n",
            "        -326.4751, -356.1403, -374.5904, -359.1709, -359.2375, -355.1047,\n",
            "        -282.3026, -334.9268, -357.6025, -357.5029, -359.8216, -333.4614,\n",
            "        -359.3239, -359.1343, -344.1166, -352.9068, -365.9951, -313.0158,\n",
            "        -315.1895, -357.9492, -349.9924, -340.6593, -371.1683, -354.3434,\n",
            "        -356.8030, -351.9859, -287.4976, -297.4158, -361.9174, -346.8381,\n",
            "        -317.0174, -352.9912, -350.8195, -350.0144, -318.2331, -355.5052,\n",
            "        -357.0237, -358.6499, -309.4048, -355.9166, -353.6597, -311.8288,\n",
            "        -359.8368, -334.1148, -359.0327, -362.8908, -316.9460, -335.3097,\n",
            "        -350.7521, -352.7815, -351.0915, -359.1404, -349.5341, -356.5519,\n",
            "        -327.2064, -353.7263, -357.8974, -353.0256, -361.9048, -325.3224,\n",
            "        -350.2311, -357.7970, -365.9117, -339.6959, -354.1910, -355.9418,\n",
            "        -366.1801, -359.2279, -356.1078, -357.6530, -341.5845, -359.1527,\n",
            "        -327.2548, -358.3433, -354.4112, -333.2889, -304.8541, -332.6016,\n",
            "        -341.7647, -330.0438, -355.1483, -358.7062, -319.0005, -350.2832,\n",
            "        -357.6579, -352.9681, -364.0270, -357.3839, -355.9168, -354.2799,\n",
            "        -311.4336, -347.4189, -358.8774, -355.1428, -278.1234, -356.5022,\n",
            "        -358.8560, -307.7751, -324.5838, -359.2652, -357.6334, -309.9845,\n",
            "        -350.7668, -359.1938, -357.5091, -357.6386, -361.4561, -321.5522,\n",
            "        -343.0545, -342.9592, -312.6510, -378.5368, -357.5952, -337.6979,\n",
            "        -332.5861, -358.9858, -352.5422, -355.9060, -357.5187, -357.5862,\n",
            "        -328.5905, -360.8700, -374.6592, -315.2870, -334.4438, -309.5535,\n",
            "        -310.6011, -323.5538, -319.4010, -325.4479, -350.4015, -349.5295,\n",
            "        -355.1384, -339.0177, -345.2539, -339.6279, -352.9780, -359.1697,\n",
            "        -370.7547, -356.7698, -348.1023, -321.0838, -357.2729, -366.0288,\n",
            "        -307.7050, -346.3806, -300.1127, -357.5944, -359.0450, -340.7006,\n",
            "        -362.5118, -361.4912, -351.7687, -359.1475, -319.1641, -357.1359,\n",
            "        -342.8839, -299.1973, -365.6984, -355.8102, -356.1052, -350.1436,\n",
            "        -371.7600, -359.0878, -355.5079, -357.5496, -347.7209, -337.9030,\n",
            "        -349.3172, -355.9860, -328.1210, -302.0282, -285.9925, -338.2097,\n",
            "        -326.9312, -347.1341, -353.9823, -366.0383, -335.7341, -325.8553,\n",
            "        -358.9289, -298.3279, -358.5676, -357.5364, -347.2319, -359.1905,\n",
            "        -345.1737, -335.6618, -358.3268, -330.1920, -344.3203, -365.9730,\n",
            "        -342.1042, -364.3734, -335.4266, -356.7322, -330.8953, -340.8107,\n",
            "        -359.7008, -359.1743, -350.0057, -346.1571, -342.8085, -290.9596,\n",
            "        -342.8330, -361.3156, -349.5904, -335.0185, -340.0653, -341.4582],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.03794703632593155 0.4950000047683716 0.0 345.2597961425781\n",
            "tensor([-340.3679, -290.3039, -340.3922, -354.9747, -354.1703, -339.6800,\n",
            "        -342.4302, -283.2106, -251.6497, -269.4460, -339.5860, -353.5434,\n",
            "        -303.8919, -265.6398, -346.1790, -338.9976, -346.1914, -331.0006,\n",
            "        -339.0467, -320.9857, -313.8898, -345.3014, -322.9633, -302.3211,\n",
            "        -312.7203, -335.4686, -340.3817, -270.3206, -346.7184, -280.0731,\n",
            "        -340.3900, -357.4044, -343.7310, -343.2117, -346.9050, -328.1909,\n",
            "        -338.3849, -335.8179, -330.3632, -309.7260, -328.4631, -312.1245,\n",
            "        -336.4805, -341.9407, -277.5947, -327.9763, -347.4813, -364.5757,\n",
            "        -336.4602, -352.5347, -351.6224, -353.8424, -297.1517, -340.4386,\n",
            "        -331.4658, -252.8435, -277.9049, -270.1658, -318.8097, -352.8105,\n",
            "        -317.3366, -283.3489, -351.2217, -285.3507, -321.3495, -336.7562,\n",
            "        -325.1805, -351.7350, -309.8239, -346.9042, -306.9017, -283.8946,\n",
            "        -351.5418, -301.1998, -329.2596, -291.9549, -318.5839, -333.2177,\n",
            "        -346.6859, -303.8965, -346.9549, -358.3028, -340.4259, -257.3413,\n",
            "        -328.2038, -319.8817, -350.8448, -291.1329, -345.9774, -306.1651,\n",
            "        -325.1064, -282.0690, -326.0957, -334.8757, -325.5900, -276.3963,\n",
            "        -344.5678, -342.1714, -332.8670, -312.0514, -345.1645, -346.7755,\n",
            "        -348.2602, -304.0004, -299.7895, -333.2188, -346.1854, -342.8321,\n",
            "        -315.9934, -284.1671, -314.8572, -312.9786, -346.2916, -349.7974,\n",
            "        -333.8791, -332.7887, -366.3222, -336.4308, -315.5609, -313.6257,\n",
            "        -343.9996, -333.4540, -346.7169, -354.9565, -344.2155, -260.5480,\n",
            "        -338.7089, -346.0917, -346.7747, -347.0410, -344.3282, -328.2637,\n",
            "        -331.2469, -343.6450, -269.0750, -332.9109, -343.1824, -323.3130,\n",
            "        -303.8968, -249.6917, -336.4807, -278.9348, -345.6323, -304.3436,\n",
            "        -340.2789, -263.1066, -287.4470, -348.7057, -341.9101, -277.3041,\n",
            "        -346.7301, -355.0862, -269.8348, -298.6163, -345.2127, -285.6789,\n",
            "        -333.9335, -320.9280, -350.7465, -340.7065, -346.8845, -320.0964,\n",
            "        -355.0553, -344.7131, -351.7383, -308.2964, -350.0835, -344.2192,\n",
            "        -327.1044, -316.3032, -351.3111, -298.5587, -343.6920, -319.0202,\n",
            "        -325.5637, -337.2376, -346.4016, -359.9489, -323.8102, -321.5083,\n",
            "        -323.3150, -343.8741, -340.3885, -353.9348, -336.6897, -343.2007,\n",
            "        -343.7579, -293.0886, -308.3007, -305.0325, -340.8800, -345.6200,\n",
            "        -341.9108, -300.0259, -335.4185, -322.5723, -320.8266, -326.9438,\n",
            "        -284.1717, -277.5675, -264.9367, -328.2160, -341.9262, -278.8306,\n",
            "        -303.4249, -325.8898, -305.0248, -303.4137, -339.9813, -360.9055,\n",
            "        -345.0769, -351.7469, -291.9572, -347.4349, -336.2919, -320.2171,\n",
            "        -287.0666, -284.0381, -327.6768, -324.6140, -360.1689, -351.1272,\n",
            "        -346.7496, -326.5500, -346.9340, -337.2739, -346.7365, -353.7710,\n",
            "        -345.5142, -346.9380, -343.7487, -283.5989, -291.2397, -325.8174,\n",
            "        -345.3147, -306.0828, -343.7680, -336.9825, -347.1572, -273.1645,\n",
            "        -345.6945, -320.2237, -327.6445, -325.6722, -339.3649, -279.1153,\n",
            "        -321.4862, -353.7932, -303.1160, -341.0304, -344.1059, -308.7193,\n",
            "        -351.2976, -299.6552, -345.2042, -315.1795, -338.7007, -338.9541,\n",
            "        -313.9622, -336.6936, -340.5145, -351.0228, -319.4243, -330.1628,\n",
            "        -351.5432, -342.2314, -347.0839, -303.1259, -343.1706, -338.3710,\n",
            "        -346.9176, -300.3836, -346.9561, -349.2321, -345.8860, -353.9243,\n",
            "        -320.1018, -346.3295, -319.7234, -346.2609, -260.1583, -312.0951,\n",
            "        -330.3436, -319.4303, -317.9448, -306.7065, -345.6986, -328.4871,\n",
            "        -317.3389, -346.6936, -346.8059, -343.0793, -346.8458, -338.9279,\n",
            "        -324.1716, -345.4017, -246.4931, -345.9239, -313.4017, -305.0437,\n",
            "        -350.0052, -354.2978, -268.5552, -346.1315, -340.3011, -352.1876,\n",
            "        -347.4061, -323.3516, -293.8345, -345.2000, -351.5172, -347.1540,\n",
            "        -277.7944, -321.1740, -346.9517, -308.9354, -341.5537, -325.7008,\n",
            "        -342.4764, -321.3012, -338.6569, -351.3354, -340.2469, -344.1823,\n",
            "        -356.5717, -339.3454, -336.2523, -320.9391, -275.1982, -313.8597,\n",
            "        -342.4586, -338.7284, -348.8212, -308.5419, -338.7419, -345.5967,\n",
            "        -343.5380, -340.7885, -343.6941, -352.2234, -345.5298, -334.2009,\n",
            "        -297.0927, -319.8094, -272.5192, -276.6040, -343.7239, -286.8991,\n",
            "        -308.6514, -274.2312, -350.9240, -322.7560, -347.1978, -342.8877,\n",
            "        -346.8166, -340.9042, -345.5924, -346.2960, -308.5728, -343.5284,\n",
            "        -278.8312, -328.2189, -349.4006, -288.5255, -331.2879, -286.6195,\n",
            "        -350.5631, -351.0953, -311.2437, -340.7860, -355.1267, -343.3811,\n",
            "        -346.1735, -341.0933, -329.0815, -351.6118, -253.8016, -312.4090,\n",
            "        -346.8234, -290.3012, -360.0721, -273.2943, -342.4839, -351.6160,\n",
            "        -320.7297, -277.2118, -353.8448, -343.1839, -257.6102, -303.5772,\n",
            "        -347.1093, -312.6965, -277.7993, -282.8259, -345.2083, -304.2432,\n",
            "        -276.5565, -314.2426, -329.7341, -296.2411, -343.4506, -320.9282,\n",
            "        -346.7931, -345.6141, -296.2141, -349.6980, -357.6302, -353.7843,\n",
            "        -312.0571, -315.5121, -321.4278, -353.7053, -349.9380, -345.3015,\n",
            "        -351.2095, -360.3876, -339.7124, -345.4182, -320.3384, -340.2810,\n",
            "        -300.2651, -346.7683, -360.5236, -328.1522, -263.0175, -309.2704,\n",
            "        -346.2449, -267.4309, -345.4930, -352.8937, -342.6254, -302.9807,\n",
            "        -344.2037, -366.7723, -349.2944, -308.5625, -312.0429, -346.9196,\n",
            "        -347.5066, -333.4535, -267.2517, -272.8940, -343.5560, -353.6791,\n",
            "        -344.2842, -271.6587, -346.8064, -345.2851, -345.1833, -340.0967,\n",
            "        -338.7416, -355.0262, -342.2193, -343.4818, -344.3120, -306.3885,\n",
            "        -340.2572, -286.3974, -346.9115, -281.3534, -359.3936, -319.4101,\n",
            "        -347.1578, -359.4312, -347.1113, -336.4505, -329.6559, -280.8414,\n",
            "        -354.3326, -351.2884, -347.0042, -339.9526, -273.6187, -283.8205,\n",
            "        -329.9077, -285.5687, -342.7437, -270.2626, -347.4039, -307.1830,\n",
            "        -345.2022, -258.1112, -281.3473, -301.2789, -311.2456, -340.1275,\n",
            "        -345.4473, -343.2586, -305.1777, -341.4934, -331.7078, -251.4301,\n",
            "        -351.1788, -303.8603, -337.4682, -346.7772, -346.8008, -336.4031,\n",
            "        -311.7225, -321.4396, -350.0857, -341.9263, -353.2366, -354.7878,\n",
            "        -313.6360, -317.1552, -346.9199, -306.0961, -304.4818, -293.4610,\n",
            "        -347.0280, -335.6086], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.03596193343400955 0.4950000047683716 0.0 326.22412109375\n",
            "tensor([-335.3347, -311.9184, -335.0745, -298.3016, -324.5918, -326.7707,\n",
            "        -320.4139, -312.2643, -285.8641, -322.9437, -333.1985, -296.5333,\n",
            "        -314.0432, -310.5247, -338.1702, -332.1237, -339.7065, -327.5822,\n",
            "        -324.7859, -307.9304, -336.0082, -327.0794, -330.6344, -257.9144,\n",
            "        -278.6511, -274.6439, -335.5407, -340.6326, -323.2365, -291.6232,\n",
            "        -334.4511, -308.2328, -325.8688, -340.0405, -314.8778, -268.9270,\n",
            "        -346.5514, -319.5459, -317.5230, -322.2523, -265.4365, -289.8628,\n",
            "        -251.6445, -228.4654, -312.0939, -323.6399, -315.8307, -320.5945,\n",
            "        -326.1742, -311.7016, -337.6342, -315.8907, -260.1448, -280.8159,\n",
            "        -313.8147, -312.0941, -288.8805, -239.1508, -331.4190, -279.3949,\n",
            "        -286.5143, -252.1274, -324.6095, -335.9643, -339.1575, -331.8769,\n",
            "        -329.0963, -325.8691, -309.7370, -333.9443, -341.2751, -310.1908,\n",
            "        -274.1945, -323.3711, -335.3333, -329.6875, -316.4411, -340.9269,\n",
            "        -332.9983, -353.9413, -333.0373, -280.0371, -338.9523, -312.3705,\n",
            "        -339.3260, -327.2682, -335.1741, -335.6270, -252.3862, -296.4826,\n",
            "        -335.5062, -288.6895, -339.5608, -330.1249, -325.8663, -269.7043,\n",
            "        -287.6207, -331.6686, -324.5963, -298.9439, -344.6327, -315.6062,\n",
            "        -336.0001, -301.5954, -298.1293, -291.9314, -335.1627, -246.8546,\n",
            "        -313.6534, -237.1203, -313.1048, -323.2709, -286.2748, -330.8509,\n",
            "        -284.4350, -302.5595, -311.0210, -335.0533, -255.7629, -270.8924,\n",
            "        -260.1747, -312.9655, -335.1767, -265.5513, -299.8717, -283.5425,\n",
            "        -335.1801, -339.4486, -335.2087, -296.9792, -335.5430, -325.9625,\n",
            "        -266.3416, -335.4631, -339.1276, -331.6761, -297.7153, -312.2809,\n",
            "        -337.8597, -332.2282, -315.1502, -276.6270, -335.1921, -329.7487,\n",
            "        -325.4295, -271.0499, -331.2959, -307.6008, -344.3594, -310.9688,\n",
            "        -317.5513, -333.2842, -331.8513, -310.7850, -334.0435, -241.7554,\n",
            "        -350.5849, -246.7079, -328.3872, -309.3173, -315.6110, -308.9674,\n",
            "        -323.9925, -317.5420, -325.8724, -335.0664, -295.0513, -335.7672,\n",
            "        -295.5461, -300.0314, -321.1325, -250.7395, -332.8841, -301.8730,\n",
            "        -326.8587, -335.3001, -325.7823, -266.3902, -316.2904, -312.4164,\n",
            "        -347.2147, -335.2200, -341.2255, -323.0861, -275.5022, -300.6389,\n",
            "        -310.6808, -243.0435, -295.6098, -263.9139, -333.8048, -347.4399,\n",
            "        -332.8665, -323.9758, -311.3180, -337.3792, -299.1281, -324.1169,\n",
            "        -308.0595, -306.2620, -227.2705, -283.7281, -324.8876, -292.6106,\n",
            "        -341.1807, -261.4070, -338.1116, -249.8731, -335.3257, -327.2790,\n",
            "        -324.2159, -327.5508, -287.5360, -325.3850, -323.5195, -324.7848,\n",
            "        -283.6683, -291.5630, -331.7204, -258.4642, -274.6387, -277.4006,\n",
            "        -327.1021, -316.7440, -287.7186, -329.2450, -251.1005, -327.8605,\n",
            "        -342.3675, -326.1136, -351.9429, -335.5240, -273.8668, -286.9358,\n",
            "        -339.5823, -322.5679, -341.0973, -233.5653, -333.4262, -302.4241,\n",
            "        -313.9169, -339.2558, -326.9019, -340.7524, -249.9767, -309.7464,\n",
            "        -328.3030, -329.1345, -228.2048, -335.5148, -315.9309, -281.5982,\n",
            "        -335.3336, -303.6340, -332.5648, -347.9070, -318.1334, -292.5435,\n",
            "        -331.0182, -297.0136, -339.8472, -334.6165, -343.8152, -296.9763,\n",
            "        -289.2897, -339.4121, -327.2575, -339.0557, -332.5074, -341.2844,\n",
            "        -338.6038, -286.4951, -326.0494, -311.0135, -329.0619, -314.9635,\n",
            "        -339.6831, -329.2226, -339.2693, -266.0569, -318.2046, -326.7705,\n",
            "        -287.3739, -330.1337, -353.9583, -296.7842, -323.0483, -335.3476],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, closs 0.03446079045534134 0.4950000047683716 0.0 312.1856994628906\n",
            "tensor([-311.9980, -325.0564, -321.7753, -328.0831, -296.8752, -320.9508,\n",
            "        -313.7197, -287.5485, -272.7066, -321.8403, -320.1923, -326.7749,\n",
            "        -317.0418, -268.8696, -275.3515, -323.6905, -332.4160, -301.3061,\n",
            "        -319.9600, -316.3484, -271.9289, -301.9153, -321.3524, -291.2409,\n",
            "        -328.7720, -325.9872, -316.5327, -300.8704, -316.7596, -275.2318,\n",
            "        -252.6655, -311.4248, -325.6378, -321.8105, -322.3580, -282.6485,\n",
            "        -322.6169, -310.4868, -322.2646, -321.2681, -274.2792, -321.6306,\n",
            "        -318.7043, -308.8330, -296.3127, -323.5113, -326.1222, -314.4763,\n",
            "        -330.3599, -309.8717, -264.0414, -332.5013, -302.9040, -313.6685,\n",
            "        -322.2663, -314.3017, -326.8273, -308.6953, -320.3651, -326.0596,\n",
            "        -325.3264, -304.6595, -327.1613, -320.3239, -257.0009, -310.7785,\n",
            "        -310.8347, -243.0512, -326.4050, -312.7729, -314.8929, -325.9231,\n",
            "        -323.3927, -322.1513, -296.3555, -309.2334, -264.1693, -272.8992,\n",
            "        -255.3530, -321.1794, -297.0721, -320.2156, -321.2521, -298.5306,\n",
            "        -305.6593, -282.2174, -309.2913, -312.8117, -324.2415, -321.5134,\n",
            "        -321.2625, -325.5258, -294.0590, -208.7485, -288.0856, -321.3358,\n",
            "        -308.9099, -301.3065, -320.3742, -302.1092, -314.0283, -288.6980,\n",
            "        -321.8467, -281.4868, -292.7502, -321.3212, -311.4937, -295.0200,\n",
            "        -313.9169, -271.7151, -245.1031, -321.6354, -317.1669, -318.3728,\n",
            "        -310.6902, -256.8324, -279.6880, -321.2224, -320.3974, -300.1631,\n",
            "        -308.0356, -318.1310, -321.4160, -270.4859, -326.9312, -258.5752,\n",
            "        -203.4453, -314.3619, -336.8813, -316.3637, -318.9600, -302.8696,\n",
            "        -339.7042, -312.7706, -301.5716, -322.3716, -290.7588, -322.1635,\n",
            "        -320.2340, -325.8829, -247.2869, -280.0872, -218.9729, -311.4756,\n",
            "        -291.9754, -320.3779, -321.7990, -305.8614, -333.0998, -300.9225,\n",
            "        -278.0249, -291.3295, -281.8414, -310.9574, -319.7688, -324.2179,\n",
            "        -256.6460, -238.9799, -226.5286, -278.9170, -332.6207, -313.7390,\n",
            "        -297.7235, -277.8806, -326.4467, -285.9407, -321.4158, -319.7747,\n",
            "        -326.9002, -322.9609, -319.7847, -285.0647, -326.7842, -219.1814,\n",
            "        -307.0999, -312.7680, -324.9586, -321.5839, -320.6689, -286.2312,\n",
            "        -285.6062, -236.9857, -322.3030, -322.2604, -311.9303, -323.6586,\n",
            "        -321.7810, -316.0215, -242.1114, -300.7448, -204.3686, -290.4742,\n",
            "        -273.5759, -300.6949, -320.7717, -328.5001, -259.8986, -277.7978,\n",
            "        -292.9196, -322.3123, -274.7749, -324.0315, -322.0148, -327.2804,\n",
            "        -286.8509, -318.1585, -272.7274, -283.1479, -318.2644, -321.3903,\n",
            "        -311.3650, -327.5289, -318.3376, -324.7299, -292.1558, -322.2986,\n",
            "        -247.9430, -313.1164, -319.6000, -327.2437, -233.5979, -215.3717,\n",
            "        -242.4036, -322.0036, -293.0043, -321.8333, -321.4262, -329.3155,\n",
            "        -312.5008, -311.4553, -324.1044, -298.8123, -326.7151, -316.5233,\n",
            "        -297.0195, -322.3773, -315.4853, -316.9541, -252.1638, -312.9885,\n",
            "        -293.3831, -321.9868, -325.0468, -296.2150, -325.9265, -320.4012,\n",
            "        -309.2255, -310.7863, -326.5510, -311.3725, -317.7606, -244.4885,\n",
            "        -270.4480, -323.7886, -312.3904, -283.5303, -296.7866, -319.9406,\n",
            "        -312.7425, -333.4440, -288.9753, -269.1841, -286.6803, -322.2661,\n",
            "        -303.9073, -322.2644, -323.4818, -321.2423, -296.4389, -230.1632,\n",
            "        -279.6842, -321.6298, -306.6214, -326.0537, -321.3075, -322.2829,\n",
            "        -266.7985, -315.3718, -321.2793, -285.7026, -256.4566, -321.2646,\n",
            "        -266.0785, -321.3136, -301.0583, -268.8324, -277.4482, -277.7513,\n",
            "        -292.4555, -309.0038, -325.0798, -325.7871, -310.8734, -310.9816,\n",
            "        -273.8890, -268.9415, -324.2389, -320.3210, -278.1900, -322.3807,\n",
            "        -301.0603, -316.8323, -222.0423, -322.2316, -266.5617, -321.5065,\n",
            "        -321.1268, -332.9819, -284.9337, -250.0887, -320.1370, -321.6243,\n",
            "        -275.2072, -321.6350, -311.9788, -286.0076, -316.3518, -296.3068,\n",
            "        -279.6259, -300.4717, -321.7804, -325.9629, -322.2563, -322.3301,\n",
            "        -322.2857, -259.6512, -280.6982, -311.1269, -306.7201, -309.2273,\n",
            "        -314.8445, -322.0144, -276.0403, -327.1961, -274.7910, -322.3866,\n",
            "        -316.6685, -321.8026, -323.5029, -245.1133, -276.2805, -322.2896,\n",
            "        -272.1750, -270.1003, -327.0098, -297.8577, -309.8198, -317.8901,\n",
            "        -286.5468, -297.5604, -258.6656, -322.2867, -284.9978, -280.9505,\n",
            "        -255.9955, -302.2455, -330.9209, -321.2577, -300.8242, -314.1251,\n",
            "        -314.4385, -310.2527, -326.8908, -299.6679, -290.9512, -299.7900,\n",
            "        -264.7615, -316.5803, -271.9202, -311.8284, -324.9664, -270.3647,\n",
            "        -288.2749, -305.4444, -322.2643, -278.5547, -313.7678, -325.7496,\n",
            "        -279.0192, -322.5987, -302.3674, -324.5930, -261.7879, -245.1393,\n",
            "        -282.3491, -325.5226, -325.9016, -294.5430, -281.8979, -319.1059,\n",
            "        -314.9282, -321.1370, -274.4718, -322.0021, -327.2503, -322.8804,\n",
            "        -317.8548, -307.5389, -226.5949, -316.6949, -267.8930, -301.3089,\n",
            "        -323.8958, -315.2379, -265.7193, -289.5033, -325.9633, -311.3600,\n",
            "        -286.9982, -310.9484, -326.0403, -322.3797, -316.6686, -323.3903,\n",
            "        -319.6913, -296.9529, -293.4821, -321.7914, -325.9029, -318.9515,\n",
            "        -254.5921, -322.3156, -233.7417, -325.9339, -292.8440, -321.5960,\n",
            "        -314.5280, -319.8857, -287.3808, -249.9399, -264.2610, -321.9648,\n",
            "        -321.7680, -325.9396, -319.0968, -310.7003, -294.2961, -321.3027,\n",
            "        -311.4474, -322.0364, -324.2082, -311.5519, -286.5910, -309.8279,\n",
            "        -323.7885, -249.0499, -283.5846, -309.4912, -226.1257, -310.6070,\n",
            "        -292.8903, -289.8727, -317.5294, -285.7960, -320.3775, -290.3214,\n",
            "        -233.5363, -320.3284, -316.1537, -322.1945, -257.1066, -243.1974,\n",
            "        -295.7201, -310.2523, -337.1898, -321.3538, -297.6529, -301.7673,\n",
            "        -331.5858, -322.3890, -301.9305, -291.2884, -281.1809, -293.7810,\n",
            "        -278.1893, -313.8152, -256.7760, -280.0585, -203.6839, -314.5051,\n",
            "        -313.1472, -325.1313, -316.0642, -268.2898, -325.4271, -322.2137,\n",
            "        -319.4066, -312.8705, -284.2133, -319.6996, -288.8492, -296.9026,\n",
            "        -295.2460, -315.8410, -261.8792, -273.3576, -287.1751, -311.2434,\n",
            "        -320.3665, -293.8081, -326.4795, -319.3580, -319.6833, -230.7623,\n",
            "        -324.1754, -321.5575, -326.0884, -321.3244, -281.6229, -326.1646,\n",
            "        -216.7444, -315.6249], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.032204531133174896 0.4950000047683716 0.0 301.99658203125\n",
            "tensor([-273.9983, -300.4826, -306.7569, -286.0541, -284.4297, -307.5621,\n",
            "        -274.8746, -293.1841, -311.7290, -307.2081, -259.5013, -307.0432,\n",
            "        -248.6614, -306.4827, -241.8644, -300.6919, -252.3182, -307.5055,\n",
            "        -294.4526, -321.9910, -299.9373, -238.2876, -305.5388, -307.0468,\n",
            "        -311.8210, -308.4153, -311.6816, -311.7119, -246.6719, -262.3151,\n",
            "        -239.3425, -306.2704, -271.8013, -306.6182, -297.5512, -322.6216,\n",
            "        -319.5161, -306.6745, -271.3581, -307.3859, -308.8027, -306.6255,\n",
            "        -253.5213, -306.6134, -235.6009, -299.3754, -296.8277, -243.5824,\n",
            "        -267.4803, -307.3274, -305.4962, -314.9914, -222.1921, -307.7498,\n",
            "        -298.3157, -307.1258, -257.0434, -305.5042, -307.2760, -304.3665,\n",
            "        -277.5876, -311.7506, -212.0329, -297.3599, -260.3807, -229.1405,\n",
            "        -306.8160, -243.9490, -295.3393, -307.0337, -306.8029, -305.4866,\n",
            "        -291.4957, -306.3192, -236.8561, -282.6240, -302.0665, -265.1142,\n",
            "        -258.6963, -306.8228, -317.5973, -242.6203, -307.1054, -299.0580,\n",
            "        -263.9796, -306.6378, -262.0522, -293.1754, -297.9412, -277.9770,\n",
            "        -289.6712, -295.3589, -290.0366, -311.7188, -297.5892, -305.2477,\n",
            "        -310.8875, -294.9072, -307.5703, -312.0982, -228.0457, -267.0035,\n",
            "        -297.6171, -295.7000, -311.7164, -274.7608, -227.4465, -300.4906,\n",
            "        -273.6778, -311.7975, -298.9982, -309.3667, -302.4560, -305.4996,\n",
            "        -307.5202, -313.6798, -310.4956, -307.0670, -305.5991, -301.1427,\n",
            "        -265.8491, -243.1488, -298.6424, -305.4193, -310.1439, -307.9318,\n",
            "        -230.0193, -307.0204, -316.4923, -311.7092, -283.7714, -296.8267,\n",
            "        -216.9288, -253.1803, -267.8316, -295.0180, -285.8349, -295.7856,\n",
            "        -310.3899, -307.4492, -282.0734, -248.4194, -192.7622, -295.7988,\n",
            "        -305.8700, -266.8509, -307.8618, -254.9589, -258.9806, -307.0441,\n",
            "        -305.6437, -307.0076, -232.7672, -307.6299, -286.5300, -274.5728,\n",
            "        -235.6866, -311.7145, -310.3591, -307.1265, -310.0914, -295.1316,\n",
            "        -295.1706, -271.0525, -281.0810, -295.2781, -307.0188, -277.4050,\n",
            "        -289.8217, -306.7574, -291.9587, -251.4503, -300.7096, -279.4741,\n",
            "        -251.4649, -236.9279, -297.3857, -300.8039, -283.8408, -298.6447,\n",
            "        -222.6031, -296.9839, -305.6299, -298.7679, -228.0177, -307.4356,\n",
            "        -295.8277, -271.3320, -306.6715, -311.8117, -284.2998, -295.2193,\n",
            "        -277.2866, -301.4456, -309.1064, -296.0861, -308.7702, -249.4474,\n",
            "        -297.6024, -307.6696, -311.7791, -305.1100, -308.6872, -306.4980,\n",
            "        -296.9953, -233.9639, -218.2783, -307.5024, -292.6855, -311.7876,\n",
            "        -307.6202, -300.5874, -216.6771, -304.3849, -307.1501, -307.3504,\n",
            "        -301.1467, -306.9467, -267.3296, -306.9815, -282.6350, -232.8436,\n",
            "        -308.5348, -307.7435, -271.5749, -306.3886, -306.3126, -319.1200,\n",
            "        -268.6351, -280.0518, -298.9960, -309.7293, -292.5487, -248.0396,\n",
            "        -276.1217, -290.4207, -307.1169, -310.1290, -289.8870, -306.4077,\n",
            "        -211.2195, -296.8577, -299.0434, -263.0987, -311.6074, -296.2713,\n",
            "        -266.2217, -304.7668, -264.3489, -299.9811, -282.3275, -266.9834,\n",
            "        -293.0054, -307.5138, -190.7042, -282.8154, -306.3925, -292.3550,\n",
            "        -305.0740, -259.2547, -311.7806, -307.1449, -307.0203, -275.5277,\n",
            "        -296.7419, -306.9871, -306.4225, -274.5918, -295.3796, -275.5413,\n",
            "        -183.7341, -305.5998, -314.5542, -305.5946, -307.9792, -266.8738,\n",
            "        -309.9438, -307.1699, -295.8471, -287.1662, -292.6519, -307.4639,\n",
            "        -271.6181, -284.8286, -295.0375, -189.7331, -272.1613, -294.9851],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.030083220452070236 0.4950000047683716 0.0 287.556640625\n",
            "#### simulate ####\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -1.3957530744988728e+23\n",
            "search -1.3957530744988728e+23\n",
            "search -1.5552307614796298e+23\n",
            "search -2.187905804619862e+23\n",
            "search -1.5552307614796298e+23\n",
            "search -1.5552307614796298e+23\n",
            "search -1.3957530744988728e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -1.3957530744988728e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -1.3957530744988728e+23\n",
            "search -1.5552307614796298e+23\n",
            "search -1.3957530744988728e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -1.106563251482546e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -1.3957530744988728e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -1.106563251482546e+23\n",
            "search -2.187905804619862e+23\n",
            "search -1.3957530744988728e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -2.187905804619862e+23\n",
            "search -1.3957530744988728e+23\n",
            "search -1.106563251482546e+23\n",
            "search -2.187905804619862e+23\n",
            "ded\n",
            "time\n",
            "9 #### train ####\n",
            "tensor([-191.1706, -286.7133, -261.4235, -289.8040, -289.9269, -211.9230,\n",
            "        -291.0926, -286.9513, -291.3925, -292.2035, -290.8125, -299.0454,\n",
            "        -287.6685, -227.2740, -290.8533, -278.3581, -249.0685, -300.1689,\n",
            "        -287.1350, -252.5838, -287.7724, -294.0404, -276.5610, -243.4817,\n",
            "        -223.8586, -274.4572, -225.5951, -272.2382, -240.2338, -213.7512,\n",
            "        -276.6473, -290.9084, -241.8003, -249.2968, -289.4687, -288.8098,\n",
            "        -289.0055, -290.1322, -283.8298, -261.0519, -290.5021, -250.3902,\n",
            "        -279.5890, -237.7299, -290.8649, -282.8340, -290.0863, -275.3338,\n",
            "        -292.0510, -294.0153, -290.8679, -279.8282, -290.2361, -284.0095,\n",
            "        -234.9694, -280.8760, -291.6420, -257.8274, -289.7393, -287.9467,\n",
            "        -273.9810, -248.1322, -288.3386, -290.8900, -288.7363, -240.2068,\n",
            "        -289.4991, -277.8413, -264.0884, -248.7894, -260.2686, -283.8261,\n",
            "        -290.7856, -237.5287, -290.6516, -205.6965, -287.1081, -236.3427,\n",
            "        -281.2693, -288.8885, -235.9266, -300.2141, -213.5146, -292.4064,\n",
            "        -288.3249, -283.5137, -275.1711, -253.8497, -266.3463, -264.2068,\n",
            "        -230.0594, -306.0202, -256.4020, -220.8479, -258.9765, -290.7937,\n",
            "        -264.2871, -294.4686, -288.5679, -255.6851, -244.6563, -217.0166,\n",
            "        -288.6543, -271.7517, -287.1776, -289.9237, -292.3254, -292.2478,\n",
            "        -279.6499, -232.9562, -247.5437, -280.7882, -301.3756, -290.8501,\n",
            "        -290.8948, -278.3421, -289.4706, -247.2633, -253.5087, -279.5245,\n",
            "        -292.3468, -274.6332, -289.9133, -252.6094, -276.0221, -164.5124,\n",
            "        -274.2413, -292.2997, -303.5150, -244.1303, -283.2838, -288.3243,\n",
            "        -280.7670, -279.9167, -290.8063, -292.4366, -289.2784, -230.8676,\n",
            "        -276.1021, -279.0134, -277.8683, -279.3737, -278.6722, -289.0115,\n",
            "        -271.9185, -293.5271, -264.1688, -290.8069, -227.1633, -294.2662,\n",
            "        -290.5361, -287.8442, -289.8556, -255.7830, -172.2020, -218.4401,\n",
            "        -289.3861, -261.0478, -289.3318, -255.5085, -296.8700, -272.4629,\n",
            "        -288.6552, -276.2151, -276.3291, -265.8175, -289.0330, -292.2450,\n",
            "        -279.5997, -301.3312, -241.2468, -300.3732, -280.0356, -258.7920,\n",
            "        -277.0485, -289.9547, -300.0233, -224.4822, -290.8333, -277.8201,\n",
            "        -287.0428, -260.9377, -288.3247, -289.5529, -290.7947, -262.1135,\n",
            "        -264.5632, -292.3411, -288.3249, -158.2298, -277.6095, -287.1684,\n",
            "        -303.2556, -285.0164, -230.1017, -206.7182, -289.9247, -221.9893,\n",
            "        -292.3598, -294.2828, -289.9166, -303.4709, -232.9841, -247.3793,\n",
            "        -276.9300, -267.2003, -291.1117, -275.6595, -303.5787, -201.2499,\n",
            "        -290.8623, -286.3076, -282.7041, -198.0648, -289.7622, -275.1417,\n",
            "        -285.6747, -294.7542, -287.9700, -265.9283, -290.3547, -284.2379,\n",
            "        -278.0751, -279.8398, -235.2360, -275.2963, -262.5697, -290.2307,\n",
            "        -290.8502, -288.2852, -277.5114, -290.8893, -287.9648, -301.6912,\n",
            "        -290.7119, -227.5255, -290.8924, -279.2052, -288.7440, -254.5556,\n",
            "        -263.7204, -258.7683, -290.4474, -285.6989, -279.7749, -287.9888,\n",
            "        -291.1048, -288.1550, -290.9276, -290.9141, -238.9617, -184.9491,\n",
            "        -280.0144, -231.6275, -245.1317, -286.2820, -268.1708, -285.7544,\n",
            "        -290.6938, -214.2759, -292.3946, -224.5366, -276.7555, -289.4066,\n",
            "        -275.3241, -287.0771, -257.0706, -218.0184, -277.2900, -255.9115,\n",
            "        -288.0672, -252.0631, -292.2393, -261.4762, -287.9708, -289.9323,\n",
            "        -288.3245, -274.7509, -284.3971, -283.3103, -290.9047, -247.6984,\n",
            "        -279.7498, -266.1563, -290.8563, -216.9948, -287.5639, -276.0125,\n",
            "        -287.6906, -253.8440, -279.7722, -281.0239, -287.6505, -193.7349,\n",
            "        -291.2017, -290.9167, -275.3282, -307.7632, -258.9056, -303.2704,\n",
            "        -290.8674, -292.0441, -290.9694, -290.5516, -238.9106, -283.7975,\n",
            "        -263.3970, -289.4663, -275.0738, -266.1594, -290.2816, -287.9136,\n",
            "        -253.2178, -235.3375, -275.2053, -293.5642, -281.2434, -236.9259,\n",
            "        -244.5534, -289.5744, -239.6712, -237.8467, -276.1067, -289.5979,\n",
            "        -264.1580, -204.3363, -277.1352, -283.3074, -289.9213, -245.0408,\n",
            "        -290.0638, -288.2110, -290.9361, -278.5318, -283.3931, -252.0746,\n",
            "        -291.0489, -242.9966, -278.0300, -277.4513, -290.7080, -293.4162,\n",
            "        -288.3350, -290.8810, -290.8100, -260.7924, -261.1334, -280.7827,\n",
            "        -291.0392, -269.2112, -247.9290, -287.2957, -259.0213, -251.8552,\n",
            "        -287.9474, -288.7682, -290.2720, -220.8646, -289.2688, -289.5487,\n",
            "        -289.4795, -260.4673, -290.9120, -219.0981, -248.2226, -200.2912,\n",
            "        -289.3828, -289.0039, -279.7024, -224.3087, -283.3549, -285.7031,\n",
            "        -256.6845, -216.2998, -289.4136, -282.6819, -292.2555, -295.4846,\n",
            "        -241.4561, -239.2003, -288.7299, -228.4351, -278.5486, -289.3891,\n",
            "        -270.8923, -237.3088, -261.0088, -288.6351, -283.8702, -272.4966,\n",
            "        -271.9770, -242.0750, -292.2420, -273.8647, -262.2194, -302.3388,\n",
            "        -288.5269, -228.4029, -289.8359, -289.0280, -277.6032, -265.5642,\n",
            "        -280.8605, -288.9976, -251.6378, -257.0419, -259.0270, -287.8837,\n",
            "        -290.9288, -283.3334, -262.0976, -220.1997, -288.7605, -265.5695,\n",
            "        -290.5000, -276.2957, -195.0718, -290.7872, -290.8486, -252.2029,\n",
            "        -290.9372, -294.8128, -288.6705, -286.6772, -289.4715, -307.0703,\n",
            "        -290.5488, -251.5334, -280.8812, -281.7613, -287.3080, -289.4328,\n",
            "        -251.6333, -301.8344, -277.1411, -289.6407, -289.9833, -282.5801,\n",
            "        -278.3434, -294.2884, -222.9501, -217.6710, -294.6898, -249.7094,\n",
            "        -268.9660, -231.9151, -288.6586, -290.7785, -203.8600, -274.5488,\n",
            "        -290.8967, -263.0305, -272.0009, -274.9861, -254.5294, -290.6909,\n",
            "        -290.0818, -224.1213, -264.8161, -273.0217, -264.5275, -294.2638,\n",
            "        -290.9839, -279.8197, -248.2630, -292.4293, -290.9271, -285.8157,\n",
            "        -264.4503, -286.0925, -281.2054, -292.8074, -290.0031, -264.7349,\n",
            "        -283.3731, -252.2328, -290.9179, -207.7595, -276.8779, -278.0128,\n",
            "        -265.4727, -291.2980, -291.0463, -260.6739, -290.7917, -284.0528,\n",
            "        -288.7630, -276.7209, -276.2598, -296.5306, -289.4038, -199.9858,\n",
            "        -251.4144, -229.3456, -242.9293, -256.2577, -295.9514, -290.5502,\n",
            "        -290.8738, -292.3268, -287.4618, -274.9125, -259.2717, -293.3946,\n",
            "        -290.0923, -307.7572, -288.5061, -264.4537, -288.7586, -200.8565,\n",
            "        -280.8642, -251.4565], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.02888024039566517 0.4950000047683716 0.0 271.9813232421875\n",
            "tensor([-295.3628, -293.1037, -245.6562, -285.0582, -247.7193, -284.9529,\n",
            "        -282.8127, -287.9233, -282.4731, -300.7159, -282.7721, -280.8132,\n",
            "        -280.1210, -274.4258, -231.8528, -256.8490, -290.7350, -274.8853,\n",
            "        -285.3062, -282.8538, -281.4088, -287.3978, -283.7795, -268.2897,\n",
            "        -282.1937, -288.8384, -231.3019, -282.0750, -282.7328, -231.3830,\n",
            "        -253.0469, -279.7945, -229.4513, -288.8559, -280.9657, -284.5573,\n",
            "        -268.0057, -182.9047, -271.5283, -269.9393, -228.7875, -177.7020,\n",
            "        -281.2016, -214.4199, -281.8845, -277.9778, -266.9751, -282.1967,\n",
            "        -252.0366, -234.8502, -276.3862, -272.5936, -283.8556, -288.2969,\n",
            "        -253.5036, -220.5458, -260.3694, -288.6727, -283.4005, -257.1695,\n",
            "        -283.6133, -263.5200, -281.4093, -281.4681, -271.9209, -285.8419,\n",
            "        -280.8033, -282.7234, -225.5512, -245.9373, -280.5215, -285.3198,\n",
            "        -282.9892, -213.4600, -269.1571, -287.3370, -284.0423, -274.7926,\n",
            "        -280.2883, -280.1697, -269.0771, -273.3479, -240.8461, -281.1566,\n",
            "        -284.4413, -212.7121, -268.5086, -281.1285, -245.2230, -281.7559,\n",
            "        -279.5257, -255.6579, -283.2263, -252.4532, -249.8297, -281.8559,\n",
            "        -282.0697, -177.2452, -283.7804, -283.2235, -244.4314, -284.6473,\n",
            "        -282.0524, -284.0996, -283.0819, -287.3535, -287.4509, -250.4866,\n",
            "        -283.8252, -222.7295, -282.5527, -279.3115, -206.8733, -229.3252,\n",
            "        -283.6844, -266.9604, -280.5250, -253.4773, -274.7334, -253.5437,\n",
            "        -279.0155, -279.8090, -283.5994, -249.5295, -268.6600, -245.1175,\n",
            "        -272.9820, -249.2883, -300.7534, -215.8605, -280.4805, -285.3141,\n",
            "        -282.9791, -273.3159, -275.9195, -281.5517, -284.8605, -198.5802,\n",
            "        -281.9290, -227.6657, -284.8698, -182.8672, -283.3332, -283.8055,\n",
            "        -244.4561, -222.1642, -283.2570, -282.5395, -281.0000, -258.1395,\n",
            "        -271.4785, -282.7554, -242.8013, -249.8371, -266.9501, -257.0016,\n",
            "        -281.3697, -189.5785, -241.5060, -281.8412, -250.1138, -295.0188,\n",
            "        -272.7238, -242.1527, -283.2081, -287.4793, -282.7652, -268.4457,\n",
            "        -268.4806, -212.8800, -282.6204, -276.0890, -283.5946, -247.9484,\n",
            "        -267.5400, -239.0288, -231.7586, -272.4772, -282.4805, -281.9052,\n",
            "        -203.4033, -285.9038, -251.5889, -283.6821, -221.0070, -285.8891,\n",
            "        -256.0432, -220.7103, -236.1869, -169.1487, -250.0317, -282.9760,\n",
            "        -270.7415, -259.2926, -283.3058, -285.0876, -272.4854, -278.8512,\n",
            "        -282.7670, -269.4844, -283.6364, -243.2142, -246.8069, -261.0009,\n",
            "        -283.1393, -233.2964, -280.1766, -267.9523, -292.1109, -285.3577,\n",
            "        -281.8672, -264.2415, -283.2968, -248.5589, -256.4164, -184.0092,\n",
            "        -282.5077, -247.0081, -251.2027, -242.8442, -271.7877, -171.9796,\n",
            "        -283.6033, -283.8239, -250.2332, -244.0870, -281.4312, -281.0002,\n",
            "        -284.8389, -287.3431, -282.9088, -288.2432, -282.8141, -264.1985,\n",
            "        -281.1031, -271.8762, -282.7291, -192.0760, -261.2527, -282.6891,\n",
            "        -275.8640, -265.8096, -268.4361, -281.4270, -268.4949, -288.1941,\n",
            "        -276.6243, -240.6595, -272.7954, -276.0329, -284.4258, -283.1405,\n",
            "        -276.0553, -228.0980, -279.3708, -281.7505, -255.4765, -250.5718,\n",
            "        -283.6460, -282.1509, -187.4495, -287.5329, -281.3391, -283.7799,\n",
            "        -242.5918, -271.4398, -272.4250, -218.9063, -285.3749, -284.2531,\n",
            "        -282.3853, -255.8787, -284.7816, -288.0284, -215.7978, -244.9993,\n",
            "        -260.2310, -284.6234, -281.4706, -279.0414, -283.4057, -242.7714,\n",
            "        -279.4915, -282.2864, -247.7033, -197.9121, -275.8498, -283.5100],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.027115998789668083 0.4950000047683716 0.0 265.2942810058594\n",
            "tensor([-234.6826, -275.3375, -209.0307, -259.2411, -275.4798, -199.3597,\n",
            "        -256.8453, -233.6142, -274.6860, -274.8307, -209.8641, -279.9330,\n",
            "        -154.2637, -235.2659, -270.6559, -270.4381, -249.6614, -274.3014,\n",
            "        -273.6960, -248.6794, -272.5466, -273.8159, -200.4371, -239.6973,\n",
            "        -274.6629, -259.1469, -275.5845, -259.2554, -275.2651, -203.6950,\n",
            "        -272.8823, -262.9439, -173.4037, -278.3022, -273.3629, -239.1336,\n",
            "        -237.0793, -267.0137, -274.9751, -256.8478, -270.6769, -257.3063,\n",
            "        -244.0140, -279.8430, -279.2290, -229.7710, -274.9703, -258.8432,\n",
            "        -285.5180, -226.8053, -275.5195, -289.0500, -201.2436, -270.6723,\n",
            "        -216.1094, -229.8676, -198.8005, -227.0725, -272.5399, -263.4989,\n",
            "        -241.9748, -214.8497, -275.6874, -259.8521, -252.4324, -181.6247,\n",
            "        -271.5283, -194.2112, -274.8578, -262.6262, -271.9391, -233.9563,\n",
            "        -257.1481, -272.9749, -249.4189, -241.5063, -177.9320, -275.2712,\n",
            "        -258.0347, -219.9775, -254.5685, -221.5477, -266.3853, -214.0182,\n",
            "        -262.6658, -274.0183, -274.2224, -237.0097, -276.9951, -262.5616,\n",
            "        -274.9045, -276.5746, -251.4608, -275.9423, -274.7123, -275.4863,\n",
            "        -245.7298, -292.5718, -262.5932, -255.4924, -275.2101, -264.9669,\n",
            "        -270.9130, -225.0127, -275.5293, -273.3810, -274.6844, -219.5330,\n",
            "        -242.3329, -280.5144, -275.0716, -275.4951, -292.5894, -274.9810,\n",
            "        -274.7594, -256.9623, -262.5947, -275.5045, -275.3516, -192.6128,\n",
            "        -248.0158, -274.7890, -246.1700, -278.5422, -237.6054, -281.1076,\n",
            "        -275.3342, -270.7589, -279.9656, -237.8813, -255.5729, -267.7170,\n",
            "        -274.6899, -258.7490, -263.6993, -204.6439, -246.4561, -257.2502,\n",
            "        -259.2459, -259.6056, -266.2395, -207.2370, -270.5273, -270.8908,\n",
            "        -207.6591, -275.1105, -266.9851, -244.7571, -257.9866, -223.0687,\n",
            "        -275.3626, -276.6541, -277.8503, -271.4784, -208.0361, -272.5913,\n",
            "        -212.9631, -270.4623, -273.9621, -270.5372, -247.1130, -284.7050,\n",
            "        -228.2336, -259.4041, -274.9381, -273.1927, -258.9112, -201.5064,\n",
            "        -273.9432, -238.6315, -275.3841, -268.2290, -274.3051, -284.4864,\n",
            "        -257.9146, -275.2145, -257.6049, -285.4314, -247.2167, -235.1502,\n",
            "        -259.6950, -275.2970, -238.8517, -206.5077, -275.4224, -257.1458,\n",
            "        -276.9093, -257.1701, -275.9285, -283.7310, -197.9975, -259.9772,\n",
            "        -203.6917, -202.1096, -269.0828, -292.5697, -256.7887, -275.2758,\n",
            "        -220.7369, -249.4034, -272.7489, -275.0223, -274.3810, -275.4705,\n",
            "        -278.8793, -200.5422, -274.9043, -258.2273, -293.7810, -239.4060,\n",
            "        -272.4619, -189.1564, -242.2480, -257.9309, -275.3539, -218.5385,\n",
            "        -258.8192, -268.4713, -259.0483, -262.4603, -193.5266, -272.0514,\n",
            "        -275.0317, -275.4969, -227.9324, -174.1467, -273.4223, -283.2512,\n",
            "        -275.4947, -242.0720, -273.4303, -183.4848, -258.4281, -274.8953,\n",
            "        -273.3768, -259.6495, -270.8953, -226.7840, -275.1165, -226.6000,\n",
            "        -279.8150, -195.2191, -272.2155, -268.4181, -272.1726, -272.9753,\n",
            "        -266.6881, -267.3333, -223.8800, -276.7379, -275.3428, -213.6003,\n",
            "        -276.7152, -279.5316, -257.9163, -262.5266, -257.2631, -229.5422,\n",
            "        -257.9673, -225.4993, -272.9785, -239.2766, -232.7705, -198.5749,\n",
            "        -271.5130, -262.6974, -266.7247, -275.3127, -222.6829, -233.1795,\n",
            "        -262.1195, -274.9246, -272.2780, -207.0173, -275.2170, -280.7184,\n",
            "        -160.7686, -276.8312, -274.4877, -277.1730, -208.5243, -272.7150,\n",
            "        -274.9480, -232.0768, -202.9120, -236.6458, -275.4344, -273.9918,\n",
            "        -275.9445, -266.5320, -266.6438, -272.7200, -272.9999, -274.0259,\n",
            "        -275.7385, -170.3740, -158.5440, -230.4227, -274.8302, -259.9617,\n",
            "        -257.0845, -284.5841, -272.9722, -275.1580, -249.2763, -236.6054,\n",
            "        -276.5764, -284.6147, -260.0851, -268.2101, -218.2744, -236.0154,\n",
            "        -256.1659, -274.7968, -275.9198, -276.7720, -236.2826, -276.7144,\n",
            "        -275.2837, -270.8918, -247.0069, -286.5660, -234.5258, -263.5353,\n",
            "        -257.9450, -275.5616, -276.6240, -272.4644, -250.1886, -271.0200,\n",
            "        -271.5000, -272.5442, -278.8635, -217.9742, -272.7491, -272.9778,\n",
            "        -228.6635, -235.1948, -263.6354, -206.7278, -247.0353, -273.9254,\n",
            "        -258.8340, -247.3285, -232.9988, -274.6719, -272.5496, -256.5033,\n",
            "        -232.6412, -220.9411, -273.4358, -275.2082, -215.2870, -236.7940,\n",
            "        -273.5175, -274.8940, -275.1887, -246.4404, -275.3498, -275.9130,\n",
            "        -131.5531, -177.5855, -210.1188, -273.0403, -242.5686, -257.0457,\n",
            "        -272.9713, -275.9131, -263.2368, -242.6726, -275.1408, -252.6498,\n",
            "        -258.2348, -276.5938, -240.4464, -222.3102, -262.6661, -275.4297,\n",
            "        -276.6494, -274.8980, -246.8809, -176.8234, -224.7461, -245.8346,\n",
            "        -183.6000, -190.6849, -245.2430, -293.1452, -272.4706, -276.0157,\n",
            "        -274.8512, -264.8535, -238.2769, -276.7276, -274.9656, -249.6098,\n",
            "        -249.4544, -264.7534, -262.5005, -271.4908, -275.8669, -284.5957,\n",
            "        -259.0494, -260.8559, -235.0247, -271.8596, -214.6637, -257.9954,\n",
            "        -272.9719, -272.0582, -262.1112, -262.1088, -122.4277, -276.7258,\n",
            "        -275.2196, -235.9646, -270.6741, -247.8891, -250.1855, -275.2185,\n",
            "        -185.7370, -274.1187, -256.6528, -277.7881, -273.0607, -247.1128,\n",
            "        -262.9377, -270.5304, -228.5499, -250.9050, -272.1261, -262.6929,\n",
            "        -286.4730, -237.3313, -276.6041, -252.3092, -259.8345, -228.1884,\n",
            "        -275.4467, -224.6086, -258.9294, -274.9232, -275.1886, -274.8849,\n",
            "        -181.2298, -236.9010, -257.9757, -276.9058, -219.7590, -284.6618,\n",
            "        -274.9659, -203.0208, -274.7530, -272.5470, -160.7447, -238.3876,\n",
            "        -249.7164, -274.5711, -193.2247, -256.7428, -265.2195, -238.9421,\n",
            "        -263.4562, -275.1263, -258.8333, -216.4192, -275.5623, -238.0181,\n",
            "        -232.2927, -272.9923, -274.9274, -239.9150, -266.7161, -198.6534,\n",
            "        -271.5967, -264.7030, -211.0466, -276.6294, -271.8854, -206.0213,\n",
            "        -223.2192, -274.8383, -271.0185, -162.2611, -201.9613, -274.6786,\n",
            "        -273.9981, -268.1064, -264.1751, -274.7641, -272.9877, -273.1998,\n",
            "        -274.0484, -263.8021, -259.0556, -262.5422, -261.0476, -244.9572,\n",
            "        -272.9699, -220.3155, -275.1595, -274.9597, -270.6992, -279.1428,\n",
            "        -272.9731, -274.3745, -259.0948, -266.7612, -192.8686, -245.0026,\n",
            "        -275.4293, -273.3224], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.02651417814195156 0.4950000047683716 0.0 254.25173950195312\n",
            "tensor([-182.8701, -267.0114, -261.7908, -270.5128, -260.8757, -233.1340,\n",
            "        -254.7868, -172.6453, -251.1497, -264.0227, -264.4964, -249.3101,\n",
            "        -200.7406, -208.6122, -265.7703, -263.2803, -209.0860, -232.5259,\n",
            "        -256.3016, -216.5049, -258.4270, -228.7671, -251.8242, -236.5444,\n",
            "        -263.5123, -266.3415, -205.0694, -267.9509, -164.5201, -215.5685,\n",
            "        -265.8082, -263.8109, -230.0999, -233.1989, -252.1881, -254.4185,\n",
            "        -228.6226, -247.2551, -266.3549, -256.0432, -232.7147, -227.8401,\n",
            "        -265.2308, -229.3326, -229.0072, -266.4218, -267.8115, -238.4416,\n",
            "        -233.8494, -277.4838, -266.7271, -275.2215, -258.3864, -230.7938,\n",
            "        -265.9201, -269.7887, -251.4220, -262.6915, -221.2157, -247.1785,\n",
            "        -226.2693, -267.3292, -262.1024, -265.9464, -265.7048, -284.3806,\n",
            "        -251.8060, -255.1670, -263.8405, -190.1240, -243.4712, -192.0232,\n",
            "        -213.3024, -263.2742, -263.6911, -266.2226, -267.2402, -211.2686,\n",
            "        -248.2741, -264.6434, -149.3892, -166.4546, -179.9361, -254.9326,\n",
            "        -265.9578, -250.2356, -267.7981, -258.7410, -228.5618, -266.3357,\n",
            "        -245.9963, -267.2260, -202.7977, -248.5949, -266.0451, -258.7088,\n",
            "        -202.8299, -266.3646, -214.9442, -238.3186, -265.7693, -265.7888,\n",
            "        -246.4143, -215.3256, -263.1284, -264.9407, -266.2272, -260.0164,\n",
            "        -191.2553, -272.3853, -250.5455, -206.5037, -259.9352, -272.1810,\n",
            "        -214.4373, -247.2672, -251.9633, -223.2238, -245.3872, -252.2577,\n",
            "        -269.8743, -247.2717, -254.6209, -256.7063, -195.4738, -220.5715,\n",
            "        -252.2810, -266.5770, -156.3770, -277.4519, -261.7765, -237.8050,\n",
            "        -247.9269, -263.9055, -266.1868, -190.5940, -254.1228, -238.5557,\n",
            "        -266.3660, -218.3802, -267.9263, -239.3658, -266.0726, -266.8279,\n",
            "        -158.8749, -180.4403, -234.5294, -218.8081, -246.9758, -257.7404,\n",
            "        -248.2690, -173.2659, -246.1521, -238.0233, -266.2343, -261.8521,\n",
            "        -214.5278, -228.7687, -267.7769, -261.8550, -198.2052, -248.7887,\n",
            "        -266.6273, -262.5176, -265.2363, -264.2095, -265.2617, -199.4087,\n",
            "        -226.1919, -265.9585, -266.3731, -262.2784, -171.6439, -266.7680,\n",
            "        -248.7156, -265.1655, -262.2798, -280.8943, -256.3207, -179.7623,\n",
            "        -266.1113, -180.1646, -207.1367, -161.9321, -257.1632, -266.4965,\n",
            "        -266.3615, -234.2588, -240.9860, -178.1703, -265.9751, -257.2329,\n",
            "        -257.8250, -215.0657, -246.2581, -234.6676, -266.6249, -264.7517,\n",
            "        -263.3815, -205.4276, -263.6689, -263.9834, -235.6250, -268.0656,\n",
            "        -246.4857, -276.7409, -266.0233, -265.3209, -212.7669, -263.8820,\n",
            "        -248.7183, -231.2481, -246.0635, -267.8738, -263.8846, -250.6512,\n",
            "        -242.0965, -265.6416, -223.7195, -266.3183, -269.7678, -186.3433,\n",
            "        -252.1974, -267.7566, -259.1268, -240.9441, -252.4135, -249.3143,\n",
            "        -245.9777, -253.3823, -262.3204, -231.9455, -264.7808, -265.9270,\n",
            "        -253.4635, -246.0694, -257.2457, -196.1062, -162.5898, -254.5114,\n",
            "        -264.8275, -284.4742, -258.2130, -247.4216, -260.4479, -267.7981,\n",
            "        -264.4996, -254.9439, -265.9554, -266.3228, -247.5643, -258.4150,\n",
            "        -136.7121, -233.6127, -261.3310, -260.5546, -174.3528, -247.6652,\n",
            "        -216.3184, -190.2620, -224.6529, -266.6216, -266.3610, -116.5892,\n",
            "        -248.9128, -246.9996, -250.2609, -265.6542, -255.1651, -277.7794,\n",
            "        -263.9319, -261.9702, -254.9765, -213.3425, -256.7771, -263.3398,\n",
            "        -263.8372, -227.8169, -220.6346, -175.8554, -225.0315, -267.8511,\n",
            "        -266.0905, -247.5641, -212.3957, -214.1247, -266.0909, -252.3514],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, closs 0.024322815239429474 0.4950000047683716 0.0 243.33949279785156\n",
            "tensor([-256.0800, -255.6591, -129.3642, -256.1609, -213.4720, -256.2124,\n",
            "        -256.7075, -256.6168, -256.2692, -259.9657, -204.5562, -130.9921,\n",
            "        -244.6733, -248.5635, -222.4884, -228.5223, -239.6362, -252.6191,\n",
            "        -243.3209, -266.2477, -234.2728, -252.2264, -254.4815, -256.5290,\n",
            "        -251.8489, -194.3965, -250.5678, -230.9282, -200.9573, -236.9274,\n",
            "        -179.1409, -241.0479, -231.0675, -257.1055, -231.9998, -240.4015,\n",
            "        -256.3281, -170.2803, -256.1447, -250.8929, -253.8044, -220.2291,\n",
            "        -248.5931, -196.7388, -221.9407, -262.2521, -237.2053, -190.9752,\n",
            "        -242.3631, -255.2377, -233.9981, -232.4062, -235.7004, -256.0136,\n",
            "        -255.8104, -256.5966, -253.5861, -244.3948, -134.6131, -148.1744,\n",
            "        -260.8726, -141.6700, -203.2641, -119.1449, -240.5313, -253.9325,\n",
            "        -225.0962, -227.6488, -204.3101, -254.4496, -129.9178, -217.4733,\n",
            "        -256.7176, -240.8714, -256.0839, -203.5569, -229.8632, -266.1902,\n",
            "        -252.5744, -212.1281, -195.2383, -250.9709, -209.5581, -252.5576,\n",
            "        -257.8607, -257.8327, -253.3634, -236.0020, -242.5628, -257.7923,\n",
            "        -256.5462, -200.9994, -229.3619, -141.3711, -217.6122, -150.1976,\n",
            "        -187.1573, -256.1351, -218.4218, -261.1356, -224.3664, -255.3183,\n",
            "        -250.1188, -252.2063, -255.4095, -256.1188, -256.6419, -187.6145,\n",
            "        -222.1931, -208.8774, -145.9177, -111.3940, -212.6668, -241.1021,\n",
            "        -154.5649, -241.5793, -235.2774, -257.7813, -254.4496, -256.1062,\n",
            "        -242.7784, -205.8179, -251.9757, -238.5407, -144.8167, -257.7929,\n",
            "        -218.4206, -202.3606, -213.8105, -223.3577, -245.9721, -234.3827,\n",
            "        -249.5597, -220.8333, -241.0533, -181.8668, -256.4861, -236.8857,\n",
            "        -237.7628, -197.5609, -193.2580, -229.5060, -234.7894, -253.7408,\n",
            "        -194.1696, -165.0803, -228.2934, -254.2529, -250.6131, -220.3246,\n",
            "        -213.6522, -249.9233, -259.8242, -256.3098, -220.8973, -259.9956,\n",
            "        -212.2395, -261.8568, -259.1068, -256.9254, -201.3103, -252.9119,\n",
            "        -224.8727, -202.0956, -251.6891, -218.3454, -253.3638, -253.4522,\n",
            "        -250.6395, -244.6966, -206.6264, -229.5316, -247.3194, -146.5066,\n",
            "        -241.2172, -221.0288, -266.3911, -253.4633, -186.2431, -259.7380,\n",
            "        -256.5860, -245.7438, -199.3405, -252.2169, -235.3349, -218.5845,\n",
            "        -236.3412, -260.8208, -257.5519, -273.7320, -257.1606, -190.3578,\n",
            "        -228.5969, -251.2313, -205.0698, -266.0799, -235.4137, -255.8036,\n",
            "        -251.8974, -217.5600, -257.1410, -256.4032, -257.0098, -163.8390,\n",
            "        -131.2316, -195.2836, -238.7762, -268.1108, -205.2475, -253.7668,\n",
            "        -166.8357, -230.5974, -255.2171, -241.8881, -251.4983, -256.1445,\n",
            "        -251.0321, -259.5563, -252.5916, -252.9987, -242.7460, -201.4406,\n",
            "        -240.4510, -240.0086, -239.9583, -252.6227, -154.6033, -173.5031,\n",
            "        -255.2668, -144.5558, -257.7328, -234.4033, -172.8572, -252.5880,\n",
            "        -255.2533, -170.3601, -166.2200, -250.9349, -254.7451, -217.7344,\n",
            "        -194.3030, -256.3671, -210.3241, -201.8011, -235.4494, -241.3312,\n",
            "        -235.2741, -244.2607, -160.3158, -254.6625, -236.2456, -242.1114,\n",
            "        -128.6507, -255.4402,  -94.8363, -207.0213, -213.9821, -210.2037,\n",
            "        -230.3736, -256.1802, -255.1385, -256.2098, -257.9085, -201.0308,\n",
            "        -244.6859, -247.7456, -257.9075, -133.7862, -157.8170, -203.9096,\n",
            "        -250.1090, -245.9910, -183.6715, -253.8788, -151.8126, -241.0065,\n",
            "        -237.7794, -255.2029, -209.4756, -254.7214, -249.8812, -260.5894,\n",
            "        -244.7928, -256.6873, -200.9364, -255.2043, -250.9657, -173.7838,\n",
            "        -189.5611, -254.1734, -145.3884, -193.6054, -254.3137, -220.6674,\n",
            "        -240.6431, -240.9489, -237.2663, -213.3664, -251.0949, -256.1428,\n",
            "        -210.4485, -253.9551, -256.3670, -262.4083, -143.3367, -252.5479,\n",
            "        -242.2909, -116.5275, -256.1102, -221.2397, -254.7807, -145.5458,\n",
            "        -138.6840, -209.7855, -254.2720, -242.0999, -201.5651, -255.1518,\n",
            "        -257.1595, -266.9466, -275.5768, -251.1269, -212.0242, -200.1303,\n",
            "        -253.8904, -236.9208, -249.9693, -235.6493, -225.1646, -235.3811,\n",
            "        -233.9942, -212.7486, -263.3076, -267.4912, -240.6627, -222.9273,\n",
            "        -165.0957, -242.5673, -185.8436, -152.9620, -198.8513, -210.6705,\n",
            "        -212.5032, -216.8733, -253.6736, -246.4704, -234.6328, -207.1744,\n",
            "        -177.9289, -221.2850, -212.6578, -199.0603, -262.4747, -250.4758,\n",
            "        -172.4307, -257.8050, -194.3961, -235.7513, -242.5581, -257.8753,\n",
            "        -251.0785, -230.9816, -236.1676, -256.2790, -243.7165, -226.8369,\n",
            "        -225.2201, -238.8849, -198.7894, -256.0728, -142.0226, -267.8090,\n",
            "        -256.3168, -233.9267, -257.5486, -243.1048, -258.9864, -249.2985,\n",
            "        -253.8989, -242.2213, -262.0707, -222.4921, -194.1505, -243.6080,\n",
            "        -262.3602, -251.9143, -243.1405, -242.3165, -233.3837, -255.7411,\n",
            "        -250.3194, -255.6299, -246.7906, -259.7441, -255.9775, -200.9106,\n",
            "        -235.4591, -256.6126, -260.1302, -150.3934, -193.1263, -256.1800,\n",
            "        -213.2529, -235.1597, -254.7867, -254.4486, -252.1628, -255.2381,\n",
            "        -253.3828, -211.4107, -256.4886, -259.7873, -232.0919, -268.6168,\n",
            "        -240.9013, -221.5917, -156.4958, -241.0638, -161.3045, -207.4795,\n",
            "        -256.1472, -234.7429, -256.1142, -253.2472, -253.3675, -237.0773,\n",
            "        -240.9989, -220.3519, -212.4958, -202.3436, -252.2818, -274.7320,\n",
            "        -172.7509, -257.8903, -183.2004, -226.6893, -237.2223, -238.8537,\n",
            "        -253.0025, -256.0191, -246.4745, -126.7568, -208.1539, -234.8322,\n",
            "        -222.3572, -173.9845, -225.8740, -269.1964, -270.2968, -244.0978,\n",
            "        -144.4511, -218.9956, -250.2162, -256.3242, -236.0089, -252.1720,\n",
            "        -257.9150, -249.8127, -199.2444, -244.9790, -235.3793, -246.0025,\n",
            "        -254.1757, -261.3466, -235.3170, -254.7466, -235.0490, -213.7884,\n",
            "        -185.2975, -255.6709, -208.6695, -254.0211, -219.4012, -204.3296,\n",
            "        -256.0896, -234.4856, -253.4120, -230.7923, -233.8843, -236.0036,\n",
            "        -242.5493, -216.2018, -150.1615, -270.9387, -254.2855, -256.6760,\n",
            "        -255.6616, -251.9899, -254.7245, -192.1142, -236.8789, -209.8408,\n",
            "        -256.2325, -149.9600, -257.7193, -268.6423, -171.3708, -256.2252,\n",
            "        -242.9271, -214.4230, -247.5729, -256.2775, -237.9564, -254.9069,\n",
            "        -241.8909, -234.7681, -236.0534, -218.4777, -259.5787, -241.7023,\n",
            "        -234.1147, -175.8814], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.023681819438934326 0.4950000047683716 0.0 229.30557250976562\n",
            "tensor([-241.0108, -243.7756, -182.7742, -216.4712, -241.0521, -155.9668,\n",
            "        -227.1089, -236.6942, -243.9780, -217.5123, -222.5895, -113.5423,\n",
            "        -220.1205, -213.9187, -244.5546, -254.9989, -235.2046, -245.4327,\n",
            "        -157.7644, -151.7991, -246.9743, -236.3832, -237.2864, -244.1322,\n",
            "        -235.3198, -214.3573, -245.3838, -183.9952, -179.1250, -199.3188,\n",
            "        -226.0193, -181.8970, -226.1492, -245.4843,  -60.2798, -176.4353,\n",
            "        -243.4894, -246.4934, -246.6002, -223.7101, -199.6947, -234.8789,\n",
            "        -221.6208, -241.9625, -220.8056, -211.6098, -239.2017, -246.7747,\n",
            "        -211.8616, -206.7721, -154.6000, -141.4202, -235.0827, -238.9408,\n",
            "        -241.0004, -245.4989, -245.3750, -234.9219, -225.5844, -248.7118,\n",
            "        -165.6316, -210.7116, -245.4449, -162.2065, -187.8335, -245.0818,\n",
            "        -181.2313, -195.6893, -226.9155, -223.4751, -232.6564, -245.3495,\n",
            "        -245.6939, -240.6483, -200.3761, -129.0703, -217.4504, -193.7569,\n",
            "        -236.6422, -254.5943, -257.8540, -246.5463, -209.8015, -260.1458,\n",
            "        -240.5770, -246.9896, -226.5635, -224.7873, -240.5245, -233.7490,\n",
            "        -240.8515, -210.7406, -214.9919, -218.7549, -205.9608, -167.7435,\n",
            "        -160.9778, -240.4644, -213.3326, -251.2450, -224.5779, -222.1971,\n",
            "        -244.3614, -168.9662, -236.4905, -245.3054, -182.0069, -194.1171,\n",
            "        -246.5282, -247.3164, -221.9639, -169.5730, -199.3407, -242.1365,\n",
            "        -217.6572, -263.6283, -241.9203, -207.5144, -246.7625, -207.1984,\n",
            "        -159.7779, -245.3977, -235.9972,  -89.1795, -265.2994, -215.8646,\n",
            "        -241.8162, -237.0928, -191.4445, -247.0553, -207.0914, -207.6560,\n",
            "        -190.2604, -240.8635, -241.4247, -242.1421, -237.8608, -247.6524,\n",
            "        -192.2731, -225.6545, -181.6046, -206.5235, -228.7065, -234.6272,\n",
            "        -224.8390, -242.7311, -212.4137, -189.0494, -222.8656, -191.4343,\n",
            "        -144.9962, -244.5232, -221.8280, -244.8458, -221.8497, -242.8166,\n",
            "        -205.7132, -259.6805, -198.0737, -227.5124, -186.4778, -240.4925,\n",
            "        -141.7420, -257.4273, -240.5548, -241.8090, -243.8085, -163.6294,\n",
            "        -248.0039, -204.4290, -245.4658, -238.4107, -257.8444, -247.6212,\n",
            "        -211.1059, -260.0260, -244.5090, -241.1989, -164.4810, -256.1337,\n",
            "        -245.1661, -238.8856, -215.0919, -207.9676, -245.5742, -235.3081,\n",
            "        -221.1771, -186.2237, -147.2749, -190.5449, -176.3455, -230.2138,\n",
            "        -250.3271, -242.1881, -195.5247, -254.4484, -231.9573, -245.4810,\n",
            "        -187.9969, -225.0115, -236.8270, -227.3450, -245.0718, -230.4535,\n",
            "        -190.4659, -236.4048, -245.4843, -235.5115, -251.3701, -245.2232,\n",
            "        -215.5589, -194.9773, -220.6039, -245.2927, -245.3165, -245.6763,\n",
            "        -198.2398, -231.6073, -199.1354, -248.4093, -260.8859, -192.8493,\n",
            "        -244.0631, -242.3644, -258.6420, -246.1149, -203.0475, -200.0100,\n",
            "        -213.1597, -247.1437, -227.6028, -245.4583, -223.8000, -242.6503,\n",
            "        -241.8013, -244.5492, -226.9146, -114.8368, -239.7836, -248.6958,\n",
            "        -253.0416, -192.4744, -249.4731, -265.3737, -183.1583, -200.2683,\n",
            "        -158.6991, -168.1448, -217.7508, -233.3783, -245.3119, -220.6299,\n",
            "        -189.4654, -236.3558, -233.2552, -246.5677, -144.9113, -246.1624,\n",
            "        -226.8618, -260.9835, -236.5814, -241.2472, -245.3907, -222.0261,\n",
            "        -236.5303, -225.1073, -221.3093, -110.9303, -159.3698, -230.8436,\n",
            "        -218.6645, -200.3714, -212.8546, -243.8153, -228.4669, -242.1549,\n",
            "        -158.5181, -247.0362, -241.9849, -238.9189, -232.7147, -163.4814,\n",
            "        -220.0406, -221.7348, -225.5691, -104.3356, -197.0389, -260.6353],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, closs 0.022526148706674576 0.4950000047683716 0.0 219.873779296875\n",
            "tensor([ -91.3919, -234.0791, -232.3932, -205.9108, -212.9713, -224.2155,\n",
            "        -234.7480, -201.6556, -193.3223, -216.2171, -218.2523, -147.3134,\n",
            "        -226.2028, -159.4141, -135.9316, -151.2242,  -30.3970, -216.0810,\n",
            "        -224.8417, -199.7325, -188.1605, -152.2381, -159.1939, -210.1677,\n",
            "        -233.2978, -206.0017, -233.4136, -182.8966, -228.7033, -240.2251,\n",
            "        -214.8505, -229.3958, -236.1117, -230.2799, -232.6314, -188.5854,\n",
            "        -236.0639,  -82.8588, -233.3973, -238.7386, -178.1311, -246.4050,\n",
            "        -190.8086, -241.4803, -212.8721,  -93.2673, -180.8828, -224.1956,\n",
            "        -178.7727, -224.3433, -197.3653, -219.5315, -235.5535, -150.8835,\n",
            "        -188.1585, -175.6715, -192.6438, -198.4531, -236.9436, -229.8285,\n",
            "        -231.6697, -191.4398,  -84.0257, -162.6528,  -35.6009, -233.4164,\n",
            "        -143.5334, -135.3728, -234.7303, -219.1039, -148.0974, -189.2966,\n",
            "        -212.6392, -233.3968, -208.3904, -234.5668, -201.3488, -246.1158,\n",
            "        -232.1733, -234.7479, -236.4130, -234.1410, -227.7752, -139.6148,\n",
            "        -186.9356, -185.6066, -203.5726, -185.7464, -209.1830, -162.6831,\n",
            "        -204.8386, -166.8503, -214.6441,  -92.4271, -224.1147, -228.4649,\n",
            "        -212.7643, -208.5181, -217.1731,  -62.9930, -217.6742, -232.1304,\n",
            "        -215.4754, -104.8513, -166.8445, -186.3551, -179.8649, -219.2128,\n",
            "        -226.6355, -189.7550, -176.6373, -152.3233,  -50.9092, -226.5749,\n",
            "        -176.1888, -149.9518, -182.0746, -196.8325, -206.7422, -197.3092,\n",
            "        -169.0396, -107.9682, -233.4057, -196.2502, -206.8263, -246.5981,\n",
            "        -253.9537, -229.3971, -151.9926, -233.4428,  -89.4284, -111.9875,\n",
            "        -227.2966, -219.1024, -224.5892, -214.9862, -222.3438, -176.7585,\n",
            "        -225.7362, -133.0169, -158.6097, -125.4474, -238.8504, -182.2471,\n",
            "        -198.3067, -212.3546, -229.7810, -132.0454, -207.1226, -234.7436,\n",
            "        -208.6170, -222.3328, -213.2045, -216.1816, -220.4309, -170.1960,\n",
            "        -208.8312, -181.2182, -194.3941, -226.9598, -217.5492, -229.7491,\n",
            "        -200.3039,  -36.0127, -183.3849, -183.3956, -213.6333, -130.9267,\n",
            "        -233.4779, -235.6908, -170.3686, -143.0291, -228.1880, -191.5365,\n",
            "        -139.0655, -152.1874, -215.2318, -208.6468, -223.1775, -230.1525,\n",
            "        -163.1505, -207.1017, -226.5536, -229.4865, -188.4241, -223.8133,\n",
            "        -228.4161,  -68.2603, -231.7637, -143.3277, -173.6451, -238.8047,\n",
            "        -197.9302, -223.9736, -178.1835, -215.6601, -182.1896, -236.3143,\n",
            "        -164.6799, -233.4152, -144.6017, -249.0472, -188.8380, -223.5660,\n",
            "        -181.4741, -207.3101, -200.0416, -138.3762,  -87.2386, -233.0289,\n",
            "        -159.0542, -108.7888, -221.3257, -235.6989, -203.5415, -176.5027,\n",
            "        -233.3147, -202.5022, -118.0085, -182.9248, -230.1489, -232.6126,\n",
            "        -226.5737, -227.3715, -220.5207, -231.9169, -239.1199, -127.7877,\n",
            "        -109.4576, -235.9379, -227.3705, -217.5718, -177.5837, -227.3531,\n",
            "        -179.0074, -228.3887, -222.7274, -198.2600, -103.7709, -140.0069,\n",
            "        -219.7390, -188.1569, -145.2955, -235.9732, -190.9085, -212.7118,\n",
            "        -216.8473, -217.1920, -209.2840, -239.5687, -239.2209, -202.7345,\n",
            "        -231.1572, -217.0765, -182.5531, -194.1669, -108.4990, -105.4042,\n",
            "        -208.4085,  -90.6265, -228.7036, -236.0830, -208.5112, -213.8814,\n",
            "        -196.5902, -238.6200, -245.4532, -177.1823, -210.6657, -240.3265,\n",
            "        -131.5648, -195.1731, -145.2004, -212.6694, -129.5965, -200.0046,\n",
            "        -139.3749, -187.3293, -180.5570, -221.8477, -230.2216, -235.1033,\n",
            "        -170.4441, -215.0538, -231.3170, -163.5519, -214.0990, -227.3571,\n",
            "         -66.8181, -231.2883, -234.8371, -179.8603, -163.0952, -220.4679,\n",
            "        -172.8562, -183.9755, -162.6901, -112.5496, -204.3718, -188.5100,\n",
            "        -161.6295, -178.5612, -203.8427, -176.6847, -231.3983, -230.4573,\n",
            "        -239.0205, -212.2775, -154.1320, -224.8126, -206.9837, -172.9997,\n",
            "        -220.4310, -207.8244, -193.2211, -251.6125, -228.7777, -246.1731,\n",
            "        -191.5105, -154.0562, -127.9664, -106.1838, -190.2423, -180.5585,\n",
            "        -197.0477, -235.1575, -234.9734, -140.9331, -233.0387, -190.0268,\n",
            "        -236.2863, -248.2923, -193.3366, -251.8623, -233.4424, -220.2646,\n",
            "        -142.3941, -206.6511, -144.7901, -113.0151, -235.5916, -196.9821,\n",
            "        -225.3365, -127.0121, -225.7335, -215.7514, -139.1301, -171.7267,\n",
            "        -145.3847, -163.8306, -230.4370, -192.5972, -235.9604, -165.3825,\n",
            "         -84.6227, -231.8270, -219.5842, -134.7354, -217.9727, -248.6266,\n",
            "        -233.0934, -227.3590, -151.7507, -209.6707, -235.1751, -217.4957,\n",
            "        -173.8815,  -89.8871, -187.8658, -147.5991, -203.4397, -224.7806,\n",
            "        -160.9571, -224.3096, -206.6240, -244.9448, -223.4261, -141.1632,\n",
            "        -229.2278, -238.5182, -205.7849, -230.3083, -184.5056, -127.9195,\n",
            "        -127.3757, -180.4421, -224.0497, -216.7523, -186.8405, -221.0396,\n",
            "        -165.0061, -169.6647, -214.6266, -204.5942, -190.8137, -139.5340,\n",
            "        -193.2342, -191.6911, -182.3397, -248.6316, -125.6191, -233.3230,\n",
            "        -248.9580, -190.4305, -166.8928, -165.4001, -207.6252, -239.4147,\n",
            "        -216.5805, -167.3682, -102.1173, -206.5726, -225.7739, -137.5998,\n",
            "        -114.7381, -161.5259, -120.6769, -214.2432, -248.2158, -151.8692,\n",
            "        -220.9460, -186.6741, -233.0312, -163.6220, -233.0679, -200.0454,\n",
            "        -252.6692, -128.4140, -185.7352, -245.4580, -188.7182, -134.6342,\n",
            "        -152.4030, -231.3229, -214.4859, -151.5998, -210.4817, -225.4770,\n",
            "        -206.1354, -176.4396, -233.4252, -196.5546, -188.0610, -246.7000,\n",
            "        -224.9688, -187.5989, -136.6440, -239.1776, -182.7545, -226.1763,\n",
            "        -145.1259, -166.1906, -111.6124, -207.9595, -222.4979, -248.2855,\n",
            "        -233.6326, -236.2549, -214.8831, -158.0673, -231.4673, -220.5414,\n",
            "        -171.8375, -215.2158, -235.7027, -232.9530, -204.7294, -154.6943,\n",
            "        -107.7711, -225.2199, -210.5747, -191.8803, -214.7879, -235.6477,\n",
            "        -195.8259, -214.6068, -218.7715, -136.0686, -248.6023, -196.4432,\n",
            "        -236.4933, -172.5993, -232.3747, -184.7957, -216.1395, -190.3785,\n",
            "        -204.8545, -246.4469, -189.1089, -162.5939, -167.6638, -170.2414,\n",
            "        -209.5441, -113.8756, -247.1218, -235.0147, -118.2419, -233.0089,\n",
            "        -227.9292, -196.4913, -186.5306, -146.7577, -233.4335, -189.3503,\n",
            "        -218.0732, -127.6943, -168.4828, -228.6481, -225.0541, -206.7690,\n",
            "        -194.2567, -113.5748], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.02070269174873829 0.4950000047683716 0.0 194.14804077148438\n",
            "tensor([-223.4839, -163.3310, -234.3368, -206.0870, -130.4531, -205.7857,\n",
            "        -221.5211, -163.1681, -209.4749, -209.5262, -207.2618, -234.7342,\n",
            "        -162.3582, -171.4186, -173.5722, -155.5483, -207.7813, -216.9113,\n",
            "         -89.5794, -183.5360, -129.1221, -142.3917, -191.3475, -179.2282,\n",
            "        -219.1587,  -75.4363, -203.8185, -242.4015, -162.4500, -228.0315,\n",
            "        -192.1596, -197.7434, -118.7447, -220.4238, -211.3152, -164.3678,\n",
            "         -99.3812,  -58.2061, -205.7323, -150.9195, -212.3907, -222.7807,\n",
            "        -172.9073, -155.9420, -160.9733, -142.1015, -159.2895, -206.1504,\n",
            "         -16.0828, -218.4610, -218.5543, -206.0696, -148.9955, -174.9933,\n",
            "        -219.8409,  -94.3191, -147.1499, -219.0005, -200.4227, -220.3444,\n",
            "        -190.8315, -229.6339, -199.8547, -161.1761,  -82.6544, -219.5145,\n",
            "        -162.5796, -183.2297, -155.6819, -187.7757,  -96.3244, -169.8502,\n",
            "        -191.3438, -104.2607, -162.5088, -239.2567, -218.2416, -182.9002,\n",
            "         -80.8775,  -88.2507, -223.0645, -220.2229, -192.3183, -222.6351,\n",
            "         -94.5470, -164.9513, -121.5641, -180.1657, -209.3354, -221.6137,\n",
            "         -88.7883, -167.8919, -210.6624, -214.5544, -231.1408, -115.2913,\n",
            "        -115.3501, -157.4575, -154.2254, -223.3958, -207.9342,  -88.9169,\n",
            "        -210.7001, -139.3280, -220.4399, -227.4045, -211.2738, -189.2188,\n",
            "        -188.5850, -133.8270, -160.7461, -233.5432,  -12.9806, -214.5100,\n",
            "        -144.2991, -211.4862, -197.9553, -206.7461, -210.6788, -199.7445,\n",
            "        -218.4868, -165.3399,  -82.1608, -210.3785, -214.5174, -124.4653,\n",
            "        -133.9635, -161.6462, -202.3114, -215.4912, -151.5743,  -48.1305,\n",
            "        -211.7837, -202.3923, -197.5909,  -56.2260, -186.8753, -168.5514,\n",
            "        -142.8910, -219.2115, -143.6710, -213.7725, -188.0329, -163.1758,\n",
            "        -150.0181, -200.0915, -145.8462, -214.6288, -101.7930, -211.0708,\n",
            "        -211.4376, -203.4160, -127.3041, -220.8805, -225.4445, -221.6223,\n",
            "        -197.9242, -190.8347, -209.6525, -166.6006,  -83.2351, -218.8142,\n",
            "        -180.7387, -188.9289, -138.9208, -106.5243, -220.2226, -128.2094,\n",
            "        -216.6393, -190.7352, -183.6787, -221.5645, -190.1320, -163.6053,\n",
            "        -180.6767, -197.9931, -202.3241, -218.4527, -229.5821, -223.0782,\n",
            "        -217.8647, -102.6749, -165.0179, -141.1063, -141.2698, -157.2910,\n",
            "        -174.6822, -199.5798, -220.4435, -219.9058, -132.6593, -217.9364,\n",
            "        -206.3812, -197.8942, -119.1158,  -41.4816, -154.4902,  -16.3929,\n",
            "        -217.6749, -190.4867, -220.3519, -154.8385, -159.2597, -192.3992,\n",
            "        -210.6201, -144.8317, -226.2889, -216.2090, -211.8022, -164.1688,\n",
            "        -223.4910, -203.9174, -138.5053, -191.8696, -219.4371, -197.3567,\n",
            "        -104.7220, -142.2323, -218.7385, -188.8391, -217.8500, -209.4511,\n",
            "        -145.6560, -239.5427, -193.0316, -212.8208, -208.5688, -187.9512,\n",
            "        -212.4950, -162.4733, -211.3679, -171.6592, -154.6694, -135.0357,\n",
            "         -85.3733, -177.8582, -163.1991, -159.8917, -190.3196, -236.4352,\n",
            "        -217.8809, -188.5145, -181.3770, -171.4299, -214.3201, -219.4591,\n",
            "        -211.2376,  -95.7901, -190.5644, -161.7266, -159.2646, -235.9670,\n",
            "        -210.4037, -100.5158, -118.4158, -226.9680, -193.2533, -218.7861,\n",
            "        -241.0954, -153.4602, -195.5304, -190.9813, -222.9571, -204.7530,\n",
            "        -216.5110, -217.8620, -163.9529, -167.6143, -208.5157, -225.4678,\n",
            "        -226.0007, -208.9659, -108.3054, -170.2448, -128.1017, -142.5475,\n",
            "        -209.7055, -198.0915, -190.7332,  -92.6730, -126.1110, -202.3710,\n",
            "        -130.2091, -212.8242, -219.6984, -144.0963, -200.0544, -139.7077],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.019373344257473946 0.4950000047683716 0.0 177.925048828125\n",
            "tensor([-149.9637,  -23.3879, -137.5965, -205.9587, -202.3844, -173.6597,\n",
            "        -199.4728, -141.5938, -196.8020, -197.0782, -199.0576, -150.7786,\n",
            "        -150.4098, -149.5349, -196.1562, -176.0064, -205.4341, -144.2456,\n",
            "        -174.7589, -174.7733, -123.9320, -197.9628, -195.1911, -206.0966,\n",
            "        -183.8799,  -91.1298, -122.8373, -154.1258, -143.0734, -195.2702,\n",
            "        -205.6545, -159.5965,  -93.1504, -185.5927, -123.8954, -177.3105,\n",
            "        -178.2091, -190.6938, -205.5909, -205.1899, -203.9469,  -88.5169,\n",
            "        -178.6167, -154.2153, -121.6631, -183.6108, -205.8765, -129.8155,\n",
            "        -205.5744,  -96.6510, -187.4917, -173.7095, -189.1264, -201.4190,\n",
            "        -172.5269, -196.3097, -201.4039, -218.2545, -149.4119, -137.1730,\n",
            "         -89.2256, -206.0431, -190.7309, -173.6663, -205.4664, -173.6112,\n",
            "        -171.5223, -105.4494, -110.4208, -133.5736, -194.7834, -136.0373,\n",
            "        -204.7031, -146.2981, -109.3618, -192.6995,   -6.6857, -205.7768,\n",
            "        -103.8444, -170.5302, -146.8047,  -88.3218, -157.0678, -199.8358,\n",
            "        -206.6015, -180.4259, -117.8624, -184.1840, -195.3041, -112.7897,\n",
            "        -137.6456, -162.8136, -179.1507, -187.5743, -176.3976, -205.9868,\n",
            "        -141.1630,  -86.2902, -174.6254, -105.1384, -194.8542, -176.7888,\n",
            "        -191.6653, -155.0650, -189.1265, -179.4225, -121.4294, -109.1960,\n",
            "          -8.0038, -205.8979, -206.2200, -198.6546, -157.2773, -156.5227,\n",
            "         -89.6342, -125.2724,  -96.8722, -201.7737, -201.5961, -206.0925,\n",
            "        -119.9078, -156.3469, -179.9013,  -50.8027,  -81.9578, -135.4567,\n",
            "        -181.9248, -192.7252, -205.9996,  -94.8546,  -83.1808, -186.7108,\n",
            "         -83.1351, -205.5417, -183.5382, -205.9472, -198.5147, -149.9682,\n",
            "         -28.9832, -200.0021,  -61.7136, -186.8678, -200.7038, -201.4735,\n",
            "        -206.1165,   -7.4063, -227.8839, -179.0535, -159.7480, -172.0090,\n",
            "        -155.4538, -126.0929, -204.3673, -126.1271, -141.6495, -182.5468,\n",
            "        -192.9465, -205.5702,  -41.0940, -175.7140,  -57.3324, -149.6877,\n",
            "        -157.6160, -188.2747, -151.0396, -205.5508, -157.3339, -202.4030,\n",
            "        -206.6962, -135.1671, -184.1712,  -89.9447,  -83.6750,  -67.0901,\n",
            "        -198.2194, -205.9704, -198.9257, -207.4749, -203.5749, -194.1008,\n",
            "        -169.2530,  -70.3690,  -91.2271, -200.6396, -206.8005, -202.4234,\n",
            "        -141.6587, -220.2266, -106.0051, -205.9946, -203.6634, -193.5956,\n",
            "        -194.6714,  -16.0480, -162.6339, -150.7081, -180.2607, -136.1195,\n",
            "        -196.4736, -204.3636, -189.8490, -204.6984,  -58.1281, -206.1178,\n",
            "         -86.8556, -160.2193, -200.5349, -205.5052, -129.2628,  -62.9603,\n",
            "        -176.3429, -190.6511, -166.7199, -205.4576, -203.6558, -205.7854,\n",
            "        -204.2358, -188.7158,  -83.4716, -163.6023,  -53.0934, -205.5470,\n",
            "        -174.0103, -205.5763, -200.2930,  -41.4679, -130.9436, -188.3866,\n",
            "        -193.3489, -205.9424, -174.5807, -135.6037, -204.7690, -192.6133,\n",
            "         -41.1794, -204.9589,  -88.4697, -188.9972, -128.6156, -206.0121,\n",
            "        -206.1030, -147.9878,  -85.1193, -204.4821,  -65.7376, -141.2699,\n",
            "        -203.2554, -205.5532, -174.4871, -175.4980, -191.9725, -148.7405,\n",
            "        -102.0416, -206.1187,  -43.1436, -205.5720, -157.6373, -207.6654,\n",
            "        -183.5300, -200.3319,  -59.6496, -200.4082, -149.2383, -205.9059,\n",
            "        -200.3242, -107.2848,  -77.9838, -180.4157,  -91.0100, -136.9870,\n",
            "        -203.7585,  -92.9049, -188.3084,  -15.1381,  -57.2283, -197.0176,\n",
            "        -179.1315, -204.6393, -194.3496, -149.5887, -206.7335, -124.6554,\n",
            "         -10.2084, -220.9299, -199.4854, -203.7556, -205.2990, -205.8950,\n",
            "        -181.1035, -110.9918, -207.9075, -186.9958, -183.5619, -118.0403,\n",
            "        -197.7592, -129.8019, -206.1483, -173.0404, -145.3749, -204.9270,\n",
            "          -8.3619, -206.5415, -196.1401, -196.6496, -106.8793,  -79.9904,\n",
            "        -213.2894, -198.8372, -140.3390, -173.8923, -108.6962, -196.3566,\n",
            "        -205.2623, -179.6723, -184.6127, -218.9333,  -13.3995, -156.8405,\n",
            "        -203.7837, -203.9546, -205.4984,  -95.1139, -196.9177, -148.5027,\n",
            "        -208.7788, -201.6627,  -88.2073, -149.0568, -187.3737,  -75.9489,\n",
            "         -82.1702, -165.4075,  -61.7785, -203.3796, -186.9045, -130.6043,\n",
            "        -202.5005,  -96.5660, -206.6967, -202.0788, -191.3318, -141.8985,\n",
            "        -203.8184, -193.0487, -169.5020, -189.6176,  -18.3998, -213.6362,\n",
            "         -84.7029, -135.9485, -174.2261, -201.8010, -172.1032, -192.5900,\n",
            "        -208.2651, -205.8497,  -77.9713, -183.7530, -178.7511, -176.4491,\n",
            "        -205.1677, -110.2866, -101.3422, -196.7964, -156.3105, -149.7301,\n",
            "        -207.1127, -175.7742, -194.3642,  -47.7726, -205.8673, -143.3335,\n",
            "         -83.6952, -158.5120, -156.2384, -177.9928, -203.0880, -208.7069,\n",
            "        -174.4181,  -96.8437,  -30.4952, -146.8249, -201.0319, -202.3079,\n",
            "        -174.4994,  -41.2968, -206.5644, -205.9953, -204.6234, -189.3191,\n",
            "        -205.1891, -174.8898, -206.5161, -155.9830, -110.8711, -132.5793,\n",
            "         -11.3842, -205.9264, -195.7550, -198.4763, -197.6659,  -28.1117,\n",
            "        -185.0780, -175.7171, -139.0407, -205.9338, -138.1371, -199.0215,\n",
            "        -183.8793,  -27.2795, -195.4109, -156.7821,  -96.7779, -163.3812,\n",
            "        -204.4025, -206.1861, -198.7038,  -94.8090, -201.8790, -206.1283,\n",
            "        -121.0341, -182.2162, -205.9618, -155.9006, -183.4290, -114.8163,\n",
            "        -121.3007, -194.9860, -154.2850, -201.8000, -203.0691, -202.2063,\n",
            "        -199.7852,  -23.5185, -196.9690, -197.9394, -207.5554, -205.5722,\n",
            "        -170.5890, -174.7610,  -62.0699, -147.8071,  -44.1780, -167.6858,\n",
            "        -155.3217, -196.8393, -196.9025, -204.2625, -151.8606, -208.8265,\n",
            "         -89.6400, -208.6662, -144.2273, -186.9086,  -84.6347, -178.1384,\n",
            "        -197.9034, -105.3898, -197.2052, -186.8932,  -28.0357, -205.5620,\n",
            "        -138.2457, -173.7955, -183.3253,  -10.7908, -222.1751, -208.7205,\n",
            "        -158.0222, -205.4977, -205.4877, -206.1220,  -57.1807,  -67.0716,\n",
            "        -144.2316, -202.2881,  -71.8176, -201.9949, -194.1443, -203.1826,\n",
            "        -183.8328,  -49.2538, -229.9807, -204.5255, -208.7246, -203.7804,\n",
            "        -196.3401, -175.7249, -175.7455, -186.5797, -202.2676, -174.5013,\n",
            "         -87.1488, -194.6770, -204.6888, -202.0933, -146.3843,  -76.6162,\n",
            "         -81.0844, -200.4618,  -65.0150, -129.5880, -205.5664, -120.8021,\n",
            "        -111.6788, -182.0067, -125.0375, -220.1450, -112.0106, -170.7764,\n",
            "        -171.0901, -200.1376], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.01900205947458744 0.4950000047683716 0.0 160.177001953125\n",
            "tensor([-160.9975,  -67.2105,  -79.9870,  -97.4108, -191.7891, -124.0390,\n",
            "        -188.5650, -107.6155, -179.8094,   -6.4414,  -90.2900, -191.0105,\n",
            "        -184.0863, -187.3988, -183.1812, -190.8035,  -65.8164,   -6.5284,\n",
            "        -143.3984, -158.0019, -144.3515, -179.4713,  -41.1445, -182.4382,\n",
            "        -183.0973, -132.3496, -188.4314,  -66.3123,  -54.8707,  -67.1700,\n",
            "        -190.7147, -119.5704, -188.5335,  -89.6767,  -67.7907, -192.2826,\n",
            "        -181.7247, -139.7976, -190.9893, -186.5637, -129.7876, -140.2363,\n",
            "          -6.9774, -149.0766, -121.1064, -164.1605, -183.9877, -180.2627,\n",
            "        -177.0629,  -77.4398, -190.6253, -174.2840, -105.6179, -176.0849,\n",
            "        -165.0918, -183.5714, -173.1017, -192.8159, -167.9222, -180.4906,\n",
            "        -176.8180, -120.6908, -123.8272, -185.9198, -101.4277,  -70.1682,\n",
            "        -132.5829, -190.7485, -190.9516, -113.5277, -190.6771, -154.5484,\n",
            "        -188.5505, -184.4988, -113.3827, -156.1212, -191.6453, -109.5309,\n",
            "        -177.6801, -109.5116, -166.9586,  -59.0762,  -34.0510, -180.1489,\n",
            "        -192.3241, -188.6058, -190.7953, -181.3413,  -97.7111,  -95.5871,\n",
            "        -134.1644, -204.8233,  -74.3067, -187.8660, -190.6405, -190.9246,\n",
            "        -177.2148,  -90.0842,  -19.4937, -117.7789, -115.8931, -163.5879,\n",
            "        -122.4489, -190.4737, -189.6818,  -88.5265, -182.9394,  -37.8082,\n",
            "        -124.0795, -120.4442, -190.6777, -190.8492,  -83.9893,  -15.9136,\n",
            "         -96.7462, -163.4640,  -55.7502, -183.3265, -157.0747, -190.4300,\n",
            "         -35.8723, -190.3057, -100.2010, -193.6558,  -87.2409, -161.9272,\n",
            "        -183.0532, -119.6627, -123.0762,  -35.9954, -132.7061, -190.4019,\n",
            "        -116.8428, -184.7617, -162.5166, -179.6739, -190.4609, -168.9179,\n",
            "        -123.8466, -157.8611,  -80.7324, -165.2296, -190.4023, -186.1768,\n",
            "        -190.7594, -162.4978, -192.0036, -194.3531, -108.0705, -150.8216,\n",
            "        -179.9040, -167.3041, -185.1096,  -55.8817, -116.1443, -156.3757,\n",
            "        -179.2177, -163.7329, -163.8121, -190.0898, -190.7464,  -49.9506,\n",
            "        -160.1781, -190.6732, -109.8633, -161.8213, -188.5400, -120.8045,\n",
            "        -189.6227,  -34.9735,  -71.3814, -120.4114,  -40.3388, -113.4667,\n",
            "        -190.7376, -150.8033, -183.2888,  -67.1588, -140.9274, -187.2482,\n",
            "        -132.6290, -183.3274, -162.8669, -184.3941, -163.9543, -129.0690,\n",
            "        -188.1497, -152.4436,  -60.7269, -186.3494, -175.9514, -189.7831,\n",
            "        -120.1210,   -6.4580, -215.9519, -145.5604, -155.3677, -110.0442,\n",
            "        -144.5907, -190.4892, -162.1455, -150.7736,  -84.1261, -180.3407,\n",
            "         -39.8435, -183.5262, -189.9928, -188.4756, -187.9333,  -89.8939,\n",
            "        -154.4302, -179.7866, -167.6364, -170.0180, -156.3438, -118.3599,\n",
            "        -190.5744, -107.6911, -128.2991, -137.9419,  -88.8681, -112.2758,\n",
            "        -187.5079, -190.6134, -179.3420,  -55.5768, -191.4226,  -84.1713,\n",
            "        -123.9999, -190.6572, -134.1006, -131.1266, -155.2827, -154.1658,\n",
            "          -8.3583,  -61.9086,   -6.4596, -179.4015, -157.1666, -185.4863,\n",
            "        -188.5222,  -70.0550, -189.7951, -193.3570,  -43.1976, -136.9857,\n",
            "        -179.8297, -180.5101, -156.0127, -125.2112,  -16.8007, -131.7331,\n",
            "          -7.0191, -189.8139,  -82.2710, -155.0908, -157.9837,  -16.0775,\n",
            "        -183.3868, -121.0518, -103.7769, -182.4962, -190.6618,  -90.1560,\n",
            "        -164.8979, -189.7519, -171.0746, -165.1032, -172.9327, -165.1852,\n",
            "        -183.0403, -171.9863, -120.1725, -124.7534,  -32.5132, -165.1542,\n",
            "        -136.7031, -190.7314, -153.1689, -177.7405, -180.9211,  -61.9788,\n",
            "         -27.7558, -106.8383, -194.3458, -156.4043, -180.4898, -122.9239],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.018753239884972572 0.4950000047683716 0.0 141.2615509033203\n",
            "tensor([-161.2533, -151.7816, -147.1310, -171.1463,  -51.9634,  -89.7813,\n",
            "        -174.1067,  -10.7011, -166.3500, -174.2767, -174.9248,   -9.3302,\n",
            "        -151.8620,  -18.8753, -165.7032,  -31.3344, -139.3172, -101.5220,\n",
            "         -56.3568, -191.4056,  -78.1367,  -29.6042, -143.6366, -144.8553,\n",
            "        -163.0634, -116.5349,  -64.3550, -130.5176, -172.7812, -174.0178,\n",
            "        -181.8534,  -15.2721, -171.9516, -181.8823,  -13.6245, -143.3195,\n",
            "        -104.1265,  -90.9708,  -92.6775,  -43.8358, -162.0326, -173.5352,\n",
            "         -12.1808, -144.5353, -173.6714, -118.1816, -122.3770,   -8.3766,\n",
            "        -111.3137,  -72.6569, -169.1907, -188.6878, -191.3418,  -17.8361,\n",
            "        -154.3337, -122.5110, -159.0840, -158.3659, -106.5550,  -66.9049,\n",
            "        -165.2209, -152.8584, -167.4300,  -56.2535, -188.6653, -165.6368,\n",
            "        -137.1571, -179.5829, -159.1819, -190.3979, -171.4955, -128.2459,\n",
            "         -61.6126, -173.0875,  -40.4609, -155.6388, -165.7422, -181.4061,\n",
            "         -12.5613, -163.2524, -158.6903,  -50.8848, -174.3973,   -6.2066,\n",
            "        -129.9309, -173.5435, -173.5263, -134.6301, -174.2700, -160.3046,\n",
            "        -105.4759,  -76.0557, -166.1245, -173.8192,  -97.8632,  -97.9631,\n",
            "         -62.8572,  -98.2773, -169.6499, -163.3918, -106.6876,  -17.3936,\n",
            "        -173.8087, -114.4383, -164.6920, -168.8656,   -6.4848, -121.5101,\n",
            "        -172.4516, -187.6227, -122.4855,  -62.0801,  -80.6476,  -39.8721,\n",
            "        -174.2669, -148.2064, -165.7420, -163.6007, -166.8441, -155.4566,\n",
            "        -148.0857,  -93.5480,  -17.6312, -159.7176, -148.1603, -143.1195,\n",
            "        -175.2524,   -8.6595, -157.0559, -104.0634, -116.4774, -116.9908,\n",
            "        -157.5612,  -81.7190, -173.9731, -177.5565, -134.0106, -125.2738,\n",
            "         -26.3617,  -98.8060, -174.1950, -120.1122,  -11.8516,  -12.6222,\n",
            "        -142.7048, -174.2154, -139.4658, -183.7390,  -39.1290,  -52.3167,\n",
            "        -155.6665, -135.4891, -153.0321, -161.2991,  -21.5159,  -48.2066,\n",
            "        -173.9310,  -78.1269, -163.1894,   -7.1094, -179.0686, -127.9693,\n",
            "        -171.1390,  -67.6623, -186.2415, -130.5152, -159.2803, -166.2299,\n",
            "        -143.3483, -174.3241,  -26.0840, -102.0541, -159.6461, -132.3656,\n",
            "        -102.1816,   -7.0810,  -93.3373,  -34.7121, -173.9402, -168.2783,\n",
            "         -93.9938, -149.0668, -137.4319, -116.3290,  -54.2732, -167.5895,\n",
            "         -65.6781, -155.5882, -172.4538,  -87.1883, -144.6704,  -33.5610,\n",
            "         -98.2457, -171.1328, -173.5175,  -32.9134, -191.2018, -152.8185,\n",
            "        -174.7498, -144.1372, -142.3712, -120.0055,  -58.2513, -159.0763,\n",
            "        -172.6237,  -62.0470, -150.4951,   -6.4856, -159.6225,  -65.6137,\n",
            "         -86.6782,  -74.3385, -168.5476, -128.7070, -173.1535,  -19.8016,\n",
            "        -173.0070, -136.9915,  -13.2143,  -24.7467, -148.2903, -113.3855,\n",
            "         -80.2772,   -7.8390, -124.3279, -162.0616, -174.2606, -107.1111,\n",
            "         -26.3641, -173.4044, -166.6433, -174.4309, -174.7384, -148.4976,\n",
            "         -63.1499,   -9.0865, -173.8323, -185.0163,  -82.6461,  -58.3072,\n",
            "        -176.2566,  -46.1178, -136.2034, -174.7949, -100.7887,  -70.9410,\n",
            "         -80.9153, -106.5769, -121.3630, -166.6670,   -6.3696,  -76.7894,\n",
            "         -95.4103, -182.8251, -151.3211,  -71.7685, -181.6002,  -55.7805,\n",
            "         -98.8469, -120.0466, -189.6286, -191.2286, -163.9812,  -77.0005,\n",
            "        -150.2511, -133.4736,  -32.2975, -135.6953, -167.3630, -195.6450,\n",
            "         -99.8201, -104.1233,  -34.4010,  -93.9128, -162.3152,  -12.7651,\n",
            "        -186.4642,  -34.1689, -137.6556,  -21.3480, -166.8951, -173.8759,\n",
            "          -8.7549,  -96.5994, -142.1891,  -38.3982, -148.6464,  -54.7117,\n",
            "        -168.3578, -155.2528, -173.9744, -140.8781, -165.6294, -177.8444,\n",
            "        -174.3629, -134.1669, -163.0805, -155.3650,  -56.5625, -121.0208,\n",
            "        -173.0789, -134.0174, -145.5052,  -20.3469, -115.5256,  -11.7553,\n",
            "        -173.8947, -174.7258,  -77.4941, -175.0588, -147.6143,  -10.5020,\n",
            "        -173.9554, -173.9391,  -29.3545,  -81.0072, -174.6581,  -77.7049,\n",
            "        -173.8088,  -13.4421, -192.4634,  -62.4714, -156.5639, -162.6647,\n",
            "        -174.7299, -116.7602,  -58.8242,  -84.3537, -150.4286, -165.8336,\n",
            "         -12.7193, -158.7620, -121.0777, -131.9877, -176.1840,  -75.4607,\n",
            "        -137.7461, -166.2145, -111.0215,  -73.4662, -119.2677, -179.5150,\n",
            "        -174.2091, -103.3164, -134.6283, -162.1105,  -12.6160,  -11.8362,\n",
            "        -164.6896, -138.5416,  -40.9289, -116.9847, -165.4513, -130.9568,\n",
            "        -169.5796, -182.7692,  -80.3594, -191.1935, -165.3378,  -98.6880,\n",
            "        -169.5475, -168.1530,   -7.0245,  -84.7991,  -96.5293,  -92.0539,\n",
            "         -96.9246, -151.8172, -182.3618, -117.7743, -101.4940, -141.4528,\n",
            "        -181.8490, -136.0557, -113.3366, -112.3751, -144.1974, -160.9189,\n",
            "         -44.0926,  -37.8875, -120.4819,  -76.0449,  -22.8721,  -13.5769,\n",
            "        -169.5024,  -32.7454,  -97.5126, -101.6238,  -84.3025,   -7.0367,\n",
            "        -148.7305,  -45.3138, -105.1222, -142.2845,  -48.0963, -150.3638,\n",
            "        -173.9345,  -96.6806,  -47.7902,  -11.4055,  -66.2049,   -8.8277,\n",
            "        -160.2757, -132.9922,  -76.1321, -166.2754, -174.7017, -158.9155,\n",
            "        -145.5121, -139.7622, -102.1695,  -37.8570, -159.7483, -140.7695,\n",
            "         -91.1603,  -38.6671,  -96.4146, -157.6546, -173.9743, -155.3168,\n",
            "        -176.3781, -129.1812,  -96.2461,  -99.5091, -134.4658, -136.2816,\n",
            "         -17.9862,  -11.8417, -164.6854,  -64.0788, -174.5994,  -13.1292,\n",
            "        -181.0158, -159.7998,  -89.2076, -129.9349,  -77.4713, -162.3437,\n",
            "        -159.3908, -168.9385, -141.1293, -152.8153,  -14.7706,  -96.5138,\n",
            "        -173.1679, -141.1716, -165.2251,  -56.4778,  -95.3905, -165.2061,\n",
            "        -163.2615, -159.5257,  -74.8681,  -77.0200, -158.3686, -156.2292,\n",
            "        -174.0865, -135.3569,  -33.8540, -134.0098, -161.3604, -174.7502,\n",
            "        -145.2294,  -34.9173, -109.0126,  -51.0300, -174.2626, -110.3601,\n",
            "        -170.4286,  -76.8538, -163.1082, -108.9452, -173.3530, -173.8864,\n",
            "         -56.6424, -156.9609, -110.5101, -178.9141,  -88.8107,   -8.1073,\n",
            "        -182.7874,  -99.2751, -174.2574,  -92.6313, -130.4119, -145.3292,\n",
            "        -137.9936, -158.9419,  -78.8928, -132.6824,   -7.4683,  -37.6103,\n",
            "        -141.1804, -153.4080, -185.4246,  -76.1345, -143.1041, -166.3289,\n",
            "         -57.2079, -173.1068, -151.9385, -174.1699, -173.9978, -140.1929,\n",
            "         -99.3018, -164.9945,  -13.9009, -147.8433, -101.0378, -167.4099,\n",
            "        -172.6285,  -77.7303], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.01953318528831005 0.4950000047683716 0.0 119.25741577148438\n",
            "tensor([-162.7852, -179.2167, -147.7572,  -19.2756, -149.3914, -152.3162,\n",
            "        -151.6471, -131.1960, -117.5507, -157.3605,  -19.8806,  -99.6774,\n",
            "         -76.5995,  -74.3441,  -44.2426,  -81.3674, -166.7846,  -53.6870,\n",
            "        -157.4850,  -47.4914,  -52.0451, -102.2081, -117.7840, -147.9451,\n",
            "        -150.5091,  -89.9484,   -6.3008,  -33.2297, -157.3554,  -51.6622,\n",
            "         -84.2188,  -15.3965,  -82.5273, -154.3622,  -19.3611,  -17.5025,\n",
            "         -79.4345, -111.2706, -151.5868, -101.2065,  -53.3800, -134.3227,\n",
            "         -13.7325,   -6.0437, -142.9062, -118.7455,  -11.9420,  -35.3361,\n",
            "        -139.0178, -173.4349, -115.3176, -164.0768,  -10.1906,  -38.6609,\n",
            "        -149.5761,  -53.7110, -159.5812, -156.7193, -130.0508,  -92.7472,\n",
            "         -80.4915,  -34.5302,  -97.1652,  -20.7472,   -6.1371, -158.6460,\n",
            "        -117.4982, -103.9596,  -13.0799, -171.0788,  -80.1245,   -6.1082,\n",
            "        -158.3808, -157.6562,   -6.0384,  -92.4449,  -91.4538,   -9.6120,\n",
            "        -133.7724,  -96.9975,  -99.4714, -156.3938, -130.5943,  -99.9367,\n",
            "        -122.4386,  -92.0840,  -95.0651, -146.4968, -155.0399, -114.6907,\n",
            "          -6.4248,  -12.3703,  -95.7873,  -98.7615,  -16.7623,   -7.8368,\n",
            "          -8.6991, -171.7372, -150.4460, -170.9327, -158.6873, -134.1952,\n",
            "        -155.6262, -152.8693, -157.4400, -117.6179, -147.5696, -135.9218,\n",
            "        -140.5183,  -96.2105, -178.5476,  -38.6927,  -52.6308, -118.0114,\n",
            "        -150.4452, -102.2227,   -8.2052, -172.0692, -157.4657, -158.8292,\n",
            "         -90.3500, -118.6934, -122.7328, -141.4362, -144.1516,  -43.2857,\n",
            "        -161.1421,  -38.1532, -165.5683, -155.6644, -156.9511, -156.1399,\n",
            "        -150.4485, -141.8711, -142.5300,  -80.4273, -148.3244, -157.2245,\n",
            "          -7.2529, -134.2520, -157.0536,  -28.7820, -158.6306,  -11.1012,\n",
            "        -175.4519, -141.9026, -142.5435, -106.4390, -108.6491, -107.2774,\n",
            "        -139.7717,  -11.1070, -139.9792,  -80.3671,   -8.5668, -140.5879,\n",
            "        -157.2291, -162.8557, -109.9364,  -14.9902, -154.4706,  -74.4418,\n",
            "         -32.8834,  -81.8830, -176.2349, -158.6688, -145.1026,  -14.3553,\n",
            "         -64.1259, -157.5630,  -49.3552,  -81.8394, -111.5951, -106.8132,\n",
            "        -174.0031,   -8.0410, -146.2156, -161.4332,  -64.6749,  -55.6880,\n",
            "        -146.4320,  -29.7620, -145.1193,  -13.4209, -148.3873, -141.6330,\n",
            "          -6.9946,   -7.8077,  -66.7667, -181.2723,   -9.3615, -147.1470,\n",
            "         -10.8811, -138.6419,   -7.4405, -136.0942, -143.7101,   -6.0439,\n",
            "        -152.1195, -116.0448, -146.1390,  -79.4325, -148.0593,  -77.5737,\n",
            "        -151.7439, -153.6400, -118.8974,   -8.9968, -163.7005, -158.5396,\n",
            "        -155.2368, -169.5735, -105.7474, -153.4139, -146.4473, -118.5090,\n",
            "        -144.8477, -154.2250,   -7.6113, -116.4174, -157.5402,  -41.8170,\n",
            "        -110.6951,   -6.4008, -103.5599,  -23.7548, -155.2616, -126.3429,\n",
            "        -139.5812,  -26.6564, -147.6600, -158.7999,  -66.3553, -155.8665,\n",
            "          -6.0654, -134.3182, -114.7810, -133.4148,  -35.3140,   -7.4748,\n",
            "         -17.1821, -115.0601,  -52.1962, -160.8916, -159.6263, -163.8680,\n",
            "        -117.9480, -179.5961, -156.4847, -150.3780,  -13.6008,  -43.5969,\n",
            "         -81.9788, -153.9085, -159.5840,  -52.8396,   -6.3035,  -90.4845,\n",
            "        -146.0344,  -44.5077,  -74.9460, -156.9473, -156.2625, -138.6422,\n",
            "         -33.0835, -153.8285,  -83.1189,  -53.0632,  -13.1694,  -75.1137,\n",
            "        -173.3426,   -8.0695, -119.6878, -157.6192, -156.9001, -158.8014,\n",
            "         -77.5157,  -17.3468,  -91.7945, -159.2464, -154.2008, -130.5776,\n",
            "          -6.0340,  -71.8158, -133.8348, -149.4709,  -73.0258,  -37.8425],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.01846534200012684 0.4950000047683716 0.0 102.36856079101562\n",
            "tensor([-129.3497, -138.6264, -108.5975, -130.9338, -129.1188,  -15.0953,\n",
            "        -139.0544, -117.9293, -129.5829,  -76.0414,  -72.9399,  -76.7696,\n",
            "          -5.8914, -127.6939,  -29.3202,  -62.6183, -138.0107, -138.7286,\n",
            "         -85.1597, -136.8262,  -63.3051,   -6.3904, -130.4719,  -96.7606,\n",
            "        -128.0416, -122.7038,   -5.9322, -130.5927,   -8.0812, -138.1219,\n",
            "         -12.6709, -152.1447, -133.1019, -109.8347,  -27.0378,  -68.6743,\n",
            "        -138.7067,   -5.8979, -106.3932, -124.0686, -130.8195, -105.3561,\n",
            "        -141.5927, -127.9224,  -20.5720,  -94.2103, -138.4812, -141.5054,\n",
            "        -135.8369, -131.6790,  -73.5708,  -94.7887, -127.7091, -139.3912,\n",
            "         -22.5051,  -15.8589, -124.2720, -162.0429,  -73.0361, -117.7748,\n",
            "          -5.9818,  -71.9680,  -81.7672, -146.2990, -134.0099, -104.7228,\n",
            "        -132.1662, -131.7642, -133.0488,  -15.9137,  -78.4166, -138.6272,\n",
            "        -121.7789,  -54.4699,   -7.7654, -134.7806,  -21.0923, -138.3368,\n",
            "         -10.0015, -135.7294,  -93.8510, -132.6330,  -11.1698, -139.4072,\n",
            "         -50.7888,  -66.5963,  -99.7123, -140.1218, -111.6316,   -5.9796,\n",
            "        -119.1600, -131.3641,  -31.3972,   -7.4269,   -8.5098,  -95.4170,\n",
            "        -111.6436,   -6.2974, -118.5927,  -13.7699, -135.0467,   -9.7243,\n",
            "         -90.1311, -135.8042, -120.3227, -165.7216,  -10.3987, -123.6657,\n",
            "          -8.0219,  -13.3474,  -80.1495, -157.4153, -130.6006,  -97.9233,\n",
            "        -125.0754, -134.2070, -130.3130,  -18.6938, -110.5888, -131.1440,\n",
            "        -119.8352, -110.3994,  -30.6271, -125.4673,  -20.1639, -115.5957,\n",
            "         -13.8509, -114.6871, -138.4860,  -50.7456, -128.8829, -130.3267,\n",
            "        -107.9211,   -8.0279,  -76.0106,  -96.5339, -134.9153, -123.3478,\n",
            "          -6.9966, -123.9901,   -6.2902, -138.4913,   -7.8214,  -97.4710,\n",
            "         -49.9623,   -6.3539, -141.1043,  -91.6553, -132.0329,  -30.4214,\n",
            "        -104.1593,  -88.8282, -124.4174, -110.3363,   -8.0027, -122.1740,\n",
            "          -5.8943, -114.1894,  -20.5604,  -34.9743,  -60.2656, -122.2035,\n",
            "        -105.6334, -130.5750, -139.5141,  -30.5754, -113.8102, -105.9313,\n",
            "        -113.2299,  -63.6400,  -18.6998, -120.3798,   -6.8933, -135.5085,\n",
            "          -5.8918, -127.0399,  -59.3037, -139.7350,  -56.6151, -115.4329,\n",
            "        -139.0237,   -6.4931,  -55.4620,  -89.6766,  -97.0201, -139.8223,\n",
            "         -13.3812, -120.4336,  -13.3259, -130.8129,  -80.0865,  -45.7225,\n",
            "        -139.0963,  -76.3798, -123.9766, -139.7899, -134.2644,   -7.9734,\n",
            "         -68.9355,  -50.0070, -128.4165,  -78.2184,  -29.6565, -138.6914,\n",
            "         -13.8275,   -8.4549,  -16.1246, -127.9366, -137.4342,  -25.5939,\n",
            "        -140.5965, -136.1870,  -95.2449,  -17.8319,  -69.1995,  -60.7234,\n",
            "        -138.7393, -134.0330,  -17.7909, -135.8157, -114.7448,  -92.6445,\n",
            "          -8.1177,  -98.0812, -127.0291, -139.3305, -134.1731, -117.7902,\n",
            "        -134.6437,  -31.6601,  -94.9544, -138.2693, -118.2036,  -55.2445,\n",
            "         -17.2419, -133.1209,  -12.3008,  -97.4969,  -17.5772,  -11.1625,\n",
            "        -104.7236,  -45.4957, -129.5775,  -58.3330, -131.3151,   -8.4601,\n",
            "        -159.1586, -129.4556,  -65.2054,  -78.1318, -143.0348, -133.6860,\n",
            "          -8.1235, -135.4033,  -11.7507,  -62.3521, -113.0329, -130.8250,\n",
            "        -107.1097,  -95.1082, -137.4976,   -8.7287,  -16.3444, -112.8271,\n",
            "         -92.8550, -148.6407,   -7.1570,  -67.6461,   -6.7160, -129.0147,\n",
            "         -15.9711, -136.0508, -111.5401, -139.1481,  -80.3607,  -28.8747,\n",
            "        -139.7664,   -7.5498,  -10.5223, -134.1647, -135.6350,  -90.5145,\n",
            "         -31.3347,   -8.1864,   -9.6975, -134.6088,  -11.0774, -147.4314,\n",
            "        -134.5287, -138.4986, -120.9606, -134.4094, -132.4533, -129.8102,\n",
            "        -124.6419, -138.7832, -100.3521,  -80.0382,   -9.9902, -130.5733,\n",
            "          -5.8898,  -24.7413,   -9.9887,  -30.3535,  -74.8985,  -98.0259,\n",
            "         -93.7800, -139.5038, -125.5668,  -10.0800,  -88.7076,  -55.4452,\n",
            "         -53.4308,  -59.7533,  -15.1497, -104.4929,  -10.1977, -138.5144,\n",
            "          -5.9854,  -16.9852, -130.8722, -128.4745,  -34.1065,  -47.8706,\n",
            "        -139.5777,  -13.2188, -156.3425, -105.5555, -116.5501,  -52.3975,\n",
            "          -6.2733,  -23.6067,  -18.8009, -128.8591,  -18.1530, -148.6958,\n",
            "        -139.9724,  -27.4070, -139.3600, -139.5455,  -97.9257,   -7.8323,\n",
            "        -102.4830,  -66.9344, -135.8199, -161.9787,  -15.6007,  -54.1725,\n",
            "         -12.0392, -133.4942, -135.3772, -129.2635,   -6.4435,  -59.9037,\n",
            "        -119.0647, -130.1488, -117.5270,  -46.2067, -123.6564, -130.1013,\n",
            "        -137.5928,  -44.1705,   -6.9464, -116.5560, -138.6371, -111.3220,\n",
            "          -5.9059,  -17.6001, -115.9466, -139.2399,  -73.2671, -113.4307,\n",
            "        -132.7177,  -54.8095, -139.3729,  -50.2775,  -74.3735, -115.4287,\n",
            "         -12.9313, -129.5752,  -13.2395, -128.0467,  -15.7111,  -30.5716,\n",
            "        -108.5479, -107.9376, -115.5716,  -93.0744, -124.0687,   -9.3768,\n",
            "         -82.0515,  -37.5088, -125.2079,  -21.2433,   -6.0345,  -47.2884,\n",
            "         -15.8983,  -93.0334,   -7.2568, -143.3669, -138.7144, -139.7720,\n",
            "        -111.4636,  -89.0682,  -68.5902,   -6.0730, -128.8512,  -55.3329,\n",
            "        -138.0786, -132.9269, -135.0571, -138.9538,  -58.0551,  -60.2121,\n",
            "        -118.7965, -153.4415, -138.1214, -134.0450, -112.8469,  -95.5619,\n",
            "        -136.5642, -132.9167,  -48.9752,  -98.1270, -133.6936,  -35.1415,\n",
            "          -7.8593,  -42.2966,  -16.6931, -117.6966,   -7.5569,  -27.9268,\n",
            "         -49.4167,  -27.8474, -129.9939, -138.6426, -138.3010,  -19.1859,\n",
            "         -57.4099, -139.5008, -107.7505, -139.6400, -135.2310, -130.5730,\n",
            "          -6.3766, -136.6220,   -7.8147, -132.6351, -137.5208, -130.4477,\n",
            "        -133.6052, -138.6802, -133.0753,  -11.7650, -108.2311, -128.2464,\n",
            "        -106.9536, -142.1703,   -9.6212, -129.6103,  -17.6939,  -94.7222,\n",
            "        -124.1755,  -39.3826, -135.7635, -111.6749,  -18.1339, -132.7524,\n",
            "          -5.8925,  -11.1031,  -60.2485, -134.1499, -114.1678,  -75.1780,\n",
            "         -15.8537,  -47.6850,  -10.4957, -121.0769,  -11.9820,  -60.8361,\n",
            "        -135.5513,  -68.8854,  -22.0816, -134.6009, -140.0442,   -6.5798,\n",
            "        -129.4859, -132.9139, -109.5711, -120.9447,  -10.3042,  -27.9292,\n",
            "        -104.4023, -136.7920,  -29.4858, -102.1462,  -91.7373, -125.2471,\n",
            "         -19.5172, -118.2155, -125.4216,   -8.8110, -138.7647,  -46.8376,\n",
            "        -134.3342, -129.3693,   -7.0942,  -65.5897,   -5.9677, -139.8549,\n",
            "        -128.2374,  -34.3053], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.019342398270964622 0.4950000047683716 0.0 85.7794189453125\n",
            "tensor([ -81.2289,  -50.4319,  -74.6773, -117.1777, -121.5600,   -7.9113,\n",
            "        -132.1146,  -19.7783, -110.8978,   -6.8040,   -6.8200,  -61.6171,\n",
            "        -117.7106,  -75.2263,   -6.2038,  -52.5318, -116.6757, -114.8624,\n",
            "          -9.1541,  -69.7808, -119.7327,  -14.7927,  -28.5155, -119.3399,\n",
            "        -113.8782,  -92.6924,   -6.2002,  -45.6602,   -5.8787, -121.5827,\n",
            "          -6.6922, -103.4949, -117.6202, -111.8618,  -59.9214, -121.3545,\n",
            "         -77.7404,   -6.0770,  -55.8140, -107.7506,  -73.6425, -107.3805,\n",
            "         -13.8610,  -14.5798, -112.9253,  -37.8110,   -7.9421, -124.9194,\n",
            "         -79.4698,  -29.3165,  -10.3760, -110.7879,  -78.7732,  -69.9209,\n",
            "         -13.9515,  -92.7674,  -76.5838, -114.8961,  -13.1338, -121.5967,\n",
            "          -6.5580,  -85.5503,  -16.6354,  -64.0308,  -72.7106,  -92.5932,\n",
            "        -108.7133, -109.6182, -112.6109,  -18.9279,  -13.1364, -117.5704,\n",
            "        -121.4739,   -7.0472,   -8.9275,   -6.6102,  -30.2195,  -69.3944,\n",
            "          -7.2983,  -67.3936,  -17.2842, -101.2918,  -60.1162, -108.1907,\n",
            "         -91.4667,   -6.0925,  -75.8021, -125.0685, -115.0843, -120.0917,\n",
            "          -7.0771,   -7.7471,   -5.7933,  -78.9256,  -87.2747, -123.0748,\n",
            "        -103.1957, -102.2461, -104.2928,  -93.0351,  -80.8977,   -6.2108,\n",
            "         -28.5033, -120.2255,  -80.2217, -125.7372,   -6.6840,  -67.1682,\n",
            "         -93.2843,  -31.4368,  -32.9632, -128.4172, -108.9037,  -74.0729,\n",
            "        -100.5388, -121.1491,  -72.4536, -122.3776,  -73.6909, -113.2737,\n",
            "          -5.9964,  -17.6747,   -7.7392,   -7.9666,   -5.8550,  -73.1123,\n",
            "          -6.5618,  -35.0122, -108.8867, -105.4561,  -99.7793, -112.1530,\n",
            "        -120.3251,   -8.6302,   -7.3859, -100.4475, -120.5256,   -9.0074,\n",
            "          -7.9353, -102.5888,   -8.7814, -121.6341, -117.6098,  -75.9123,\n",
            "        -117.3387, -121.3551,  -87.5854,  -79.6086, -118.5082,  -22.7449,\n",
            "         -41.7675, -107.4981, -110.1345, -114.7657,  -76.1878,  -80.5676,\n",
            "          -8.9052, -121.5677,   -8.2663, -101.0676,  -76.7073, -100.5299,\n",
            "        -110.9002,  -30.7869, -119.7637,   -9.4816,  -15.3853, -119.8523,\n",
            "        -120.7567, -132.0392,   -6.5645, -101.7195, -109.9569, -118.5550,\n",
            "         -45.4428, -130.5363,  -92.1178,  -71.3743,  -71.5636, -121.2087,\n",
            "        -120.5829,   -7.1417,  -14.3552, -122.8579,  -75.2095, -112.6891,\n",
            "         -16.6244,  -81.2716,   -6.5909, -116.8557, -122.4710,  -44.5623,\n",
            "        -121.4063,  -89.5813,  -92.7481, -109.9070,  -76.6793,   -6.4046,\n",
            "         -10.4279,  -27.8841,  -79.8866,   -8.4410,  -18.0191, -102.6976,\n",
            "          -8.2795, -116.1678,   -6.1557, -107.7662, -115.8444,   -7.8636,\n",
            "         -71.3660, -121.6138,  -49.4896,   -7.5271,  -98.9612, -118.6028,\n",
            "         -33.7429, -137.9455, -122.3720,  -16.0544,   -6.0285,  -74.8546,\n",
            "          -5.8147,  -67.0586, -120.6104,  -28.5022, -125.2891, -112.4188,\n",
            "        -119.4487,  -33.6353, -129.5394, -104.4589,   -6.7250, -134.9003,\n",
            "        -101.0236,  -80.0213,  -42.0615,  -84.6598,   -5.7502, -100.7688,\n",
            "          -9.6173, -116.2415,  -48.8872, -116.8690, -115.8518,  -18.9819,\n",
            "         -31.5140,  -45.2967, -117.4000,   -6.6852,   -6.7771, -107.7976,\n",
            "         -26.8085,  -94.8160,   -5.9790,  -79.9887,  -43.4681, -112.4200,\n",
            "         -12.1560,  -99.6023, -106.0583,  -82.8373,  -57.3483, -116.4584,\n",
            "        -117.7735, -106.0792,   -6.4391, -123.4296,   -9.0856,  -43.9079,\n",
            "         -21.8144,  -88.6546, -117.8447, -121.6194,  -53.3936, -109.2077,\n",
            "        -111.2083,   -5.7513, -105.6579,   -6.0357, -121.2523,  -11.5848,\n",
            "         -93.2882,  -68.4074,   -5.9743, -121.3880,   -6.0569,  -94.0243],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "repr, std, cov, closs 0.01903604157269001 0.4950000047683716 0.0 70.21259307861328\n",
            "tensor([ -91.5377,   -6.8881,  -77.7517,  -63.4686,  -84.4271,   -8.4738,\n",
            "         -49.5752,   -5.9751,   -7.8339,  -85.4797,   -7.7212,  -19.4460,\n",
            "         -11.6972,   -7.3155,  -12.1797, -100.1481,  -14.1872,   -7.0339,\n",
            "         -96.9160,  -54.9322,  -23.8542, -103.0133, -100.2284,  -99.8507,\n",
            "        -103.3850,  -58.5003,   -5.8599,   -5.9290,  -66.0122,  -67.8723,\n",
            "        -103.3414, -100.1556,  -36.7507,  -76.9888,  -91.3812,  -38.9179,\n",
            "          -7.5468, -103.0571,  -54.0698,  -69.1845,  -66.6641,  -85.3236,\n",
            "          -5.6506,  -12.2634,  -49.5480,  -85.7117,  -93.1927,  -94.7643,\n",
            "          -5.7707,  -95.8246,  -52.3610,  -15.2293,   -5.8844,  -82.1632,\n",
            "         -82.8255,  -87.7287, -108.7558,   -9.5997,  -99.3994,   -5.9580,\n",
            "         -10.5641,  -69.2935,  -80.0420, -103.3372, -121.1915,  -13.4163,\n",
            "         -19.7869, -120.1108,  -53.3945,  -56.5933,   -8.5754,  -93.4433,\n",
            "          -6.8441,  -85.5686,   -6.0400,   -5.8265,  -78.0571,  -85.0924,\n",
            "         -75.1281,  -97.8958,  -94.8585,  -13.3611,  -93.2974,  -13.5807,\n",
            "        -111.8647,  -12.5803,  -81.4452,  -16.7377,   -6.0966,  -95.2122,\n",
            "          -5.6186,  -97.7783,  -75.2565,  -31.1173,  -72.2639,  -94.5366,\n",
            "          -7.4831,  -55.7488, -103.0714,  -17.3804,  -33.0445, -103.4724,\n",
            "         -96.5590, -107.4739,  -68.4931,  -87.9886,   -5.6206,  -47.9485,\n",
            "        -103.0969,   -5.8702, -103.3329, -100.2023,  -59.8761,  -44.9847,\n",
            "         -49.4283,  -54.0769,  -67.2402, -102.2258,  -81.2773,  -69.5872,\n",
            "         -96.1446,  -89.5374,   -6.3685,  -10.4144,  -23.1047,  -26.4420,\n",
            "         -50.0700,  -52.5991,  -59.0483,  -82.1175,   -8.7469,   -5.6645,\n",
            "         -17.6415,  -43.7830,  -49.9146,   -5.9263,   -6.1503,  -59.7676,\n",
            "         -87.1458,  -14.1656,   -5.9235,  -82.5980,  -59.9580,  -98.5728,\n",
            "         -90.0222, -116.3807, -102.1368, -104.8372,   -6.0386,  -11.2508,\n",
            "         -95.4787,  -96.6790,  -74.7554, -103.0953,   -5.6259,  -10.0748,\n",
            "         -19.3666,  -92.5161,  -85.5015,  -97.6199,  -88.0983,  -34.0626,\n",
            "         -88.6083,  -65.3470,  -97.0313,   -8.4482, -103.3295, -119.7074,\n",
            "         -93.8211,  -96.5861,   -8.6336, -108.0306,  -56.3010,  -26.3704,\n",
            "         -68.7777,  -97.5922,  -11.9855,  -99.5465,   -5.9259,  -95.0234,\n",
            "         -85.6825, -101.2624,  -49.7599,  -71.6007,  -57.4473,   -9.2532,\n",
            "         -22.5077,  -99.7950,  -96.5687,  -37.0259,  -98.9979,  -35.9579,\n",
            "          -9.3377,  -76.2890,  -98.3120,  -74.0391,   -5.8557,  -26.0982,\n",
            "         -97.2480, -113.2133, -106.3824, -101.6817,   -7.0651,  -89.4335,\n",
            "         -88.9740,  -53.9704,  -43.5729, -103.5688,  -26.6601,  -66.5850,\n",
            "         -42.6760,  -93.0102,  -96.5996,  -93.9842,  -54.0471,  -97.3883,\n",
            "        -106.9374,  -15.9950,  -20.4148,  -94.5519,  -92.9312,  -35.5575,\n",
            "         -98.3901,  -91.1854,  -86.4895, -120.8306, -101.5409,  -15.0913,\n",
            "          -8.6989,  -86.4245, -102.9867,  -84.6402,  -37.1857,  -22.7009,\n",
            "         -70.3337,  -91.2066,  -10.7939,   -5.9703,  -12.3402,  -99.6463,\n",
            "        -112.3456,  -94.8834,   -5.9518,   -8.9597,  -10.1212,  -17.5035,\n",
            "         -81.5367,  -46.4538,  -63.6868,  -95.6965,   -5.6180, -109.2252,\n",
            "         -50.6839,  -76.2257,  -97.3131,  -98.3811,  -95.1344,   -9.6564,\n",
            "        -103.3450,  -81.8773,  -81.2029,  -95.2573, -103.3400,  -15.1215,\n",
            "         -31.0476,  -50.2281,  -92.4853,  -95.6026,  -96.6398,   -7.2630,\n",
            "        -103.1529, -103.1028,  -47.3485,  -11.1513,  -95.6754,  -50.6332,\n",
            "          -5.7216,  -98.1124, -100.1040,  -83.1408, -105.1534,  -95.4625,\n",
            "         -50.8830,  -86.1307,  -94.5474, -103.8796,   -7.8011,   -6.9139,\n",
            "         -90.9467,  -15.7747,  -67.1846,   -7.2064,  -51.2676,  -76.3893,\n",
            "         -21.3932, -107.9698,   -5.6369, -103.3723,  -15.8833,  -37.1537,\n",
            "          -7.9712,  -86.7000,  -92.8400,  -97.5119,  -39.7771,  -77.5282,\n",
            "         -64.2347,   -5.9306,  -55.0272,  -91.8011,  -97.5050,  -86.2183,\n",
            "         -46.7396, -103.3520, -107.8774,   -6.3339,  -10.7026,  -94.5905,\n",
            "         -66.9747,  -95.8696,  -84.9460,  -54.5910, -103.4591, -128.6699,\n",
            "         -66.0667,  -86.4797,  -46.2413,  -52.6361,   -5.6286,  -97.7508,\n",
            "        -103.4463, -104.5418,  -95.5496,  -94.9396, -103.4821,  -86.9621,\n",
            "         -95.4483, -102.1313,  -94.7111,  -58.7082, -102.6081,  -96.1656,\n",
            "         -58.4184,  -45.0931,   -8.7432, -102.8125, -102.9807,   -5.9182,\n",
            "         -50.6590, -106.2957,  -70.4133,  -11.2501,  -53.1325,  -60.7586,\n",
            "         -77.4169,  -60.2162, -120.0611,  -50.0324,  -88.8602,  -17.0156,\n",
            "         -71.9341,  -65.7734,  -12.1538,  -98.4027,  -76.6034,  -20.6587,\n",
            "         -13.9347,  -97.4630,  -96.1570, -109.5627,  -97.0134,  -86.5088,\n",
            "         -12.0553,  -27.1257,  -27.6420, -104.6273, -104.8440,   -8.8586,\n",
            "         -86.2903,  -76.6363,  -21.0324,   -5.6645,  -28.1399,  -94.5465,\n",
            "         -91.8647,  -95.3066,  -53.0361,   -6.4416,  -92.6611,   -5.8628,\n",
            "          -6.5691,  -99.1924,  -73.7229, -103.4197,  -68.5122,   -5.6318,\n",
            "         -50.2846,  -97.9036,  -15.9978,  -73.9301,   -6.9610,   -5.9494,\n",
            "         -95.9515,  -79.4231,  -26.9108,  -12.2841, -103.5019, -100.0728,\n",
            "          -7.9271,  -99.7818, -103.3310,  -74.2560, -103.3486,  -32.6532,\n",
            "         -90.6471,   -5.6285, -125.4384, -115.6278,  -82.7466,  -81.9815,\n",
            "         -17.3272, -100.6565,  -62.2113,   -6.2307,  -93.7849, -103.0821,\n",
            "        -103.2359,   -5.6736,  -99.3605,  -52.2812, -103.4330, -103.3400,\n",
            "          -6.0333,  -57.3216,  -91.8313,  -74.6065,  -79.4590,  -92.0887,\n",
            "         -32.6406,  -95.7985,  -85.8986, -103.3559,   -6.0757,  -58.0297,\n",
            "         -47.7848, -103.1527,  -68.4882,  -70.1721,  -72.8305,  -29.6440,\n",
            "         -94.5152,  -94.9843,  -93.9412,  -95.4738,  -96.5618,  -70.2069,\n",
            "        -106.6844,  -70.8294,  -70.1777,  -90.1678,  -94.6738,  -62.9981,\n",
            "        -102.4330,  -10.7732,  -18.2899, -104.5664,  -50.3984,   -6.5676,\n",
            "          -6.4959,  -99.8039,  -81.3743,  -10.8356,  -68.0529,  -59.0053,\n",
            "         -86.3957,  -91.7081, -101.6936,  -14.5645,  -47.9126, -103.3294,\n",
            "         -70.8222, -112.3653,  -98.2422,  -34.9536,  -74.7663, -101.8569,\n",
            "         -97.6415, -111.0281, -105.5651,  -89.4123,  -12.8277,  -68.8792,\n",
            "         -12.4394,   -6.5750,  -87.6380,  -91.9196,   -6.1733,  -69.8467,\n",
            "         -72.8859, -100.0750,   -8.2216, -103.0630, -103.3098,  -20.5970,\n",
            "         -12.5636,  -13.9990, -100.1852,  -77.7675,  -59.9393,   -5.6201,\n",
            "         -25.2901,  -74.4621], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.019186832010746002 0.4950000047683716 0.0 63.26966857910156\n",
            "tensor([-103.0493,  -43.1681,  -72.0768,  -31.0641,  -16.8295,  -76.7258,\n",
            "         -59.6884,   -7.9444,  -10.7988,  -87.7021,  -40.5723,  -27.8113,\n",
            "         -41.8576,   -5.4911,  -79.6927,   -8.1043,  -82.8771, -100.8067,\n",
            "         -86.6216,  -79.3236,  -89.8917,  -10.5069,  -73.9889,  -15.4235,\n",
            "         -94.3455,  -69.3193,  -77.4403,  -63.8355,   -7.6388,   -7.0075,\n",
            "         -35.0121,  -38.0651,  -70.7547,  -49.3059,  -15.4333,  -90.0738,\n",
            "        -103.0013,  -73.6195,  -79.3997,  -83.2361, -103.7243,  -64.8448,\n",
            "         -84.0754,  -66.3099,  -44.6820,  -71.8518,   -6.1443,  -87.6223,\n",
            "         -89.0549,  -11.4788,  -76.3430,   -5.7756,  -77.9782,   -5.8727,\n",
            "          -8.9129,   -7.8777,  -92.7139,  -18.7125,   -9.4176,  -23.9563,\n",
            "         -76.1260,  -71.8168,  -85.3358,  -82.8277,   -8.3211,  -86.5397,\n",
            "         -55.1170,  -35.2308,   -9.1920,  -86.7477,  -80.7993,   -6.1679,\n",
            "         -16.5445,  -82.1787,  -85.1346,   -6.6340,   -8.1048,   -8.5119,\n",
            "         -42.9623,  -77.4500,   -5.6433,  -88.1971,  -39.4064,  -87.5206,\n",
            "          -5.8550,  -68.7928,  -85.3240, -103.1955,  -23.6157,  -10.2483,\n",
            "         -36.8158,  -90.3579,  -67.7123,  -27.5945,  -79.6587,  -72.1347,\n",
            "          -9.0505,   -6.3261,  -66.3327,  -75.7715,   -9.7034,   -9.6762,\n",
            "          -7.2720,   -5.5366,  -70.3013,   -5.7634,  -75.7248, -103.9659,\n",
            "         -66.2873,   -8.8326,  -42.3407,  -64.5225,  -92.6104,   -6.2112,\n",
            "         -82.8298,   -8.2583,  -85.5077,  -53.2106,   -5.9837,  -11.1227,\n",
            "          -8.0125,  -87.6456,  -83.9585,  -32.4965,  -77.1046,   -6.6171,\n",
            "         -82.1546,  -79.4413,  -97.9279,  -69.0759,  -42.3743,  -82.8458,\n",
            "         -12.3182,  -39.5239,  -31.7807,  -59.8905,  -44.2456,  -67.7710,\n",
            "          -8.1957,  -84.5503,  -13.1825,  -39.9390,  -53.7313,  -79.3745,\n",
            "          -6.9831,  -42.2999,  -65.5013,   -7.3157,  -12.2042,  -76.4193,\n",
            "         -44.4209,  -82.8660,   -9.9925,  -88.3282,   -9.2095,   -6.0727,\n",
            "         -58.7017,  -72.1939,  -82.6724,  -32.5622,  -13.1405,  -41.8239,\n",
            "         -83.8346,   -5.7097,  -84.4374,  -84.0177,   -7.3326,  -71.2749,\n",
            "         -19.8441,  -32.2690,  -84.4386,  -25.9441,  -62.4510,   -7.6797,\n",
            "          -5.4471,  -80.9370,  -20.4328, -102.3160,  -82.8447,   -5.6284,\n",
            "          -6.4110,  -34.9731,  -63.2149,  -10.1371, -108.0633,  -78.5235,\n",
            "         -73.7715,  -90.9282,  -57.4163,  -34.6877,  -87.5316,  -70.3227,\n",
            "          -6.7326,  -88.6323,  -77.9629,   -5.9174,  -90.1245,  -36.1670,\n",
            "         -87.6941,   -5.9179,   -9.6435,   -7.8806,  -74.7529,  -50.3407,\n",
            "         -42.7029,  -17.1918,  -79.1449,  -87.9044,  -82.6366,   -6.9154,\n",
            "         -32.6506,  -76.0366,  -80.6307,  -87.9101,  -71.4336,  -95.7850,\n",
            "        -103.1239,  -71.3225,  -78.6422,   -6.2649,  -11.7590,  -12.0029,\n",
            "         -87.5968,  -37.3707,   -5.8964,  -14.5596,  -62.1882,   -5.7728,\n",
            "          -6.2228,  -54.8998,   -8.5177,  -30.9003,  -12.8997,  -87.2660,\n",
            "         -84.2524,  -38.5111,  -14.9424,   -7.2350,  -55.7403,  -72.6725,\n",
            "          -8.4067,  -82.5980,   -6.1568,   -8.3815,   -7.2515,  -73.3330,\n",
            "          -5.4631,  -97.6117,  -63.8614,  -49.3745,   -9.7275,  -13.1101,\n",
            "         -76.1106,  -72.0355,  -77.6524,  -11.5119,  -95.2328,  -64.1567,\n",
            "         -76.8654,  -96.7375, -102.4606,  -66.7231,  -81.4217,   -7.3517,\n",
            "         -80.0009,  -71.3676,  -67.0366,  -92.5667,  -79.5023,   -7.4761,\n",
            "         -87.8934,  -42.7464,   -5.7313,  -91.6778,  -78.6461,   -5.4633,\n",
            "         -68.0278,   -5.9850,  -52.8282,  -25.7748,   -5.5887,  -68.7386,\n",
            "          -6.1912,  -26.6472,  -63.5184,   -5.7506,  -79.4091,  -62.9532],\n",
            "       device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
            "repr, std, cov, closs 0.01933133415877819 0.4950000047683716 0.0 49.8986930847168\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(10):\n",
        "    # # buffer=[]\n",
        "    print(\"#### simulate ####\")\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    agent.train_jepa(train_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "d9d9d30c-c0d2-441c-ed06-ca192def787f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "###save"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K=torch.rand(2,7)*10\n",
        "K = F.normalize(K, dim=-1)\n",
        "print(K)\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvCFiajfkNnP",
        "outputId": "263c2dc3-f59d-4972-891a-fc9526083a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0141, 0.1213, 0.4933, 0.4729, 0.5260, 0.1898, 0.4533],\n",
            "        [0.2187, 0.6114, 0.2308, 0.5138, 0.4547, 0.1117, 0.2044]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZeny7pRU6bG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cc5da7-fb7a-428a-d481-e1e18a9fe024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.8929]], device='cuda:0', requires_grad=True)\n",
            "argm 0.3448590040206909 0.46135491132736206\n",
            "argm 0.34481173753738403 0.22136220335960388\n",
            "argm 0.344801664352417 0.2574569582939148\n",
            "argm 0.34480124711990356 0.2916858494281769\n",
            "argm 0.344801127910614 0.28045156598091125\n",
            "argm 0.34480106830596924 0.26978030800819397\n",
            "argm 0.3448010981082916 0.30337217450141907\n",
            "argm 0.3448011875152588 0.2915523052215576\n",
            "argm 0.344801127910614 0.2803245782852173\n",
            "argm 0.3448010981082916 0.2696596682071686\n",
            "argm 0.3448010981082916 0.3032577931880951\n",
            "argm 0.3448011875152588 0.2914435565471649\n",
            "argm 0.344801127910614 0.28022152185440063\n",
            "argm 0.3448010981082916 0.269561767578125\n",
            "argm 0.3448010981082916 0.3031650483608246\n",
            "argm 0.3448011875152588 0.29135534167289734\n",
            "argm 0.344801127910614 0.2801375687122345\n",
            "argm 0.3448010981082916 0.2694820165634155\n",
            "argm 0.3448010981082916 0.3030891716480255\n",
            "argm 0.3448011577129364 0.29128339886665344\n"
          ]
        }
      ],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "batch=sx.size(dim=0)\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone())\n",
        "print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e2)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 20\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uT9m-J1BUWyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "64a96179-0b71-43d0-8ae7-c6cfae51844c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.4)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.7.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240706_095823-5d0bao6j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/5d0bao6j' target=\"_blank\">fanciful-shadow-1</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/5d0bao6j' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/5d0bao6j</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErwMF9NijD17",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUCet57LcPdf",
        "outputId": "a27b59d8-f5a6-418b-a8dc-cc6b7a71607b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0001010101010102\n",
            "1.0101020202020203\n",
            "1.0203050709111317\n",
            "1.0307163471449186\n",
            "1.041342288868062\n",
            "1.0521896043771044\n",
            "1.0632652844231791\n",
            "1.0745766172361917\n",
            "1.0861312045183014\n",
            "1.097936978480457\n",
            "1.1100022200022202\n",
            "1.1223355780022448\n",
            "1.1349460901146295\n",
            "1.147843204775023\n",
            "1.1610368048299085\n",
            "1.174537232793047\n",
            "1.18835531788473\n",
            "1.2025024050024051\n",
            "1.2169903857855666\n",
            "1.2318317319536833\n",
            "1.2470395311136053\n",
            "1.2626275252525254\n",
            "1.2786101521544562\n",
            "1.29500259000259\n",
            "1.311820805457169\n",
            "1.3290816055289738\n",
            "1.3468026936026936\n",
            "1.36500273000273\n",
            "1.383701397537014\n",
            "1.4029194725028058\n",
            "1.4226789016929862\n",
            "1.4430028860028858\n",
            "1.4639159713072756\n",
            "1.485444147355912\n",
            "1.5076149555254033\n",
            "1.5304576063666973\n",
            "1.5540031080031078\n",
            "1.5782844065656563\n",
            "1.6033365400032062\n",
            "1.6291968067774514\n",
            "1.6559049511508523\n",
            "1.6835033670033666\n",
            "1.7120373223763048\n",
            "1.7415552072448617\n",
            "1.7721088073719646\n",
            "1.8037536075036067\n",
            "1.8365491276400356\n",
            "1.8705592966704068\n",
            "1.9058528683056974\n",
            "1.9425038850038838\n",
            "1.980592196474548\n",
            "2.020204040404039\n",
            "2.0614326942898358\n",
            "2.104379208754207\n",
            "2.1491532344723816\n",
            "2.195873956960912\n",
            "2.244671156004488\n",
            "2.2956864095500444\n",
            "2.3490744655860922\n",
            "2.4050048100048085\n",
            "2.463663463907365\n",
            "2.5252550505050486\n",
            "2.590005180005178\n",
            "2.658163211057946\n",
            "2.730005460005458\n",
            "2.8058389450056094\n",
            "2.8860057720057695\n",
            "2.970888294711821\n",
            "3.0609152127333914\n",
            "3.15656881313131\n",
            "3.2583936135549005\n",
            "3.367006734006731\n",
            "3.483110414489722\n",
            "3.6075072150072125\n",
            "3.7411185933408126\n",
            "3.885007770007767\n",
            "4.040408080808078\n",
            "4.208758417508415\n",
            "4.391747913921824\n",
            "4.591372819100089\n",
            "4.810009620009617\n",
            "5.050510101010098\n",
            "5.316326422115893\n",
            "5.61167789001122\n",
            "5.941776589423644\n",
            "6.3131376262626215\n",
            "6.734013468013463\n",
            "7.215014430014424\n",
            "7.7700155400155335\n",
            "8.417516835016828\n",
            "9.182745638200176\n",
            "10.101020202020194\n",
            "11.22335578002244\n",
            "12.626275252525245\n",
            "14.43002886002885\n",
            "16.83503367003366\n",
            "20.202040404040392\n",
            "25.25255050505049\n",
            "33.67006734006732\n",
            "50.50510101010098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j-nT5j864BIn"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(16):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "### trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EGwfW9HxOMj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "wUhKd009Qvk3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}