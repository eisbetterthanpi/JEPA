{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WkwnVjJTrW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fcba14-18e3-4101-d0c0-0fe34148d9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq procgen faiss-cpu vector-quantize-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r0mXVAUnVYX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995505a3-78cf-4c60-9579-5a6f21262130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential()\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 32):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2), # 64\n",
        "            nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        # d_model = 32\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(d_model, d_model, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_model, d_model, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_model, d_model, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_model, d_model, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_model, d_model, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_model, 3, 3, stride=2, padding=1, output_padding=1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # self.decoder = nn.Sequential( # 'nearest', 'linear', 'bilinear', 'bicubic', 'trilinear'. # https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html\n",
        "        #     nn.Upsample(scale_factor=2), nn.ConvTranspose2d(d_model, d_model, 3, 1, padding=1), nn.ReLU(),\n",
        "        #     nn.Upsample(scale_factor=2), nn.ConvTranspose2d(d_model, d_model, 3, 1, padding=1), nn.ReLU(),\n",
        "        #     # nn.Upsample((7,7)), nn.ConvTranspose2d(32, 32, 3, 1, padding=1), nn.ReLU(),\n",
        "        #     nn.Upsample(scale_factor=2), nn.ConvTranspose2d(d_model, d_model, 3, 1, padding=1), nn.ReLU(),\n",
        "        #     nn.Upsample(scale_factor=2), nn.ConvTranspose2d(d_model, d_model, 3, 1, padding=1), nn.ReLU(),\n",
        "        #     nn.Upsample(scale_factor=2), nn.ConvTranspose2d(d_model, d_model, 3, 1, padding=1), nn.ReLU(),\n",
        "        #     nn.Upsample(scale_factor=2), nn.ConvTranspose2d(d_model, 3, 3, 1, padding=1), nn.Sigmoid()\n",
        "        # )\n",
        "    def forward(self, x):\n",
        "        x = self.decoder(x.unsqueeze(-1).unsqueeze(-1))\n",
        "        return x\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# conv = Conv().to(device)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# # input = torch.rand((4,1,256,256), device=device)\n",
        "# out = conv(input)\n",
        "# print(out.shape)\n",
        "\n",
        "conv = Deconv(256).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "nEY9MmwZhA8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d8b704-e4cd-4d9e-b5c3-ed126b755dac",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        )\n",
        "        self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "            nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "            # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "bff65adb-a4b3-492c-a92c-3f9b372429da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2957568\n",
            "2957315\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oqycJZ5s3RR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3178304f-d235-4c95-8665-e9a38277366c",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23296\n",
            "23265\n",
            "torch.Size([4, 32])\n",
            "torch.Size([4, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# @title conv_autoencoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential( # 28 # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "            nn.Conv2d(1, 16, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 14\n",
        "            nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2), # 7\n",
        "            nn.Conv2d(32, 32, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2), # 3\n",
        "            nn.Conv2d(32, 32, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2), # 1\n",
        "            # nn.Conv2d(32, 32, 3, stride=1, padding=1), # 1\n",
        "            # nn.Conv2d(16, 8, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 32, 3, stride=2, padding=0, output_padding=0), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 32, 3, stride=2, padding=0, output_padding=0), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # self.decoder = nn.Sequential( # 'nearest', 'linear', 'bilinear', 'bicubic', 'trilinear'. # https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html\n",
        "        #     nn.Upsample((3,3)), nn.ConvTranspose2d(32, 32, 3, 1, padding=1), nn.ReLU(),\n",
        "        #     nn.Upsample((7,7)), nn.ConvTranspose2d(32, 32, 3, 1, padding=1), nn.ReLU(),\n",
        "        #     nn.Upsample(scale_factor=2), nn.ConvTranspose2d(32, 16, 3, 1, padding=1), nn.ReLU(),\n",
        "        #     nn.Upsample(scale_factor=2), nn.ConvTranspose2d(16, 1, 3, 1, padding=1), nn.Sigmoid()\n",
        "        # )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.encoder(x).squeeze()\n",
        "    def decode(self, x): return self.decoder(x.unsqueeze(-1).unsqueeze(-1))\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder().to(device)\n",
        "print(sum(p.numel() for p in model.encoder.parameters() if p.requires_grad))\n",
        "print(sum(p.numel() for p in model.decoder.parameters() if p.requires_grad))\n",
        "\n",
        "# 477920\n",
        "# 23296, 23265\n",
        "\n",
        "input = torch.rand((4,1,28,28), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "import faiss\n",
        "import torch\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z5-_pfGZTsip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028566de-ba7a-4f64-c16d-afa899f01ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FuA25qQknUAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bc78b7-7903-4fbb-c735-c89d594fa3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        # self.enc = nn.Sequential(nn.Linear(in_dim, d_model), nn.ReLU(),)\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        # self.pred = gru(emb_dim, rnn_units, num_layers)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25.0 # 25.0 # λ\n",
        "        self.std_coeff=1.0 # 25.0 # µ\n",
        "        self.cov_coeff=25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy):\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.rand((batch,self.dim_z),device=device)*2 -1)#*self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "        optim = torch.optim.SGD([z], lr=3e3)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        num_steps = 10\n",
        "        for i in range(num_steps):\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            sy_ = self.pred(sxaz)\n",
        "            # print(\"y_, y\",y_.shape, y.shape)\n",
        "            loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "        if loss.item()>0.1: print(\"argm\",loss.item(), z[0].item())\n",
        "        return z#.detach()\n",
        "\n",
        "    def loss(self, x, y, a, z=None):\n",
        "        sx, sy = self.enc(x), self.enc(y)\n",
        "        z = self.argm(sx, a, sy)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "        # v_c_loss = self.v_creg(self.exp(sx))\n",
        "        vx, vy = self.exp(sx), self.exp(sy)\n",
        "        v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "        return repr_loss + v_c_loss\n",
        "\n",
        "    def forward(self, sx, a): # state, ctrl\n",
        "        batch=sx.size(dim=0)\n",
        "        z=torch.zeros((batch,self.dim_z),device=device)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCD647ZpPrGf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title agent\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title agent pixel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        # self.conv = Conv()\n",
        "        self.conv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    # state_ = self.conv(sy.detach())\n",
        "                    state_ = self.conv(sy)\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ],
      "metadata": {
        "id": "29O1eyvhnRSD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShHQ_ynlwoyJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "import pickle\n",
        "# def save(folder=''):\n",
        "#     agent.save(folder)\n",
        "#     with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# def load(folder=''):\n",
        "#     agent.load(folder)\n",
        "#     with open(folder+'buffer.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "\n",
        "def save(folder, name='ae_res.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    # agent.mem.save(file=folder+name)\n",
        "    # with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# save(folder, name='agentres-4.pth')\n",
        "buffer = load(folder)\n",
        "# save('/content/')\n",
        "# buffer = load('/content/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubGk_uT3FL-S"
      },
      "outputs": [],
      "source": [
        "# name='agent.pth'\n",
        "# print(folder+name)\n",
        "# torch.load(folder+name, map_location='o')\n",
        "# with open(folder+'buffer_rand512.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "with open(folder+'buffer_rand512.pkl', 'rb') as f: buffer = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjm2kV3H7ZVR",
        "outputId": "d4040132-28f1-4347-8028-2e951476da85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6872065\n"
          ]
        }
      ],
      "source": [
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVcknabHMxH6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = self.data_process(buffer)\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    # def data_process(self, data): # str 10780437\n",
        "    #     return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data)//self.seq_len\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        # return state, action, reward\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(\"__getitem__\",state)\n",
        "        state = self.transform(state)\n",
        "        # print(\"__getitem__\",type(state))\n",
        "        return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "    def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "        lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "        with torch.no_grad():\n",
        "            imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "            data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "            # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "            data=data.flatten(start_dim=-3)\n",
        "            data=lin(data) # random projection\n",
        "            data = F.normalize(data, dim=-1)\n",
        "            idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "            sample = data[idx]\n",
        "            index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "            # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "            index.add(data)\n",
        "            D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "            priority = (2**-D).sum(-1) # L2\n",
        "            # priority = -D.sum(-1) # IP\n",
        "            topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "            index_list = idx[topk.values] # most clustered\n",
        "            for i in reversed(index_list): data.pop(i)\n",
        "        return data\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 32 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "def train(dataloader, agent, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    agent.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "for i in range(1):\n",
        "    train(train_loader,agent,optim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzzjgoXCnhT7",
        "outputId": "f29a889c-27bd-4379-cb44-cb8f1951adb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.720053  [    0/ 1837]\n",
            "loss: 0.350687  [  183/ 1837]\n",
            "loss: 0.300806  [  366/ 1837]\n",
            "loss: 0.275361  [  549/ 1837]\n",
            "loss: 0.315960  [  732/ 1837]\n",
            "loss: 0.313087  [  915/ 1837]\n",
            "loss: 0.315067  [ 1098/ 1837]\n",
            "loss: 0.304609  [ 1281/ 1837]\n",
            "loss: 0.329010  [ 1464/ 1837]\n",
            "loss: 0.270910  [ 1647/ 1837]\n",
            "loss: 0.338863  [ 1830/ 1837]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    train(train_loader,agent,optim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d83PMjzrI0h4",
        "outputId": "a3722821-711b-490f-a070-0236b96fb7ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.305921  [    0/ 1837]\n",
            "loss: 0.329479  [  183/ 1837]\n",
            "loss: 0.340010  [  366/ 1837]\n",
            "loss: 0.302271  [  549/ 1837]\n",
            "loss: 0.298969  [  732/ 1837]\n",
            "loss: 0.298797  [  915/ 1837]\n",
            "loss: 0.306726  [ 1098/ 1837]\n",
            "loss: 0.304757  [ 1281/ 1837]\n",
            "loss: 0.283640  [ 1464/ 1837]\n",
            "loss: 0.321499  [ 1647/ 1837]\n",
            "loss: 0.315013  [ 1830/ 1837]\n",
            "loss: 0.307972  [    0/ 1837]\n",
            "loss: 0.281682  [  183/ 1837]\n",
            "loss: 0.320288  [  366/ 1837]\n",
            "loss: 0.321415  [  549/ 1837]\n",
            "loss: 0.316693  [  732/ 1837]\n",
            "loss: 0.326580  [  915/ 1837]\n",
            "loss: 0.295802  [ 1098/ 1837]\n",
            "loss: 0.303027  [ 1281/ 1837]\n",
            "loss: 0.293977  [ 1464/ 1837]\n",
            "loss: 0.249331  [ 1647/ 1837]\n",
            "loss: 0.292596  [ 1830/ 1837]\n",
            "loss: 0.295071  [    0/ 1837]\n",
            "loss: 0.328204  [  183/ 1837]\n",
            "loss: 0.282755  [  366/ 1837]\n",
            "loss: 0.296927  [  549/ 1837]\n",
            "loss: 0.305306  [  732/ 1837]\n",
            "loss: 0.315487  [  915/ 1837]\n",
            "loss: 0.323477  [ 1098/ 1837]\n",
            "loss: 0.313471  [ 1281/ 1837]\n",
            "loss: 0.323208  [ 1464/ 1837]\n",
            "loss: 0.301611  [ 1647/ 1837]\n",
            "loss: 0.310837  [ 1830/ 1837]\n",
            "loss: 0.295686  [    0/ 1837]\n",
            "loss: 0.327278  [  183/ 1837]\n",
            "loss: 0.320005  [  366/ 1837]\n",
            "loss: 0.345157  [  549/ 1837]\n",
            "loss: 0.307387  [  732/ 1837]\n",
            "loss: 0.293485  [  915/ 1837]\n",
            "loss: 0.268492  [ 1098/ 1837]\n",
            "loss: 0.308347  [ 1281/ 1837]\n",
            "loss: 0.334455  [ 1464/ 1837]\n",
            "loss: 0.315504  [ 1647/ 1837]\n",
            "loss: 0.307956  [ 1830/ 1837]\n",
            "loss: 0.276970  [    0/ 1837]\n",
            "loss: 0.275785  [  183/ 1837]\n",
            "loss: 0.266867  [  366/ 1837]\n",
            "loss: 0.323190  [  549/ 1837]\n",
            "loss: 0.333848  [  732/ 1837]\n",
            "loss: 0.294789  [  915/ 1837]\n",
            "loss: 0.276714  [ 1098/ 1837]\n",
            "loss: 0.287320  [ 1281/ 1837]\n",
            "loss: 0.319170  [ 1464/ 1837]\n",
            "loss: 0.278178  [ 1647/ 1837]\n",
            "loss: 0.329311  [ 1830/ 1837]\n",
            "loss: 0.292201  [    0/ 1837]\n",
            "loss: 0.329036  [  183/ 1837]\n",
            "loss: 0.308264  [  366/ 1837]\n",
            "loss: 0.273637  [  549/ 1837]\n",
            "loss: 0.276474  [  732/ 1837]\n",
            "loss: 0.287734  [  915/ 1837]\n",
            "loss: 0.287555  [ 1098/ 1837]\n",
            "loss: 0.324353  [ 1281/ 1837]\n",
            "loss: 0.346287  [ 1464/ 1837]\n",
            "loss: 0.318143  [ 1647/ 1837]\n",
            "loss: 0.347982  [ 1830/ 1837]\n",
            "loss: 0.305506  [    0/ 1837]\n",
            "loss: 0.295839  [  183/ 1837]\n",
            "loss: 0.337044  [  366/ 1837]\n",
            "loss: 0.275167  [  549/ 1837]\n",
            "loss: 0.294172  [  732/ 1837]\n",
            "loss: 0.258497  [  915/ 1837]\n",
            "loss: 0.324803  [ 1098/ 1837]\n",
            "loss: 0.291663  [ 1281/ 1837]\n",
            "loss: 0.320710  [ 1464/ 1837]\n",
            "loss: 0.313621  [ 1647/ 1837]\n",
            "loss: 0.297724  [ 1830/ 1837]\n",
            "loss: 0.285085  [    0/ 1837]\n",
            "loss: 0.274992  [  183/ 1837]\n",
            "loss: 0.257719  [  366/ 1837]\n",
            "loss: 0.309596  [  549/ 1837]\n",
            "loss: 0.347235  [  732/ 1837]\n",
            "loss: 0.316963  [  915/ 1837]\n",
            "loss: 0.249924  [ 1098/ 1837]\n",
            "loss: 0.299244  [ 1281/ 1837]\n",
            "loss: 0.314989  [ 1464/ 1837]\n",
            "loss: 0.298590  [ 1647/ 1837]\n",
            "loss: 0.299007  [ 1830/ 1837]\n",
            "loss: 0.284447  [    0/ 1837]\n",
            "loss: 0.296367  [  183/ 1837]\n",
            "loss: 0.332359  [  366/ 1837]\n",
            "loss: 0.283537  [  549/ 1837]\n",
            "loss: 0.285539  [  732/ 1837]\n",
            "loss: 0.316712  [  915/ 1837]\n",
            "loss: 0.320098  [ 1098/ 1837]\n",
            "loss: 0.239483  [ 1281/ 1837]\n",
            "loss: 0.310083  [ 1464/ 1837]\n",
            "loss: 0.251285  [ 1647/ 1837]\n",
            "loss: 0.290664  [ 1830/ 1837]\n",
            "loss: 0.323785  [    0/ 1837]\n",
            "loss: 0.298354  [  183/ 1837]\n",
            "loss: 0.330614  [  366/ 1837]\n",
            "loss: 0.297056  [  549/ 1837]\n",
            "loss: 0.271068  [  732/ 1837]\n",
            "loss: 0.296722  [  915/ 1837]\n",
            "loss: 0.325359  [ 1098/ 1837]\n",
            "loss: 0.332709  [ 1281/ 1837]\n",
            "loss: 0.280120  [ 1464/ 1837]\n",
            "loss: 0.308435  [ 1647/ 1837]\n",
            "loss: 0.322234  [ 1830/ 1837]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PraFUAPB3j7v"
      },
      "outputs": [],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    # out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    out = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cm6KjvBrnNO"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(10):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    # buffer = simulate(agent, buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    # train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZtFWQs1krGZ",
        "outputId": "0015e196-ec27-43a1-dda4-e0f4e30baaf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "154\n"
          ]
        }
      ],
      "source": [
        "# for x in buffer:\n",
        "#     print(len(x))\n",
        "print(len(buffer))\n",
        "data=buffer\n",
        "buffer=data[-128:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "state = env.reset()\n",
        "state = transform(state).unsqueeze(0).to(device)\n",
        "# sx_ = agent.jepa.enc(state)\n",
        "# a=torch.tensor([0], device=device).unsqueeze(0)\n",
        "# la=quantizer.indices_to_codes(a)\n",
        "# print(la)\n",
        "# # loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "\n",
        "\n",
        "sx_ = model.encode(state)\n",
        "out= model.decode(sx_)\n",
        "\n",
        "# out = agent.conv(sx_)\n",
        "# print(out.shape)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "imshow(state.detach().cpu()[0])\n",
        "imshow(out.detach().cpu())\n"
      ],
      "metadata": {
        "id": "PybuUstFOiSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3a5351c-6354-45c5-f5b8-043c3de6d7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3000x1400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGUAAARhCAYAAACLezhIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABrzElEQVR4nO39f7Bld1kn/j5r73P6dHfSP9L50Z2YTghjNCAEMZHQgjMOZExxLQqGlKKFNRmGGksnoJCy9KZKwJpyCOOUgkyFIA4DWt9iUKYKFOsCw0SN1ztJgCAzIBp+GE0g6Q6/0t3pdJ9z9lrr/sHkYAutnPM84bP75PWq6iro7vXkWZ9fa+1376S7cRzHAAAAAODbatK6AQAAAIDHI6EMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaGChdQN/3zAMcf/998eOHTui67rW7QAAAAB8y8ZxjKNHj8YFF1wQk8k//F2YuQtl7r///ti/f3/rNgAAAAA27L777osLL7zwH/w9cxfK7NixIyIifvn2P4+tZ+7YcJ1/JIz6lky7XJGlaf6bPmcsTNM1thYMRhdDusZY8G/LjeOYrrEwyc3LYpefk4q1UWFlyM9rn5+SiPT6KljjJfeRNxYsjZVhlq4xRL6R7E5ZKji7Fgpq9AXnTsXZNS349mj2G6gVPVSomJPlvuC5VjAck+ReWyzoYV6+mbza9+ka+dOvRvYdMrsuIiIWCt6FK86ulXwb0ResjeycZN8fqxS8ukVf8NJTsTaGghqT5Pm1UPAOWfG5s2I8x4KHUlewNrLPlIrPFmPBRlmOXCOPHD0a//byJ6/lG/+QuQtlHp3ErWfuiK3fwg2cynQuQpl8E9uFMifXqAhlkoHIlk0UyiwKZdZsplBmoZ+TUCZZouLsEsqcTCjzdQsFocxQMBxTocyailBmtaCPCgtCma/3kW+jJJTJzslmCmVmQpk1i0KZk2yWUGYo2CjTgjmJ+NbGw3/oFwAAAKABoQwAAABAA49ZKHPzzTfHE57whNi6dWtcddVV8eEPf/ix+kcBAAAAnHYek1Dmd3/3d+OGG26I1772tfGxj30snva0p8U111wTDz744GPxjwMAAAA47Twmocyv//qvx7/9t/82XvrSl8aTn/zkeMtb3hLbt2+P//pf/+tj8Y8DAAAAOO2UhzIrKytx1113xdVXX/31f8hkEldffXXcfvvt3/D7l5eX48iRIyf9AAAAANjsykOZL33pS9H3fezdu/ekn9+7d28cPHjwG37/TTfdFLt27Vr7sX///uqWAAAAAOZO87996cYbb4zDhw+v/bjvvvtatwQAAADwmFuoLnjOOefEdDqNQ4cOnfTzhw4din379n3D719aWoqlpaXqNgAAAADmWvk3ZbZs2RJXXHFF3HrrrWs/NwxD3HrrrXHgwIHqfxwAAADAaan8mzIRETfccENcd911ceWVV8YznvGMeOMb3xjHjh2Ll770pY/FPw4AAADgtPOYhDIvfvGL44tf/GK85jWviYMHD8b3fu/3xgc+8IFv+I//AgAAADxePSahTETEy1/+8nj5y1/+WJUHAAAAOK01/9uXAAAAAB6PHrNvymR10cUkug1fX5E2LSSLbCloYtrli3TdmG9kKLiZjU/n10t0+SLZGgtRMJ4FgzEMQ7rGOOb7GMdZukbXTXPXV0xJ5Mez4uTpS9ZXwbkx5vuYTHLrq2K/c7LNMqYVu7XCWLBPYpPMCSdLr41NtC66oeQhnZZ9vk4Knq0V75BDwQk4KXj/69MVamT32jgnX1GoeD6PY8XTsf3ZM61oYZKf2EmfG8/1ZBlzsgwBAAAAHl+EMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAAwutGziVbjJGNxk3fP20IG9aTNaYdvkept3Gx+BRXbpCRCTm4lHjWNJJ2jRy99JNp0Wd5IxdfjyHMT+vmyXb7QrGs2I4h2GYiz4qxiNbY3OsrDoVc7JZjCVnF/CPqXkWFDQyB1u+5NyZk2O85HkyJ+dwtouKea0YiXl474qo+bw2D68r3Zh/n+6Sn8O7dawM77wAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQAMLrRs4lUl0MYluw9dPC+KmxUmuyNLG218zjTFdo+vygzH06RKRmM41FfO6ZTJNXT8vSeY45tdGxJCuULG+NouxYI1XTOtQcG5MCzZs1+VqZK+vUrPXAL51Fed4yYvXJpI+y+dkOCuejcOcPNcqhjR7J0PJ57WKd+H5mJMK2b02LVjjk+Tn+IiIWfY+1tGDT1MAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGFlo3cCpd10XXdRu+fjLJ500LXa5GN914/4+a5EvEOI75IpFvpCtoo2Je50HFnNTUKJjXgjVasTY2i4o5iZiPAZ1k26gYCjiFzDvGmpLnK/CPqXjnKdnzc6DiPipGouRtZQ7O0IoehoIX2cV0hYi+oMZmUbFPpslpnaxjl2yOT7gAAAAApxmhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQwELrBk5lYfK1Hxu/vkv3sJgssTCkW4iYFtQoMOaHMybdmK6xUNBIwdJIq1ga/Zgfz4iKwai4m/b58FiwtsaomJP5ULFfmT/td9r8GCvO0LFgROfgmcTJStZG135iK+6jK7iPisdJRR/ZNob2UxoRNed4X1BjUnGEVsxrcp3X7Pd8iYq36ZJ9UjGvyXfqir1W8RF6muxjPdd7PwMAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADSw0LqBU5l2XUy7jWdG0y7fwzgZU9dPCpqYRr7GLHL3ERHRdfk+FgpqTApixIp7mQ8VmWp+bcyDijkdx/kYi7Fiec7HrWyivUaledlrwLdH1xXs+Tk4NirOrqHiXThdYXPZLG8aFe9MXcEaHefgnbrkPaFiPJM11nO9fQ0AAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhgoXUDp7Iw6WJh0m34+sUunzdtSWZW09h4/5XGoaKPMV1hMs3PyWZJEfsxP55DwZzUmIdZGdIVxi6/TyrmdSyoUaErGI9Jeo3me6gYz3EsGIt52CbU6/Jnz3ycofOh4tyJOTlD+bqKea14p17dJEujYjy7oeDZmK4QEWPF+dfnWijpIF+l6/PPk4WSl42K51r7z8AV7+TT5F5bz2x4EwAAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADSw0LqBU5mMXUzHbsPXTzd+6dd7mOau77p8E8MwpGvEZMyXKLiXSUEEWDGmWRVzMibW9t+pkq7QdfkaEe3nZF6MY348K2pMCjbb1Lwyxyr2SYmx4sGWL5FuoeDZOjdzMieG5JAmX0E3nZL3v+QaLXnGpytERMFYVIxnzdFV8DlnDo6ezXT+1TwPstdXfMaZg4frOvimDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIGF1g2cymL3tR+Z67OmkSsyjmO6h6HgPrp8G+mxiKhJAOchRRy7/FgMBWujRn5EK9ZXxJDtoqKJtIo9Py+6gnVeUSMru7LgH9RVrLB5eLKxGVWszmlBjc2i4t1tXt4SuoKXt3HI16h4T5iHd6+StTHJ77b5eefJdvL4ey4+/u4YAAAAYA4IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADC60bOJXFyTQWJ9MNXz8tiJu6rktdP45juoeKGtn7mKca8Fgp2a8FfWwm85D6V8xrhLOrUs2c5JX0YWms8YznVKYFa2M2F0/YiqfaPNxHjZo9X/A5Jzb+efFrHfTpHjhZ/jN0voeaZ/y377k2D+/MAAAAAI87QhkAAACABoQyAAAAAA0IZQAAAAAaWHco86d/+qfx/Oc/Py644ILoui7e+973nvTr4zjGa17zmjj//PNj27ZtcfXVV8dnPvOZqn4BAAAANoV1hzLHjh2Lpz3taXHzzTd/01//1V/91XjTm94Ub3nLW+LOO++MM844I6655po4ceJEulkAAACAzWLdfyX28573vHje8573TX9tHMd44xvfGL/0S78UL3jBCyIi4nd+53di79698d73vjd+/Md/PNctAAAAwCZR+t+Uueeee+LgwYNx9dVXr/3crl274qqrrorbb7/9m16zvLwcR44cOekHAAAAwGZXGsocPHgwIiL27t170s/v3bt37df+vptuuil27dq19mP//v2VLQEAAADMpeZ/+9KNN94Yhw8fXvtx3333tW4JAAAA4DFXGsrs27cvIiIOHTp00s8fOnRo7df+vqWlpdi5c+dJPwAAAAA2u9JQ5pJLLol9+/bFrbfeuvZzR44ciTvvvDMOHDhQ+Y8CAAAAOK2t+29fevjhh+Ozn/3s2v+/55574uMf/3js2bMnLrroonjlK18Zv/IrvxKXXnppXHLJJfHqV786LrjggnjhC19Y2TcAAADAaW3docxHP/rR+Of//J+v/f8bbrghIiKuu+66eMc73hG/8Au/EMeOHYuf+qmfioceeiie/exnxwc+8IHYunVrXdcAAAAAp7luHMexdRN/15EjR2LXrl3x/3zu87F9x8b/+zJb1h03faOFLvdvd1UM7Wwc0jW6rkvXyI5FRMSWSb6PacG9ZPUF87oy5GtUrK+KtdGVnCC5dV5xH33BfawM/Vz0MZnk9+vWghoVez5rNuTP0HHM30fBcJacf/Nwhg4Fc7Ja8XwtOIeHguHMPl8XC3qoODMqrPb5M3SlYG1UPF+zYzqNiv2eLjEXZ0ZExHKfPzey714V7xpLBS9Ni9NpukbFu2zF+0ofBfs1eRAPkT93KmyZ5Oe1Ys9XyJ6hFe9dXcFeW0ie48eOHInn7z8/Dh8+/I/+d3Pn4ykMAAAA8DhT8H2Sx8Z08rUfG1XxzY5swDYvX0GqSPYnBX9iU5EAVtxLNr2t+JP3KBjPzSQ7rxV/qjmU/GlgxSov+HZcweHTFXxLLyL/pz7zoWIsCp5Jc/In1vOgZs8XNEKpzfQnhek1OgfvO1V9VJjMwVt1xXiOBd9Km48nUkTBlw1rXoe75IiULK38iNbs1/x716Rghc3Hd4/az8l6rt9Mzz8AAACA04ZQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoYKF1A6cyiS4m0W34+m6s6GLIXd1tvP9HdYkxeNSkYCy6giJdNx8ZYD/m7mVMXh8RUbE8u4r1VbJPNoeKeR2G3JkREVGw5WvWRkEfm0XNeFbs14rDfLNMbMXzpGC/Uqpin0TFPtkkKp5rm0l2fc3LeFb0MSl5FszHeMyDijnpC6ZkUjAnFU/X/F7L91AyJ8mX8n781q+fj0/JAAAAAI8zQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIGF1g2cyrSLWEhFRkO6h3HsctfHmO6hwiRy9xERMe3yNSqMY/sxHbuCLLPkPvJrfPPksvn7GMY+XSN5ZJTpuvz66irW+RyoODO6gvNvc4zm/CjZa+0fJ8ypiqUxD4+DirNrc8m+N+XHs+Q9dm7mNf8eOik4zLNvb/PyOKlYGzXrK18i+85T8QmnwpCc2fVc7z0RAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQwELrBk6l67roum7j1xf0MBTUmAddNxbUyOd3mfl81Djm76VP1qjogfkzjhWnRsVey/cxqaiRrgA8nng21ioZz4rH2pyoeDbGHKzRvuA9YWFOJrZiTrqCe5mL0RgrPjXOxZ3MhYrPrhXv9ZPkfl3P9d67AQAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADSy0buBUJjHGJMbWbaSMY77/ySSfm027Lt9HukKNijEdx+x4zMu6nI9Z6br24zHMzZwMBTWmBTXyfXRd+8dDX7Dfa/bJfKyvruAsnwdjxX2UrI3NYV7WxSTyffQFe61kPDbJ+qp4Z5oXFSf5Znn7y7/HRn4woubzRT8UjOocHIEV585QMK8VZ+i0YEDTa2NOzq7sXlvP9fPxqQ4AAADgcUYoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABpYaN3AqXRdF13Xbfj6Ycj3MI4FRZKmsfExeFRmHOfNOI7pGu1nNaLr8vcRBWtjfmTz4fys/s8//1y6xsqQn9fpQn5eF6f5o/0FT39iugbw+DFWPJIqHo38HfPxZ68V76FzUaNgfZa8x26id8iazyjJ8Rjz+6SLPl2jYlYr1lfJMZyc14p1UTAU31bzcVoDAAAAPM4IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABhZaN/BYGbt8jX4cU9d3Xb6JihoVyVtFH2NyPCMixoo+hiFdIy8/K11+OOdGN/ap69//f+5L97BaMJ7fce6OdI2nPuGCdI0/+9S96Rp8XcX5x/ypeCZZG19X8a4xD0/neTEULK1JwXOtYk4WCvZahfx+LXiPTVeYHyXnX8HayK7zvitY5XMysUPFZ62C03w+zvJ8F2OXG4v15BG+KQMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhgoXUDpzKJXGLUVzWSMI0uXyNfIrquoMicGMexdQs8Bv78819JXT/OxY6PePL+vekaC+MsXeN5T35CukbEUFAjp2a/b57zD76Zin1S8Z4w2monyc7LZpqT9k+Tr8nOybw8k/rI97GZXqcrxiNrXtbGWLDph67gmTK0P/9Ot++enF7dAgAAAGwSQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIGF1g2cymwYYjYMG75+dRzTPQzZGl26hRgL7iO6fCMVffQFNTJrYq2P5PVdwXhOI38fY8H6KjHms92Dhx9JXf+9Tzgv3cNHPv1Ausb//PPPpWv8v77vknSN2WQ1XWNLTNM1sufGEBXnX8GzIL9dYzrNj+cwFIzHHPxRTMXzZDbmJyX9jI+a4eyTa3RSsCwqFnnFs7HiPWEouJeKPd9lz56KOSl4Ee0r3iFL3ofzNfI9VLwLV+y1/MkzK5jXij0/FoxHdloqngUV+nFWUCW/NlYrzr9pro+u4H2n4h2yTx5e69nvc/B6BgAAAPD4I5QBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABpYaN3AqQzjEP04bPj6fujSPXTpEhvv/+um6QrjOBb0kTcb8uPRF9zKGLkiFePZRcX6zNeoWKP3PnQ838Ukt853bt+W7uG5l1+YrnHw8Il0jXlRsc77ZI2KHipqTLv8n1/Mxj7fR8Gfo3Rz8DioeBZUzOtQ0UfJOdy+h/zqrHhbqZmTYQ7Oroj8eFS8QRa8akQ3FLzzFJyh87Dnh+T7Y5WSsShYG5OKd/KS53z7HiqMY8XzJH+aj13+NJ8lPsNHRCwUnBkV2zW7NtZzvW/KAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKCBdYUyN910U3z/939/7NixI84777x44QtfGHffffdJv+fEiRNx/fXXx9lnnx1nnnlmXHvttXHo0KHSpgEAAABOd+sKZW677ba4/vrr44477ogPfehDsbq6Gj/8wz8cx44dW/s9r3rVq+J973tfvPvd747bbrst7r///njRi15U3jgAAADA6WxdfyX2Bz7wgZP+/zve8Y4477zz4q677op/+k//aRw+fDje9ra3xTvf+c54znOeExERb3/72+NJT3pS3HHHHfHMZz6zrnMAAACA01jqvylz+PDhiIjYs2dPRETcddddsbq6GldfffXa77nsssvioosuittvv/2b1lheXo4jR46c9AMAAABgs9twKDMMQ7zyla+MZz3rWfGUpzwlIiIOHjwYW7Zsid27d5/0e/fu3RsHDx78pnVuuumm2LVr19qP/fv3b7QlAAAAgNPGhkOZ66+/Pj75yU/Gu971rlQDN954Yxw+fHjtx3333ZeqBwAAAHA6WNd/U+ZRL3/5y+MP//AP40//9E/jwgsvXPv5ffv2xcrKSjz00EMnfVvm0KFDsW/fvm9aa2lpKZaWljbSBgAAAMBpa13flBnHMV7+8pfHe97znvijP/qjuOSSS0769SuuuCIWFxfj1ltvXfu5u+++O+699944cOBATccAAAAAm8C6vilz/fXXxzvf+c74/d///dixY8fafydm165dsW3btti1a1e87GUvixtuuCH27NkTO3fujFe84hVx4MABf/MSAAAAwN+xrlDmlltuiYiIH/qhHzrp59/+9rfHv/7X/zoiIt7whjfEZDKJa6+9NpaXl+Oaa66JN7/5zSXNAgAAAGwW6wplxnH8R3/P1q1b4+abb46bb755w00BAAAAbHYb/tuXAAAAANi4Df3tS98OqxGx+o9/MeeU+nFI97DQdekaaV1iEP6v/Eh8a9+S+sdUzMmsoI9shUnBuujTXUREwVhMI38vnzn0YLpGdpFWrK2xYCz27j6joI/50BesryF5N7OSwcjP66TiUVBwEPeTgiJzsMDGgvEchvbrMyJiUjCeQ3KvDQXjWfK2U/FYK2hkLFgb49ina/TJUZ10BX9uWjAnfcn6KmikYm1kr69YFwWLvOum6RqzgvemxUm+j5J3jWSJimdBhaFgkQ8VH/oiv84jcmujm1Z83iuokfwcPnbf+oT4pgwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKCBhdYNnErf99H3/cYLdPkexjGbWeWbGId0iRi7MV1jVtDILN9G9EO+j67LzUvBlFQsz+gK5jWxw9YM4zRfY1hNXf8n//tv0j086/KL0jW6yM/JOFbMa8FmKzAk9+swH7cRQ0Ejk0nBn4EUHD7Z869kfQ75k6cv6GMc8yfxUHAOd8l7mRWMxWJXcI4XnDvZM6Osj4I/s5wk35sqxqLibWNSsDbGOXkmZVWcfxWzOitYGwsFL6IVnw0qxjRbYl7WZ8FQlIznbMgvjq7LrY2u4ANK9n0nIiK71dZzvW/KAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANLLRu4FRm0cUsug1fP42xsJt2xo0PwZq+YCj6IV9kNuZrDAXzmh3SScF4lqzOgiKHjy+na5y7I79ILzn3gtT1s6FP99D3Q7rGOC3YsAXGgr1WsUZnY25Mh/yURNfl56QvmNZuTuYke35VnMF9QY2hYHHMCtbXZFpQI7m+KvZ7xZxUPBuHgr02FryvDGP+mZJdXpNuPv7cdChYX7OKd7eCszy7Vyr2ScW7cIV+yK+vriu4l+wBGBFj8v2tr7iPAv0wS9cYK75vUbBGu+QH2EnBfq94I88OxXrOnPk48QEAAAAeZ4QyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADC60bOJVhGGMYhg1fPymIm7pJl7p+HMd8E+PGx+BRQ+TuIyKij/y9ZObzUSVjmu2hdQOFVmazdI0dS1vTNfrkOp8V7JMK4zgtqJFfYRXbZFaw0rPzWnF2VQzGZMz3MYv8Gp0WjMeQLFHxLKhY49n7iIgYoi8oUjAnyXmdTAt6qHg+d/k+KtZGyfvKHLxrdBXPgoI+ZkN+nywm36erDMkRmZfn81ixxktqFKgYj+Tyqjj/KlTs15o1WnAOJ58HfeTfp6eTgrFI3sd61qZvygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABpYaN3AqQwxxhDjxq/vusJuNtpDvka38SFY08eQrjEb+nyNMX8zY0GNxW6auv4rj6yme9i9vSIPzdc4Z/sZ6Rr/3888kK7xfWdsSV3/kU99Id3DFU/+jnSNCplz7+s15sMw5g7BvmAsppE/iCvmZFLwTOoLzr9sFxVjUTGvsz7/TOoL1kYUzEl2PLqhYF1Mc8/FiIix5OwqGM+h4J2n4N0rreIgn8zL2ZW/mUmXf+fJvkNWLIuhYH2uFnw4mBTU6LqKj5H58RiT62soOMfnRR/5Z2PFZ+gx+f43LdhtFZ+hs2trPdf7pgwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKCBhdYNnMoQY/Qxbvj6hegKeuhT109jmu4hMwZrNcYhXyNfIvqhoEiB7JhOxvyc/MWh5XSNrd1qusY/Oe/MdI1xzO2TiIj/36cOpa4fhny+PBbM65A/dkpU3EvJeAy5GmPBePYFNSb5oaiZk4LnwWTM7ZWKHirGokLF2TUWvGtkx6NiNIeC53PXtR+LearRdcl5Leihn5OHUkUbXcm5kVvnJc/Fgh1bMatDyT7Jn6EV50bWvJwZFUqeSQVrdEh+9qz4/Dud5mOOMXkf61kXvikDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoYKF1A6fyfz77+Vg648wNX79r58avfdTTLzw7df04jukeKsz6Pl1jdRjyfYz5GgtdQY44rKYu//986vPpFiYFW+/sM/JjcfG5+X1y1s7t6Rr3P3gkdf3Tn3RuuoeK/TqO+b1W0ccw5tfGOHbpGn3k9vww5MdiOikYi65gLArO0ElBH0OXG9OhYH2uFsxrxfOkYo3Pkms8ImKS3K99yXMxfx9dwfqsOP/6KDhDC2pkS4wFa2uh4M9eK/Z8xRrtStZGbo3WPJ8r1lbBGZrfrumzKyJisWCvZee14swYK86MAiXvK33+7BmSfXQFz+eu4D1hkjy7hnVc75syAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABhZaN3Aq9x8dY7EfN379ieV0D2csHU1df+m+3ekehmFI15htfBjX9AV9jNHl++jyNzMbcllkNy6me4iC+/i+f7IvXaNiXvefuzVd4wuHDqeuX1nO38fi1nxGPY75eR26fB8VaXsf+XupWF9ZQ8GcFJSIrsuffxX3kr2ZijVesk8KltYQ+SLT/LTGbMz1MRkL1lbBoTHNl0iPRUTV+mp/dnXT/IhWjOek4NipWKN9wV7LLo2hpIf5OEP7gj1fsU/6gkN0LFjnWUPBO1OFcQ7Oroj82ugr3pkqxiJ5DA/Rf8u/1zdlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGFlo3cCpbt2yJLUtbNnz9LLp0D0O3mLq+H4Z0D32M6RqrY76PWZfvIwruZRzz83rk+GqyQn48L9izNV1je8HurVhffV+xNnI++bkvpWssbpmmazzju89L1xjHPl2j4vzr88u8ZH1ldWPBGq/oo6BGha7LdTIUjGdJjYJZGQqeJxXPtS7ZR8W7RkzyYzEWzGvFlAwFc1KxRtPGWbrEpODkGQveIfuhYGIr1mhyPCrWeMVQVKzPScGxMZsW9DHmvx+QPcuHgvf6is8nJadOl7+XoeC9Pv3+1+fXxaRgkS8m+xjW8arimzIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGFlo3cCqrfR8x6zd8/XRxMd3Dd56zlLq+jzHdQ99vfAwqawzjkK7RdV2+j3yJr62txp58/q50jZL1VTCvw5jvY2Gam9jEUbFmOqlY49N0jaFgXif5EjErmNdhyI1pxZnRd/n76ArG4g//95fSNV7wtPPSNcbkvQxjwZwUrPGKPrLrMyKiK/ijreyez85pRMT//Ph96RrPeepF6RoFt1IyrxVjmu5hyC+useD8q1BwK9EVPOfH5DtPxbqYlxoV+2QoeCkfCp7zWSXjGXNy7lScoQVzMg7J51rFe0LBGu8nubFYz/PdN2UAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0MC6QplbbrklLr/88ti5c2fs3LkzDhw4EO9///vXfv3EiRNx/fXXx9lnnx1nnnlmXHvttXHo0KHypgEAAABOd+sKZS688MJ4/etfH3fddVd89KMfjec85znxghe8IP7iL/4iIiJe9apXxfve975497vfHbfddlvcf//98aIXvegxaRwAAADgdLauvxL7+c9//kn//z/8h/8Qt9xyS9xxxx1x4YUXxtve9rZ45zvfGc95znMiIuLtb397POlJT4o77rgjnvnMZ9Z1DQAAAHCa2/B/U6bv+3jXu94Vx44diwMHDsRdd90Vq6urcfXVV6/9nssuuywuuuiiuP32209ZZ3l5OY4cOXLSDwAAAIDNbt2hzCc+8Yk488wzY2lpKX76p3863vOe98STn/zkOHjwYGzZsiV279590u/fu3dvHDx48JT1brrppti1a9faj/3796/7JgAAAABON+sOZb77u787Pv7xj8edd94ZP/MzPxPXXXddfOpTn9pwAzfeeGMcPnx47cd999234VoAAAAAp4t1/TdlIiK2bNkS3/md3xkREVdccUV85CMfid/4jd+IF7/4xbGyshIPPfTQSd+WOXToUOzbt++U9ZaWlmJpaWn9nQMAAACcxjb835R51DAMsby8HFdccUUsLi7GrbfeuvZrd999d9x7771x4MCB7D8GAAAAYFNZ1zdlbrzxxnje854XF110URw9ejTe+c53xp/8yZ/EBz/4wdi1a1e87GUvixtuuCH27NkTO3fujFe84hVx4MABf/MSAAAAwN+zrlDmwQcfjH/1r/5VPPDAA7Fr1664/PLL44Mf/GD8i3/xLyIi4g1veENMJpO49tprY3l5Oa655pp485vf/Jg0DgAAAHA6W1co87a3ve0f/PWtW7fGzTffHDfffHOqKQAAAIDNLv3flAEAAABg/db9ty99u3RdxCQRGXVdl+5hechdP+mSBSKiz5eI2dCna4wF+d0kxnSNCkcfmaWun8Y03cOWxfz6HIb84piN+bWxWrC+ztmzJXX9GdsX0z3s2b49XWOIgr025NfXMMnvtXEsqJFc5mPBmVHxJw/jkN+v04Jn0u9/4ivpGs9/6lmp62cF66IfC56NBWtjKKiRn9WISXJMh4JF/siYfx183ye/mK7xvKeek65RsLxK1lfWUHAjCwULtGKNV+z5kkayz6SCHiqerSXjWWBaMCB9tzm+H1Azr+3PnYiIYWj/fJ0VrPGx4D4mmTAi1jcOm2MnAAAAAJxmhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQAMLrRs4lclkEpNJIjPKXPtoiW5IXZ+7+mtWhz5d4/Ajs3SNM7cWLJVpRYkuXePuB46krv+hp5yT7mGIMV2jwsq4NV2jH4+mawxjbjwOfXk53UPX5RforjO2p2ssLBScHAXLqx/zfWRrTAr2e8V9dJN8H8+8dFe6xq6Cc3g5uc6nsZruYUzu94iIYcjPa0UffcFemyQ3bMUan07z5981Tzk7XWMoeOeZl7UxJo+NScHa+h93PZiu8S++77x0jaHgLJ9MCl4ik0rWxZycfxV/LD8b8nMyTX7Wisi/K8zPvKZLRNfl91rFS2R2PGYV725d/p2p73PPpPVc75syAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABhZaN3AqYzeNsZtu+Pp9O7emexiS16/2Y7qHoyf6dI3plnz21kf+Xrp8iTi2mq9xwVnbU9dPF7p0D0PBeH7krw+nazz45YfTNZ791LPSNQ4/nFvnqyv58fzCwWPpGn9+PD+eT7lwV7rGd5y9lK5RsUZnQ+4UnXb5vTYp+LOHlYJH5R2f+Uq6xg8/ZU+6xrTPHaIFj7Xox+zTtWZ9jnPyXBv63Bo93uXX+LbhaLrGn3/mRLrG9/yT89M18qsrYjbk373Spht/B15TsEBv/d+H0jX+xdPOS9foC54HWRXnTkWNfizoI/l8joiYlpzl7ed1LGihYk7GgjVe8VyrmJJxyPVRMZ6TgvW5mmxjdR09+KYMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACggYXWDZxK13XRdd2Gr/+eC5bSPcyGIXV9P47pHr7y8LF0jWGcpmuctzu/VMYxnwH+2V8cTNd4zuXnpK4fC+a1wpceWskX6fP38kcfzs/JBXty+3VhujXdQ9fl90nX9ekaF56Tv5cheXZF5M+/iIK9kngGPGqI/Bofh3wfFQq2a1rF+VdRo+/zey2/wiP+7H/fm67xQ1denLr+nnuPpHt4ymXfla6xcyE/Jwcfztd45JET6Rp7zqw4e3K6gjP4B7/3rHSNhTH//jeL/Hh2BWf5JNnHvJx/81Kj4l2j4CiPbiG3RsdhTsZzTj5fVMh+Bi55jy14h5wmS6znPnxTBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoYKF1A6ey2H3tx0YtTBIX/1+zGFLXr4yzdA/Ly2O6xtHjJ9I1du/cka7xx3/x5XSNH37auekasyE3r/mVVVPjOU/dna7xoY8eTNd4xlP2pGvce/9y6vpLLzkz3cPtf3k4XeOap5+XrjEk12dERD/ma1T0MUTu/Mp3EDEZ82do1x1P1zhzYSVdo8KYHI+KtZU9g7/WR35eV4d8jR942kXpGn1yOC7Z/x3pHu78q/yz4DkFz4Kzt/XpGmdumaZrLK/m+xiG3J97roz5N4UtBX/22nX5/doV3EvFubEw2Rx/Fp09xyMiCqYkVvv8Pqn4vDZJro2Kd42K8ewLnknzIrtGs++PERFDn/8cPu0WU9fP1jGnm+N0AgAAADjNCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAYWWjdwKk/ctxhbz9yy4ev7Md/DH3/qcOr61eXldA/fff72dI04ni/xt/c/kq7xnWcvpWus9n26xjwYx/wC7cdhLvr4xKdz+yQiYv++nanrjz2ymu6h66bpGiXzGvl5HYZ8jYr1lVVyH12XrjGNfI1nfOc56RoVc9Ilx+PLx06kezief5zEjjPyNSZdfr9W7JLsybMQx/I9dPlna8V+rTCO+f1acZaP3Sx1/WLBn5uOXcV7Qv7ZWHJ2zcG8VqzwvmBtDVGwPoeCPib5NVqyNob82sgqOTPmpEb2PaGqj3kwSz7X+nVc75syAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABhZaN3Aqe7ZvjW1nbN3w9UOM6R62Lnap60+cSLcQd99/PF3jnDOn6RpdlxuLiIiVIZ8B9uMsXWMyyS37sWBtVZgN+RrjmL+X3duX0jWOPbyaun5lJT8YB568J12jYEpiHPJVhoIa/Rys866gh65gLCYFf3zx4NH82bWwbWe6xo7F3F7bsXUx3cOWhVwPERGrFQdggZK9Ns0tsIp9MvT5OamYkaHgmdRHn++jYEyzz9ehy/fwub95KF3juy86J12j4r1pUlBjltyvFe9M81KjHwt27JDfa7Oh4jNK7l4qPuNUKJmTChVrNLlfK8aiYp9k18Z67sM3ZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0stG7gVCaLk5gsbjwz6scx3cP+s7rU9ccf7tM9PLI8pGscPbGUrnHWmekS0Y0VGWB+yc7G3JgudPn76CO/Pvs+v76GIb++5sGR46vpGgtdvsYwTNM1ui537kRE9Mk1HhExDvk+xuQ5PE7z+2To8jW+cDhdIvbtXMwXiePpCtktn53TiJq1VbHGK1Ts1yF5lFfsk/PO2ZWuUXLuFDwbK9ZXxbNxSN5LwVYrWZ+zMf9cm85m6RrdYn5Osu9vJeszPyUle63iPbSr2CcFNfppbl4LjtCS8axQ8YyuODeyKu4j+3kvIqJLPqDX04NvygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABpYaN3AqSyvDDFZGTZ8/XRhTPewY8di6vqzztya7uHEysPpGkeXV9M1dm8ryO9ywxkREatjl67RRa7GLDa+Lh/V50tEn7yPiIhxzO+T6PI3c/Dwcur6H/zec9M99FExFgU1CvqYjQVrtOJWsj0U1BgLxnNpmu/jRMGer+gjazbkb2So2GsFKs6/invJ1qjYJ8ePfCVdY/W870jX6GKWrjEPcxKRP4cXC953tm/flq4xi9zzOSIiKt7dCs7Qefij6IKhKNEXnOUV47k65k+w9NqY5G+k4qlWcnYVvP9lPydVqHgnr3jGrw659bme6+fgeAIAAAB4/BHKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANLLRu4FQeOHIilobFDV9/wZ6NX/uofhhS14/jmO7hskt2pGt86fBqusbBY326xnfszM9JdLk5iYgYx1wWme8goi9YG3f8n6+ka1Ss0SGm6RpP+q7tuR42UbxcMicFNSrWaNd1qeuHfAtx+Ei+xtn5YzgOPXQsXeO8PdvyjSTVrK38KTorqFFhUnKG5mr0BRvl4idekq5x9JHj6Rpnbs3Pa3Y8IyL6ghrZs7xijR87lj93zt2df3dbHbema0zHWbpGxfN1M/QQUdNH9nNSRMRQUGMccx9nK8ZiXua14uyKOXj/m5d34exYrGePbKKPMgAAAACnD6EMAAAAQANCGQAAAIAGhDIAAAAADaRCmde//vXRdV288pWvXPu5EydOxPXXXx9nn312nHnmmXHttdfGoUOHsn0CAAAAbCobDmU+8pGPxG/+5m/G5ZdfftLPv+pVr4r3ve998e53vztuu+22uP/+++NFL3pRulEAAACAzWRDoczDDz8cL3nJS+K3fuu34qyzzlr7+cOHD8fb3va2+PVf//V4znOeE1dccUW8/e1vj//1v/5X3HHHHd+01vLychw5cuSkHwAAAACb3YZCmeuvvz5+5Ed+JK6++uqTfv6uu+6K1dXVk37+sssui4suuihuv/32b1rrpptuil27dq392L9//0ZaAgAAADitrDuUede73hUf+9jH4qabbvqGXzt48GBs2bIldu/efdLP7927Nw4ePPhN6914441x+PDhtR/33XffelsCAAAAOO0srOc333ffffFzP/dz8aEPfSi2bt1a0sDS0lIsLS2V1AIAAAA4XazrmzJ33XVXPPjgg/F93/d9sbCwEAsLC3HbbbfFm970plhYWIi9e/fGyspKPPTQQyddd+jQodi3b19l3wAAAACntXV9U+a5z31ufOITnzjp51760pfGZZddFr/4i78Y+/fvj8XFxbj11lvj2muvjYiIu+++O+699944cOBAXdcAAAAAp7l1hTI7duyIpzzlKSf93BlnnBFnn3322s+/7GUvixtuuCH27NkTO3fujFe84hVx4MCBeOYzn1nXNQAAAMBpbl2hzLfiDW94Q0wmk7j22mtjeXk5rrnmmnjzm99c/Y8BAAAAOK2lQ5k/+ZM/Oen/b926NW6++ea4+eabs6UBAAAANq11/5XYAAAAAOSV/+tLVQ4+NIstq6sbvv68Xfm86djyLHV9N6ZbiK7r0jUu3Lc9XePLn3k4XWPr9vyAPPBAvsa+83PLvmBaY7nv0zXGMV+jwtKZ+b129Cu5vbZybJru4ayz0iViV8FYjGN+z/cFi7QvWOnTZIkh3UHE1m35x9ysYE6Wtuf36zBWnD45FWtrGPIz2xfUqFDxjO6GXI1umj//Focj6RqHj+XPv51L6RIxFuyTihrZ/ToWrPE9Z+9M17j30NF0jQsL/gLWz34h38elF+Ye9BX7vWJtVTyf//wTX0nXuPyp+Ren2Zhf5wvJGl3BM34omJOKeS05/9IVIrrs+VfQQ4XVIffuNlvH9b4pAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0sNC6gVN56hOWYuuOrRu+/lP3PJLuYVs3pK4fx3QLcbzv0zW2d12+kQJnnJlfbsePzdI1HnxwJXX9Oefks8y/vi+/Pochtz6rauzela9x+IvTXIHJ8XQP27ftStcYCzb9rGBOKvqoWBvdNDevn/vbY+keLrs4f/6N41K6xrbF/LlRMa/z0EMf+RrzMBYRNfukn+TWxnTIvyeM3WK6xoNf+Uq6xjl7dqRr5EdjPs7QSXJdRERszx9dse3cjb+LP6pin1x0Qf4ZPUTueTAZ88+TkvMvXSEiuvw7+Uqfn9fFSb7GbMzV6ArmteJ7DuOYP72GgrOr4vzrkp89K3pY6fOfGbN9rOfs800ZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAAwutGziV1WGM6TBu+PozJ/m86cgjK6nrv3TkRLqHXf1SusbxR9IlIoYhXeKhR47n20hXiJitTlPXH1vO9/DQ4fza6Ps+XePJ37MrXWOWbyOy2/WsPVvTPfSx8fNmrUbizHrUULDXVvvZXPRx/ESuj0v2b0/3sDLm56Tr8mfXF7+Sn5Pz9iyma2T/LGZ1yG/4WcHZNRsrngZ5BVs+uuRe67v8e8I4zb3vREQ86Ym70zXufSC/1845J/9qu1qwvobkM2Uo6KEbVtM1jh3Pnzs7tnfpGn3BM+mLX82t871nb0n3MBY8kz7/hfyLaNfl5+Sv/vJYusZZu/Kf1y594lmp6yeTimdrfl4LHicxRsXzNd9HF/n1lTUWtJA9d/p1nOO+KQMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhgoXUDp/LgAyuxdGR5w9fPhj7dwxePPJK6vivIvI4eXk3XiGFMl5hEvsbBQ/l72b11MV0jaxiGdI3Zan599n1+Tr7yxVm6xkOHN75PH3X+WTtT1xdMScxm+bHoui7fR59fGyt9/l6WT6RLxLYzcvcy6/OPqIo5GfIlIrqVdIm+YDwicnPSz8n6rHjGV6yNihlZ7bL3kh/Pij+i++zffCVdY8/ZZ6RrrBasjYp1nl2jFeuzYmKPPpw/ux780sPpGhfvPydd46yduR17YpZ/B+2G4+kau86epms88OV8H+OwJV1j3wVL6RrZPX/vffkXnmMn8u/kl128NV3jf/9Vfl4vuzQ/J9mPjeOYH8+K9/o++QFjPe/0vikDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoYKF1A6ey2g8x6YeNX7/Sp3voZxv/50dETAoir67r0jXO37stXePezx9N13jo6CxdY+eWxXSNyZi7fmVcTvcwDrm1FRExJvbHo77whfy8XvTE/PoalnP38uUH8/t9aTG/1844O9/Hap/fJ32f72MWyY0SEauzaer6cZIfi4ozdIjVdI1xyD9uH1nNj8c0+VxaHfI9zIaC9VmwxivWRn6XRIzJNsbJw+keuln+HL9g//Z8HwV7rWCbxErBOTwkn/PT6VK6h8/+zSPpGsvH8/uk67ama3zqr47n+1jIPZOm0xPpHqLLv7s94cL8nPyTS3eka3z6L/Lra9bnT9HlWe7cWNqaWxcREeefl//Q9xefW0nX2Ht2/nPS6qzgEE2ajfl9UvBRK/rks2B1He8qvikDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoYKF1A6cyRB999Bu+vu83fu2jztu9LXX9lw6fSPfwHRemS0Qfx9M1xnFM1zh/x9Z0jWEY0jW2nLOaun51ll9bq6u5HiIihiE/J2fszNf44qFH0jX27cqtjW278+tiYXElXWM1vzRiNpvlaxScf5NJl65x6IFcH+ftW0z30HX5+/j05womNvE8e1TXTdM1zj8vt+e3bU+3EKsFa3x1qBjP/NpYLHg2Zp+vQ+R76KbL6RqPLOfn5Njh/Dl87JHD6Rp79+1O13jkeG59ff5vHk73MM0foSX7ZG5qDPkaaX3+fWXW5/9MveJz0iXflV9gy7P8+3B2bTxwMD8WD301PyfDan5tnLEjv8ZX+vwzOvtc6/v8c+2+v8mvrfMvyl2/OnzrY+mbMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAYWWjdwKt324zE5Y+OZ0ezEmO5hWJ2mrj9n19Z0D8srQ7rGdMssXWNhUjCeQ/5etp6Vv5eVoUtdP+vzPSxtzY/nsYfzfZxz/pZ0jejzNbpZn7p+NXLXR0SMs3xGPY75eZ2N+X1SsUa7LrdPIiJ2nJ0b07/92xPpHpaX5+PPHirGM7qC9bWaG4/lWX5tVazPvmCfVMxJxXNtIXLzOpnkX+W6gjk5fiy/1/acsy1dY/tqfp+srKZLxN9+7pHU9SVnRp+vUdFHyb0UWEj2MQ75tbV9R/7MWB3y7zyzghp9n68xmeTPjeUhd35d/ITc572IiPvuKXjXKHjGf/ru/HvThU9Ml0i/Dz/ycP7MWC3IAmbJ+1jPO/18vK0CAAAAPM4IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADC60bOJWV2UrE6uKGrz/88Jju4YzFpdT1XdelexjGlXSN2ZDP3hZiS76PMT8nq7MhXaOb5PpYna2mexgLxmJSEKkWDGdMhnyR1ZXcXun7WbqHMbkuIiIKhiL6MX8v/dCna1ScX9PkOj/3/Pwi/9vP5ceiQsV4VtRYOjNXY7aan5NZwfrs+zlZ49NpusaYPL8W4kS6hy62pmt88cF0ifjig/nzb1awNlaW8+9eQ/KBUHJm9AXrc1LwYJsT2TGdFLx47T4rP6+zIb9PsuszImI2Vrz0FLzXJ+f14H0Fnxl35O9j91n5/fq5T+f7+OtP5c/Qiy7N3ctXv5j/rNX3BZ87V3Njsbr6rd+Hb8oAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaWGjdwKnc81fHY2HbxjOjoe/SPWw7bzF1/Wx1TPdwxq50iViZDeka45i/l3HM9zE71qdrDNtyNWb9LN3D0SMn0jW278xnqv2wmq4x5LdaTPrknCwXrK2j+fGc7s6vjVm/kq7RR36/dl1+Yhcn09T1k4L7OPeCfI1Dn0+XKBnPCsurufV17Gh+r0235msMQ75GxZxU7LXJkHwVG3ake4jJ8XSJvuDZWGHoC95X+vZrtGJ9ltQY5+Ps6gv2fERujU4m+feEglfy6JPvTBERKwX7taKPeXDO+TvTNSaTh9I1Vma5d6aIiH1PyJ9/9/1Vfp3/9V8up2vMg1lyja9nj/imDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIGF1g2cymx1jFgYN3z9hd+d76H/Sp+6/ksPraR7WDpn42PwqHHs0jUeOpobi4iIXWduTdc48XB+yXYruTGdbVlN97C6mp+TbTvza2PW5+d1YSF/L+OYu5flry7me5jk76Mf8nt+GId8HwXzOpnkM/vsvE7GfA+TabpEDEN+bURFjUl+z//tZ3J9LG3L38eZC7N0jX7I75Ouy9/LwjS/wMbkvB47fjzdQ59fWiXnTsWcDLP8zQwF6ytbo2IsKlT0MS81Jslpza+KiNUhvz77Ib/XKt7/KvZ8hfyIfjldoeKdKaLg2VgwJ+dckn+XPfhXW1LX95H/rLV9R8V4Zq//1k8N35QBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQAPrCmV++Zd/ObquO+nHZZddtvbrJ06ciOuvvz7OPvvsOPPMM+Paa6+NQ4cOlTcNAAAAcLpb9zdlvud7viceeOCBtR9/9md/tvZrr3rVq+J973tfvPvd747bbrst7r///njRi15U2jAAAADAZrDuv194YWEh9u3b9w0/f/jw4Xjb294W73znO+M5z3lORES8/e1vjyc96Ulxxx13xDOf+cx8twAAAACbxLq/KfOZz3wmLrjggnjiE58YL3nJS+Lee++NiIi77rorVldX4+qrr177vZdddllcdNFFcfvtt5+y3vLychw5cuSkHwAAAACb3bpCmauuuire8Y53xAc+8IG45ZZb4p577okf/MEfjKNHj8bBgwdjy5YtsXv37pOu2bt3bxw8ePCUNW+66abYtWvX2o/9+/dv6EYAAAAATifr+teXnve8563978svvzyuuuqquPjii+P3fu/3Ytu2bRtq4MYbb4wbbrhh7f8fOXJEMAMAAABseqm/Env37t3xXd/1XfHZz3429u3bFysrK/HQQw+d9HsOHTr0Tf8bNI9aWlqKnTt3nvQDAAAAYLNLhTIPP/xwfO5zn4vzzz8/rrjiilhcXIxbb7117dfvvvvuuPfee+PAgQPpRgEAAAA2k3X960s///M/H89//vPj4osvjvvvvz9e+9rXxnQ6jZ/4iZ+IXbt2xcte9rK44YYbYs+ePbFz5854xSteEQcOHPA3LwEAAAD8PesKZT7/+c/HT/zET8SXv/zlOPfcc+PZz3523HHHHXHuuedGRMQb3vCGmEwmce2118by8nJcc8018eY3v/kxaRwAAADgdLauUOZd73rXP/jrW7dujZtvvjluvvnmVFMAAAAAm13qvykDAAAAwMas65sy307nPaGPxTNmG77+xINb0z1Mtwyp6/t+TPewOuZ6+Foj+RJHj218Lh61OM0vt6Ut+RxxONalrj/+SMGcDPkaw7RgfQ35Gl2fG8+IiNj5cOry1ZWldAvdSn59jl/KnzuzWX7DjrseSdeYTPJ7fjqdpq7vCvZJ1+XX5+7z8+fOV+7L9zFWPA+GXB9Hv5pfFw99KX/uDAVn18JiukRs3Zkfj4XF3PpaeTjfw9hvSdfoctu9zJhfGjEWFBmS51fF2VVhM/WRrbFjd36vrc7ar62qGit9fjwqTCM3phVjkX3fiahZ432ff4f80r35NboyLKeun1Qc5IsF95Ecz9V17BHflAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADSw0LqBUzkxW41+trrh66ezM9I9rJxYSV0/G3LXR0ScWO3TNaZdl64x9FvSNb5y+JF0jXN3b0/XGCM3pocODukepttn6Rp9fmnEOI7pGqsF66tL1hgX84Mx6RfTNfqVpXSNYcivr9UhPx6LY76PcczN64nD+bV17Gi6RAyz/Hh2Mc3XqDjLk2ujYn1W1JgVHID9UDCeX82/Rk0muT4mk/yfr3Vdfk665H6vUrG++pL1le8jq+Ilv+LcqahR8b7S97kaS7s3/rnk6z3kx6Jifa70+ffQir1WMa/ZPsaF/E7pI38fDx/KvyccP5pfXyur7eckunwPDxWMZ/dAbjxnx7/1teWbMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAYWWjdwKisPT2IcNp4Zben7dA8LC7nhOWd3PvNaGQ6na0ximq4xKxjP6Tima3z5yIl0jW3ZVd8P6R52nJuvMRvy4zmJfI0YunyNpMkkv9cmY36fjAX75KsnvpqucfzBxXSNSZef18kkt74mk/Zrq0rX5fd8hTF5DvcFa7zieTKOFWsjf/7Nw7NxGPJrq+IM7QrOjIoaFWu0LxjTyLeRNitoYlqwNiqUrK+F3Dvk6nwc4yV7vh/zNVb7WbpG9pkUkV8bFc+TyaTg3ImCee3z+7VkTpKfDfqCjyfjWPCukbx+WMc+m4+TFgAAAOBxRigDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADSw0LqBU5mcWIpJt3XD1w8xFHazMZMxn3mNx6bpGqtbltM1+r7gXsYxXWPbYn7JZtsYhvzaGsZZukY3dOkaFfq+zxdZya2vww/kx+KsXUvpGtPIj8XRr1bMa359TSb5PZ/drRVnRoWum4+9ViE7pn3B+VewTSLGgj4K5rUf8zeTnZNpwV6dFxV7reQZXVBjHObg/Co4ujbTObx9Z/L8Kzi75mWNV7y7VayNint5+Mu5t43+cH5OuoX857WKo3yseDYWrPPs+qpYW/Nwdq1nHDbPkxwAAADgNCKUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0stG7gVB6ZfDUWpssbvn768K50D9u254ZnmA3pHr74+XyNmC2lS0wK4rtx7NM1ti7lG+mH1dz1ff4+ZgU1+uX8WBx/IL82Fhbyx8hkmrt+7PL7ZDLO0jX6Id/Hubu2pWt86asr6RoxjukSQ+TW+dh16R4qdHPSx6Sgj/QazR9dMRTsk4oak4IH26Tiz7aSW21W8GydFOz3ivVZoeIcjpI1mh/TrJL1WTCtQ8EzusKWrbk+ZkN+MCqeJ8NqvsbDh9Il4pHDBc+kWfuPotNpfn1O+/xYjAXncEWNvi94/0ueoRXP+Iq9lh3PcR334ZsyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABhZaN3AqK+MshmF1w9cf/8rRdA8LR3LDs2v3tnQPiwW52bGVlXSNySTfx3nnnpGusVJwL7NhzF2/svF1+agvfSo/nt00X2M6naVrDMOQrpFdX13XpXtYWc6PxZFj+bWxuJC/l9XV5XSNij3f9bl7qZjXeVFxL31BH+OYO//6Pt9FtoeqGhX3UrJPkmujooexy4/nbJKvUaJgo1Q812az3DOl4syoqFGxvipU3MtD9+fmdfX4lnQPY8E+6SrOv1l+Xmez/D4ZhoonW07F82Qe3oWrVNxL9vla0UPFmbEwza2NofvWx2E+Zh8AAADgcUYoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0sNC6gVMb/u+Pjdl+ybF0B7Pj09T1Dx15JN3D9sUz0zUmXZ+uMVvJ11hdzdcYxzFd48FDDyd7yN9H13XpGhWJ6tjlx7Pf+Db9eo0xdzfnnLMr3cMX7n8oXWMY8oNRscYrTCYFK2ySX+dZFXutwrz0kV1fY8GGn5d9UjEnJTWmub02N/exSdZ4RETfF7yvVDwck+ZlXitqjAXLa/WruSKTyUq+iTlRcQ73q7O56COr5J285J0p97kzouZeppP8nGxdykUMS0tb0j3MwzNptvCtP498UwYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANDAQusGTqV/aDFiecuGr3/4KyvpHsZhmrq+69ItxMKuPl1jaevGx/FRO3bkxiIiYjabpWuM45iusXvPUrKHgomNIV3h0AXvy3dxYjVd4+KjL07XmExy+fAw5u9j1+7cuoiIWF7Jr/ETx5fTNVaW8+trGPI1uopDMKnizJiH+5gXFeNZsbbmRcXa6PpcjU21PicF9zLk1+hmWecl63MTra+5uJeCNd7ll2fJ+qzYJxU1FrfkPqMsLuY/42xZWkzXWNoytx/L1y27vlb7gkUe+RqT5Oe19Xz29U0ZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAAwutGziVbliMSb+44esXFoZ0D9u2b0ldv7AwJ8PbdekSs35M1xjHfI0K+T7y9/HQlX+SrrEYO9I1lv/mK+kaq7P8Xpt0fbpG1jDm98lkks+5t5+xraBGukRMSs6N3Lwef2Ql3cPyiVm6xliwNrqu4vxr/+cow5Df7xUqniddwRqveB7kz435eLaWjGfyzIio2a8V62se3nlKlnjB+qo4/6bTabrGwmJury0u5ntYWtr4Z5tHFbxqlOyTvuB5MA97reLsqqixspo//yYFe63ifXgezr8K2Rnp17FF2r/hAQAAADwOCWUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAYWWjfwWNl+xlLrFqLv+9YtRERE13XpGhV3MinoYxzHfI10hbwdH/6n6RpHn/Gn6Rp7v/SidI2hG/I10hUqFKytgvU5L4aC/Zq1reAcn4dnQUTN2lhdmeVrrK6mru/7/LqYzfL3MRnzf6Y0zsXTICK63HhMJvmxqKhRcWRMp9N0jZp7yd/MdCHXR8V91Dxd5+PPbyvmZB4MBc+CoeClfBzza6PiuTYMBX0kr69YWRXrs6JGzft0+/fhufm8l+xj1n/rz/f5OGkBAAAAHmeEMgAAAAANCGUAAAAAGhDKAAAAADSw7lDmC1/4QvzkT/5knH322bFt27Z46lOfGh/96EfXfn0cx3jNa14T559/fmzbti2uvvrq+MxnPlPaNAAAAMDpbl2hzFe/+tV41rOeFYuLi/H+978/PvWpT8Wv/dqvxVlnnbX2e371V3813vSmN8Vb3vKWuPPOO+OMM86Ia665Jk6cOFHePAAAAMDpal1/JfZ//I//Mfbv3x9vf/vb137ukksuWfvf4zjGG9/4xvilX/qleMELXhAREb/zO78Te/fujfe+973x4z/+499Qc3l5OZaXl9f+/5EjR9Z9EwAAAACnm3V9U+YP/uAP4sorr4wf/dEfjfPOOy+e/vSnx2/91m+t/fo999wTBw8ejKuvvnrt53bt2hVXXXVV3H777d+05k033RS7du1a+7F///4N3goAAADA6WNdocxf//Vfxy233BKXXnppfPCDH4yf+ZmfiZ/92Z+N3/7t346IiIMHD0ZExN69e0+6bu/evWu/9vfdeOONcfjw4bUf991330buAwAAAOC0sq5/fWkYhrjyyivjda97XUREPP3pT49PfvKT8Za3vCWuu+66DTWwtLQUS0tLG7oWAAAA4HS1rm/KnH/++fHkJz/5pJ970pOeFPfee29EROzbty8iIg4dOnTS7zl06NDarwEAAACwzlDmWc96Vtx9990n/dynP/3puPjiiyPia//R33379sWtt9669utHjhyJO++8Mw4cOFDQLgAAAMDmsK5/felVr3pV/MAP/EC87nWvix/7sR+LD3/4w/HWt7413vrWt0ZERNd18cpXvjJ+5Vd+JS699NK45JJL4tWvfnVccMEF8cIXvvCx6B8AAADgtLSuUOb7v//74z3veU/ceOON8e///b+PSy65JN74xjfGS17ykrXf8wu/8Atx7Nix+Kmf+ql46KGH4tnPfnZ84AMfiK1bt5Y3DwAAAHC66sZxHFs38XcdOXIkdu3aFVfe9I5Y2Lq9dTubQtd16RoVi2RS0UfBcp2rBZ9w9Bl/mq6x+6M/lK5Rsb42izk7TlPmYl4LepiDu4iImrWxujLL11hdTV3f9/n7mM3y99GN6/q3r7+pedmt3STXyWSSH4uKGhVnxnSarzE397KQ66PiPiKGghoVfeTNxTOpwLzcR8n7dEGNYciv0WwXFTNSMa/zsjYqZNfG3HzeS/YxO/FIfOT/fV0cPnw4du7c+Q/+3nV9U+bbaRyG3Eadg5f5uXnha93A/9XPwcEbMR/jUXEfZ975g+kaffTpGiUPkWSNijkteQBUvP92+T7m5cE+D31sljMjIv8hLyJispD82w5LXpQ2z9+4WLI2suffHOyziChZGyUq/gCo4H1lSI7H0Bc8n9MVIqLgPWFuJNdGxR8uVpibD6wF7zxjwTtPdjwq5jW73yPm511jHlSM5zxYz33MR/wNAAAA8DgjlAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGlho3cCpjOMY4zi27aHpP71O63Gs1RXUSI7HWNBDl5+TYU7mtevy49Flx7RgPKPgPsaCU6PiVirWxqRgPIZhSNeYCwV7vuTYqFhfyXmdl3NnXtScf7kxnZcZmZc+Kt4SKszDu9dYsD5LFIxFxWhmn2t9QQ+bScUaH4b2+6TiTaXiWVBhPrqI/Dv1nJwZaeu4D9+UAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANLDQuoFTGruv/djw9XWtbFg3D01sLuOQH9N5qNB1ibX9aI15iVTH/HgM2TGt2GoF91Ghoov86oroC8ajYp3PhTk5yyu6GIahoAqPGufg3Ng0+6xIzeOg/byW2Cz3UST7XLPXTjYv+yQ7K/NyHyVnV0GNEnMypq2tZ23Ny8c6AAAAgMcVoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoYKF1A3/fOI4REdGfON64kwLd2LqDTWccWndQo+u6fI1NFKnaKbXyq6tmTirWObUefcayedhn9ewTvhl77WTzsk+ys1JxHxVrYz5Gk0qP5hnfyhrrxnnZUf/X5z//+di/f3/rNgAAAAA27L777osLL7zwH/w9cxfKDMMQ999/f+zYseOUqeORI0di//79cd9998XOnTu/zR3CP8z6ZN5Zo8wz65N5Z40yz6xP5tnjaX2O4xhHjx6NCy64ICaTf/hfcZi7f31pMpn8o0nSo3bu3LnpJ5PTl/XJvLNGmWfWJ/POGmWeWZ/Ms8fL+ty1a9e39Ps20X+VAgAAAOD0IZQBAAAAaOC0DGWWlpbita99bSwtLbVuBb6B9cm8s0aZZ9Yn884aZZ5Zn8wz6/Obm7v/0C8AAADA48Fp+U0ZAAAAgNOdUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADZx2oczNN98cT3jCE2Lr1q1x1VVXxYc//OHWLfE49ad/+qfx/Oc/Py644ILoui7e+973nvTr4zjGa17zmjj//PNj27ZtcfXVV8dnPvOZNs3yuHPTTTfF93//98eOHTvivPPOixe+8IVx9913n/R7Tpw4Eddff32cffbZceaZZ8a1114bhw4datQxjye33HJLXH755bFz587YuXNnHDhwIN7//vev/bq1yTx5/etfH13XxStf+cq1n7NGaemXf/mXo+u6k35cdtlla79ufTIPvvCFL8RP/uRPxtlnnx3btm2Lpz71qfHRj3507dd9Vvq60yqU+d3f/d244YYb4rWvfW187GMfi6c97WlxzTXXxIMPPti6NR6Hjh07Fk972tPi5ptv/qa//qu/+qvxpje9Kd7ylrfEnXfeGWeccUZcc801ceLEiW9zpzwe3XbbbXH99dfHHXfcER/60IdidXU1fviHfziOHTu29nte9apXxfve975497vfHbfddlvcf//98aIXvahh1zxeXHjhhfH6178+7rrrrvjoRz8az3nOc+IFL3hB/MVf/EVEWJvMj4985CPxm7/5m3H55Zef9PPWKK19z/d8TzzwwANrP/7sz/5s7desT1r76le/Gs961rNicXEx3v/+98enPvWp+LVf+7U466yz1n6Pz0p/x3gaecYznjFef/31a/+/7/vxggsuGG+66aaGXcE4RsT4nve8Z+3/D8Mw7tu3b/xP/+k/rf3cQw89NC4tLY3/7b/9twYd8nj34IMPjhEx3nbbbeM4fm09Li4uju9+97vXfs9f/uVfjhEx3n777a3a5HHsrLPOGv/Lf/kv1iZz4+jRo+Oll146fuhDHxr/2T/7Z+PP/dzPjePo/KS91772tePTnva0b/pr1ifz4Bd/8RfHZz/72af8dZ+VTnbafFNmZWUl7rrrrrj66qvXfm4ymcTVV18dt99+e8PO4Bvdc889cfDgwZPW665du+Kqq66yXmni8OHDERGxZ8+eiIi46667YnV19aQ1etlll8VFF11kjfJt1fd9vOtd74pjx47FgQMHrE3mxvXXXx8/8iM/ctJajHB+Mh8+85nPxAUXXBBPfOIT4yUveUnce++9EWF9Mh/+4A/+IK688sr40R/90TjvvPPi6U9/evzWb/3W2q/7rHSy0yaU+dKXvhR938fevXtP+vm9e/fGwYMHG3UF39yja9J6ZR4MwxCvfOUr41nPelY85SlPiYivrdEtW7bE7t27T/q91ijfLp/4xCfizDPPjKWlpfjpn/7peM973hNPfvKTrU3mwrve9a742Mc+FjfddNM3/Jo1SmtXXXVVvOMd74gPfOADccstt8Q999wTP/iDPxhHjx61PpkLf/3Xfx233HJLXHrppfHBD34wfuZnfiZ+9md/Nn77t387InxW+vsWWjcAwGPr+uuvj09+8pMn/fvm0Np3f/d3x8c//vE4fPhw/Pf//t/juuuui9tuu611WxD33Xdf/NzP/Vx86EMfiq1bt7ZuB77B8573vLX/ffnll8dVV10VF198cfze7/1ebNu2rWFn8DXDMMSVV14Zr3vd6yIi4ulPf3p88pOfjLe85S1x3XXXNe5u/pw235Q555xzYjqdfsN/OfzQoUOxb9++Rl3BN/fomrReae3lL395/OEf/mH88R//cVx44YVrP79v375YWVmJhx566KTfb43y7bJly5b4zu/8zrjiiivipptuiqc97WnxG7/xG9Ymzd11113x4IMPxvd93/fFwsJCLCwsxG233RZvetObYmFhIfbu3WuNMld2794d3/Vd3xWf/exnnaHMhfPPPz+e/OQnn/RzT3rSk9b+NTuflU522oQyW7ZsiSuuuCJuvfXWtZ8bhiFuvfXWOHDgQMPO4BtdcsklsW/fvpPW65EjR+LOO++0Xvm2GMcxXv7yl8d73vOe+KM/+qO45JJLTvr1K664IhYXF09ao3fffXfce++91ihNDMMQy8vL1ibNPfe5z41PfOIT8fGPf3ztx5VXXhkveclL1v63Nco8efjhh+Nzn/tcnH/++c5Q5sKznvWsuPvuu0/6uU9/+tNx8cUXR4TPSn/fafWvL91www1x3XXXxZVXXhnPeMYz4o1vfGMcO3YsXvrSl7Zujcehhx9+OD772c+u/f977rknPv7xj8eePXvioosuile+8pXxK7/yK3HppZfGJZdcEq9+9avjggsuiBe+8IXtmuZx4/rrr493vvOd8fu///uxY8eOtX8/d9euXbFt27bYtWtXvOxlL4sbbrgh9uzZEzt37oxXvOIVceDAgXjmM5/ZuHs2uxtvvDGe97znxUUXXRRHjx6Nd77znfEnf/In8cEPftDapLkdO3as/fe3HnXGGWfE2Wefvfbz1igt/fzP/3w8//nPj4svvjjuv//+eO1rXxvT6TR+4id+whnKXHjVq14VP/ADPxCve93r4sd+7Mfiwx/+cLz1rW+Nt771rRER0XWdz0p/V+u//mm9/vN//s/jRRddNG7ZsmV8xjOeMd5xxx2tW+Jx6o//+I/HiPiGH9ddd904jl/7q95e/epXj3v37h2XlpbG5z73uePdd9/dtmkeN77Z2oyI8e1vf/va7zl+/Pj47/7dvxvPOuuscfv27eO//Jf/cnzggQfaNc3jxr/5N/9mvPjii8ctW7aM55577vjc5z53/B//43+s/bq1ybz5u38l9jhao7T14he/eDz//PPHLVu2jN/xHd8xvvjFLx4/+9nPrv269ck8eN/73jc+5SlPGZeWlsbLLrtsfOtb33rSr/us9HXdOI5jozwIAAAA4HHrtPlvygAAAABsJkIZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAAN/P8BHoiDVZY7pMYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3000x1400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, state in enumerate(train_loader):\n",
        "    # state = state.to(device)\n",
        "    state=state[0]\n",
        "    print(state.shape)\n",
        "    imshow(state.detach().cpu())\n",
        "    # sx_ = agent.jepa.enc(state)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "2iFaTrgsTbuC",
        "outputId": "db570878-3697-4f7e-b9f2-098b905dacde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 64, 64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3000x1400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGUAAARhCAYAAACLezhIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABwVElEQVR4nOz9fbBnZ1kn/F7r97b37pfdnQTSnUwSjCMaUGEwaOgBZ5TJmEP5UFCkHPXgM4yHGksroJCa0kqVgmU5hnFKQapCUIcBfephUKYKRqwCispoONYkKGE8I+pEVJwEQ3d4Sffut71/b+v8wUNjC81093XB/evm86nqqmT3Xleutda97vte3/1Ld9f3fR8AAAAAfFUNWjcAAAAA8LVIKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaGLVu4O9bLpfx2GOPxd69e6PrutbtAAAAAJy3vu/j+PHjce2118Zg8OU/C7Nyocxjjz0W119/fes2AAAAAC7ao48+Gtddd92X/Z6VC2X27t0bERH/46//V+zdu3nRdQYFH7IZJj+pMyzoYVTwaaGKPiquZ8XnnpZ9vka2RMV5VHwIrOJa7BQUmRbUmCVLVPQwL7ies4I+CtooGaMVfWQvR0UPFfoVmHeqLJMnU/GsVcxdy3yJkptSsTZmF4SK/w999yh/IhU19hRsWDYKalTsvbJjY1U+K14xdy0KilTMPdvJRo4XnEjFeVTck+GKjLCCaSN2JZ/59YI5Y1wwEVfMOxXXs8Iq7HkqLkX23WBraytuvOH6M/nGl7Nyoczn/5elvXs3Y3NTKCOUOZtQ5gsup1BmepmEMquy2RLK1BLKfEHFGK94QbucQpns/6q9KqHMnoIae4UyZ6zIu9VlFcqMs40UnEjFvmtVQpmKMboKoUzFnCGUOdsq7HlWIZT5vPNZ5/1BvwAAAAANCGUAAAAAGviKhTL33HNPfN3XfV2sr6/HLbfcEn/4h3/4lfpPAQAAAFxyviKhzG/91m/FnXfeGa997WvjIx/5SDzzmc+M2267LR5//PGvxH8OAAAA4JLzFQllfvmXfzn+9b/+1/HDP/zD8fSnPz3e/OY3x65du+I//sf/+JX4zwEAAABccspDmel0Gg899FDceuutX/iPDAZx6623xgMPPPBF37+zsxNbW1tn/QIAAAC43JWHMp/+9KdjsVjEgQMHzvr6gQMH4vDhw1/0/XfffXfs27fvzK/rr7++uiUAAACAldP8b1+666674tixY2d+Pfroo61bAgAAAPiKG1UXfNKTnhTD4TCOHDly1tePHDkSBw8e/KLvX1tbi7W1teo2AAAAAFZa+SdlJpNJ3HzzzXHfffed+dpyuYz77rsvDh06VP2fAwAAALgklX9SJiLizjvvjJe97GXx7Gc/O77jO74j3vCGN8TJkyfjh3/4h78S/zkAAACAS85XJJT5/u///vjUpz4Vr3nNa+Lw4cPxj/7RP4r3ve99X/SH/wIAAAB8rfqKhDIREa94xSviFa94xVeqPAAAAMAlrfnfvgQAAADwtegr9kmZrD2jLvaMuqY99NnjswUiYlFQZFHQR8Wd6AqKdAWdZCsUXM5Yrsh9rRijg4IbO0w2MhkUjItl/mIMCvqYLdMlYrYi4yt7KhXPScFtTZ9HRE0fFdLr2grMwRER44Iio4IfS40Knvm1ZB8bw3wPFXPopGItKLiv84pnvmDuyV6OQcmTklexnuwULCinCybiaXIi3i6YyGcrshgUPPIlNbqCeWORXNmmBWOr4rbOV+SeVLx+j5L3teI8VuNJO38+KQMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhg1LqBc/nvR2exezm76OOHgy7dwyhZYpxvIUYF5zHMtxFdwblUGBQ0kr0eBbekpka+RIwqrucKnMukoIe1ghOpeE4qxnjF41pRY9nnjl/0yQIRMc2XKOmjRsVdyZ1LzVKwGs/a5WJVLsVsRZ7XCqswhw67/MUoOY+CIuOCTc+k5MfIuT6WBR3MsgtjREwLnrVZwcnMStbXfI1j81yRUcGztneYH6DrFS9sBdfz4t++v2CeHKMVc1fFu1b2cl7IdfBJGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACggVHrBs7l2/ZPYnNzcvEFunwPBSXS+ooafb7KsqCPRcHJLAtqDJI3tmJclNzXghoV13NRMb6SJWYVF6NAV9BHV3Bnu4JBWpHYD5KNjLMPa0SspStEdBUXdEUs+9y5VMzj84I5Y74iz3zNc1JQJGlV1viCoVGyJlWsKdkaFedRYVQw/60VLI7rw3wf42SJ7JoWETGqeD8p6CN7LSJq9pAVc092PZgVNHF0ni+ymOX7qFCxJmXXxoqxtQoz6IlTi/P+Xp+UAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANDBq3cC5fGa2iOlscdHHz5b5HpZ9nzo+d/TnDLuuoEZBI5Evskhez4iIWcFFregjq2RsFNyTUUEsWzG8xskig4LnpEJ2zqiyGl1EDNL3taCHfImSGhVDtGKUZ8+l4lkbFdzYccEgr3hOKmoskkWmBfPOdJmvMS+4GBVjvOKeVDzzk+TJ9AXPWsn6XHAxJgXnMi6YN7L7v4rnpGIfOy/o41R24omI7YIaOytwTUt6KHjvrHg/KWhjJebQScHzXlFjLXkiJy9gYPikDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIFR6wbO5YHPTGPXdHrRx/cFPQy63PHDLlkgIkb5ErExzBfZW9DI/nE+A9xT0MfuYa6PcXZgRMR0mR+h84JBXvGcVPRxOlnk1HKR7uFEwYlU1Di1yNeoGF8FbaSNC+a/iue14lIU3JIouBxRsCylFdySkp8oVdzXguU1JskLsie5pkVEbAzTJWL3KN/HesGNrRjiFWMjW6Oih4p5/MQ8X2PWL9M1KubQRZ8rUrHfqZA8jTIV92QnPzRiO9lIxfWsmHeya0FEzfq6Cu+v6wUnslnw3jlJljg+Pf/F1SdlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADYxaN3Au//iqtdi7uXbRx3cFPWRrDAqaGBbUGHf5IhV9VFj0+RqzPldka7ZM93Cq4ESOzfI1js7z53Jinu/jdPJ6bBdcz+kyX2ORv5wlY7wibR8UFBknJ8FJQQ/bBfekYvoruK0lfYySRdYLFoONgsVxd/ZEIuLKcX6APalgkO5JnsukYrOxIlblWauYhxfJvUbB0lpSY1awNk4L5uHTBX3sLHIjrE/e04j8uIiI6Ar29RXPWsk7SsH8tTd5NhXPe4WKPc+ugptSUWMtWWOY7iCi4DFJLyjDC1jffVIGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQwKh1A+fyye1FHJ8sLvr4krSpqyiSs+zzNeZ9vsiioI9pwcnsLPN9zJN9XPyo/IKdggt6uqDGrOB6Vhgln7X1Yf5h3RznZ41JwcQz7PLn0kd+bFTMPRU1sgouZ8lSUHFfN7MPSkRcvZYbpFcVDPK9Bc/rcLACC3TUrI3L5Bo9L5jHhwVzV8Ud2S64oEcKNgqf2M6v9E8kF9hZyQSavysV97ViHi6YNmKcbKRija/Yr+wq6GNSMIfuLViTKmqsJ8+lZt+Vl10LPlcj30fFupadOIYFLYwKnpNB8kSWF9CET8oAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaGLVu4Fz+4e5RbO6++PbmfZ/uYbrMHb+9zPewU1Ajex4REbOCGuOuS9dYG+Svx6zP9TEtuCej/KWItUG+yDx/KrEoeNayZzIuuBbrw3yNjYI+Ngr6mBTE7RXXNPvMjwrOY1LwrE0qrkXJuRTMocnxVfGTnIIptKRGwW0tuR6z5LmcWOQX6M8WbBS2ChaU0xV7nkW+RkGJ9Do/KZgAK/YJu4bpErGrYF1bH+avxzjZRsX1rFgLRgVrQcU+oWBolMhOPacL5tBTBZPG0exiEBHH5vlzOVbw0nc6eT0q5uBhwXNycD33oJzcmp739/qkDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIFR6wbO5ZPbizg+WVz08cs+38O8zxVZFPSwCucREXGq4GSW6QoRg65L18hWGBf0MB61P4+ImlR2p2CQnk6Or1nB4Hpimi9yuOBazFfkWRsN8iPs6rXcCPuG3fklas8kP8oLSsRawfWcFNTIVqgYWxUKLkUULI0l89802ci0oIftFZjHIyKOFUzmFfuVk/N8jVnymhZsNWJUUGRPwX5lc1xQY5S/J/uSfUzG+cVg16BiTcpfz4Lpr2TuqVhTssNrX8F93RhWXNH81Tg2z3dxdJo/l0dOzlLHV+zJ5yuwYdk5ceq8v9cnZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABkatGziXyaCLtUF30cefXPTpHk4na2zN8z2cLKgx7/M1Rt3F34vPmxREgOOCGolhFRERBbckCkrEsuC+Lgv6qLge82Qj02W+ic/u5K/GZ3cW6RonshcjIqYF81/F2Mj6rwXzzt5xvsY1G/ml8ut3D9M1rivo46q13CS6Ocpfz3F2Eo6IfIWaIuvDfJH1ZCObBTu5g2v58Vmx19gumLtOVNQoWNi2k+tSxbWYFazPs4L1tWKfUHJfkzUOF+wTnlSwkX1Sch6PiNgc5WusF8zlw4J9fbaLij3kqRV5XocFC9tGwbq2b5xbUwouZ8k8vkiOjQu5lD4pAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGLjiU+eAHPxgvfOEL49prr42u6+Ld7373Wb/f93285jWviWuuuSY2Njbi1ltvjY997GNV/QIAAABcFi44lDl58mQ885nPjHvuuedL/v4v/uIvxhvf+MZ485vfHB/60Idi9+7dcdttt8X29na6WQAAAIDLxQX/RYoveMEL4gUveMGX/L2+7+MNb3hD/PRP/3S86EUvioiI3/zN34wDBw7Eu9/97viBH/iBXLcAAAAAl4nSP1Pm4x//eBw+fDhuvfXWM1/bt29f3HLLLfHAAw98yWN2dnZia2vrrF8AAAAAl7vSUObw4cMREXHgwIGzvn7gwIEzv/f33X333bFv374zv66//vrKlgAAAABWUvO/femuu+6KY8eOnfn16KOPtm4JAAAA4CuuNJQ5ePBgREQcOXLkrK8fOXLkzO/9fWtra7G5uXnWLwAAAIDLXWkoc+ONN8bBgwfjvvvuO/O1ra2t+NCHPhSHDh2q/E8BAAAAXNIu+G9fOnHiRPzlX/7lmX//+Mc/Hn/8x38cV155Zdxwww3xqle9Kn7+538+nvrUp8aNN94YP/MzPxPXXnttvPjFL67sGwAAAOCSdsGhzIc//OH47u/+7jP/fuedd0ZExMte9rJ429veFj/5kz8ZJ0+ejB/5kR+Jo0ePxvOe97x43/veF+vr63VdAwAAAFziLjiU+a7v+q7o+/6cv991Xfzcz/1c/NzP/VyqMQAAAIDLWfO/fQkAAADga9EFf1Lmq+X3P7UdG9uTiz5+0HXpHpZf5hNB52OxTLcQs2QPERHzihoF57LItxEFpxLjYbaJfA87BRfj+Cx/U07M8n2cKhgc8+T1qHjWss97RP48Pidfo+I5ifwUGpNhLvfftZb/ucEgsg98xNFhfoA9Oshf0NPLebrGp2e5a7p3lL8nm6P8tSgYGiVrUoVF8viK9XlaMGlsF1zQE/N8ja2CC3JsWlAjuUZ/ZiffQ8Veo8KyoI2CbX3avnG+iZv2jdM1hl3+9a3gVGLYFazRfb6R7NgYF6zPaxXvJ91q7CEXBetB9pnfKZg0KtakneSaNL2A9yyflAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQwat3AufzpE9OYzKYXffz2fJnuYbroU8f3ucMjImKRP42YFRSZJ69FRMSy4FxWwbLgxhYMjZLx1eVLlKi4Hlmrci3Go3wnu9byefuVG8N0jafsGaeOf+pm7viIiKfsyp/HP1jP19g3yt+T8WXyY5SKpaBi/qvoY1HQSHa7cmqZ7+HoNF9ju2CfcHSWvytbBTVOzdvf1/VBfi2oWNemBeOrQldwMlcn5/JvLliTbtydX0+uKFgMdg+tSZUq1qTtgmft2Cxf45OnF+kaf7U1Sx1/dCc/j28XrAXT5Dv07OT5XwePEwAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIFR6wbO5R9ujmN97/iijz8979M9HN1ZpI5/YrpM93B6lq+xWHbpGoNB/nrmK0QslwV9VDSS7iHfRNfl7+tomK+xNs5nu5NkH5NR/jz2TvLnsbfgWuwv6OOqtWG6xpUFfVyRvB7/YCPfw5Mn+WuxVvCcVPwEpGD6i+y0MS+Yu0rW54Iaf7udW+MjIj55Ol/j0zu5df5YwT7hZEGNrYI9z4mC+zpb5GssC8b5IPmwjQomjYq5a72gkYKlMdYG+XM5Oc+N0Y+fmqd7KNi6xXB3wd6tYF8/jHwfi4JnLTt9FUwZUTA844aN/Gt5RY2b91/8+/fnZdfXTxSsrX9bUeNk7pnf3pjG75zn9/qkDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIFR6wbOZWu+jOlsedHHd32+h6vWh6njn7JnnO5h77hL19g3zmdve0b5PnaP8n3sHub7mCRrTAqizHGXP498hYg+8g/KouBZm178ox4RETvLfBOnCk6kosa04FwKSsSgYIAlb2t8epo/kdOLRbpGds6IiMitJnWyV3ReMLaOJ9b2zztaUONUxYNSMJevJcfXaJ5uIXYX7BN2FazxBXekZA5dhXm47yuuRn581nSRr1Ix92wnl4PHTuXXk08W1Pj/fmo7XWNUMHddNcmvbNdsVNTIzT1PKtjY7ymY/yqWpIp3lM2C9WB/spFv2J2PKD6TfbmIiMeTNU5szeN15/m9PikDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoYNS6gXP55n2T2LV30rSHPnn8MlsgIqaLfJHDpxfpGqdmy3SN4wU1tgpqHN3J1Tg1z/dwelpQo+BaTOf58bUoGOh9n6sx6Lp0D8NBvsZomK+xNs5n5Zsbw3SNK9bzNfZPcueyb5LvYWOUvyd7Rvl7UnBbY1bwrM2SJQouZ+wa5i/GWn5olKi4HuvJeWNf8jmLiDhVsBZUrM+fKtivHD4xT9c4djJfY1awV8gqWBpjXDD/7SlYk67clX9luTrZx2bBmpR93iMiCm5JDAsGR8X890TBvHFkJzdvnMgujBFxepE/j/WCfehawU3pIl9jmdzXV8yeBY9ajJLPyenj0/P+Xp+UAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaGLVu4Fx+/b9/Jka7phd9/HKZ72G57FPHT2f5Jna25+kas2m+j1W4nhERUVAiqxt06RoFJaIvqFGhoo3BMFdlOMrny+NxvsauXfkpdVgwOKbz/IOys8jXyD7yFc9JVzFn9Pkigy5/Ml1BjXFyEj1VMLaO7uTXtWMF6+t2wRifFaxrx5Nr9KdP5q/nidOLdI35PH9PCh61WJY8r/k+suvSxtow3cPe9XyNXZOCtXGUv6DryX1CRMRassa+cb6HKwvuyaJgT36s4N3gswU1KubyrWQf2Tk4IuL4Tn4Ordi7bRdcz5L3taSKd4tRwbvBJDl3zU6eOu/v9UkZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA6PWDZzLcNDFcNBd9PHb00W6h+l0mTp+VtDDLNlDRMRy0adr9PkSMRxe/P38vEFBjDhIjKuIiLX1YbqHjY1xusbaWv5ijEf5Gpnn9POy93U8LLgWBeNz9yTfx5MKxtfBXfkaVxf0cd1GrsaTC8b4vnG+xq6CMV4wvCK/GkTMkkV2lvnFoKLGyYJ1batgfT0xz/dxYp7ro6KHk4v8tSi4nLFTcF9PF9TYrqiRvC995HsYF8xduwr2CQXTcMHVyD/znzgxT/dwKvm8R0TMCp75gjZKXg7mBevBNLmwndoueF8ruKBdxV5jRWpsrOX2f3uT+8eIiM2CfewkuXnbGZx/1OKTMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAZGrRs4lxuv3ojJnl0Xffx80ad7mC1zNRbJ4yMi8hUiui5fYzLM53e7x/lGdo3yfYwHuT6mBWPr5GyRrjFdpkuUWBT0cXqeK7I2rBhbBTXGBeNzmC4Rn9nJ35StWb7GY6fmqeP3T/IX48q1/D3ZLJh3dleMr4KxsSs5l08KfpRTcClis6DI7mH+gi76/How73N9nF6BOfhzfeSvxXZ+aYxpQR+zgs3XTnKvcKLgnhwr2CgcLahx+HT+xh7fzvdxIjnATu3kz+N0QY1ZQY1FweatYPorkV0NBgXvOOOCxXGjYA+5f884XePg3oIayQ3L3oJrUfIOnTz+dHf+19InZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA2MWjdwLoOIGCaO31706R52FsvU8ct8CzHsunSN8TBfY5QvEZNBvsiT1/I54vW7c8P+2vXMyPyc/ZP8eawVXM9Fnx+kx2f5Gp+eLlLH/+WJebqHRwpqPH46dx4REZ8t6OPEyVm6xuntfB+L5DzcFcx/6wXP68ZaQY2CPnYX9LE3OYfun+R72Fcw/1U4lpx3IiKemOb2CRERJ3dyNU4V9DAtmMeXBetJX1EjXSFiPs9f03ly/sseHxGxmOdrVFyL5bLges7yNfpkiYLhmV4XP9dHvkbF+joomMq7gr1sto9BwQvboOBda1BwLfYW7DWuLKhxzUbuXevJ6/nBtavg5XXvMNfHifHaeX/vauyMAAAAAL7GCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAZGrRs4l73jQayNLz4zunpjmO5hMuxSx49yh0dExKDLF5kM8jX2jvM1rpjkM8BxwbnMln3q+E+cXqR7+NjJebrG8dkyXeMzO/kanzwxS9f4xGd3Usd/5li+h+l2/p4s5rmxFRExm+bH12K7oMYify7Z6asreN6HBRNxN8rPXcOCGhu780v25t5x6vjdG7njIyL278qvz/vX8zUq9glPWs/fk3mfe9Z2Cp7VE/P8WpA8jYiISC7PERFxuuB6nC5YX4+eyq0pJ07n16Rplz+Peb6NmE/zfVSs0bPk/m1RsD4vC/YJUVBiULI25msMxgXva7ty8/CuPfl1bZx4Z620XTB3nSyosbPM1Ti9WI3339kg97BdyPq+GiMIAAAA4GuMUAYAAACgAaEMAAAAQANCGQAAAIAGLiiUufvuu+Pbv/3bY+/evXH11VfHi1/84nj44YfP+p7t7e2444474qqrroo9e/bE7bffHkeOHCltGgAAAOBSd0GhzP333x933HFHPPjgg/GBD3wgZrNZfM/3fE+cPHnyzPe8+tWvjve85z3xzne+M+6///547LHH4iUveUl54wAAAACXsgv6O8Te9773nfXvb3vb2+Lqq6+Ohx56KP7JP/kncezYsXjLW94Sb3/72+P5z39+RES89a1vjac97Wnx4IMPxnOe85y6zgEAAAAuYak/U+bYsWMREXHllVdGRMRDDz0Us9ksbr311jPfc9NNN8UNN9wQDzzwwJessbOzE1tbW2f9AgAAALjcXXQos1wu41WvelU897nPjW/5lm+JiIjDhw/HZDKJ/fv3n/W9Bw4ciMOHD3/JOnfffXfs27fvzK/rr7/+YlsCAAAAuGRcdChzxx13xEc/+tF4xzvekWrgrrvuimPHjp359eijj6bqAQAAAFwKLujPlPm8V7ziFfG7v/u78cEPfjCuu+66M18/ePBgTKfTOHr06Fmfljly5EgcPHjwS9ZaW1uLtbW1i2kDAAAA4JJ1QZ+U6fs+XvGKV8S73vWu+K//9b/GjTfeeNbv33zzzTEej+O+++4787WHH344HnnkkTh06FBNxwAAAACXgQv6pMwdd9wRb3/72+O//Jf/Env37j3z58Ts27cvNjY2Yt++ffHyl7887rzzzrjyyitjc3MzXvnKV8ahQ4f8zUsAAAAAf8cFhTL33ntvRER813d911lff+tb3xr/6l/9q4iIeP3rXx+DwSBuv/322NnZidtuuy3e9KY3lTQLAAAAcLm4oFCm7/v/7fesr6/HPffcE/fcc89FNwUAAABwubvov30JAAAAgIt3UX/70lfDhx89GaNdjTOjLnl4lyxQ0ENERPzvP+D0v7Vc5ossZst0je3pIl1jZydXYzbNn0e/yNeI+Wrck0XBPVkkz6UvGJ9RUWNVnvkCFdd0MMydzGBcsQYM8xXyJWIwyN/YtfX8kn1gf+5vO/yWJ+X/tsR/uDlO17hykh8bu5LjM6JkGo5p8ln7zE5+Hv/EqXm6xqMn8zUOH8/X+ORnttM1jh3dSdfYTl6PxXZ+bV0WrPHn8yn5/32NdIkS2TVpOMkvBmub+Rrru/Jz6Mae/HqyeyNfY2M9fz12Je/L2ji/FmyM8mvSxijfx7hgXRsW7GU/tZ2be7LHR0SMCvZd2QrbJ85/LfFJGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQAOj1g2cy8Er1mKye+2ij58tlukeTk771PGnp4t0D9vT/HlMt+fpGrPTBedyYpqucfpovsbs5Cx1fD/PX4vlLF+jX+TGZ0RE3+drVOi6Lnf8MHd8REQ3yGfUFX3EoOBcKtoYDfN9rOeWmK7gWgzH+fs6nuSvxf4rJukaN16zka7x9Csvfl2NiPi63fltw5WT/D3ZMyp4TtIVIk7M83PoX53IrdF/9sROuoe/+Uy+xmcL1ufTp/P7lempgj3PTn6NXhTs37IGBc/aoGAOHVXMw+v5eXgtuSatV/RQsJ4MC/Yag4qNQoFZwRz62Z3cvv50wfO+XTB3LQquRcFWNibJ5yQiYpJ85rPHR9TshbOvSfNTJ877e31SBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoYNS6gXP5vq/fE7v37r3o40cFcdP6oGt6fETNecyX+RrHC4p8cjtf45GTs3yN4/PU8Ye38j08cTxf48TxabrG6RP5Pmbbi3SNxTQ3NhazfA/9vE/XWC7yY7xf5vuIghp9wbyxmObuS78ouCez/InMd/Lja3o6N+9ERDzx2Z10jT/fnVv2r9y/nu7huisn6RrX7slvX66YDNM1Nkb5dX53ssZ3HtxI9/D8a3ela+SvRE2NWcEUul0w92wn14PjBSdyNLm2RkQ8UTD/feZUvsbWqfwcejI5D5/eyfdwomD/Nyu4r/Pk+lxVY1ZwX5fJ69H3+Wet6/Kz12Ccf+kraCO6gvfX7GQ+3pVf49d2jdM11pN7pvmp839X80kZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA6PWDZzLT77jr2K4vueij9/Yt5buYdfmOHX8VfvzPVx3xSRfY2/uPCIinrw+TNfYM8pngE/ZnT+X65M1tp+8nu7hU9vzdI1PnMjXeOzYNF3j8c/maxx7Yjt1/HbBecyXi3SNftanayxny3SNWORr9PlTidjJHd51+RZmBTVikC/SFdQYDPNz6BOR6+OTBdfzz4cF12KcvxajgnVtuJ7fRmXPZVgwLmp+RFcwOArmncUyP/8tlvlGBslnflgwAQ7zQzy6gj4qavQVi1KyxLKgh4rT6Aqe18lGfnBU1OgK3tfS96XgeV8u8jUW8/zcVdLHNL8fzm6pp8dn6R52tvI1tpLHL7dPnPf3+qQMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACggVHrBs7lH918dYx3773o46eLPt3DfJ6rMV8s0z18/NM76Rp/8cnT6Rqz5LWoMh516Roba8PU8bs3csdHROye5GusFVyLg/sm6RpX7B6na5y4ej11/ONP5J+TTx0+la5x6tPb6RrLScGz1udrdF1+fMUwV2Mwyv/cYH1Pfpnbs38tXWPvZv5Z270r/6ztWstd010Fc9d6wdy1nhxbEREbBX1MCvoYD3I1ClpYGQXbpjhdUOTEND+Hnpjl+jixs0j3cHInfy1Obc/TNXaS1yIivyePiOiXuRrZ48tqFDzzy4I+oqDEsuB5XSbHxrLgnXExzT+vs4JnrS84l5L7mrwnFc9JxYlkt9PL2fmPC5+UAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANDBq3cC5PHr4VIx2DS/6+F3rF3/s562v5WqsT/KZV9d16RqLtT5dY3tnka5x4tQ8XWNra5au8ent3Lks58t0D4tZvsZykb+v0edrDEb5cT5OPmujScHzvnucrrF7cy1dY1lwTyrua0EXMUjOX4NBfv4bF8zDa+P8+Co4lTh5Ypqu8enHc/PwyZP5OfhUwTw+S87jEUVzaIGuYnAk9QVPfMV+ZZhcCyIixhsFNdby2+NRcu4ZFoyLwTBfY1Swxg8LfgQ8KTiXZbKPRX4bG4uKNX6ZrzEqeF7Ha/kbOxpP0jWy47xiBl4U3JPZLF9jOs0P0mnB+jrbzvUx3yl415oW7BOS72vd4vyfEZ+UAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANDBq3cC5PPLhIzFYO3HRx3eDfN40mAxTx4/3jNM9bB7YSNd40tX5GpNx7lpERFy1by1do9ufr7E9XaSOP3Fqlu5h67M76RpHP3E8XWN2LN/Hcpa7nhERy3muRr9YpnuIvs/XKJh3ukFXUGM1+ii5piugX+bPo6JGxKr0kdMNC8ZnRY0uP8YHo/zamN1rdKOCn68VDIuKeXi6yDdyOl0hIgrmv+x9XducpHvYuCK/Z1rfnX9VGI/zNWZ9fnzNkvuV+TQ/PndOTPM1judrTLfyNRbb+f1wn9z/RazKHrKiREGRku1wQR/J9XVQMAdX7Hay++nl9OR5f69PygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoYtW7gK2WxPU3XmJ/OHT/bShaIiO1Pn0rXeOLRtXSNzWt2p2s86eqNdI29e8b5Grtzw34zeXxExNVXrqdrHL9mV7rGkcP58XXiSH6c7xzdTh0/O7GT7mFxKtdDRMRiJ1+jXyzTNaLPl6jQ9yvQSF9wPaMrqFGgy/fRDXI/i8keHxHRDQtqjIfpGqNd+bVxMMn3MdmfWw9GBT1UPKr9Ml+kYs4YDAqe12G+xnCUG+ejtfxzUjE2JpP8nmc4qphDC+a/5By6XMzSPSzm+TVpfnqe72O74FwK3rWWs0W6RlbFujZYyz8ng4pnbVzweYuCvUa6RsWUUbH9y7awc/5rmk/KAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANjFo3cC7XPvNJMdy19+IL9PkelstckcUi38R8tkzXWM7zNeY7i3SNxz6+la6xzLcRffK+9l2+h9Ekn4eu7xqna+wqqLH/pvV0jdEoeT0K7sk8OS4iIqbT/ADdOTXP1ziZrzHbztdYJOeN7PEREfPTs5WosdzJ11hsF/Qxy13TfpFfT/ppfmzFPF9juZOvMT+5k66x89mTqeMHk/xWrhsO0zUG2Xk8IrpBwWTe5WsMCtboYfK+DNfz92RnnB/jJz+7na7RF+yHZwVr43Qrdy7z4/lrMT+dnzMWp6fpGgWPSQzWJuka481d6RqT/Ru5Hnblz6MbFlzQinfXgne+RcHaOEuujfOT+TG+2C6ocfp06vh+duq8v9cnZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA2MWjdwLn/x7o9GN9510ccPJpN0D8Nda6njx3tyx0dErO3fyNfYl+9jOBmmayzmfbrG/NQ8XWM5W+aOny/SPZye5WtsLXLnERHRL/L3JPp8jT5Zo18WnEdFjS5fIob5rHw4ztcYbYzzfaznlpjRRn6JWpX5r+KelNQY5WoMCn6UMygY44OKZ63LF+kqfrSVnHoq5r9lwVqwLOijZC4vULCsxTK5Ri9W5b5W7DUK9n+DUX4enuzJrWv9Yk+6h4q98HKa30P2Bff1f93w7HSNf/7o/52u8ec735A6fufo6XQPy3n+ei6n+Xec+Yntghon0zUWp06kju/n+WvR9xXvSblnrZ+f/9jySRkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANHBBocy9994bz3jGM2JzczM2Nzfj0KFD8d73vvfM729vb8cdd9wRV111VezZsyduv/32OHLkSHnTAAAAAJe6Cwplrrvuunjd614XDz30UHz4wx+O5z//+fGiF70o/vRP/zQiIl796lfHe97znnjnO98Z999/fzz22GPxkpe85CvSOAAAAMCl7IL+vtEXvvCFZ/37v/23/zbuvffeePDBB+O6666Lt7zlLfH2t789nv/850dExFvf+tZ42tOeFg8++GA85znPqesaAAAA4BJ30X+mzGKxiHe84x1x8uTJOHToUDz00EMxm83i1ltvPfM9N910U9xwww3xwAMPnLPOzs5ObG1tnfULAAAA4HJ3waHMn/zJn8SePXtibW0tfvRHfzTe9a53xdOf/vQ4fPhwTCaT2L9//1nff+DAgTh8+PA56919992xb9++M7+uv/76Cz4JAAAAgEvNBYcy3/RN3xR//Md/HB/60Ifix37sx+JlL3tZ/Nmf/dlFN3DXXXfFsWPHzvx69NFHL7oWAAAAwKXigv5MmYiIyWQS3/AN3xARETfffHP80R/9UfzKr/xKfP/3f39Mp9M4evToWZ+WOXLkSBw8ePCc9dbW1mJtbe3COwcAAAC4hF30nynzecvlMnZ2duLmm2+O8Xgc991335nfe/jhh+ORRx6JQ4cOZf8zAAAAAJeVC/qkzF133RUveMEL4oYbbojjx4/H29/+9vj93//9eP/73x/79u2Ll7/85XHnnXfGlVdeGZubm/HKV74yDh065G9eAgAAAPh7LiiUefzxx+Nf/st/GZ/85Cdj37598YxnPCPe//73xz//5/88IiJe//rXx2AwiNtvvz12dnbitttuize96U1fkcYBAAAALmUXFMq85S1v+bK/v76+Hvfcc0/cc889qaYAAAAALnfpP1MGAAAAgAt3wX/70ldNv/jcr4s9fLqTbmE+m+aOP34y3cPOpwpu0SCfvXVdl67Rz+fpGsvFxY+JLzSSPX6Zb6HPNhER+VsSXVcwNobDfI3xOHd8weVcTHPPe0RELAvGxjI/xvvFLF0jKsZo9lkpaKHkPCpUPK/D/HrQjSbte5jknveIiMEk38dgVDB3FayvFWNjJVwu5xERsczPG8tFbv7rk8eX1ZgXrEkVNSrOJbm+9hV70GV+L1yyOBasjXsf+r8K+igYG/3DyeNX4N0iYoX2KwXvBsm1sZvk/2bmwahgr5F8P+lnp+J8EwmflAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQwat3AuXSjcXSj8cUfP8yfWjfo0jXS+j5fY7FMl1gu8zX6khrzfI3FIldgmTw+IvpF/jyi4HqWKBij2QpdV/CsdvmMuhsOC2pc/Lz3ecNdu/I11tfSNUYbuRqj3ZN0D+OCGqM9+Wuxtjd/X9d25WtM1nJjdG09P8bXJ/lnbTjMP/MVy+tsnp+HZ7NcjYLTiNFwNe7JqKDGoGDvVjK+kkNjsSxYWyvW54IBVrFbqVjms6dScEtK5ozFIt/IfJrfy852VqPG9HSuxnw738Oi4DyWs3yNigWh4v13MM6tKYNRfk3K9hARMUzWWGyfiOO/e37f65MyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABkatGziXxekT0c2WF318P5/nm1he/H8/IiK6Lt1CNyjIzValj2G+RjdeT9eYXLE3dfzaVXvSPaxftTtdY21zkq4x2T1O1xiv5e/reDJMHT8cVozxfI3o8yUW8+S8ExHLZb6R+WxR0Efu+H6RP49FRY2d/LWYb+dr7ByfpWuk14OCsbUsGFuLacH4nBc8sH37GhXPe8XcVTGHDkb59WQwzq0nERGjgnVttJHbYo/W8lv08Xr+Wgwr7knBGl3Rx2iU62NS0MN6wXMyLBiffcHYyK7xERF9wRzaZy9pwfxXYVlwLeYF61rJvim5l50X7IUXiRzh82bTXI1uef7PmU/KAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhi1buBcljs70S0SmVHf1zVzsbqudQcREdGN8re5m6yna4x270rX2PgHV+RrPHl36vjx+jDdw3CSr9EVjK+d4zvpGsceneb7+Oyp1PGzE9vpHpY783SNfjbL9zEv6GOe72MVlMxdw/yzViO/JvWLZb7GfJE8Pj+2+kV+jJcomENLxldyv7Kc5efgfpEbFxERUTA+o1+RGpEfG326j4J97CrshYt0XcHPkbPP6zDfQ9cXjK0oGOPLijWpYN5YVtS4PMZ5yeNa8u5Z8azl9m8Va2s3KqgxyF2Lfn76vL/XJ2UAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAZGrRs4l43rro9usvviC/R9uod+mavRz+bpHuanTqZrLE/nayxObhX0cSJdY7aV7+P0326mjt99w1XpHvZen+shImJ99zBdY7K2lq4x3shPI5M949Tx0xO70j1Mj51O19j5bMGzdmwnX6PgmV+ePpWu0c9nuQKL/Bwa/TJfo+vyNQb557UbVPwcJXsuBddiZRSMjYrrkR1fFeOz5L7m912xXOS7mE/zbczz+5WYJ+evgn1syT2pGBvDgvlvOMn3McjtNUr0BWN8mVxbIyIKnpO+4Hktmb5WYV3rVuRZKxjj3Si/p+4iWWO4nu6hJOYYJGtcwN7PJ2UAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANjFo3cC7zE6ejG3cXffxgXHBq3cX/9yMi+vm8oIVcDxERg7X1dI1YLNIl+mW+xnJnO11j+umd1PGzo0+ke9j62O50jdGeXekak82NdI21K/M1xrsnqeMne3LHR0RM9uZr7DqwJ11jfvpJBTVm6RqLghrLaW4OXM4L5p3FciVq1JxLQY3kulTRQ/R9vkaFgvW1G+R/ttUNh7njR/n9Ttflz2O5yO95lqdPpWssTp1I1+hO5fdNy2muj36Wvxb9IrffiYiIZf6+xiy/npTMGl3yWRnm9wndcJyuEQXPazfJ71eGu69K15g8+ep0jfWrc32sPyl/LSb71vI1duXn8vFabj35XI38+BqO2n/uYz7P793m01yNxenj8dHfO7/vbX/FAAAAAL4GCWUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAZGrRs4p+Xic78u0vzkdrqFfrqTOn45neZ72DmVrzEtqLEsOJflPF0j+mW+xjJZo6KHCl2XrzHITwHdcFJQYy13/Dh3fEREN8qfR4yG+T4G43yNYb6Py0bB89pn54yI6GezfI1FvkZ+/uvzPVQY5H+m1K2tp2sMd+1N1xiNd6eOH0zyc8Zgkl8LJmu584iI6Jeb6RoV+7/Z8ZPpGosTW7njjz+R7mG5cyxdI/qL34t/oUbBvNEVPPOjjdTxo31Xp3sYX/nkdI21J+1L11h/cv553bgiP4eu78vv34bD3H54up0f4zvH8+9JW4/kn9dTR3LzTkTEzuOfTddYHP1M6vh+lp/Ha+au3J6pX5z/efikDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIFR6wbOZbl9Krp5f/HHz2f5HnZ2cgXm03QP/TzZQ0T0y4I+FvnrGctlvkZc/Jg4YzROHd4N19ItdCWP3mpcz77P99HPT6eOX85OpHuIxYo8JxU1YpEvMZjka3TDZIGC531ZcC0qdF1BjYqfoxT0sQK6QX4O7db2pmv004J5Y5arMVrsy/fQ59e15XSe72ORf177WUEfyXsSEdHvbOcKFFyLiuekZKsxyK4FEd1kT7rGaP9VueM3r0j3sPakzXyN/bvSNSZ78mv8YJRfk05+JvmcRMT2E7k95OnDW+keZkcLahz7bLrG8vixdI1+kb8n/exUsoeK9878WpDdu/WL83+P90kZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA6PWDZzL9FP/K7rh+kUf3y920j3005O542e54yMi+vmpdI1YLvI1+mW+RoWudQMRJU10w3yJ8e50jcH6Ffk+Jpv5GqPcuXSDgny5y9/XbjTJ1xiP0zUqxmg/zc+hy+1juR52jqd76JfzghrTdI1YFsyhfcFcnh0bFc/JIL/16Ee78n0s8ve1n+Wfk8V29nr06R667YJ5Z5i/r92gYG0sGKODccFcvv+q1PH95r50DyX7v65gfR3lx9egYN6IQXJsLPLz+PZjn07XOPU3FfNO/v2i3654zylYX7Nz4CA/Pivmroo5dLCxN9/HoGBfn92XF8w73bBgPUnek352Kk7+xfl9r0/KAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhi1buBc5p/6k+gG44sv0PcFXWRrFPTQL/M1KnQF+V03LKhRMGQHk1wLw7V8D8PE2P58H8nziIjol4t0jZidyPexOJ06PjVXfKFIukQ/K3hOTs7yfSymBTXyfcQ8d18rziP6gjFeMQ9XLEnRfj2oOY2C+5ocWxERi50n8n0cfyxfY5BbG+cFa1I3zK8nMVzP9zEqWF+T1zMioiuoEV2XLZDvoWTuytco2WtUrGvz7eTx+Xkn20NERCwL9gnLeb6PEvlx3mX3byXvJwXPa8kKW9DHIP+u1SXXg26yO93DYG1/vsb63tTxFzJn+KQMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABpIhTKve93rouu6eNWrXnXma9vb23HHHXfEVVddFXv27Inbb789jhw5ku0TAAAA4LJy0aHMH/3RH8Wv/uqvxjOe8Yyzvv7qV7863vOe98Q73/nOuP/+++Oxxx6Ll7zkJelGAQAAAC4nFxXKnDhxIl760pfGr//6r8cVV1xx5uvHjh2Lt7zlLfHLv/zL8fznPz9uvvnmeOtb3xr/7b/9t3jwwQe/ZK2dnZ3Y2to66xcAAADA5e6iQpk77rgjvvd7vzduvfXWs77+0EMPxWw2O+vrN910U9xwww3xwAMPfMlad999d+zbt+/Mr+uvv/5iWgIAAAC4pFxwKPOOd7wjPvKRj8Tdd9/9Rb93+PDhmEwmsX///rO+fuDAgTh8+PCXrHfXXXfFsWPHzvx69NFHL7QlAAAAgEvO6EK++dFHH42f+ImfiA984AOxvr5e0sDa2lqsra2V1AIAAAC4VFzQJ2UeeuihePzxx+Pbvu3bYjQaxWg0ivvvvz/e+MY3xmg0igMHDsR0Oo2jR4+eddyRI0fi4MGDlX0DAAAAXNIu6JMy/+yf/bP4kz/5k7O+9sM//MNx0003xU/91E/F9ddfH+PxOO677764/fbbIyLi4YcfjkceeSQOHTpU1zUAAADAJe6CQpm9e/fGt3zLt5z1td27d8dVV1115usvf/nL484774wrr7wyNjc345WvfGUcOnQonvOc59R1DQAAAHCJu6BQ5ny8/vWvj8FgELfffnvs7OzEbbfdFm9605uq/zMAAAAAl7R0KPP7v//7Z/37+vp63HPPPXHPPfdkSwMAAABcti74r8QGAAAAIK/8f1+q0i+2I/rFRR/fdQWnNkjWyB4fETEYp0t0g/xfOd6Nd6VrxHCSr9Hlc8QuWaPvl+keYjFLl+iXO/k+5qfyffT5NqJrXqCoRsHFKLmgBWO0oo2sruCeFMwZ0Q1Xoo+uYIz2yRvbVcx/ibX9jOWK9NFP8zWSbfSzk+kWKh73krWxYu6qUDL/rcQkWlCiYt9VMYcW1Bhm9+UV60lBjWHBvn60nq9R8Y4yzPcR41yNwSj/jtONN/I1Jvkag1270zVGu/fka+zN9THanR/jw/WKd+jcHLrcORGffPD8vtcnZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABkatG/hK6ft5vsgiWWORb6FC3w3TNbppxVApyAAH43yNLtlH1+VbiHyN9HlERBSMjRL9Mnl8X9BDvkbfVzz0BedSIXtPIiKWyTm04noWnEdfcU8qrmeJVRhfFfNfxTxcMP8NVmEeruhhNWp0JWt8wX3tCvY82etRMWeUzKGrsZntKzbVs1myiYIelskeIiIq3nEq9jwV88ag/bO2qFiTCs6jG0zyNYbr6Ro7oxXoY7Ai63NyH9svds77e31SBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0MCodQPnMrri6dEN1xIVluke+nSBdIWIrsvXyJ9JxHKRr7GYpUv0y518jXmyxmI738Myfy2iz4/xirHRV4yv7LlUPGt9wRgvmHdKzqVCydyTrXEZXc+KOTQKaqzEs1Yxd+X16fEZq/GclPSwGvquYFs6GOZrdCtQo8v/3LSruJ4FfZRcz8GkoEbuenSjgh663fkaJT9Tr5jLV2TvlazRLyv2GgXvnYuCd5yC94tunh9f2avRVcw7yec9IpJZRES/mJ739/qkDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoIFR6wbOqes+9+tiDcYFLQxzxw8KLu9wki7RDQuuRcH1jK4gA+yX+RL9IldgPs33sNjO15idzteYF/SxyF+P6Oe545fJ4yOiX8zSNWK5k+9jnr+vsSy4J8uC65F91gqe94i+oEaFxHp2pkTBHDrclWthsi/dQrd2RUGNvekag1HuWkRExGgtX2OY2ytUrM8l+5WKGhVjfGUk569lcv6MiH6en8f7RcW6VrAmzU+lSyyz+6aCvVsU7DX6kvW5YH3NrvERJeM8u4fsS/YJ+TW+YJdQdC4VcnN5vyLvndmreSHP6uW0+gEAAABcMoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADo9YNnMty52h0g8lFH9/PT+WbWOzkjl/O8j30y3yNCt0wX2N48ffzC22s5/sYJId9QQ+D8a50jW6SrzFYvzJdIwb5sdEnn5V+ejLfw6ygxvR4ukYk5r0v6Atq5DP7bpCsUXEtBuN0ia5g7orxRrrEYG1/QY09qeO7tbV0D13FepIdWxExGFeMjfw2qu+Tz2uXbiG6gnm8G+avZ8U9GawX1Jjkr8dgmBujg0l+bA3GBdv8gvFVoSsZ6LnD089qRCxn+X39Yjv/frGczlejxs60oI/t1PGL7dPpHvrT+T3kcqdgH7rIX8+KGrHMjY1+sSLv0MtcFtAvprE4z+/1SRkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADo9YNnEs/3YoYjC/6+G5QcGqjXbnjB8N8D12Xr1GRvZX0UVGjz5dYLnPHz0+mW1jsPJGuEf08X6PinnQV00jyvvaLfAvL/PXsK+5Jnx/jJU9axTOfHRvD/H3t1tfyNSZ70jUGe65O1xhfdTBdY7Rnb+r44cYk3cNglF8bu0HF+MzXWM4K5o1Fbk1aTvM9LLZ30jWWp/Nr4+yJfI3l9ql0jX6a76Of5froF7N0DxVbpooVpavYD5e8suQuSMHyHDHMn0c3vPj3o88bTArWxlF+PehG+XNJ39d5/llbTrfTNbJzRkRELPJzefTJ96SIiGVu/9aVvJ8UvP9ms4Du/Oc+n5QBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0MGrdwLn0x/48+m6YKLAsaGLR9viIiKg4j3yJEl1XUaSgRLLGYJLvYbiWLtF1BX0M8n3EoGIayd7XFRnkXUHO3RVcz9FGvo1hQY3J7tTxg90H0z2Mn3R9usbkydeka6xduTddY7wn/7yOdiXnjYppfJl/Xhez/Pp6+sixdI3tTz6erjH91CdSxy+3/jbdw/L0p9I1+p2j6RoxP5GvsdhJl+iX03wf2f1bX7GuVdQoeOgHif3857voxgV9JOfQgr1bDNfTJbqCPhYVe42KfX3BHrLLvC8W6SveO5fzgkYKaiwL3l9L3oGTSvbkuTHeL2fn/b0+KQMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANDBq3cA5DcYR3bB1FyugK6jRF5RY5mtU9FHgmXtyWeT/72RFlpm/r30/y7cx38nXKLmvuWvadQX3pBvna4z3FrRRUGP9inSNwdr+fB+TXc17GKztTtfol/kxPjuRf9a6Ufs1cTlbpGtMj2+na8y3TuVrnDierjH77OF0jeXWY7njT+Z76Lcfz9eYHUvXiEV+bMRynq8RBXueLrnOV6xJg9Wo0XWTfB/D9eY1uuFavodBxatXxZ6n4P2i4D2ti/brWl8wZ/Sz/HoSs5P5Gov82tgvC94N0tc0v9dYCf35n4dPygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoYtW7gnPplRHSZAgU9ZGtU9LDM14iCGv1iNWoUeMPX701WWI3zqHB4mh+j/++/WM83Mhgnj1/L9zDK1+gq+ljO0iX6059O11ic/lS+j+y5LPPP2s7HC+a/QX6p7Cb70zWGe67N19h3Xer40RVPTvcw3rcvX+OKPekaG9dcka7RDZ6SrtEvbkkdv1zkx/hyZ56usdjeyfcxneb7OH16Jfro57ka/c52uoflLF+j3zmVrzE7ka8xLziX7Jq0KFifF/nnJOb5Md7PThb0UTA2FgXnskxe0z4//5W8811Osu/Q6XfwiJr33+x5nP8+1idlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADXR93/etm/i7tra2Yt++fRFr10d0mcxokW9mOc8d3yePj4jol/kasVK3OOXk//GtrVuIrXl+bJ0qqPHIie10jVXx3f+jYpyvgIpHresKSozzfQxG+Rpdssag4DyyPUSUXItuRfrI1yj4Wc4wf1+74Ua+xnhvvsZa+xqDtd35HkaTdI2KuatfrMaepxsWtJG8Hl1qD1ynn03TNZbT/H6l3zlRUGMrdfxyWtHDEytRIxYnC2rM8jUK3pX6SM4bq/KuVfJKXnEu+bk8vVcoWE9yOcLnJe9Jv4jY+h9x7Nix2Nzc/LLfuhozPgAAAMDXGKEMAAAAQANCGQAAAIAGhDIAAAAADVxQKPOzP/uz0XXdWb9uuummM7+/vb0dd9xxR1x11VWxZ8+euP322+PIkSPlTQMAAABc6i74kzLf/M3fHJ/85CfP/PqDP/iDM7/36le/Ot7znvfEO9/5zrj//vvjsccei5e85CWlDQMAAABcDi7478YcjUZx8ODBL/r6sWPH4i1veUu8/e1vj+c///kREfHWt741nva0p8WDDz4Yz3nOc/LdAgAAAFwmLviTMh/72Mfi2muvja//+q+Pl770pfHII49ERMRDDz0Us9ksbr311jPfe9NNN8UNN9wQDzzwwDnr7ezsxNbW1lm/AAAAAC53FxTK3HLLLfG2t70t3ve+98W9994bH//4x+M7v/M74/jx43H48OGYTCaxf//+s445cOBAHD58+Jw177777ti3b9+ZX9dff/1FnQgAAADApeSC/velF7zgBWf++RnPeEbccsst8ZSnPCV++7d/OzY2Ni6qgbvuuivuvPPOM/++tbUlmAEAAAAue6m/Env//v3xjd/4jfGXf/mXcfDgwZhOp3H06NGzvufIkSNf8s+g+by1tbXY3Nw86xcAAADA5S4Vypw4cSL+6q/+Kq655pq4+eabYzwex3333Xfm9x9++OF45JFH4tChQ+lGAQAAAC4nF/S/L/2bf/Nv4oUvfGE85SlPicceeyxe+9rXxnA4jB/8wR+Mffv2xctf/vK4884748orr4zNzc145StfGYcOHfI3LwEAAAD8PRcUynziE5+IH/zBH4zPfOYz8eQnPzme97znxYMPPhhPfvKTIyLi9a9/fQwGg7j99ttjZ2cnbrvttnjTm970FWkcAAAA4FJ2QaHMO97xji/7++vr63HPPffEPffck2oKAAAA4HKX+jNlAAAAALg4F/RJma+qxYmIrnFm1PfJAsuSNlZDl65w8v/41oI+Lg+7RsN0jZv27y7oZEXMHksd/ns37ytqJOe7P/yZgir5Z63vCqb2QUGNLjnO+4I5dDkvqLGTLtEvp/k++llBjUW+Rlp+jJfsD7pxvsYgP5dHVNRI6gruScV9XZmxsQI/s1yZMT7JtzHale9juCffxyT3N7x24/y+q+JadKONdI1+WbCeVKyv85P5GtufSh3ez4/ne1gWrK0V83B231Um+Q5dMP91FXvh5Bza9+f/jKzAqgMAAADwtUcoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0MGrdwDktlxFdnylQ0ETmv79KuoIaq5Hf/cXWqXSNb9zcVdAJlT75z65NHf8/j54s6iRpsFFQo2BarqjRjfMlsjUKpq5+OcsXWWzna8wLxugiP/9Fn13XFvkeKmosK9bnab5ExVYjvb4WrM/diuwTumFBjYr9SsH1KLmmqyB/PftpxX3N1+iHu3MtrB9M99CtX5GuEYNJvkbFszYoqDHZly7Rjffkjl/k9wn97ES+xs6n0jVK9gnLeb5GdnFc5OfPmrf4ZJX+/K/DarxpAwAAAHyNEcoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA2MWjdwbvOI6C7+8H5Z0EOyRt8X9FAgcRm/YDXyu2/c3NW6hZXxP4+ebN1CmZv2727dQonf+7b8+Pzuj5zIN7Kc5msUTBx9l503CiavrmICLJjLh2v5GunrGRHLndzxi1Pte4iI6Bf5GhVWZJlfCRXjs+SZr+hjFc5lVea/Vemj4J4k569+kd939Tt70jW68Wa6Rqxdme9jlN+7dYNJukbfDXMFBvmJvButp2tEXJUvsSh4T1rO0yX67Do/L9hr9LN8jey1uIA8YjXetAEAAAC+xghlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGRq0bOKfxlRFdIjPqZ/ke+nny+EVBDxU1lvkaUVGDz/ufR0+2bqHMTft3t25hZXz3Hx4pqFLwrJU8831BjaxV6CEi+oI+ui5fo0LFuWQNJvkaJWtjxT5hFdbGVRlbq3AtomZscHnKvFdERMxP5HuYPZEu0U93FfRxLN/HcCNdoxsW7CHHe3I9jPLnEeP8eXQF1zPmp9Il+uln830skn0MhgU9FKwF6b3b+R/vkzIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQAOj1g2c0+xoRNdd/PFdwakNxsnj1/M9dMN8jUhcR74ibtq/u3ULZd57+Gi6xv/nI3+VOv73/vFN6R6++789nK5R8qxl5r0zKvL2fkVqZFX0UHBP+svlZyAV47PiOSlYG0vW1xXQXy7PaoQ+/q6CHkrGxjJfouRyVlyP5Ll0BT0sV2FsRcS0YB4ezfI1KsZoci7vu/z63GXHVkTEcp4u0c+O52vMT6ZrxHKaPL5gbC23C2okz+MCxsXlsksEAAAAuKQIZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABoQyAAAAAA0IZQAAAAAaEMoAAAAANCCUAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADQhkAAACABrq+7/vWTfxdW1tbsW/fvojRkyK6TGZUcVrJGqn+P2+YL1HRR8m5dOkKJ/9fX5dvI2lrvmjdQkRE/M+jJ1u3EBER3/2RrYIq2bGRH1s1NSpU9LFS03pCwXlULHF9xTNfUKOfF9RYXvo9VNUosQrP2oo8JyVjvOK+VtRYhfWgoIduVdbGVbieUXA9VuVaVIyNiveLUb7GYC1fY7ieO360O91CN9yTrhGTzXwfBecSw/w96QbZ8bUic0ZyTeqX01g++ltx7Nix2Nz88vfXJ2UAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANjFo3cE6L4xFdlyiQObZKQQ/dMF9jJa5Fjd3v+e+tW4jfO/QPW7cQEREv/+O/Sdf461Pb+UZKZPPhvqSLvIpnrWLeWJE+sjUul/OIuIzm8ooeCn4eVDI2VkV2/qqY/ypqVNyTVTmXAunLsSpjvOLntxVzaL5E9KvwrFUo6KOfFdSY52ssT+drzJPXYyc/uPoVeU76kmljFebyFdm7Zc/jAuYcn5QBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0MGrdwLl1/8+viz28Im9KXp7BON9CV3CLumFBjYrrmbiflTW6XI3vfmgr30OFfn++xqTP1yiR7WNVzqNijBc8a8kxXid5X/plQQsVNear0UdUnMsqPGurcB6XkYrnvWTOqJi78iVKhmjJ9cjWWJF5fFXuSYVuFRpZhR4iSvpYlVNZiUZWZF0rWRsr9itJJeexAuPiAs7DJ2UAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANjFo3cE7j/RFdIjPqF/kesjX6ZUEPO/ka0RWUKMjvumG+RsW5RJ88PHl8RERXcR4V93VF+iipkVVxPfMlIirmjXyJkj6W2Tm0YB6PirWg4p4U1Ki4J9kaFfNfyQAtqFHynKyAknm8Yo2v+DlfRR+Xy5q0IgN0RdpYDSuwB41YoXm4wqr0kVUxd63KtSg4l+yep2QKXoXnpD/vEj4pAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaGDUuoFzWmxHdF2iQJ9u4Zs/+H+mjt/5m8+me/jLl70vXSO6Yb5GZO7FqkmeS2pcFtYoUdHHClyPPv+8RywKShTU6OcFNQr6iGW+RPa+VAzPkrFRcC0K1qSSc0nPPQU/yymZ/yr6qPi5VMX8l1yjS86jYJ9QsjZeRvuVPjtvVMzBFXPXCqwFnytSUCPbQsXaeplcizKrcC4lm42CGisyd5Wsa8l1qeJZW4X57wLmPp+UAQAAAGhAKAMAAADQgFAGAAAAoAGhDAAAAEADFxzK/O3f/m380A/9UFx11VWxsbER3/qt3xof/vCHz/x+3/fxmte8Jq655prY2NiIW2+9NT72sY+VNg0AAABwqbugUOaJJ56I5z73uTEej+O9731v/Nmf/Vn80i/9UlxxxRVnvucXf/EX441vfGO8+c1vjg996EOxe/fuuO2222J7e7u8eQAAAIBL1QX9ldj/7t/9u7j++uvjrW9965mv3XjjjWf+ue/7eMMb3hA//dM/HS960YsiIuI3f/M348CBA/Hud787fuAHfuCLau7s7MTOzs6Zf9/a2rrgkwAAAAC41FzQJ2V+53d+J5797GfH933f98XVV18dz3rWs+LXf/3Xz/z+xz/+8Th8+HDceuutZ762b9++uOWWW+KBBx74kjXvvvvu2Ldv35lf119//UWeCgAAAMCl44JCmb/+67+Oe++9N5761KfG+9///vixH/ux+PEf//H4jd/4jYiIOHz4cEREHDhw4KzjDhw4cOb3/r677rorjh07dubXo48+ejHnAQAAAHBJuaD/fWm5XMazn/3s+IVf+IWIiHjWs54VH/3oR+PNb35zvOxlL7uoBtbW1mJtbe2ijgUAAAC4VF3QJ2WuueaaePrTn37W1572tKfFI488EhERBw8ejIiII0eOnPU9R44cOfN7AAAAAFxgKPPc5z43Hn744bO+9hd/8RfxlKc8JSI+94f+Hjx4MO67774zv7+1tRUf+tCH4tChQwXtAgAAAFweLuh/X3r1q18d//gf/+P4hV/4hfgX/+JfxB/+4R/Gr/3ar8Wv/dqvRURE13Xxqle9Kn7+538+nvrUp8aNN94YP/MzPxPXXnttvPjFL/5K9A8AAABwSbqgUObbv/3b413velfcdddd8XM/93Nx4403xhve8IZ46UtfeuZ7fvInfzJOnjwZP/IjPxJHjx6N5z3vefG+970v1tfXy5sHAAAAuFR1fd/3rZv4u7a2tmLfvn0Rw/0RXZeolD+tb/7g/5k6fudvPpvu4S9f9r50jeiG+RqRuReVVqWPpNTYrlTRR0GN7PUomcaWBSUW+Rr9vKBGQR8V1yN7XyqG56qMjYI1qeRc0nPPBf1fz+foIV+ipkjBuZTMf8k1uqs4j4J9QsW6djntV/rsvFExB1fMXSuwFnyuSEGNbAsrshaswrUoswrnsiJzxsr0sQIq9rGrsBfu+4iYxbFjx2Jzc/PLfusFfVLmq2q5k1rgv/kPXp5uYf7ZU+kaaYvTBUVWZAEoWcwqrMICcDmp2IhnXyoqXigqXmw4S3porMLYiihZKkuC2FW4HgXP2mBcUKPg07eDii3QCqyNFUHucroaNRbb+RolIWqF5PNaEVBVjPFukq9REcqswktayXqyIgF9iYo+LpO91+X0w9aV2K8U7BNWIQzu+4jF0fP61svkSQAAAAC4tAhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGRq0bOKd+HhHdRR/+p899c0UTqaO/4f/+/nwLy2m+xsrIXc/PufgxsVoqrsVlpF8kC8xK2uDvqnjWkjVKHvcVOI8qXUUfyZ/FdMN8C/28oEZ2zoiIfpKv0RVso7LXtOKeVIzxij4GBWtjyfha5mtEskbFGF/s5GuszBxa8HPkLjv/Vbw2VTxrFc9JxRxa8JyUrGtJfcX1rJgzKt4NKs5lFd5RVmBcRES6jwu4lj4pAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACgAaEMAAAAQANCGQAAAIAGhDIAAAAADQhlAAAAABoQygAAAAA0IJQBAAAAaKDr+75v3cTftbW1Ffv27Yu44jnRdaOLL7Scp3vpl6dzBRbb6R5isZOvsSzoY1nQRz8rqLEoqLHMFsj3UGJF+qhooyuokVbRREGNriAr74YFNcYFNRJzeETNtaj42UPJ9SyoMSi4J4NJ7viS8yioEcmxFRHdIF8jfT0jCp6TgvPoVmISjpIFpWJbW7LXyPaxCvudohol9zXfR5+9piXXcxXGVsTK7CFL9l7J+7IseD8peV+rqDHN1+jz79ARyRqrMsazz3zfR8Q0jh07Fpubm1/2W31SBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoQCgDAAAA0IBQBgAAAKABoQwAAABAA0IZAAAAgAaEMgAAAAANCGUAAAAAGhDKAAAAADQglAEAAABoYNS6gb+v7/v/5x/m0acKzQuaWbQ9PiKiX65IjdTduMxqFPRQYlX6KHAZnUpexcVYlRoFc89K6FajRslcXrAupXuoKJK/nhXLSSwLfraVPZWS4VlRpMIqrPFRtH/L9lHxrK7IHrLkvlb0kayxKnNwyeS1Khuvirkne18vp/e1FXnny46vVeghouydsT+POisXyhw/fvxz/3D0w20bAWhhRdYhAAAg5/jx47Fv374v+z1dfz7RzVfRcrmMxx57LPbu3RvdOX5ys7W1Fddff308+uijsbm5+VXuEL4845NVZ4yyyoxPVp0xyiozPlllX0vjs+/7OH78eFx77bUxGHz5T9au3CdlBoNBXHfddef1vZubm5f9zeTSZXyy6oxRVpnxyaozRlllxier7GtlfP7vPiHzef6gXwAAAIAGhDIAAAAADVySocza2lq89rWvjbW1tdatwBcxPll1xiirzPhk1RmjrDLjk1VmfH5pK/cH/QIAAAB8LbgkPykDAAAAcKkTygAAAAA0IJQBAAAAaEAoAwAAANCAUAYAAACggUsulLnnnnvi677u62J9fT1uueWW+MM//MPWLfE16oMf/GC88IUvjGuvvTa6rot3v/vdZ/1+3/fxmte8Jq655prY2NiIW2+9NT72sY+1aZavOXfffXd8+7d/e+zduzeuvvrqePGLXxwPP/zwWd+zvb0dd9xxR1x11VWxZ8+euP322+PIkSONOuZryb333hvPeMYzYnNzMzY3N+PQoUPx3ve+98zvG5uskte97nXRdV286lWvOvM1Y5SWfvZnfza6rjvr10033XTm941PVsHf/u3fxg/90A/FVVddFRsbG/Gt3/qt8eEPf/jM73tX+oJLKpT5rd/6rbjzzjvjta99bXzkIx+JZz7zmXHbbbfF448/3ro1vgadPHkynvnMZ8Y999zzJX//F3/xF+ONb3xjvPnNb44PfehDsXv37rjttttie3v7q9wpX4vuv//+uOOOO+LBBx+MD3zgAzGbzeJ7vud74uTJk2e+59WvfnW85z3viXe+851x//33x2OPPRYveclLGnbN14rrrrsuXve618VDDz0UH/7wh+P5z39+vOhFL4o//dM/jQhjk9XxR3/0R/Grv/qr8YxnPOOsrxujtPbN3/zN8clPfvLMrz/4gz8483vGJ6098cQT8dznPjfG43G8973vjT/7sz+LX/qlX4orrrjizPd4V/o7+kvId3zHd/R33HHHmX9fLBb9tdde2999990Nu4K+j4j+Xe9615l/Xy6X/cGDB/t//+///ZmvHT16tF9bW+v/03/6Tw065Gvd448/3kdEf//99/d9/7nxOB6P+3e+851nvufP//zP+4joH3jggVZt8jXsiiuu6P/Df/gPxiYr4/jx4/1Tn/rU/gMf+ED/T//pP+1/4id+ou978yftvfa1r+2f+cxnfsnfMz5ZBT/1Uz/VP+95zzvn73tXOtsl80mZ6XQaDz30UNx6661nvjYYDOLWW2+NBx54oGFn8MU+/vGPx+HDh88ar/v27YtbbrnFeKWJY8eORUTElVdeGRERDz30UMxms7PG6E033RQ33HCDMcpX1WKxiHe84x1x8uTJOHTokLHJyrjjjjvie7/3e88aixHmT1bDxz72sbj22mvj67/+6+OlL31pPPLIIxFhfLIafud3fiee/exnx/d93/fF1VdfHc961rPi13/918/8vnels10yocynP/3pWCwWceDAgbO+fuDAgTh8+HCjruBL+/yYNF5ZBcvlMl71qlfFc5/73PiWb/mWiPjcGJ1MJrF///6zvtcY5avlT/7kT2LPnj2xtrYWP/qjPxrvete74ulPf7qxyUp4xzveER/5yEfi7rvv/qLfM0Zp7ZZbbom3ve1t8b73vS/uvffe+PjHPx7f+Z3fGcePHzc+WQl//dd/Hffee2889alPjfe///3xYz/2Y/HjP/7j8Ru/8RsR4V3p7xu1bgCAr6w77rgjPvrRj571/5tDa9/0Td8Uf/zHfxzHjh2L//yf/3O87GUvi/vvv791WxCPPvpo/MRP/ER84AMfiPX19dbtwBd5wQtecOafn/GMZ8Qtt9wST3nKU+K3f/u3Y2Njo2Fn8DnL5TKe/exnxy/8wi9ERMSznvWs+OhHPxpvfvOb42Uve1nj7lbPJfNJmSc96UkxHA6/6E8OP3LkSBw8eLBRV/ClfX5MGq+09opXvCJ+93d/N37v934vrrvuujNfP3jwYEyn0zh69OhZ32+M8tUymUziG77hG+Lmm2+Ou+++O575zGfGr/zKrxibNPfQQw/F448/Ht/2bd8Wo9EoRqNR3H///fHGN74xRqNRHDhwwBhlpezfvz++8Ru/Mf7/7d0/SPJbHMfxT0Q/qTDJiLJAcQgq2hTJGp0aglqSaJBq6w+UtDRISzQHbS5trhFNBVZjNQlNhhE01F5GtPi9w+V6H5/nWe5yjz2+X/ADf+c4fIcPB84Xzu9UKhXWUDSFUCik8fHxhrGxsbH6MTv2So2+TVPG8zzFYjEVi8X6WK1WU7FYVDKZdFgZ8KtoNKrBwcGGvL69ven29pa84n9hZtrY2NDJyYkuLy8VjUYb5mOxmDo6OhoyWi6X9fz8TEbhRK1W09fXF9mEc6lUSvf39yqVSvUnHo9raWmp/puMoplUq1U9Pj4qFAqxhqIpTE9Pq1wuN4w9PDwoEolIYq/0s291fCmbzSqTySgejyuRSOjw8FAfHx9aXl52XRpaULVaVaVSqb8/PT2pVCopGAwqHA5ra2tL+/v7GhkZUTQaVS6X09DQkObm5twVjZaxvr6uQqGg09NT+f3++vncQCCgzs5OBQIBra6uKpvNKhgMqqenR5ubm0omk5qcnHRcPf50u7u7mpmZUTgc1vv7uwqFgq6vr3V+fk424Zzf769/f+sf3d3d6uvrq4+TUbi0s7Oj2dlZRSIRvby8aG9vT+3t7VpcXGQNRVPY3t7W1NSUDg4OtLCwoLu7O+XzeeXzeUlSW1sbe6Ufub7+6b86OjqycDhsnudZIpGwm5sb1yWhRV1dXZmkX55MJmNmf1/1lsvlbGBgwHw+n6VSKSuXy26LRsv4XTYl2fHxcf0/n5+ftra2Zr29vdbV1WXz8/P2+vrqrmi0jJWVFYtEIuZ5nvX391sqlbKLi4v6PNlEs/nxSmwzMgq30um0hUIh8zzPhoeHLZ1OW6VSqc+TTzSDs7Mzm5iYMJ/PZ6Ojo5bP5xvm2Sv9q83MzFE/CAAAAAAAoGV9m2/KAAAAAAAA/EloygAAAAAAADhAUwYAAAAAAMABmjIAAAAAAAAO0JQBAAAAAABwgKYMAAAAAACAAzRlAAAAAAAAHKApAwAAAAAA4ABNGQAAAAAAAAdoygAAAAAAADhAUwYAAAAAAMCBvwBBE7m3iyT2HgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "###save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "batch=sx.size(dim=0)\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone())\n",
        "print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e2)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 20\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uT9m-J1BUWyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744,
          "referenced_widgets": [
            "435e5a6dd006421491107940a9577446",
            "8b9ff992b1fd456fac50c642ea3f8125",
            "d9db98f8adec4450b4b8bcad76f263bd",
            "4ab52801b4ec4db58bfd550eb20b707f",
            "852cc0b8546a47a0ae8737224a2f9865",
            "618717fe7bfa4850932dd9d5b9888e27",
            "c39182f9581146d0a0b181d756e3b278",
            "f83922e7737c4035b386b6c03bde2956"
          ]
        },
        "outputId": "db93090c-53c4-42bf-f715-f8c2976ed00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.4)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.9.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:bwqavbh4) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "435e5a6dd006421491107940a9577446"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▃▂▅▅▃▂▄▄▅▅▃▂▃█▆▂▁▆▇▄▅▄▂▄▆▅▅▄▆▄▄▆▄▅▂▅▃▄▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.33328</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">swept-water-7</strong> at: <a href='https://wandb.ai/bobdole/procgen/runs/bwqavbh4' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/bwqavbh4</a><br/> View project at: <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240713_081010-bwqavbh4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:bwqavbh4). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240713_081329-wcpfe1b3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/wcpfe1b3' target=\"_blank\">vital-vortex-8</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/wcpfe1b3' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/wcpfe1b3</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-nT5j864BIn",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbefcd99-bb26-4902-8d65-f1cf6d75abc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "### trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wUhKd009Qvk3"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "435e5a6dd006421491107940a9577446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b9ff992b1fd456fac50c642ea3f8125",
              "IPY_MODEL_d9db98f8adec4450b4b8bcad76f263bd"
            ],
            "layout": "IPY_MODEL_4ab52801b4ec4db58bfd550eb20b707f"
          }
        },
        "8b9ff992b1fd456fac50c642ea3f8125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_852cc0b8546a47a0ae8737224a2f9865",
            "placeholder": "​",
            "style": "IPY_MODEL_618717fe7bfa4850932dd9d5b9888e27",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "d9db98f8adec4450b4b8bcad76f263bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c39182f9581146d0a0b181d756e3b278",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f83922e7737c4035b386b6c03bde2956",
            "value": 1
          }
        },
        "4ab52801b4ec4db58bfd550eb20b707f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852cc0b8546a47a0ae8737224a2f9865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618717fe7bfa4850932dd9d5b9888e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c39182f9581146d0a0b181d756e3b278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83922e7737c4035b386b6c03bde2956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}