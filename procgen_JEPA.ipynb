{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "c0a083d9-29b9-48fa-ba06-be87827d1c18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Jx0k_ndHOEMe",
        "outputId": "b5fb2840-eeb5-45ae-f0e0-e2a15f8e7ccc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-05466a22e258>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# visualise(agent.sense,layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mvisualise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N2TGs69fnrZo"
      },
      "outputs": [],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "for name, param in agent.emb.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred[0].weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "73b9f10c-2c8d-4926-dfdb-bad316d5a362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "2f792ab8-9efa-4519-b709-ed42e3c5b3b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcOidvtW9KAH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "986433c2-c70c-488d-ccb0-5aeb67659fb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-2ce37b725f67>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "037c1ed0-a03b-4656-9108-083a4df4dae9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-40-2b7c3113812e>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state, h0=None): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6, h0=h0) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, sx_, hn = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact, h0#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9, h0=None): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                out, h0 = self.jepa.pred(sxaz, h0)\n",
        "                out = out[:, -1, :]\n",
        "                sx = sx + out\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx, h0\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.randn((self.jepa.pred.num_layers, batch_size, self.d_model), device=device)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0)\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    # _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    # stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "496d97fd-d6f8-4881-c198-4b90fd33310b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1HRwU4u7Y6YjQWmC8xlKrDe4YGcNUWxvQ\n",
            "From (redirected): https://drive.google.com/uc?id=1HRwU4u7Y6YjQWmC8xlKrDe4YGcNUWxvQ&confirm=t&uuid=5a89569a-d401-47cf-9b58-bc0cde7bc2a1\n",
            "To: /content/buffer512.pkl\n",
            "100% 706M/706M [00:11<00:00, 60.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3 convenc4\n",
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "import pickle\n",
        "# !gdown 19VQp7UjXqH8kJjEPABOTHDV8reg8r7Zn -O buffer512down.pkl # B\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "!gdown 1HRwU4u7Y6YjQWmC8xlKrDe4YGcNUWxvQ -O buffer512.pkl # B\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "5b46fd13-de96-4fd5-fa75-048926e33807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "def save(folder, name='agent.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    # agent.mem.save(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer512.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# save(folder, name='agent_jepa753333256.pth')\n",
        "# buffer = load(folder)\n",
        "# save('/content/')\n",
        "# buffer = load('/content/')\n",
        "\n",
        "# name='agent.pth'\n",
        "# print(folder+name)\n",
        "# torch.load(folder+name, map_location='o')\n",
        "# with open(folder+'buffer512down.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "# with open(folder+'buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# # modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG4Wn3c8IN4V"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = buffer[7][80][0]\n",
        "state = transform(state).unsqueeze(0).to(device)#[0]\n",
        "act = agent(state).cpu()[:1].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 25 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "1e3fpbtNOiz1"
      },
      "outputs": [],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "train_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "train_data = Datasetme(train_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    train_data=list(zip(state,reward))\n",
        "    train_data = Datasetme(train_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range((len(labels)//10)+1):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# try:\n",
        "with torch.no_grad():\n",
        "    pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    # print(pred)\n",
        "    for x in range((len(pred)//10)+1):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(F.mse_loss(labels, pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OksdjCeJYpYh",
        "outputId": "f3703e7a-7b4b-4c97-e740-01591fd6b104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-40-2b7c3113812e>:156: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "argm 1.5085809230804443 0.2995770573616028\n",
            "argm 1.4234641790390015 0.2761803865432739\n",
            "argm 1.3142964839935303 0.2948407530784607\n",
            "repr, std, cov, closslb 23.512353897094727 0.488525390625 8.230144158005714e-05 0.0009204460075125098 0.23895592987537384\n",
            "0.08627813139340396 0.07232505819253308 1.0\n",
            "argm 1.5318599939346313 0.1864749938249588\n",
            "argm 1.5663204193115234 0.1376895308494568\n",
            "argm 1.6223618984222412 0.2695368528366089\n",
            "argm 1.696702241897583 0.08017849177122116\n",
            "argm 1.642984390258789 0.1194532960653305\n",
            "argm 1.3619887828826904 0.14467445015907288\n",
            "argm 1.1149694919586182 0.24104398488998413\n",
            "argm 1.0240864753723145 0.3495253622531891\n",
            "argm 1.0518027544021606 0.2808384597301483\n",
            "argm 1.0767115354537964 0.11983048170804977\n",
            "argm 1.097961664199829 0.18154922127723694\n",
            "argm 1.1131670475006104 0.2190442681312561\n",
            "argm 1.1379637718200684 0.14307630062103271\n",
            "argm 1.1956617832183838 0.141433447599411\n",
            "argm 1.423233985900879 0.6335797309875488\n",
            "argm 1.672587513923645 0.1425454318523407\n",
            "argm 1.741571068763733 0.25716519355773926\n",
            "argm 1.7748740911483765 0.16022565960884094\n",
            "argm 1.7864434719085693 0.29906630516052246\n",
            "argm 1.7578208446502686 0.24980026483535767\n",
            "argm 1.6820095777511597 0.2071598768234253\n",
            "argm 1.6243184804916382 0.14174076914787292\n",
            "argm 1.5324422121047974 0.20285043120384216\n",
            "argm 1.4091622829437256 0.18114131689071655\n",
            "argm 1.2553107738494873 0.3001319468021393\n",
            "repr, std, cov, closslb 21.39047622680664 0.487548828125 0.00013668439351022243 0.0162995345890522 0.23391065001487732\n",
            "0.08653722470825645 0.07415504943120524 1.0\n",
            "argm 1.492266297340393 0.10603386908769608\n",
            "argm 1.519132375717163 0.1635448932647705\n",
            "argm 1.5828789472579956 0.2916874289512634\n",
            "argm 1.6505773067474365 0.10328739881515503\n",
            "argm 1.5951464176177979 0.09369291365146637\n",
            "argm 1.2996253967285156 0.14854130148887634\n",
            "argm 1.0643935203552246 0.23152492940425873\n",
            "argm 1.013200283050537 0.38369840383529663\n",
            "argm 1.0527325868606567 0.16826391220092773\n",
            "argm 1.0723066329956055 0.08000805974006653\n",
            "argm 1.102092981338501 0.2751583158969879\n",
            "argm 1.1214964389801025 0.0984901562333107\n",
            "argm 1.161108136177063 0.09551458060741425\n",
            "argm 1.2487682104110718 0.1847958266735077\n",
            "argm 1.5273585319519043 0.40308013558387756\n",
            "argm 1.6774758100509644 0.16470161080360413\n",
            "argm 1.6983416080474854 0.26588791608810425\n",
            "argm 1.7371925115585327 0.19723457098007202\n",
            "argm 1.7511489391326904 0.27243420481681824\n",
            "argm 1.7026851177215576 0.18443259596824646\n",
            "argm 1.63759446144104 0.15062279999256134\n",
            "argm 1.5620174407958984 0.13073647022247314\n",
            "argm 1.4294061660766602 0.21804404258728027\n",
            "argm 1.2092489004135132 0.31532999873161316\n",
            "argm 1.0343568325042725 0.25024449825286865\n",
            "repr, std, cov, closslb 19.208086013793945 0.488525390625 6.967666558921337e-05 0.00045086408499628305 0.25512731075286865\n",
            "0.08679709608059254 0.07603134368044262 1.0\n",
            "argm 1.4720063209533691 0.07108412683010101\n",
            "argm 1.4972400665283203 0.19998103380203247\n",
            "argm 1.5544414520263672 0.3028185963630676\n",
            "argm 1.6284847259521484 0.13268274068832397\n",
            "argm 1.5654983520507812 0.08546188473701477\n",
            "argm 1.261683464050293 0.11004600673913956\n",
            "argm 1.0456831455230713 0.237389475107193\n",
            "argm 1.0266505479812622 0.37321263551712036\n",
            "argm 1.061103105545044 0.1443084329366684\n",
            "argm 1.0793023109436035 0.1011500135064125\n",
            "argm 1.1105132102966309 0.2574557363986969\n",
            "argm 1.1383092403411865 0.12982729077339172\n",
            "argm 1.178247094154358 0.08478295058012009\n",
            "argm 1.3566399812698364 0.2961522042751312\n",
            "argm 1.6124168634414673 0.20771193504333496\n",
            "argm 1.674178123474121 0.22569391131401062\n",
            "argm 1.6922664642333984 0.25328436493873596\n",
            "argm 1.7008241415023804 0.2697240710258484\n",
            "argm 1.6830947399139404 0.14131329953670502\n",
            "argm 1.6509947776794434 0.1960359513759613\n",
            "argm 1.6047941446304321 0.14284998178482056\n",
            "argm 1.4606027603149414 0.18577370047569275\n",
            "argm 1.2307907342910767 0.2323715090751648\n",
            "argm 1.0214465856552124 0.22119003534317017\n",
            "argm 0.9806513786315918 0.15022605657577515\n",
            "repr, std, cov, closslb 16.967954635620117 0.48828125 7.067574188113213e-05 0.01806728169322014 0.24442507326602936\n",
            "0.08714480559476655 0.07795511251349761 1.0\n",
            "argm 1.4634767770767212 0.06808899343013763\n",
            "argm 1.4759292602539062 0.1708008348941803\n",
            "argm 1.5347895622253418 0.33126676082611084\n",
            "argm 1.613877773284912 0.14260411262512207\n",
            "argm 1.5325568914413452 0.0831233337521553\n",
            "argm 1.226668119430542 0.12396275997161865\n",
            "argm 1.0427978038787842 0.23612380027770996\n",
            "argm 1.0462861061096191 0.32600295543670654\n",
            "argm 1.0674011707305908 0.056432560086250305\n",
            "argm 1.0946141481399536 0.1293439418077469\n",
            "argm 1.1378870010375977 0.18297694623470306\n",
            "argm 1.1786128282546997 0.16886740922927856\n",
            "argm 1.245269775390625 0.13247039914131165\n",
            "argm 1.4915053844451904 0.21223537623882294\n",
            "argm 1.6571775674819946 0.12151524424552917\n",
            "argm 1.6433470249176025 0.2124197781085968\n",
            "argm 1.6369346380233765 0.2054348587989807\n",
            "argm 1.6372772455215454 0.14350935816764832\n",
            "argm 1.6242098808288574 0.13186968863010406\n",
            "argm 1.583921194076538 0.1234542578458786\n",
            "argm 1.45826256275177 0.1452164649963379\n",
            "argm 1.207149863243103 0.22093859314918518\n",
            "argm 1.040797233581543 0.280086874961853\n",
            "argm 1.0327162742614746 0.11492148041725159\n",
            "argm 1.0742759704589844 0.17006322741508484\n",
            "repr, std, cov, closslb 14.031510353088379 0.48876953125 5.65436203032732e-05 0.0025195307098329067 0.21711671352386475\n",
            "0.0874939080346455 0.07992755714713541 1.0\n",
            "1\n",
            "argm 1.5181584358215332 0.07053650915622711\n",
            "argm 1.5199103355407715 0.11720694601535797\n",
            "argm 1.5825767517089844 0.36394503712654114\n",
            "argm 1.6648263931274414 0.13736262917518616\n",
            "argm 1.5680694580078125 0.09109731763601303\n",
            "argm 1.2410297393798828 0.1765899360179901\n",
            "argm 1.0730476379394531 0.28363800048828125\n",
            "argm 1.0976638793945312 0.1872153878211975\n",
            "argm 1.1017115116119385 0.22267085313796997\n",
            "argm 1.1463040113449097 0.2889968454837799\n",
            "argm 1.1944458484649658 0.1085205003619194\n",
            "argm 1.2512376308441162 0.10025594383478165\n",
            "argm 1.4609326124191284 0.2689473330974579\n",
            "argm 1.6870818138122559 0.12896674871444702\n",
            "argm 1.679823875427246 0.103815458714962\n",
            "argm 1.673670768737793 0.2857474684715271\n",
            "argm 1.6681711673736572 0.23516826331615448\n",
            "argm 1.6840862035751343 0.09402818977832794\n",
            "argm 1.6450861692428589 0.19415560364723206\n",
            "argm 1.425887942314148 0.17199945449829102\n",
            "argm 1.190716028213501 0.2527925968170166\n",
            "argm 1.0773814916610718 0.17814338207244873\n",
            "argm 1.1051316261291504 0.1097358763217926\n",
            "argm 1.1261193752288818 0.20742975175380707\n",
            "argm 1.1751408576965332 0.3679068684577942\n",
            "repr, std, cov, closslb 10.64928913116455 0.487548828125 9.193667210638523e-05 0.0012980459723621607 0.2059037685394287\n",
            "0.0878444089802954 0.08194990919168352 1.0\n",
            "argm 1.523072361946106 0.06516091525554657\n",
            "argm 1.5018248558044434 0.07868651300668716\n",
            "argm 1.5585777759552002 0.4188408851623535\n",
            "argm 1.6413344144821167 0.10786391794681549\n",
            "argm 1.5180141925811768 0.1046452671289444\n",
            "argm 1.2086111307144165 0.20171049237251282\n",
            "argm 1.0963391065597534 0.3395788371562958\n",
            "argm 1.110680341720581 0.08235219866037369\n",
            "argm 1.1322510242462158 0.12475065886974335\n",
            "argm 1.1992459297180176 0.21970364451408386\n",
            "argm 1.2430241107940674 0.1552356481552124\n",
            "argm 1.4126101732254028 0.2037612795829773\n",
            "argm 1.655050277709961 0.1293635070323944\n",
            "argm 1.6456725597381592 0.10280963033437729\n",
            "argm 1.6265002489089966 0.41865652799606323\n",
            "argm 1.6277856826782227 0.14184415340423584\n",
            "argm 1.6484346389770508 0.17484650015830994\n",
            "argm 1.5554101467132568 0.1620110124349594\n",
            "argm 1.2642695903778076 0.1903558075428009\n",
            "argm 1.1481175422668457 0.20215195417404175\n",
            "argm 1.1184391975402832 0.1158546656370163\n",
            "argm 1.1316804885864258 0.37443357706069946\n",
            "argm 1.1857200860977173 0.2735154628753662\n",
            "argm 1.2169331312179565 0.13097840547561646\n",
            "argm 1.2691949605941772 0.46313872933387756\n",
            "repr, std, cov, closslb 6.567253112792969 0.487548828125 8.777575567364693e-05 0.018176570534706116 0.23079656064510345\n",
            "0.0878444089802954 0.08402343142005893 1.0\n",
            "argm 1.5791313648223877 0.06558001041412354\n",
            "argm 1.5483429431915283 0.09985361248254776\n",
            "argm 1.604687213897705 0.4022558927536011\n",
            "argm 1.6801910400390625 0.05810614675283432\n",
            "argm 1.5209026336669922 0.15676674246788025\n",
            "argm 1.21165132522583 0.2716965973377228\n",
            "argm 1.122713565826416 0.28832072019577026\n",
            "argm 1.12221360206604 0.16826950013637543\n",
            "argm 1.1945160627365112 0.40853115916252136\n",
            "argm 1.2515910863876343 0.20735430717468262\n",
            "argm 1.3916511535644531 0.21048076450824738\n",
            "argm 1.6579744815826416 0.08828379213809967\n",
            "argm 1.6585273742675781 0.2258559763431549\n",
            "argm 1.6345393657684326 0.47870445251464844\n",
            "argm 1.6482415199279785 0.11821277439594269\n",
            "argm 1.6488090753555298 0.26543810963630676\n",
            "argm 1.4677364826202393 0.23287442326545715\n",
            "argm 1.2241188287734985 0.09341072291135788\n",
            "argm 1.1375614404678345 0.3347843885421753\n",
            "argm 1.1398870944976807 0.34961196780204773\n",
            "argm 1.194047212600708 0.2827271819114685\n",
            "argm 1.2251006364822388 0.3868304193019867\n",
            "argm 1.3670158386230469 0.3914991319179535\n",
            "argm 1.6200299263000488 0.2873344421386719\n",
            "argm 1.7249929904937744 0.302433043718338\n",
            "repr, std, cov, closslb 5.841297149658203 0.487060546875 9.229639545083046e-05 0.005320277065038681 0.24932749569416046\n",
            "0.0878444089802954 0.08614941855625391 1.0\n",
            "argm 1.6716129779815674 0.05926843732595444\n",
            "argm 1.6282085180282593 0.07848534733057022\n",
            "argm 1.689577579498291 0.39464515447616577\n",
            "argm 1.7576707601547241 0.07461011409759521\n",
            "argm 1.5336358547210693 0.2733590006828308\n",
            "argm 1.235780954360962 0.37829485535621643\n",
            "argm 1.1719766855239868 0.07126852869987488\n",
            "argm 1.1933192014694214 0.2135680913925171\n",
            "argm 1.2918074131011963 0.19037258625030518\n",
            "argm 1.3816099166870117 0.2646446228027344\n",
            "argm 1.6958553791046143 0.07629208266735077\n",
            "argm 1.7445080280303955 0.1301988959312439\n",
            "argm 1.7103865146636963 0.4324307441711426\n",
            "argm 1.721435308456421 0.20141713321208954\n",
            "argm 1.7309353351593018 0.2310723066329956\n",
            "argm 1.4733818769454956 0.3224015235900879\n",
            "argm 1.2773033380508423 0.22248364984989166\n",
            "argm 1.202160120010376 0.3838576376438141\n",
            "argm 1.2324166297912598 0.18572062253952026\n",
            "argm 1.261687994003296 0.2039177417755127\n",
            "argm 1.322281837463379 0.31264498829841614\n",
            "argm 1.635967493057251 0.21055340766906738\n",
            "argm 1.8386170864105225 0.27498507499694824\n",
            "argm 1.834815502166748 0.4466800093650818\n",
            "argm 1.7930291891098022 0.40258559584617615\n",
            "repr, std, cov, closslb 6.045613765716553 0.48681640625 0.00013393396511673927 0.01696275733411312 0.18019592761993408\n",
            "0.08758140194268015 0.08797676281992052 1.0\n",
            "argm 1.7808592319488525 0.06077159196138382\n",
            "argm 1.7484140396118164 0.08911639451980591\n",
            "argm 1.8082587718963623 0.3677336573600769\n",
            "argm 1.8576290607452393 0.12704171240329742\n",
            "argm 1.5739953517913818 0.3763865828514099\n",
            "argm 1.2709087133407593 0.573467493057251\n",
            "argm 1.2182483673095703 0.24199393391609192\n",
            "argm 1.2858266830444336 0.3922954201698303\n",
            "argm 1.3738147020339966 0.21877604722976685\n",
            "argm 1.645838737487793 0.33514201641082764\n",
            "argm 1.8601568937301636 0.10583549737930298\n",
            "argm 1.8306103944778442 0.2866862416267395\n",
            "argm 1.8273401260375977 0.24346354603767395\n",
            "argm 1.863600730895996 0.09071733802556992\n",
            "argm 1.61612868309021 0.4508849084377289\n",
            "argm 1.333303689956665 0.634504497051239\n",
            "argm 1.2486079931259155 0.26015472412109375\n",
            "argm 1.2820348739624023 0.17974433302879333\n",
            "argm 1.325408935546875 0.3002118468284607\n",
            "argm 1.400546669960022 0.26349908113479614\n",
            "argm 1.809863567352295 0.2430696040391922\n",
            "argm 1.9404369592666626 0.14526396989822388\n",
            "argm 1.9216301441192627 0.6738804578781128\n",
            "argm 1.8711085319519043 0.20754151046276093\n",
            "argm 1.806220293045044 0.25304239988327026\n",
            "repr, std, cov, closslb 6.331905841827393 0.487060546875 0.00011182436719536781 0.015723351389169693 0.2262449860572815\n",
            "0.08679709608059254 0.08930569197209524 1.0\n",
            "argm 1.8531209230422974 0.06773695349693298\n",
            "argm 1.8191982507705688 0.1179249957203865\n",
            "argm 1.8884985446929932 0.36584770679473877\n",
            "argm 1.9307351112365723 0.16462057828903198\n",
            "argm 1.5867406129837036 0.4560174345970154\n",
            "argm 1.3006725311279297 0.548617959022522\n",
            "argm 1.2587510347366333 0.09332690387964249\n",
            "argm 1.350912094116211 0.31645315885543823\n",
            "argm 1.492319107055664 0.3222031593322754\n",
            "argm 1.8539568185806274 0.1008789986371994\n",
            "argm 1.898935317993164 0.07635354995727539\n",
            "argm 1.8671495914459229 0.3287578225135803\n",
            "argm 1.91795814037323 0.16712677478790283\n",
            "argm 1.8106975555419922 0.1070704311132431\n",
            "argm 1.4182898998260498 0.8317520618438721\n",
            "argm 1.2777674198150635 0.39705076813697815\n",
            "argm 1.2822606563568115 0.40411388874053955\n",
            "argm 1.3569386005401611 0.23194122314453125\n",
            "argm 1.4276000261306763 0.2479199320077896\n",
            "argm 1.8340249061584473 0.248447984457016\n",
            "argm 1.9750241041183472 0.1511313021183014\n",
            "argm 1.9492335319519043 0.44082945585250854\n",
            "argm 1.8969004154205322 0.13635680079460144\n",
            "argm 1.7495501041412354 0.2255011349916458\n",
            "argm 1.4436028003692627 0.6088470220565796\n",
            "repr, std, cov, closslb 6.72783899307251 0.48779296875 7.85021111369133e-05 0.00020669441437348723 0.21257516741752625\n",
            "0.08653722470825645 0.09110887613557463 1.0\n",
            "argm 1.979257345199585 0.0690612941980362\n",
            "argm 1.9401541948318481 0.15605705976486206\n",
            "argm 2.012497901916504 0.3966623842716217\n",
            "argm 2.0159802436828613 0.2030515968799591\n",
            "argm 1.6129906177520752 0.5949838161468506\n",
            "argm 1.3297231197357178 0.3567896783351898\n",
            "argm 1.3124114274978638 0.4112468957901001\n",
            "argm 1.4511619806289673 0.2655666172504425\n",
            "argm 1.707146406173706 0.5199573636054993\n",
            "argm 2.0052542686462402 0.2223830670118332\n",
            "argm 1.9848966598510742 0.15354809165000916\n",
            "argm 2.0187597274780273 0.30474644899368286\n",
            "argm 2.008906841278076 0.1315595507621765\n",
            "argm 1.5915346145629883 0.7463237047195435\n",
            "argm 1.3481712341308594 0.5022101402282715\n",
            "argm 1.3090267181396484 0.2739848494529724\n",
            "argm 1.3910977840423584 0.20649097859859467\n",
            "argm 1.461780309677124 0.20528677105903625\n",
            "argm 1.8182322978973389 0.2605375051498413\n",
            "argm 2.0654702186584473 0.24085679650306702\n",
            "argm 2.0362284183502197 0.40815117955207825\n",
            "argm 1.9970791339874268 0.18913206458091736\n",
            "argm 1.8711364269256592 0.3728773593902588\n",
            "argm 1.5092003345489502 0.6469254493713379\n",
            "argm 1.3536667823791504 0.4722733497619629\n",
            "repr, std, cov, closslb 5.790384769439697 0.488037109375 6.684311665594578e-05 0.032867442816495895 0.1971774697303772\n",
            "0.08679709608059254 0.0934141414096036 1.0\n",
            "argm 1.8998682498931885 0.06105098873376846\n",
            "argm 1.8544225692749023 0.17242133617401123\n",
            "argm 1.936830759048462 0.4101600646972656\n",
            "argm 1.930014729499817 0.16916915774345398\n",
            "argm 1.513979196548462 0.741847038269043\n",
            "argm 1.2880420684814453 0.12050382792949677\n",
            "argm 1.3227479457855225 0.5454384088516235\n",
            "argm 1.4965112209320068 0.3869338035583496\n",
            "argm 1.8195208311080933 0.3352314233779907\n",
            "argm 1.9201656579971313 0.11256895959377289\n",
            "argm 1.882983922958374 0.32907071709632874\n",
            "argm 1.9558489322662354 0.12353528290987015\n",
            "argm 1.7858504056930542 0.22093020379543304\n",
            "argm 1.3636348247528076 0.72975754737854\n",
            "argm 1.2924928665161133 0.2423655390739441\n",
            "argm 1.339046597480774 0.4775756895542145\n",
            "argm 1.4149820804595947 0.2039959728717804\n",
            "argm 1.6248884201049805 0.37667155265808105\n",
            "argm 1.9619927406311035 0.09365938603878021\n",
            "argm 1.9502577781677246 0.3053611218929291\n",
            "argm 1.9450122117996216 0.27796345949172974\n",
            "argm 1.895815372467041 0.2615600824356079\n",
            "argm 1.478560209274292 0.6239757537841797\n",
            "argm 1.3364149332046509 0.3650123178958893\n",
            "argm 1.2884386777877808 0.21342001855373383\n",
            "repr, std, cov, closslb 5.0642991065979 0.48876953125 4.789326339960098e-05 0.001588143641129136 0.23430901765823364\n",
            "0.08705774784691964 0.0957777352264601 1.0\n",
            "argm 2.024271249771118 0.11336524039506912\n",
            "argm 1.9916036128997803 0.22767484188079834\n",
            "argm 2.0739235877990723 0.42087769508361816\n",
            "argm 2.0330631732940674 0.21397602558135986\n",
            "argm 1.5493829250335693 0.943346381187439\n",
            "argm 1.3333075046539307 0.09128730744123459\n",
            "argm 1.4225646257400513 0.4559839069843292\n",
            "argm 1.6348247528076172 0.5377606153488159\n",
            "argm 2.006723165512085 0.33065488934516907\n",
            "argm 2.007011651992798 0.19103196263313293\n",
            "argm 2.0318915843963623 0.3235470652580261\n",
            "argm 2.0508108139038086 0.12054014950990677\n",
            "argm 1.6066843271255493 0.785230278968811\n",
            "argm 1.3605034351348877 0.42452943325042725\n",
            "argm 1.33506178855896 0.5610231161117554\n",
            "argm 1.4419775009155273 0.2815956175327301\n",
            "argm 1.6459314823150635 0.365861713886261\n",
            "argm 1.991023302078247 0.4436792731285095\n",
            "argm 2.0255749225616455 0.17346349358558655\n",
            "argm 2.040034294128418 0.3227619528770447\n",
            "argm 1.9453176259994507 0.13021565973758698\n",
            "argm 1.5078682899475098 0.7599745392799377\n",
            "argm 1.3336114883422852 0.6784195899963379\n",
            "argm 1.3601412773132324 0.48881861567497253\n",
            "argm 1.4494044780731201 0.3953995108604431\n",
            "repr, std, cov, closslb 4.34492826461792 0.486328125 0.00021205819211900234 0.0025233179330825806 0.262939453125\n",
            "0.08524949222308595 0.0957777352264601 1.0\n",
            "argm 1.9312553405761719 0.09109452366828918\n",
            "argm 1.8894308805465698 0.23591983318328857\n",
            "argm 1.982361078262329 0.40142330527305603\n",
            "argm 1.9210323095321655 0.17967447638511658\n",
            "argm 1.4533274173736572 0.9757285714149475\n",
            "argm 1.2744636535644531 0.15933997929096222\n",
            "argm 1.4239498376846313 0.3777220845222473\n",
            "argm 1.6664944887161255 0.5326029062271118\n",
            "argm 1.9484881162643433 0.29558223485946655\n",
            "argm 1.8990588188171387 0.34033042192459106\n",
            "argm 1.9748750925064087 0.2216314971446991\n",
            "argm 1.874729871749878 0.2188514918088913\n",
            "argm 1.385710597038269 0.76723313331604\n",
            "argm 1.2748876810073853 0.20044761896133423\n",
            "argm 1.3384339809417725 0.6175909042358398\n",
            "argm 1.4766229391098022 0.24540814757347107\n",
            "argm 1.762564778327942 0.5972508788108826\n",
            "argm 1.9388993978500366 0.11915992945432663\n",
            "argm 1.9488657712936401 0.2197008579969406\n",
            "argm 1.931011438369751 0.1452835202217102\n",
            "argm 1.5813974142074585 0.5007137060165405\n",
            "argm 1.2944151163101196 0.6942043900489807\n",
            "argm 1.2816162109375 0.4752466678619385\n",
            "argm 1.402016282081604 0.39760956168174744\n",
            "argm 1.594698190689087 0.4044184386730194\n",
            "repr, std, cov, closslb 3.560974597930908 0.488037109375 6.352225318551064e-05 0.0009959775488823652 0.17857865989208221\n",
            "0.0849942543921521 0.09751646690256005 1.0\n",
            "argm 1.9156227111816406 0.08006114512681961\n",
            "argm 1.863318920135498 0.266150563955307\n",
            "argm 1.9611765146255493 0.3615254759788513\n",
            "argm 1.8786615133285522 0.201252281665802\n",
            "argm 1.410344123840332 0.943474292755127\n",
            "argm 1.2689788341522217 0.40151551365852356\n",
            "argm 1.4560723304748535 0.32529887557029724\n",
            "argm 1.7387598752975464 0.667341947555542\n",
            "argm 1.9072998762130737 0.22394490242004395\n",
            "argm 1.8570148944854736 0.4045637249946594\n",
            "argm 1.9599982500076294 0.08420180529356003\n",
            "argm 1.6679754257202148 0.45730265974998474\n",
            "argm 1.3012275695800781 0.6168756484985352\n",
            "argm 1.2391972541809082 0.3159167170524597\n",
            "argm 1.3724392652511597 0.12637394666671753\n",
            "argm 1.541008472442627 0.20933523774147034\n",
            "argm 1.8669078350067139 0.4958761930465698\n",
            "argm 1.8581162691116333 0.41839665174484253\n",
            "argm 1.9055557250976562 0.18363353610038757\n",
            "argm 1.725189447402954 0.2949368357658386\n",
            "argm 1.3359419107437134 0.4725186228752136\n",
            "argm 1.2501707077026367 0.4156920909881592\n",
            "argm 1.3362797498703003 0.5370638370513916\n",
            "argm 1.495943546295166 0.25116094946861267\n",
            "argm 1.8268342018127441 0.5260661244392395\n",
            "repr, std, cov, closslb 3.6164121627807617 0.488037109375 9.252107702195644e-05 0.03274723142385483 0.17090407013893127\n",
            "0.0849942543921521 0.09978419087590744 1.0\n",
            "argm 1.938476800918579 0.130578875541687\n",
            "argm 1.8912285566329956 0.32278430461883545\n",
            "argm 1.9897905588150024 0.29929542541503906\n",
            "argm 1.890122652053833 0.23525488376617432\n",
            "argm 1.3990998268127441 0.9337863922119141\n",
            "argm 1.2917234897613525 0.6194086670875549\n",
            "argm 1.4957129955291748 0.4116520285606384\n",
            "argm 1.8337293863296509 0.5896026492118835\n",
            "argm 1.901719570159912 0.2897009253501892\n",
            "argm 1.9022367000579834 0.18194037675857544\n",
            "argm 1.9642605781555176 0.11933036148548126\n",
            "argm 1.5256630182266235 0.5826735496520996\n",
            "argm 1.2727570533752441 0.3581838607788086\n",
            "argm 1.293810248374939 0.6513465046882629\n",
            "argm 1.4447499513626099 0.12269988656044006\n",
            "argm 1.755818486213684 0.595056414604187\n",
            "argm 1.9101266860961914 0.171708881855011\n",
            "argm 1.8988533020019531 0.22167620062828064\n",
            "argm 1.917943000793457 0.13369694352149963\n",
            "argm 1.4919919967651367 0.7001645565032959\n",
            "argm 1.2900950908660889 0.5636925101280212\n",
            "argm 1.3054102659225464 0.47447943687438965\n",
            "argm 1.4636579751968384 0.42526423931121826\n",
            "argm 1.8056912422180176 0.49694743752479553\n",
            "argm 1.9260258674621582 0.16950443387031555\n",
            "repr, std, cov, closslb 3.6760952472686768 0.487548828125 9.601796045899391e-05 0.0020829609129577875 0.17039528489112854\n",
            "0.08431734997884348 0.10129147661740025 1.0\n",
            "argm 1.8987979888916016 0.17457269132137299\n",
            "argm 1.8517276048660278 0.35604649782180786\n",
            "argm 1.9597876071929932 0.2645663917064667\n",
            "argm 1.8469698429107666 0.23981183767318726\n",
            "argm 1.3598637580871582 0.8958256840705872\n",
            "argm 1.294199824333191 0.7293317317962646\n",
            "argm 1.5013275146484375 0.44358429312705994\n",
            "argm 1.8424458503723145 0.4698280096054077\n",
            "argm 1.855616569519043 0.33214128017425537\n",
            "argm 1.8893115520477295 0.18850061297416687\n",
            "argm 1.8951990604400635 0.14355964958667755\n",
            "argm 1.401548981666565 0.7988697290420532\n",
            "argm 1.2333961725234985 0.1156926155090332\n",
            "argm 1.3148236274719238 0.5340876579284668\n",
            "argm 1.4727154970169067 0.2167643904685974\n",
            "argm 1.8085131645202637 0.510745644569397\n",
            "argm 1.8601765632629395 0.24786405265331268\n",
            "argm 1.900886058807373 0.15420466661453247\n",
            "argm 1.804176688194275 0.20221062004566193\n",
            "argm 1.3733887672424316 0.8708716630935669\n",
            "argm 1.2527663707733154 0.39409196376800537\n",
            "argm 1.346360683441162 0.3307219445705414\n",
            "argm 1.5116944313049316 0.31300652027130127\n",
            "argm 1.8279565572738647 0.18713437020778656\n",
            "argm 1.882888913154602 0.22902712225914001\n",
            "repr, std, cov, closslb 3.5039401054382324 0.487548828125 0.0001083605457097292 0.0021889416966587305 0.17886818945407867\n",
            "0.08381321183215121 0.10302727648062171 1.0\n",
            "argm 1.827559471130371 0.18743890523910522\n",
            "argm 1.7701494693756104 0.35724231600761414\n",
            "argm 1.8805575370788574 0.2094106674194336\n",
            "argm 1.7553749084472656 0.24913810193538666\n",
            "argm 1.303955078125 0.8394818902015686\n",
            "argm 1.2713099718093872 0.7588852643966675\n",
            "argm 1.472008466720581 0.41626206040382385\n",
            "argm 1.8036096096038818 0.39665961265563965\n",
            "argm 1.7895934581756592 0.3938041627407074\n",
            "argm 1.8531343936920166 0.20289793610572815\n",
            "argm 1.7955957651138306 0.2525160014629364\n",
            "argm 1.3224167823791504 0.7923240661621094\n",
            "argm 1.200763463973999 0.13926252722740173\n",
            "argm 1.2910839319229126 0.46169477701187134\n",
            "argm 1.4439014196395874 0.17983093857765198\n",
            "argm 1.7931541204452515 0.25819334387779236\n",
            "argm 1.788956642150879 0.47398823499679565\n",
            "argm 1.833486795425415 0.14029350876808167\n",
            "argm 1.7087113857269287 0.1587560474872589\n",
            "argm 1.3212072849273682 0.8006859421730042\n",
            "argm 1.1945295333862305 0.3922535181045532\n",
            "argm 1.2953029870986938 0.38343575596809387\n",
            "argm 1.4398088455200195 0.564227819442749\n",
            "argm 1.7781221866607666 0.14031864702701569\n",
            "argm 1.8028805255889893 0.4336377680301666\n",
            "repr, std, cov, closslb 3.3595380783081055 0.48681640625 0.0001399561297148466 0.0005634664557874203 0.149809792637825\n",
            "0.08273122909762164 0.10375063459835486 1.0\n",
            "argm 1.8502594232559204 0.23413729667663574\n",
            "argm 1.7856743335723877 0.3775181174278259\n",
            "argm 1.896998643875122 0.1719743013381958\n",
            "argm 1.7533332109451294 0.27345120906829834\n",
            "argm 1.2904105186462402 0.8173949718475342\n",
            "argm 1.286184310913086 0.7675220966339111\n",
            "argm 1.500746488571167 0.4379991590976715\n",
            "argm 1.8172343969345093 0.4257168471813202\n",
            "argm 1.7753880023956299 0.46064984798431396\n",
            "argm 1.876082420349121 0.06994418799877167\n",
            "argm 1.7306631803512573 0.36661607027053833\n",
            "argm 1.2854492664337158 0.7097108364105225\n",
            "argm 1.1932181119918823 0.38983672857284546\n",
            "argm 1.3104019165039062 0.28904715180397034\n",
            "argm 1.5232502222061157 0.2399654984474182\n",
            "argm 1.8388217687606812 0.19953958690166473\n",
            "argm 1.783017635345459 0.46822428703308105\n",
            "argm 1.872936487197876 0.12556928396224976\n",
            "argm 1.5519294738769531 0.5276386141777039\n",
            "argm 1.2585750818252563 0.3094654381275177\n",
            "argm 1.1845093965530396 0.5667339563369751\n",
            "argm 1.3229291439056396 0.38434380292892456\n",
            "argm 1.5657837390899658 0.3844974637031555\n",
            "argm 1.83733332157135 0.42934736609458923\n",
            "argm 1.7870694398880005 0.31360891461372375\n",
            "repr, std, cov, closslb 3.305634021759033 0.4873046875 0.00012430804781615734 0.015254574827849865 0.17299038171768188\n",
            "0.08150013240361212 0.10427042631571776 1.0\n",
            "argm 1.8554706573486328 0.2330392599105835\n",
            "argm 1.8120472431182861 0.4062708616256714\n",
            "argm 1.9267501831054688 0.16922223567962646\n",
            "argm 1.7661031484603882 0.30575788021087646\n",
            "argm 1.2941184043884277 0.8324533700942993\n",
            "argm 1.3006014823913574 0.6977342367172241\n",
            "argm 1.52146577835083 0.37081819772720337\n",
            "argm 1.8329064846038818 0.37888437509536743\n",
            "argm 1.7828136682510376 0.4286477267742157\n",
            "argm 1.8869915008544922 0.08397269994020462\n",
            "argm 1.6968454122543335 0.41885489225387573\n",
            "argm 1.2801051139831543 0.628304660320282\n",
            "argm 1.2098290920257568 0.5216337442398071\n",
            "argm 1.3209481239318848 0.26981624960899353\n",
            "argm 1.5764278173446655 0.08418504148721695\n",
            "argm 1.8560514450073242 0.11494383215904236\n",
            "argm 1.8292673826217651 0.2924306392669678\n",
            "argm 1.875559687614441 0.13425853848457336\n",
            "argm 1.4852370023727417 0.8553969860076904\n",
            "argm 1.250478982925415 0.2952413558959961\n",
            "argm 1.2177033424377441 0.3589661717414856\n",
            "argm 1.33075749874115 0.12340117245912552\n",
            "argm 1.6083414554595947 0.17947611212730408\n",
            "argm 1.8481591939926147 0.25921034812927246\n",
            "argm 1.8361501693725586 0.346896231174469\n",
            "repr, std, cov, closslb 2.8353304862976074 0.48779296875 0.00010041636414825916 0.01529318280518055 0.16782744228839874\n",
            "0.08044801025968118 0.1050025126319969 1.0\n",
            "argm 1.7669587135314941 0.2241460680961609\n",
            "argm 1.7220869064331055 0.431600958108902\n",
            "argm 1.8427706956863403 0.1695798635482788\n",
            "argm 1.6623585224151611 0.3333175480365753\n",
            "argm 1.2275729179382324 0.6931164264678955\n",
            "argm 1.274520993232727 0.6317061185836792\n",
            "argm 1.4884371757507324 0.33180323243141174\n",
            "argm 1.7506375312805176 0.2690032124519348\n",
            "argm 1.6961725950241089 0.5123438239097595\n",
            "argm 1.8145657777786255 0.05843304097652435\n",
            "argm 1.552110195159912 0.4856586456298828\n",
            "argm 1.2157248258590698 0.47402453422546387\n",
            "argm 1.181614637374878 0.5023750066757202\n",
            "argm 1.3142088651657104 0.24725216627120972\n",
            "argm 1.6110904216766357 0.4043877124786377\n",
            "argm 1.754036784172058 0.12110453099012375\n",
            "argm 1.755347490310669 0.14163461327552795\n",
            "argm 1.746187686920166 0.1031951978802681\n",
            "argm 1.2863352298736572 0.9105210304260254\n",
            "argm 1.1487064361572266 0.19581523537635803\n",
            "argm 1.230852723121643 0.16195791959762573\n",
            "argm 1.3898229598999023 0.15682262182235718\n",
            "argm 1.7165648937225342 0.30267333984375\n",
            "argm 1.769972324371338 0.3401208519935608\n",
            "argm 1.7921936511993408 0.18173083662986755\n",
            "repr, std, cov, closslb 2.8555045127868652 0.487060546875 0.0001508491113781929 0.015614675357937813 0.1479043811559677\n",
            "0.07901361145340069 0.10542315311771569 1.0\n",
            "argm 1.8121212720870972 0.2551227807998657\n",
            "argm 1.754806399345398 0.4464816451072693\n",
            "argm 1.8912490606307983 0.17216429114341736\n",
            "argm 1.6967124938964844 0.3888393044471741\n",
            "argm 1.2410306930541992 0.6930124163627625\n",
            "argm 1.3049218654632568 0.6723343133926392\n",
            "argm 1.5449892282485962 0.4287399351596832\n",
            "argm 1.7891159057617188 0.2733003497123718\n",
            "argm 1.739849328994751 0.5728220343589783\n",
            "argm 1.855812907218933 0.1006079837679863\n",
            "argm 1.5140998363494873 0.5627945065498352\n",
            "argm 1.1995649337768555 0.27870386838912964\n",
            "argm 1.210351824760437 0.5606989860534668\n",
            "argm 1.3886443376541138 0.27827638387680054\n",
            "argm 1.740479826927185 0.3150617778301239\n",
            "argm 1.7596105337142944 0.4392620027065277\n",
            "argm 1.8295905590057373 0.09265076369047165\n",
            "argm 1.673440933227539 0.2346457839012146\n",
            "argm 1.2231056690216064 0.6895283460617065\n",
            "argm 1.135021686553955 0.48542115092277527\n",
            "argm 1.3077927827835083 0.3447840213775635\n",
            "argm 1.5691721439361572 0.3033578395843506\n",
            "argm 1.780930519104004 0.2068290412425995\n",
            "argm 1.7375904321670532 0.4053041338920593\n",
            "argm 1.7809033393859863 0.1756092607975006\n",
            "repr, std, cov, closslb 2.8123254776000977 0.488525390625 5.841138772666454e-05 0.0006109669338911772 0.10123587399721146\n",
            "0.07861972584035015 0.10722975620884555 1.0\n",
            "argm 1.8375142812728882 0.2314997911453247\n",
            "argm 1.7955894470214844 0.43019556999206543\n",
            "argm 1.9239513874053955 0.15367381274700165\n",
            "argm 1.7216522693634033 0.3838045597076416\n",
            "argm 1.247772455215454 0.5971324443817139\n",
            "argm 1.3182134628295898 0.6181117296218872\n",
            "argm 1.5531569719314575 0.41962599754333496\n",
            "argm 1.7833806276321411 0.21414366364479065\n",
            "argm 1.7379714250564575 0.517680287361145\n",
            "argm 1.8579226732254028 0.10236259549856186\n",
            "argm 1.5307505130767822 0.5366401672363281\n",
            "argm 1.206007957458496 0.17293822765350342\n",
            "argm 1.2358837127685547 0.6264327168464661\n",
            "argm 1.4007391929626465 0.21730083227157593\n",
            "argm 1.7480130195617676 0.3056097626686096\n",
            "argm 1.7828088998794556 0.453069806098938\n",
            "argm 1.8319332599639893 0.09150523692369461\n",
            "argm 1.687940239906311 0.21373294293880463\n",
            "argm 1.2438533306121826 0.5555816292762756\n",
            "argm 1.1504478454589844 0.4235331118106842\n",
            "argm 1.31961989402771 0.3377376198768616\n",
            "argm 1.5578997135162354 0.5586382150650024\n",
            "argm 1.8070220947265625 0.12140627950429916\n",
            "argm 1.7569072246551514 0.2873483896255493\n",
            "argm 1.827157974243164 0.2584196627140045\n",
            "repr, std, cov, closslb 2.5210506916046143 0.48486328125 0.00039567030034959316 0.0005664362106472254 0.07870665192604065\n",
            "0.07690982452556627 0.10722975620884555 1.0\n",
            "argm 1.7502810955047607 0.20511355996131897\n",
            "argm 1.7022089958190918 0.4551401138305664\n",
            "argm 1.839855670928955 0.14997181296348572\n",
            "argm 1.6451295614242554 0.33960676193237305\n",
            "argm 1.1943137645721436 0.4875696897506714\n",
            "argm 1.2857232093811035 0.6178804039955139\n",
            "argm 1.517077922821045 0.4454702138900757\n",
            "argm 1.7338850498199463 0.19981898367404938\n",
            "argm 1.6836129426956177 0.4332270622253418\n",
            "argm 1.8031601905822754 0.06711669266223907\n",
            "argm 1.4507639408111572 0.5806200504302979\n",
            "argm 1.1485248804092407 0.0866968184709549\n",
            "argm 1.199987769126892 0.6356751918792725\n",
            "argm 1.3606281280517578 0.2332683652639389\n",
            "argm 1.7010829448699951 0.2614399492740631\n",
            "argm 1.7044140100479126 0.4278654456138611\n",
            "argm 1.7771629095077515 0.06958935409784317\n",
            "argm 1.6075692176818848 0.30943191051483154\n",
            "argm 1.2005915641784668 0.4884610176086426\n",
            "argm 1.1307587623596191 0.6111552715301514\n",
            "argm 1.2971011400222778 0.24441909790039062\n",
            "argm 1.5749011039733887 0.5632415413856506\n",
            "argm 1.7563207149505615 0.17548911273479462\n",
            "argm 1.7467093467712402 0.1624133437871933\n",
            "argm 1.7661278247833252 0.16537494957447052\n",
            "repr, std, cov, closslb 2.3010611534118652 0.488037109375 8.345511741936207e-05 0.015125165693461895 0.08993867039680481\n",
            "0.07714078480552634 0.10994291703154094 1.0\n",
            "argm 1.7784850597381592 0.22024008631706238\n",
            "argm 1.7337514162063599 0.44648441672325134\n",
            "argm 1.8510866165161133 0.17031747102737427\n",
            "argm 1.6414883136749268 0.31795352697372437\n",
            "argm 1.1871249675750732 0.43682849407196045\n",
            "argm 1.2997581958770752 0.6266350150108337\n",
            "argm 1.5567985773086548 0.4734797179698944\n",
            "argm 1.7490453720092773 0.24934764206409454\n",
            "argm 1.7194372415542603 0.268690288066864\n",
            "argm 1.8338871002197266 0.06794370710849762\n",
            "argm 1.414872646331787 0.7117252945899963\n",
            "argm 1.1267136335372925 0.10650884360074997\n",
            "argm 1.2362971305847168 0.5156043767929077\n",
            "argm 1.4222207069396973 0.4507368505001068\n",
            "argm 1.744938611984253 0.27630382776260376\n",
            "argm 1.7225337028503418 0.3521796464920044\n",
            "argm 1.829466700553894 0.09506475180387497\n",
            "argm 1.4799476861953735 0.6490828990936279\n",
            "argm 1.1661161184310913 0.22076815366744995\n",
            "argm 1.1747854948043823 0.7282623052597046\n",
            "argm 1.341804027557373 0.4140101373195648\n",
            "argm 1.6956400871276855 0.22314581274986267\n",
            "argm 1.7447775602340698 0.15314854681491852\n",
            "argm 1.7816758155822754 0.11153798550367355\n",
            "argm 1.6419692039489746 0.3423588275909424\n",
            "repr, std, cov, closslb 2.2061240673065186 0.48779296875 8.820928633213043e-05 0.0003325386205688119 0.10159367322921753\n",
            "0.07683299153403225 0.11193880085520394 1.0\n",
            "argm 1.6790223121643066 0.15368220210075378\n",
            "argm 1.625693917274475 0.43991580605506897\n",
            "argm 1.7668896913528442 0.16408413648605347\n",
            "argm 1.5615938901901245 0.27045607566833496\n",
            "argm 1.148877739906311 0.36843493580818176\n",
            "argm 1.2609176635742188 0.5152802467346191\n",
            "argm 1.4974186420440674 0.4663858413696289\n",
            "argm 1.6846923828125 0.16312859952449799\n",
            "argm 1.6519465446472168 0.20461902022361755\n",
            "argm 1.7517813444137573 0.09256415069103241\n",
            "argm 1.3549280166625977 0.6322245597839355\n",
            "argm 1.1048974990844727 0.07774773985147476\n",
            "argm 1.2023365497589111 0.399702250957489\n",
            "argm 1.3601611852645874 0.3406042456626892\n",
            "argm 1.680834174156189 0.19521452486515045\n",
            "argm 1.6442136764526367 0.24090707302093506\n",
            "argm 1.7397716045379639 0.0832853838801384\n",
            "argm 1.49757981300354 0.40167197585105896\n",
            "argm 1.1555484533309937 0.28603243827819824\n",
            "argm 1.1212246417999268 0.7068274617195129\n",
            "argm 1.2498950958251953 0.23624393343925476\n",
            "argm 1.5278266668319702 0.391638845205307\n",
            "argm 1.7098945379257202 0.17892010509967804\n",
            "argm 1.6782324314117432 0.13954472541809082\n",
            "argm 1.703879475593567 0.1320764422416687\n",
            "repr, std, cov, closslb 2.266437530517578 0.486328125 0.00020787259563803673 0.00020894902991130948 0.10199572890996933\n",
            "0.07629730547918527 0.11340276858746569 1.0\n",
            "argm 1.696260690689087 0.17668771743774414\n",
            "argm 1.6286771297454834 0.44320428371429443\n",
            "argm 1.7629177570343018 0.17835013568401337\n",
            "argm 1.562147855758667 0.2991389334201813\n",
            "argm 1.1427513360977173 0.37432461977005005\n",
            "argm 1.2617409229278564 0.4852730631828308\n",
            "argm 1.5055912733078003 0.4772767424583435\n",
            "argm 1.67584228515625 0.15726685523986816\n",
            "argm 1.6448311805725098 0.20470842719078064\n",
            "argm 1.7443561553955078 0.14475268125534058\n",
            "argm 1.3241440057754517 0.7138749957084656\n",
            "argm 1.0965125560760498 0.100758858025074\n",
            "argm 1.201446533203125 0.30268728733062744\n",
            "argm 1.3740975856781006 0.3836117684841156\n",
            "argm 1.6758716106414795 0.17603951692581177\n",
            "argm 1.6244568824768066 0.3870287835597992\n",
            "argm 1.7167086601257324 0.09693112224340439\n",
            "argm 1.423531174659729 0.5206558704376221\n",
            "argm 1.1424214839935303 0.11277571320533752\n",
            "argm 1.1338324546813965 0.5917763710021973\n",
            "argm 1.2703074216842651 0.3232508897781372\n",
            "argm 1.5811668634414673 0.39927756786346436\n",
            "argm 1.6849067211151123 0.28635653853416443\n",
            "argm 1.69370698928833 0.22287480533123016\n",
            "argm 1.657806634902954 0.15783123672008514\n",
            "repr, std, cov, closslb 2.0752644538879395 0.486572265625 0.00018820748664438725 0.014702370390295982 0.1427067518234253\n",
            "0.07546304899619168 0.11431317237218536 1.0\n",
            "argm 1.6588085889816284 0.1701330840587616\n",
            "argm 1.6033728122711182 0.4659472107887268\n",
            "argm 1.7551020383834839 0.16226805746555328\n",
            "argm 1.5546153783798218 0.27792155742645264\n",
            "argm 1.1339383125305176 0.3498159348964691\n",
            "argm 1.2539359331130981 0.4817526936531067\n",
            "argm 1.4882445335388184 0.5314433574676514\n",
            "argm 1.6467878818511963 0.1287907361984253\n",
            "argm 1.6207265853881836 0.2293568104505539\n",
            "argm 1.7226431369781494 0.13797171413898468\n",
            "argm 1.3138772249221802 0.638717770576477\n",
            "argm 1.0888032913208008 0.16526319086551666\n",
            "argm 1.202104091644287 0.2576066255569458\n",
            "argm 1.3844578266143799 0.4534972906112671\n",
            "argm 1.663980484008789 0.16153044998645782\n",
            "argm 1.615239143371582 0.5114972591400146\n",
            "argm 1.7196266651153564 0.12754462659358978\n",
            "argm 1.3740862607955933 0.5082031488418579\n",
            "argm 1.1041390895843506 0.14209561049938202\n",
            "argm 1.1309850215911865 0.5590265989303589\n",
            "argm 1.2891771793365479 0.43992698192596436\n",
            "argm 1.5998435020446777 0.17638877034187317\n",
            "argm 1.643165111541748 0.5997553467750549\n",
            "argm 1.7046098709106445 0.12969878315925598\n",
            "argm 1.554198980331421 0.20062364637851715\n",
            "repr, std, cov, closslb 2.298703908920288 0.485595703125 0.000254917424172163 0.00015186094969976693 0.07711632549762726\n",
            "0.07493691429550663 0.11557692339220939 1.0\n",
            "argm 1.643249750137329 0.14755502343177795\n",
            "argm 1.5796208381652832 0.4600072205066681\n",
            "argm 1.7290115356445312 0.13931840658187866\n",
            "argm 1.5251561403274536 0.23062247037887573\n",
            "argm 1.109168291091919 0.2954872250556946\n",
            "argm 1.2294130325317383 0.4569562077522278\n",
            "argm 1.453418254852295 0.54551100730896\n",
            "argm 1.6400694847106934 0.07615238428115845\n",
            "argm 1.6139137744903564 0.2601407468318939\n",
            "argm 1.71809720993042 0.08437782526016235\n",
            "argm 1.3340141773223877 0.5242182016372681\n",
            "argm 1.0848281383514404 0.10916590690612793\n",
            "argm 1.1693278551101685 0.32981112599372864\n",
            "argm 1.3144302368164062 0.28778985142707825\n",
            "argm 1.648217797279358 0.17279013991355896\n",
            "argm 1.6275147199630737 0.46573206782341003\n",
            "argm 1.7194515466690063 0.09720493108034134\n",
            "argm 1.5116519927978516 0.4363618791103363\n",
            "argm 1.1654906272888184 0.24636369943618774\n",
            "argm 1.1151158809661865 0.6909186244010925\n",
            "argm 1.238898515701294 0.29745417833328247\n",
            "argm 1.477412223815918 0.21982379257678986\n",
            "argm 1.6833915710449219 0.11970195919275284\n",
            "argm 1.6443548202514648 0.1302715390920639\n",
            "argm 1.6871896982192993 0.22082683444023132\n",
            "repr, std, cov, closslb 1.9680927991867065 0.487060546875 0.0001314198598265648 0.00037815264659002423 0.06771861016750336\n",
            "0.07486205224326337 0.11744008833122449 1.0\n",
            "argm 1.7359685897827148 0.18343794345855713\n",
            "argm 1.681654691696167 0.47939175367355347\n",
            "argm 1.8298883438110352 0.15219859778881073\n",
            "argm 1.5942330360412598 0.24432408809661865\n",
            "argm 1.14544677734375 0.37848204374313354\n",
            "argm 1.2740157842636108 0.460554838180542\n",
            "argm 1.5327421426773071 0.569567084312439\n",
            "argm 1.7197881937026978 0.09627733379602432\n",
            "argm 1.7110679149627686 0.14573615789413452\n",
            "argm 1.8106575012207031 0.13685134053230286\n",
            "argm 1.3308024406433105 0.7524943351745605\n",
            "argm 1.0919760465621948 0.29334425926208496\n",
            "argm 1.2214295864105225 0.14506839215755463\n",
            "argm 1.4049831628799438 0.41761156916618347\n",
            "argm 1.7176551818847656 0.1876009702682495\n",
            "argm 1.6899785995483398 0.41739922761917114\n",
            "argm 1.7883387804031372 0.09672995656728745\n",
            "argm 1.3979259729385376 0.772794246673584\n",
            "argm 1.1366283893585205 0.35473892092704773\n",
            "argm 1.1779921054840088 0.32268092036247253\n",
            "argm 1.3342220783233643 0.1924009919166565\n",
            "argm 1.6657756567001343 0.23768562078475952\n",
            "argm 1.7024357318878174 0.3949580788612366\n",
            "argm 1.7652606964111328 0.11152401566505432\n",
            "argm 1.509535551071167 0.734073281288147\n",
            "repr, std, cov, closslb 2.059363842010498 0.485595703125 0.0002401068340986967 0.00012184939987491816 0.09954865276813507\n",
            "0.07493691429550663 0.1196916465206343 1.0\n",
            "argm 1.7084920406341553 0.15174317359924316\n",
            "argm 1.650498390197754 0.4814565181732178\n",
            "argm 1.806479811668396 0.15577487647533417\n",
            "argm 1.5771703720092773 0.21688175201416016\n",
            "argm 1.1199841499328613 0.37585294246673584\n",
            "argm 1.2633514404296875 0.4524858593940735\n",
            "argm 1.5183871984481812 0.6121024489402771\n",
            "argm 1.6871963739395142 0.06323866546154022\n",
            "argm 1.694320797920227 0.11345744132995605\n",
            "argm 1.7829558849334717 0.10379869490861893\n",
            "argm 1.3171513080596924 0.7688044309616089\n",
            "argm 1.0810036659240723 0.3109574317932129\n",
            "argm 1.225020408630371 0.15568546950817108\n",
            "argm 1.409250020980835 0.4115486741065979\n",
            "argm 1.6992766857147217 0.11378712952136993\n",
            "argm 1.6830132007598877 0.3301156759262085\n",
            "argm 1.7894678115844727 0.10869372636079788\n",
            "argm 1.3644046783447266 0.7506040334701538\n",
            "argm 1.1055176258087158 0.36922281980514526\n",
            "argm 1.1847951412200928 0.2570645809173584\n",
            "argm 1.3487043380737305 0.12857559323310852\n",
            "argm 1.6751532554626465 0.264345645904541\n",
            "argm 1.7046759128570557 0.4072571098804474\n",
            "argm 1.7809778451919556 0.11935550719499588\n",
            "argm 1.4588253498077393 0.7160946130752563\n",
            "repr, std, cov, closslb 1.8984845876693726 0.487060546875 0.00012700771912932396 0.01616484299302101 0.0543716736137867\n",
            "0.07508686306101192 0.12210835789412905 1.0\n",
            "argm 1.6708928346633911 0.16110017895698547\n",
            "argm 1.617448329925537 0.5022519826889038\n",
            "argm 1.7793978452682495 0.15866103768348694\n",
            "argm 1.5616737604141235 0.26070231199264526\n",
            "argm 1.127819299697876 0.35297033190727234\n",
            "argm 1.2671698331832886 0.47092604637145996\n",
            "argm 1.5168771743774414 0.5928045511245728\n",
            "argm 1.6825289726257324 0.09285751730203629\n",
            "argm 1.678389072418213 0.14203134179115295\n",
            "argm 1.7712050676345825 0.0725816935300827\n",
            "argm 1.3376091718673706 0.730073869228363\n",
            "argm 1.0874016284942627 0.1862766146659851\n",
            "argm 1.2132019996643066 0.2049235701560974\n",
            "argm 1.392852783203125 0.37123727798461914\n",
            "argm 1.6859803199768066 0.18987524509429932\n",
            "argm 1.6647688150405884 0.394960880279541\n",
            "argm 1.7892239093780518 0.10330136865377426\n",
            "argm 1.4062347412109375 0.6743465662002563\n",
            "argm 1.1286187171936035 0.22486132383346558\n",
            "argm 1.174870252609253 0.33639371395111084\n",
            "argm 1.312044620513916 0.0835144892334938\n",
            "argm 1.6484870910644531 0.42056480050086975\n",
            "argm 1.6828861236572266 0.39733850955963135\n",
            "argm 1.7572088241577148 0.19829627871513367\n",
            "argm 1.5300521850585938 0.5079740285873413\n",
            "repr, std, cov, closslb 1.7395310401916504 0.486083984375 0.0001802018377929926 7.73030988057144e-05 0.09805065393447876\n",
            "0.07396951970849287 0.12247504941499343 1.0\n",
            "argm 1.6270439624786377 0.15559884905815125\n",
            "argm 1.5561109781265259 0.48312172293663025\n",
            "argm 1.7227163314819336 0.15724730491638184\n",
            "argm 1.4941344261169434 0.2355426549911499\n",
            "argm 1.0965361595153809 0.3107171654701233\n",
            "argm 1.240120530128479 0.3896411657333374\n",
            "argm 1.4713683128356934 0.5874792337417603\n",
            "argm 1.621932029724121 0.07879547774791718\n",
            "argm 1.6149969100952148 0.09395834058523178\n",
            "argm 1.7166390419006348 0.09192992001771927\n",
            "argm 1.2755671739578247 0.7530900835990906\n",
            "argm 1.0659878253936768 0.25681033730506897\n",
            "argm 1.2085965871810913 0.1173047348856926\n",
            "argm 1.3799772262573242 0.4467889666557312\n",
            "argm 1.6392929553985596 0.09104982018470764\n",
            "argm 1.6135715246200562 0.2700257897377014\n",
            "argm 1.7161927223205566 0.08999370038509369\n",
            "argm 1.3042925596237183 0.7172903418540955\n",
            "argm 1.0663094520568848 0.12271665036678314\n",
            "argm 1.1740808486938477 0.22207292914390564\n",
            "argm 1.320331335067749 0.24634134769439697\n",
            "argm 1.6411058902740479 0.18904544413089752\n",
            "argm 1.635425090789795 0.48232823610305786\n",
            "argm 1.7156381607055664 0.2099890261888504\n",
            "argm 1.3830852508544922 0.6325018405914307\n",
            "repr, std, cov, closslb 1.7406076192855835 0.4853515625 0.00026094214990735054 0.015730440616607666 0.03905252367258072\n",
            "0.07301461365661277 0.12308865063792547 1.0\n",
            "argm 1.6326172351837158 0.15850737690925598\n",
            "argm 1.5733251571655273 0.476441353559494\n",
            "argm 1.7329317331314087 0.18405821919441223\n",
            "argm 1.505178451538086 0.2338942140340805\n",
            "argm 1.0985468626022339 0.4013199508190155\n",
            "argm 1.2475415468215942 0.41218286752700806\n",
            "argm 1.4874227046966553 0.5966686010360718\n",
            "argm 1.627131462097168 0.08578039705753326\n",
            "argm 1.6154496669769287 0.05258247256278992\n",
            "argm 1.7017498016357422 0.09858235716819763\n",
            "argm 1.2735111713409424 0.7477205991744995\n",
            "argm 1.0554317235946655 0.33535993099212646\n",
            "argm 1.204587697982788 0.16536656022071838\n",
            "argm 1.3758478164672852 0.3517884910106659\n",
            "argm 1.6349750757217407 0.1232614740729332\n",
            "argm 1.6107302904129028 0.3361813724040985\n",
            "argm 1.7276053428649902 0.06932951509952545\n",
            "argm 1.344861626625061 0.6607387661933899\n",
            "argm 1.080255150794983 0.11041481047868729\n",
            "argm 1.1638786792755127 0.3372933864593506\n",
            "argm 1.306572675704956 0.2044653594493866\n",
            "argm 1.631725549697876 0.34426265954971313\n",
            "argm 1.6246978044509888 0.5149813294410706\n",
            "argm 1.7142928838729858 0.09078718721866608\n",
            "argm 1.469921350479126 0.4210040271282196\n",
            "repr, std, cov, closslb 1.640404224395752 0.485107421875 0.00023985188454389572 6.0990380006842315e-05 0.04987845942378044\n",
            "0.07200003488482175 0.12358174426485864 1.0\n",
            "argm 1.6625123023986816 0.18335413932800293\n",
            "argm 1.6008424758911133 0.4615858197212219\n",
            "argm 1.7714632749557495 0.2079131007194519\n",
            "argm 1.5287284851074219 0.27521979808807373\n",
            "argm 1.1135594844818115 0.4856530427932739\n",
            "argm 1.2653188705444336 0.400448203086853\n",
            "argm 1.5289998054504395 0.567007839679718\n",
            "argm 1.6550958156585693 0.13881829380989075\n",
            "argm 1.6595336198806763 0.05592126399278641\n",
            "argm 1.7367202043533325 0.17092376947402954\n",
            "argm 1.2620216608047485 0.8457022905349731\n",
            "argm 1.076643943786621 0.5618752837181091\n",
            "argm 1.2317609786987305 0.11802557855844498\n",
            "argm 1.457890510559082 0.37601494789123535\n",
            "argm 1.6800012588500977 0.13217422366142273\n",
            "argm 1.6614470481872559 0.1387903392314911\n",
            "argm 1.7238930463790894 0.074777752161026\n",
            "argm 1.2772719860076904 0.8092383146286011\n",
            "argm 1.0801827907562256 0.43910837173461914\n",
            "argm 1.216591238975525 0.20839087665081024\n",
            "argm 1.4343340396881104 0.29660481214523315\n",
            "argm 1.690276026725769 0.1142062246799469\n",
            "argm 1.6784265041351318 0.36676135659217834\n",
            "argm 1.7443801164627075 0.1377146691083908\n",
            "argm 1.3190292119979858 0.8869271278381348\n",
            "repr, std, cov, closslb 1.6151643991470337 0.484375 0.000392416724935174 0.01559453271329403 0.0648028701543808\n",
            "0.07128397874922085 0.1246984393024892 1.0\n",
            "argm 1.6678242683410645 0.19186455011367798\n",
            "argm 1.6138360500335693 0.4577217698097229\n",
            "argm 1.7841953039169312 0.232203871011734\n",
            "argm 1.532238245010376 0.2904357314109802\n",
            "argm 1.1157506704330444 0.500642716884613\n",
            "argm 1.2746739387512207 0.358725905418396\n",
            "argm 1.5485713481903076 0.5773091316223145\n",
            "argm 1.678856372833252 0.1337304711341858\n",
            "argm 1.6994481086730957 0.05263276398181915\n",
            "argm 1.7609856128692627 0.2312399446964264\n",
            "argm 1.271217942237854 0.8398444652557373\n",
            "argm 1.0947985649108887 0.62199866771698\n",
            "argm 1.2449275255203247 0.09535811841487885\n",
            "argm 1.503793478012085 0.4199109971523285\n",
            "argm 1.726313829421997 0.11228956282138824\n",
            "argm 1.7381231784820557 0.09435229003429413\n",
            "argm 1.7455029487609863 0.14886818826198578\n",
            "argm 1.2718205451965332 0.8214317560195923\n",
            "argm 1.101728916168213 0.5836347341537476\n",
            "argm 1.2410922050476074 0.1611504703760147\n",
            "argm 1.5061211585998535 0.37318187952041626\n",
            "argm 1.7225596904754639 0.16094091534614563\n",
            "argm 1.7115328311920166 0.2909693717956543\n",
            "argm 1.7447484731674194 0.2280212938785553\n",
            "argm 1.290165901184082 0.9125841856002808\n",
            "repr, std, cov, closslb 1.6668860912322998 0.482177734375 0.0006902450695633888 5.5756721849320456e-05 0.05038794130086899\n",
            "0.07015307181781205 0.12482313774179167 1.0\n",
            "argm 1.6427794694900513 0.18191245198249817\n",
            "argm 1.5659453868865967 0.45048537850379944\n",
            "argm 1.7380107641220093 0.22381916642189026\n",
            "argm 1.4979851245880127 0.28191134333610535\n",
            "argm 1.0989155769348145 0.5418089628219604\n",
            "argm 1.2600274085998535 0.35829001665115356\n",
            "argm 1.5257736444473267 0.615527868270874\n",
            "argm 1.6322124004364014 0.09345822036266327\n",
            "argm 1.6476833820343018 0.04143174737691879\n",
            "argm 1.6994351148605347 0.24505051970481873\n",
            "argm 1.2218016386032104 0.857210636138916\n",
            "argm 1.0780915021896362 0.6408803462982178\n",
            "argm 1.2307114601135254 0.11910684406757355\n",
            "argm 1.4899441003799438 0.4794895648956299\n",
            "argm 1.6717197895050049 0.11750590056180954\n",
            "argm 1.6629767417907715 0.06280560046434402\n",
            "argm 1.6631999015808105 0.18967968225479126\n",
            "argm 1.2014245986938477 0.8292285203933716\n",
            "argm 1.0922621488571167 0.713560938835144\n",
            "argm 1.2252781391143799 0.22068992257118225\n",
            "argm 1.512428641319275 0.3568260073661804\n",
            "argm 1.6870840787887573 0.15798769891262054\n",
            "argm 1.6921364068984985 0.08782278746366501\n",
            "argm 1.6539796590805054 0.19844713807106018\n",
            "argm 1.1859452724456787 0.7664397954940796\n",
            "repr, std, cov, closslb 1.583798885345459 0.486328125 0.00016266154125332832 7.90396734373644e-05 0.04700637608766556\n",
            "0.070363741562634 0.12759827406093888 1.0\n",
            "argm 1.6452134847640991 0.20202621817588806\n",
            "argm 1.581223726272583 0.44294726848602295\n",
            "argm 1.7679250240325928 0.21608546376228333\n",
            "argm 1.5207295417785645 0.32313072681427\n",
            "argm 1.1094642877578735 0.5908263921737671\n",
            "argm 1.2736921310424805 0.3789709806442261\n",
            "argm 1.538210153579712 0.6543583869934082\n",
            "argm 1.6459300518035889 0.12623704969882965\n",
            "argm 1.6768648624420166 0.047042034566402435\n",
            "argm 1.7125216722488403 0.2759881317615509\n",
            "argm 1.2121691703796387 0.8578231334686279\n",
            "argm 1.0768067836761475 0.6054583787918091\n",
            "argm 1.2304627895355225 0.13883225619792938\n",
            "argm 1.4863636493682861 0.5628979206085205\n",
            "argm 1.655114769935608 0.10001007467508316\n",
            "argm 1.6716725826263428 0.06499607115983963\n",
            "argm 1.6635737419128418 0.29018428921699524\n",
            "argm 1.1906371116638184 0.8754855394363403\n",
            "argm 1.0954183340072632 0.646709680557251\n",
            "argm 1.2305138111114502 0.18026399612426758\n",
            "argm 1.5132427215576172 0.4677520990371704\n",
            "argm 1.674560546875 0.14710518717765808\n",
            "argm 1.7019867897033691 0.08769426494836807\n",
            "argm 1.630340576171875 0.2998570203781128\n",
            "argm 1.1729726791381836 0.820511519908905\n",
            "repr, std, cov, closslb 1.512660026550293 0.4833984375 0.00044980505481362343 0.0001209400943480432 0.07198303937911987\n",
            "0.07015307181781205 0.1296552299266217 1.0\n",
            "argm 1.687135100364685 0.21228286623954773\n",
            "argm 1.6308001279830933 0.4569590091705322\n",
            "argm 1.8065872192382812 0.2238331437110901\n",
            "argm 1.542229413986206 0.34530365467071533\n",
            "argm 1.1204533576965332 0.6468258500099182\n",
            "argm 1.2923836708068848 0.38275960087776184\n",
            "argm 1.5847251415252686 0.7091928720474243\n",
            "argm 1.6958186626434326 0.12974068522453308\n",
            "argm 1.7097234725952148 0.04811212420463562\n",
            "argm 1.7425930500030518 0.2712216377258301\n",
            "argm 1.2385849952697754 0.9186832308769226\n",
            "argm 1.0931743383407593 0.599169135093689\n",
            "argm 1.24948251247406 0.14023762941360474\n",
            "argm 1.5197792053222656 0.5869539380073547\n",
            "argm 1.6846599578857422 0.09992066770792007\n",
            "argm 1.7013568878173828 0.06103981286287308\n",
            "argm 1.6857690811157227 0.2965182065963745\n",
            "argm 1.200576663017273 0.9220551252365112\n",
            "argm 1.110694169998169 0.5633337497711182\n",
            "argm 1.2587754726409912 0.18119719624519348\n",
            "argm 1.5710220336914062 0.468523234128952\n",
            "argm 1.7164173126220703 0.12791621685028076\n",
            "argm 1.7549214363098145 0.14886260032653809\n",
            "argm 1.652633786201477 0.24908500909805298\n",
            "argm 1.182800054550171 0.8384572267532349\n",
            "repr, std, cov, closslb 1.60365891456604 0.4833984375 0.0004328025970607996 0.015526305884122849 0.04789372906088829\n",
            "0.06945538413492752 0.1306961093822485 1.0\n",
            "2\n",
            "argm 1.6129215955734253 0.14580878615379333\n",
            "argm 1.5439872741699219 0.43165403604507446\n",
            "argm 1.7280011177062988 0.2232016921043396\n",
            "argm 1.4967608451843262 0.28816425800323486\n",
            "argm 1.0957658290863037 0.6000140905380249\n",
            "argm 1.256330966949463 0.30776673555374146\n",
            "argm 1.512030839920044 0.6679733991622925\n",
            "argm 1.6339253187179565 0.09572692215442657\n",
            "argm 1.6403651237487793 0.06802473217248917\n",
            "argm 1.7185444831848145 0.19537657499313354\n",
            "argm 1.2539833784103394 0.8807806968688965\n",
            "argm 1.0764356851577759 0.5096336603164673\n",
            "argm 1.2386407852172852 0.09859073907136917\n",
            "argm 1.467073917388916 0.4682689905166626\n",
            "argm 1.6503829956054688 0.12340396642684937\n",
            "argm 1.6279325485229492 0.09208079427480698\n",
            "argm 1.6914327144622803 0.09799282997846603\n",
            "argm 1.2351323366165161 0.8996669054031372\n",
            "argm 1.0636056661605835 0.4870668053627014\n",
            "argm 1.207690715789795 0.11836644262075424\n",
            "argm 1.4507133960723877 0.3763139247894287\n",
            "argm 1.6415047645568848 0.15499815344810486\n",
            "argm 1.6390578746795654 0.10724924504756927\n",
            "argm 1.6742810010910034 0.1050001010298729\n",
            "argm 1.2146434783935547 0.8302087783813477\n",
            "repr, std, cov, closslb 1.4216679334640503 0.48486328125 0.00028410088270902634 0.015652185305953026 0.06119580194354057\n",
            "0.06910914661892964 0.13240539057345282 1.0\n",
            "argm 1.637662649154663 0.17922744154930115\n",
            "argm 1.5712376832962036 0.4287399351596832\n",
            "argm 1.7469916343688965 0.2329498529434204\n",
            "argm 1.5092589855194092 0.29973405599594116\n",
            "argm 1.1039535999298096 0.6790403127670288\n",
            "argm 1.2693901062011719 0.3428282141685486\n",
            "argm 1.5419480800628662 0.7105674743652344\n",
            "argm 1.6329998970031738 0.10961014777421951\n",
            "argm 1.6454436779022217 0.09296368807554245\n",
            "argm 1.7018438577651978 0.22505410015583038\n",
            "argm 1.2074251174926758 0.9261078834533691\n",
            "argm 1.0709139108657837 0.590583324432373\n",
            "argm 1.2401175498962402 0.16497820615768433\n",
            "argm 1.5220048427581787 0.5582543015480042\n",
            "argm 1.6417627334594727 0.1703733503818512\n",
            "argm 1.6773920059204102 0.15289150178432465\n",
            "argm 1.6425632238388062 0.15382468700408936\n",
            "argm 1.15541672706604 0.6967039108276367\n",
            "argm 1.1021748781204224 0.638664722442627\n",
            "argm 1.2435988187789917 0.2687070369720459\n",
            "argm 1.5657808780670166 0.3991965353488922\n",
            "argm 1.618468999862671 0.32190418243408203\n",
            "argm 1.7019593715667725 0.12020207941532135\n",
            "argm 1.526892900466919 0.3563985228538513\n",
            "argm 1.1148616075515747 0.2634902000427246\n",
            "repr, std, cov, closslb 1.5313167572021484 0.483154296875 0.0004748741630464792 0.03182034194469452 0.024830922484397888\n",
            "0.06904010651241722 0.13453983985807252 1.0\n",
            "argm 1.6663572788238525 0.20267441868782043\n",
            "argm 1.6000858545303345 0.44189953804016113\n",
            "argm 1.777449131011963 0.23694801330566406\n",
            "argm 1.5190763473510742 0.3624753952026367\n",
            "argm 1.1044151782989502 0.6617093682289124\n",
            "argm 1.2676455974578857 0.30894020199775696\n",
            "argm 1.5356814861297607 0.7338495254516602\n",
            "argm 1.6502726078033447 0.09736418724060059\n",
            "argm 1.665877342224121 0.05423650145530701\n",
            "argm 1.7304271459579468 0.2436283975839615\n",
            "argm 1.2333340644836426 0.9146554470062256\n",
            "argm 1.071037769317627 0.5618473291397095\n",
            "argm 1.2453261613845825 0.07571373134851456\n",
            "argm 1.514944076538086 0.6509749293327332\n",
            "argm 1.6621699333190918 0.10610930621623993\n",
            "argm 1.6853139400482178 0.06050616502761841\n",
            "argm 1.7046542167663574 0.3100913166999817\n",
            "argm 1.2118370532989502 0.9372324347496033\n",
            "argm 1.0858745574951172 0.5791922807693481\n",
            "argm 1.2445294857025146 0.07856637239456177\n",
            "argm 1.536367654800415 0.5714781284332275\n",
            "argm 1.6627833843231201 0.1100907102227211\n",
            "argm 1.7078146934509277 0.15987363457679749\n",
            "argm 1.6548149585723877 0.34807249903678894\n",
            "argm 1.1867859363555908 0.9170718193054199\n",
            "repr, std, cov, closslb 1.7107806205749512 0.485107421875 0.00020377431064844131 6.181110074976459e-05 0.04327429085969925\n",
            "0.0692474340213141 0.1371192340201407 1.0\n",
            "argm 1.6123908758163452 0.17811544239521027\n",
            "argm 1.5260310173034668 0.43008941411972046\n",
            "argm 1.7137112617492676 0.23191329836845398\n",
            "argm 1.4742506742477417 0.36543700098991394\n",
            "argm 1.0834009647369385 0.6338087320327759\n",
            "argm 1.2533891201019287 0.2595875561237335\n",
            "argm 1.5091099739074707 0.7528876662254333\n",
            "argm 1.60887610912323 0.08585862815380096\n",
            "argm 1.6324814558029175 0.060587190091609955\n",
            "argm 1.688398838043213 0.25649183988571167\n",
            "argm 1.2046313285827637 0.890074610710144\n",
            "argm 1.0625966787338257 0.5613388419151306\n",
            "argm 1.2269785404205322 0.11587142944335938\n",
            "argm 1.4917956590652466 0.6230632066726685\n",
            "argm 1.613485336303711 0.11317245662212372\n",
            "argm 1.6471538543701172 0.06657745689153671\n",
            "argm 1.6197139024734497 0.41143131256103516\n",
            "argm 1.1429741382598877 0.8845129013061523\n",
            "argm 1.0903279781341553 0.5126539468765259\n",
            "argm 1.2476316690444946 0.16596727073192596\n",
            "argm 1.5447436571121216 0.4401895999908447\n",
            "argm 1.6258212327957153 0.14583952724933624\n",
            "argm 1.6965417861938477 0.29081010818481445\n",
            "argm 1.562711477279663 0.3991658091545105\n",
            "argm 1.1111202239990234 0.7642273902893066\n",
            "repr, std, cov, closslb 1.6640572547912598 0.48583984375 0.00017953081987798214 4.066748442710377e-05 0.03422483429312706\n",
            "0.06910914661892964 0.13919048262684638 1.0\n",
            "argm 1.6198080778121948 0.15225447714328766\n",
            "argm 1.524517297744751 0.3810664415359497\n",
            "argm 1.7102237939834595 0.2308012843132019\n",
            "argm 1.467718243598938 0.3574853539466858\n",
            "argm 1.0814054012298584 0.6789464950561523\n",
            "argm 1.252695083618164 0.28606876730918884\n",
            "argm 1.4986366033554077 0.7359143495559692\n",
            "argm 1.5816148519515991 0.0782255083322525\n",
            "argm 1.59590482711792 0.050710514187812805\n",
            "argm 1.648259162902832 0.24308077991008759\n",
            "argm 1.1828314065933228 0.9070405960083008\n",
            "argm 1.0657627582550049 0.5791475772857666\n",
            "argm 1.2224559783935547 0.17528516054153442\n",
            "argm 1.4803647994995117 0.5753533840179443\n",
            "argm 1.5844228267669678 0.11120829731225967\n",
            "argm 1.6300395727157593 0.07873121649026871\n",
            "argm 1.5762990713119507 0.3320518732070923\n",
            "argm 1.1176996231079102 0.6668285131454468\n",
            "argm 1.0949152708053589 0.4763910472393036\n",
            "argm 1.243941307067871 0.23231002688407898\n",
            "argm 1.5446534156799316 0.48205164074897766\n",
            "argm 1.5753921270370483 0.36721956729888916\n",
            "argm 1.6677165031433105 0.08195266127586365\n",
            "argm 1.463042140007019 0.46791136264801025\n",
            "argm 1.0750402212142944 0.17673522233963013\n",
            "repr, std, cov, closslb 1.5070090293884277 0.486083984375 0.00017387582920491695 0.015382455661892891 0.034062616527080536\n",
            "0.06883339974415216 0.14129301838900796 1.0\n",
            "argm 1.614154577255249 0.15608501434326172\n",
            "argm 1.5387842655181885 0.3584185540676117\n",
            "argm 1.7337695360183716 0.22494512796401978\n",
            "argm 1.4797391891479492 0.3842683434486389\n",
            "argm 1.0833755731582642 0.6930604577064514\n",
            "argm 1.2480584383010864 0.2676229774951935\n",
            "argm 1.5010130405426025 0.7211593985557556\n",
            "argm 1.6097980737686157 0.11508911848068237\n",
            "argm 1.6352589130401611 0.05958694964647293\n",
            "argm 1.7020021677017212 0.20237547159194946\n",
            "argm 1.2330739498138428 0.9089245200157166\n",
            "argm 1.064800500869751 0.5999737977981567\n",
            "argm 1.222031831741333 0.10154955089092255\n",
            "argm 1.4686731100082397 0.6175506711006165\n",
            "argm 1.6064889430999756 0.1881485879421234\n",
            "argm 1.6334611177444458 0.1693982630968094\n",
            "argm 1.6608426570892334 0.11929403990507126\n",
            "argm 1.1858457326889038 0.8775213956832886\n",
            "argm 1.0678991079330444 0.6627403497695923\n",
            "argm 1.2260751724243164 0.15400630235671997\n",
            "argm 1.5317761898040771 0.5440804958343506\n",
            "argm 1.5930131673812866 0.40673744678497314\n",
            "argm 1.6597793102264404 0.16827508807182312\n",
            "argm 1.5684397220611572 0.29900482296943665\n",
            "argm 1.1314027309417725 0.3483295440673828\n",
            "repr, std, cov, closslb 1.6750215291976929 0.483154296875 0.00048351334407925606 0.00017701703472994268 0.030745025724172592\n",
            "0.06780911215907992 0.14143431140739696 1.0\n",
            "argm 1.6341321468353271 0.15906336903572083\n",
            "argm 1.5550644397735596 0.3555296063423157\n",
            "argm 1.7489879131317139 0.21568593382835388\n",
            "argm 1.4946637153625488 0.4284018874168396\n",
            "argm 1.0925310850143433 0.7471472024917603\n",
            "argm 1.2635693550109863 0.2584001123905182\n",
            "argm 1.5575754642486572 0.7309354543685913\n",
            "argm 1.6473859548568726 0.12698861956596375\n",
            "argm 1.6886190176010132 0.07150880992412567\n",
            "argm 1.6967167854309082 0.39835554361343384\n",
            "argm 1.1763324737548828 0.9230256676673889\n",
            "argm 1.0953987836837769 0.5725538730621338\n",
            "argm 1.2507140636444092 0.07165968418121338\n",
            "argm 1.5727827548980713 0.7339217066764832\n",
            "argm 1.6412353515625 0.2527562975883484\n",
            "argm 1.7202409505844116 0.0899098813533783\n",
            "argm 1.6048192977905273 0.3898981809616089\n",
            "argm 1.1344037055969238 0.5937678813934326\n",
            "argm 1.1217050552368164 0.5169047117233276\n",
            "argm 1.2831937074661255 0.1385137438774109\n",
            "argm 1.6234997510910034 0.2751694917678833\n",
            "argm 1.6184765100479126 0.3860425353050232\n",
            "argm 1.7290863990783691 0.12242048978805542\n",
            "argm 1.5027295351028442 0.5994105339050293\n",
            "argm 1.0940321683883667 0.18040090799331665\n",
            "repr, std, cov, closslb 1.5330893993377686 0.48095703125 0.0007630039472132921 0.015260163694620132 0.030930697917938232\n",
            "0.06720187040467035 0.14256975398960517 1.0\n",
            "argm 1.635230541229248 0.14548468589782715\n",
            "argm 1.5607349872589111 0.3398889899253845\n",
            "argm 1.7517926692962646 0.20333659648895264\n",
            "argm 1.4894145727157593 0.4294077157974243\n",
            "argm 1.0806397199630737 0.6975950002670288\n",
            "argm 1.255676031112671 0.23363438248634338\n",
            "argm 1.5260703563690186 0.7229335904121399\n",
            "argm 1.645780086517334 0.10396633297204971\n",
            "argm 1.6658952236175537 0.05161575973033905\n",
            "argm 1.7076271772384644 0.3161570131778717\n",
            "argm 1.2199766635894775 0.9484398365020752\n",
            "argm 1.085995078086853 0.5396828055381775\n",
            "argm 1.2465813159942627 0.05311332643032074\n",
            "argm 1.512801170349121 0.7623648643493652\n",
            "argm 1.6463096141815186 0.10916870087385178\n",
            "argm 1.6752197742462158 0.06555486470460892\n",
            "argm 1.650881052017212 0.39016640186309814\n",
            "argm 1.1660736799240112 0.8921102285385132\n",
            "argm 1.084291934967041 0.5766833424568176\n",
            "argm 1.2387436628341675 0.11973269283771515\n",
            "argm 1.5444402694702148 0.5687819719314575\n",
            "argm 1.6436808109283447 0.16761571168899536\n",
            "argm 1.7011946439743042 0.10779127478599548\n",
            "argm 1.5556941032409668 0.39482396841049194\n",
            "argm 1.1025469303131104 0.41197559237480164\n",
            "repr, std, cov, closslb 1.7258517742156982 0.484130859375 0.000258769141510129 0.01575453206896782 0.014217160642147064\n",
            "0.06713473566900136 0.14472333518789257 1.0\n",
            "argm 1.636046290397644 0.16505084931850433\n",
            "argm 1.5584194660186768 0.33681002259254456\n",
            "argm 1.7573332786560059 0.21605193614959717\n",
            "argm 1.492533564567566 0.486910343170166\n",
            "argm 1.0963513851165771 0.7625296711921692\n",
            "argm 1.266327142715454 0.27937161922454834\n",
            "argm 1.5418769121170044 0.7164543271064758\n",
            "argm 1.6353739500045776 0.10921340435743332\n",
            "argm 1.6606965065002441 0.06439536809921265\n",
            "argm 1.691564917564392 0.3672139644622803\n",
            "argm 1.1831763982772827 0.9565008878707886\n",
            "argm 1.0808666944503784 0.5620093941688538\n",
            "argm 1.2350932359695435 0.13141147792339325\n",
            "argm 1.5234065055847168 0.7112799286842346\n",
            "argm 1.6371936798095703 0.14113448560237885\n",
            "argm 1.680734395980835 0.08927565068006516\n",
            "argm 1.6052799224853516 0.5454440116882324\n",
            "argm 1.1167998313903809 0.7398544549942017\n",
            "argm 1.0996276140213013 0.5088289976119995\n",
            "argm 1.243295431137085 0.19164103269577026\n",
            "argm 1.5704041719436646 0.5592014789581299\n",
            "argm 1.6235103607177734 0.3681538701057434\n",
            "argm 1.7079105377197266 0.21998025476932526\n",
            "argm 1.4662501811981201 0.6953185796737671\n",
            "argm 1.0766372680664062 0.3487626314163208\n",
            "repr, std, cov, closslb 1.535597562789917 0.483642578125 0.00028329133056104183 0.00010330945951864123 0.012393655255436897\n",
            "0.06720187040467035 0.1472034130168788 1.0\n",
            "argm 1.6189310550689697 0.14177429676055908\n",
            "argm 1.5355639457702637 0.2961689531803131\n",
            "argm 1.7384979724884033 0.2000676393508911\n",
            "argm 1.4763820171356201 0.49242284893989563\n",
            "argm 1.088606834411621 0.8058966398239136\n",
            "argm 1.2586390972137451 0.31199121475219727\n",
            "argm 1.5407509803771973 0.6974050998687744\n",
            "argm 1.6215965747833252 0.11268351227045059\n",
            "argm 1.6734585762023926 0.08327700197696686\n",
            "argm 1.65354323387146 0.45522674918174744\n",
            "argm 1.1395883560180664 0.8512735366821289\n",
            "argm 1.093233346939087 0.5921311974525452\n",
            "argm 1.2388863563537598 0.18969643115997314\n",
            "argm 1.557478666305542 0.665464460849762\n",
            "argm 1.6175107955932617 0.2801874577999115\n",
            "argm 1.703979253768921 0.1204395666718483\n",
            "argm 1.5270849466323853 0.6015552282333374\n",
            "argm 1.0812678337097168 0.4432277977466583\n",
            "argm 1.1206889152526855 0.48279595375061035\n",
            "argm 1.2797338962554932 0.1914147287607193\n",
            "argm 1.593428134918213 0.4226630628108978\n",
            "argm 1.5956214666366577 0.3481982350349426\n",
            "argm 1.7053205966949463 0.15843194723129272\n",
            "argm 1.4112628698349 0.8551024198532104\n",
            "argm 1.0663721561431885 0.3700498342514038\n",
            "repr, std, cov, closslb 1.4500453472137451 0.482177734375 0.000530594028532505 9.165857045445591e-05 0.05250893533229828\n",
            "0.06686686673333332 0.1491285814579296 1.0\n",
            "argm 1.6098172664642334 0.14932361245155334\n",
            "argm 1.5333510637283325 0.29611867666244507\n",
            "argm 1.7274699211120605 0.20010116696357727\n",
            "argm 1.453019380569458 0.5146600008010864\n",
            "argm 1.06733238697052 0.8240797519683838\n",
            "argm 1.2466187477111816 0.29480549693107605\n",
            "argm 1.5148910284042358 0.7106037735939026\n",
            "argm 1.577911376953125 0.125957652926445\n",
            "argm 1.6423444747924805 0.056276097893714905\n",
            "argm 1.64359712600708 0.46752578020095825\n",
            "argm 1.140947937965393 0.8679932355880737\n",
            "argm 1.0920228958129883 0.6202564239501953\n",
            "argm 1.2336633205413818 0.23603719472885132\n",
            "argm 1.5347115993499756 0.6197468042373657\n",
            "argm 1.56280517578125 0.3175455927848816\n",
            "argm 1.6704638004302979 0.05758926272392273\n",
            "argm 1.5184773206710815 0.5354862809181213\n",
            "argm 1.0780103206634521 0.38420408964157104\n",
            "argm 1.115938425064087 0.6119096279144287\n",
            "argm 1.2879643440246582 0.27208495140075684\n",
            "argm 1.589958667755127 0.39769336581230164\n",
            "argm 1.5610032081604004 0.3040730953216553\n",
            "argm 1.6864445209503174 0.069446861743927\n",
            "argm 1.40218186378479 0.8233456611633301\n",
            "argm 1.0413789749145508 0.20658038556575775\n",
            "repr, std, cov, closslb 1.3848237991333008 0.483642578125 0.00035112304612994194 0.015546631067991257 0.014525143429636955\n",
            "0.06666666666666667 0.15123000673163342 1.0\n",
            "argm 1.6663442850112915 0.15568825602531433\n",
            "argm 1.600066900253296 0.30744820833206177\n",
            "argm 1.7943278551101685 0.19687694311141968\n",
            "argm 1.5118422508239746 0.5959309935569763\n",
            "argm 1.0933523178100586 0.8317340612411499\n",
            "argm 1.268258810043335 0.2381661832332611\n",
            "argm 1.5584551095962524 0.7272334098815918\n",
            "argm 1.6603554487228394 0.1265108585357666\n",
            "argm 1.7022578716278076 0.05528423935174942\n",
            "argm 1.7036991119384766 0.44919735193252563\n",
            "argm 1.1600117683410645 0.9048424363136292\n",
            "argm 1.0951836109161377 0.6050560474395752\n",
            "argm 1.2470717430114746 0.14351773262023926\n",
            "argm 1.5750197172164917 0.6449651122093201\n",
            "argm 1.6372886896133423 0.39113593101501465\n",
            "argm 1.7302380800247192 0.055898912250995636\n",
            "argm 1.565443992614746 0.5675749778747559\n",
            "argm 1.0903418064117432 0.31601452827453613\n",
            "argm 1.1447629928588867 0.5353074669837952\n",
            "argm 1.3173859119415283 0.3695636987686157\n",
            "argm 1.6330430507659912 0.22744014859199524\n",
            "argm 1.6209700107574463 0.31681638956069946\n",
            "argm 1.7559170722961426 0.093117356300354\n",
            "argm 1.3832471370697021 0.8935885429382324\n",
            "argm 1.0345393419265747 0.1540258526802063\n",
            "repr, std, cov, closslb 1.40690279006958 0.4833984375 0.00028704479336738586 0.015319261699914932 0.041898421943187714\n",
            "0.06666666666666667 0.1538215873242561 1.0\n",
            "argm 1.6290769577026367 0.16436633467674255\n",
            "argm 1.545508623123169 0.31685829162597656\n",
            "argm 1.748117446899414 0.18291547894477844\n",
            "argm 1.4820506572723389 0.6018094420433044\n",
            "argm 1.0917894840240479 0.8156503438949585\n",
            "argm 1.2637054920196533 0.21588149666786194\n",
            "argm 1.5468112230300903 0.7448997497558594\n",
            "argm 1.6232709884643555 0.09647011756896973\n",
            "argm 1.6745884418487549 0.060986727476119995\n",
            "argm 1.648288607597351 0.5606822967529297\n",
            "argm 1.1306724548339844 0.8244866132736206\n",
            "argm 1.1049296855926514 0.5133049488067627\n",
            "argm 1.2480320930480957 0.0979006290435791\n",
            "argm 1.5809264183044434 0.6257062554359436\n",
            "argm 1.6116864681243896 0.33706986904144287\n",
            "argm 1.7152332067489624 0.1287795603275299\n",
            "argm 1.4749045372009277 0.7505921721458435\n",
            "argm 1.0509400367736816 0.3230329751968384\n",
            "argm 1.1597634553909302 0.1898864209651947\n",
            "argm 1.3282356262207031 0.4772990942001343\n",
            "argm 1.6133171319961548 0.10694749653339386\n",
            "argm 1.6428946256637573 0.1896098256111145\n",
            "argm 1.7169849872589111 0.09119231253862381\n",
            "argm 1.270257592201233 0.9316157102584839\n",
            "argm 1.0375216007232666 0.4983963370323181\n",
            "repr, std, cov, closslb 1.2651822566986084 0.482177734375 0.00044463714584708214 5.562262231251225e-05 0.02224329672753811\n",
            "0.06660006660006661 0.15630127773264016 1.0\n",
            "argm 1.6004034280776978 0.18323677778244019\n",
            "argm 1.514650583267212 0.31446385383605957\n",
            "argm 1.7188047170639038 0.19507762789726257\n",
            "argm 1.4590238332748413 0.5908777713775635\n",
            "argm 1.0675190687179565 0.8334786295890808\n",
            "argm 1.2445049285888672 0.23500341176986694\n",
            "argm 1.5236786603927612 0.745182991027832\n",
            "argm 1.5910990238189697 0.11394359171390533\n",
            "argm 1.6608052253723145 0.07058121263980865\n",
            "argm 1.638411521911621 0.5926675796508789\n",
            "argm 1.108250617980957 0.7714608907699585\n",
            "argm 1.099478006362915 0.5253525376319885\n",
            "argm 1.2450859546661377 0.14683976769447327\n",
            "argm 1.5677303075790405 0.6708216071128845\n",
            "argm 1.5798637866973877 0.29581692814826965\n",
            "argm 1.6831883192062378 0.1535201370716095\n",
            "argm 1.4194285869598389 0.7171120643615723\n",
            "argm 1.0223112106323242 0.17426255345344543\n",
            "argm 1.135301113128662 0.12374203652143478\n",
            "argm 1.301754355430603 0.3638807535171509\n",
            "argm 1.5572209358215332 0.23626908659934998\n",
            "argm 1.580422043800354 0.0928044319152832\n",
            "argm 1.6308547258377075 0.3426354229450226\n",
            "argm 1.1463193893432617 0.9870375990867615\n",
            "argm 1.0203804969787598 0.3411266803741455\n",
            "repr, std, cov, closslb 1.2870090007781982 0.482421875 0.0003981215413659811 0.00019438630260992795 0.020041365176439285\n",
            "0.06673333333333333 0.15913874284878704 1.0\n",
            "argm 1.5431904792785645 0.18058252334594727\n",
            "argm 1.4497116804122925 0.2716127634048462\n",
            "argm 1.6482360363006592 0.19722618162631989\n",
            "argm 1.3951306343078613 0.6280012130737305\n",
            "argm 1.0421007871627808 0.8546457290649414\n",
            "argm 1.2149279117584229 0.31992605328559875\n",
            "argm 1.4760655164718628 0.720941424369812\n",
            "argm 1.5234107971191406 0.23008882999420166\n",
            "argm 1.596226692199707 0.050788745284080505\n",
            "argm 1.5336875915527344 0.5403841137886047\n",
            "argm 1.0498201847076416 0.5137670636177063\n",
            "argm 1.0686547756195068 0.5637779831886292\n",
            "argm 1.2168498039245605 0.25767087936401367\n",
            "argm 1.5111074447631836 0.4445873200893402\n",
            "argm 1.4939260482788086 0.47340989112854004\n",
            "argm 1.6319681406021118 0.1186346635222435\n",
            "argm 1.3260067701339722 0.6648229360580444\n",
            "argm 0.9774105548858643 0.22315141558647156\n",
            "argm 1.1341772079467773 0.30930620431900024\n",
            "argm 1.3372961282730103 0.597006618976593\n",
            "argm 1.5283293724060059 0.2726716995239258\n",
            "argm 1.5419267416000366 0.18705613911151886\n",
            "argm 1.58378267288208 0.2772621810436249\n",
            "argm 1.0945900678634644 0.9342050552368164\n",
            "argm 1.0389115810394287 0.6790566444396973\n",
            "repr, std, cov, closslb 1.2111663818359375 0.482666015625 0.0004323984030634165 0.03141680732369423 0.01595943793654442\n",
            "0.06633433100465831 0.16089804796146212 1.0\n",
            "argm 1.4925765991210938 0.13861991465091705\n",
            "argm 1.3961153030395508 0.27461349964141846\n",
            "argm 1.6059067249298096 0.17853453755378723\n",
            "argm 1.3611191511154175 0.6370855569839478\n",
            "argm 1.029191255569458 0.8543165922164917\n",
            "argm 1.1998417377471924 0.32322853803634644\n",
            "argm 1.4402213096618652 0.657102108001709\n",
            "argm 1.461850643157959 0.2639405429363251\n",
            "argm 1.5484815835952759 0.048170797526836395\n",
            "argm 1.4778107404708862 0.5963053703308105\n",
            "argm 1.0301799774169922 0.47040635347366333\n",
            "argm 1.073042869567871 0.5160961151123047\n",
            "argm 1.2004342079162598 0.2676257789134979\n",
            "argm 1.480751395225525 0.3677375614643097\n",
            "argm 1.447213888168335 0.45223161578178406\n",
            "argm 1.5666000843048096 0.07404573261737823\n",
            "argm 1.2980128526687622 0.7296044826507568\n",
            "argm 0.9729135632514954 0.31279027462005615\n",
            "argm 1.1258232593536377 0.3114463686943054\n",
            "argm 1.3161669969558716 0.493526428937912\n",
            "argm 1.4640103578567505 0.1814095377922058\n",
            "argm 1.4810595512390137 0.11509191244840622\n",
            "argm 1.5177958011627197 0.20080246031284332\n",
            "argm 1.0588326454162598 0.7717264890670776\n",
            "argm 1.0692307949066162 0.5646486282348633\n",
            "repr, std, cov, closslb 1.1820640563964844 0.48291015625 0.00041519245132803917 0.00011863597319461405 0.011186854913830757\n",
            "0.0660036520475336 0.16283947928550757 1.0\n",
            "argm 1.4817689657211304 0.1365104615688324\n",
            "argm 1.3776793479919434 0.2861134707927704\n",
            "argm 1.5771455764770508 0.18340441584587097\n",
            "argm 1.3424530029296875 0.6729131937026978\n",
            "argm 1.0123345851898193 0.8395348787307739\n",
            "argm 1.1877802610397339 0.3297859728336334\n",
            "argm 1.4352281093597412 0.6458507776260376\n",
            "argm 1.4437695741653442 0.3245919942855835\n",
            "argm 1.5291787385940552 0.046226195991039276\n",
            "argm 1.448491096496582 0.663070023059845\n",
            "argm 1.0004901885986328 0.4323078393936157\n",
            "argm 1.0599827766418457 0.4729097783565521\n",
            "argm 1.1858532428741455 0.30977559089660645\n",
            "argm 1.4562331438064575 0.328305184841156\n",
            "argm 1.419557809829712 0.39917418360710144\n",
            "argm 1.5517749786376953 0.07042195647954941\n",
            "argm 1.2697620391845703 0.8572397232055664\n",
            "argm 0.9681216478347778 0.495320200920105\n",
            "argm 1.1257257461547852 0.22875890135765076\n",
            "argm 1.3152623176574707 0.5584471225738525\n",
            "argm 1.4359052181243896 0.12529268860816956\n",
            "argm 1.4857008457183838 0.06632320582866669\n",
            "argm 1.4942668676376343 0.19604432582855225\n",
            "argm 1.0345535278320312 0.7956512570381165\n",
            "argm 1.0659096240997314 0.5014356374740601\n",
            "repr, std, cov, closslb 1.1517812013626099 0.48193359375 0.0004273683298379183 0.01550293993204832 0.023844575509428978\n",
            "0.06580603645425546 0.1651341098250187 1.0\n",
            "argm 1.4548835754394531 0.15589502453804016\n",
            "argm 1.3561222553253174 0.2664634883403778\n",
            "argm 1.5662932395935059 0.18622912466526031\n",
            "argm 1.3220350742340088 0.7192796468734741\n",
            "argm 1.0085769891738892 0.8541300296783447\n",
            "argm 1.1810956001281738 0.4064692258834839\n",
            "argm 1.428686261177063 0.592031717300415\n",
            "argm 1.4159679412841797 0.37051084637641907\n",
            "argm 1.5227274894714355 0.04637427628040314\n",
            "argm 1.3728482723236084 0.7067452669143677\n",
            "argm 0.9707733392715454 0.29993802309036255\n",
            "argm 1.0717647075653076 0.3439542055130005\n",
            "argm 1.2093079090118408 0.5158894062042236\n",
            "argm 1.441443681716919 0.20396801829338074\n",
            "argm 1.4429686069488525 0.15986524522304535\n",
            "argm 1.5280532836914062 0.06961170583963394\n",
            "argm 1.1336230039596558 0.9674322605133057\n",
            "argm 0.9642864465713501 0.7950917482376099\n",
            "argm 1.1521217823028564 0.28572511672973633\n",
            "argm 1.4204368591308594 0.48180297017097473\n",
            "argm 1.4095019102096558 0.5264338254928589\n",
            "argm 1.513350486755371 0.11591892689466476\n",
            "argm 1.2734644412994385 0.6163582801818848\n",
            "argm 0.940816342830658 0.3170008063316345\n",
            "argm 1.0817822217941284 0.5164313912391663\n",
            "repr, std, cov, closslb 1.4079405069351196 0.48193359375 0.000542044872418046 0.00012231872824486345 0.005803273059427738\n",
            "0.06515158099130984 0.16612739480077626 1.0\n",
            "argm 1.4841482639312744 0.1864330768585205\n",
            "argm 1.4006649255752563 0.27416086196899414\n",
            "argm 1.6020015478134155 0.19451044499874115\n",
            "argm 1.3500480651855469 0.7497601509094238\n",
            "argm 1.0038694143295288 0.8870736956596375\n",
            "argm 1.1764110326766968 0.413021057844162\n",
            "argm 1.437281608581543 0.6298995018005371\n",
            "argm 1.424973964691162 0.36005860567092896\n",
            "argm 1.5595098733901978 0.04452187567949295\n",
            "argm 1.3870298862457275 0.7774254083633423\n",
            "argm 0.9635881185531616 0.2885218560695648\n",
            "argm 1.0737473964691162 0.30905473232269287\n",
            "argm 1.2316474914550781 0.509952187538147\n",
            "argm 1.4509401321411133 0.21027961373329163\n",
            "argm 1.4494884014129639 0.0802231952548027\n",
            "argm 1.5176620483398438 0.1557581126689911\n",
            "argm 1.0832741260528564 0.9728094935417175\n",
            "argm 0.9750205874443054 0.7256057262420654\n",
            "argm 1.1578794717788696 0.4955548942089081\n",
            "argm 1.423855185508728 0.38064175844192505\n",
            "argm 1.4053596258163452 0.33787453174591064\n",
            "argm 1.5330848693847656 0.10912399739027023\n",
            "argm 1.1510869264602661 0.8383504748344421\n",
            "argm 0.9170722961425781 0.7156173586845398\n",
            "argm 1.1085753440856934 0.46845340728759766\n",
            "repr, std, cov, closslb 1.0094311237335205 0.484619140625 0.00016766972839832306 7.42113625165075e-05 0.006740351673215628\n",
            "0.06443919503642037 0.16729378105994913 1.0\n",
            "argm 1.481177568435669 0.17245766520500183\n",
            "argm 1.3942652940750122 0.2798521816730499\n",
            "argm 1.5986673831939697 0.2032918930053711\n",
            "argm 1.3627122640609741 0.8045499324798584\n",
            "argm 1.0367217063903809 0.9307056665420532\n",
            "argm 1.203442096710205 0.4583112895488739\n",
            "argm 1.452955722808838 0.6559604406356812\n",
            "argm 1.4178688526153564 0.44139379262924194\n",
            "argm 1.5650144815444946 0.05107652395963669\n",
            "argm 1.3764526844024658 0.8061692714691162\n",
            "argm 0.9814474582672119 0.2974793314933777\n",
            "argm 1.094236135482788 0.3193337321281433\n",
            "argm 1.2575864791870117 0.5117570757865906\n",
            "argm 1.4597309827804565 0.2160351723432541\n",
            "argm 1.4595060348510742 0.06444845348596573\n",
            "argm 1.5380216836929321 0.20479224622249603\n",
            "argm 1.0850311517715454 0.9790473580360413\n",
            "argm 1.0112918615341187 0.7079495191574097\n",
            "argm 1.1957330703735352 0.4295613765716553\n",
            "argm 1.4419214725494385 0.4518181085586548\n",
            "argm 1.427364468574524 0.21525844931602478\n",
            "argm 1.5441339015960693 0.11791940778493881\n",
            "argm 1.1901869773864746 0.9841358065605164\n",
            "argm 0.9777908325195312 0.609851598739624\n",
            "argm 1.149065375328064 0.30224305391311646\n",
            "repr, std, cov, closslb 0.9913405776023865 0.48291015625 0.000345778651535511 0.000839670654386282 0.016492445021867752\n",
            "0.06367092758500807 0.16830005650141067 1.0\n",
            "argm 1.4860131740570068 0.20378363132476807\n",
            "argm 1.3950016498565674 0.25995633006095886\n",
            "argm 1.604318380355835 0.23050233721733093\n",
            "argm 1.3526625633239746 0.8581914305686951\n",
            "argm 1.029681921005249 0.9474902153015137\n",
            "argm 1.1995837688446045 0.5420660972595215\n",
            "argm 1.4508179426193237 0.6071375608444214\n",
            "argm 1.4050768613815308 0.47468113899230957\n",
            "argm 1.5686869621276855 0.0563962385058403\n",
            "argm 1.2821543216705322 0.8820434808731079\n",
            "argm 0.950461745262146 0.4098275899887085\n",
            "argm 1.1019576787948608 0.2878066301345825\n",
            "argm 1.2990620136260986 0.6632236838340759\n",
            "argm 1.466575264930725 0.2772230803966522\n",
            "argm 1.5060203075408936 0.0837966799736023\n",
            "argm 1.4852263927459717 0.442793607711792\n",
            "argm 0.9787847995758057 0.5798767805099487\n",
            "argm 1.0404341220855713 0.6614489555358887\n",
            "argm 1.2458828687667847 0.7381222248077393\n",
            "argm 1.4419885873794556 0.37811437249183655\n",
            "argm 1.4710873365402222 0.13049784302711487\n",
            "argm 1.4789748191833496 0.29239988327026367\n",
            "argm 1.0169692039489746 0.8838167190551758\n",
            "argm 1.0326334238052368 0.6574513912200928\n",
            "argm 1.2466870546340942 0.6863383054733276\n",
            "repr, std, cov, closslb 1.0372542142868042 0.48046875 0.0006133988499641418 0.01547064445912838 0.04927903413772583\n",
            "0.06284897072602949 0.16897426720112377 1.0\n",
            "argm 1.4681673049926758 0.19484013319015503\n",
            "argm 1.3731615543365479 0.2559441924095154\n",
            "argm 1.5823184251785278 0.23188814520835876\n",
            "argm 1.3245024681091309 0.8385524749755859\n",
            "argm 1.019086241722107 0.941426157951355\n",
            "argm 1.1876078844070435 0.528786301612854\n",
            "argm 1.4258074760437012 0.5408200025558472\n",
            "argm 1.3856594562530518 0.4142839312553406\n",
            "argm 1.5442243814468384 0.06247870624065399\n",
            "argm 1.2526676654815674 0.8981631398200989\n",
            "argm 0.953233003616333 0.540870189666748\n",
            "argm 1.1069177389144897 0.2870997488498688\n",
            "argm 1.2925416231155396 0.7054416537284851\n",
            "argm 1.4351544380187988 0.19244569540023804\n",
            "argm 1.4728920459747314 0.08150003850460052\n",
            "argm 1.424931287765503 0.5253636837005615\n",
            "argm 0.9581634998321533 0.4798052906990051\n",
            "argm 1.046390414237976 0.5866645574569702\n",
            "argm 1.2434947490692139 0.7679500579833984\n",
            "argm 1.4312392473220825 0.29845163226127625\n",
            "argm 1.4711962938308716 0.21009239554405212\n",
            "argm 1.4347355365753174 0.3371257185935974\n",
            "argm 0.9789745211601257 0.7020978927612305\n",
            "argm 1.0339431762695312 0.5958968997001648\n",
            "argm 1.241615891456604 0.8129576444625854\n",
            "repr, std, cov, closslb 0.9548856616020203 0.4833984375 0.0003209812566637993 0.0002673054696060717 0.0054296888411045074\n",
            "0.06297473151645226 0.17221383727061473 1.0\n",
            "argm 1.4946684837341309 0.1990087330341339\n",
            "argm 1.405860424041748 0.27682632207870483\n",
            "argm 1.6093332767486572 0.23427419364452362\n",
            "argm 1.3602339029312134 0.8674153685569763\n",
            "argm 1.0424193143844604 0.9654802083969116\n",
            "argm 1.208429217338562 0.5364334583282471\n",
            "argm 1.4478912353515625 0.5470644235610962\n",
            "argm 1.3983185291290283 0.4281839430332184\n",
            "argm 1.559319257736206 0.052568502724170685\n",
            "argm 1.2702956199645996 0.9195611476898193\n",
            "argm 0.9646335244178772 0.624926745891571\n",
            "argm 1.1171822547912598 0.2782568335533142\n",
            "argm 1.3073610067367554 0.7363306283950806\n",
            "argm 1.4565978050231934 0.13647973537445068\n",
            "argm 1.497222661972046 0.077052041888237\n",
            "argm 1.4599027633666992 0.49257931113243103\n",
            "argm 0.9873857498168945 0.6800070405006409\n",
            "argm 1.0663599967956543 0.5408797264099121\n",
            "argm 1.2547719478607178 0.6924933195114136\n",
            "argm 1.445246934890747 0.2030627727508545\n",
            "argm 1.4758919477462769 0.19438472390174866\n",
            "argm 1.4760769605636597 0.2591656446456909\n",
            "argm 1.0128898620605469 0.8289313912391663\n",
            "argm 1.047059416770935 0.6073857545852661\n",
            "argm 1.2501927614212036 0.7332690954208374\n",
            "repr, std, cov, closslb 0.9548649191856384 0.482177734375 0.0003745872527360916 0.00015550805255770683 0.014484453946352005\n",
            "0.06310074395421668 0.17551551628964565 1.0\n",
            "argm 1.5261013507843018 0.17766281962394714\n",
            "argm 1.4316108226776123 0.2513984143733978\n",
            "argm 1.6289223432540894 0.2327151596546173\n",
            "argm 1.3801307678222656 0.8478078842163086\n",
            "argm 1.0621941089630127 0.9643375873565674\n",
            "argm 1.2259080410003662 0.5363775491714478\n",
            "argm 1.4799506664276123 0.4596719741821289\n",
            "argm 1.4287159442901611 0.4207771420478821\n",
            "argm 1.5950260162353516 0.0609029084444046\n",
            "argm 1.2948968410491943 0.9169991612434387\n",
            "argm 0.9862576127052307 0.6631649732589722\n",
            "argm 1.1502578258514404 0.26807838678359985\n",
            "argm 1.369258165359497 0.8160632848739624\n",
            "argm 1.4849638938903809 0.17395801842212677\n",
            "argm 1.523606300354004 0.07041916251182556\n",
            "argm 1.4608312845230103 0.4825936555862427\n",
            "argm 0.9919531941413879 0.5451852679252625\n",
            "argm 1.0992878675460815 0.49110686779022217\n",
            "argm 1.3105655908584595 0.8392465710639954\n",
            "argm 1.4717543125152588 0.19959546625614166\n",
            "argm 1.5233590602874756 0.13836008310317993\n",
            "argm 1.506084680557251 0.3807647228240967\n",
            "argm 1.020246148109436 0.6994269490242004\n",
            "argm 1.103142261505127 0.4999191164970398\n",
            "argm 1.3094193935394287 0.7262076139450073\n",
            "repr, std, cov, closslb 0.9496237635612488 0.4814453125 0.0005585185717791319 0.015506763011217117 0.020828643813729286\n",
            "0.06253566641163781 0.1762186321501395 1.0\n",
            "argm 1.528122901916504 0.18213596940040588\n",
            "argm 1.434224247932434 0.23515430092811584\n",
            "argm 1.6457232236862183 0.23061968386173248\n",
            "argm 1.4012186527252197 0.8582969903945923\n",
            "argm 1.066342830657959 0.9894527196884155\n",
            "argm 1.2344944477081299 0.5690836906433105\n",
            "argm 1.485643982887268 0.48706793785095215\n",
            "argm 1.4314675331115723 0.4117414355278015\n",
            "argm 1.6017870903015137 0.05266070365905762\n",
            "argm 1.297929286956787 0.9369938969612122\n",
            "argm 0.9825430512428284 0.6781797409057617\n",
            "argm 1.136711597442627 0.28629228472709656\n",
            "argm 1.3560466766357422 0.8016882538795471\n",
            "argm 1.4742894172668457 0.22377724945545197\n",
            "argm 1.522733211517334 0.07291696965694427\n",
            "argm 1.4984924793243408 0.4831720292568207\n",
            "argm 1.0040991306304932 0.6055388450622559\n",
            "argm 1.0847138166427612 0.498740017414093\n",
            "argm 1.3027727603912354 0.8689091205596924\n",
            "argm 1.4628593921661377 0.2608504295349121\n",
            "argm 1.4955366849899292 0.14379434287548065\n",
            "argm 1.5267610549926758 0.4278654456138611\n",
            "argm 1.0313106775283813 0.7973232269287109\n",
            "argm 1.078313946723938 0.4333611726760864\n",
            "argm 1.2986329793930054 0.8560758829116821\n",
            "repr, std, cov, closslb 0.9127305746078491 0.48095703125 0.0005459843669086695 0.00011043534323107451 0.046275947242975235\n",
            "0.06272346108040762 0.17959709098802126 1.0\n",
            "argm 1.5125287771224976 0.20534545183181763\n",
            "argm 1.4204118251800537 0.234285369515419\n",
            "argm 1.6273691654205322 0.22411532700061798\n",
            "argm 1.3944045305252075 0.8592318296432495\n",
            "argm 1.0652896165847778 0.9869028925895691\n",
            "argm 1.2311564683914185 0.6076544523239136\n",
            "argm 1.4731398820877075 0.4968680739402771\n",
            "argm 1.4245833158493042 0.312563955783844\n",
            "argm 1.589263677597046 0.060422345995903015\n",
            "argm 1.259789228439331 0.9641733169555664\n",
            "argm 0.9837796688079834 0.7795757055282593\n",
            "argm 1.143560767173767 0.326693058013916\n",
            "argm 1.3761019706726074 0.7308052778244019\n",
            "argm 1.458141565322876 0.2959398627281189\n",
            "argm 1.5303301811218262 0.08754339069128036\n",
            "argm 1.4367296695709229 0.7184151411056519\n",
            "argm 0.9694491624832153 0.5974704027175903\n",
            "argm 1.1019669771194458 0.45419299602508545\n",
            "argm 1.3145973682403564 0.7615126967430115\n",
            "argm 1.4481048583984375 0.24500581622123718\n",
            "argm 1.4939968585968018 0.09729713201522827\n",
            "argm 1.480651617050171 0.5784323215484619\n",
            "argm 0.9938544034957886 0.7990118861198425\n",
            "argm 1.086961030960083 0.5499640703201294\n",
            "argm 1.3153190612792969 0.7396979928016663\n",
            "repr, std, cov, closslb 0.9382427930831909 0.477294921875 0.0012750837486237288 0.01536159310489893 0.01838229037821293\n",
            "0.06197564923141156 0.18031655765308716 1.0\n",
            "argm 1.5334973335266113 0.14636479318141937\n",
            "argm 1.4304609298706055 0.24647825956344604\n",
            "argm 1.6363343000411987 0.18341559171676636\n",
            "argm 1.389564037322998 0.7969927787780762\n",
            "argm 1.0593156814575195 0.9827862977981567\n",
            "argm 1.2244253158569336 0.4958314895629883\n",
            "argm 1.4604679346084595 0.49821752309799194\n",
            "argm 1.4150198698043823 0.33587682247161865\n",
            "argm 1.5893572568893433 0.05653593689203262\n",
            "argm 1.283544659614563 0.9515182375907898\n",
            "argm 0.9797603487968445 0.7235605716705322\n",
            "argm 1.1285748481750488 0.1809401512145996\n",
            "argm 1.323470115661621 0.7372090816497803\n",
            "argm 1.4531337022781372 0.21248683333396912\n",
            "argm 1.5002007484436035 0.07856637239456177\n",
            "argm 1.4767448902130127 0.514095664024353\n",
            "argm 0.9838365912437439 0.7756703495979309\n",
            "argm 1.0715628862380981 0.4114508628845215\n",
            "argm 1.2659322023391724 0.7295067310333252\n",
            "argm 1.4459121227264404 0.22179633378982544\n",
            "argm 1.483177661895752 0.17949846386909485\n",
            "argm 1.4971084594726562 0.364646315574646\n",
            "argm 1.0226905345916748 0.9404857754707336\n",
            "argm 1.0423948764801025 0.6087453365325928\n",
            "argm 1.2605571746826172 0.7401863932609558\n",
            "repr, std, cov, closslb 1.0044840574264526 0.481201171875 0.00046407501213252544 9.836212848313153e-05 0.041100241243839264\n",
            "0.06172836515363095 0.18249229699677147 1.0\n",
            "argm 1.5502703189849854 0.16840361058712006\n",
            "argm 1.4319491386413574 0.26332587003707886\n",
            "argm 1.6391141414642334 0.19105151295661926\n",
            "argm 1.4100080728530884 0.8054305911064148\n",
            "argm 1.0694129467010498 0.9836664199829102\n",
            "argm 1.2366085052490234 0.5072615742683411\n",
            "argm 1.4819786548614502 0.5488928556442261\n",
            "argm 1.4328498840332031 0.33733248710632324\n",
            "argm 1.6031746864318848 0.07089413702487946\n",
            "argm 1.2693703174591064 0.9570409059524536\n",
            "argm 0.980715274810791 0.7699112892150879\n",
            "argm 1.1340056657791138 0.185259610414505\n",
            "argm 1.3476494550704956 0.7976487874984741\n",
            "argm 1.454108715057373 0.3246534764766693\n",
            "argm 1.5255651473999023 0.06687641143798828\n",
            "argm 1.4486474990844727 0.5220640301704407\n",
            "argm 0.9680571556091309 0.7135486602783203\n",
            "argm 1.0848702192306519 0.32661205530166626\n",
            "argm 1.293607473373413 0.7477250695228577\n",
            "argm 1.444340467453003 0.292900025844574\n",
            "argm 1.5116429328918457 0.1514274626970291\n",
            "argm 1.4591395854949951 0.4958733916282654\n",
            "argm 1.0010606050491333 0.8075491189956665\n",
            "argm 1.087764024734497 0.41565579175949097\n",
            "argm 1.3205885887145996 0.7094739675521851\n",
            "repr, std, cov, closslb 0.9475747346878052 0.4765625 0.0013737778645008802 0.01555380318313837 0.045259103178977966\n",
            "0.061790093518784575 0.18543417535297999 1.0\n",
            "argm 1.603941559791565 0.19189807772636414\n",
            "argm 1.504770040512085 0.23365673422813416\n",
            "argm 1.7091076374053955 0.24585798382759094\n",
            "argm 1.4795876741409302 0.8205376863479614\n",
            "argm 1.1104360818862915 0.9836121797561646\n",
            "argm 1.2783458232879639 0.6326459646224976\n",
            "argm 1.532390832901001 0.5409373044967651\n",
            "argm 1.4857540130615234 0.3153635263442993\n",
            "argm 1.6590681076049805 0.07455144077539444\n",
            "argm 1.283651351928711 0.9779538512229919\n",
            "argm 1.0148887634277344 0.8131866455078125\n",
            "argm 1.1760444641113281 0.30701515078544617\n",
            "argm 1.4471012353897095 0.751936674118042\n",
            "argm 1.5301103591918945 0.4442799985408783\n",
            "argm 1.6004341840744019 0.11198502033948898\n",
            "argm 1.443429946899414 0.7739688754081726\n",
            "argm 0.9889460206031799 0.6285024881362915\n",
            "argm 1.1404664516448975 0.3650262951850891\n",
            "argm 1.3659155368804932 0.7507643103599548\n",
            "argm 1.5084166526794434 0.4422683119773865\n",
            "argm 1.5682272911071777 0.08638668805360794\n",
            "argm 1.4598562717437744 0.7395157814025879\n",
            "argm 1.0048505067825317 0.5091168284416199\n",
            "argm 1.147424578666687 0.39631035923957825\n",
            "argm 1.3683192729949951 0.8322974443435669\n",
            "repr, std, cov, closslb 1.4893543720245361 0.478759765625 0.0007287277840077877 6.873653910588473e-05 0.01997520960867405\n",
            "0.061482067744257726 0.18561960952833295 1.0\n",
            "argm 1.6270184516906738 0.18423981964588165\n",
            "argm 1.5307960510253906 0.23456476628780365\n",
            "argm 1.7328004837036133 0.2727443277835846\n",
            "argm 1.5079691410064697 0.8247050046920776\n",
            "argm 1.1227926015853882 0.9822280406951904\n",
            "argm 1.280502438545227 0.5890997052192688\n",
            "argm 1.5558464527130127 0.5472036004066467\n",
            "argm 1.5175929069519043 0.320387065410614\n",
            "argm 1.69289231300354 0.09236577898263931\n",
            "argm 1.338999629020691 0.9752067923545837\n",
            "argm 1.0232479572296143 0.7481184005737305\n",
            "argm 1.18448007106781 0.2545975148677826\n",
            "argm 1.4560281038284302 0.8200251460075378\n",
            "argm 1.591195821762085 0.34180283546447754\n",
            "argm 1.6568577289581299 0.13739056885242462\n",
            "argm 1.5309278964996338 0.7105205655097961\n",
            "argm 1.0221842527389526 0.6841315031051636\n",
            "argm 1.1681435108184814 0.34775957465171814\n",
            "argm 1.4158860445022583 0.891386091709137\n",
            "argm 1.5872889757156372 0.4755808115005493\n",
            "argm 1.6517882347106934 0.10136514902114868\n",
            "argm 1.547324299812317 0.6372101306915283\n",
            "argm 1.040043592453003 0.6914210319519043\n",
            "argm 1.1753525733947754 0.38720202445983887\n",
            "argm 1.4208481311798096 0.964753270149231\n",
            "repr, std, cov, closslb 1.196244239807129 0.476318359375 0.0013068742118775845 7.517232006648555e-05 0.009904591366648674\n",
            "0.06062774012879563 0.1826747892937682 1.0\n",
            "argm 1.512310266494751 0.13362151384353638\n",
            "argm 1.3948699235916138 0.2471180558204651\n",
            "argm 1.6077651977539062 0.221793532371521\n",
            "argm 1.4087107181549072 0.738879919052124\n",
            "argm 1.0544368028640747 0.9705731272697449\n",
            "argm 1.2121288776397705 0.44889843463897705\n",
            "argm 1.467421054840088 0.5285153388977051\n",
            "argm 1.4178996086120605 0.38010814785957336\n",
            "argm 1.5753273963928223 0.10235141962766647\n",
            "argm 1.2797183990478516 0.9683889150619507\n",
            "argm 0.969290018081665 0.6084004640579224\n",
            "argm 1.116445541381836 0.1276060938835144\n",
            "argm 1.3265118598937988 0.8329634070396423\n",
            "argm 1.4647393226623535 0.23747888207435608\n",
            "argm 1.499233603477478 0.08569378405809402\n",
            "argm 1.4689801931381226 0.4427321255207062\n",
            "argm 0.9817725419998169 0.7744784355163574\n",
            "argm 1.0785810947418213 0.3181183636188507\n",
            "argm 1.2975021600723267 0.8905857801437378\n",
            "argm 1.4498705863952637 0.3644367754459381\n",
            "argm 1.4940041303634644 0.08263159543275833\n",
            "argm 1.4755669832229614 0.34479236602783203\n",
            "argm 1.0002415180206299 0.9315467476844788\n",
            "argm 1.0901548862457275 0.4166979193687439\n",
            "argm 1.32341468334198 0.8343057632446289\n",
            "repr, std, cov, closslb 1.2364106178283691 0.47802734375 0.0008000223897397518 0.0005255079013295472 0.01927938684821129\n",
            "0.06093148571342147 0.18673611519482333 1.0\n",
            "argm 1.506089687347412 0.16015581786632538\n",
            "argm 1.4096322059631348 0.25063568353652954\n",
            "argm 1.6196153163909912 0.2224557101726532\n",
            "argm 1.4143770933151245 0.7813839912414551\n",
            "argm 1.0437028408050537 0.9677484631538391\n",
            "argm 1.204052448272705 0.45052728056907654\n",
            "argm 1.4475600719451904 0.5586717128753662\n",
            "argm 1.4061729907989502 0.3779567778110504\n",
            "argm 1.5714607238769531 0.07860269397497177\n",
            "argm 1.2701427936553955 0.9742199778556824\n",
            "argm 0.9525830149650574 0.6104093194007874\n",
            "argm 1.1063178777694702 0.1666126847267151\n",
            "argm 1.3154492378234863 0.8503794074058533\n",
            "argm 1.4589738845825195 0.21757185459136963\n",
            "argm 1.5118381977081299 0.07575284689664841\n",
            "argm 1.486647605895996 0.4636785089969635\n",
            "argm 0.9677767157554626 0.7994610071182251\n",
            "argm 1.0561615228652954 0.4256526231765747\n",
            "argm 1.2884783744812012 0.8895018100738525\n",
            "argm 1.4420753717422485 0.24585239589214325\n",
            "argm 1.4977161884307861 0.08534174412488937\n",
            "argm 1.450051188468933 0.43010058999061584\n",
            "argm 0.9722319841384888 0.9196673631668091\n",
            "argm 1.0718841552734375 0.40559303760528564\n",
            "argm 1.3355578184127808 0.8232640027999878\n",
            "repr, std, cov, closslb 1.0917224884033203 0.476318359375 0.0011252022814005613 0.015023707412183285 0.011963017284870148\n",
            "0.06038583423524554 0.18804719600190975 1.0\n",
            "argm 1.469280481338501 0.13579800724983215\n",
            "argm 1.3561855554580688 0.2091396600008011\n",
            "argm 1.5712233781814575 0.17569027841091156\n",
            "argm 1.3707691431045532 0.7111860513687134\n",
            "argm 1.0249617099761963 0.9688061475753784\n",
            "argm 1.1805557012557983 0.44086575508117676\n",
            "argm 1.4079461097717285 0.5122444033622742\n",
            "argm 1.3589799404144287 0.31009411811828613\n",
            "argm 1.5277959108352661 0.059670768678188324\n",
            "argm 1.2559854984283447 0.9561048746109009\n",
            "argm 0.95224928855896 0.5735289454460144\n",
            "argm 1.1003549098968506 0.21783170104026794\n",
            "argm 1.2802197933197021 0.7690194845199585\n",
            "argm 1.4179563522338867 0.12854206562042236\n",
            "argm 1.461236834526062 0.0815587118268013\n",
            "argm 1.4375320672988892 0.4270355999469757\n",
            "argm 0.9551791548728943 0.7315094470977783\n",
            "argm 1.0510483980178833 0.4799254238605499\n",
            "argm 1.2568198442459106 0.7270234227180481\n",
            "argm 1.3819524049758911 0.16098842024803162\n",
            "argm 1.438971757888794 0.09903497993946075\n",
            "argm 1.3802227973937988 0.45624208450317383\n",
            "argm 0.9426231980323792 0.7741410136222839\n",
            "argm 1.0590784549713135 0.443480908870697\n",
            "argm 1.2766644954681396 0.6199283599853516\n",
            "repr, std, cov, closslb 1.0697262287139893 0.48193359375 0.0003917580470442772 0.0001885997917270288 0.01953347772359848\n",
            "0.060688367868924425 0.19222796418944088 1.0\n",
            "argm 1.4858174324035645 0.1319143921136856\n",
            "argm 1.3770296573638916 0.19189250469207764\n",
            "argm 1.5850276947021484 0.16435235738754272\n",
            "argm 1.3783270120620728 0.7386507391929626\n",
            "argm 1.0348962545394897 0.9737565517425537\n",
            "argm 1.1898038387298584 0.4544416666030884\n",
            "argm 1.414652705192566 0.5151014924049377\n",
            "argm 1.3624699115753174 0.24371221661567688\n",
            "argm 1.5367765426635742 0.07908046245574951\n",
            "argm 1.2308342456817627 0.9799174070358276\n",
            "argm 0.953691840171814 0.6353734135627747\n",
            "argm 1.1066954135894775 0.22826716303825378\n",
            "argm 1.316992163658142 0.8323582410812378\n",
            "argm 1.4084153175354004 0.23886188864707947\n",
            "argm 1.4786896705627441 0.06065983325242996\n",
            "argm 1.386513590812683 0.6169136762619019\n",
            "argm 0.9403476715087891 0.6013931632041931\n",
            "argm 1.0751088857650757 0.35588163137435913\n",
            "argm 1.2965569496154785 0.7716352343559265\n",
            "argm 1.3929715156555176 0.3143520951271057\n",
            "argm 1.4903018474578857 0.10043196380138397\n",
            "argm 1.3818213939666748 0.7023744583129883\n",
            "argm 0.9510384798049927 0.6137005686759949\n",
            "argm 1.0765377283096313 0.2360120415687561\n",
            "argm 1.3147897720336914 0.7313283681869507\n",
            "repr, std, cov, closslb 0.9041430354118347 0.479248046875 0.0006796864327043295 0.00016317242989316583 0.0070020100101828575\n",
            "0.060749056236793345 0.19571763587125834 1.0\n",
            "argm 1.4629724025726318 0.14740973711013794\n",
            "argm 1.3462562561035156 0.19389577209949493\n",
            "argm 1.564666509628296 0.18530431389808655\n",
            "argm 1.3723022937774658 0.7346984148025513\n",
            "argm 1.0159711837768555 0.9648408889770508\n",
            "argm 1.1789765357971191 0.44832006096839905\n",
            "argm 1.3983052968978882 0.528854489326477\n",
            "argm 1.338834524154663 0.2326900064945221\n",
            "argm 1.5158556699752808 0.05424767732620239\n",
            "argm 1.2078852653503418 0.9585227966308594\n",
            "argm 0.9382911920547485 0.6751203536987305\n",
            "argm 1.0976393222808838 0.17078685760498047\n",
            "argm 1.3119165897369385 0.8185716867446899\n",
            "argm 1.3846755027770996 0.37773606181144714\n",
            "argm 1.442457675933838 0.09620748460292816\n",
            "argm 1.359384298324585 0.5528982877731323\n",
            "argm 0.9273418188095093 0.6500484347343445\n",
            "argm 1.0728522539138794 0.3055315613746643\n",
            "argm 1.2674083709716797 0.7836079597473145\n",
            "argm 1.3719335794448853 0.44317078590393066\n",
            "argm 1.4256770610809326 0.07584784179925919\n",
            "argm 1.389275074005127 0.4119034707546234\n",
            "argm 0.9428428411483765 0.766647458076477\n",
            "argm 1.0589919090270996 0.38051605224609375\n",
            "argm 1.263087511062622 0.8315150737762451\n",
            "repr, std, cov, closslb 0.819480836391449 0.48095703125 0.00045701395720243454 0.0003331182524561882 0.0208013653755188\n",
            "0.06087061509832315 0.19946992907981764 1.0\n",
            "3\n",
            "argm 1.392936110496521 0.15291385352611542\n",
            "argm 1.2812533378601074 0.1906994730234146\n",
            "argm 1.4973318576812744 0.20519457757472992\n",
            "argm 1.3024851083755493 0.6895092725753784\n",
            "argm 0.9713451862335205 0.9527357816696167\n",
            "argm 1.1243155002593994 0.4359874725341797\n",
            "argm 1.3432197570800781 0.5242377519607544\n",
            "argm 1.2806971073150635 0.2819755971431732\n",
            "argm 1.4475300312042236 0.05775131285190582\n",
            "argm 1.1730883121490479 0.9470930695533752\n",
            "argm 1.0422024726867676 0.12001208961009979\n",
            "argm 1.2350271940231323 0.8116610646247864\n",
            "argm 1.3118176460266113 0.313877135515213\n",
            "argm 1.3563156127929688 0.057717785239219666\n",
            "argm 1.32979154586792 0.285319983959198\n",
            "argm 1.0060279369354248 0.3480277955532074\n",
            "argm 1.2012332677841187 0.8276978731155396\n",
            "argm 1.3096919059753418 0.32916852831840515\n",
            "argm 1.3577041625976562 0.0814804807305336\n",
            "argm 1.3311619758605957 0.3069089651107788\n",
            "argm 1.0064417123794556 0.3714580237865448\n",
            "argm 1.2125635147094727 0.8170970678329468\n",
            "repr, std, cov, closslb 0.9366673231124878 0.478759765625 0.0007261533755809069 0.00013263002620078623 0.026892131194472313\n",
            "0.06087061509832315 0.202888181900056 1.0\n",
            "argm 1.3760905265808105 0.1475159078836441\n",
            "argm 1.2624287605285645 0.19663944840431213\n",
            "argm 1.4763176441192627 0.22312626242637634\n",
            "argm 1.2794562578201294 0.6711669564247131\n",
            "argm 0.9585978984832764 0.9598676562309265\n",
            "argm 1.1123607158660889 0.4526562988758087\n",
            "argm 1.3155711889266968 0.4620607793331146\n",
            "argm 1.2580348253250122 0.23946818709373474\n",
            "argm 1.4234271049499512 0.0865403562784195\n",
            "argm 1.1334257125854492 0.9750603437423706\n",
            "argm 1.032972812652588 0.1238510012626648\n",
            "argm 1.2270138263702393 0.7558057308197021\n",
            "argm 1.2856385707855225 0.36398133635520935\n",
            "argm 1.357010841369629 0.11127814650535583\n",
            "argm 1.3096617460250854 0.47271421551704407\n",
            "argm 1.012280821800232 0.3179730772972107\n",
            "argm 1.2118749618530273 0.7374700307846069\n",
            "argm 1.2854762077331543 0.3715585768222809\n",
            "argm 1.3410027027130127 0.0768899917602539\n",
            "argm 1.2904326915740967 0.4567717909812927\n",
            "argm 1.0066602230072021 0.3848858177661896\n",
            "argm 1.209778070449829 0.5497829914093018\n",
            "repr, std, cov, closslb 0.9071359038352966 0.481201171875 0.0005222642794251442 5.184197289054282e-05 0.03267936408519745\n",
            "0.06044622006948078 0.20492621807648137 1.0\n",
            "argm 1.3782390356063843 0.11564791202545166\n",
            "argm 1.2616610527038574 0.18349942564964294\n",
            "argm 1.4778084754943848 0.2035265862941742\n",
            "argm 1.2931091785430908 0.6816973686218262\n",
            "argm 0.9694002866744995 0.9584567546844482\n",
            "argm 1.1250252723693848 0.45922213792800903\n",
            "argm 1.3159096240997314 0.43637025356292725\n",
            "argm 1.2693493366241455 0.14813058078289032\n",
            "argm 1.4323362112045288 0.08137151598930359\n",
            "argm 1.1375669240951538 0.9858300685882568\n",
            "argm 0.9070374965667725 0.5897200107574463\n",
            "argm 1.0414481163024902 0.12760329246520996\n",
            "argm 1.2488481998443604 0.7682042121887207\n",
            "argm 1.2942636013031006 0.24806800484657288\n",
            "argm 1.3714630603790283 0.1459568738937378\n",
            "argm 1.286275863647461 0.7178776264190674\n",
            "argm 1.0137085914611816 0.19991956651210785\n",
            "argm 1.2121846675872803 0.7499149441719055\n",
            "argm 1.299726128578186 0.30398088693618774\n",
            "argm 1.343324065208435 0.0608665868639946\n",
            "argm 1.3006584644317627 0.5953314304351807\n",
            "argm 0.9040548801422119 0.8111286163330078\n",
            "argm 1.0246222019195557 0.19269436597824097\n",
            "argm 1.2322180271148682 0.7124192714691162\n",
            "repr, std, cov, closslb 0.8658223152160645 0.481201171875 0.0005193937104195356 4.957420242135413e-05 0.02649475261569023\n",
            "0.060749056236793345 0.20948224992136213 1.0\n",
            "argm 1.3970181941986084 0.12810063362121582\n",
            "argm 1.2813141345977783 0.1622931957244873\n",
            "argm 1.507177710533142 0.19504410028457642\n",
            "argm 1.316190481185913 0.6801612973213196\n",
            "argm 0.9760929346084595 0.9569692611694336\n",
            "argm 1.1290780305862427 0.4755389094352722\n",
            "argm 1.3390944004058838 0.46199655532836914\n",
            "argm 1.2958202362060547 0.0959029421210289\n",
            "argm 1.4644086360931396 0.07921736687421799\n",
            "argm 1.1342837810516357 0.9797056913375854\n",
            "argm 0.9067903757095337 0.5929721593856812\n",
            "argm 1.0463712215423584 0.15946850180625916\n",
            "argm 1.2690765857696533 0.813164234161377\n",
            "argm 1.3046667575836182 0.21724216639995575\n",
            "argm 1.4039430618286133 0.12216344475746155\n",
            "argm 1.2881567478179932 0.8051595687866211\n",
            "argm 1.0264567136764526 0.23369863629341125\n",
            "argm 1.224595308303833 0.7393615245819092\n",
            "argm 1.3112750053405762 0.24982261657714844\n",
            "argm 1.3704698085784912 0.07239729166030884\n",
            "argm 1.3095341920852661 0.5241159200668335\n",
            "argm 1.0205881595611572 0.19547995924949646\n",
            "argm 1.2389121055603027 0.8414124250411987\n",
            "repr, std, cov, closslb 0.8369811177253723 0.476318359375 0.0011061157565563917 4.138716758461669e-05 0.04685446247458458\n",
            "0.06014489355104038 0.20927297694441774 1.0\n",
            "argm 1.4130915403366089 0.1486949622631073\n",
            "argm 1.288501262664795 0.14564114809036255\n",
            "argm 1.510002851486206 0.1846756786108017\n",
            "argm 1.3329651355743408 0.6691843271255493\n",
            "argm 0.9808005690574646 0.9571458101272583\n",
            "argm 1.1397939920425415 0.46987274289131165\n",
            "argm 1.3374192714691162 0.4835575819015503\n",
            "argm 1.2850925922393799 0.0981353223323822\n",
            "argm 1.4544456005096436 0.08390843868255615\n",
            "argm 1.1238293647766113 0.9824326038360596\n",
            "argm 0.9047097563743591 0.6688870787620544\n",
            "argm 1.0505421161651611 0.14707446098327637\n",
            "argm 1.2794735431671143 0.8012306094169617\n",
            "argm 1.3113517761230469 0.3331471085548401\n",
            "argm 1.4231736660003662 0.0754455104470253\n",
            "argm 1.2748196125030518 0.7678868770599365\n",
            "argm 1.0237140655517578 0.3099544048309326\n",
            "argm 1.233306884765625 0.7794628143310547\n",
            "argm 1.3180898427963257 0.35240036249160767\n",
            "argm 1.4059617519378662 0.07019564509391785\n",
            "argm 1.3075287342071533 0.5408881306648254\n",
            "argm 1.017451524734497 0.3055986166000366\n",
            "argm 1.2337311506271362 0.7826932668685913\n",
            "repr, std, cov, closslb 0.7607166767120361 0.4765625 0.0011312125716358423 3.634274617070332e-05 0.019906867295503616\n",
            "0.059249896819819715 0.2073989030091609 1.0\n",
            "argm 1.3284635543823242 0.10995380580425262\n",
            "argm 1.199275016784668 0.13741850852966309\n",
            "argm 1.4240669012069702 0.16142147779464722\n",
            "argm 1.2493599653244019 0.6169416308403015\n",
            "argm 0.9353692531585693 0.9381533265113831\n",
            "argm 1.0879546403884888 0.4097660779953003\n",
            "argm 1.2681924104690552 0.4491526484489441\n",
            "argm 1.2057833671569824 0.08899904787540436\n",
            "argm 1.3799471855163574 0.06920099258422852\n",
            "argm 1.0733006000518799 0.9712996482849121\n",
            "argm 1.0240479707717896 0.13223010301589966\n",
            "argm 1.2304853200912476 0.7508587837219238\n",
            "argm 1.232465386390686 0.36059504747390747\n",
            "argm 1.3476760387420654 0.052889809012413025\n",
            "argm 1.192419409751892 0.74322509765625\n",
            "argm 0.9921127557754517 0.305464506149292\n",
            "argm 1.1817100048065186 0.7202926874160767\n",
            "argm 1.2427449226379395 0.3937818109989166\n",
            "argm 1.3446433544158936 0.07949117571115494\n",
            "argm 1.220186471939087 0.6013233065605164\n",
            "argm 0.9918506145477295 0.3130417466163635\n",
            "argm 1.1990405321121216 0.6393140554428101\n",
            "repr, std, cov, closslb 0.77313232421875 0.476806640625 0.001039433991536498 7.176569488365203e-05 0.014137442223727703\n",
            "0.05919070611370601 0.20990142390345473 1.0\n",
            "argm 1.3039640188217163 0.12608617544174194\n",
            "argm 1.1755744218826294 0.139483243227005\n",
            "argm 1.402064561843872 0.1767519861459732\n",
            "argm 1.2247288227081299 0.6130943298339844\n",
            "argm 0.9085801243782043 0.9284743666648865\n",
            "argm 1.061015248298645 0.39436855912208557\n",
            "argm 1.247659683227539 0.45552849769592285\n",
            "argm 1.16996431350708 0.11270027607679367\n",
            "argm 1.3481953144073486 0.05329493433237076\n",
            "argm 1.066228985786438 0.9650204181671143\n",
            "argm 0.9816563129425049 0.0953497365117073\n",
            "argm 1.1740214824676514 0.7855106592178345\n",
            "argm 1.2128362655639648 0.33653900027275085\n",
            "argm 1.2942774295806885 0.054244883358478546\n",
            "argm 1.2078105211257935 0.5767503976821899\n",
            "argm 0.9650525450706482 0.30659323930740356\n",
            "argm 1.1478031873703003 0.7842254638671875\n",
            "argm 1.2031277418136597 0.32957082986831665\n",
            "argm 1.279741644859314 0.0697234645485878\n",
            "argm 1.2138783931732178 0.4926742911338806\n",
            "argm 0.9557936787605286 0.3093509078025818\n",
            "argm 1.1577050685882568 0.7329896688461304\n",
            "repr, std, cov, closslb 0.7063464522361755 0.477294921875 0.0010618322994560003 0.00022771357907913625 0.01350184716284275\n",
            "0.05842658646407489 0.20906391303138638 1.0\n",
            "argm 1.3149538040161133 0.11602509766817093\n",
            "argm 1.1793922185897827 0.14918950200080872\n",
            "argm 1.3934211730957031 0.17717388272285461\n",
            "argm 1.2256255149841309 0.5656219720840454\n",
            "argm 0.9023867249488831 0.941541314125061\n",
            "argm 1.0555522441864014 0.3836424946784973\n",
            "argm 1.2402262687683105 0.4240935742855072\n",
            "argm 1.1870040893554688 0.11676829308271408\n",
            "argm 1.3529095649719238 0.0561615452170372\n",
            "argm 1.0639822483062744 0.9610406756401062\n",
            "argm 0.9805738925933838 0.09411480277776718\n",
            "argm 1.1786972284317017 0.7464878559112549\n",
            "argm 1.2256367206573486 0.36096668243408203\n",
            "argm 1.2997047901153564 0.061880797147750854\n",
            "argm 1.2107622623443604 0.6063105463981628\n",
            "argm 0.9715492725372314 0.32906514406204224\n",
            "argm 1.1552393436431885 0.7347671389579773\n",
            "argm 1.2233061790466309 0.38017797470092773\n",
            "argm 1.2927898168563843 0.06594602018594742\n",
            "argm 1.2479796409606934 0.4488872289657593\n",
            "argm 0.9732877612113953 0.4099365472793579\n",
            "argm 1.18064284324646 0.7159760594367981\n",
            "repr, std, cov, closslb 0.7047610282897949 0.476318359375 0.0011585718020796776 0.015387195162475109 0.014818987809121609\n",
            "0.0581353278897035 0.20990142390345473 1.0\n",
            "argm 1.3404313325881958 0.11983327567577362\n",
            "argm 1.2215650081634521 0.1587197184562683\n",
            "argm 1.438852071762085 0.1950664520263672\n",
            "argm 1.269615650177002 0.5777031183242798\n",
            "argm 0.9329124689102173 0.9557505249977112\n",
            "argm 1.0897501707077026 0.42868125438690186\n",
            "argm 1.294844627380371 0.42478927969932556\n",
            "argm 1.2503142356872559 0.09496696293354034\n",
            "argm 1.4206856489181519 0.08148327469825745\n",
            "argm 1.087501049041748 0.9836328625679016\n",
            "argm 1.0071353912353516 0.08931756019592285\n",
            "argm 1.2219949960708618 0.7399410009384155\n",
            "argm 1.2528823614120483 0.3920886516571045\n",
            "argm 1.3591666221618652 0.048955902457237244\n",
            "argm 1.2332067489624023 0.7764716148376465\n",
            "argm 0.9908198118209839 0.28951093554496765\n",
            "argm 1.1852394342422485 0.7658020257949829\n",
            "argm 1.2666373252868652 0.39845332503318787\n",
            "argm 1.3356045484542847 0.064043328166008\n",
            "argm 1.2803010940551758 0.5415056347846985\n",
            "argm 1.0077264308929443 0.38974565267562866\n",
            "argm 1.2078665494918823 0.7728205919265747\n",
            "repr, std, cov, closslb 0.714398980140686 0.470458984375 0.0035142828710377216 0.03061121702194214 0.029975254088640213\n",
            "0.05744221742844468 0.2073989030091609 1.0\n",
            "argm 1.3668463230133057 0.10814331471920013\n",
            "argm 1.2415393590927124 0.1543276011943817\n",
            "argm 1.4577157497406006 0.185307115316391\n",
            "argm 1.2910237312316895 0.5693296194076538\n",
            "argm 0.9445894956588745 0.9707351922988892\n",
            "argm 1.093858242034912 0.42620861530303955\n",
            "argm 1.298403024673462 0.4346463680267334\n",
            "argm 1.2424430847167969 0.11585745960474014\n",
            "argm 1.4290149211883545 0.05858391523361206\n",
            "argm 1.123458743095398 0.9777565002441406\n",
            "argm 1.0176706314086914 0.08636713027954102\n",
            "argm 1.2394697666168213 0.8540277481079102\n",
            "argm 1.2876561880111694 0.3518834710121155\n",
            "argm 1.3697845935821533 0.05568657070398331\n",
            "argm 1.2579095363616943 0.6545009016990662\n",
            "argm 0.9828088879585266 0.1830216497182846\n",
            "argm 1.1773803234100342 0.913662314414978\n",
            "argm 1.26259183883667 0.3204401731491089\n",
            "argm 1.3457481861114502 0.1170700415968895\n",
            "argm 1.259134292602539 0.6538745164871216\n",
            "argm 0.9876658916473389 0.10911840945482254\n",
            "argm 1.193361759185791 0.8566278219223022\n",
            "repr, std, cov, closslb 0.7035671472549438 0.4755859375 0.001325141405686736 0.015398037619888783 0.00183794554322958\n",
            "0.05675737048145412 0.2041085583749929 1.0\n",
            "argm 1.3516812324523926 0.08908286690711975\n",
            "argm 1.227482795715332 0.1502344310283661\n",
            "argm 1.4572665691375732 0.16861316561698914\n",
            "argm 1.2803341150283813 0.5586426258087158\n",
            "argm 0.9370113611221313 0.9625856876373291\n",
            "argm 1.0875029563903809 0.4422767162322998\n",
            "argm 1.2926769256591797 0.3645457327365875\n",
            "argm 1.2407571077346802 0.09187683463096619\n",
            "argm 1.403794765472412 0.0560469925403595\n",
            "argm 1.0843446254730225 0.9781963229179382\n",
            "argm 1.0088422298431396 0.11457223445177078\n",
            "argm 1.217505931854248 0.7223351001739502\n",
            "argm 1.25602388381958 0.3273356556892395\n",
            "argm 1.3579864501953125 0.1102527603507042\n",
            "argm 1.236411213874817 0.6757093667984009\n",
            "argm 0.9858200550079346 0.26501619815826416\n",
            "argm 1.1739349365234375 0.7415497303009033\n",
            "argm 1.246219515800476 0.3421492874622345\n",
            "argm 1.3431135416030884 0.09867455810308456\n",
            "argm 1.2402468919754028 0.6448449492454529\n",
            "argm 0.9877768158912659 0.25584083795547485\n",
            "argm 1.1804087162017822 0.5291131734848022\n",
            "repr, std, cov, closslb 0.7096774578094482 0.48046875 0.00046159024350345135 5.637717549689114e-05 0.0027416986413300037\n",
            "0.05715586596810004 0.20802172212229625 1.0\n",
            "argm 1.3588037490844727 0.12201815843582153\n",
            "argm 1.2302522659301758 0.15416276454925537\n",
            "argm 1.4566622972488403 0.20247884094715118\n",
            "argm 1.2795522212982178 0.5810391306877136\n",
            "argm 0.9267257452011108 0.9607399702072144\n",
            "argm 1.0724694728851318 0.4634493887424469\n",
            "argm 1.285203456878662 0.4140744209289551\n",
            "argm 1.2558648586273193 0.10501965880393982\n",
            "argm 1.419966459274292 0.06817560642957687\n",
            "argm 1.086660623550415 0.9817553162574768\n",
            "argm 0.9920733571052551 0.11072773486375809\n",
            "argm 1.2122291326522827 0.7920776009559631\n",
            "argm 1.249605655670166 0.3597736358642578\n",
            "argm 1.3321278095245361 0.0889655202627182\n",
            "argm 1.223198652267456 0.752406120300293\n",
            "argm 0.9790366888046265 0.24756789207458496\n",
            "argm 1.1708670854568481 0.7787230014801025\n",
            "argm 1.260075330734253 0.3790547847747803\n",
            "argm 1.3518593311309814 0.06834883242845535\n",
            "argm 1.2434959411621094 0.8027623295783997\n",
            "argm 0.987555205821991 0.21563005447387695\n",
            "argm 1.1982948780059814 0.5900546908378601\n",
            "repr, std, cov, closslb 0.6900761127471924 0.475830078125 0.0012914389371871948 0.015046180225908756 0.015753116458654404\n",
            "0.05727023485590219 0.2103214366526855 1.0\n",
            "argm 1.345083475112915 0.1412769854068756\n",
            "argm 1.219238042831421 0.146353617310524\n",
            "argm 1.445130467414856 0.2280883491039276\n",
            "argm 1.255936622619629 0.55254065990448\n",
            "argm 0.9009153842926025 0.9579421281814575\n",
            "argm 1.0536532402038574 0.4531787633895874\n",
            "argm 1.2608132362365723 0.4735104739665985\n",
            "argm 1.2150025367736816 0.07827021181583405\n",
            "argm 1.3879085779190063 0.07569417357444763\n",
            "argm 1.0657801628112793 0.9734973907470703\n",
            "argm 0.9828875660896301 0.07447879761457443\n",
            "argm 1.2005597352981567 0.8712252378463745\n",
            "argm 1.2194854021072388 0.36987942457199097\n",
            "argm 1.3444857597351074 0.05022156983613968\n",
            "argm 1.2047340869903564 0.7172584533691406\n",
            "argm 0.9648551344871521 0.18455833196640015\n",
            "argm 1.170472264289856 0.9130319952964783\n",
            "argm 1.2136499881744385 0.32243505120277405\n",
            "argm 1.3261663913726807 0.06134156137704849\n",
            "argm 1.1523849964141846 0.8327518701553345\n",
            "argm 0.9649202823638916 0.12900587916374207\n",
            "argm 1.1980708837509155 0.7257018089294434\n",
            "repr, std, cov, closslb 0.8620664477348328 0.4775390625 0.0008517696987837553 0.015434825792908669 0.024571876972913742\n",
            "0.05744221742844468 0.21179811082803982 1.0\n",
            "argm 1.3307313919067383 0.11255219578742981\n",
            "argm 1.194882869720459 0.16029271483421326\n",
            "argm 1.4224187135696411 0.22407901287078857\n",
            "argm 1.2564091682434082 0.53155517578125\n",
            "argm 0.9092972278594971 0.9524323344230652\n",
            "argm 1.062193512916565 0.42029374837875366\n",
            "argm 1.2597206830978394 0.47311931848526\n",
            "argm 1.2084167003631592 0.08857157081365585\n",
            "argm 1.3754048347473145 0.07141940295696259\n",
            "argm 1.0890092849731445 0.9780398607254028\n",
            "argm 0.9770695567131042 0.060531310737133026\n",
            "argm 1.1802995204925537 0.896638035774231\n",
            "argm 1.2309067249298096 0.3887638449668884\n",
            "argm 1.3320794105529785 0.06048381328582764\n",
            "argm 1.2465561628341675 0.5949921607971191\n",
            "argm 0.9549185037612915 0.15925057232379913\n",
            "argm 1.1549174785614014 0.899776816368103\n",
            "argm 1.2192531824111938 0.45306140184402466\n",
            "argm 1.3185205459594727 0.10789185762405396\n",
            "argm 1.2373032569885254 0.6147215366363525\n",
            "argm 0.9584248661994934 0.10881107300519943\n",
            "argm 1.172295093536377 0.8769596815109253\n",
            "repr, std, cov, closslb 0.8738547563552856 0.482666015625 0.00026250164955854416 0.0001287312334170565 0.021740391850471497\n",
            "0.05790336677074854 0.21629063172212254 1.0\n",
            "argm 1.3297406435012817 0.13337843120098114\n",
            "argm 1.1959190368652344 0.1370580792427063\n",
            "argm 1.4224908351898193 0.20445697009563446\n",
            "argm 1.2473812103271484 0.5458378791809082\n",
            "argm 1.0529149770736694 0.4653465151786804\n",
            "argm 1.2506635189056396 0.46114715933799744\n",
            "argm 1.206116795539856 0.04417542368173599\n",
            "argm 1.375002384185791 0.08228514343500137\n",
            "argm 1.0398776531219482 0.9747943878173828\n",
            "argm 0.9787664413452148 0.1125158742070198\n",
            "argm 1.1932640075683594 0.7425008416175842\n",
            "argm 1.2142187356948853 0.3153802752494812\n",
            "argm 1.3445302248001099 0.05605258047580719\n",
            "argm 1.1351358890533447 0.9096812009811401\n",
            "argm 0.9707891941070557 0.2843504548072815\n",
            "argm 1.182579517364502 0.7254196405410767\n",
            "argm 1.2160474061965942 0.33779069781303406\n",
            "argm 1.3388113975524902 0.07500685751438141\n",
            "argm 1.1295201778411865 0.879781186580658\n",
            "argm 0.9805178046226501 0.4259208142757416\n",
            "argm 1.1999249458312988 0.517680287361145\n",
            "repr, std, cov, closslb 0.8873831033706665 0.48046875 0.0004276472609490156 0.0005595487309619784 0.034838054329156876\n",
            "0.05848501305053896 0.22176328492075104 1.0\n",
            "argm 1.3285102844238281 0.10939780622720718\n",
            "argm 1.203336238861084 0.13877078890800476\n",
            "argm 1.4311829805374146 0.19859802722930908\n",
            "argm 1.2686493396759033 0.5243159532546997\n",
            "argm 1.041719675064087 0.39171987771987915\n",
            "argm 1.2455717325210571 0.47264713048934937\n",
            "argm 1.2000510692596436 0.12461096048355103\n",
            "argm 1.3766365051269531 0.059748999774456024\n",
            "argm 1.1033599376678467 0.9701038002967834\n",
            "argm 0.9696273803710938 0.0817570835351944\n",
            "argm 1.1692842245101929 0.8174474239349365\n",
            "argm 1.2358447313308716 0.39187073707580566\n",
            "argm 1.3221004009246826 0.04236772656440735\n",
            "argm 1.246605396270752 0.484418123960495\n",
            "argm 0.9576011896133423 0.22305920720100403\n",
            "argm 1.1640667915344238 0.8179905414581299\n",
            "argm 1.2138376235961914 0.37990695238113403\n",
            "argm 1.3336820602416992 0.06517767906188965\n",
            "argm 1.2161173820495605 0.7039240598678589\n",
            "argm 0.9709094166755676 0.20569470524787903\n",
            "argm 1.1940897703170776 0.7427361607551575\n",
            "repr, std, cov, closslb 0.8693076372146606 0.47607421875 0.0011100126430392265 6.136463343864307e-05 0.01314206700772047\n",
            "0.059013488548581156 0.22669364765133743 1.0\n",
            "argm 1.3556792736053467 0.1255301833152771\n",
            "argm 1.219968318939209 0.13877637684345245\n",
            "argm 1.4259490966796875 0.1925574541091919\n",
            "argm 1.2658346891403198 0.5269954204559326\n",
            "argm 1.0355656147003174 0.38384926319122314\n",
            "argm 1.245180368423462 0.5002822875976562\n",
            "argm 1.1951253414154053 0.11394917964935303\n",
            "argm 1.3696198463439941 0.05274452269077301\n",
            "argm 1.0764682292938232 0.9684369564056396\n",
            "argm 0.9672806859016418 0.08513219654560089\n",
            "argm 1.1649484634399414 0.8637206554412842\n",
            "argm 1.214916467666626 0.3937371075153351\n",
            "argm 1.3260014057159424 0.049637630581855774\n",
            "argm 1.1856409311294556 0.6332148313522339\n",
            "argm 0.9552202820777893 0.1733824610710144\n",
            "argm 1.1544522047042847 0.8347074389457703\n",
            "argm 1.189129114151001 0.39781349897384644\n",
            "argm 1.3242013454437256 0.07468834519386292\n",
            "argm 1.1362570524215698 0.8600376844406128\n",
            "argm 0.9635470509529114 0.1745922565460205\n",
            "argm 1.1772267818450928 0.6345771551132202\n",
            "repr, std, cov, closslb 0.7434687614440918 0.481201171875 0.0003880683798342943 0.00019523027003742754 0.031554341316223145\n",
            "0.05913157453916685 0.23034807661349374 1.0\n",
            "argm 1.3099923133850098 0.10785553604364395\n",
            "argm 1.174500823020935 0.1345910131931305\n",
            "argm 1.3970732688903809 0.20028278231620789\n",
            "argm 1.236340880393982 0.5231900215148926\n",
            "argm 1.0367960929870605 0.4141051173210144\n",
            "argm 1.225770354270935 0.4711383879184723\n",
            "argm 1.174229621887207 0.07402338087558746\n",
            "argm 1.3436732292175293 0.07479730993509293\n",
            "argm 1.0271376371383667 0.979587197303772\n",
            "argm 0.9607262015342712 0.08506793528795242\n",
            "argm 1.171063780784607 0.7570998668670654\n",
            "argm 1.1721899509429932 0.3517409861087799\n",
            "argm 1.291835069656372 0.06158463656902313\n",
            "argm 1.1023342609405518 0.8626030683517456\n",
            "argm 0.9419233798980713 0.16181263327598572\n",
            "argm 1.138366937637329 0.7542304992675781\n",
            "argm 1.161871075630188 0.28256234526634216\n",
            "argm 1.2999176979064941 0.08232984691858292\n",
            "argm 1.0732346773147583 0.9128584861755371\n",
            "argm 0.9554327726364136 0.18954557180404663\n",
            "argm 1.1499032974243164 0.6282778978347778\n",
            "repr, std, cov, closslb 0.7619529366493225 0.47900390625 0.0006003105081617832 0.015609809197485447 0.010471248999238014\n",
            "0.059249896819819715 0.2335939954387943 1.0\n",
            "argm 1.3201940059661865 0.10519567877054214\n",
            "argm 1.1922112703323364 0.1496337354183197\n",
            "argm 1.4151020050048828 0.2141520380973816\n",
            "argm 1.2650150060653687 0.4872400164604187\n",
            "argm 1.0173307657241821 0.33997559547424316\n",
            "argm 1.2265989780426025 0.546768307685852\n",
            "argm 1.1755081415176392 0.1305537223815918\n",
            "argm 1.3555936813354492 0.054747797548770905\n",
            "argm 1.0978230237960815 0.9582613110542297\n",
            "argm 0.952439546585083 0.08444488048553467\n",
            "argm 1.1347463130950928 0.8999651670455933\n",
            "argm 1.2109452486038208 0.3541242480278015\n",
            "argm 1.288647174835205 0.05448516458272934\n",
            "argm 1.2254059314727783 0.5632862448692322\n",
            "argm 0.9321183562278748 0.19421148300170898\n",
            "argm 1.1367435455322266 0.8815025687217712\n",
            "argm 1.181473731994629 0.4235515296459198\n",
            "argm 1.2950146198272705 0.19934681057929993\n",
            "argm 1.1769670248031616 0.6296513676643372\n",
            "argm 0.9424606561660767 0.1522321254014969\n",
            "argm 1.150519847869873 0.7677851915359497\n",
            "repr, std, cov, closslb 0.9030316472053528 0.4755859375 0.0011942351702600718 0.015539025887846947 0.018131373450160027\n",
            "0.05854349806358949 0.23080900311479727 1.0\n",
            "argm 1.3189996480941772 0.10999571532011032\n",
            "argm 1.1763606071472168 0.1583760678768158\n",
            "argm 1.4065979719161987 0.23239105939865112\n",
            "argm 1.249949336051941 0.4814883768558502\n",
            "argm 1.0097696781158447 0.32541900873184204\n",
            "argm 1.2151801586151123 0.5527529716491699\n",
            "argm 1.1705772876739502 0.15622469782829285\n",
            "argm 1.3444453477859497 0.07353723049163818\n",
            "argm 1.0934292078018188 0.9561216831207275\n",
            "argm 0.9466953277587891 0.06804428994655609\n",
            "argm 1.1252955198287964 0.8798551559448242\n",
            "argm 1.1998023986816406 0.34091994166374207\n",
            "argm 1.2766515016555786 0.05312729626893997\n",
            "argm 1.187415599822998 0.5246568322181702\n",
            "argm 0.9295348525047302 0.1619132161140442\n",
            "argm 1.132243037223816 0.7913891673088074\n",
            "argm 1.1751725673675537 0.36953017115592957\n",
            "argm 1.3136515617370605 0.07676426321268082\n",
            "argm 1.131394386291504 0.8545676469802856\n",
            "argm 0.9605305790901184 0.19246526062488556\n",
            "argm 1.162056565284729 0.44194144010543823\n",
            "repr, std, cov, closslb 0.8310529589653015 0.472900390625 0.0018766080029308796 7.22143886378035e-05 0.04616018757224083\n",
            "0.058019231407656795 0.22965841217208263 1.0\n",
            "argm 1.306994080543518 0.0756802037358284\n",
            "argm 1.167314052581787 0.1682192087173462\n",
            "argm 1.4039161205291748 0.21727290749549866\n",
            "argm 1.248169183731079 0.452924519777298\n",
            "argm 1.0094428062438965 0.3312695622444153\n",
            "argm 1.2087249755859375 0.5331767797470093\n",
            "argm 1.1571913957595825 0.15549547970294952\n",
            "argm 1.344322919845581 0.06539002060890198\n",
            "argm 1.0802518129348755 0.9541947841644287\n",
            "argm 0.9502410888671875 0.09667407721281052\n",
            "argm 1.1401294469833374 0.8705027103424072\n",
            "argm 1.1889365911483765 0.3909040093421936\n",
            "argm 1.274329423904419 0.04424247890710831\n",
            "argm 1.1645264625549316 0.5332622528076172\n",
            "argm 0.9327435493469238 0.23191887140274048\n",
            "argm 1.1274949312210083 0.7711904048919678\n",
            "argm 1.1467469930648804 0.2914806604385376\n",
            "argm 1.3050692081451416 0.0693323090672493\n",
            "argm 1.0734509229660034 0.921785831451416\n",
            "argm 0.9501273036003113 0.27877652645111084\n",
            "argm 1.173419713973999 0.521031379699707\n",
            "repr, std, cov, closslb 0.8149375915527344 0.4765625 0.0010324525646865368 0.0008274584542959929 0.014390322379767895\n",
            "0.05738483259584884 0.22737440890192798 1.0\n",
            "argm 1.3227895498275757 0.11899508535861969\n",
            "argm 1.1873588562011719 0.17552542686462402\n",
            "argm 1.4092003107070923 0.2538766860961914\n",
            "argm 1.2443881034851074 0.48905330896377563\n",
            "argm 1.0072646141052246 0.37981197237968445\n",
            "argm 1.2199912071228027 0.5636774301528931\n",
            "argm 1.1727371215820312 0.1675877720117569\n",
            "argm 1.3459445238113403 0.0646691769361496\n",
            "argm 1.0782437324523926 0.9629166126251221\n",
            "argm 0.9469113945960999 0.12388452887535095\n",
            "argm 1.1552822589874268 0.830643355846405\n",
            "argm 1.200182557106018 0.5127126574516296\n",
            "argm 1.3075709342956543 0.05952268838882446\n",
            "argm 1.1660332679748535 0.6815376281738281\n",
            "argm 0.9235947728157043 0.27576741576194763\n",
            "argm 1.1426045894622803 0.780369758605957\n",
            "argm 1.1775507926940918 0.3637131154537201\n",
            "argm 1.3291573524475098 0.1123230904340744\n",
            "argm 1.0821120738983154 0.9602483510971069\n",
            "argm 0.9519269466400146 0.32833313941955566\n",
            "argm 1.1760344505310059 0.5677230358123779\n",
            "repr, std, cov, closslb 0.7106776237487793 0.474609375 0.0016358124557882547 0.0004185107827652246 0.02956533059477806\n",
            "0.05647443300706202 0.22176328492075104 1.0\n",
            "argm 1.2754032611846924 0.12914277613162994\n",
            "argm 1.1324334144592285 0.17693638801574707\n",
            "argm 1.365253210067749 0.24959909915924072\n",
            "argm 1.2152544260025024 0.4675118327140808\n",
            "argm 0.9891289472579956 0.32798945903778076\n",
            "argm 1.1856929063796997 0.5624687671661377\n",
            "argm 1.1281887292861938 0.17875805497169495\n",
            "argm 1.3151631355285645 0.10132044553756714\n",
            "argm 1.059847116470337 0.9678423404693604\n",
            "argm 0.9234290719032288 0.09758211672306061\n",
            "argm 1.1030137538909912 0.6818757057189941\n",
            "argm 1.1595895290374756 0.4519103169441223\n",
            "argm 1.2481958866119385 0.1169443130493164\n",
            "argm 1.1229654550552368 0.7364770174026489\n",
            "argm 1.0904269218444824 0.6812866926193237\n",
            "argm 1.117912769317627 0.25592464208602905\n",
            "argm 1.271375060081482 0.08190516382455826\n",
            "argm 1.014988660812378 0.9116200804710388\n",
            "argm 0.9201387763023376 0.22066476941108704\n",
            "argm 1.1179869174957275 0.5802260637283325\n",
            "repr, std, cov, closslb 1.02077054977417 0.4755859375 0.0012703642714768648 7.444046786986291e-05 0.003160321619361639\n",
            "0.056587438347509136 0.22109932343138733 1.0\n",
            "argm 1.2459803819656372 0.11047627776861191\n",
            "argm 1.1017327308654785 0.1463117003440857\n",
            "argm 1.3296527862548828 0.23340526223182678\n",
            "argm 1.1795161962509155 0.4531228840351105\n",
            "argm 0.9662980437278748 0.2760244607925415\n",
            "argm 1.1524955034255981 0.5466169118881226\n",
            "argm 1.098037838935852 0.08552893996238708\n",
            "argm 1.2684605121612549 0.04983041435480118\n",
            "argm 1.0007786750793457 0.9682810306549072\n",
            "argm 0.9081593155860901 0.10758452117443085\n",
            "argm 1.0892640352249146 0.6926519870758057\n",
            "argm 1.1110467910766602 0.36544370651245117\n",
            "argm 1.2473797798156738 0.04768744111061096\n",
            "argm 1.0382745265960693 0.88282310962677\n",
            "argm 0.9144923686981201 0.2310667186975479\n",
            "argm 1.1024872064590454 0.6465660333633423\n",
            "argm 1.0876796245574951 0.14887936413288116\n",
            "argm 1.2479617595672607 0.07026828825473785\n",
            "argm 0.9460532069206238 0.9681239128112793\n",
            "argm 0.9391595125198364 0.33947545289993286\n",
            "argm 1.1309864521026611 0.5089362859725952\n",
            "repr, std, cov, closslb 0.873933732509613 0.47412109375 0.0016287474427372217 0.015331580303609371 0.028920993208885193\n",
            "0.056700669811642485 0.2230971955181427 1.0\n",
            "argm 1.2734813690185547 0.13962015509605408\n",
            "argm 1.118160605430603 0.15135201811790466\n",
            "argm 1.3473033905029297 0.2407170832157135\n",
            "argm 1.1964422464370728 0.4776874780654907\n",
            "argm 0.977292001247406 0.3040507435798645\n",
            "argm 1.1674147844314575 0.5910443067550659\n",
            "argm 1.122266411781311 0.08191075176000595\n",
            "argm 1.2918171882629395 0.05490705370903015\n",
            "argm 0.9927685856819153 0.9792349934577942\n",
            "argm 0.9248335361480713 0.1344597041606903\n",
            "argm 1.1154918670654297 0.7795997858047485\n",
            "argm 1.1048296689987183 0.3311215043067932\n",
            "argm 1.2460083961486816 0.05703046917915344\n",
            "argm 1.0282814502716064 0.8805594444274902\n",
            "argm 0.9317318201065063 0.28324127197265625\n",
            "argm 1.1154866218566895 0.6012702584266663\n",
            "argm 1.0759961605072021 0.15151965618133545\n",
            "argm 1.2468326091766357 0.07518008351325989\n",
            "argm 0.951561450958252 0.9656898975372314\n",
            "argm 0.962304413318634 0.45840346813201904\n",
            "argm 1.1436007022857666 0.5178027153015137\n",
            "repr, std, cov, closslb 0.9696136116981506 0.47509765625 0.0015643923543393612 0.015599610283970833 0.02895975112915039\n",
            "0.056700669811642485 0.22287432119694578 1.0\n",
            "argm 1.243679165840149 0.12200977653265\n",
            "argm 1.0951231718063354 0.1673167645931244\n",
            "argm 1.3121838569641113 0.23928098380565643\n",
            "argm 1.175966501235962 0.4504881799221039\n",
            "argm 0.9532155394554138 0.24955160915851593\n",
            "argm 1.1346126794815063 0.6012685894966125\n",
            "argm 1.075211524963379 0.1756986677646637\n",
            "argm 1.2531243562698364 0.0628223642706871\n",
            "argm 1.0004918575286865 0.9464062452316284\n",
            "argm 0.907951295375824 0.095319002866745\n",
            "argm 1.0731247663497925 0.7948313355445862\n",
            "argm 1.0892518758773804 0.3360137343406677\n",
            "argm 1.2134028673171997 0.09435787796974182\n",
            "argm 1.0530281066894531 0.7268590927124023\n",
            "argm 0.9051175117492676 0.22086036205291748\n",
            "argm 1.0852985382080078 0.6259214282035828\n",
            "argm 1.0788003206253052 0.2076001763343811\n",
            "argm 1.2377078533172607 0.14114007353782654\n",
            "argm 1.0055464506149292 0.913297176361084\n",
            "argm 0.9313719272613525 0.3487933278083801\n",
            "argm 1.1097967624664307 0.4137866199016571\n",
            "repr, std, cov, closslb 0.9633355140686035 0.477294921875 0.0008666357025504112 8.146865002345294e-05 0.010034605860710144\n",
            "0.05721302183406813 0.2285135569654077 1.0\n",
            "argm 1.2152466773986816 0.1421067863702774\n",
            "argm 1.0635435581207275 0.148845836520195\n",
            "argm 1.2893807888031006 0.2567349076271057\n",
            "argm 1.1418988704681396 0.430916428565979\n",
            "argm 0.9339463710784912 0.2573188543319702\n",
            "argm 1.105564832687378 0.6238466501235962\n",
            "argm 1.036642074584961 0.16749557852745056\n",
            "argm 1.2139729261398315 0.08291937410831451\n",
            "argm 0.9628937244415283 0.9432529211044312\n",
            "argm 1.0620968341827393 0.7797215580940247\n",
            "argm 1.0561630725860596 0.37398093938827515\n",
            "argm 1.1906055212020874 0.09537488222122192\n",
            "argm 1.0112862586975098 0.7571765184402466\n",
            "argm 1.0748637914657593 0.593237578868866\n",
            "argm 1.0434575080871582 0.09585823863744736\n",
            "argm 1.204264760017395 0.09318999946117401\n",
            "argm 0.9365143775939941 0.48549771308898926\n",
            "argm 1.0979437828063965 0.40139538049697876\n",
            "repr, std, cov, closslb 0.8211613297462463 0.473388671875 0.0018421625718474388 0.0003503068001009524 0.013807939365506172\n",
            "0.05715586596810004 0.23011795865483892 1.0\n",
            "argm 1.2133426666259766 0.15543121099472046\n",
            "argm 1.0591553449630737 0.12492947280406952\n",
            "argm 1.2869622707366943 0.26061850786209106\n",
            "argm 1.1341450214385986 0.4431651830673218\n",
            "argm 0.9280930161476135 0.30891501903533936\n",
            "argm 1.1012675762176514 0.6047582030296326\n",
            "argm 1.0281827449798584 0.07498450577259064\n",
            "argm 1.2023546695709229 0.06237812340259552\n",
            "argm 0.9373801946640015 0.9664838314056396\n",
            "argm 1.0554507970809937 0.710807740688324\n",
            "argm 1.0228561162948608 0.2130037248134613\n",
            "argm 1.18910551071167 0.07197819650173187\n",
            "argm 0.9314176440238953 0.890049934387207\n",
            "argm 1.0730500221252441 0.5660886168479919\n",
            "argm 1.0407764911651611 0.11599157005548477\n",
            "argm 1.1549465656280518 0.1253010630607605\n",
            "argm 0.9402710199356079 0.7348180413246155\n",
            "argm 1.0450018644332886 0.5883928537368774\n",
            "repr, std, cov, closslb 1.0155010223388672 0.478271484375 0.0006773620843887329 0.015589325688779354 0.02130979672074318\n",
            "0.05767233118128932 0.23594047520336983 1.0\n",
            "argm 1.2529383897781372 0.15498977899551392\n",
            "argm 1.0719051361083984 0.14243927597999573\n",
            "argm 1.3091083765029907 0.25476235151290894\n",
            "argm 1.1470887660980225 0.4427991807460785\n",
            "argm 0.9340075254440308 0.3130696713924408\n",
            "argm 1.118857502937317 0.6400695443153381\n",
            "argm 1.052861213684082 0.12310780584812164\n",
            "argm 1.2346745729446411 0.05464162677526474\n",
            "argm 0.9855250716209412 0.9683289527893066\n",
            "argm 1.0553405284881592 0.755335807800293\n",
            "argm 1.0372703075408936 0.3632325530052185\n",
            "argm 1.183852195739746 0.06039440631866455\n",
            "argm 0.9829609990119934 0.893298864364624\n",
            "argm 0.9015969038009644 0.36068445444107056\n",
            "argm 1.0761115550994873 0.6158994436264038\n",
            "argm 1.036433458328247 0.07017888128757477\n",
            "argm 1.1911929845809937 0.09745359420776367\n",
            "argm 0.9366723299026489 0.5887638926506042\n",
            "argm 1.0913162231445312 0.5673822164535522\n",
            "repr, std, cov, closslb 0.9106848239898682 0.472412109375 0.0017586173489689827 0.015482854098081589 0.02886301279067993\n",
            "0.05744221742844468 0.23406141702366728 1.0\n",
            "argm 1.218980073928833 0.13103708624839783\n",
            "argm 1.0602130889892578 0.16869977116584778\n",
            "argm 1.295236349105835 0.24194923043251038\n",
            "argm 1.1367847919464111 0.40658098459243774\n",
            "argm 0.930925190448761 0.3297775983810425\n",
            "argm 1.0933122634887695 0.5718358159065247\n",
            "argm 1.036440134048462 0.11379271745681763\n",
            "argm 1.2206544876098633 0.077015720307827\n",
            "argm 0.9406923651695251 0.9538981318473816\n",
            "argm 1.0567936897277832 0.6181581020355225\n",
            "argm 1.0299941301345825 0.2950988709926605\n",
            "argm 1.194551944732666 0.06261002272367477\n",
            "argm 0.9393717646598816 0.9437587261199951\n",
            "argm 0.9037197828292847 0.42740440368652344\n",
            "argm 1.0700130462646484 0.44073164463043213\n",
            "argm 1.0498900413513184 0.10736100375652313\n",
            "argm 1.1656553745269775 0.1284303069114685\n",
            "argm 0.961193323135376 0.808340847492218\n",
            "argm 1.0581097602844238 0.4115486443042755\n",
            "repr, std, cov, closslb 0.8985968232154846 0.476806640625 0.0007605922874063253 4.1310333472210914e-05 0.007541967555880547\n",
            "0.05790336677074854 0.23783461856749794 1.0\n",
            "argm 1.2198498249053955 0.16425177454948425\n",
            "argm 1.062416672706604 0.1833345741033554\n",
            "argm 1.2893706560134888 0.25498586893081665\n",
            "argm 1.143417239189148 0.41179174184799194\n",
            "argm 0.9245617389678955 0.3294954299926758\n",
            "argm 1.0919703245162964 0.574028491973877\n",
            "argm 1.029935359954834 0.141022726893425\n",
            "argm 1.1960725784301758 0.11056289076805115\n",
            "argm 0.9344459772109985 0.9526613354682922\n",
            "argm 1.043373465538025 0.6221383810043335\n",
            "argm 1.0334573984146118 0.3007119297981262\n",
            "argm 1.1851882934570312 0.08358433842658997\n",
            "argm 0.9366428256034851 0.932397723197937\n",
            "argm 1.0620604753494263 0.43677371740341187\n",
            "argm 1.0457072257995605 0.14332495629787445\n",
            "argm 1.1624982357025146 0.10778848081827164\n",
            "argm 0.9477919340133667 0.8701608180999756\n",
            "argm 1.0461583137512207 0.355655312538147\n",
            "repr, std, cov, closslb 0.9954276084899902 0.47607421875 0.0011226674541831017 0.0315508209168911 0.016420748084783554\n",
            "0.05749965964587312 0.23594047520336983 1.0\n",
            "argm 1.2082440853118896 0.1617400050163269\n",
            "argm 1.043107032775879 0.16408413648605347\n",
            "argm 1.2798906564712524 0.2595903277397156\n",
            "argm 1.1409803628921509 0.39756202697753906\n",
            "argm 0.9143310785293579 0.3354102373123169\n",
            "argm 1.0865776538848877 0.6612058877944946\n",
            "argm 1.0310192108154297 0.11544954031705856\n",
            "argm 1.2077093124389648 0.13910606503486633\n",
            "argm 0.9318021535873413 0.9471319913864136\n",
            "argm 1.0295164585113525 0.7262439131736755\n",
            "argm 1.025349736213684 0.2863314151763916\n",
            "argm 1.168914556503296 0.10390765964984894\n",
            "argm 0.9400401711463928 0.8524379730224609\n",
            "argm 1.0377877950668335 0.6148042678833008\n",
            "argm 1.015276312828064 0.11472869664430618\n",
            "argm 1.1626434326171875 0.10377354919910431\n",
            "argm 1.0554554462432861 0.546292781829834\n",
            "repr, std, cov, closslb 0.7677085995674133 0.474609375 0.0013348215725272894 0.00024680799106135964 0.008002202957868576\n",
            "0.0576147164648245 0.23783461856749794 1.0\n",
            "argm 1.2182865142822266 0.16160868108272552\n",
            "argm 1.0515916347503662 0.15935954451560974\n",
            "argm 1.2709742784500122 0.2565281391143799\n",
            "argm 1.116286039352417 0.4183156490325928\n",
            "argm 0.9126924872398376 0.4033595323562622\n",
            "argm 1.0724220275878906 0.6192829608917236\n",
            "argm 1.0146361589431763 0.04390999674797058\n",
            "argm 1.181149959564209 0.1743631362915039\n",
            "argm 1.049872875213623 0.5845064520835876\n",
            "argm 1.0089921951293945 0.0687791034579277\n",
            "argm 1.1319713592529297 0.2081058919429779\n",
            "argm 0.931490421295166 0.7610662579536438\n",
            "argm 1.0133119821548462 0.4277145564556122\n",
            "argm 1.124023199081421 0.08175987750291824\n",
            "argm 0.9785248041152954 0.7883706092834473\n",
            "argm 1.0187798738479614 0.6842433214187622\n",
            "argm 1.0192688703536987 0.14061200618743896\n",
            "repr, std, cov, closslb 1.112550973892212 0.475341796875 0.0010966833215206861 0.015607555396854877 0.016879450529813766\n",
            "0.0577300035124706 0.24022369590116766 1.0\n",
            "argm 1.2101000547409058 0.1850975751876831\n",
            "argm 1.0565876960754395 0.16381032764911652\n",
            "argm 1.2902576923370361 0.2779690623283386\n",
            "argm 1.1316139698028564 0.40456652641296387\n",
            "argm 0.9133816957473755 0.40550923347473145\n",
            "argm 1.076775312423706 0.6168689727783203\n",
            "argm 1.0196093320846558 0.04485715180635452\n",
            "argm 1.1968636512756348 0.17223134636878967\n",
            "argm 1.0715073347091675 0.5678834915161133\n",
            "argm 1.0327916145324707 0.07457379251718521\n",
            "argm 1.1582040786743164 0.2009868621826172\n",
            "argm 0.9515631198883057 0.8359367847442627\n",
            "argm 1.0237120389938354 0.386162668466568\n",
            "argm 1.1509629487991333 0.06231386214494705\n",
            "argm 0.9670761227607727 0.7619749307632446\n",
            "argm 1.051590919494629 0.48475340008735657\n",
            "argm 1.055198311805725 0.1313248574733734\n",
            "repr, std, cov, closslb 0.8855739235877991 0.475341796875 0.0009914329275488853 0.0004698449920397252 0.04257946461439133\n",
            "0.05727023485590219 0.23735966188406796 1.0\n",
            "argm 1.2134950160980225 0.17269793152809143\n",
            "argm 1.046166181564331 0.16508996486663818\n",
            "argm 1.2845282554626465 0.3016619086265564\n",
            "argm 1.1294935941696167 0.3716815114021301\n",
            "argm 0.9054979085922241 0.37624406814575195\n",
            "argm 1.0640449523925781 0.6419906616210938\n",
            "argm 1.010457158088684 0.053415074944496155\n",
            "argm 1.1835614442825317 0.1348564326763153\n",
            "argm 1.0422271490097046 0.6194807887077332\n",
            "argm 1.013232946395874 0.14464929699897766\n",
            "argm 1.140130877494812 0.20943301916122437\n",
            "argm 0.943112850189209 0.907111644744873\n",
            "argm 1.0080292224884033 0.47166645526885986\n",
            "argm 1.1516157388687134 0.06598792970180511\n",
            "argm 0.9407258033752441 0.8096930384635925\n",
            "argm 1.0536794662475586 0.6115906238555908\n",
            "argm 1.0624589920043945 0.14025437831878662\n",
            "repr, std, cov, closslb 0.8634900450706482 0.473388671875 0.0013160989619791508 0.031084207817912102 0.0039510647766292095\n",
            "0.05675737048145412 0.2335939954387943 1.0\n",
            "4\n",
            "argm 1.2113492488861084 0.1770341694355011\n",
            "argm 1.031575322151184 0.1699766218662262\n",
            "argm 1.2671353816986084 0.3184983432292938\n",
            "argm 1.1203731298446655 0.3493577241897583\n",
            "argm 1.0653389692306519 0.6459507942199707\n",
            "argm 1.0107734203338623 0.06266310811042786\n",
            "argm 1.1839802265167236 0.1271003782749176\n",
            "argm 1.0313801765441895 0.6169231534004211\n",
            "argm 0.9896875023841858 0.08514896035194397\n",
            "argm 1.1348876953125 0.16316212713718414\n",
            "argm 0.9029210805892944 0.8922555446624756\n",
            "argm 1.0121586322784424 0.5706287622451782\n",
            "argm 1.0981419086456299 0.09819678962230682\n",
            "argm 1.0121686458587646 0.6496388912200928\n",
            "argm 1.00461745262146 0.7169047594070435\n",
            "argm 1.0046772956848145 0.10211113840341568\n",
            "repr, std, cov, closslb 0.8598268032073975 0.476318359375 0.0008142206352204084 0.015408948063850403 0.004001724533736706\n",
            "0.056984740734689045 0.23499906799674372 1.0\n",
            "argm 1.2396202087402344 0.20511634647846222\n",
            "argm 1.0814480781555176 0.17155520617961884\n",
            "argm 1.2950079441070557 0.3508608937263489\n",
            "argm 1.1458951234817505 0.38578546047210693\n",
            "argm 0.9163771867752075 0.4671737551689148\n",
            "argm 1.0738215446472168 0.6386842727661133\n",
            "argm 1.0284969806671143 0.050710514187812805\n",
            "argm 1.1839438676834106 0.1822812557220459\n",
            "argm 1.052101492881775 0.4508011043071747\n",
            "argm 1.029069185256958 0.07878150790929794\n",
            "argm 1.1212143898010254 0.17807912826538086\n",
            "argm 0.9471679925918579 0.9330800175666809\n",
            "argm 1.0081028938293457 0.525017261505127\n",
            "argm 1.1449675559997559 0.07188878953456879\n",
            "argm 0.9590548276901245 0.7222860455513\n",
            "argm 1.0549492835998535 0.609200119972229\n",
            "argm 1.0367732048034668 0.13832934200763702\n",
            "repr, std, cov, closslb 0.8334861397743225 0.47119140625 0.0020361971110105515 0.030948637053370476 0.03733542189002037\n",
            "0.05647443300706202 0.23080900311479727 1.0\n",
            "argm 1.2035691738128662 0.20712800323963165\n",
            "argm 1.0463473796844482 0.1579066812992096\n",
            "argm 1.276565432548523 0.3671720623970032\n",
            "argm 1.1193664073944092 0.38519594073295593\n",
            "argm 1.055408000946045 0.655595600605011\n",
            "argm 1.0169366598129272 0.0649821013212204\n",
            "argm 1.1610686779022217 0.2330280840396881\n",
            "argm 1.0357575416564941 0.5030566453933716\n",
            "argm 1.0724551677703857 0.060360878705978394\n",
            "argm 1.0353007316589355 0.529451310634613\n",
            "argm 1.001488208770752 0.6834693551063538\n",
            "argm 0.9878836274147034 0.09764917194843292\n",
            "argm 1.1431939601898193 0.23193004727363586\n",
            "argm 0.9098712205886841 0.7782832980155945\n",
            "argm 1.0087320804595947 0.4385523498058319\n",
            "argm 1.1169507503509521 0.09103305637836456\n",
            "repr, std, cov, closslb 1.0035277605056763 0.472412109375 0.0016031451523303986 0.01566406525671482 0.03186662495136261\n",
            "0.05563405521345984 0.22533823366569236 1.0\n",
            "argm 1.246305227279663 0.2023056149482727\n",
            "argm 1.0828863382339478 0.14455989003181458\n",
            "argm 1.3054430484771729 0.3897417485713959\n",
            "argm 1.1194345951080322 0.42516645789146423\n",
            "argm 0.9029165506362915 0.6431490182876587\n",
            "argm 1.0660812854766846 0.6021280288696289\n",
            "argm 1.0584566593170166 0.07256772369146347\n",
            "argm 1.154167652130127 0.3117956221103668\n",
            "argm 0.9207766652107239 0.8294346332550049\n",
            "argm 1.0250086784362793 0.48221367597579956\n",
            "argm 1.147667407989502 0.14649051427841187\n",
            "argm 0.9823089838027954 0.8722987174987793\n",
            "argm 1.0330212116241455 0.5938282012939453\n",
            "argm 1.0330448150634766 0.059975311160087585\n",
            "argm 1.1012749671936035 0.36288613080978394\n",
            "argm 0.9590092897415161 0.782685399055481\n",
            "argm 0.9966199398040771 0.2256743609905243\n",
            "argm 1.1433587074279785 0.09059440344572067\n",
            "repr, std, cov, closslb 0.8158957958221436 0.471435546875 0.0018074947874993086 0.015532203949987888 0.009301407262682915\n",
            "0.05469673464429831 0.2199971355810548 1.0\n",
            "argm 1.1960493326187134 0.206041157245636\n",
            "argm 1.0419642925262451 0.1401035189628601\n",
            "argm 1.261586308479309 0.36897698044776917\n",
            "argm 1.0834944248199463 0.4096766710281372\n",
            "argm 1.0333003997802734 0.559660792350769\n",
            "argm 1.009002447128296 0.06075762212276459\n",
            "argm 1.0802524089813232 0.3334125280380249\n",
            "argm 0.9221185445785522 0.8476613163948059\n",
            "argm 0.9734763503074646 0.43119585514068604\n",
            "argm 1.1227211952209473 0.09091291576623917\n",
            "argm 1.0137736797332764 0.5555609464645386\n",
            "argm 1.0113991498947144 0.07429160177707672\n",
            "argm 1.0457552671432495 0.39303022623062134\n",
            "argm 0.9421063661575317 0.8935446739196777\n",
            "argm 0.9540688991546631 0.16480498015880585\n",
            "argm 1.1096419095993042 0.07719174027442932\n",
            "repr, std, cov, closslb 0.8810659646987915 0.470947265625 0.0022298789117485285 0.0001197651945403777 0.02507249265909195\n",
            "0.054315384427293695 0.21629063172212254 1.0\n",
            "argm 1.1959205865859985 0.19820965826511383\n",
            "argm 1.0446581840515137 0.15930086374282837\n",
            "argm 1.2739238739013672 0.3686668276786804\n",
            "argm 1.087539792060852 0.3983471691608429\n",
            "argm 1.0293545722961426 0.6008175611495972\n",
            "argm 1.0375386476516724 0.08308980613946915\n",
            "argm 1.0753051042556763 0.39786940813064575\n",
            "argm 0.9397506713867188 0.9761665463447571\n",
            "argm 0.9751315116882324 0.3796471357345581\n",
            "argm 1.144416093826294 0.09943172335624695\n",
            "argm 1.0093947649002075 0.6015971302986145\n",
            "argm 1.0627378225326538 0.07123779505491257\n",
            "argm 0.9975007176399231 0.7253587245941162\n",
            "argm 0.963508129119873 0.8964873552322388\n",
            "argm 0.992110013961792 0.1405421644449234\n",
            "argm 1.0990869998931885 0.28147828578948975\n",
            "repr, std, cov, closslb 0.7695649862289429 0.475341796875 0.0009937963914126158 3.54073490598239e-05 0.017810558900237083\n",
            "0.05458750504669988 0.2160745571649576 1.0\n",
            "argm 1.1936414241790771 0.20723697543144226\n",
            "argm 1.028223991394043 0.14535337686538696\n",
            "argm 1.2458763122558594 0.3737546503543854\n",
            "argm 1.053119421005249 0.41079986095428467\n",
            "argm 0.9995829463005066 0.5872333645820618\n",
            "argm 1.0135626792907715 0.058047473430633545\n",
            "argm 1.0133934020996094 0.4973178803920746\n",
            "argm 0.9279636144638062 0.9586633443832397\n",
            "argm 0.9385202527046204 0.21506845951080322\n",
            "argm 1.1129982471466064 0.1299530267715454\n",
            "argm 0.9780431985855103 0.5833117961883545\n",
            "argm 1.0688459873199463 0.08431077003479004\n",
            "argm 0.9609955549240112 0.7176083326339722\n",
            "argm 1.007267951965332 0.07520243525505066\n",
            "argm 0.9668013453483582 0.6881660223007202\n",
            "repr, std, cov, closslb 0.7865680456161499 0.473388671875 0.0015234372112900019 0.00014041840040590614 0.0006398599362000823\n",
            "0.05426112330398971 0.21285922148232853 1.0\n",
            "argm 1.1677945852279663 0.18603913486003876\n",
            "argm 1.0040128231048584 0.1478651463985443\n",
            "argm 1.2306218147277832 0.3726901412010193\n",
            "argm 1.039199948310852 0.4125041961669922\n",
            "argm 0.9813182950019836 0.49276649951934814\n",
            "argm 1.0176513195037842 0.03682728856801987\n",
            "argm 0.9644899368286133 0.5875853896141052\n",
            "argm 0.9368104934692383 0.7374706268310547\n",
            "argm 0.921131432056427 0.13763922452926636\n",
            "argm 1.0773882865905762 0.19118282198905945\n",
            "argm 0.9423589110374451 0.6055673360824585\n",
            "argm 1.0768961906433105 0.0872444361448288\n",
            "argm 0.983506441116333 0.4721302390098572\n",
            "argm 1.0035345554351807 0.11588539928197861\n",
            "argm 1.00946044921875 0.12907293438911438\n",
            "repr, std, cov, closslb 0.96251380443573 0.478515625 0.0004947935231029987 0.00021960557205602527 0.010002950206398964\n",
            "0.05480618281032154 0.21759162420098926 1.0\n",
            "argm 1.1749475002288818 0.18260255455970764\n",
            "argm 1.0188937187194824 0.12311339378356934\n",
            "argm 1.2490299940109253 0.3744950592517853\n",
            "argm 1.0493546724319458 0.42907243967056274\n",
            "argm 0.9875807166099548 0.5226060748100281\n",
            "argm 1.0484541654586792 0.05585700273513794\n",
            "argm 0.9533864855766296 0.6663277745246887\n",
            "argm 0.9463304281234741 0.7120756506919861\n",
            "argm 0.9248006939888 0.15815812349319458\n",
            "argm 1.066570520401001 0.19646622240543365\n",
            "argm 0.9121778011322021 0.49930718541145325\n",
            "argm 1.0603694915771484 0.10626297444105148\n",
            "argm 0.9635264873504639 0.5426779389381409\n",
            "argm 1.003690242767334 0.09215064346790314\n",
            "argm 0.9419595003128052 0.567547082901001\n",
            "repr, std, cov, closslb 0.8272225260734558 0.47509765625 0.0009028853382915258 0.00019731435168068856 0.0011650064261630177\n",
            "0.054642092551746575 0.21521241536792893 1.0\n",
            "argm 1.1874017715454102 0.20203180611133575\n",
            "argm 1.0211070775985718 0.12932159006595612\n",
            "argm 1.2481400966644287 0.36671385169029236\n",
            "argm 1.0480353832244873 0.4392452538013458\n",
            "argm 0.9712017774581909 0.6084283590316772\n",
            "argm 1.0351508855819702 0.03874395042657852\n",
            "argm 0.9441190958023071 0.7053455114364624\n",
            "argm 0.9496331214904785 0.9173262715339661\n",
            "argm 0.9284757971763611 0.07932353764772415\n",
            "argm 1.0634739398956299 0.2011740505695343\n",
            "argm 0.9392445683479309 0.2920841872692108\n",
            "argm 1.0970388650894165 0.17103272676467896\n",
            "argm 0.968544065952301 0.454142689704895\n",
            "argm 1.0657702684402466 0.2220226526260376\n",
            "repr, std, cov, closslb 0.8795626163482666 0.471923828125 0.0019499738700687885 0.015632765367627144 0.0008679615566506982\n",
            "0.054642092551746575 0.21328515278451463 1.0\n",
            "argm 1.1678311824798584 0.21399837732315063\n",
            "argm 1.0087695121765137 0.1486893743276596\n",
            "argm 1.2266796827316284 0.363864004611969\n",
            "argm 1.017572283744812 0.44096076488494873\n",
            "argm 0.9576152563095093 0.5899094343185425\n",
            "argm 1.024139404296875 0.046768225729465485\n",
            "argm 0.93357914686203 0.7334277629852295\n",
            "argm 0.9190236926078796 0.9334274530410767\n",
            "argm 0.9169880151748657 0.06390362977981567\n",
            "argm 1.0494520664215088 0.24070031940937042\n",
            "argm 0.9257023930549622 0.4556318521499634\n",
            "argm 1.075249433517456 0.10782759636640549\n",
            "argm 0.9513183832168579 0.5512090921401978\n",
            "argm 0.9731442928314209 0.07970072329044342\n",
            "argm 0.9498203992843628 0.6479217410087585\n",
            "repr, std, cov, closslb 0.8065989017486572 0.473876953125 0.0014291144907474518 9.999459871323779e-05 0.011623992584645748\n",
            "0.0538828102057498 0.20822974384441853 1.0\n",
            "argm 1.117660403251648 0.1812698245048523\n",
            "argm 0.953324019908905 0.12323632836341858\n",
            "argm 1.176537275314331 0.3386540412902832\n",
            "argm 0.9734435081481934 0.4134680926799774\n",
            "argm 0.9134040474891663 0.539003849029541\n",
            "argm 1.005132794380188 0.04169438034296036\n",
            "argm 0.928551435470581 0.8070303797721863\n",
            "argm 0.9139972925186157 0.06230827420949936\n",
            "argm 0.9976968765258789 0.3359522819519043\n",
            "argm 1.0623111724853516 0.10715983808040619\n",
            "argm 0.9218177199363708 0.45891034603118896\n",
            "argm 1.004166603088379 0.08169561624526978\n",
            "repr, std, cov, closslb 0.6697869300842285 0.475341796875 0.001044522039592266 0.015511426143348217 0.007982134819030762\n",
            "0.05415276362397814 0.2078139082140822 1.0\n",
            "argm 1.129650354385376 0.1805964857339859\n",
            "argm 0.952896773815155 0.11290982365608215\n",
            "argm 1.1850957870483398 0.34089478850364685\n",
            "argm 0.9769749045372009 0.42079946398735046\n",
            "argm 0.9194658994674683 0.5118604898452759\n",
            "argm 1.0187089443206787 0.07630325853824615\n",
            "argm 0.9339362382888794 0.6821355223655701\n",
            "argm 0.913852334022522 0.07908046245574951\n",
            "argm 0.9930259585380554 0.3284309208393097\n",
            "argm 1.045242428779602 0.07778964936733246\n",
            "argm 0.9883555173873901 0.08055847138166428\n",
            "repr, std, cov, closslb 0.6233469247817993 0.4736328125 0.0011864574626088142 0.00037114572478458285 0.00547759048640728\n",
            "0.05447849358104422 0.20760630191217003 1.0\n",
            "argm 1.1279821395874023 0.19205734133720398\n",
            "argm 0.9585423469543457 0.10540522634983063\n",
            "argm 1.177475094795227 0.3506038188934326\n",
            "argm 0.9701663255691528 0.4460737109184265\n",
            "argm 0.9048219323158264 0.529029369354248\n",
            "argm 1.031942367553711 0.08697342127561569\n",
            "argm 0.9469778537750244 0.684848427772522\n",
            "argm 0.9157668948173523 0.06250664591789246\n",
            "argm 0.9907528162002563 0.30358973145484924\n",
            "argm 1.0497925281524658 0.12775418162345886\n",
            "argm 0.9954335689544678 0.11142902076244354\n",
            "repr, std, cov, closslb 0.6483369469642639 0.47412109375 0.001433271449059248 0.00022453760902862996 0.03297380357980728\n",
            "0.05453297207462526 0.20636501223319056 1.0\n",
            "argm 1.0878243446350098 0.19842758774757385\n",
            "argm 0.9190781712532043 0.09066145867109299\n",
            "argm 1.1317243576049805 0.3243768811225891\n",
            "argm 0.907177746295929 0.4377449154853821\n",
            "argm 0.9800934791564941 0.04646088927984238\n",
            "argm 0.9251618385314941 0.34949183464050293\n",
            "argm 0.9758520126342773 0.18077808618545532\n",
            "argm 0.9922159910202026 0.08572451770305634\n",
            "repr, std, cov, closslb 0.6560750007629395 0.474365234375 0.001118744257837534 0.015351695939898491 0.006010727025568485\n",
            "0.05458750504669988 0.20657137724542374 1.0\n",
            "argm 1.0629897117614746 0.17123949527740479\n",
            "argm 0.9023861289024353 0.06933789700269699\n",
            "argm 1.1249597072601318 0.3189677596092224\n",
            "argm 0.9004906415939331 0.4271501898765564\n",
            "argm 0.9702470302581787 0.04582386463880539\n",
            "argm 0.9127850532531738 0.33336228132247925\n",
            "argm 0.9564542174339294 0.27156805992126465\n",
            "argm 0.9635598659515381 0.08459575474262238\n",
            "repr, std, cov, closslb 0.6464736461639404 0.47607421875 0.0009548943489789963 0.015492110513150692 0.0021297866478562355\n",
            "0.055025736597939204 0.2109530321372749 1.0\n",
            "argm 1.0843067169189453 0.1978520303964615\n",
            "argm 0.9179649353027344 0.08263438940048218\n",
            "argm 1.1310186386108398 0.3534899950027466\n",
            "argm 0.9111021161079407 0.43901893496513367\n",
            "argm 1.0116596221923828 0.06173271685838699\n",
            "argm 0.9191460013389587 0.4092855453491211\n",
            "argm 0.9683315753936768 0.2911314368247986\n",
            "argm 0.9854726791381836 0.05969032645225525\n",
            "repr, std, cov, closslb 0.6688545942306519 0.47509765625 0.0009733745828270912 0.015421568416059017 0.046360958367586136\n",
            "0.05491584998212498 0.20969173217128348 1.0\n",
            "argm 1.0790657997131348 0.18009915947914124\n",
            "argm 0.9150749444961548 0.07023196667432785\n",
            "argm 1.1298387050628662 0.3533838391304016\n",
            "argm 0.9076331853866577 0.43132996559143066\n",
            "argm 1.0030951499938965 0.0647110864520073\n",
            "argm 0.9401776790618896 0.2552652657032013\n",
            "argm 0.9802150726318359 0.11464487761259079\n",
            "repr, std, cov, closslb 0.6599178314208984 0.47802734375 0.0005789280403405428 0.015525850467383862 0.018367376178503036\n",
            "0.05541207422242113 0.21456806724741695 1.0\n",
            "argm 1.1013102531433105 0.18150731921195984\n",
            "argm 0.9400033354759216 0.06661936640739441\n",
            "argm 1.1416081190109253 0.35205110907554626\n",
            "argm 0.9238775372505188 0.46067333221435547\n",
            "argm 0.9889514446258545 0.10842829942703247\n",
            "argm 0.926216721534729 0.2979571223258972\n",
            "argm 0.9745157957077026 0.2503758370876312\n",
            "argm 0.9765000939369202 0.26755034923553467\n",
            "repr, std, cov, closslb 0.741346001625061 0.476806640625 0.0007091639563441277 0.015264044515788555 0.0009409214253537357\n",
            "0.05563405521345984 0.21715709285818008 1.0\n",
            "argm 1.0644211769104004 0.17196032404899597\n",
            "argm 0.9061463475227356 0.07146131247282028\n",
            "argm 1.1152660846710205 0.3362651765346527\n",
            "argm 0.9912309646606445 0.15079882740974426\n",
            "argm 0.9178154468536377 0.26697200536727905\n",
            "argm 0.9785385727882385 0.14893803000450134\n",
            "argm 0.912365198135376 0.15683099627494812\n",
            "repr, std, cov, closslb 0.7394940257072449 0.476806640625 0.0008114376105368137 3.9916776586323977e-05 0.015542423352599144\n",
            "0.05557847673672312 0.21715709285818008 1.0\n",
            "argm 1.0573997497558594 0.18455833196640015\n",
            "argm 1.1068295240402222 0.35700762271881104\n",
            "argm 0.9873907566070557 0.1762462854385376\n",
            "argm 0.9511443376541138 0.10722968727350235\n",
            "argm 0.9504583477973938 0.08990149945020676\n",
            "repr, std, cov, closslb 0.7930068969726562 0.473876953125 0.0013369962107390165 6.95777271175757e-05 0.020516298711299896\n",
            "0.0547514313789426 0.21179811082803982 1.0\n",
            "argm 1.0686500072479248 0.18713995814323425\n",
            "argm 0.91566002368927 0.08735340088605881\n",
            "argm 1.1326210498809814 0.3534899950027466\n",
            "argm 0.9869370460510254 0.1138513907790184\n",
            "argm 0.9071415662765503 0.048249028623104095\n",
            "argm 0.9555609226226807 0.13573653995990753\n",
            "repr, std, cov, closslb 0.6994527578353882 0.474365234375 0.001054679974913597 0.0312906950712204 0.015825947746634483\n",
            "0.054642092551746575 0.20969173217128348 1.0\n",
            "argm 1.0922210216522217 0.2204580307006836\n",
            "argm 0.9269696474075317 0.08975900709629059\n",
            "argm 1.1395299434661865 0.3605000674724579\n",
            "argm 1.009880542755127 0.13005080819129944\n",
            "argm 0.9045232534408569 0.05399342626333237\n",
            "argm 0.9072396159172058 0.40762871503829956\n",
            "argm 0.9639618992805481 0.3203144073486328\n",
            "argm 0.9839813709259033 0.2651028335094452\n",
            "repr, std, cov, closslb 0.5848809480667114 0.471435546875 0.0016238123644143343 0.00015297997742891312 0.001839553122408688\n",
            "0.05366781671725557 0.20492621807648137 1.0\n",
            "argm 1.1114985942840576 0.2202233374118805\n",
            "argm 0.9380251169204712 0.1099761575460434\n",
            "argm 1.139451503753662 0.35340338945388794\n",
            "argm 1.018129587173462 0.15739259123802185\n",
            "argm 0.9016637802124023 0.04611164331436157\n",
            "argm 0.9177018404006958 0.372961163520813\n",
            "argm 0.977785587310791 0.36562418937683105\n",
            "argm 0.9799414873123169 0.29905232787132263\n",
            "repr, std, cov, closslb 0.636519193649292 0.47314453125 0.0012430930510163307 6.937456782907248e-05 0.05431536212563515\n",
            "0.052922072477263056 0.20046927542020423 1.0\n",
            "argm 1.033937931060791 0.19493791460990906\n",
            "argm 1.0836464166641235 0.32443273067474365\n",
            "argm 0.9563697576522827 0.15642307698726654\n",
            "repr, std, cov, closslb 0.6069254875183105 0.47265625 0.0012732974719256163 6.378351827152073e-05 0.025874411687254906\n",
            "0.05318721258985949 0.1998690684079063 1.0\n",
            "argm 1.0470523834228516 0.18606148660182953\n",
            "argm 1.093947172164917 0.3025531768798828\n",
            "argm 0.9847824573516846 0.16202497482299805\n",
            "argm 0.9260056018829346 0.1320876181125641\n",
            "argm 0.9491945505142212 0.10664016008377075\n",
            "repr, std, cov, closslb 0.6155142188072205 0.470703125 0.001843190984800458 0.00032316576107405126 0.00038242735899984837\n",
            "0.052816386887101975 0.19709177624968174 1.0\n",
            "argm 1.0363627672195435 0.20709726214408875\n",
            "argm 1.0907056331634521 0.31980592012405396\n",
            "argm 0.9753551483154297 0.2056863158941269\n",
            "argm 0.9191433191299438 0.15024003386497498\n",
            "repr, std, cov, closslb 0.6726709604263306 0.471435546875 0.001517340773716569 0.015358428470790386 0.03422271087765694\n",
            "0.052291116337562914 0.19357760346072553 1.0\n",
            "argm 1.0129761695861816 0.2125035971403122\n",
            "argm 1.0684313774108887 0.3341752886772156\n",
            "argm 0.9479853510856628 0.2175886183977127\n",
            "repr, std, cov, closslb 0.6582764983177185 0.47216796875 0.0017032623291015625 3.644591924967244e-05 0.013639320619404316\n",
            "0.05146152760030545 0.18898931433529156 1.0\n",
            "argm 1.0074994564056396 0.1934124231338501\n",
            "argm 1.056487798690796 0.2937912940979004\n",
            "argm 0.9327524900436401 0.170686274766922\n",
            "repr, std, cov, closslb 0.6286245584487915 0.474609375 0.0010064912494271994 0.00015826585877221078 0.018784446641802788\n",
            "0.05182284078854286 0.19088773471624246 1.0\n",
            "argm 1.0178302526474 0.21034665405750275\n",
            "argm 1.0474255084991455 0.28684550523757935\n",
            "argm 0.9340149760246277 0.16111694276332855\n",
            "repr, std, cov, closslb 0.755815327167511 0.47607421875 0.0008478234522044659 0.0003040463197976351 0.0017184041207656264\n",
            "0.052343407453900474 0.1926126123457839 1.0\n",
            "argm 1.0193513631820679 0.20278897881507874\n",
            "argm 1.0648632049560547 0.30889827013015747\n",
            "argm 0.9253283739089966 0.18457509577274323\n",
            "repr, std, cov, closslb 0.717544436454773 0.47509765625 0.0009418088011443615 0.01570773497223854 0.020957598462700844\n",
            "0.052291116337562914 0.19088773471624246 1.0\n",
            "argm 0.9848188161849976 0.21619722247123718\n",
            "argm 1.0258941650390625 0.31389668583869934\n",
            "argm 0.9061596989631653 0.1826668083667755\n",
            "repr, std, cov, closslb 0.5479947328567505 0.4775390625 0.0006321314722299576 3.3022137358784676e-05 0.028454391285777092\n",
            "0.05265825409738927 0.19474197661999856 1.0\n",
            "argm 0.9473921060562134 0.19288715720176697\n",
            "argm 0.9946334362030029 0.2851411700248718\n",
            "repr, std, cov, closslb 0.5838955640792847 0.478271484375 0.0005603032186627388 0.00011984081356786191 0.03156963363289833\n",
            "0.05286920327398907 0.19669818318512836 1.0\n",
            "argm 0.9650592803955078 0.2039121389389038\n",
            "argm 1.0113606452941895 0.27665311098098755\n",
            "repr, std, cov, closslb 0.5626059770584106 0.479248046875 0.0004279117565602064 0.00012147441157139838 0.010788856074213982\n",
            "0.052816386887101975 0.19571763587125834 1.0\n",
            "argm 0.988479733467102 0.17139874398708344\n",
            "argm 1.016655683517456 0.26218873262405396\n",
            "repr, std, cov, closslb 0.7898473143577576 0.475830078125 0.0008150467183440924 0.015890302136540413 0.01575329154729843\n",
            "0.053240399802449344 0.19669818318512836 1.0\n",
            "5\n",
            "argm 0.9849776029586792 0.1626005470752716\n",
            "argm 1.0115416049957275 0.24682191014289856\n",
            "repr, std, cov, closslb 0.6110643148422241 0.475830078125 0.0008097898680716753 5.246658838586882e-05 0.015156535431742668\n",
            "0.05361420251474083 0.19966939900889744 1.0\n",
            "argm 0.9584113955497742 0.1780819147825241\n",
            "argm 0.9851401448249817 0.2522282302379608\n",
            "repr, std, cov, closslb 0.5818756222724915 0.476318359375 0.0006583980284631252 2.7216206945013255e-05 0.02271212264895439\n",
            "0.053775206018506785 0.202078654003601 1.0\n",
            "argm 0.9654222130775452 0.2241097390651703\n",
            "argm 0.9945443272590637 0.2780836224555969\n",
            "repr, std, cov, closslb 0.5508484840393066 0.474365234375 0.0010502208024263382 5.7872010074788705e-05 0.0010448598768562078\n",
            "0.05345368105707278 0.20026900641379047 1.0\n",
            "argm 1.005516767501831 0.20162947475910187\n",
            "argm 1.0318186283111572 0.27119648456573486\n",
            "repr, std, cov, closslb 0.6194824576377869 0.47509765625 0.0008540810085833073 0.015804292634129524 0.016203803941607475\n",
            "0.053027969544290045 0.19709177624968174 1.0\n",
            "argm 0.9923823475837708 0.17139874398708344\n",
            "argm 1.0081238746643066 0.21161511540412903\n",
            "repr, std, cov, closslb 0.6185152530670166 0.474609375 0.0009090581443160772 0.030983082950115204 0.015924258157610893\n",
            "0.05265825409738927 0.1943530761146931 1.0\n",
            "argm 1.0046143531799316 0.16604550182819366\n",
            "argm 1.0317020416259766 0.2167224884033203\n",
            "repr, std, cov, closslb 0.7282520532608032 0.47314453125 0.0012613495346158743 5.196616257308051e-05 0.001017246046103537\n",
            "0.052082473739381005 0.19012608884364046 1.0\n",
            "argm 1.0505688190460205 0.19413325190544128\n",
            "argm 1.0639036893844604 0.26998668909072876\n",
            "argm 0.9295797944068909 0.17268118262290955\n",
            "argm 0.9135962724685669 0.11176709085702896\n",
            "repr, std, cov, closslb 0.583210289478302 0.4716796875 0.001650394406169653 0.01576005108654499 0.034706372767686844\n",
            "0.05120499008766032 0.18543417535297999 1.0\n",
            "argm 0.9828038215637207 0.2126293182373047\n",
            "argm 1.0102732181549072 0.2358304262161255\n",
            "repr, std, cov, closslb 0.6298553943634033 0.468994140625 0.002188607817515731 0.015727631747722626 0.051343005150556564\n",
            "0.0505945056268907 0.18158256652176377 1.0\n",
            "argm 0.971276044845581 0.19637122750282288\n",
            "argm 0.983534038066864 0.22342242300510406\n",
            "repr, std, cov, closslb 0.5340968370437622 0.470703125 0.001558714546263218 0.015365474857389927 0.017586983740329742\n",
            "0.0505945056268907 0.18013642123185533 1.0\n",
            "argm 0.9373924732208252 0.192009836435318\n",
            "argm 0.9680060744285583 0.21154247224330902\n",
            "repr, std, cov, closslb 0.4721185564994812 0.474609375 0.001030823215842247 0.00012772677291650325 0.0018798194359987974\n",
            "0.05089883259088574 0.17995646476708826 1.0\n",
            "argm 0.9588954448699951 0.2012634575366974\n",
            "argm 0.9597681760787964 0.2078404724597931\n",
            "repr, std, cov, closslb 0.4685491621494293 0.473388671875 0.001213669078424573 2.117756230290979e-05 0.0025142403319478035\n",
            "0.050695745232650094 0.17781095853483883 1.0\n",
            "argm 0.916565477848053 0.23462623357772827\n",
            "argm 0.9371015429496765 0.22490042448043823\n",
            "repr, std, cov, closslb 0.4957108497619629 0.4755859375 0.0008008009754121304 1.982158937607892e-05 0.028283260762691498\n",
            "0.05100068115490009 0.17798876949337364 1.0\n",
            "repr, std, cov, closslb 0.5179300904273987 0.471435546875 0.0016009234823286533 2.6988707759301178e-05 0.033285561949014664\n",
            "0.05089883259088574 0.1762186321501395 1.0\n",
            "argm 0.902860164642334 0.14457106590270996\n",
            "repr, std, cov, closslb 0.42299842834472656 0.47705078125 0.0006184137891978025 2.1913258024142124e-05 0.02297646924853325\n",
            "0.05110273351789103 0.17959709098802126 1.0\n",
            "argm 0.9176800847053528 0.2178764045238495\n",
            "argm 0.9198381900787354 0.14965328574180603\n",
            "repr, std, cov, closslb 0.4949492812156677 0.477783203125 0.0005243017803877592 7.96151434769854e-05 0.001828481676056981\n",
            "0.051410117482822634 0.18212785915061114 1.0\n",
            "argm 0.9207832217216492 0.15565752983093262\n",
            "repr, std, cov, closslb 0.41039180755615234 0.47705078125 0.0006356341764330864 7.00020173098892e-05 0.053395919501781464\n",
            "0.05171935036845559 0.18524892642655344 1.0\n",
            "repr, std, cov, closslb 0.42655467987060547 0.478271484375 0.0004586090799421072 0.01556810550391674 0.0059116799384355545\n",
            "0.05197846483125367 0.18842347844110952 1.0\n",
            "repr, std, cov, closslb 0.4290703535079956 0.476806640625 0.0006188780535012484 0.0001591418986208737 0.0030049853958189487\n",
            "0.052291116337562914 0.19184408417700272 1.0\n",
            "argm 0.9094296097755432 0.24219229817390442\n",
            "argm 0.9310545921325684 0.12044794857501984\n",
            "repr, std, cov, closslb 0.39783960580825806 0.476806640625 0.0006424929015338421 0.00041144859278574586 0.0004950147122144699\n",
            "0.05197846483125367 0.1910786224509587 1.0\n",
            "argm 0.9173802137374878 0.21792389452457428\n",
            "argm 0.9288233518600464 0.0799158588051796\n",
            "repr, std, cov, closslb 0.3758716285228729 0.47216796875 0.0013086530379951 0.015562242828309536 0.03624198958277702\n",
            "0.051051681836054984 0.18654956562919417 1.0\n",
            "argm 0.9083817601203918 0.17782486975193024\n",
            "argm 0.9140132665634155 0.0483999028801918\n",
            "repr, std, cov, closslb 0.3792032301425934 0.474609375 0.0008913488127291203 0.03140677884221077 0.0451970174908638\n",
            "0.051153836251408916 0.18823524319791163 1.0\n",
            "repr, std, cov, closslb 0.5095911026000977 0.4736328125 0.0010685089509934187 4.1136805521091446e-05 0.026343822479248047\n",
            "0.050493468197028456 0.18395735538941468 1.0\n",
            "argm 0.913461446762085 0.25965458154678345\n",
            "argm 0.9154307842254639 0.08648168295621872\n",
            "repr, std, cov, closslb 0.45321375131607056 0.471435546875 0.0013064853847026825 0.015513758175075054 0.0015942426398396492\n",
            "0.049692398809813416 0.17959709098802126 1.0\n",
            "argm 0.9361327886581421 0.26857852935791016\n",
            "argm 0.941836416721344 0.09128730744123459\n",
            "repr, std, cov, closslb 0.53370600938797 0.47265625 0.0011635571718215942 0.00015402593999169767 0.016834789887070656\n",
            "0.049001895218451295 0.17551551628964565 1.0\n",
            "argm 0.9128882884979248 0.24669617414474487\n",
            "argm 0.914746105670929 0.07065106183290482\n",
            "repr, std, cov, closslb 0.4316643476486206 0.4716796875 0.0012793613132089376 0.015254917554557323 0.002563282148912549\n",
            "0.04866024989909689 0.17307663031834003 1.0\n",
            "repr, std, cov, closslb 0.49374842643737793 0.4716796875 0.0013051142450422049 0.00018358728266321123 0.0012512211687862873\n",
            "0.04870891014899598 0.17186992554959005 1.0\n",
            "repr, std, cov, closslb 0.4166557788848877 0.47412109375 0.0010473269503563643 0.00023946422152221203 0.009587632492184639\n",
            "0.04812818485843192 0.16863682491446996 1.0\n",
            "repr, std, cov, closslb 0.3789980113506317 0.47314453125 0.0011734499130398035 0.0001456720638088882 0.018842097371816635\n",
            "0.04798408859240486 0.1671266544055436 1.0\n",
            "repr, std, cov, closslb 0.4467109739780426 0.47021484375 0.001593871507793665 0.015645800158381462 0.01759685017168522\n",
            "0.04764953950612974 0.1648043363479864 1.0\n",
            "repr, std, cov, closslb 0.5169082283973694 0.47216796875 0.001239212229847908 0.00030149781377986073 0.022598853334784508\n",
            "0.04788826417578912 0.1644752214299052 1.0\n",
            "repr, std, cov, closslb 0.41540035605430603 0.47265625 0.001064833253622055 0.00011666373029584065 0.0028862091712653637\n",
            "0.0479361524399649 0.16349181489104603 1.0\n",
            "repr, std, cov, closslb 0.3986085057258606 0.468505859375 0.00273320940323174 0.03110698238015175 0.015430158004164696\n",
            "0.04788826417578912 0.16218974651205917 1.0\n",
            "repr, std, cov, closslb 0.4245820939540863 0.476318359375 0.0006688432767987251 0.0002729470143094659 0.05464491248130798\n",
            "0.04822448935633363 0.16316532108355786 1.0\n",
            "repr, std, cov, closslb 0.505401074886322 0.4755859375 0.0007643015123903751 0.0004762246971949935 0.003942367620766163\n",
            "0.04827271384568996 0.16218974651205917 1.0\n",
            "repr, std, cov, closslb 0.504148542881012 0.476318359375 0.0005975232925266027 0.030876418575644493 0.0007404433563351631\n",
            "0.04861163826083606 0.16381896201264298 1.0\n",
            "6\n",
            "repr, std, cov, closslb 0.6033844351768494 0.47509765625 0.0008438441436737776 0.01610061340034008 0.016193296760320663\n",
            "0.048904038237937195 0.16629352219557703 1.0\n",
            "repr, std, cov, closslb 0.41060104966163635 0.474853515625 0.0008971171919256449 0.015537699684500694 0.017054539173841476\n",
            "0.049149047958794184 0.16746107484100906 1.0\n",
            "repr, std, cov, closslb 0.5117233395576477 0.474853515625 0.0009344783611595631 0.00014896116044837981 0.030572842806577682\n",
            "0.04939528518080399 0.16762853591585006 1.0\n",
            "repr, std, cov, closslb 0.3476787507534027 0.476318359375 0.0006216007750481367 4.166570943198167e-05 0.022596130147576332\n",
            "0.049149047958794184 0.1671266544055436 1.0\n",
            "repr, std, cov, closslb 0.35285377502441406 0.4765625 0.0007858930621296167 2.0716124708997086e-05 0.01578076183795929\n",
            "0.049099948010783406 0.1671266544055436 1.0\n",
            "repr, std, cov, closslb 0.318947970867157 0.474365234375 0.0009998949244618416 0.030373776331543922 0.0030082426965236664\n",
            "0.04827271384568996 0.16300231876479307 1.0\n",
            "repr, std, cov, closslb 0.30528655648231506 0.47509765625 0.0008710341062396765 0.000391036388464272 0.014206958934664726\n",
            "0.04760193756856118 0.16025606153775743 1.0\n",
            "repr, std, cov, closslb 0.32422780990600586 0.471435546875 0.0013772582169622183 0.015771502628922462 0.016170116141438484\n",
            "0.04679994182973249 0.15630127773264016 1.0\n",
            "repr, std, cov, closslb 0.29873406887054443 0.47607421875 0.0007013911381363869 0.000131611421238631 0.015586868859827518\n",
            "0.04665982233659589 0.15490157407800212 1.0\n",
            "repr, std, cov, closslb 0.31161797046661377 0.4755859375 0.0008061756379902363 0.00015724028344266117 0.0006766888545826077\n",
            "0.046846741771562214 0.15614513260004015 1.0\n",
            "repr, std, cov, closslb 0.3081166744232178 0.4765625 0.0007425309158861637 8.508843893650919e-05 0.0038520991802215576\n",
            "0.047081444416539414 0.15834543065694964 1.0\n",
            "repr, std, cov, closslb 0.327330082654953 0.47314453125 0.0010729790665209293 0.015488752163946629 0.046894870698451996\n",
            "0.047081444416539414 0.15882094214355783 1.0\n",
            "repr, std, cov, closslb 0.2973341643810272 0.471923828125 0.0015586563386023045 5.38237982254941e-05 0.009516118094325066\n",
            "0.047034410006532884 0.16009596557218525 1.0\n",
            "repr, std, cov, closslb 0.29106444120407104 0.476806640625 0.0007026777602732182 0.00010312806989531964 0.041907571256160736\n",
            "0.047081444416539414 0.16089804796146212 1.0\n",
            "repr, std, cov, closslb 0.29832297563552856 0.476318359375 0.0007748070638626814 3.901650779880583e-05 0.0029557272791862488\n",
            "0.04679994182973249 0.15993602954264263 1.0\n",
            "repr, std, cov, closslb 0.29988959431648254 0.475830078125 0.0007442270871251822 0.015427161939442158 0.018090028315782547\n",
            "0.04665982233659589 0.15882094214355783 1.0\n",
            "repr, std, cov, closslb 0.2915274202823639 0.477294921875 0.0006215835455805063 0.00020434374164324254 0.01881013996899128\n",
            "0.0467531886410914 0.1597762532893533 1.0\n",
            "repr, std, cov, closslb 0.29663360118865967 0.47216796875 0.0014672609977424145 0.045818962156772614 0.004169990308582783\n",
            "0.04689358851333377 0.1618658529403256 1.0\n",
            "repr, std, cov, closslb 0.25773125886917114 0.475830078125 0.0006802836433053017 0.00013900912017561495 0.029129022732377052\n",
            "0.04656664248498344 0.16009596557218525 1.0\n",
            "repr, std, cov, closslb 0.24992692470550537 0.473876953125 0.0009902785532176495 0.01565825566649437 0.013943245634436607\n",
            "0.04638084065176276 0.16025606153775743 1.0\n",
            "repr, std, cov, closslb 0.27498286962509155 0.47607421875 0.0009214594028890133 1.8134152924176306e-05 0.03203842043876648\n",
            "0.046241975951737735 0.15882094214355783 1.0\n",
            "repr, std, cov, closslb 0.3605363368988037 0.47216796875 0.0013630171306431293 0.015552070923149586 0.006734635680913925\n",
            "0.045553864045996845 0.15490157407800212 1.0\n",
            "repr, std, cov, closslb 0.2676520347595215 0.474365234375 0.0011335229501128197 3.6123823520028964e-05 0.001330436673015356\n",
            "0.04514592169956809 0.1521396582489936 1.0\n",
            "repr, std, cov, closslb 0.29253658652305603 0.470947265625 0.002017081016674638 0.0470447912812233 0.046277955174446106\n",
            "0.04519106762126765 0.15138123673836504 1.0\n",
            "repr, std, cov, closslb 0.27010577917099 0.478759765625 0.0005318070761859417 0.0002522276481613517 0.005386228673160076\n",
            "0.04541747532218685 0.1521396582489936 1.0\n",
            "repr, std, cov, closslb 0.2828635573387146 0.471435546875 0.0012729736045002937 0.01545355562120676 0.017601555213332176\n",
            "0.04537210321896789 0.15107892780382962 1.0\n",
            "repr, std, cov, closslb 0.25748685002326965 0.476318359375 0.0007602362893521786 0.00026179238921031356 0.04446304216980934\n",
            "0.04496578857064598 0.14883077108498857 1.0\n",
            "repr, std, cov, closslb 0.2991105020046234 0.473876953125 0.0011478669475764036 0.00033464853186160326 0.003127600532025099\n",
            "0.04496578857064598 0.14749796704632553 1.0\n",
            "repr, std, cov, closslb 0.2491360604763031 0.474365234375 0.00106736458837986 0.00017119492986239493 0.0164262093603611\n",
            "0.04460767564937794 0.14530309744769326 1.0\n",
            "repr, std, cov, closslb 0.26297396421432495 0.47705078125 0.000645617488771677 9.512299584457651e-05 0.004170459229499102\n",
            "0.044831160550681134 0.14573944279463172 1.0\n",
            "repr, std, cov, closslb 0.2493128776550293 0.47509765625 0.00082989945076406 2.1981120880809613e-05 0.003750158939510584\n",
            "0.04501075435921662 0.14764546501337183 1.0\n",
            "repr, std, cov, closslb 0.2506862282752991 0.476806640625 0.0006201392970979214 3.3418273233110085e-05 0.03169994056224823\n",
            "0.04532677644252537 0.15047611986559317 1.0\n",
            "repr, std, cov, closslb 0.24359670281410217 0.478271484375 0.0005434171762317419 7.635197835043073e-05 0.04519781097769737\n",
            "0.045553864045996845 0.1524440897051498 1.0\n",
            "repr, std, cov, closslb 0.26249629259109497 0.47802734375 0.0005932115018367767 0.00012573416461236775 0.01457161270081997\n",
            "0.045553864045996845 0.15336104395589556 1.0\n",
            "repr, std, cov, closslb 0.28249964118003845 0.477294921875 0.0006241460796445608 8.99093720363453e-05 0.031349994242191315\n",
            "0.04550835569030654 0.15259653379485494 1.0\n",
            "7\n",
            "repr, std, cov, closslb 0.3108149766921997 0.47607421875 0.0008195645641535521 8.433844777755439e-05 0.0032057822681963444\n",
            "0.045553864045996845 0.1516841505930785 1.0\n",
            "repr, std, cov, closslb 0.25100842118263245 0.474609375 0.0009366972371935844 7.086124969646335e-05 0.014010937884449959\n",
            "0.044786374176504634 0.14823693333694485 1.0\n",
            "repr, std, cov, closslb 0.2660960257053375 0.470947265625 0.0018555440474301577 0.015477470122277737 0.03653617575764656\n",
            "0.04460767564937794 0.1477931104783852 1.0\n",
            "repr, std, cov, closslb 0.25711214542388916 0.4765625 0.0008557841647416353 2.567966112110298e-05 0.010148782283067703\n",
            "0.0444296901329422 0.1469094472130056 1.0\n",
            "repr, std, cov, closslb 0.2277451455593109 0.47314453125 0.001130858901888132 0.046503324061632156 0.016754942014813423\n",
            "0.04416404253316169 0.14617709848708338 1.0\n",
            "repr, std, cov, closslb 0.19546160101890564 0.47509765625 0.0009017684496939182 0.00017075115465559065 0.0031911004334688187\n",
            "0.04376854626701475 0.14472333518789257 1.0\n",
            "repr, std, cov, closslb 0.18878823518753052 0.474365234375 0.0008747517131268978 0.000444982637418434 0.015980936586856842\n",
            "0.043856127128095036 0.14544840054514094 1.0\n",
            "repr, std, cov, closslb 0.1869957447052002 0.4775390625 0.0005144078750163317 5.374578540795483e-05 0.01519882120192051\n",
            "0.04407584676378736 0.1480888444924524 1.0\n",
            "repr, std, cov, closslb 0.22681355476379395 0.475830078125 0.0007334407418966293 3.983401256846264e-05 0.0004757728602271527\n",
            "0.04355035853503287 0.14603106741966373 1.0\n",
            "repr, std, cov, closslb 0.23223838210105896 0.47412109375 0.0008966799359768629 0.015641208738088608 0.0014778273180127144\n",
            "0.04363750280246147 0.14646959886115601 1.0\n",
            "repr, std, cov, closslb 0.1764833629131317 0.473388671875 0.0010138950310647488 0.015409701503813267 0.001401964109390974\n",
            "0.04346338829505447 0.14588518223742633 1.0\n",
            "repr, std, cov, closslb 0.18188418447971344 0.47509765625 0.0008228945080190897 0.00015534297563135624 0.0011823349632322788\n",
            "0.04328996850800825 0.14646959886115601 1.0\n",
            "repr, std, cov, closslb 0.21144670248031616 0.478515625 0.00047022406943142414 0.00018058621208183467 0.039939034730196\n",
            "0.04350685168334952 0.14868208899599258 1.0\n",
            "repr, std, cov, closslb 0.1774902045726776 0.474853515625 0.0007673252839595079 0.00015193663421086967 0.0004490454157348722\n",
            "0.043246721786222034 0.1470563566602186 1.0\n",
            "repr, std, cov, closslb 0.1617100089788437 0.474609375 0.0008974012453109026 0.015407979488372803 0.0003971585538238287\n",
            "0.043246721786222034 0.1480888444924524 1.0\n",
            "repr, std, cov, closslb 0.15211151540279388 0.475830078125 0.0008631108794361353 0.0001385517098242417 0.005856554955244064\n",
            "0.043246721786222034 0.15047611986559317 1.0\n",
            "repr, std, cov, closslb 0.18769705295562744 0.4765625 0.0006395275704562664 0.0001169345632661134 0.008450743742287159\n",
            "0.043117240669374675 0.15062659598545874 1.0\n",
            "repr, std, cov, closslb 0.1498338282108307 0.4736328125 0.0010599782690405846 0.015401601791381836 0.03437194973230362\n",
            "0.042603181290106075 0.14853355544055205 1.0\n",
            "repr, std, cov, closslb 0.14575819671154022 0.476318359375 0.0006553293205797672 0.0001392325502820313 0.0005406867130659521\n",
            "0.042306148340411284 0.14735061642989566 1.0\n",
            "repr, std, cov, closslb 0.13441434502601624 0.47705078125 0.0006616003811359406 0.0002331008145119995 0.03397304564714432\n",
            "0.04234845448875169 0.1492777100393875 1.0\n",
            "repr, std, cov, closslb 0.13655830919742584 0.474609375 0.0008797980844974518 0.00018720992375165224 0.0011310321278870106\n",
            "0.04175999909892366 0.14735061642989566 1.0\n",
            "repr, std, cov, closslb 0.1342451125383377 0.474853515625 0.0008329597767442465 0.01577283814549446 0.004882723558694124\n",
            "0.04126212124584718 0.1450129265816035 1.0\n",
            "repr, std, cov, closslb 0.1342819333076477 0.47509765625 0.0008857166394591331 0.0002949172630906105 0.02277730405330658\n",
            "0.04068876106219974 0.14228504162132094 1.0\n",
            "repr, std, cov, closslb 0.13606248795986176 0.471435546875 0.0013629656750708818 0.0002240920002805069 0.010968828573822975\n",
            "0.040203654897470145 0.1400277161657451 1.0\n",
            "repr, std, cov, closslb 0.13692477345466614 0.47509765625 0.0007483132649213076 0.015355926007032394 0.0022805414628237486\n",
            "0.040043241511761044 0.13960847178536523 1.0\n",
            "repr, std, cov, closslb 0.11399516463279724 0.471435546875 0.0015412992797791958 0.015621739439666271 0.0011476603103801608\n",
            "0.03956583146520257 0.13822003491913742 1.0\n",
            "repr, std, cov, closslb 0.11462072283029556 0.47509765625 0.0008383064996451139 0.00022083740623202175 0.004450536333024502\n",
            "0.039329265148160916 0.13753100321702239 1.0\n",
            "repr, std, cov, closslb 0.1178537905216217 0.47509765625 0.000799965113401413 0.015594377182424068 0.0032205365132540464\n",
            "0.03917234059500879 0.13698225176837234 1.0\n",
            "repr, std, cov, closslb 0.11806106567382812 0.476318359375 0.0007081292569637299 0.00010798490257002413 0.016049699857831\n",
            "0.03909411327434683 0.1376685342202394 1.0\n",
            "repr, std, cov, closslb 0.1322733610868454 0.474853515625 0.0009612087160348892 0.030584078282117844 0.01680535078048706\n",
            "0.03909411327434683 0.13905143119565075 1.0\n",
            "repr, std, cov, closslb 0.1305074691772461 0.4755859375 0.0006698775105178356 0.015420717187225819 0.004282945767045021\n",
            "0.0390550582161307 0.13877374493204178 1.0\n",
            "repr, std, cov, closslb 0.11305851489305496 0.4755859375 0.0007768073119223118 0.00030025612795725465 0.035106781870126724\n",
            "0.03870531374500279 0.13753100321702239 1.0\n",
            "repr, std, cov, closslb 0.12428352236747742 0.477783203125 0.0005143119487911463 0.015292054042220116 0.00029404149972833693\n",
            "0.03878276307780652 0.13946900278258267 1.0\n",
            "repr, std, cov, closslb 0.1144786924123764 0.478759765625 0.00040271319448947906 0.0002600845764391124 0.015613089315593243\n",
            "0.03878276307780652 0.1404482195374185 1.0\n",
            "repr, std, cov, closslb 0.11772368848323822 0.474609375 0.0007959043141454458 0.00013634182687383145 0.027653653174638748\n",
            "0.03851236640400249 0.13863510982221958 1.0\n",
            "8\n",
            "repr, std, cov, closslb 0.1061137467622757 0.47412109375 0.000921015627682209 0.01548936776816845 0.019001107662916183\n",
            "0.038015192812821466 0.136708697664346 1.0\n",
            "repr, std, cov, closslb 0.11440542340278625 0.474853515625 0.0008062154520303011 0.015173222869634628 0.047768980264663696\n",
            "0.0375244374608412 0.13427116326038852 1.0\n",
            "repr, std, cov, closslb 0.1032702773809433 0.476806640625 0.000637679360806942 0.00046155822928994894 0.014253444969654083\n",
            "0.036929119309886474 0.13200896751200533 1.0\n",
            "repr, std, cov, closslb 0.09704697132110596 0.475830078125 0.0006861586589366198 0.00022036577865947038 0.027102939784526825\n",
            "0.036818553157934655 0.13187709042158377 1.0\n",
            "repr, std, cov, closslb 0.09322046488523483 0.478759765625 0.0004349884111434221 0.015140404924750328 0.014636543579399586\n",
            "0.036855371711092585 0.13440543442364888 1.0\n",
            "repr, std, cov, closslb 0.08804404735565186 0.47998046875 0.0003611738793551922 0.00016312615480273962 0.006968961097300053\n",
            "0.036929119309886474 0.13780620275445962 1.0\n",
            "repr, std, cov, closslb 0.0893525630235672 0.4755859375 0.0007343918550759554 0.015052362345159054 0.006607441231608391\n",
            "0.03645238457294908 0.1376685342202394 1.0\n",
            "repr, std, cov, closslb 0.08675338327884674 0.477294921875 0.0005809222348034382 0.0001317650603596121 0.015311360359191895\n",
            "0.03612594747245202 0.13877374493204178 1.0\n",
            "repr, std, cov, closslb 0.08651843667030334 0.4765625 0.000689168693497777 3.2326723157893866e-05 0.002550676930695772\n",
            "0.03601778602500117 0.1404482195374185 1.0\n",
            "repr, std, cov, closslb 0.10565653443336487 0.475830078125 0.0006265090778470039 0.015225007198750973 0.0015207217074930668\n",
            "0.036053803811026165 0.14299789110340566 1.0\n",
            "repr, std, cov, closslb 0.0885489210486412 0.4775390625 0.0006803828291594982 0.0005609640502370894 0.0005433250917121768\n",
            "0.035588368918544444 0.14342731391338703 1.0\n",
            "repr, std, cov, closslb 0.11136631667613983 0.476806640625 0.0005891053006052971 0.01563415117561817 0.0021309126168489456\n",
            "0.03516407147855444 0.14271232374359477 1.0\n",
            "repr, std, cov, closslb 0.13413949310779572 0.4736328125 0.0010803635232150555 0.015612282790243626 0.014935079030692577\n",
            "0.03484917143375328 0.1411518665224855 1.0\n",
            "repr, std, cov, closslb 0.14303357899188995 0.474365234375 0.0009133229032158852 0.015629781410098076 0.021173223853111267\n",
            "0.03450258878187449 0.1389125186769738 1.0\n",
            "repr, std, cov, closslb 0.09752289950847626 0.47265625 0.0013115738984197378 0.0003092274419032037 0.0005154249956831336\n",
            "0.0337521915845938 0.13548444878732208 1.0\n",
            "repr, std, cov, closslb 0.09268321096897125 0.475341796875 0.0008537811227142811 3.0445298762060702e-05 0.016859007999300957\n",
            "0.033316469247401906 0.13360181682142697 1.0\n",
            "repr, std, cov, closslb 0.10728774964809418 0.471435546875 0.0013536112383008003 0.030440853908658028 0.01584508828818798\n",
            "0.03278790972164218 0.13095763229712234 1.0\n",
            "repr, std, cov, closslb 0.0898904949426651 0.473876953125 0.0009365400765091181 0.0004548424622043967 0.00048615262494422495\n",
            "0.03226773569346314 0.12836578023329556 1.0\n",
            "repr, std, cov, closslb 0.08925147354602814 0.47607421875 0.0005927789025008678 0.01550374273210764 0.031260330229997635\n",
            "0.03236463573601833 0.13004458471174654 1.0\n",
            "repr, std, cov, closslb 0.08573490381240845 0.4765625 0.0006025207694619894 0.015523175708949566 0.0005313853034749627\n",
            "0.03242939737212609 0.13280300409375023 1.0\n",
            "repr, std, cov, closslb 0.09454409778118134 0.477294921875 0.0005608415231108665 6.594863953068852e-05 0.017375139519572258\n",
            "0.032461826769498214 0.13534909968763445 1.0\n",
            "repr, std, cov, closslb 0.07140719890594482 0.47998046875 0.0002920939587056637 4.789287049788982e-05 0.015270395204424858\n",
            "0.03252678288486398 0.13877374493204178 1.0\n",
            "repr, std, cov, closslb 0.08036474883556366 0.478271484375 0.00043998449109494686 0.015364033170044422 0.016256257891654968\n",
            "0.03252678288486398 0.14143431140739696 1.0\n",
            "repr, std, cov, closslb 0.06701086461544037 0.47900390625 0.0004098983481526375 0.00016884194337762892 0.01530078798532486\n",
            "0.03259186897741658 0.1450129265816035 1.0\n",
            "repr, std, cov, closslb 0.06720912456512451 0.479248046875 0.0003485309425741434 0.00027129327645525336 0.02199409529566765\n",
            "0.03252678288486398 0.14764546501337183 1.0\n",
            "repr, std, cov, closslb 0.07544036209583282 0.477783203125 0.0005841108504682779 0.00023070801398716867 0.0029921792447566986\n",
            "0.03207480509881636 0.14735061642989566 1.0\n",
            "repr, std, cov, closslb 0.07231725752353668 0.4765625 0.0007756194099783897 0.00021256486070342362 0.01573694869875908\n",
            "0.03134586393264019 0.14400188430677668 1.0\n",
            "repr, std, cov, closslb 0.0703517273068428 0.474365234375 0.0009231304284185171 0.0002565130707807839 0.0018804033752530813\n",
            "0.030633488891249786 0.1404482195374185 1.0\n",
            "repr, std, cov, closslb 0.06279505044221878 0.477294921875 0.0005890766624361277 4.6132729039527476e-05 0.03135933354496956\n",
            "0.030511260658974394 0.14101085566681867 1.0\n",
            "repr, std, cov, closslb 0.0742587149143219 0.47509765625 0.0009286527056246996 0.015381679870188236 0.005444080103188753\n",
            "0.030027205240482035 0.13849661320901058 1.0\n",
            "repr, std, cov, closslb 0.07391123473644257 0.474853515625 0.0008002703543752432 0.00013858781312592328 0.004424062091857195\n",
            "0.02987751857350419 0.1376685342202394 1.0\n",
            "repr, std, cov, closslb 0.08482781052589417 0.4755859375 0.00074907043017447 0.015475209802389145 0.0006506397621706128\n",
            "0.029937303488169767 0.1398878283374077 1.0\n",
            "repr, std, cov, closslb 0.07442840933799744 0.474609375 0.0007901294156908989 0.015351179987192154 0.01635613851249218\n",
            "0.029758306677889593 0.13960847178536523 1.0\n",
            "repr, std, cov, closslb 0.08356986939907074 0.4765625 0.0006456600967794657 0.00031714659417048097 0.001274922862648964\n",
            "0.029639570440118567 0.14030791162579273 1.0\n",
            "repr, std, cov, closslb 0.07657195627689362 0.477294921875 0.0005441096145659685 0.015568118542432785 0.009080513380467892\n",
            "0.029728578099789807 0.14342731391338703 1.0\n",
            "9\n",
            "repr, std, cov, closslb 0.06751998513936996 0.4775390625 0.0005111934151500463 1.4659794942417648e-05 0.03540690988302231\n",
            "0.029491816146160667 0.14371431196852769 1.0\n",
            "repr, std, cov, closslb 0.06128703057765961 0.47607421875 0.0006930397357791662 0.00021323931287042797 0.016402116045355797\n",
            "0.028994939084266418 0.14072925642471285 1.0\n",
            "repr, std, cov, closslb 0.07272085547447205 0.47265625 0.0010724717285484076 0.0005102697177790105 0.014715942554175854\n",
            "0.028649250905797802 0.13877374493204178 1.0\n",
            "repr, std, cov, closslb 0.07015836238861084 0.474609375 0.0007978149224072695 0.015533368103206158 0.0009126700460910797\n",
            "0.028364327823131478 0.1371192340201407 1.0\n",
            "repr, std, cov, closslb 0.061794981360435486 0.4736328125 0.0010049415286630392 0.00029686751076951623 0.003459859872236848\n",
            "0.0277197120214922 0.13373541863824837 1.0\n",
            "repr, std, cov, closslb 0.06375889480113983 0.47509765625 0.0007670784834772348 0.01546561811119318 0.027890445664525032\n",
            "0.027416616790076013 0.13227311745599685 1.0\n",
            "repr, std, cov, closslb 0.05565930902957916 0.475830078125 0.0006641922518610954 0.03044828213751316 0.0008733216673135757\n",
            "0.027416616790076013 0.13453983985807252 1.0\n",
            "repr, std, cov, closslb 0.05473274737596512 0.476806640625 0.0005749438423663378 3.271056630183011e-05 0.01663396507501602\n",
            "0.027307223941709338 0.13521388580183263 1.0\n",
            "repr, std, cov, closslb 0.05134767293930054 0.47412109375 0.0009037754498422146 0.00026176503160968423 0.0012961594620719552\n",
            "0.026981657315202218 0.13373541863824837 1.0\n",
            "repr, std, cov, closslb 0.05188703164458275 0.474609375 0.0010164293926209211 0.0006016826955601573 0.004496407695114613\n",
            "0.02674003213529243 0.13333501345949456 1.0\n",
            "repr, std, cov, closslb 0.048283033072948456 0.474365234375 0.0009892568923532963 0.015348607674241066 0.0027237010654062033\n",
            "0.02652707131936621 0.13267033375999027 1.0\n",
            "repr, std, cov, closslb 0.05512167513370514 0.474853515625 0.0008258023299276829 0.015478512272238731 0.043225888162851334\n",
            "0.026421227775187486 0.13373541863824837 1.0\n",
            "repr, std, cov, closslb 0.058278247714042664 0.47509765625 0.0007535119075328112 0.030533336102962494 0.017215466126799583\n",
            "0.026474096651965633 0.136708697664346 1.0\n",
            "repr, std, cov, closslb 0.039834246039390564 0.47802734375 0.000544469803571701 0.00010500617645448074 0.021190380677580833\n",
            "0.02652707131936621 0.1398878283374077 1.0\n",
            "repr, std, cov, closslb 0.04038722813129425 0.480224609375 0.00029384391382336617 2.156900882255286e-05 0.0016804358456283808\n",
            "0.02652707131936621 0.14214289872259836 1.0\n",
            "repr, std, cov, closslb 0.04203878343105316 0.476806640625 0.0005301882047206163 0.00011982169962720945 0.0012573272688314319\n",
            "0.02652707131936621 0.1451579395081851 1.0\n",
            "repr, std, cov, closslb 0.0379863902926445 0.477783203125 0.0005339188501238823 2.0463477994780988e-05 0.002994565526023507\n",
            "0.02634212235541207 0.14661606846001715 1.0\n",
            "repr, std, cov, closslb 0.04115822911262512 0.475341796875 0.0008125684689730406 6.300807581283152e-05 0.0012567692901939154\n",
            "0.02574346377182435 0.14328402988350356 1.0\n",
            "repr, std, cov, closslb 0.043103788048028946 0.474853515625 0.0007377211004495621 0.015216079540550709 0.0002578440180514008\n",
            "0.02525919516692022 0.14030791162579273 1.0\n",
            "repr, std, cov, closslb 0.04117954894900322 0.473388671875 0.0009247735142707825 0.00013070518616586924 0.017307749018073082\n",
            "0.024709832642888965 0.13698225176837234 1.0\n",
            "repr, std, cov, closslb 0.0537862628698349 0.470947265625 0.0014770529232919216 7.376961002591997e-05 0.02499232068657875\n",
            "0.024196590628874765 0.1338691540568866 1.0\n",
            "repr, std, cov, closslb 0.05019534006714821 0.472412109375 0.001043466618284583 0.015506013296544552 0.013093885034322739\n",
            "0.023694009041774673 0.1306961093822485 1.0\n",
            "repr, std, cov, closslb 0.06149442493915558 0.476806640625 0.0005180488806217909 0.00048104708548635244 0.025394195690751076\n",
            "0.023694009041774673 0.1306961093822485 1.0\n",
            "repr, std, cov, closslb 0.05047183483839035 0.47412109375 0.0007759705185890198 0.015499451197683811 0.0168454647064209\n",
            "0.02362306894211843 0.13082680549163073 1.0\n",
            "repr, std, cov, closslb 0.049768805503845215 0.4755859375 0.0006352800410240889 9.702758688945323e-06 0.03764718770980835\n",
            "0.023599469472645785 0.13280300409375023 1.0\n",
            "repr, std, cov, closslb 0.039628393948078156 0.477783203125 0.0004291634541004896 0.00011699770402628928 0.0030166078358888626\n",
            "0.023434931994072095 0.13267033375999027 1.0\n",
            "repr, std, cov, closslb 0.037407439202070236 0.47802734375 0.0004391069523990154 0.0003219006466679275 0.021915048360824585\n",
            "0.023364767573683564 0.13400302321094346 1.0\n",
            "repr, std, cov, closslb 0.03894422948360443 0.47802734375 0.0004085167311131954 0.015496551990509033 0.015028675086796284\n",
            "0.023294813226270267 0.13589130872251481 1.0\n",
            "repr, std, cov, closslb 0.035037875175476074 0.47705078125 0.0005175501573830843 3.454717079875991e-05 0.017934368923306465\n",
            "0.02324829339119449 0.13808195296617126 1.0\n",
            "repr, std, cov, closslb 0.0381631962954998 0.477294921875 0.0004162350669503212 0.015636900439858437 0.0012550215469673276\n",
            "0.02306314120048539 0.13877374493204178 1.0\n",
            "repr, std, cov, closslb 0.043020524084568024 0.473388671875 0.0008187561761587858 0.030563335865736008 0.014296150766313076\n",
            "0.02278817406749057 0.13698225176837234 1.0\n",
            "repr, std, cov, closslb 0.04296611621975899 0.474853515625 0.0006586972158402205 0.01523920614272356 0.014381223358213902\n",
            "0.02278817406749057 0.13849661320901058 1.0\n",
            "repr, std, cov, closslb 0.033611442893743515 0.4755859375 0.0006764053832739592 7.573910261271521e-05 0.007130198180675507\n",
            "0.022674574223770352 0.13835825495405654 1.0\n",
            "repr, std, cov, closslb 0.03419596701860428 0.474853515625 0.0007784625049680471 7.323286263272166e-05 0.014988805167376995\n",
            "0.02215926538475161 0.13521388580183263 1.0\n",
            "repr, std, cov, closslb 0.03578347712755203 0.473876953125 0.0007569862063974142 0.015324362553656101 0.0005336307222023606\n",
            "0.021677323268939928 0.13214097647951734 1.0\n",
            "10\n",
            "repr, std, cov, closslb 0.03615284711122513 0.472412109375 0.001058477908372879 0.015401594340801239 0.0242442786693573\n",
            "0.021184678239467174 0.12888001406250157 1.0\n",
            "repr, std, cov, closslb 0.03329506143927574 0.473388671875 0.0008891113102436066 0.00014939383254386485 0.0026230651419609785\n",
            "0.02105801386459537 0.1277258723349998 1.0\n",
            "repr, std, cov, closslb 0.05244951695203781 0.47021484375 0.0013915468007326126 0.031089235097169876 0.005578679498285055\n",
            "0.02062062290354756 0.12457386543705216 1.0\n",
            "repr, std, cov, closslb 0.030473632737994194 0.47265625 0.0010975007899105549 0.00015988285304047167 0.0003265339182689786\n",
            "0.02015199274787607 0.12162114310823134 1.0\n",
            "repr, std, cov, closslb 0.031695105135440826 0.474609375 0.0009336278308182955 0.00029800288029946387 0.005046772304922342\n",
            "0.020172144740623944 0.12162114310823134 1.0\n",
            "repr, std, cov, closslb 0.035676710307598114 0.475341796875 0.0007141244132071733 0.0003253737813793123 0.002102541271597147\n",
            "0.020232721711452177 0.12210835789412906 1.0\n",
            "repr, std, cov, closslb 0.03844348341226578 0.47607421875 0.0006343659479171038 0.030412614345550537 0.00038291592500172555\n",
            "0.020252954433163627 0.12407681322683418 1.0\n",
            "repr, std, cov, closslb 0.03431239724159241 0.47705078125 0.0004747530911117792 0.00011286514200037345 0.0009104071650654078\n",
            "0.02027320738759679 0.12620307816659407 1.0\n",
            "repr, std, cov, closslb 0.030226988717913628 0.4765625 0.0005686965305358171 8.137436816468835e-05 0.05927722156047821\n",
            "0.020374776359442095 0.12862264015954236 1.0\n",
            "repr, std, cov, closslb 0.031019773334264755 0.479736328125 0.0003460976295173168 9.741653047967702e-05 0.015375371091067791\n",
            "0.020395151135801533 0.13161373134516213 1.0\n",
            "repr, std, cov, closslb 0.03666336089372635 0.477783203125 0.00047191744670271873 0.00011783611262217164 0.01522025652229786\n",
            "0.02041554628693733 0.1348090540776285 1.0\n",
            "repr, std, cov, closslb 0.04206079617142677 0.47314453125 0.0010826780926436186 4.6229837607825175e-05 0.0011952362256124616\n",
            "0.02001149136589313 0.13200896751200533 1.0\n",
            "repr, std, cov, closslb 0.02964375540614128 0.478271484375 0.0004702433943748474 2.8078384275431745e-05 0.015403969213366508\n",
            "0.019931645115812684 0.13373541863824837 1.0\n",
            "repr, std, cov, closslb 0.03326099365949631 0.47509765625 0.0008360275533050299 6.909608782734722e-05 0.015573504380881786\n",
            "0.019576261211735484 0.13214097647951734 1.0\n",
            "repr, std, cov, closslb 0.0286579467356205 0.4765625 0.0006356078665703535 0.00012154650903539732 0.015513928607106209\n",
            "0.01926568750521854 0.12991467004170484 1.0\n",
            "repr, std, cov, closslb 0.03152083978056908 0.47412109375 0.0007806045468896627 0.015454563312232494 0.015832213684916496\n",
            "0.019055030987277132 0.12888001406250157 1.0\n",
            "repr, std, cov, closslb 0.02429664507508278 0.4765625 0.0005689146928489208 6.261720045586117e-06 0.019075831398367882\n",
            "0.018809040956785896 0.12759827406093888 1.0\n",
            "repr, std, cov, closslb 0.031000517308712006 0.47509765625 0.0006634474266320467 7.194483714556554e-06 0.04940181225538254\n",
            "0.0186965806499642 0.12683535685087058 1.0\n",
            "repr, std, cov, closslb 0.03367568179965019 0.472900390625 0.0009878785349428654 0.015483367256820202 0.039082203060388565\n",
            "0.01839996291030248 0.12407681322683418 1.0\n",
            "repr, std, cov, closslb 0.027584828436374664 0.4736328125 0.0008649765513837337 0.015357576310634613 0.00048749707639217377\n",
            "0.017981800106815035 0.12113587231905663 1.0\n",
            "repr, std, cov, closslb 0.0322478823363781 0.47265625 0.000958228250965476 0.0002514564257580787 0.0062439292669296265\n",
            "0.017661182200607885 0.11861978813601416 1.0\n",
            "repr, std, cov, closslb 0.03435923904180527 0.474853515625 0.0005751026328653097 0.015491972677409649 0.013988812454044819\n",
            "0.017678843382808493 0.11873840792415016 1.0\n",
            "repr, std, cov, closslb 0.03298034518957138 0.47509765625 0.0005829485598951578 0.015582610853016376 0.01883244328200817\n",
            "0.0176965222261913 0.11969164652063431 1.0\n",
            "repr, std, cov, closslb 0.02906085178256035 0.4765625 0.0005280452314764261 0.0002989576605614275 0.015451817773282528\n",
            "0.0176965222261913 0.12186450701559089 1.0\n",
            "repr, std, cov, closslb 0.02787996456027031 0.47607421875 0.000577135244384408 3.0015384254511446e-05 0.0020506950095295906\n",
            "0.01752052643203463 0.12125700819137568 1.0\n",
            "repr, std, cov, closslb 0.045358020812273026 0.47265625 0.0009983577765524387 0.015677550807595253 0.003015556838363409\n",
            "0.017156611774961698 0.1182646393059139 1.0\n",
            "repr, std, cov, closslb 0.02755371667444706 0.475830078125 0.0005764705128967762 0.015472309663891792 0.00023334880825132132\n",
            "0.01717376838673666 0.11850128684916501 1.0\n",
            "repr, std, cov, closslb 0.025572940707206726 0.474853515625 0.0006468878127634525 0.00022601691307500005 0.018970495089888573\n",
            "0.016901309777554684 0.11615596493475157 1.0\n",
            "repr, std, cov, closslb 0.02898501232266426 0.472900390625 0.0008643884211778641 0.015473369508981705 0.000336701690685004\n",
            "0.016800255902046667 0.11511576916037479 1.0\n",
            "repr, std, cov, closslb 0.02340088225901127 0.474853515625 0.0006482407916337252 7.2368075052509084e-06 0.015078508295118809\n",
            "0.01681705615794871 0.11592400100873311 1.0\n",
            "repr, std, cov, closslb 0.02516762539744377 0.47607421875 0.0005606196355074644 4.019794687337708e-06 0.01862727291882038\n",
            "0.01681705615794871 0.11767508594797527 1.0\n",
            "repr, std, cov, closslb 0.02341366745531559 0.47607421875 0.0005740646738559008 5.4198375437408686e-05 0.014957832172513008\n",
            "0.016683123110196423 0.11767508594797527 1.0\n",
            "repr, std, cov, closslb 0.03166116774082184 0.475341796875 0.0006101282779127359 0.030746178701519966 0.006238759029656649\n",
            "0.016550256721264305 0.11732276556565886 1.0\n",
            "repr, std, cov, closslb 0.02034856006503105 0.4765625 0.0005230091046541929 0.0001561425597174093 0.014298739843070507\n",
            "0.016550256721264305 0.11732276556565886 1.0\n",
            "repr, std, cov, closslb 0.0416020005941391 0.47216796875 0.0009656546171754599 0.030485931783914566 0.030518218874931335\n",
            "0.01627141858893288 0.11534611581446468 1.0\n",
            "11\n",
            "repr, std, cov, closslb 0.028943538665771484 0.473388671875 0.0008402753155678511 0.03080972470343113 0.0002002323599299416\n",
            "0.016045318154232272 0.11374331721493659 1.0\n",
            "repr, std, cov, closslb 0.024002470076084137 0.472412109375 0.0010182964615523815 0.031072091311216354 0.00022376840934157372\n",
            "0.015806552958193693 0.11126951251039405 1.0\n",
            "repr, std, cov, closslb 0.029818227514624596 0.475830078125 0.0005164789035916328 0.0006069078808650374 0.028927292674779892\n",
            "0.01577498720878891 0.11104730684938843 1.0\n",
            "repr, std, cov, closslb 0.052498601377010345 0.472900390625 0.0007538627833127975 0.030278280377388 0.001954359235242009\n",
            "0.01577498720878891 0.1107148301038717 1.0\n",
            "repr, std, cov, closslb 0.024419883266091347 0.475830078125 0.0006604446098208427 0.0002566890907473862 0.004274848848581314\n",
            "0.015665002676175797 0.11005285994857247 1.0\n",
            "repr, std, cov, closslb 0.04144836962223053 0.475830078125 0.0005134628154337406 0.015492909587919712 0.03477318957448006\n",
            "0.015712044694877348 0.11193880085520395 1.0\n",
            "repr, std, cov, closslb 0.025014402344822884 0.4775390625 0.0003992673009634018 3.0460638299700804e-05 0.013891387730836868\n",
            "0.015743484496311796 0.11465645494313219 1.0\n",
            "repr, std, cov, closslb 0.01827954687178135 0.477783203125 0.0004041283391416073 4.673623698181473e-05 0.002578751416876912\n",
            "0.015790762195997697 0.11708847153411912 1.0\n",
            "repr, std, cov, closslb 0.02435099333524704 0.476806640625 0.0005326597020030022 0.015362463891506195 0.0013203320559114218\n",
            "0.015759227980808106 0.11921407446136661 1.0\n",
            "repr, std, cov, closslb 0.025130534544587135 0.476318359375 0.00049024005420506 0.015449167229235172 0.016060983762145042\n",
            "0.015462776057466508 0.11802846434875208 1.0\n",
            "repr, std, cov, closslb 0.020560337230563164 0.47509765625 0.0006626206450164318 1.1920925317099318e-05 0.01765836589038372\n",
            "0.015171900786925416 0.11534611581446468 1.0\n",
            "repr, std, cov, closslb 0.018744543194770813 0.473876953125 0.0007124282419681549 3.437898340052925e-05 0.01696125976741314\n",
            "0.014856768870459515 0.11283745182488428 1.0\n",
            "repr, std, cov, closslb 0.019067542627453804 0.473876953125 0.0007974908221513033 0.0001108207434299402 0.0320722721517086\n",
            "0.014664976108228601 0.11126951251039405 1.0\n",
            "repr, std, cov, closslb 0.01763981208205223 0.4765625 0.0005344131495803595 0.00010309681238140911 0.002382355509325862\n",
            "0.014432319017242061 0.10950424259756866 1.0\n",
            "repr, std, cov, closslb 0.020810682326555252 0.472900390625 0.0008470602333545685 1.608391903573647e-05 0.0015959144802764058\n",
            "0.014118430346184175 0.107015617957313 1.0\n",
            "repr, std, cov, closslb 0.021299395710229874 0.474609375 0.0007916060276329517 0.015653138980269432 0.01508316956460476\n",
            "0.01383900499167159 0.10437469674203348 1.0\n",
            "repr, std, cov, closslb 0.020665792748332024 0.474853515625 0.0006350057665258646 0.030758420005440712 0.016243021935224533\n",
            "0.013866696840659923 0.1044790714387755 1.0\n",
            "repr, std, cov, closslb 0.02827884443104267 0.47412109375 0.000661937752738595 0.0002548068296164274 0.003324665129184723\n",
            "0.013770017068389928 0.10364698761074415 1.0\n",
            "repr, std, cov, closslb 0.013738307170569897 0.4765625 0.00048226071521639824 0.00012513257388491184 0.0009989229729399085\n",
            "0.01364670430028817 0.10282153059789535 1.0\n",
            "repr, std, cov, closslb 0.019438348710536957 0.474609375 0.0007080861832946539 0.015585916116833687 0.00016891758423298597\n",
            "0.013565109892777368 0.10200264762347996 1.0\n",
            "repr, std, cov, closslb 0.015809612348675728 0.475341796875 0.0006376588717103004 4.4323392103251535e-06 0.002199109410867095\n",
            "0.013551558334442926 0.10292435212849324 1.0\n",
            "repr, std, cov, closslb 0.016675489023327827 0.47216796875 0.0010434247087687254 2.6996196538675576e-05 0.0002520713023841381\n",
            "0.013389991741651333 0.10129147661740026 1.0\n",
            "repr, std, cov, closslb 0.0162198469042778 0.474853515625 0.0006139304023236036 0.015357278287410736 0.0008948053000494838\n",
            "0.013336565407270216 0.1008873216034069 1.0\n",
            "repr, std, cov, closslb 0.017922602593898773 0.474853515625 0.0005895763169974089 0.00025773252127692103 0.0018485381733626127\n",
            "0.013336565407270216 0.1011902863310692 1.0\n",
            "repr, std, cov, closslb 0.017403360456228256 0.478271484375 0.0003247519489377737 0.0001748437061905861 0.03185998648405075\n",
            "0.013349901972677485 0.10313030375710233 1.0\n",
            "repr, std, cov, closslb 0.01790909469127655 0.476806640625 0.00044431467540562153 2.8261387342354283e-05 0.016437266021966934\n",
            "0.013349901972677485 0.10500251263199692 1.0\n",
            "repr, std, cov, closslb 0.0214584618806839 0.47509765625 0.0005614929832518101 8.303321010316722e-06 0.0008120067068375647\n",
            "0.01328335224502994 0.10563410484710423 1.0\n",
            "repr, std, cov, closslb 0.02497163414955139 0.47509765625 0.0004982161335647106 0.030853688716888428 0.00031707363086752594\n",
            "0.013309932232872242 0.1051075151446289 1.0\n",
            "repr, std, cov, closslb 0.023859545588493347 0.474365234375 0.0006124831270426512 0.00028786330949515104 0.0014441190287470818\n",
            "0.013164397640480791 0.10458355051021426 1.0\n",
            "repr, std, cov, closslb 0.027363929897546768 0.472900390625 0.0007544814143329859 0.01541224867105484 0.002089328831061721\n",
            "0.012916757908070882 0.10200264762347996 1.0\n",
            "repr, std, cov, closslb 0.024552971124649048 0.474853515625 0.0005190325900912285 0.00018098561849910766 0.015215090475976467\n",
            "0.012775522395802276 0.1005852639551643 1.0\n",
            "repr, std, cov, closslb 0.022111518308520317 0.473876953125 0.000723562203347683 4.695307870861143e-05 0.0004281070432625711\n",
            "0.012522675245871381 0.09849603153507525 1.0\n",
            "repr, std, cov, closslb 0.014413190074265003 0.47509765625 0.0006701399106532335 2.8456015570554882e-05 0.005680200643837452\n",
            "0.012522675245871381 0.09829933456660749 1.0\n",
            "repr, std, cov, closslb 0.017180178314447403 0.475830078125 0.0005006163846701384 8.200507727451622e-05 0.006014492362737656\n",
            "0.012535197921117252 0.10018392674379283 1.0\n",
            "repr, std, cov, closslb 0.017619693651795387 0.475830078125 0.0004994058981537819 8.50193464430049e-05 0.003286713734269142\n",
            "0.012522675245871381 0.10129147661740026 1.0\n",
            "12\n",
            "repr, std, cov, closslb 0.02582889422774315 0.47509765625 0.0005070401821285486 0.030829615890979767 0.015896935015916824\n",
            "0.012410532628985844 0.09968450636953792 1.0\n",
            "repr, std, cov, closslb 0.015833780169487 0.47705078125 0.0003711371682584286 0.00017962181300390512 0.032747261226177216\n",
            "0.012435366104776443 0.10190074687660337 1.0\n",
            "repr, std, cov, closslb 0.012705355882644653 0.4765625 0.00041026738472282887 0.00020169062190689147 0.003006073646247387\n",
            "0.012447801470881217 0.10427042631571777 1.0\n",
            "repr, std, cov, closslb 0.022620031610131264 0.47607421875 0.0004305988550186157 0.00011634265683824196 0.00011826362606370822\n",
            "0.012410532628985844 0.10531783528243328 1.0\n",
            "repr, std, cov, closslb 0.019626151770353317 0.4775390625 0.00032860180363059044 6.231876614037901e-05 0.0009376893867738545\n",
            "0.012398134494491354 0.10616333276925748 1.0\n",
            "repr, std, cov, closslb 0.04151006415486336 0.472412109375 0.0008103703148663044 0.015573952347040176 0.02039044164121151\n",
            "0.012189251168687952 0.10375063459835487 1.0\n",
            "repr, std, cov, closslb 0.024206195026636124 0.47412109375 0.000620970968157053 0.015456573106348515 0.001412886194884777\n",
            "0.011983887101674496 0.10190074687660337 1.0\n",
            "repr, std, cov, closslb 0.022635340690612793 0.47509765625 0.0005562084261327982 0.0001498207711847499 8.298594912048429e-05\n",
            "0.011770212788316652 0.09938604996211566 1.0\n",
            "repr, std, cov, closslb 0.02239363268017769 0.475341796875 0.0005648955702781677 0.00013455687440000474 0.007560672704130411\n",
            "0.011629884043111972 0.09800502537739418 1.0\n",
            "repr, std, cov, closslb 0.020228737965226173 0.47509765625 0.0005792975425720215 0.00010688126349123195 0.0002935794764198363\n",
            "0.011433944173391046 0.09587351296168657 1.0\n",
            "repr, std, cov, closslb 0.019900048151612282 0.473388671875 0.0007258541882038116 8.152898226398975e-05 0.00022807210916653275\n",
            "0.011230075416159528 0.0936010631065642 1.0\n",
            "repr, std, cov, closslb 0.019574785605072975 0.472900390625 0.0007667201571166515 4.662731953430921e-05 0.014483263716101646\n",
            "0.011085101283297477 0.09230042414007437 1.0\n",
            "repr, std, cov, closslb 0.01593182608485222 0.474609375 0.0005774789024144411 6.747864063072484e-06 0.0006685011321678758\n",
            "0.010996818210114475 0.09110887613557464 1.0\n",
            "repr, std, cov, closslb 0.01924065127968788 0.473388671875 0.0006669368594884872 5.283277641865425e-05 0.01572231948375702\n",
            "0.010789953513887191 0.08912734814845023 1.0\n",
            "repr, std, cov, closslb 0.02429346926510334 0.473388671875 0.0006685380358248949 0.00017571184434928 0.014117403887212276\n",
            "0.010629391701324386 0.08710181462151641 1.0\n",
            "repr, std, cov, closslb 0.016322072595357895 0.475341796875 0.0004864081274718046 0.00020297864102758467 0.01541341096162796\n",
            "0.010597567196435924 0.08658102800514571 1.0\n",
            "repr, std, cov, closslb 0.017594479024410248 0.47509765625 0.0005474472418427467 2.5163373720715754e-05 0.014784100465476513\n",
            "0.010608164763632359 0.08684103091882617 1.0\n",
            "repr, std, cov, closslb 0.02724197506904602 0.47607421875 0.00045961770229041576 3.86480633096653e-06 0.030445531010627747\n",
            "0.010661311775232852 0.08692787194974498 1.0\n",
            "repr, std, cov, closslb 0.023393813520669937 0.474609375 0.0005771243013441563 4.128182990825735e-05 0.00025984673993662\n",
            "0.010565837974432868 0.08554877759158029 1.0\n",
            "repr, std, cov, closslb 0.03327058628201485 0.473876953125 0.0006253209430724382 0.015338544733822346 0.00029416964389383793\n",
            "0.01052368006996734 0.08452883404178003 1.0\n",
            "repr, std, cov, closslb 0.017302576452493668 0.477294921875 0.0003567303065210581 0.00012312446779105812 0.015487069264054298\n",
            "0.010555282691741127 0.08623556797481016 1.0\n",
            "repr, std, cov, closslb 0.02132733352482319 0.478271484375 0.00024940306320786476 3.455709929767181e-06 0.0011300483020022511\n",
            "0.010597567196435924 0.08806473958274044 1.0\n",
            "repr, std, cov, closslb 0.030545542016625404 0.475341796875 0.00041493726894259453 0.015603598207235336 0.043483514338731766\n",
            "0.01064002109302571 0.08975311438237894 1.0\n",
            "repr, std, cov, closslb 0.02094784751534462 0.47705078125 0.0003913219552487135 0.0001386658987030387 0.000145032157888636\n",
            "0.010671973087008083 0.0915653325165582 1.0\n",
            "repr, std, cov, closslb 0.02295491099357605 0.4755859375 0.0005625230260193348 0.01532191876322031 0.001739821513183415\n",
            "0.01052368006996734 0.09101785827729736 1.0\n",
            "repr, std, cov, closslb 0.02071308344602585 0.476806640625 0.0004724229220300913 0.03036709688603878 0.00024464773014187813\n",
            "0.010450308090735532 0.0912911849967219 1.0\n",
            "repr, std, cov, closslb 0.020065443590283394 0.476318359375 0.0005370629951357841 0.015377121046185493 0.0005025765858590603\n",
            "0.010294800431992733 0.08939499766406735 1.0\n",
            "repr, std, cov, closslb 0.021632011979818344 0.477294921875 0.000337046105414629 0.015376847237348557 0.00026897844509221613\n",
            "0.010336041443712788 0.09047365741710275 1.0\n",
            "repr, std, cov, closslb 0.021788232028484344 0.4755859375 0.00044110091403126717 0.01553342491388321 0.0005292288260534406\n",
            "0.010305095232424724 0.09074534990079994 1.0\n",
            "repr, std, cov, closslb 0.02055893838405609 0.474609375 0.0006607514806091785 0.01540043018758297 0.028521593660116196\n",
            "0.010172062087942734 0.0898428674967613 1.0\n",
            "repr, std, cov, closslb 0.027234915643930435 0.47412109375 0.0006156924646347761 0.00021277126506902277 0.02302202582359314\n",
            "0.009970741403889804 0.0876257337798081 1.0\n",
            "repr, std, cov, closslb 0.024404160678386688 0.475341796875 0.0005729582626372576 0.00029044333496131003 0.047768834978342056\n",
            "0.009832192392112787 0.08563432636917186 1.0\n",
            "repr, std, cov, closslb 0.059872135519981384 0.471923828125 0.0008125000167638063 0.04595746099948883 0.031759560108184814\n",
            "0.009753887635275774 0.0850372766703864 1.0\n",
            "repr, std, cov, closslb 0.02412247471511364 0.474853515625 0.0005174954421818256 0.015615982934832573 0.030441131442785263\n",
            "0.00977340516443396 0.0846133628758218 1.0\n",
            "repr, std, cov, closslb 0.021332114934921265 0.4755859375 0.0004182301927357912 1.9826758943963796e-05 0.01708468794822693\n",
            "0.009783178569598393 0.0852926436972648 1.0\n",
            "13\n",
            "repr, std, cov, closslb 0.020977184176445007 0.477783203125 0.00029628188349306583 6.028804273228161e-05 0.0012206113897264004\n",
            "0.009802754709916157 0.08684103091882617 1.0\n",
            "repr, std, cov, closslb 0.024785341694951057 0.475830078125 0.0004153382033109665 0.045784130692481995 0.005612725857645273\n",
            "0.009822370022090698 0.08850594480913772 1.0\n",
            "repr, std, cov, closslb 0.015397178009152412 0.477294921875 0.0003887261264026165 0.00018153619021177292 0.0017651363741606474\n",
            "0.00979296174816799 0.08886049997815536 1.0\n",
            "repr, std, cov, closslb 0.02384299226105213 0.47509765625 0.00045489449985325336 0.015445873141288757 0.025862662121653557\n",
            "0.00974414349178399 0.08841752728185587 1.0\n",
            "repr, std, cov, closslb 0.018599791452288628 0.47607421875 0.0004071590956300497 0.015402321703732014 0.006238529924303293\n",
            "0.0096183519275375 0.08727610535257403 1.0\n",
            "repr, std, cov, closslb 0.021989818662405014 0.47509765625 0.00047347345389425755 0.015438373200595379 0.0002812407328747213\n",
            "0.009532218003747078 0.08606335520105288 1.0\n",
            "repr, std, cov, closslb 0.014116168022155762 0.477783203125 0.0002919035032391548 0.00011828314745798707 0.03070446103811264\n",
            "0.009541750221750824 0.08692787194974498 1.0\n",
            "repr, std, cov, closslb 0.014638945460319519 0.477294921875 0.0003454280085861683 0.00015590302064083517 0.016394784674048424\n",
            "0.009551291971972574 0.08780107287310147 1.0\n",
            "repr, std, cov, closslb 0.013179865665733814 0.47607421875 0.000410665525123477 0.00016054618754424155 0.0003170664713252336\n",
            "0.009427990015392972 0.08640812534632773 1.0\n",
            "repr, std, cov, closslb 0.016130540519952774 0.474853515625 0.0005035374779254198 4.3945190554950386e-05 0.05940946936607361\n",
            "0.009296982836812783 0.08444438965212792 1.0\n",
            "repr, std, cov, closslb 0.019190244376659393 0.474853515625 0.000509775010868907 0.01551547460258007 0.044275540858507156\n",
            "0.009195326976816425 0.0831877999693406 1.0\n",
            "repr, std, cov, closslb 0.02299647033214569 0.474853515625 0.0004464834928512573 0.00018859875854104757 0.020344328135252\n",
            "0.009140347603040035 0.08219600485093605 1.0\n",
            "repr, std, cov, closslb 0.03170277178287506 0.47265625 0.0006510831881314516 0.01539582945406437 0.0011021080426871777\n",
            "0.009049444841768433 0.08113489937422282 1.0\n",
            "repr, std, cov, closslb 0.017173264175653458 0.474853515625 0.0004700359422713518 5.376349508878775e-05 0.00012853760563302785\n",
            "0.008986351354074473 0.0801675796811758 1.0\n",
            "repr, std, cov, closslb 0.01813734509050846 0.474853515625 0.00047561852261424065 0.015587462112307549 0.01683715730905533\n",
            "0.009022350713553283 0.07992755714713543 1.0\n",
            "repr, std, cov, closslb 0.020352531224489212 0.473876953125 0.0005217306315898895 0.015404161997139454 0.0010148193687200546\n",
            "0.008914782976486863 0.07889573608985972 1.0\n",
            "repr, std, cov, closslb 0.015034463256597519 0.4755859375 0.000411847373470664 0.00028713338542729616 0.00020442312234081328\n",
            "0.008914782976486863 0.07881691917068905 1.0\n",
            "repr, std, cov, closslb 0.02194751612842083 0.47509765625 0.00044468254782259464 0.015403148718178272 0.015498497523367405\n",
            "0.008852628362601029 0.07764407005819061 1.0\n",
            "repr, std, cov, closslb 0.01682780310511589 0.476318359375 0.0003818594850599766 0.00020717759616672993 0.02847834676504135\n",
            "0.008817306198691033 0.07741160293715875 1.0\n",
            "repr, std, cov, closslb 0.028476886451244354 0.47509765625 0.0004392019473016262 0.015423181466758251 0.021529842168092728\n",
            "0.008799698002987058 0.07671836929816143 1.0\n",
            "repr, std, cov, closslb 0.014738824218511581 0.476318359375 0.0003983411006629467 1.4058092347113416e-05 0.00906998198479414\n",
            "0.008790907095891169 0.07779943584237704 1.0\n",
            "repr, std, cov, closslb 0.015043465420603752 0.476806640625 0.00038528465665876865 1.1019503290299326e-05 0.015979580581188202\n",
            "0.008843784578023007 0.07952911548287737 1.0\n",
            "repr, std, cov, closslb 0.030714470893144608 0.475830078125 0.0003591724671423435 0.015448898077011108 0.016914144158363342\n",
            "0.00889698011926821 0.08105384552869413 1.0\n",
            "repr, std, cov, closslb 0.013354804366827011 0.47705078125 0.0003743490669876337 0.00020888488506898284 0.0009578332537785172\n",
            "0.008808497700990044 0.0801675796811758 1.0\n",
            "repr, std, cov, closslb 0.013370228931307793 0.476318359375 0.00042377039790153503 0.0003130373952444643 0.015914062038064003\n",
            "0.008790907095891169 0.07960864459836024 1.0\n",
            "repr, std, cov, closslb 0.013640068471431732 0.476318359375 0.0003855291288346052 0.000271117954980582 0.014970654621720314\n",
            "0.00877335161930095 0.0801675796811758 1.0\n",
            "repr, std, cov, closslb 0.013874903321266174 0.4755859375 0.00042957975529134274 0.00017013787874020636 0.0001798819866962731\n",
            "0.008703479597090922 0.08081116950585689 1.0\n",
            "repr, std, cov, closslb 0.017576897516846657 0.475341796875 0.0005182211752980947 4.08487903769128e-05 0.013819724321365356\n",
            "0.008599713557110181 0.07944966581706032 1.0\n",
            "repr, std, cov, closslb 0.01841779798269272 0.473388671875 0.0007018726319074631 4.654350959754083e-06 0.0011261776089668274\n",
            "0.0084210909670755 0.0774890145400959 1.0\n",
            "repr, std, cov, closslb 0.029270686209201813 0.473876953125 0.0005573316011577845 0.01550831738859415 0.01451838482171297\n",
            "0.008370740793661208 0.07648867373444825 1.0\n",
            "repr, std, cov, closslb 0.045106884092092514 0.47265625 0.000666480977088213 0.015347720123827457 0.017300888895988464\n",
            "0.008279212728145502 0.07475037032591669 1.0\n",
            "repr, std, cov, closslb 0.017277833074331284 0.47509765625 0.00045059574767947197 0.00047699728747829795 0.001716705271974206\n",
            "0.008279212728145502 0.07437773711880219 1.0\n",
            "repr, std, cov, closslb 0.015371352434158325 0.474853515625 0.0005510544870048761 0.00045048753963783383 0.014429048635065556\n",
            "0.008156012438124492 0.07290568781497272 1.0\n",
            "repr, std, cov, closslb 0.013587363064289093 0.476318359375 0.0003905172925442457 0.000159039453137666 0.0006083741318434477\n",
            "0.008180504951632191 0.07305157209629047 1.0\n",
            "repr, std, cov, closslb 0.024229824542999268 0.476806640625 0.0003303908742964268 0.015673547983169556 0.00950128585100174\n",
            "0.008221489363285825 0.07334421698638752 1.0\n",
            "14\n",
            "repr, std, cov, closslb 0.017137065529823303 0.4765625 0.0003654363099485636 1.052893276209943e-05 0.0006348918541334569\n",
            "0.008262679107251892 0.07422920448063645 1.0\n",
            "repr, std, cov, closslb 0.01787845976650715 0.476318359375 0.00040403311140835285 9.501921886112541e-05 0.017884107306599617\n",
            "0.008295779432814518 0.07572797710118767 1.0\n",
            "repr, std, cov, closslb 0.023277319967746735 0.4765625 0.00036373292095959187 0.00038439236232079566 0.04026346653699875\n",
            "0.008345678712142967 0.07641226147297528 1.0\n",
            "repr, std, cov, closslb 0.02361992374062538 0.4755859375 0.0004060922656208277 0.0003122331982012838 0.0002626858768053353\n",
            "0.008270941786359143 0.07497484576275576 1.0\n",
            "repr, std, cov, closslb 0.0233757346868515 0.477294921875 0.00031306943856179714 0.015496216714382172 0.015480566769838333\n",
            "0.008354024390855109 0.0755012467816013 1.0\n",
            "repr, std, cov, closslb 0.021616239100694656 0.475341796875 0.00043794303201138973 8.010661986190826e-06 0.00044620491098612547\n",
            "0.008262679107251892 0.07542582096064067 1.0\n",
            "repr, std, cov, closslb 0.025168387219309807 0.475830078125 0.00039199995808303356 0.015480633825063705 0.0010258173570036888\n",
            "0.008139724848702241 0.07460109353774767 1.0\n",
            "repr, std, cov, closslb 0.03172168880701065 0.474853515625 0.00047290418297052383 0.015427250415086746 0.015332076698541641\n",
            "0.008164168450562616 0.07378538391954553 1.0\n",
            "repr, std, cov, closslb 0.022556979209184647 0.474609375 0.0005291530396789312 0.00024569802917540073 0.015498843975365162\n",
            "0.008213276087198627 0.07349097876457726 1.0\n",
            "repr, std, cov, closslb 0.024363793432712555 0.474365234375 0.00047351629473268986 0.00013424738426692784 0.00012561680341605097\n",
            "0.00822971085264911 0.07349097876457726 1.0\n",
            "repr, std, cov, closslb 0.022243451327085495 0.4765625 0.0003139527980238199 2.2218917365535162e-05 0.03139586001634598\n",
            "0.008221489363285825 0.07378538391954553 1.0\n",
            "repr, std, cov, closslb 0.017144840210676193 0.476318359375 0.000401974655687809 6.193618173711002e-05 0.0044750128872692585\n",
            "0.008196874142040405 0.0740069615012413 1.0\n",
            "repr, std, cov, closslb 0.01916463114321232 0.475830078125 0.0003884576726704836 0.0001027325852192007 0.021833209320902824\n",
            "0.008188685456583822 0.07378538391954553 1.0\n",
            "repr, std, cov, closslb 0.025852080434560776 0.4755859375 0.0003779865801334381 0.015415697358548641 0.00024949456565082073\n",
            "0.008156012438124492 0.07349097876457726 1.0\n",
            "repr, std, cov, closslb 0.032822247594594955 0.474609375 0.00045213173143565655 0.00015589251415804029 0.00025875127175822854\n",
            "0.008131593255446795 0.07283285496001272 1.0\n",
            "repr, std, cov, closslb 0.019671324640512466 0.4755859375 0.0004355765413492918 0.00011363732483005151 0.0002210352977272123\n",
            "0.008115354431229906 0.07334421698638752 1.0\n",
            "repr, std, cov, closslb 0.035552866756916046 0.47607421875 0.0003542066551744938 0.03097832389175892 0.0001617765228729695\n",
            "0.008156012438124492 0.07460109353774767 1.0\n",
            "repr, std, cov, closslb 0.016670243814587593 0.47607421875 0.0003818739205598831 7.3081300797639415e-06 0.0009677447960712016\n",
            "0.008131593255446795 0.07535047049015052 1.0\n",
            "repr, std, cov, closslb 0.013073693960905075 0.47705078125 0.0003195148892700672 1.446489022782771e-05 0.002454801695421338\n",
            "0.008180504951632191 0.07710272909591254 1.0\n",
            "repr, std, cov, closslb 0.016193844377994537 0.477294921875 0.00028924806974828243 0.01540535781532526 0.0003600212512537837\n",
            "0.008164168450562616 0.07803306762601112 1.0\n",
            "repr, std, cov, closslb 0.01826643943786621 0.4765625 0.00032989028841257095 0.0002477475209161639 0.014917707070708275\n",
            "0.008172332619013178 0.07905360645777551 1.0\n",
            "repr, std, cov, closslb 0.014559431001543999 0.475341796875 0.0004223119467496872 9.002100705401972e-05 0.03024934232234955\n",
            "0.00803464545181963 0.07741160293715875 1.0\n",
            "repr, std, cov, closslb 0.026175882667303085 0.4755859375 0.0003871731460094452 0.03099695034325123 0.0001254084927495569\n",
            "0.007970656573928975 0.07741160293715875 1.0\n",
            "repr, std, cov, closslb 0.02140331082046032 0.474609375 0.00046533485874533653 0.03074008971452713 0.00046682689571753144\n",
            "0.007797302673113706 0.0755012467816013 1.0\n",
            "repr, std, cov, closslb 0.02544712834060192 0.47314453125 0.0005757331382483244 0.04570004716515541 0.0005404888652265072\n",
            "0.0076353467673144475 0.07363803421308517 1.0\n",
            "repr, std, cov, closslb 0.01594427600502968 0.473388671875 0.0005995547398924828 0.00029596456442959607 0.0006641208310611546\n",
            "0.00749920751100195 0.07210851624613746 1.0\n",
            "repr, std, cov, closslb 0.020619388669729233 0.47412109375 0.00048483628779649734 0.0001185227301903069 0.01250013429671526\n",
            "0.007409799239548014 0.07103549271042038 1.0\n",
            "repr, std, cov, closslb 0.02203165367245674 0.47509765625 0.00045874551869928837 3.6178225855110213e-06 0.0005910369800403714\n",
            "0.007409799239548014 0.0705402273031467 1.0\n",
            "repr, std, cov, closslb 0.01846330240368843 0.474853515625 0.00047281733714044094 2.3372638224827824e-06 0.009360688738524914\n",
            "0.007328778382855239 0.06935176808062767 1.0\n",
            "repr, std, cov, closslb 0.015309827402234077 0.47412109375 0.0005020503886044025 3.5706369089894e-05 0.018247755244374275\n",
            "0.00721972119929019 0.06764031176109932 1.0\n",
            "repr, std, cov, closslb 0.017063144594430923 0.473876953125 0.0005491250194609165 0.00022938290203455836 0.005227827001363039\n",
            "0.007105181678764258 0.06603706168802265 1.0\n",
            "repr, std, cov, closslb 0.017389507964253426 0.476318359375 0.0003474547993391752 0.00030767370481044054 0.02211465686559677\n",
            "0.007098083595169089 0.06531499363483156 1.0\n",
            "repr, std, cov, closslb 0.020698174834251404 0.4755859375 0.00033658393658697605 0.015402459539473057 0.00020309747196733952\n",
            "0.007105181678764258 0.06505438550621111 1.0\n",
            "repr, std, cov, closslb 0.020787794142961502 0.47705078125 0.0003049103543162346 0.01558377593755722 0.020292313769459724\n",
            "0.007112286860443021 0.06650071021174406 1.0\n",
            "repr, std, cov, closslb 0.01587950438261032 0.4775390625 0.0002840929664671421 1.2798443094652612e-05 0.03181576356291771\n",
            "0.007147919488772275 0.06818333197651343 1.0\n",
            "15\n",
            "repr, std, cov, closslb 0.015237348154187202 0.47802734375 0.000265918904915452 0.0001245531311724335 0.015865806490182877\n",
            "0.007169384698144976 0.06969922113257608 1.0\n",
            "repr, std, cov, closslb 0.0185506884008646 0.47607421875 0.0003299126401543617 0.00038484373362734914 0.011167635209858418\n",
            "0.007190914367562888 0.06990852796333641 1.0\n",
            "repr, std, cov, closslb 0.017817901447415352 0.4755859375 0.0004044866655021906 0.0003678154607769102 0.0002516597742214799\n",
            "0.007126518546450767 0.06962959154103505 1.0\n",
            "repr, std, cov, closslb 0.019527703523635864 0.47607421875 0.00037386524491012096 1.5341243852162734e-05 0.001139019150286913\n",
            "0.007140778710062213 0.07082281173595453 1.0\n",
            "repr, std, cov, closslb 0.028749369084835052 0.475830078125 0.00031007174402475357 9.637259063310921e-05 0.010035115294158459\n",
            "0.007190914367562888 0.07167737539849248 1.0\n",
            "repr, std, cov, closslb 0.01767479069530964 0.475830078125 0.0003595547750592232 0.015536901541054249 0.016512682661414146\n",
            "0.007147919488772275 0.0723250581925331 1.0\n",
            "repr, std, cov, closslb 0.0286093819886446 0.47705078125 0.00029143551364541054 9.045585466083139e-05 0.0018922556191682816\n",
            "0.007248643431300649 0.07312462366838675 1.0\n",
            "repr, std, cov, closslb 0.04706567898392677 0.476318359375 0.00032417057082057 0.015372226014733315 0.0015983637422323227\n",
            "0.007343443268399331 0.07363803421308517 1.0\n",
            "repr, std, cov, closslb 0.02300981618463993 0.475830078125 0.00038735708221793175 0.0001978976360987872 0.024327415972948074\n",
            "0.007241402029271378 0.0718926226284915 1.0\n",
            "repr, std, cov, closslb 0.03143463656306267 0.474365234375 0.0004645702429115772 0.01534247025847435 0.0010097611229866743\n",
            "0.007140778710062213 0.07089363454769047 1.0\n",
            "repr, std, cov, closslb 0.03866249695420265 0.473876953125 0.0005229022353887558 0.015425828285515308 0.017422210425138474\n",
            "0.007034519088895001 0.06921327232270993 1.0\n",
            "repr, std, cov, closslb 0.031014330685138702 0.474853515625 0.0004086247645318508 6.94157206453383e-05 0.015814276412129402\n",
            "0.006985473832381631 0.0685934560830419 1.0\n",
            "repr, std, cov, closslb 0.020135775208473206 0.476806640625 0.0003170936834067106 9.321539255324751e-05 0.016592521220445633\n",
            "0.006950651001323734 0.06791127912064314 1.0\n",
            "repr, std, cov, closslb 0.018201962113380432 0.47412109375 0.0004663874860852957 0.00011441525566624478 0.0004155533970333636\n",
            "0.006860921897608753 0.06650071021174406 1.0\n",
            "repr, std, cov, closslb 0.02040514163672924 0.473876953125 0.0005216321442276239 0.015342514030635357 0.00035514688352122903\n",
            "0.006731858881646323 0.06505438550621111 1.0\n",
            "repr, std, cov, closslb 0.01535236369818449 0.476318359375 0.00036680162884294987 0.000118588337500114 0.016999520361423492\n",
            "0.006731858881646323 0.06564222240641861 1.0\n",
            "repr, std, cov, closslb 0.017692577093839645 0.476318359375 0.00035819574259221554 5.226770372246392e-05 0.009783783927559853\n",
            "0.006725133747898426 0.06623537105030883 1.0\n",
            "repr, std, cov, closslb 0.01647583395242691 0.47607421875 0.0003557233139872551 1.642300958337728e-05 0.0007088950951583683\n",
            "0.0067520746605997635 0.06636790802778048 1.0\n",
            "repr, std, cov, closslb 0.027960311621427536 0.474365234375 0.0003969718236476183 0.03078223578631878 0.03010568767786026\n",
            "0.006745329331268496 0.06590518541201322 1.0\n",
            "repr, std, cov, closslb 0.01847919449210167 0.475830078125 0.0003850946668535471 0.015386726707220078 0.0005914096254855394\n",
            "0.0067520746605997635 0.06610309874971067 1.0\n",
            "repr, std, cov, closslb 0.014593640342354774 0.4765625 0.0003259985242038965 0.00016834457346703857 0.03181394562125206\n",
            "0.006691608721255389 0.06557664576065796 1.0\n",
            "repr, std, cov, closslb 0.02982805110514164 0.475341796875 0.00042624305933713913 0.0153550636023283 0.0338437557220459\n",
            "0.006691608721255389 0.06531499363483156 1.0\n",
            "repr, std, cov, closslb 0.014406012371182442 0.47607421875 0.000337215606123209 0.00015064523904584348 0.015625176951289177\n",
            "0.006745329331268496 0.06656721092195579 1.0\n",
            "repr, std, cov, closslb 0.025431785732507706 0.474609375 0.00039941398426890373 0.015526598319411278 0.015062989667057991\n",
            "0.00671841533256586 0.06670041191101062 1.0\n",
            "repr, std, cov, closslb 0.03679477050900459 0.475341796875 0.00040980824269354343 0.015509501099586487 0.007616374641656876\n",
            "0.006704998630306619 0.06676711232292162 1.0\n",
            "repr, std, cov, closslb 0.03989580273628235 0.47509765625 0.00037229619920253754 0.015485746785998344 0.037845201790332794\n",
            "0.006691608721255389 0.06730312244428568 1.0\n",
            "repr, std, cov, closslb 0.0174277164041996 0.47509765625 0.0003705646377056837 5.675142165273428e-05 0.01795095205307007\n",
            "0.006684923797457932 0.06730312244428568 1.0\n",
            "repr, std, cov, closslb 0.022805413231253624 0.475830078125 0.0003306467551738024 0.015426725149154663 0.02163141407072544\n",
            "0.0066582508180412 0.06710161622366444 1.0\n",
            "repr, std, cov, closslb 0.019171863794326782 0.474609375 0.0004703272134065628 0.00013929166016168892 0.007108332589268684\n",
            "0.006605223711728188 0.06557664576065796 1.0\n",
            "repr, std, cov, closslb 0.020836759358644485 0.476318359375 0.0002939982805401087 0.015433089807629585 0.02361394464969635\n",
            "0.006487451687575754 0.06415041854342032 1.0\n",
            "repr, std, cov, closslb 0.017391446977853775 0.474365234375 0.00046182936057448387 7.837271550670266e-05 0.01604384183883667\n",
            "0.00638452948528862 0.06262988795241868 1.0\n",
            "repr, std, cov, closslb 0.018218576908111572 0.473876953125 0.0005030890461057425 5.4667776566930115e-05 0.0008522741263732314\n",
            "0.006264428040734996 0.061084313549717016 1.0\n",
            "repr, std, cov, closslb 0.032632745802402496 0.47412109375 0.00040119211189448833 0.015460317954421043 0.0306570902466774\n",
            "0.006276963161244506 0.06065842859736564 1.0\n",
            "repr, std, cov, closslb 0.01616814360022545 0.47607421875 0.0003236623015254736 0.015484597533941269 0.015981236472725868\n",
            "0.006302108700782578 0.06126774980439111 1.0\n",
            "repr, std, cov, closslb 0.027799483388662338 0.474609375 0.0004134313203394413 0.00011612082016654313 0.016197439283132553\n",
            "0.006308410809483359 0.06139034657174968 1.0\n",
            "16\n",
            "repr, std, cov, closslb 0.01964738219976425 0.475830078125 0.00031886366195976734 0.00011381650256225839 0.0156724750995636\n",
            "0.006352702382792043 0.061451736918321426 1.0\n",
            "repr, std, cov, closslb 0.029397442936897278 0.476806640625 0.00029312283731997013 6.104963540565223e-05 0.0016562938690185547\n",
            "0.006403702233717469 0.06300660798199002 1.0\n",
            "repr, std, cov, closslb 0.023484770208597183 0.477294921875 0.0002306988462805748 2.023113120230846e-05 0.01838929019868374\n",
            "0.0064551115143056895 0.06447181278214782 1.0\n",
            "repr, std, cov, closslb 0.01680847257375717 0.477294921875 0.0002751699648797512 1.737073944241274e-05 0.008141386322677135\n",
            "0.006500433078402592 0.06610309874971067 1.0\n",
            "repr, std, cov, closslb 0.013791080564260483 0.477294921875 0.00030322535894811153 4.832358536077663e-05 0.0007507071131840348\n",
            "0.006513440444992474 0.06750523378828897 1.0\n",
            "repr, std, cov, closslb 0.01553424634039402 0.477294921875 0.00027940189465880394 0.00010832978296093643 0.0003909912775270641\n",
            "0.0065722964408039325 0.06921327232270993 1.0\n",
            "repr, std, cov, closslb 0.015472945757210255 0.47705078125 0.0002510917838662863 0.00013261301501188427 0.00022083258954808116\n",
            "0.006565730710093839 0.07025877038786758 1.0\n",
            "repr, std, cov, closslb 0.013959080912172794 0.47607421875 0.0003693625330924988 9.472759120399132e-05 0.030259110033512115\n",
            "0.006442220630823414 0.06942111984870829 1.0\n",
            "repr, std, cov, closslb 0.016067024320364 0.476318359375 0.0003942255862057209 9.688606951385736e-06 0.030648481100797653\n",
            "0.006378151333954666 0.06818333197651343 1.0\n",
            "repr, std, cov, closslb 0.03464599326252937 0.47412109375 0.00044459872879087925 0.015422324649989605 0.00010435259173391387\n",
            "0.006308410809483359 0.06650071021174406 1.0\n",
            "repr, std, cov, closslb 0.016465531662106514 0.474365234375 0.000499221496284008 0.00021334759367164224 0.001752357929944992\n",
            "0.006177380319885124 0.06485961202643613 1.0\n",
            "repr, std, cov, closslb 0.020555900409817696 0.4755859375 0.00036360579542815685 0.015350314788520336 0.030680669471621513\n",
            "0.006097633717832411 0.0636395169332093 1.0\n",
            "repr, std, cov, closslb 0.020100831985473633 0.4755859375 0.0003816180396825075 9.657302871346474e-05 0.0008404678083024919\n",
            "0.0060249355174064574 0.06281796556856967 1.0\n",
            "repr, std, cov, closslb 0.01642845943570137 0.474609375 0.0004389414098113775 3.43348910973873e-05 0.0012457264820113778\n",
            "0.00596501621249584 0.06182137034584198 1.0\n",
            "repr, std, cov, closslb 0.011549431830644608 0.477294921875 0.0003001920413225889 1.7398084310116246e-05 0.0007959019858390093\n",
            "0.005923427619650864 0.06120654326112999 1.0\n",
            "repr, std, cov, closslb 0.019012415781617165 0.474609375 0.0004055122844874859 0.015434716828167439 0.00022539567726198584\n",
            "0.00587038235087583 0.06065842859736564 1.0\n",
            "repr, std, cov, closslb 0.02218789979815483 0.474365234375 0.0004223678261041641 0.0154091352596879 0.03759963810443878\n",
            "0.005852806367501418 0.060719087025963 1.0\n",
            "repr, std, cov, closslb 0.011558987200260162 0.477783203125 0.00028413906693458557 0.00014133559307083488 0.02894439548254013\n",
            "0.00587038235087583 0.0620070199828119 1.0\n",
            "repr, std, cov, closslb 0.01368737779557705 0.4765625 0.0003372367937117815 0.00010649867181200534 0.00014850666048005223\n",
            "0.005911598511030293 0.06325901270565529 1.0\n",
            "repr, std, cov, closslb 0.02463526651263237 0.474609375 0.00040637305937707424 0.015454573556780815 0.02023143507540226\n",
            "0.00588801111494589 0.062255420352953096 1.0\n",
            "repr, std, cov, closslb 0.019333234056830406 0.476318359375 0.00031329086050391197 0.03095569834113121 0.007461836561560631\n",
            "0.005858659173868919 0.06231767577330604 1.0\n",
            "repr, std, cov, closslb 0.017536204308271408 0.47607421875 0.00030148401856422424 0.015428276732563972 0.00024026962637435645\n",
            "0.005846959408093325 0.06231767577330604 1.0\n",
            "repr, std, cov, closslb 0.025463279336690903 0.47607421875 0.00029167626053094864 0.01534326747059822 0.0004362699401099235\n",
            "0.005852806367501418 0.06281796556856967 1.0\n",
            "repr, std, cov, closslb 0.017813777551054955 0.476318359375 0.00029119057580828667 0.015397193841636181 0.0007566340500488877\n",
            "0.005864517833042787 0.06408633221120912 1.0\n",
            "repr, std, cov, closslb 0.017705615609884262 0.474853515625 0.0003770336043089628 0.030724862590432167 0.017922453582286835\n",
            "0.0058178121112089564 0.06415041854342032 1.0\n",
            "repr, std, cov, closslb 0.012216608971357346 0.4755859375 0.0003462834283709526 2.7464915547170676e-05 0.0003578888135962188\n",
            "0.005788810114617674 0.06421456896196373 1.0\n",
            "repr, std, cov, closslb 0.0140013936907053 0.47607421875 0.0003207768313586712 0.015368334017693996 0.03295125812292099\n",
            "0.0057255136150020204 0.06357594099221708 1.0\n",
            "repr, std, cov, closslb 0.01233644224703312 0.475830078125 0.0003714610356837511 9.891380614135414e-05 0.00034115766175091267\n",
            "0.0056234269397916386 0.06231767577330604 1.0\n",
            "repr, std, cov, closslb 0.016987886279821396 0.476318359375 0.0003262122627347708 0.015613624826073647 0.0001662305585341528\n",
            "0.005573068226456742 0.06231767577330604 1.0\n",
            "repr, std, cov, closslb 0.012598592787981033 0.475830078125 0.00036936812102794647 2.1026877220720053e-05 0.0007276973919942975\n",
            "0.005528683644167766 0.06182137034584198 1.0\n",
            "repr, std, cov, closslb 0.02740584686398506 0.4736328125 0.0004332461394369602 0.015377339906990528 0.0001830495020840317\n",
            "0.005479173372631048 0.060295748463537746 1.0\n",
            "repr, std, cov, closslb 0.016206132248044014 0.47509765625 0.0003860862925648689 0.00024284777464345098 0.013989431783556938\n",
            "0.005430106473720692 0.058925450091807365 1.0\n",
            "repr, std, cov, closslb 0.018698470667004585 0.474853515625 0.00037007033824920654 0.015388717874884605 0.001018086215481162\n",
            "0.005365366775170688 0.057990608895868406 1.0\n",
            "repr, std, cov, closslb 0.013914301991462708 0.473876953125 0.0004806912038475275 3.238879071432166e-05 0.012423304840922356\n",
            "0.005285526485462593 0.05655952217933731 1.0\n",
            "repr, std, cov, closslb 0.021534394472837448 0.474609375 0.0003868322819471359 0.015466161072254181 0.014820889569818974\n",
            "0.00525917795130846 0.05577359607945167 1.0\n",
            "17\n",
            "repr, std, cov, closslb 0.01471327431499958 0.4755859375 0.00034734723158180714 0.00016797575517557561 0.0002813817700371146\n",
            "0.005296102823960002 0.05560660937588928 1.0\n",
            "repr, std, cov, closslb 0.015335265547037125 0.4765625 0.0002933430951088667 0.00014984002336859703 0.0008880291716195643\n",
            "0.0053279589884476465 0.05616518445155457 1.0\n",
            "repr, std, cov, closslb 0.01416611485183239 0.47607421875 0.000341631006449461 6.1626938986592e-05 0.0015097572468221188\n",
            "0.0053546521162860005 0.05684288595133384 1.0\n",
            "repr, std, cov, closslb 0.01459060050547123 0.47802734375 0.0002564922906458378 7.195562830020208e-06 0.003129606833681464\n",
            "0.005381478976961891 0.05793267621964877 1.0\n",
            "repr, std, cov, closslb 0.023570191115140915 0.476806640625 0.00029627629555761814 0.015492070466279984 0.00035493524046614766\n",
            "0.005408440240478169 0.059398506909097634 1.0\n",
            "repr, std, cov, closslb 0.012916002422571182 0.47802734375 0.00022093625739216805 0.00012681535736192018 0.00037468032678589225\n",
            "0.00544641308889138 0.060901426505021033 1.0\n",
            "repr, std, cov, closslb 0.012851890176534653 0.478759765625 0.0001933281309902668 0.00017111421038862318 0.00013677316019311547\n",
            "0.005490137198549682 0.06244237344252841 1.0\n",
            "repr, std, cov, closslb 0.021151656284928322 0.477783203125 0.0002249197568744421 0.015418252907693386 0.0004137788782827556\n",
            "0.005534212327811933 0.06402230990130782 1.0\n",
            "repr, std, cov, closslb 0.0157933309674263 0.477783203125 0.00022929138503968716 2.448480336170178e-05 0.004045191686600447\n",
            "0.005550831572966563 0.06524974389094063 1.0\n",
            "repr, std, cov, closslb 0.022786732763051987 0.476806640625 0.00026577734388411045 0.015540007501840591 0.0020119305700063705\n",
            "0.005528683644167766 0.06590518541201322 1.0\n",
            "repr, std, cov, closslb 0.014632553793489933 0.477294921875 0.00029886653646826744 0.0001799563760869205 0.00030628714011982083\n",
            "0.005440972116774606 0.06505438550621111 1.0\n",
            "repr, std, cov, closslb 0.013642976991832256 0.475830078125 0.00039016432128846645 1.626439916435629e-05 0.00016253972717095166\n",
            "0.005365366775170688 0.06440740537677106 1.0\n",
            "repr, std, cov, closslb 0.01759406551718712 0.476318359375 0.00031876005232334137 2.2398130568035413e-06 0.015302818268537521\n",
            "0.005333286947436094 0.0638944570926654 1.0\n",
            "repr, std, cov, closslb 0.029247887432575226 0.474853515625 0.0003750582691282034 0.030837029218673706 0.012533686123788357\n",
            "0.00525392402728118 0.06237999344907934 1.0\n",
            "repr, std, cov, closslb 0.022212054580450058 0.474365234375 0.0003978838212788105 0.01537693664431572 0.03281313553452492\n",
            "0.005144796030009398 0.060840585919101936 1.0\n",
            "repr, std, cov, closslb 0.016504066064953804 0.473388671875 0.0005153806414455175 0.0003760076069738716 0.0004131401947233826\n",
            "0.005083457906034115 0.059398506909097634 1.0\n",
            "repr, std, cov, closslb 0.0316782146692276 0.473388671875 0.0005033123306930065 0.00011706740042427555 0.00018253643065690994\n",
            "0.0050278739284939 0.05804859950476427 1.0\n",
            "repr, std, cov, closslb 0.02563910186290741 0.474365234375 0.0004707705229520798 0.015570509247481823 0.004414800554513931\n",
            "0.004982848490347387 0.05684288595133384 1.0\n",
            "repr, std, cov, closslb 0.021266531199216843 0.474853515625 0.00038831098936498165 0.015604225918650627 0.0019366194028407335\n",
            "0.0050028098013360925 0.05667269778321815 1.0\n",
            "repr, std, cov, closslb 0.027212917804718018 0.474365234375 0.00043727573938667774 0.01536041870713234 0.00018397337407805026\n",
            "0.005012820423748565 0.05627757098564212 1.0\n",
            "repr, std, cov, closslb 0.020796258002519608 0.4755859375 0.00034581683576107025 9.080982999876142e-05 0.00042252804269082844\n",
            "0.0050581166908067144 0.05661608170151664 1.0\n",
            "repr, std, cov, closslb 0.04190804809331894 0.476318359375 0.0002673058770596981 0.01545430812984705 0.03583871200680733\n",
            "0.005149940826039407 0.05707059877988324 1.0\n",
            "repr, std, cov, closslb 0.033859819173812866 0.476318359375 0.00028049713000655174 0.015382712706923485 0.0078276377171278\n",
            "0.005222510521962997 0.05845616075562107 1.0\n",
            "repr, std, cov, closslb 0.02289149910211563 0.4794921875 0.00017228652723133564 5.6997705542016774e-05 0.015546070411801338\n",
            "0.005296102823960002 0.05993523681449447 1.0\n",
            "repr, std, cov, closslb 0.020093537867069244 0.478515625 0.00019947858527302742 8.113541116472334e-05 0.0014294714201241732\n",
            "0.005322636352095552 0.061329017554195495 1.0\n",
            "repr, std, cov, closslb 0.027133654803037643 0.476318359375 0.00030587962828576565 0.015441998839378357 0.00032794717117212713\n",
            "0.005322636352095552 0.06244237344252841 1.0\n",
            "repr, std, cov, closslb 0.020238831639289856 0.47705078125 0.00027685659006237984 0.00010151398601010442 0.03349749743938446\n",
            "0.005296102823960002 0.06275521035821147 1.0\n",
            "repr, std, cov, closslb 0.055026836693286896 0.4755859375 0.0003719853702932596 0.04586440324783325 0.014370054937899113\n",
            "0.005274971267955416 0.06313268420456197 1.0\n",
            "repr, std, cov, closslb 0.030840720981359482 0.47509765625 0.0003340570256114006 0.015367683954536915 0.014752167277038097\n",
            "0.005227733032484959 0.06294366431767236 1.0\n",
            "repr, std, cov, closslb 0.02719830721616745 0.4765625 0.00026799109764397144 5.149733988218941e-05 0.015680862590670586\n",
            "0.005212081147586677 0.06313268420456197 1.0\n",
            "repr, std, cov, closslb 0.019607527181506157 0.4765625 0.00029047299176454544 1.564197737025097e-05 0.015836238861083984\n",
            "0.005129392459324655 0.062255420352953096 1.0\n",
            "repr, std, cov, closslb 0.020233580842614174 0.474365234375 0.00039880163967609406 0.00013577539357356727 0.01704307459294796\n",
            "0.00504297263892904 0.060719087025963 1.0\n",
            "repr, std, cov, closslb 0.017043346539139748 0.474609375 0.000418683048337698 0.0002539411943871528 0.006211398169398308\n",
            "0.0049431644886688475 0.05922066718631648 1.0\n",
            "repr, std, cov, closslb 0.026804329827427864 0.47412109375 0.00046167708933353424 0.015404379926621914 0.008785349316895008\n",
            "0.004850177026221804 0.05775922520858817 1.0\n",
            "repr, std, cov, closslb 0.01824711635708809 0.47509765625 0.00038316636346280575 4.5475280785467476e-06 0.0010480362689122558\n",
            "0.0047636977197205695 0.05633384855662776 1.0\n",
            "18\n",
            "repr, std, cov, closslb 0.024053171277046204 0.474365234375 0.0004130885936319828 3.5214850413467502e-06 0.0015196094755083323\n",
            "0.004692810672858394 0.054998590860978754 1.0\n",
            "repr, std, cov, closslb 0.024708688259124756 0.472412109375 0.0005228461232036352 0.015421525575220585 0.03341422230005264\n",
            "0.004613746366760738 0.053641340880203274 1.0\n",
            "repr, std, cov, closslb 0.025791915133595467 0.4755859375 0.0003494571428745985 0.015335803851485252 0.03152061626315117\n",
            "0.004650785781286107 0.05342731085913892 1.0\n",
            "repr, std, cov, closslb 0.022098205983638763 0.47607421875 0.00030241417698562145 0.015386068262159824 0.02809334732592106\n",
            "0.0046740862645814635 0.053587753127076206 1.0\n",
            "repr, std, cov, closslb 0.018617630004882812 0.4775390625 0.00022579333744943142 0.00016095767205115408 0.0019101619254797697\n",
            "0.004692810672858394 0.0546697512132159 1.0\n",
            "repr, std, cov, closslb 0.030846474692225456 0.476806640625 0.0002600923180580139 0.03082360327243805 0.029157720506191254\n",
            "0.004725759061005249 0.05605302235382458 1.0\n",
            "repr, std, cov, closslb 0.016627682372927666 0.4775390625 0.0002694812137633562 8.999966667033732e-05 0.03303166478872299\n",
            "0.0047636977197205695 0.05729922382693489 1.0\n",
            "repr, std, cov, closslb 0.01580098271369934 0.4775390625 0.00024789338931441307 0.00012073195830453187 0.0017674146220088005\n",
            "0.004782781111845322 0.05863170466482634 1.0\n",
            "repr, std, cov, closslb 0.038483746349811554 0.4765625 0.0002651810646057129 0.030622761696577072 0.01656194031238556\n",
            "0.00481636118559642 0.06011522239058362 1.0\n",
            "repr, std, cov, closslb 0.017415868118405342 0.478271484375 0.00020544975996017456 0.00017874788318295032 0.015265078283846378\n",
            "0.004835655547776178 0.06163627654573887 1.0\n",
            "repr, std, cov, closslb 0.012621155008673668 0.4794921875 0.00015245727263391018 0.00011280690523562953 0.01490146666765213\n",
            "0.004874476461649199 0.06313268420456197 1.0\n",
            "repr, std, cov, closslb 0.024042360484600067 0.476806640625 0.00028322101570665836 0.015482448041439056 0.0026628286577761173\n",
            "0.004894003633857344 0.0643430623144566 1.0\n",
            "repr, std, cov, closslb 0.015814336016774178 0.475830078125 0.0003178848419338465 0.015620280057191849 0.005234801210463047\n",
            "0.004835655547776178 0.06485961202643613 1.0\n",
            "repr, std, cov, closslb 0.017161890864372253 0.477783203125 0.00019589648582041264 1.6240692275459878e-05 0.000750810606405139\n",
            "0.004773229878857729 0.0643430623144566 1.0\n",
            "repr, std, cov, closslb 0.04074167460203171 0.475341796875 0.00033145444467663765 0.01592208445072174 0.02737685851752758\n",
            "0.004754184596343288 0.06288078353413823 1.0\n",
            "repr, std, cov, closslb 0.016707347705960274 0.47607421875 0.0003321324475109577 5.416025669546798e-05 0.009338309057056904\n",
            "0.004664752095638093 0.061451736918321426 1.0\n",
            "repr, std, cov, closslb 0.024681441485881805 0.4755859375 0.000340545317158103 1.8967783034895547e-05 0.0016356233973056078\n",
            "0.0046694168477337305 0.06077980611298896 1.0\n",
            "repr, std, cov, closslb 0.04785033315420151 0.475341796875 0.00031733489595353603 0.015540837310254574 0.0005855154013261199\n",
            "0.0047210380229822674 0.06011522239058362 1.0\n",
            "repr, std, cov, closslb 0.04188133403658867 0.474365234375 0.00039508682675659657 0.00011493373312987387 0.0008730044355615973\n",
            "0.004739950520191205 0.05904335991744106 1.0\n",
            "repr, std, cov, closslb 0.04018475115299225 0.474853515625 0.0003886246122419834 0.00011440423259045929 0.0021140140015631914\n",
            "0.004735215304886319 0.057990608895868406 1.0\n",
            "repr, std, cov, closslb 0.03600647673010826 0.476318359375 0.0003173754084855318 7.450106204487383e-05 0.03560105711221695\n",
            "0.004739950520191205 0.0575862935115866 1.0\n",
            "repr, std, cov, closslb 0.04036888852715492 0.476318359375 0.0002602564636617899 0.00010208069579675794 0.015151455998420715\n",
            "0.004778003108736586 0.057643879805098185 1.0\n",
            "repr, std, cov, closslb 0.050018005073070526 0.475830078125 0.0003097213339060545 0.015415161848068237 0.001123787835240364\n",
            "0.00476846141744029 0.05678609985148236 1.0\n",
            "repr, std, cov, closslb 0.030861837789416313 0.475341796875 0.00035336147993803024 0.00015277479542419314 0.015790479257702827\n",
            "0.004754184596343288 0.0561090753761784 1.0\n",
            "repr, std, cov, closslb 0.031197629868984222 0.474853515625 0.0003562381025403738 0.015545595437288284 0.00022838599397800863\n",
            "0.004739950520191205 0.055329408485804636 1.0\n",
            "repr, std, cov, closslb 0.03468598425388336 0.476318359375 0.0002550992649048567 0.015498052351176739 0.0001883834192994982\n",
            "0.004778003108736586 0.0554955627548169 1.0\n",
            "repr, std, cov, closslb 0.036001525819301605 0.4755859375 0.0003354314249008894 0.015453503467142582 0.0008809646824374795\n",
            "0.00476846141744029 0.05510864304129156 1.0\n",
            "repr, std, cov, closslb 0.03354192525148392 0.474609375 0.0003400305286049843 0.0001595609646756202 0.000400643446482718\n",
            "0.004744690470711395 0.05472442096442911 1.0\n",
            "repr, std, cov, closslb 0.02955756150186062 0.4755859375 0.00031832698732614517 0.00014019591617397964 0.00013564436812885106\n",
            "0.004697503483531252 0.05391008453469474 1.0\n",
            "repr, std, cov, closslb 0.038433536887168884 0.474609375 0.0003574928268790245 0.015565089881420135 0.0010951498989015818\n",
            "0.004660092003634459 0.053107865982301154 1.0\n",
            "repr, std, cov, closslb 0.027477219700813293 0.475830078125 0.00028043845668435097 7.884859769546892e-06 0.002142546232789755\n",
            "0.0046694168477337305 0.05321413482213173 1.0\n",
            "repr, std, cov, closslb 0.028444526717066765 0.475830078125 0.0002859653905034065 3.228362402296625e-06 0.017373450100421906\n",
            "0.004697503483531252 0.053267348956953856 1.0\n",
            "repr, std, cov, closslb 0.022430066019296646 0.4765625 0.00028028665110468864 4.4339340092847124e-05 0.00042091432260349393\n",
            "0.004725759061005249 0.05428858912695374 1.0\n",
            "repr, std, cov, closslb 0.02214469015598297 0.476318359375 0.0002999429125338793 0.00011468098091427237 0.0002115796523867175\n",
            "0.0047210380229822674 0.054342877716080686 1.0\n",
            "repr, std, cov, closslb 0.020768918097019196 0.47802734375 0.00021100719459354877 0.000109177126432769 0.0018065397161990404\n",
            "0.0047494351611821065 0.055717878201250425 1.0\n",
            "19\n",
            "repr, std, cov, closslb 0.022624583914875984 0.476318359375 0.00029731751419603825 0.015397614799439907 0.000853972916956991\n",
            "0.004725759061005249 0.056446572587589554 1.0\n",
            "repr, std, cov, closslb 0.02096598967909813 0.478515625 0.0002135483082383871 2.9226965125417337e-05 0.00010227216989733279\n",
            "0.00476846141744029 0.057874801418230544 1.0\n",
            "repr, std, cov, closslb 0.030041920021176338 0.47607421875 0.00026355194859206676 6.138865137472749e-05 0.015963425859808922\n",
            "0.004735215304886319 0.057413879573812575 1.0\n",
            "repr, std, cov, closslb 0.019072506576776505 0.47705078125 0.0002595714759081602 0.00011603837629081681 0.002075605560094118\n",
            "0.004730484820066254 0.057874801418230544 1.0\n",
            "repr, std, cov, closslb 0.026590993627905846 0.475830078125 0.0002734942827373743 0.00013721274444833398 0.004225284792482853\n",
            "0.004664752095638093 0.05729922382693489 1.0\n",
            "repr, std, cov, closslb 0.01587095484137535 0.477783203125 0.0002388914581388235 1.3116767149767838e-05 0.00046058616135269403\n",
            "0.004618360113127499 0.05667269778321815 1.0\n",
            "repr, std, cov, closslb 0.017571888864040375 0.475830078125 0.00035578315146267414 2.9892189559177496e-06 0.0012498056748881936\n",
            "0.004531482650609149 0.05555105831757171 1.0\n",
            "repr, std, cov, closslb 0.05841492861509323 0.473876953125 0.00048239086754620075 0.031025948002934456 0.002504063304513693\n",
            "0.004490902478015576 0.054888758455309695 1.0\n",
            "repr, std, cov, closslb 0.018344353884458542 0.47412109375 0.0004942775703966618 7.862350321374834e-05 0.0004869760014116764\n",
            "0.004437360310094699 0.053963994619229426 1.0\n",
            "repr, std, cov, closslb 0.016109369695186615 0.47607421875 0.00033287680707871914 0.0001855623268056661 0.015296866185963154\n",
            "0.004371329384802384 0.05284312141531496 1.0\n",
            "repr, std, cov, closslb 0.01633487269282341 0.474365234375 0.00046317698433995247 0.00012170382251497358 0.025278346613049507\n",
            "0.004306281044378155 0.05195282239880014 1.0\n",
            "repr, std, cov, closslb 0.01528048887848854 0.476318359375 0.0002837174106389284 4.936400000588037e-05 0.0022483007051050663\n",
            "0.004310587325422533 0.05247469474505792 1.0\n",
            "repr, std, cov, closslb 0.024850968271493912 0.474609375 0.00036851572804152966 2.6128873287234455e-05 0.002350807888433337\n",
            "0.004237962704589429 0.05133342201002367 1.0\n",
            "repr, std, cov, closslb 0.017091311514377594 0.474609375 0.0003707262221723795 1.8047807316179387e-05 0.001326333382166922\n",
            "0.004191623612549627 0.05016680402470718 1.0\n",
            "repr, std, cov, closslb 0.022828074172139168 0.475341796875 0.0003195756580680609 0.00014027168799657375 0.00017199377180077136\n",
            "0.004125124291377253 0.04912480135692682 1.0\n",
            "repr, std, cov, closslb 0.01737378165125847 0.4765625 0.00025134114548563957 0.00022126482508610934 0.030045650899410248\n",
            "0.004149936996554271 0.05011668733736982 1.0\n",
            "repr, std, cov, closslb 0.014200826175510883 0.47607421875 0.00035492563620209694 0.0002199305163230747 0.030858080834150314\n",
            "0.004162399261504859 0.05082289987138209 1.0\n",
            "repr, std, cov, closslb 0.014070790261030197 0.47705078125 0.00027233618311583996 9.543400665279478e-05 0.0011878362856805325\n",
            "0.004208415273512185 0.05200477522119894 1.0\n",
            "repr, std, cov, closslb 0.020341668277978897 0.47607421875 0.0002716875169426203 0.015512232668697834 0.015629593282938004\n",
            "0.004246442867961311 0.05305481117113003 1.0\n",
            "repr, std, cov, closslb 0.0211972426623106 0.4755859375 0.00029066973365843296 0.015417179092764854 0.0003633874875959009\n",
            "0.00425919494014024 0.05321413482213173 1.0\n",
            "repr, std, cov, closslb 0.01424693875014782 0.477783203125 0.00020936783403158188 0.0003622350050136447 0.0001410979311913252\n",
            "0.004289098897035946 0.05423435477218156 1.0\n",
            "repr, std, cov, closslb 0.030002348124980927 0.476806640625 0.00025005917996168137 0.015363995917141438 0.005504305474460125\n",
            "0.004297681383928914 0.05418017459758398 1.0\n",
            "repr, std, cov, closslb 0.030492028221488 0.476318359375 0.0002703352365642786 5.807579873362556e-05 0.0006952813710086048\n",
            "0.004314897912747955 0.05461513607713877 1.0\n",
            "repr, std, cov, closslb 0.028070956468582153 0.475830078125 0.0002807904966175556 0.015605202876031399 0.00035405735252425075\n",
            "0.004332183411050329 0.055329408485804636 1.0\n",
            "repr, std, cov, closslb 0.01975892297923565 0.475341796875 0.00032227300107479095 0.015479904599487782 7.460884808097035e-05\n",
            "0.00425919494014024 0.053963994619229426 1.0\n",
            "repr, std, cov, closslb 0.019435230642557144 0.474609375 0.0003678190987557173 0.00012024208263028413 0.01576158218085766\n",
            "0.004242200667294018 0.053963994619229426 1.0\n",
            "repr, std, cov, closslb 0.027285192161798477 0.4755859375 0.00030570710077881813 0.015438348054885864 0.00039604873745702207\n",
            "0.004229499476137678 0.053641340880203274 1.0\n",
            "repr, std, cov, closslb 0.014739727601408958 0.4765625 0.0002520021516829729 0.0001199249891215004 0.0003421098808757961\n",
            "0.004242200667294018 0.05348073816999805 1.0\n",
            "repr, std, cov, closslb 0.017070306465029716 0.47607421875 0.00026973290368914604 0.00011513035860843956 0.022602856159210205\n",
            "0.004246442867961311 0.054506069432204936 1.0\n",
            "repr, std, cov, closslb 0.023529646918177605 0.47607421875 0.0002833656035363674 0.030931519344449043 0.00014475526404567063\n",
            "0.004271985306804675 0.055163751684332844 1.0\n",
            "repr, std, cov, closslb 0.03646441921591759 0.476806640625 0.0002172519452869892 0.030991166830062866 8.534633525414392e-05\n",
            "0.004289098897035946 0.055329408485804636 1.0\n",
            "repr, std, cov, closslb 0.025320515036582947 0.4755859375 0.0002818005159497261 5.7808552810456604e-05 0.00010597282380331308\n",
            "0.004237962704589429 0.05428858912695374 1.0\n",
            "repr, std, cov, closslb 0.032689206302165985 0.474609375 0.0003170180134475231 0.030710389837622643 0.0010520610958337784\n",
            "0.004216836312474483 0.053963994619229426 1.0\n",
            "repr, std, cov, closslb 0.027616020292043686 0.474609375 0.0003272721078246832 0.015418157912790775 0.016216540709137917\n",
            "0.004154086933550825 0.05268490858215785 1.0\n",
            "repr, std, cov, closslb 0.019865302368998528 0.474609375 0.00033814506605267525 4.408734093885869e-05 0.00036083380109630525\n",
            "0.004121003288089164 0.05179727512978813 1.0\n",
            "20\n",
            "repr, std, cov, closslb 0.017434721812605858 0.475341796875 0.0003197391051799059 3.9320693758782e-05 0.006630006246268749\n",
            "0.0040637396494121215 0.05077212774363846 1.0\n",
            "repr, std, cov, closslb 0.02869459241628647 0.472900390625 0.0004543240647763014 0.030833836644887924 0.0012298866640776396\n",
            "0.004007271720926335 0.0495686971946672 1.0\n",
            "repr, std, cov, closslb 0.028500504791736603 0.475341796875 0.00029493169859051704 9.216753824148327e-05 0.032628633081912994\n",
            "0.00395949557427502 0.04858765677822385 1.0\n",
            "repr, std, cov, closslb 0.02572079375386238 0.47412109375 0.0003915999550372362 0.015462194569408894 0.0004072439332958311\n",
            "0.003900575599742993 0.0473886152174162 1.0\n",
            "repr, std, cov, closslb 0.025712627917528152 0.475830078125 0.00030051148496568203 5.495893856277689e-05 0.0011941653210669756\n",
            "0.003935821529002581 0.047341273943472734 1.0\n",
            "repr, std, cov, closslb 0.015374723821878433 0.476318359375 0.000256106024608016 7.295383693417534e-05 0.0016758269630372524\n",
            "0.00395949557427502 0.04767365868643234 1.0\n",
            "repr, std, cov, closslb 0.02066534385085106 0.47607421875 0.00025724712759256363 0.015404647216200829 0.03652111440896988\n",
            "0.003967418524919144 0.048297148463547934 1.0\n",
            "repr, std, cov, closslb 0.01918761245906353 0.47607421875 0.0002729506231844425 0.015399867668747902 0.015582487918436527\n",
            "0.00398331201940361 0.04897772121115203 1.0\n",
            "repr, std, cov, closslb 0.02276715263724327 0.476806640625 0.00022115185856819153 0.00013430093531496823 0.016100021079182625\n",
            "0.004015290271639907 0.04986685390041087 1.0\n",
            "repr, std, cov, closslb 0.014224029146134853 0.47705078125 0.00026661925949156284 5.935543595114723e-05 0.003332265419885516\n",
            "0.004031375540533271 0.05082289987138209 1.0\n",
            "repr, std, cov, closslb 0.01563611812889576 0.477294921875 0.00022061052732169628 3.852064037346281e-05 0.0029798420146107674\n",
            "0.0040515727723252565 0.05184907240491791 1.0\n",
            "repr, std, cov, closslb 0.0229280237108469 0.478759765625 0.00016211275942623615 0.01544304471462965 0.016586177051067352\n",
            "0.004116886401687478 0.053001809361768265 1.0\n",
            "repr, std, cov, closslb 0.03653760254383087 0.474853515625 0.00032657524570822716 0.045442305505275726 0.015210420824587345\n",
            "0.004112773628059419 0.05348073816999805 1.0\n",
            "repr, std, cov, closslb 0.014397768303751945 0.47705078125 0.0002432831097394228 0.00020659793517552316 0.0159778892993927\n",
            "0.0041292494156686295 0.05461513607713877 1.0\n",
            "repr, std, cov, closslb 0.020155474543571472 0.476318359375 0.00026243715547025204 4.9380392738385126e-05 0.01630793884396553\n",
            "0.004133378665084298 0.05472442096442911 1.0\n",
            "repr, std, cov, closslb 0.026423480361700058 0.475341796875 0.0003193302545696497 4.701588295574766e-06 0.04701107740402222\n",
            "0.004133378665084298 0.05391008453469474 1.0\n",
            "repr, std, cov, closslb 0.029326599091291428 0.475341796875 0.0003007033374160528 0.015545827336609364 0.02591998316347599\n",
            "0.004100459942750879 0.05257969660924277 1.0\n",
            "repr, std, cov, closslb 0.029635462909936905 0.4755859375 0.0003122000489383936 0.00014980639389250427 0.012346412055194378\n",
            "0.004112773628059419 0.05226531966536495 1.0\n",
            "repr, std, cov, closslb 0.026192115619778633 0.474609375 0.0003777423407882452 0.00012755703937727958 0.016056040301918983\n",
            "0.0040637396494121215 0.050975521090518734 1.0\n",
            "repr, std, cov, closslb 0.030649840831756592 0.47412109375 0.0003934986889362335 0.01538387406617403 0.004158247262239456\n",
            "0.0040112789926472605 0.05011668733736982 1.0\n",
            "repr, std, cov, closslb 0.017275851219892502 0.475830078125 0.0003072016406804323 0.00018994873971678317 0.0017488020239397883\n",
            "0.003967418524919144 0.049272323184526456 1.0\n",
            "repr, std, cov, closslb 0.024306099861860275 0.474365234375 0.00036737555637955666 0.015406753867864609 0.015174273401498795\n",
            "0.0039044761753427356 0.04839379105762348 1.0\n",
            "repr, std, cov, closslb 0.039899010211229324 0.473876953125 0.00040442775934934616 0.015575261786580086 0.0005854729097336531\n",
            "0.0038579255944635803 0.04729397996350923 1.0\n",
            "repr, std, cov, closslb 0.03211332857608795 0.475341796875 0.0003153958823531866 0.01565740816295147 0.017917398363351822\n",
            "0.0038579255944635803 0.046636803739443296 1.0\n",
            "repr, std, cov, closslb 0.016275448724627495 0.476318359375 0.00026610749773681164 5.7846889831125736e-05 0.0012002408038824797\n",
            "0.00388113109413068 0.047246733230278955 1.0\n",
            "repr, std, cov, closslb 0.03008987195789814 0.4755859375 0.0002855162601917982 0.015382504090666771 0.00039195979479700327\n",
            "0.003912289032169595 0.0480563854914317 1.0\n",
            "repr, std, cov, closslb 0.021941522136330605 0.4765625 0.00026553869247436523 0.00021491808001883328 0.03549429029226303\n",
            "0.003939757350531583 0.04912480135692682 1.0\n",
            "repr, std, cov, closslb 0.02214408852159977 0.477294921875 0.00022035744041204453 0.015440135262906551 0.015139100141823292\n",
            "0.003951588445794986 0.04986685390041087 1.0\n",
            "repr, std, cov, closslb 0.03402062505483627 0.476806640625 0.00024552084505558014 0.015501253306865692 0.0003725164569914341\n",
            "0.0039713859434440625 0.05112860063132906 1.0\n",
            "repr, std, cov, closslb 0.020302988588809967 0.47705078125 0.0002467494923621416 1.869280094979331e-05 0.000700507138390094\n",
            "0.003967418524919144 0.05164219357085269 1.0\n",
            "repr, std, cov, closslb 0.01725105755031109 0.47607421875 0.00028064893558621407 0.0001240137207787484 9.225531539414078e-05\n",
            "0.003920117522522966 0.05117972923196038 1.0\n",
            "repr, std, cov, closslb 0.01737949438393116 0.477294921875 0.00025044381618499756 0.000296656449791044 0.014729110524058342\n",
            "0.0039044761753427356 0.05164219357085269 1.0\n",
            "repr, std, cov, closslb 0.016778133809566498 0.47705078125 0.00023795501329004765 0.00012838120164815336 7.195901707746089e-05\n",
            "0.003900575599742993 0.05221310655880615 1.0\n",
            "repr, std, cov, closslb 0.019719701260328293 0.4765625 0.000245062168687582 2.7653968572849408e-06 0.015121419914066792\n",
            "0.00387725384029039 0.05216094561319296 1.0\n",
            "repr, std, cov, closslb 0.015804961323738098 0.478271484375 0.00019342917948961258 1.1280058970442042e-05 0.014778266660869122\n",
            "0.003861783520058043 0.05268490858215785 1.0\n",
            "21\n",
            "repr, std, cov, closslb 0.02489442750811577 0.475830078125 0.00028039864264428616 0.0157867930829525 0.0021169083192944527\n",
            "0.00384637492671229 0.05210883677641655 1.0\n",
            "repr, std, cov, closslb 0.023107971996068954 0.475830078125 0.00028079631738364697 0.015488450415432453 0.0006072765681892633\n",
            "0.003823377236112159 0.05190092147732282 1.0\n",
            "repr, std, cov, closslb 0.022619912400841713 0.477294921875 0.00023223855532705784 0.00020454591140151024 0.0003715825732797384\n",
            "0.0038195576784337258 0.05148757632765318 1.0\n",
            "repr, std, cov, closslb 0.026370827108621597 0.4775390625 0.00019934098236262798 0.01541669387370348 0.018045155331492424\n",
            "0.00385407152294064 0.05236990257001533 1.0\n",
            "repr, std, cov, closslb 0.018475744873285294 0.47705078125 0.0002532452344894409 0.0002294158039148897 0.029827993363142014\n",
            "0.00384637492671229 0.05257969660924277 1.0\n",
            "repr, std, cov, closslb 0.029278170317411423 0.476806640625 0.0002483273856341839 0.015369856730103493 0.0007166584255173802\n",
            "0.003831027813961619 0.052895964536730275 1.0\n",
            "repr, std, cov, closslb 0.025834646075963974 0.476318359375 0.00022391881793737411 7.779768202453852e-05 0.04545898735523224\n",
            "0.0037853529110821148 0.05226531966536495 1.0\n",
            "repr, std, cov, closslb 0.02220747619867325 0.475830078125 0.00027136970311403275 3.694161932799034e-06 0.0006878076819702983\n",
            "0.003758961113409587 0.05117972923196038 1.0\n",
            "repr, std, cov, closslb 0.01739664562046528 0.4755859375 0.0003479421138763428 0.00012105402129236609 0.0157163143157959\n",
            "0.0037104349808874 0.05046855835500465 1.0\n",
            "repr, std, cov, closslb 0.02473716251552105 0.47607421875 0.0003053417894989252 0.00023420844809152186 0.0004250638885423541\n",
            "0.0037067282526347654 0.050016604112540636 1.0\n",
            "repr, std, cov, closslb 0.03764907270669937 0.47509765625 0.00034080352634191513 0.01536996103823185 0.0009273249888792634\n",
            "0.003721577420845438 0.0494202880203219 1.0\n",
            "repr, std, cov, closslb 0.040812961757183075 0.474853515625 0.00036138808354735374 0.01541450247168541 0.014255190268158913\n",
            "0.003729024297264549 0.048297148463547934 1.0\n",
            "repr, std, cov, closslb 0.045193180441856384 0.474365234375 0.00036026956513524055 0.015367146581411362 0.030423317104578018\n",
            "0.003721577420845438 0.04715238131526712 1.0\n",
            "repr, std, cov, closslb 0.03218713402748108 0.475830078125 0.00027368543669581413 0.00010933582234429196 0.013539068400859833\n",
            "0.003766482794597519 0.04767365868643234 1.0\n",
            "repr, std, cov, closslb 0.03123149648308754 0.476318359375 0.00028470903635025024 4.28406792707392e-06 0.0009802707936614752\n",
            "0.0037891382639931963 0.047341273943472734 1.0\n",
            "repr, std, cov, closslb 0.031487248837947845 0.4765625 0.00023098383098840714 3.35730001097545e-05 0.0018770741298794746\n",
            "0.0038195576784337258 0.04748343983646624 1.0\n",
            "repr, std, cov, closslb 0.03368118032813072 0.47509765625 0.0002798628993332386 0.0002543960581533611 0.0033112664241343737\n",
            "0.0038119300064907383 0.046964242372419286 1.0\n",
            "repr, std, cov, closslb 0.03524979576468468 0.476806640625 0.0002464568242430687 0.0005567027255892754 0.001949241734109819\n",
            "0.003827200613348271 0.047341273943472734 1.0\n",
            "repr, std, cov, closslb 0.030743543058633804 0.477294921875 0.00024741399101912975 0.000707354280166328 0.020190563052892685\n",
            "0.0038081218846061325 0.04682363096181734 1.0\n",
            "repr, std, cov, closslb 0.030211305245757103 0.4765625 0.0002594569232314825 0.015453964471817017 0.0018201752100139856\n",
            "0.0038195576784337258 0.04729397996350923 1.0\n",
            "repr, std, cov, closslb 0.02760390192270279 0.4775390625 0.00021353038027882576 0.0003020116710104048 0.0005865422426722944\n",
            "0.0038425323943179724 0.04820069886511884 1.0\n",
            "repr, std, cov, closslb 0.028936756774783134 0.4775390625 0.00019549601711332798 4.912510485155508e-05 0.00224507343955338\n",
            "0.003861783520058043 0.0494202880203219 1.0\n",
            "repr, std, cov, closslb 0.031578969210386276 0.477783203125 0.00019129132851958275 0.0157469492405653 0.002257017884403467\n",
            "0.0038966789208221713 0.05067073560169947 1.0\n",
            "repr, std, cov, closslb 0.025073839351534843 0.47705078125 0.00023184320889413357 1.4142057125354768e-06 0.010381102561950684\n",
            "0.003916201321201764 0.051745529600187946 1.0\n",
            "repr, std, cov, closslb 0.02920377254486084 0.47705078125 0.0002238431479781866 0.015467829070985317 0.0004960140213370323\n",
            "0.0038927861346874843 0.05226531966536495 1.0\n",
            "repr, std, cov, closslb 0.03064051643013954 0.476318359375 0.0002467511221766472 0.015395750291645527 0.015332735143601894\n",
            "0.0038850122252248103 0.05305481117113003 1.0\n",
            "repr, std, cov, closslb 0.02633671462535858 0.477294921875 0.00021319696679711342 0.015478594228625298 0.01689203456044197\n",
            "0.0038195576784337258 0.05305481117113003 1.0\n",
            "repr, std, cov, closslb 0.03013567067682743 0.475830078125 0.00030522909946739674 0.015482453629374504 0.02823973260819912\n",
            "0.003758961113409587 0.05263227630585201 1.0\n",
            "repr, std, cov, closslb 0.024498585611581802 0.47509765625 0.0003267573192715645 0.0004596144426614046 0.0378054715692997\n",
            "0.003680884632668929 0.05221310655880615 1.0\n",
            "repr, std, cov, closslb 0.03372134268283844 0.475830078125 0.0003096647560596466 0.00010555011249380186 0.0011702560586854815\n",
            "0.003651569626959984 0.05216094561319296 1.0\n",
            "repr, std, cov, closslb 0.03717948496341705 0.47607421875 0.00028221122920513153 1.1709636282830616e-06 0.0014240345917642117\n",
            "0.0035972317989749624 0.05164219357085269 1.0\n",
            "repr, std, cov, closslb 0.024021929129958153 0.4765625 0.0002613894175738096 3.9405022107530385e-06 0.02080509252846241\n",
            "0.0035757237490562587 0.05133342201002367 1.0\n",
            "repr, std, cov, closslb 0.026280812919139862 0.474365234375 0.00039521604776382446 4.5438078814186156e-05 0.015157591551542282\n",
            "0.0035084596556605114 0.05021697082873188 1.0\n",
            "repr, std, cov, closslb 0.03435719013214111 0.474853515625 0.000341370003297925 0.04603130370378494 0.03156421333551407\n",
            "0.0034874823793467624 0.049618265891861864 1.0\n",
            "repr, std, cov, closslb 0.03458592668175697 0.4755859375 0.0003068887162953615 0.015357108786702156 0.000528281438164413\n",
            "0.0034700971578675226 0.04951917801665055 1.0\n",
            "22\n",
            "repr, std, cov, closslb 0.023028723895549774 0.475341796875 0.0003089581150561571 0.00013922451762482524 0.00042870425386354327\n",
            "0.0034424608888329994 0.04897772121115203 1.0\n",
            "repr, std, cov, closslb 0.023587629199028015 0.47607421875 0.0002378860954195261 0.00011311552952975035 0.0003827624605037272\n",
            "0.003428725401157633 0.04883108142480228 1.0\n",
            "repr, std, cov, closslb 0.026503760367631912 0.47607421875 0.00024532806128263474 7.520723738707602e-05 0.027039209380745888\n",
            "0.003415044718352321 0.049370917103218684 1.0\n",
            "repr, std, cov, closslb 0.03062302991747856 0.475341796875 0.0002840661909431219 3.724676571437158e-05 0.0003440142027102411\n",
            "0.0034048200403662826 0.04868488067943707 1.0\n",
            "repr, std, cov, closslb 0.040028758347034454 0.47412109375 0.0003756915684789419 0.015532286837697029 0.0001443516230210662\n",
            "0.0034048200403662826 0.048152546318800046 1.0\n",
            "repr, std, cov, closslb 0.020597271621227264 0.47705078125 0.00021934206597507 3.9293168811127543e-05 0.001225548330694437\n",
            "0.003418459763070673 0.04897772121115203 1.0\n",
            "repr, std, cov, closslb 0.01796642504632473 0.47705078125 0.00020983279682695866 8.629346848465502e-05 0.015174010768532753\n",
            "0.00343215412655879 0.0495686971946672 1.0\n",
            "repr, std, cov, closslb 0.023002928122878075 0.476318359375 0.00029867514967918396 0.00016098633932415396 0.0010348688811063766\n",
            "0.0034218782228337434 0.04996663747506558 1.0\n",
            "repr, std, cov, closslb 0.026974836364388466 0.475830078125 0.00025238096714019775 0.015397942624986172 0.012760735116899014\n",
            "0.003428725401157633 0.05016680402470718 1.0\n",
            "repr, std, cov, closslb 0.021381158381700516 0.47607421875 0.0002355845645070076 0.0154493423178792 0.00028282642597332597\n",
            "0.003415044718352321 0.04991672075431127 1.0\n",
            "repr, std, cov, closslb 0.025011101737618446 0.473876953125 0.0003606509417295456 0.015527295880019665 0.0029051920864731073\n",
            "0.0033743293167905137 0.04902669893236317 1.0\n",
            "repr, std, cov, closslb 0.024757694453001022 0.476318359375 0.0002259851898998022 8.333573350682855e-05 0.0297545213252306\n",
            "0.003370958358432082 0.04863624443500207 1.0\n",
            "repr, std, cov, closslb 0.030938394367694855 0.475341796875 0.00029105180874466896 0.01537503581494093 0.0013809655793011189\n",
            "0.0033374334383501434 0.047578454199579 1.0\n",
            "repr, std, cov, closslb 0.021908044815063477 0.4755859375 0.00026081129908561707 0.00020921572286169976 0.00024821466649882495\n",
            "0.0033241170122990825 0.047011206614791703 1.0\n",
            "repr, std, cov, closslb 0.02081882394850254 0.4755859375 0.0002663065679371357 3.4460063034202904e-05 0.03152143955230713\n",
            "0.0032910579396761342 0.04626538265339925 1.0\n",
            "repr, std, cov, closslb 0.026578310877084732 0.475830078125 0.00025267782621085644 0.015593292191624641 0.0010467361425980926\n",
            "0.003284485683822806 0.04626538265339925 1.0\n",
            "repr, std, cov, closslb 0.024025078862905502 0.4765625 0.00021830620244145393 2.7192359084438067e-06 0.0003066198551096022\n",
            "0.0032910579396761342 0.04715238131526712 1.0\n",
            "repr, std, cov, closslb 0.02209513634443283 0.476806640625 0.00020456034690141678 2.7860645786859095e-05 0.015145664103329182\n",
            "0.0033174787373456546 0.04796041669761977 1.0\n",
            "repr, std, cov, closslb 0.02283574640750885 0.47705078125 0.00020551960915327072 0.015394139103591442 0.00026102151605300605\n",
            "0.0033508032100572436 0.0487335655601165 1.0\n",
            "repr, std, cov, closslb 0.027137789875268936 0.475341796875 0.0003064167685806751 0.0003470522933639586 0.00019663541752379388\n",
            "0.0033241170122990825 0.04796041669761977 1.0\n",
            "repr, std, cov, closslb 0.023517558351159096 0.476318359375 0.00023128162138164043 0.00023769157996866852 0.00013641055556945503\n",
            "0.0033042419309499984 0.04743600383263361 1.0\n",
            "repr, std, cov, closslb 0.015985477715730667 0.47705078125 0.0002105215098708868 3.479721954136039e-06 0.015124661847949028\n",
            "0.0032812044793434627 0.04673012398372591 1.0\n",
            "repr, std, cov, closslb 0.024244386702775955 0.475830078125 0.0002511765342205763 1.528653592686169e-05 0.01583273895084858\n",
            "0.003284485683822806 0.047246733230278955 1.0\n",
            "repr, std, cov, closslb 0.021232087165117264 0.477294921875 0.00021099974401295185 1.152908407675568e-05 0.019944768399000168\n",
            "0.003307546172880948 0.0484421848486811 1.0\n",
            "repr, std, cov, closslb 0.028367802500724792 0.476806640625 0.00025304267182946205 0.015410170890390873 0.0011180941946804523\n",
            "0.003307546172880948 0.0485391176605633 1.0\n",
            "repr, std, cov, closslb 0.02905738539993763 0.47705078125 0.00023762555792927742 0.00025260841357521713 0.0004629321047104895\n",
            "0.003347455754302941 0.049618265891861864 1.0\n",
            "repr, std, cov, closslb 0.030359499156475067 0.47705078125 0.00020566396415233612 0.01536806020885706 0.0002114813105436042\n",
            "0.0034014186217445385 0.05082289987138209 1.0\n",
            "repr, std, cov, closslb 0.02535439282655716 0.477294921875 0.00020895595662295818 0.046039383858442307 0.00011140619608340785\n",
            "0.0033946259751682275 0.051745529600187946 1.0\n",
            "repr, std, cov, closslb 0.024143036454916 0.476806640625 0.000218698987737298 0.015609482303261757 0.0165603905916214\n",
            "0.003370958358432082 0.05226531966536495 1.0\n",
            "repr, std, cov, closslb 0.03349752724170685 0.4755859375 0.0002766347024589777 0.01575244404375553 0.0010164333507418633\n",
            "0.003327441129311381 0.05123090896119234 1.0\n",
            "repr, std, cov, closslb 0.03034043125808239 0.474609375 0.0003353757783770561 0.01563584804534912 0.015314529649913311\n",
            "0.003271380520369414 0.04996663747506558 1.0\n",
            "repr, std, cov, closslb 0.023611638695001602 0.475341796875 0.0003055371344089508 1.5427245671162382e-05 0.0010971499141305685\n",
            "0.003229148786687563 0.04883108142480228 1.0\n",
            "repr, std, cov, closslb 0.02616826258599758 0.4755859375 0.0003129479009658098 8.413080649916083e-05 0.0003934150154236704\n",
            "0.003200231228087512 0.04767365868643234 1.0\n",
            "repr, std, cov, closslb 0.030741631984710693 0.474853515625 0.00034730485640466213 0.00018521865422371775 0.01575571671128273\n",
            "0.0031779189478164382 0.046497172683377955 1.0\n",
            "repr, std, cov, closslb 0.0350092351436615 0.4736328125 0.00037780427373945713 0.015410837717354298 0.0001672552025411278\n",
            "0.003149460160851108 0.04534971988972096 1.0\n",
            "23\n",
            "repr, std, cov, closslb 0.03569050505757332 0.474609375 0.00029849237762391567 0.0002667347143869847 0.00011220173473702744\n",
            "0.0031652389877673223 0.04489870698081385 1.0\n",
            "repr, std, cov, closslb 0.041478149592876434 0.474853515625 0.00027144490741193295 0.00018684696988202631 0.000763587246183306\n",
            "0.00315576223063297 0.04458566943366029 1.0\n",
            "repr, std, cov, closslb 0.02721306122839451 0.47509765625 0.00033995136618614197 6.34573952993378e-05 0.01591578684747219\n",
            "0.003130629362575652 0.04409815701706797 1.0\n",
            "repr, std, cov, closslb 0.04063718020915985 0.47412109375 0.00033167656511068344 0.015541265718638897 0.0001517578202765435\n",
            "0.0031368937519301652 0.04370325076655621 1.0\n",
            "repr, std, cov, closslb 0.059408679604530334 0.472900390625 0.0005229287780821323 0.046895235776901245 0.0161784365773201\n",
            "0.0031526096210119587 0.04326861235412824 1.0\n",
            "repr, std, cov, closslb 0.031815528869628906 0.47412109375 0.0003259340301156044 0.015582587569952011 0.0003344690194353461\n",
            "0.0031652389877673223 0.04313906569670199 1.0\n",
            "repr, std, cov, closslb 0.02313632145524025 0.475830078125 0.00024469755589962006 0.015416126698255539 0.000191709550563246\n",
            "0.0031747442036128256 0.043225386967161085 1.0\n",
            "repr, std, cov, closslb 0.023028848692774773 0.474853515625 0.000305230263620615 0.00017568175098858774 0.016105404123663902\n",
            "0.003168404226755089 0.042924015947350964 1.0\n",
            "repr, std, cov, closslb 0.028130102902650833 0.47705078125 0.00018383166752755642 0.015420356765389442 0.037557777017354965\n",
            "0.0031747442036128256 0.04387832616398373 1.0\n",
            "repr, std, cov, closslb 0.01944108121097088 0.47802734375 0.00018318742513656616 0.00015030289068818092 0.02255147695541382\n",
            "0.0031970341938936187 0.04498854929348245 1.0\n",
            "repr, std, cov, closslb 0.022985560819506645 0.477783203125 0.0001680576242506504 0.00010587727592792362 0.008769585750997066\n",
            "0.003216264418558545 0.04612686363577418 1.0\n",
            "repr, std, cov, closslb 0.021785490214824677 0.4775390625 0.0001917341724038124 5.901943950448185e-05 5.8068049838766456e-05\n",
            "0.0032356103134097243 0.047246733230278955 1.0\n",
            "repr, std, cov, closslb 0.021554620936512947 0.476806640625 0.00022623548284173012 0.01548608299344778 0.00011312968126730993\n",
            "0.00325182075345219 0.04839379105762348 1.0\n",
            "repr, std, cov, closslb 0.020670782774686813 0.477294921875 0.00020597712136805058 0.015553701668977737 0.0003025903715752065\n",
            "0.0032550725742056417 0.04951917801665055 1.0\n",
            "repr, std, cov, closslb 0.020796865224838257 0.4765625 0.0002598697319626808 0.015490952879190445 0.00025494079454801977\n",
            "0.003200231228087512 0.04917392615828374 1.0\n",
            "repr, std, cov, closslb 0.020866170525550842 0.476318359375 0.00023598992265760899 0.015470641665160656 0.00038025836693122983\n",
            "0.003149460160851108 0.048248899563983955 1.0\n",
            "repr, std, cov, closslb 0.01904820278286934 0.47607421875 0.00025853863917291164 9.425245661986992e-05 0.015539469197392464\n",
            "0.0031243774832317057 0.047246733230278955 1.0\n",
            "repr, std, cov, closslb 0.017522577196359634 0.476318359375 0.00025444081984460354 0.00014964093861635774 0.0001774589909473434\n",
            "0.003115023065849936 0.04687045459277915 1.0\n",
            "repr, std, cov, closslb 0.018162479624152184 0.475341796875 0.0002940148115158081 0.00013997871428728104 0.00040815738611854613\n",
            "0.00307788463122892 0.04603474810481645 1.0\n",
            "repr, std, cov, closslb 0.020082715898752213 0.47607421875 0.00023315963335335255 0.015416032634675503 0.01765493117272854\n",
            "0.0030594817870443045 0.04566812186189505 1.0\n",
            "repr, std, cov, closslb 0.01668991521000862 0.47607421875 0.0002697806339710951 6.299535743892193e-05 0.00025485712103545666\n",
            "0.0030594817870443045 0.046450721961416544 1.0\n",
            "repr, std, cov, closslb 0.019454292953014374 0.476318359375 0.0002305419184267521 3.953377381549217e-05 0.018334250897169113\n",
            "0.003074809821407513 0.04729397996350923 1.0\n",
            "repr, std, cov, closslb 0.018338864669203758 0.475341796875 0.00027538067661225796 0.015461793169379234 0.0002032696211244911\n",
            "0.0030625412688313485 0.046311648036052644 1.0\n",
            "repr, std, cov, closslb 0.026753442361950874 0.47607421875 0.00024177483282983303 9.325552673544735e-05 0.03240414336323738\n",
            "0.0030351157079616825 0.04539506960961068 1.0\n",
            "repr, std, cov, closslb 0.01966775767505169 0.475341796875 0.00024286913685500622 0.0001669511548243463 0.00015769954188726842\n",
            "0.0030109436832487825 0.04454112830535494 1.0\n",
            "repr, std, cov, closslb 0.021993163973093033 0.47607421875 0.00023403018712997437 0.00022019482275936753 0.015464827418327332\n",
            "0.0030199855501405214 0.045168773601951304 1.0\n",
            "repr, std, cov, closslb 0.019146457314491272 0.4765625 0.00022485177032649517 0.00022605588310398161 0.0018239961937069893\n",
            "0.0030049308166845974 0.045168773601951304 1.0\n",
            "repr, std, cov, closslb 0.017769739031791687 0.4765625 0.0002368185669183731 0.00020644179312512279 0.0006681818631477654\n",
            "0.0029959340238151473 0.045576922440092434 1.0\n",
            "repr, std, cov, closslb 0.02075028046965599 0.476318359375 0.00023210560902953148 0.015435950830578804 0.01573762483894825\n",
            "0.0029899511316008147 0.04562249936253252 1.0\n",
            "repr, std, cov, closslb 0.021332846954464912 0.47607421875 0.00026421225629746914 0.0154761066660285 0.00021716774790547788\n",
            "0.002942516118591837 0.044496631673681265 1.0\n",
            "repr, std, cov, closslb 0.01761576533317566 0.476806640625 0.00022229296155273914 1.343509393336717e-05 0.00021588735398836434\n",
            "0.0029395765420497876 0.04467488535819703 1.0\n",
            "repr, std, cov, closslb 0.021101761609315872 0.47705078125 0.0002151129301637411 3.219966311007738e-05 0.006326647009700537\n",
            "0.002957258153785857 0.0454859051438995 1.0\n",
            "repr, std, cov, closslb 0.0169072225689888 0.476318359375 0.00024096551351249218 7.466734678018838e-05 0.016505548730492592\n",
            "0.002975046120770417 0.04654366985606133 1.0\n",
            "repr, std, cov, closslb 0.02267216145992279 0.476318359375 0.00020758295431733131 0.015374959446489811 0.008739268407225609\n",
            "0.002983980187246136 0.04705821782140649 1.0\n",
            "repr, std, cov, closslb 0.022262439131736755 0.475830078125 0.00025114184245467186 0.015402297489345074 0.0001995111524593085\n",
            "0.002963175627351582 0.04626538265339925 1.0\n",
            "24\n",
            "repr, std, cov, closslb 0.022312268614768982 0.474609375 0.0003210790455341339 0.0001771326205926016 0.0002667215303517878\n",
            "0.0029278475729582008 0.045259156317928796 1.0\n",
            "repr, std, cov, closslb 0.023235134780406952 0.4765625 0.0002098658587783575 0.01538524404168129 0.00022696235100738704\n",
            "0.0029366399021476404 0.04539506960961068 1.0\n",
            "repr, std, cov, closslb 0.039830178022384644 0.475830078125 0.00022040563635528088 0.00015767457080073655 0.0010098397033289075\n",
            "0.002951352497438483 0.045713789983756935 1.0\n",
            "repr, std, cov, closslb 0.02024664729833603 0.476318359375 0.0002492761705070734 9.573017450748011e-05 0.030228007584810257\n",
            "0.002957258153785857 0.04589691960933272 1.0\n",
            "repr, std, cov, closslb 0.028719361871480942 0.475341796875 0.00025318004190921783 0.03090757131576538 0.016434192657470703\n",
            "0.002924922650307893 0.04530441547424672 1.0\n",
            "repr, std, cov, closslb 0.019302356988191605 0.47607421875 0.00024336529895663261 8.265800715889782e-06 0.016700711101293564\n",
            "0.0028929407124406387 0.04440777172246462 1.0\n",
            "repr, std, cov, closslb 0.03267267346382141 0.475341796875 0.000247476389631629 1.9715927919605747e-05 0.00025945750530809164\n",
            "0.0028900506617788604 0.044319089224925556 1.0\n",
            "repr, std, cov, closslb 0.01787734031677246 0.473876953125 0.00033700396306812763 4.6324505092343315e-05 0.007281570229679346\n",
            "0.0028470447515072067 0.04326861235412824 1.0\n",
            "repr, std, cov, closslb 0.02953249216079712 0.4755859375 0.00025198981165885925 0.04599839448928833 0.0008476784569211304\n",
            "0.0028670339524538096 0.04370325076655621 1.0\n",
            "repr, std, cov, closslb 0.023969795554876328 0.476318359375 0.00023519317619502544 0.015399880707263947 0.01844755932688713\n",
            "0.002869900986406263 0.043746954017322766 1.0\n",
            "repr, std, cov, closslb 0.01923249289393425 0.474853515625 0.00028352229855954647 0.00018217755132354796 0.0008172218804247677\n",
            "0.002872770887392669 0.04335519284744884 1.0\n",
            "repr, std, cov, closslb 0.026164747774600983 0.475341796875 0.0002789953723549843 0.015345102176070213 0.00010651713819243014\n",
            "0.0028356849861072867 0.04241226026328923 1.0\n",
            "repr, std, cov, closslb 0.025689011439681053 0.473876953125 0.00032618758268654346 0.015472057275474072 0.000383161474019289\n",
            "0.0027879094670489927 0.04140698016335964 1.0\n",
            "repr, std, cov, closslb 0.02640582248568535 0.4755859375 0.00023088161833584309 5.563309969147667e-06 8.38089399621822e-05\n",
            "0.0027684719286445096 0.04058747262408353 1.0\n",
            "repr, std, cov, closslb 0.02489086054265499 0.47509765625 0.000274227699264884 3.836701580439694e-06 0.015888744965195656\n",
            "0.0027740116409737266 0.040668688156804314 1.0\n",
            "repr, std, cov, closslb 0.01850743591785431 0.475830078125 0.00026880158111453056 2.7005193260265514e-05 0.029649045318365097\n",
            "0.002762943279142945 0.04030449336410541 1.0\n",
            "repr, std, cov, closslb 0.026912711560726166 0.47607421875 0.0002198978327214718 0.045941777527332306 0.00017998903058469296\n",
            "0.0027906973765160414 0.04107721001813263 1.0\n",
            "repr, std, cov, closslb 0.019617676734924316 0.47607421875 0.0002485720906406641 0.00024199698236770928 0.015360638499259949\n",
            "0.002818730267133577 0.041990461034513676 1.0\n",
            "repr, std, cov, closslb 0.036599233746528625 0.47607421875 0.00023104832507669926 0.015372580848634243 0.014937239699065685\n",
            "0.0028356849861072867 0.043009906903261605 1.0\n",
            "repr, std, cov, closslb 0.019359620288014412 0.476806640625 0.00021626846864819527 0.015376845374703407 0.001645057462155819\n",
            "0.002841359191764487 0.04365959117538084 1.0\n",
            "repr, std, cov, closslb 0.02633390948176384 0.4765625 0.00019820453599095345 0.015466184355318546 0.001633973908610642\n",
            "0.0028670339524538096 0.044764279803798775 1.0\n",
            "repr, std, cov, closslb 0.02562328241765499 0.476806640625 0.00020226999185979366 1.3649069842358585e-05 0.0007619720418006182\n",
            "0.002878519301938341 0.04580526327751443 1.0\n",
            "repr, std, cov, closslb 0.022091008722782135 0.47705078125 0.0002009307499974966 5.001964746043086e-05 0.013006087392568588\n",
            "0.002869900986406263 0.04594281652894205 1.0\n",
            "repr, std, cov, closslb 0.024571234360337257 0.47705078125 0.00022201449610292912 0.0002264968352392316 0.00039303459925577044\n",
            "0.002881397821240279 0.046497172683377955 1.0\n",
            "repr, std, cov, closslb 0.021895524114370346 0.47705078125 0.00019723037257790565 0.0001795310527086258 0.0001616023073438555\n",
            "0.0028641697826711386 0.04673012398372591 1.0\n",
            "repr, std, cov, closslb 0.023240653797984123 0.476318359375 0.0002476498484611511 0.00014817246119491756 0.0024038038682192564\n",
            "0.002844200550956251 0.04640431764377278 1.0\n",
            "repr, std, cov, closslb 0.01879708468914032 0.4765625 0.0002279581967741251 0.00017956027295440435 0.0006451750523410738\n",
            "0.002815914352780797 0.04566812186189505 1.0\n",
            "repr, std, cov, closslb 0.026288170367479324 0.47509765625 0.00025402591563761234 0.00011816350161097944 0.0005961165297776461\n",
            "0.0027879094670489927 0.044630255103093944 1.0\n",
            "repr, std, cov, closslb 0.04125654697418213 0.47412109375 0.00031912652775645256 1.8971643385157222e-06 0.0006389117916114628\n",
            "0.0027740116409737266 0.04352887392345983 1.0\n",
            "repr, std, cov, closslb 0.05962373688817024 0.475341796875 0.00025716423988342285 9.184027294395491e-05 0.00016981996304821223\n",
            "0.0027879094670489927 0.04266737085765453 1.0\n",
            "repr, std, cov, closslb 0.07339582592248917 0.474853515625 0.000301779480651021 1.6180685634026304e-05 0.00011312132119201124\n",
            "0.0027879094670489927 0.04161442954825493 1.0\n",
            "repr, std, cov, closslb 0.08468262106180191 0.474609375 0.00028201332315802574 6.48809946142137e-05 0.0014881619717925787\n",
            "0.0028074834770916083 0.04087243869136015 1.0\n",
            "repr, std, cov, closslb 0.08056320250034332 0.474365234375 0.0003272588364779949 0.015386576764285564 0.00017551339988131076\n",
            "0.0028074834770916083 0.039863792546801315 1.0\n",
            "repr, std, cov, closslb 0.06721353530883789 0.47509765625 0.0002688816748559475 0.00012267514830455184 0.0006400358979590237\n",
            "0.002844200550956251 0.03950680572670036 1.0\n",
            "repr, std, cov, closslb 0.0512232631444931 0.4755859375 0.0002615423873066902 3.3143351174658164e-05 0.04477984830737114\n",
            "0.002872770887392669 0.03942791047783423 1.0\n",
            "25\n",
            "repr, std, cov, closslb 0.06054246053099632 0.476318359375 0.00022020540200173855 2.4897297407733276e-05 0.01522276271134615\n",
            "0.0029132521505369213 0.039744439953875395 1.0\n",
            "repr, std, cov, closslb 0.07518620043992996 0.475341796875 0.0002670574467629194 0.015456469729542732 0.031401459127664566\n",
            "0.0029484040933451384 0.04046595332578028 1.0\n",
            "repr, std, cov, closslb 0.07200393080711365 0.47607421875 0.00024267681874334812 0.015306999906897545 0.01609368436038494\n",
            "0.0029869641674333818 0.04091331113005151 1.0\n",
            "repr, std, cov, closslb 0.06360026448965073 0.477294921875 0.00019639777019619942 0.0002704432117752731 6.598715845029801e-05\n",
            "0.003026028541226352 0.04173939772180277 1.0\n",
            "repr, std, cov, closslb 0.07194054126739502 0.4765625 0.00022152205929160118 0.015375730581581593 0.0160391703248024\n",
            "0.0030594817870443045 0.04271003822851218 1.0\n",
            "repr, std, cov, closslb 0.04545880854129791 0.477783203125 0.00015900633297860622 0.0001547519932501018 0.0041101290844380856\n",
            "0.003080962515860149 0.043790700971340094 1.0\n",
            "repr, std, cov, closslb 0.045694172382354736 0.477783203125 0.00017493427731096745 3.222301984351361e-06 0.00017539520922582597\n",
            "0.0031025940616255868 0.04489870698081386 1.0\n",
            "repr, std, cov, closslb 0.05472368001937866 0.47705078125 0.00019716075621545315 0.015858981758356094 0.00021365868451539427\n",
            "0.003115023065849936 0.045805263277514434 1.0\n",
            "repr, std, cov, closslb 0.0398721881210804 0.47607421875 0.0002471024636179209 2.581839362392202e-05 0.015452337451279163\n",
            "0.003080962515860149 0.04544046467922029 1.0\n",
            "repr, std, cov, closslb 0.043437499552965164 0.47802734375 0.0001679451670497656 0.015353378839790821 0.0003158031031489372\n",
            "0.0030503216680249044 0.04557692244009244 1.0\n",
            "repr, std, cov, closslb 0.04786410927772522 0.476806640625 0.00021552573889493942 0.030278006568551064 0.0010880469344556332\n",
            "0.0030625412688313485 0.04649717268337796 1.0\n",
            "repr, std, cov, closslb 0.0421188548207283 0.477294921875 0.00019464828073978424 0.000533985672518611 0.0012865790631622076\n",
            "0.0030230055356906616 0.04626538265339926 1.0\n",
            "repr, std, cov, closslb 0.04694967344403267 0.4755859375 0.00025612302124500275 0.00028788932831957936 8.540414273738861e-05\n",
            "0.002978021166891187 0.04534971988972097 1.0\n",
            "repr, std, cov, closslb 0.0488114207983017 0.47509765625 0.00026826211251318455 4.449646803550422e-05 0.00014088282478041947\n",
            "0.0029543038499359213 0.04489870698081386 1.0\n",
            "repr, std, cov, closslb 0.05863995850086212 0.47509765625 0.000291639007627964 0.015663165599107742 0.015371846966445446\n",
            "0.002951352497438483 0.044988549293482456 1.0\n",
            "repr, std, cov, closslb 0.0434359610080719 0.47607421875 0.0002304399386048317 2.1566989744314924e-05 0.01883111521601677\n",
            "0.0029366399021476404 0.044988549293482456 1.0\n",
            "repr, std, cov, closslb 0.03975122421979904 0.477294921875 0.00019930792041122913 3.862834091705736e-06 0.0158048365265131\n",
            "0.0029337061959516892 0.0455313910490434 1.0\n",
            "repr, std, cov, closslb 0.04508582502603531 0.475830078125 0.0002483425196260214 7.835471478756517e-05 0.0009079389274120331\n",
            "0.002942516118591837 0.0463579596840887 1.0\n",
            "repr, std, cov, closslb 0.04339418560266495 0.47607421875 0.00022129621356725693 0.00021159012976568192 0.00038521154783666134\n",
            "0.002916165402687458 0.046034748104816454 1.0\n",
            "repr, std, cov, closslb 0.046221472322940826 0.47509765625 0.00024767639115452766 0.01530472096055746 9.148397657554597e-05\n",
            "0.0028670339524538096 0.045123649951999315 1.0\n",
            "repr, std, cov, closslb 0.03237105906009674 0.4765625 0.00021740072406828403 0.00022340426221489906 0.0001430588454240933\n",
            "0.0028215489974007103 0.044274814410515055 1.0\n",
            "repr, std, cov, closslb 0.029144570231437683 0.4765625 0.00020207324996590614 0.00012943333422299474 0.0003346203302498907\n",
            "0.0028074834770916083 0.04405410291415383 1.0\n",
            "repr, std, cov, closslb 0.02834932506084442 0.47705078125 0.00024204677902162075 5.416145722847432e-05 0.0007669010083191097\n",
            "0.0027879094670489927 0.04383449167231143 1.0\n",
            "repr, std, cov, closslb 0.035103898495435715 0.4755859375 0.00024340837262570858 1.4553534128936008e-05 0.015623414888978004\n",
            "0.0027574256703765225 0.043572402797383294 1.0\n",
            "repr, std, cov, closslb 0.03010011464357376 0.476806640625 0.00020144623704254627 4.247375727572944e-06 0.00021527355420403183\n",
            "0.0027684719286445096 0.04396612669463786 1.0\n",
            "repr, std, cov, closslb 0.05856158211827278 0.47509765625 0.0002983568701893091 0.030870158225297928 0.014551229774951935\n",
            "0.002732732469884453 0.04309596972697503 1.0\n",
            "repr, std, cov, closslb 0.029359448701143265 0.474609375 0.0002957475371658802 7.332875247811899e-05 0.00024332472821697593\n",
            "0.0026866914911312357 0.04215867498942176 1.0\n",
            "repr, std, cov, closslb 0.02312411554157734 0.476806640625 0.00022524502128362656 0.00013754941755905747 0.00017150351777672768\n",
            "0.0026706276125888877 0.04186474117490073 1.0\n",
            "repr, std, cov, closslb 0.04007577523589134 0.475830078125 0.0002139112912118435 0.01531576830893755 0.0007675173692405224\n",
            "0.0026813261574900986 0.04249712719607607 1.0\n",
            "repr, std, cov, closslb 0.028276506811380386 0.476318359375 0.00021518953144550323 0.030423827469348907 0.0007216137601062655\n",
            "0.002675971538441677 0.04271003822851218 1.0\n",
            "repr, std, cov, closslb 0.025077346712350845 0.476318359375 0.00020843115635216236 8.962613355834037e-05 0.0004200509574729949\n",
            "0.002675971538441677 0.043139065696702 1.0\n",
            "repr, std, cov, closslb 0.03943444788455963 0.475830078125 0.00023842277005314827 0.03062370792031288 0.00036430719774216413\n",
            "0.0026893781826223665 0.043790700971340094 1.0\n",
            "repr, std, cov, closslb 0.02698078751564026 0.475341796875 0.0002565109170973301 3.372349965502508e-05 0.017265427857637405\n",
            "0.0026546597808739013 0.04288113481253844 1.0\n",
            "repr, std, cov, closslb 0.025706302374601364 0.47705078125 0.00017783092334866524 5.1042203267570585e-05 0.015185866504907608\n",
            "0.0026652943585773747 0.04352887392345984 1.0\n",
            "repr, std, cov, closslb 0.0362013541162014 0.474853515625 0.00026675895787775517 0.015288267284631729 0.015209823846817017\n",
            "0.0026308868658279625 0.04245467252355252 1.0\n",
            "26\n",
            "repr, std, cov, closslb 0.031987570226192474 0.4755859375 0.0002475534565746784 0.0001053062587743625 0.00367352575995028\n",
            "0.002612544102156188 0.04194851252199169 1.0\n",
            "repr, std, cov, closslb 0.02830246277153492 0.476318359375 0.00022505410015583038 0.0001294950779993087 0.00022173240722622722\n",
            "0.0026177718029046023 0.042327562810106224 1.0\n",
            "repr, std, cov, closslb 0.025621667504310608 0.4765625 0.00024328078143298626 0.00013378288713283837 0.00024367116566281766\n",
            "0.0026177718029046023 0.04245467252355252 1.0\n",
            "repr, std, cov, closslb 0.02895461767911911 0.47705078125 0.00019168341532349586 0.015285876579582691 0.0004310118092689663\n",
            "0.002628258607220742 0.04300990690326161 1.0\n",
            "repr, std, cov, closslb 0.024625301361083984 0.47705078125 0.00019170367158949375 7.152991747716442e-05 0.01818746328353882\n",
            "0.0026308868658279625 0.04344194658833658 1.0\n",
            "repr, std, cov, closslb 0.022467724978923798 0.477294921875 0.0001878943294286728 7.572104368591681e-05 7.505183748435229e-05\n",
            "0.002612544102156188 0.04296693996329832 1.0\n",
            "repr, std, cov, closslb 0.03557039797306061 0.476318359375 0.00022431183606386185 0.01521278079599142 0.00028335931710898876\n",
            "0.0026099341679882 0.04322538696716109 1.0\n",
            "repr, std, cov, closslb 0.02635275200009346 0.47509765625 0.0002586729824542999 9.02455358300358e-05 0.0007981692906469107\n",
            "0.00256852805271674 0.04245467252355252 1.0\n",
            "repr, std, cov, closslb 0.02146279811859131 0.47705078125 0.00022104661911725998 0.00011110580089734867 0.0005927836173214018\n",
            "0.002548071987927124 0.04173939772180277 1.0\n",
            "repr, std, cov, closslb 0.02500680834054947 0.4765625 0.00023549585603177547 0.015241524204611778 0.03223858028650284\n",
            "0.0025277788376854866 0.041614429548254936 1.0\n",
            "repr, std, cov, closslb 0.03317123278975487 0.474609375 0.0002711024135351181 0.0150851896032691 0.000853490550071001\n",
            "0.002490163777396829 0.04075006620180608 1.0\n",
            "repr, std, cov, closslb 0.022119617089629173 0.475341796875 0.00027190824039280415 4.71320963697508e-05 0.0011707788798958063\n",
            "0.002475274949035738 0.04022400512984061 1.0\n",
            "repr, std, cov, closslb 0.04034539684653282 0.475830078125 0.0002398500218987465 0.015149226412177086 0.00010570914309937507\n",
            "0.0024827082021829663 0.04002348705923882 1.0\n",
            "repr, std, cov, closslb 0.02768689952790737 0.476318359375 0.00019946834072470665 0.00022173486649990082 0.0007188048912212253\n",
            "0.002510154951825391 0.040587472624083534 1.0\n",
            "repr, std, cov, closslb 0.02443397045135498 0.477783203125 0.00017170468345284462 0.00039836461655795574 0.01734301634132862\n",
            "0.0025429834779876715 0.04136561454881084 1.0\n",
            "repr, std, cov, closslb 0.046641431748867035 0.4765625 0.00020058546215295792 0.013494518585503101 0.0029537344817072153\n",
            "0.002573667677350226 0.04199046103451368 1.0\n",
            "repr, std, cov, closslb 0.029835717752575874 0.47607421875 0.00021880818530917168 0.014813071116805077 0.016307946294546127\n",
            "0.002558279574505595 0.04194851252199169 1.0\n",
            "repr, std, cov, closslb 0.025008652359247208 0.476318359375 0.00020421762019395828 3.656524495454505e-05 0.040292851626873016\n",
            "0.0025379051298228964 0.041448387143523 1.0\n",
            "repr, std, cov, closslb 0.027246693149209023 0.475830078125 0.0002448027953505516 3.429747812333517e-05 0.030735664069652557\n",
            "0.0025076473045208708 0.0404255277979823 1.0\n",
            "repr, std, cov, closslb 0.0461307168006897 0.475341796875 0.0002364173997193575 0.014933040365576744 0.032115619629621506\n",
            "0.0024827082021829663 0.03970473521865674 1.0\n",
            "repr, std, cov, closslb 0.034661516547203064 0.47607421875 0.0002368898130953312 7.822172483429313e-05 0.0007417219458147883\n",
            "0.0024976417417105147 0.03994355999568746 1.0\n",
            "repr, std, cov, closslb 0.030902912840247154 0.47607421875 0.00022803223691880703 0.0001707551855361089 0.030689671635627747\n",
            "0.002522730853248138 0.04050641927910605 1.0\n",
            "repr, std, cov, closslb 0.035797011107206345 0.4765625 0.00018828315660357475 0.014086015522480011 0.024708565324544907\n",
            "0.0025506200599150508 0.041200564920894286 1.0\n",
            "repr, std, cov, closslb 0.03554971516132355 0.477294921875 0.0001671179197728634 4.13116104027722e-05 0.01620657369494438\n",
            "0.002578817586372603 0.042243034498075586 1.0\n",
            "repr, std, cov, closslb 0.037186525762081146 0.47705078125 0.00018722075037658215 0.015356892719864845 0.015403501689434052\n",
            "0.0026021199990289975 0.04296693996329832 1.0\n",
            "repr, std, cov, closslb 0.04148643836379051 0.4775390625 0.00016928277909755707 0.0303135197609663 0.0151624521240592\n",
            "0.0026308868658279625 0.04383449167231143 1.0\n",
            "repr, std, cov, closslb 0.042399127036333084 0.4775390625 0.0001777235884219408 0.015115778893232346 0.015405919402837753\n",
            "0.002673298240201476 0.044809044083602576 1.0\n",
            "repr, std, cov, closslb 0.04486582428216934 0.4775390625 0.00018479255959391594 0.014977296814322472 0.00023865237017162144\n",
            "0.0027191097030673916 0.045805263277514434 1.0\n",
            "repr, std, cov, closslb 0.04724004864692688 0.477783203125 0.00018562586046755314 0.0002775173052214086 0.00038776977453380823\n",
            "0.0027491699103864625 0.04659021352591739 1.0\n",
            "repr, std, cov, closslb 0.047449421137571335 0.478271484375 0.00016615120694041252 0.00013175004278309643 0.015816854313015938\n",
            "0.0027684719286445096 0.047199533696582384 1.0\n",
            "repr, std, cov, closslb 0.0519988127052784 0.47705078125 0.00019511464051902294 8.195973350666463e-05 0.00024645309895277023\n",
            "0.002751919080296849 0.04729397996350924 1.0\n",
            "repr, std, cov, closslb 0.062359292060136795 0.47705078125 0.0002079459372907877 0.04376897215843201 0.016088562086224556\n",
            "0.0027546709993771456 0.04762603265377858 1.0\n",
            "repr, std, cov, closslb 0.07912535965442657 0.47705078125 0.00022008107043802738 0.00032071606256067753 0.00045069216866977513\n",
            "0.0027409388682242473 0.0470582178214065 1.0\n",
            "repr, std, cov, closslb 0.07058693468570709 0.475830078125 0.00027984799817204475 0.0004222830757498741 0.015508254058659077\n",
            "0.0027109686614660415 0.045942816528942054 1.0\n",
            "repr, std, cov, closslb 0.06289702653884888 0.4755859375 0.0002719850745052099 0.00011174964311067015 0.015751073136925697\n",
            "0.002667959652935952 0.044809044083602576 1.0\n",
            "27\n",
            "repr, std, cov, closslb 0.07979341596364975 0.473876953125 0.00032187881879508495 0.028949666768312454 0.0155001450330019\n",
            "0.002652007773100801 0.04370325076655622 1.0\n",
            "repr, std, cov, closslb 0.07764938473701477 0.47412109375 0.00032159825786948204 0.0001823281345423311 6.458158895839006e-05\n",
            "0.0026308868658279625 0.042624746111542995 1.0\n",
            "repr, std, cov, closslb 0.07813061773777008 0.47412109375 0.00031537842005491257 0.014722822234034538 7.207873568404466e-05\n",
            "0.0026073268411470536 0.041614429548254936 1.0\n",
            "repr, std, cov, closslb 0.0775560513138771 0.474365234375 0.00033804168924689293 0.0003537766169756651 0.0014729449758306146\n",
            "0.002578817586372603 0.040587472624083534 1.0\n",
            "repr, std, cov, closslb 0.08095075190067291 0.4755859375 0.00025766436010599136 0.029094776138663292 0.00040244744741357863\n",
            "0.002586561778163297 0.04014367763090119 1.0\n",
            "repr, std, cov, closslb 0.08302997052669525 0.4755859375 0.000264591071754694 0.013731492683291435 0.00022445889771915972\n",
            "0.0025813964039589753 0.039546312532427054 1.0\n",
            "repr, std, cov, closslb 0.06602585315704346 0.476318359375 0.00024350755847990513 0.000185153738129884 0.015748711302876472\n",
            "0.0026099341679882 0.03966507014850824 1.0\n",
            "repr, std, cov, closslb 0.05761284381151199 0.4765625 0.00022214418277144432 0.0005272416165098548 0.0012273207539692521\n",
            "0.0026308868658279625 0.03994355999568746 1.0\n",
            "repr, std, cov, closslb 0.061274122446775436 0.475830078125 0.00023782765492796898 0.00041068263817578554 0.03078111819922924\n",
            "0.002646711702983132 0.040344797857469514 1.0\n",
            "repr, std, cov, closslb 0.07652074098587036 0.4765625 0.0001911385916173458 0.013838761486113071 0.00013129221042618155\n",
            "0.0026706276125888877 0.04128300725130099 1.0\n",
            "repr, std, cov, closslb 0.07998371124267578 0.476806640625 0.00024560163728892803 0.0008588302880525589 0.00020005049009341747\n",
            "0.0027028519942245343 0.042243034498075586 1.0\n",
            "repr, std, cov, closslb 0.08604744076728821 0.47705078125 0.00020753778517246246 0.0008734024595469236 0.003911007661372423\n",
            "0.002735465202354337 0.04305291681016487 1.0\n",
            "repr, std, cov, closslb 0.08493217825889587 0.476318359375 0.00025483383797109127 0.010591109283268452 0.0166705884039402\n",
            "0.0027218288127704585 0.04300990690326161 1.0\n",
            "repr, std, cov, closslb 0.07387009263038635 0.476806640625 0.0002138603013008833 0.020804990082979202 0.0002005293354159221\n",
            "0.002732732469884453 0.04374695401732277 1.0\n",
            "repr, std, cov, closslb 0.07636905461549759 0.476318359375 0.00024374481290578842 0.0010853169951587915 0.00017666822532191873\n",
            "0.002713679630127507 0.04296693996329832 1.0\n",
            "repr, std, cov, closslb 0.08272295445203781 0.475830078125 0.0002664916682988405 0.0036818014923483133 0.0001411339035257697\n",
            "0.0026893781826223665 0.041906605916075625 1.0\n",
            "repr, std, cov, closslb 0.07478548586368561 0.4755859375 0.0002930432092398405 0.0024610229302197695 0.0001809592649806291\n",
            "0.0026599717550954294 0.04095422444118156 1.0\n",
            "repr, std, cov, closslb 0.08296115696430206 0.475830078125 0.0002533902879804373 0.0028033065609633923 0.015378274954855442\n",
            "0.002673298240201476 0.04075006620180608 1.0\n",
            "repr, std, cov, closslb 0.09294924885034561 0.47705078125 0.00020283018238842487 0.00127870321739465 4.0513368730898947e-05\n",
            "0.0027109686614660415 0.04173939772180277 1.0\n",
            "repr, std, cov, closslb 0.10020020604133606 0.476806640625 0.00023020338267087936 1.8267659470438957e-05 0.016042174771428108\n",
            "0.0027795624382673145 0.04279550101500742 1.0\n",
            "repr, std, cov, closslb 0.0838126540184021 0.477294921875 0.00020241877064108849 0.000525056617334485 0.01546007115393877\n",
            "0.002815914352780797 0.04374695401732277 1.0\n",
            "repr, std, cov, closslb 0.06259778141975403 0.4765625 0.00023826537653803825 0.000617733399849385 0.0002633354743011296\n",
            "0.002841359191764487 0.04471956024355523 1.0\n",
            "repr, std, cov, closslb 0.053671251982450485 0.477783203125 0.00018867128528654575 0.0004806729848496616 0.015840815380215645\n",
            "0.0028670339524538096 0.04585106854079194 1.0\n",
            "repr, std, cov, closslb 0.05485022813081741 0.477783203125 0.00018644332885742188 5.724058428313583e-05 0.00025424876366741955\n",
            "0.002895833653153079 0.04701120661479171 1.0\n",
            "repr, std, cov, closslb 0.06391583383083344 0.4775390625 0.00019550626166164875 0.0011258794693276286 0.028132028877735138\n",
            "0.002881397821240279 0.047388615217416205 1.0\n",
            "repr, std, cov, closslb 0.0787309929728508 0.4775390625 0.0002075093798339367 0.002464035525918007 0.015160747803747654\n",
            "0.0029016282162930376 0.04815254631880005 1.0\n",
            "repr, std, cov, closslb 0.0644967183470726 0.477783203125 0.00018713390454649925 0.013014755211770535 0.0040864585898816586\n",
            "0.0029045298445093304 0.048782299125676616 1.0\n",
            "repr, std, cov, closslb 0.08601740002632141 0.476806640625 0.00023383740335702896 0.016359880566596985 0.0001475152384955436\n",
            "0.0028641697826711386 0.048345445612011484 1.0\n",
            "repr, std, cov, closslb 0.08059846609830856 0.47607421875 0.000258489279076457 0.0019101907964795828 0.0003128916141577065\n",
            "0.0028470447515072067 0.048345445612011484 1.0\n",
            "repr, std, cov, closslb 0.058059900999069214 0.4765625 0.00022728415206074715 0.0007618062081746757 0.015542152337729931\n",
            "0.002841359191764487 0.048879912506227086 1.0\n",
            "repr, std, cov, closslb 0.05901766195893288 0.476318359375 0.00022436631843447685 0.0012185241794213653 0.00010309166827937588\n",
            "0.0028018769213719436 0.048539117660563305 1.0\n",
            "repr, std, cov, closslb 0.053380146622657776 0.476318359375 0.00023762835189700127 5.099652480566874e-05 0.00015766658179927617\n",
            "0.002771240400573154 0.04858765677822386 1.0\n",
            "repr, std, cov, closslb 0.04953955486416817 0.474853515625 0.00029083737172186375 0.00044718716526404023 0.015235846862196922\n",
            "0.002732732469884453 0.04767365868643235 1.0\n",
            "repr, std, cov, closslb 0.059522517025470734 0.474365234375 0.0003041012678295374 0.006999067030847073 0.00043394480599090457\n",
            "0.0026920675608049886 0.046543669856061336 1.0\n",
            "repr, std, cov, closslb 0.09112797677516937 0.47412109375 0.00031523010693490505 0.00017637239943724126 0.00048630184028297663\n",
            "0.0026840074836475885 0.045395069609610685 1.0\n",
            "28\n",
            "repr, std, cov, closslb 0.1221742331981659 0.4736328125 0.00032401084899902344 0.00017398345516994596 0.016013257205486298\n",
            "0.0026840074836475885 0.044274814410515055 1.0\n",
            "repr, std, cov, closslb 0.13148930668830872 0.474365234375 0.0003048982471227646 0.012057027779519558 7.488261326216161e-05\n",
            "0.0026893781826223665 0.043268612354128246 1.0\n",
            "repr, std, cov, closslb 0.09201140701770782 0.475830078125 0.0002460998948663473 0.004711005371063948 0.015683908015489578\n",
            "0.0026840074836475885 0.04309596972697503 1.0\n",
            "repr, std, cov, closslb 0.06269979476928711 0.476318359375 0.00020995247177779675 0.005182185675948858 0.04485437273979187\n",
            "0.0026866914911312357 0.04339854804029629 1.0\n",
            "repr, std, cov, closslb 0.05802839994430542 0.475341796875 0.00026230746880173683 0.0037248916923999786 0.0006086508510634303\n",
            "0.0026974543879941587 0.04396612669463786 1.0\n",
            "repr, std, cov, closslb 0.05603397265076637 0.47607421875 0.00023281131871044636 0.0005780038191005588 0.00025339412968605757\n",
            "0.0027001518423821523 0.04414225517408504 1.0\n",
            "repr, std, cov, closslb 0.060939639806747437 0.476318359375 0.0002537178806960583 9.562891136738472e-06 0.029756411910057068\n",
            "0.002713679630127507 0.04445217949418709 1.0\n",
            "repr, std, cov, closslb 0.06524339318275452 0.475341796875 0.00030319648794829845 0.02958834543824196 0.01548910140991211\n",
            "0.0027001518423821523 0.04405410291415383 1.0\n",
            "repr, std, cov, closslb 0.05605057626962662 0.476318359375 0.00024216575548052788 9.713433428260032e-06 0.027658941224217415\n",
            "0.002716393309757634 0.04467488535819704 1.0\n",
            "repr, std, cov, closslb 0.050634391605854034 0.4775390625 0.0001989603042602539 2.5745372113306075e-05 0.029808657243847847\n",
            "0.002730002467417036 0.045485905143899504 1.0\n",
            "repr, std, cov, closslb 0.07325942814350128 0.4765625 0.00020605209283530712 0.000345731241395697 0.0006305925780907273\n",
            "0.0027684719286445096 0.0466368037394433 1.0\n",
            "repr, std, cov, closslb 0.07078960537910461 0.476806640625 0.00022123800590634346 0.0001456554455216974 0.00039175356505438685\n",
            "0.0028102909605686997 0.047199533696582384 1.0\n",
            "repr, std, cov, closslb 0.06853339076042175 0.476318359375 0.0002201632596552372 0.015925418585538864 0.0015311400638893247\n",
            "0.0028328521339733136 0.047483439836466246 1.0\n",
            "repr, std, cov, closslb 0.06706714630126953 0.474853515625 0.0002696842420846224 5.080859045847319e-05 0.00011340670607751235\n",
            "0.002824370546398111 0.04677685410770964 1.0\n",
            "repr, std, cov, closslb 0.0884658619761467 0.474609375 0.0002870485186576843 0.00021163871861062944 0.02882610633969307\n",
            "0.0028271949169445085 0.04585106854079194 1.0\n",
            "repr, std, cov, closslb 0.07152396440505981 0.4755859375 0.00026741763576865196 1.654114566917997e-05 0.0006800760165788233\n",
            "0.0027962815619664494 0.04471956024355523 1.0\n",
            "repr, std, cov, closslb 0.07735367119312286 0.47412109375 0.0002805872354656458 0.003700783709064126 0.029718996956944466\n",
            "0.0027546709993771456 0.04361597520018067 1.0\n",
            "repr, std, cov, closslb 0.05771096795797348 0.4755859375 0.00027327844873070717 0.0002587964409030974 0.00033457655808888376\n",
            "0.0027272751922248115 0.04292401594735097 1.0\n",
            "repr, std, cov, closslb 0.05401002988219261 0.47607421875 0.00023249327205121517 0.0007247936446219683 0.012364033609628677\n",
            "0.002708260401064977 0.04249712719607607 1.0\n",
            "repr, std, cov, closslb 0.054067470133304596 0.4755859375 0.00026985304430127144 0.00493299774825573 0.015192859806120396\n",
            "0.0027028519942245343 0.04199046103451368 1.0\n",
            "repr, std, cov, closslb 0.055528681725263596 0.474853515625 0.00031716865487396717 0.0002910851326305419 0.015165913850069046\n",
            "0.0026652943585773747 0.04107721001813264 1.0\n",
            "repr, std, cov, closslb 0.05989271402359009 0.4755859375 0.00027103954926133156 0.00019239768153056502 0.00033800932578742504\n",
            "0.0026599717550954294 0.04046595332578028 1.0\n",
            "repr, std, cov, closslb 0.07462210953235626 0.475341796875 0.0002623717300593853 0.00020948259043507278 0.0001761250023264438\n",
            "0.002657314440654775 0.03966507014850824 1.0\n",
            "repr, std, cov, closslb 0.07420673221349716 0.474609375 0.0003126745577901602 0.0019765158649533987 0.00015789855387993157\n",
            "0.002667959652935952 0.03942791047783423 1.0\n",
            "repr, std, cov, closslb 0.05973847582936287 0.47705078125 0.00019795610569417477 0.0003808520850725472 0.02853350341320038\n",
            "0.0026974543879941587 0.03994355999568746 1.0\n",
            "repr, std, cov, closslb 0.059722594916820526 0.47607421875 0.00020776502788066864 0.0009402952273376286 0.015282126143574715\n",
            "0.0027245506415832284 0.04091331113005151 1.0\n",
            "repr, std, cov, closslb 0.08265750855207443 0.4765625 0.00020189350470900536 0.026779241859912872 0.00028946297243237495\n",
            "0.0027574256703765225 0.041906605916075625 1.0\n",
            "repr, std, cov, closslb 0.06353219598531723 0.478271484375 0.00016574631445109844 7.470804121112451e-05 0.00018323711992707103\n",
            "0.0027879094670489927 0.04296693996329832 1.0\n",
            "repr, std, cov, closslb 0.0837537944316864 0.477294921875 0.00017644232138991356 0.0016817688010632992 0.012603729963302612\n",
            "0.0028300221118614526 0.04405410291415383 1.0\n",
            "repr, std, cov, closslb 0.0912601500749588 0.478271484375 0.0001611716579645872 9.094184497371316e-05 0.00020492097246460617\n",
            "0.0028871634982805803 0.045123649951999315 1.0\n",
            "repr, std, cov, closslb 0.10106301307678223 0.4775390625 0.00017464300617575645 0.01345028355717659 0.01610589027404785\n",
            "0.0029366399021476404 0.04598875934547099 1.0\n",
            "repr, std, cov, closslb 0.08552711457014084 0.4765625 0.0002256873995065689 3.1480078177992254e-05 0.0002456011134199798\n",
            "0.002916165402687458 0.04571378998375694 1.0\n",
            "repr, std, cov, closslb 0.06807755678892136 0.47705078125 0.0002122507430613041 4.542747774394229e-05 0.03019016608595848\n",
            "0.0028555944297430267 0.04489870698081386 1.0\n",
            "repr, std, cov, closslb 0.07992547750473022 0.474609375 0.0002984805032610893 0.010354524478316307 0.015494847670197487\n",
            "0.0027934880738925572 0.04396612669463786 1.0\n",
            "repr, std, cov, closslb 0.07179610431194305 0.4755859375 0.00023787515237927437 0.004606966394931078 0.015356660820543766\n",
            "0.002743679807092471 0.04296693996329832 1.0\n",
            "29\n",
            "repr, std, cov, closslb 0.07856586575508118 0.474609375 0.00030345749109983444 0.007746343035250902 9.372028580401093e-05\n",
            "0.0027109686614660415 0.04215867498942176 1.0\n",
            "repr, std, cov, closslb 0.0865156427025795 0.47607421875 0.00023739668540656567 3.1227231374941766e-05 0.00014314982399810106\n",
            "0.002716393309757634 0.041200564920894286 1.0\n",
            "repr, std, cov, closslb 0.12037239968776703 0.474853515625 0.0003070174716413021 0.00012196004536235705 0.011524621397256851\n",
            "0.002730002467417036 0.0404255277979823 1.0\n",
            "repr, std, cov, closslb 0.11651633679866791 0.475341796875 0.0002508931793272495 5.729937765863724e-05 0.027267783880233765\n",
            "0.002762943279142945 0.04002348705923882 1.0\n",
            "repr, std, cov, closslb 0.0810643881559372 0.473388671875 0.00039684795774519444 8.54565150802955e-05 0.013896046206355095\n",
            "0.002730002467417036 0.039192168798082294 1.0\n",
            "repr, std, cov, closslb 0.08945968747138977 0.474853515625 0.00026880274526774883 0.023073911666870117 0.015062481164932251\n",
            "0.0027191097030673916 0.03903579126210412 1.0\n",
            "repr, std, cov, closslb 0.04739795997738838 0.476318359375 0.00024319416843354702 0.0017094772774726152 0.00041515426710247993\n",
            "0.002716393309757634 0.03903579126210412 1.0\n",
            "repr, std, cov, closslb 0.050040483474731445 0.4755859375 0.00026401784271001816 0.006772012449800968 0.00045081088319420815\n",
            "0.002708260401064977 0.03927059232784725 1.0\n",
            "repr, std, cov, closslb 0.04176546633243561 0.477294921875 0.00017853942699730396 8.188475476345047e-05 0.0161301176995039\n",
            "0.002716393309757634 0.03994355999568746 1.0\n",
            "repr, std, cov, closslb 0.042945265769958496 0.4775390625 0.0001704646274447441 0.010625194758176804 0.000210450409213081\n",
            "0.0027245506415832284 0.04079081626800788 1.0\n",
            "repr, std, cov, closslb 0.05020438879728317 0.4765625 0.00020279479213058949 0.0043517728336155415 0.015691494569182396\n",
            "0.0027409388682242473 0.04182291825664409 1.0\n",
            "repr, std, cov, closslb 0.04734329879283905 0.4775390625 0.00017287395894527435 0.00016984654939733446 0.00022125391114968807\n",
            "0.0027684719286445096 0.04288113481253844 1.0\n",
            "repr, std, cov, closslb 0.049293845891952515 0.477783203125 0.00017493660561740398 0.00010820203897310421 0.015263094566762447\n",
            "0.0028018769213719436 0.04396612669463786 1.0\n",
            "repr, std, cov, closslb 0.060087092220783234 0.47705078125 0.0001852510031312704 0.02118075080215931 0.015548544935882092\n",
            "0.0028385206710933937 0.0450785713806187 1.0\n",
            "repr, std, cov, closslb 0.055007465183734894 0.478271484375 0.00015622260980308056 2.7456342650111765e-05 0.00046985410153865814\n",
            "0.0028584500241727695 0.04608078285292127 1.0\n",
            "repr, std, cov, closslb 0.06111409515142441 0.47802734375 0.00015550456009805202 0.00020459434017539024 0.005354984197765589\n",
            "0.0028900506617788604 0.04687045459277916 1.0\n",
            "repr, std, cov, closslb 0.06520374864339828 0.476318359375 0.00021358439698815346 0.0074146403931081295 0.0004200364928692579\n",
            "0.0029045298445093304 0.047530923276302706 1.0\n",
            "repr, std, cov, closslb 0.07217975705862045 0.4755859375 0.0002610872033983469 0.007384380325675011 0.004603540524840355\n",
            "0.0028929407124406387 0.04724673323027896 1.0\n",
            "repr, std, cov, closslb 0.08717458695173264 0.474853515625 0.0002736307214945555 0.007477236911654472 0.00012527257786132395\n",
            "0.0028756436582800613 0.04612686363577419 1.0\n",
            "repr, std, cov, closslb 0.07005982846021652 0.475341796875 0.0002693857531994581 7.659051334485412e-05 0.00034218034124933183\n",
            "0.002841359191764487 0.044988549293482456 1.0\n",
            "repr, std, cov, closslb 0.08250340819358826 0.473876953125 0.00033897487446665764 0.009321714751422405 0.0003761167172342539\n",
            "0.0028215489974007103 0.04387832616398374 1.0\n",
            "repr, std, cov, closslb 0.0790376365184784 0.472412109375 0.00038953498005867004 0.004487122409045696 0.0005250656977295876\n",
            "0.0028102909605686997 0.042838296516022424 1.0\n",
            "repr, std, cov, closslb 0.07582572102546692 0.473876953125 0.0003165039233863354 4.743792669614777e-05 0.00020964861323591322\n",
            "0.0027934880738925572 0.04194851252199169 1.0\n",
            "repr, std, cov, closslb 0.08049018681049347 0.474365234375 0.0002881796099245548 3.358268440933898e-05 0.00045476254308596253\n",
            "0.002785124342706287 0.04107721001813264 1.0\n",
            "repr, std, cov, closslb 0.08300011605024338 0.4736328125 0.00031851744279265404 0.019510040059685707 0.015848953276872635\n",
            "0.002762943279142945 0.04014367763090119 1.0\n",
            "repr, std, cov, closslb 0.06941233575344086 0.47412109375 0.0002711811102926731 5.560448335018009e-05 0.008273793384432793\n",
            "0.002730002467417036 0.03938852195587835 1.0\n",
            "repr, std, cov, closslb 0.06547854840755463 0.474365234375 0.00032954546622931957 2.829852564900648e-05 0.0012335192877799273\n",
            "0.0027409388682242473 0.03930986292017509 1.0\n",
            "repr, std, cov, closslb 0.07612118124961853 0.4765625 0.00020591448992490768 1.3684063560504e-05 6.56396005069837e-05\n",
            "0.002765706222422088 0.03966507014850824 1.0\n",
            "repr, std, cov, closslb 0.098911352455616 0.476318359375 0.00021199206821620464 0.003864207537844777 0.004940707702189684\n",
            "0.002813101251529268 0.039903656339348115 1.0\n",
            "repr, std, cov, closslb 0.10947731137275696 0.476318359375 0.00025124172680079937 0.002198317553848028 0.010905052535235882\n",
            "0.0028584500241727695 0.040587472624083534 1.0\n",
            "repr, std, cov, closslb 0.11723074316978455 0.476806640625 0.0002055596560239792 9.372854401590303e-05 0.0005402208189480007\n",
            "0.002924922650307893 0.04148983553066652 1.0\n",
            "repr, std, cov, closslb 0.09991146624088287 0.4765625 0.0002031950280070305 0.00999477133154869 0.028442680835723877\n",
            "0.0029366399021476404 0.04107721001813264 1.0\n",
            "repr, std, cov, closslb 0.10529348254203796 0.475830078125 0.0002915463410317898 0.008789852261543274 0.015365436673164368\n",
            "0.002975046120770417 0.04091331113005151 1.0\n",
            "repr, std, cov, closslb 0.12504521012306213 0.4755859375 0.00023442436940968037 0.00010028944234363735 0.0003300613316241652\n",
            "0.0030230055356906616 0.041200564920894286 1.0\n",
            "repr, std, cov, closslb 0.1211303174495697 0.476806640625 0.0002121548168361187 0.0002758569025900215 0.004633366130292416\n",
            "0.0030871275218543846 0.04203245149554819 1.0\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "for i in range(30):\n",
        "    print(i)\n",
        "    # agent.train_ae(train_loader, optim)\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "# 10 epochs 15m23s\n",
        "\n",
        "\n",
        "\n",
        "# loss 0.00027325598057359457\n",
        "# loss 0.00027538512949831784\n",
        "# loss 0.000279315427178517\n",
        "# loss 0.00028544830274768174\n",
        "# loss 0.00029633755912072957\n",
        "# loss 0.0002964686427731067\n",
        "# loss 0.00030574199627153575\n",
        "# loss 0.00031030713580548763\n",
        "# loss 0.00011697990703396499\n",
        "# loss 0.00012466282350942492\n",
        "\n",
        "# loss 0.0002805441035889089\n",
        "# loss 0.0002813159371726215\n",
        "# loss 0.00028616547933779657\n",
        "# loss 0.00029815093148499727\n",
        "# loss 0.0003055527340620756\n",
        "# loss 0.0002878434315789491\n",
        "# loss 0.0002965773455798626\n",
        "# loss 0.00030164531199261546\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "5c8ba867-ddb8-4d06-da87-3491a7ecf4d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PraFUAPB3j7v",
        "outputId": "1819d64c-5873-4310-da1e-7b1ae8688d03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-555661cfa97e>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# buffer = simulate(agent, buffer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-555661cfa97e>\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(agent, buffer)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mact_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# while not done:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "def simulate(agent, buffer=[]):\n",
        "    agent.eval()\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # out = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    h0 = torch.randn((self.jepa.pred.num_layers, batch_size, self.d_model), device=device)\n",
        "    while True:\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        if len(act)<=0: act, h0 = agent(state, h0).cpu()[0,:4].tolist()\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9cm6KjvBrnNO"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(30):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "    c_loader = make_weighted(buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    # train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# print(optim.param_groups[0][\"lr\"])\n",
        "optim.param_groups[0][\"lr\"] = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b8zxYU9jpE8K"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "a6d68be2-0763-4c63-85d2-6c30d0d227fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  self.hub = sentry_sdk.Hub(client)\n",
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240831_000722-ljkc95dp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/ljkc95dp' target=\"_blank\">crisp-elevator-25</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/ljkc95dp' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/ljkc95dp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mhkK_9AQm8_q",
        "wUhKd009Qvk3"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}