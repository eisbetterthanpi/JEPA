{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkMHE3f6YjnGqGOSuw5+b9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkwnVjJTrW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c521c47e-763f-41cd-9170-2412075f79a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\")\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "fffd2e13-8231-4409-e988-ef895e0f895d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "mask = vecsearch(query, index, k=5, treshold=37)\n",
        "print(mask)\n",
        "rag = ltmk[mask]\n",
        "print(rag)\n",
        "\n",
        "\n",
        "removing = torch.tensor([998, 769, 643])\n",
        "index.remove_ids(removing)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1dc759-d4f4-439c-cd46-0e1156f6d195"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "tensor([], size=(0, 256))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        if ltmk is None: ltmk = torch.tensor([])\n",
        "        if ltmv is None: ltmv = torch.tensor([])\n",
        "        self.ltmk = ltmk # [len_ltm, d_model]\n",
        "        self.ltmv = ltmv\n",
        "        self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def add(self, k, v):\n",
        "        self.ltmk.append(k)\n",
        "        self.ltmv.append(v)\n",
        "        self.index.add(k)\n",
        "        if torch.rand(1)<0.1: self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        attn = q @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pickle save/load\n",
        "import pickle\n",
        "# # save\n",
        "# with open('semantic_wiki.pkl', 'wb') as f: pickle.dump(semantic_wiki, f)\n",
        "# # load\n",
        "# with open('semantic_wiki.pkl', 'rb') as f: semantic_wiki = pickle.load(f)\n",
        "\n",
        "# def save_mem():\n",
        "#     for name in [semantic_wiki, ]\n",
        "#     with open(name+'.pkl', 'wb') as f: pickle.dump(name, f)\n"
      ],
      "metadata": {
        "id": "4-4GHdQZM_eK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n"
      ],
      "metadata": {
        "id": "9OFjAK232GNp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "batch_size=3\n",
        "L=5\n",
        "d_model=8\n",
        "n_heads=2\n",
        "\n",
        "\n",
        "\n",
        "trg = torch.rand(batch_size,L, d_model)\n",
        "src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "mha = MultiHeadAttention(d_model, n_heads)\n",
        "x, attn = mha(trg,src,src)\n",
        "\n",
        "head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "print(x.shape)\n",
        "print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S87jVbpM21X7",
        "outputId": "0c45bb84-0cd7-4f78-fd8d-0a79ba8f84ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 5, 8])\n",
            "torch.Size([3, 2, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ],
      "metadata": {
        "id": "hEUffQ24mkRY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "# model = get_res(10).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, ema=False):\n",
        "        super().__init__()\n",
        "        dim_class=10\n",
        "        dim_embd=256\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ],
      "metadata": {
        "id": "V15LtR8myLL9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = nn.Sequential(nn.Linear(in_dim, d_model),)\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        # self.pred = gru(emb_dim, rnn_units, num_layers)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v),# nn.ReLU(True),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=10.0 # 25.0 # λ\n",
        "        self.std_coeff=10.0 # 25.0 # µ\n",
        "        self.cov_coeff=1.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        loss = self.std_coeff * std_loss + self.cov_coeff * cov_loss\n",
        "        return loss\n",
        "\n",
        "    def argm(self, sx, a, sy):\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.rand((batch,self.dim_z),device=device), requires_grad=True)\n",
        "        optim = torch.optim.SGD([z], lr=1e-1, momentum=0.9)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        # x, y = x.detach(), y.detach().unsqueeze(-1).float()\n",
        "        num_steps = 10\n",
        "        for i in range(num_steps):\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            sy_ = self.pred(sxaz)\n",
        "            # print(\"y_, y\",y_.shape, y.shape)\n",
        "            loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "        return z#.detach()\n",
        "\n",
        "    def loss(self, x, y, a, z=None):\n",
        "        sx, sy = self.enc(x), self.enc(y)\n",
        "        z = self.argm(sx, a, sy)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "        # v_c_loss = self.v_creg(self.exp(sx))\n",
        "        vx, vy = self.exp(sx), self.exp(sy)\n",
        "        v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "        return repr_loss + v_c_loss\n",
        "\n",
        "    def forward(self, sx, a): # state, ctrl\n",
        "        batch=sx.size(dim=0)\n",
        "        z=torch.zeros((batch,self.dim_z),device=self.device)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        return sy_ # state1\n",
        "\n",
        "\n",
        "\n",
        "d_model=16\n",
        "dim_z= 1#-5\n",
        "dim_v=32\n",
        "dim_a=4\n",
        "model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ],
      "metadata": {
        "id": "FuA25qQknUAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDtHEU4tCo5z",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title agent\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.sense = VICReg()\n",
        "        self.ltm = Ltm()\n",
        "        # self.ltm.load()\n",
        "        self.stm = torch.rand(d_model, d_model)\n",
        "        self.world_state = torch.rand(d_model, d_model) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(in_dim, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, state): # live run in env\n",
        "        current = self.sense(state)\n",
        "        ltm = self.ltm(current)\n",
        "        stm = self.stm @ current\n",
        "        ltm, stm = F.normalize(ltm, dim=-1), F.normalize(stm, dim=-1)\n",
        "        # a=0.5\n",
        "        # obs = torch.cat([current, a*stm + (1-a)*ltm], dim=-1)\n",
        "        obs = current + stm + ltm\n",
        "        # Q = self.q(obs)\n",
        "        K, V = self.k(obs), self.v(obs)\n",
        "        V_ = self.world_state @ K\n",
        "        self.world_state = self.world_state + (V - V_) @ K.T # -V_.K^T, + V.K^T # update world state\n",
        "\n",
        "        sx = self.jepa.enc(self.world_state)\n",
        "\n",
        "        # self.ltm.add(self, k, v)\n",
        "\n",
        "    def plan(self, ): # mpc\n",
        "        # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "        xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "    def loss(self, lx): # offline\n",
        "        self.sense.loss(lx) #\n",
        "        self.jepa.loss(lx, ly, la)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RCD647ZpPrGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT9m-J1BUWyz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"vicreg\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA_rcOQQTxan",
        "outputId": "6fcb236c-593b-4b40-b21c-5d41fc1db64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "b8zxYU9jpE8K",
        "outputId": "4af52095-9072-4e37-e0e9-e6caa5569166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rnn train\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jw28GX1HzTeb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ],
      "metadata": {
        "id": "fsealXK3OPQa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}