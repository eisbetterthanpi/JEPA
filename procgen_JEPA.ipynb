{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "ff88a280-b1c2-49f9-d436-a1a38df1617d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install -qq procgen faiss-cpu\n",
        "!pip install -qq procgen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "cellView": "form",
        "id": "nEY9MmwZhA8a"
      },
      "outputs": [],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# conv = Conv().to(device)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# # input = torch.rand((4,1,256,256), device=device)\n",
        "# out = conv(input)\n",
        "# print(out.shape)\n",
        "\n",
        "# conv = Deconv(256).to(device)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# input = torch.rand((4,256), device=device)\n",
        "# out = conv(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self._levels = levels\n",
        "        self._eps = eps\n",
        "        self._levels_tensor = torch.tensor(levels)\n",
        "        self._basis = torch.cat([torch.ones(1), torch.cumprod(self._levels_tensor[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(self._levels)\n",
        "        self.codebook_size = torch.prod(self._levels_tensor).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size))\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self._levels_tensor - 1) * (1 - self._eps) / 2\n",
        "        offset = torch.where(self._levels_tensor % 2 == 1, 0.0, 0.5)\n",
        "        shift = torch.tan(offset / half_l)\n",
        "        return torch.tanh(z + shift) * half_l - offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        half_width = self._levels_tensor // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self._levels_tensor // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self._levels_tensor // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self._basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self._basis, self._levels_tensor)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# # z = np.asarray([0.25, 0.6, -1.4])\n",
        "# z = torch.tensor([0.25, 0.6, -1.4])\n",
        "\n",
        "# # print(fsq.codebook)\n",
        "\n",
        "# # x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "# x = torch.tensor([[[-0.57,0.57,-1.7]]])\n",
        "# # x = torch.tensor([[[0.55,0.55,-3.9]]])\n",
        "# # x = torch.tensor([[[0.1,0.1,-1.6]]])\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1))\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    xx = fsq(x.clone())\n",
        "    print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    loss = model(xx)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# model = nn.Sequential(nn.Linear(3,1))\n",
        "model = nn.Sequential(nn.Linear(3*2,1))\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 1\n",
        "seq_len = 3\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        a = la[:,t] # [1, dim_a]\n",
        "        sxaz = torch.cat([sx, a], dim=-1)\n",
        "        # with torch.cuda.amp.autocast():\n",
        "        cost = cost + model(sxaz)\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "    return cost, sx\n",
        "\n",
        "\n",
        "# def ste_clamp(input, min=-1, max=1):\n",
        "#     clamped_output = torch.clamp(input, min, max)\n",
        "#     clamp_mask = (input < min) | (input > max)\n",
        "#     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "def ste_clamp(x, min=-1, max=1):\n",
        "    return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "def ste_abs(x): return x.sign() * x\n",
        "def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "sx_ = sx.detach()\n",
        "for i in range(10): # num epochs\n",
        "    # la = fsq(x.clone())\n",
        "    la = fsq(x)\n",
        "    print(i)\n",
        "    print(x,x.requires_grad)\n",
        "    print(la,la.requires_grad)\n",
        "    loss, sx_ = rnn_pred(sx_, la)\n",
        "    # loss.backward()\n",
        "    loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.tanh(x)\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "    # x = symlog(x.clone())\n",
        "    # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ],
      "metadata": {
        "id": "7z_VgsenYLpM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pGZld_gLH1RA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "2d2f1a97-e330-424a-da29-637d2c4d3408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "cellView": "form",
        "id": "FuA25qQknUAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea36a80-f58c-4d9a-bf53-610b30dcd2f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-90-a5e320619cc7>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model)\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        # self.pred = gru(emb_dim, rnn_units, num_layers)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=1e5): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        num_steps = 5 # 10\n",
        "        for i in range(num_steps):\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                sy_ = self.pred(sxaz)\n",
        "                # print(\"y_, y\",y_.shape, y.shape)\n",
        "                loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z = torch.clamp(z, min=-1, max=1)\n",
        "            # print(\"argm in\",loss.item())\n",
        "        # print(z.squeeze())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCD647ZpPrGf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cIF--UQMEEFx"
      },
      "outputs": [],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(3): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "                sx_ = sx_.detach()\n",
        "                # print(loss.item(), lact)\n",
        "        # print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "29O1eyvhnRSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f32b6cc-a8fc-4327-87dc-e17fa382028a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-122-6b0870240141>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent pixel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_clamp(input, min=-1, max=1):\n",
        "    clamped_output = torch.clamp(input, min, max)\n",
        "    clamp_mask = (input < min) | (input > max)\n",
        "    return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la = self.search(sx, T=6) # 20\n",
        "        lact = fsq.codes_to_indexes(la)\n",
        "        return lact[0]\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        optim = torch.optim.SGD([x], lr=1e3) #, maximize=True)\n",
        "\n",
        "        for _ in range(10): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            # la = fsq(x.clone())\n",
        "            la = fsq(x)\n",
        "            # print(x,x.requires_grad)\n",
        "            # print(la,la.requires_grad)\n",
        "            loss, sx_ = self.rnn_pred(sx_, la)\n",
        "            # loss.backward()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            # x = torch.tanh(x) # clamp\n",
        "            # x = torch.clamp(x, min=-1, max=1)\n",
        "            # x = ste_clamp(x, min=-1, max=1)\n",
        "            print(x)\n",
        "        la = fsq(x)\n",
        "        print(\"search\",loss.item())\n",
        "        return la # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=32):\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    # z = self.jepa.argm(sx_, a, sy)\n",
        "                    # sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy).squeeze(-1)\n",
        "                    clossl = F.mse_loss(reward_, reward) # [batch_size]\n",
        "                    try:\n",
        "                        st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    # closs = F.mse_loss(reward_, reward) + F.mse_loss(stt, r)\n",
        "                    closs = clossl + clossb\n",
        "                    # print(loss, jloss, clossl, clossb)\n",
        "                    # print(loss.dtype, jloss.dtype, clossl.dtype, clossb.dtype)\n",
        "\n",
        "                    # loss = loss + jloss + conv_loss + closs\n",
        "                    loss = loss + jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # print(loss, loss.dtype)\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "!gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "69ab0a3d-4041-4b7e-a390-f3babceb08ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_\n",
            "From (redirected): https://drive.google.com/uc?id=12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_&confirm=t&uuid=7a45c8ca-269c-4a6a-845c-a79ddbc78173\n",
            "To: /content/agentoptim.pkl\n",
            "100% 44.2M/44.2M [00:01<00:00, 38.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "cellView": "form",
        "id": "ShHQ_ynlwoyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a9d502a-685f-4a6d-94e1-4a32b9d87244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-118-7fc540255cf2>:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "def save(folder, name='agent.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    # agent.mem.save(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer512.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# save(folder, name='agent_jepa753333256.pth')\n",
        "# buffer = load(folder)\n",
        "# save('/content/')\n",
        "# buffer = load('/content/')\n",
        "\n",
        "# name='agent.pth'\n",
        "# print(folder+name)\n",
        "# torch.load(folder+name, map_location='o')\n",
        "# with open(folder+'buffer512down.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "with open(folder+'buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "agent.load_state_dict(modelsd)\n",
        "optim.load_state_dict(optimsd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "NVcknabHMxH6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "1e3fpbtNOiz1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "train_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "train_data = Datasetme(train_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    train_data=list(zip(state,reward))\n",
        "    train_data = Datasetme(train_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# trainiter = iter(c_loader)\n",
        "# images, labels = next(trainiter)\n",
        "# # imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# # print(labels)\n",
        "# for x in range((len(labels)//10)+1):\n",
        "#     print(labels[10*x:10*x+10])\n",
        "\n",
        "# try:\n",
        "#     with torch.no_grad():\n",
        "#         # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "#         pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "#         # print(pred)\n",
        "#         for x in range((len(pred)//10)+1):\n",
        "#             print(pred[10*x:10*x+10])\n",
        "#         # print((labels==pred).sum())\n",
        "# except: pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OksdjCeJYpYh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    # agent.train_ae(train_loader, optim)\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "# 10 epochs 15m23s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "PraFUAPB3j7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7f70ba5-1def-4801-e379-81a2ecb2ee99",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: early reset ignored\n",
            "Parameter containing:\n",
            "tensor([[[ 0.2764, -0.0152, -0.2476],\n",
            "         [-0.3600, -0.3954,  0.1899],\n",
            "         [-0.2633, -0.3974, -0.0440],\n",
            "         [-0.0618, -0.0077,  0.0152],\n",
            "         [-0.0971, -0.0210, -0.0259],\n",
            "         [-0.0267,  0.4342, -0.5034]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.2504,  0.0493, -0.2463],\n",
            "         [-0.3887, -0.3622,  0.1854],\n",
            "         [-0.2811, -0.3771, -0.0482],\n",
            "         [-0.0738,  0.0207,  0.0124],\n",
            "         [-0.1055, -0.0012, -0.0280],\n",
            "         [-0.0322,  0.4454, -0.5067]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.2241,  0.1136, -0.2450],\n",
            "         [-0.4168, -0.3281,  0.1809],\n",
            "         [-0.2988, -0.3564, -0.0524],\n",
            "         [-0.0859,  0.0491,  0.0096],\n",
            "         [-0.1136,  0.0186, -0.0301],\n",
            "         [-0.0392,  0.4551, -0.5089]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.1975,  0.1773, -0.2437],\n",
            "         [-0.4444, -0.2933,  0.1763],\n",
            "         [-0.3162, -0.3355, -0.0567],\n",
            "         [-0.0979,  0.0774,  0.0068],\n",
            "         [-0.1224,  0.0383, -0.0322],\n",
            "         [-0.0447,  0.4661, -0.5123]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.1705,  0.2398, -0.2423],\n",
            "         [-0.4713, -0.2578,  0.1717],\n",
            "         [-0.3335, -0.3143, -0.0610],\n",
            "         [-0.1099,  0.1057,  0.0040],\n",
            "         [-0.1307,  0.0581, -0.0343],\n",
            "         [-0.0502,  0.4770, -0.5156]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.1433,  0.3007, -0.2410],\n",
            "         [-0.4976, -0.2215,  0.1671],\n",
            "         [-0.3506, -0.2928, -0.0653],\n",
            "         [-0.1216,  0.1337,  0.0011],\n",
            "         [-0.1411,  0.0755, -0.0357],\n",
            "         [-0.0557,  0.4877, -0.5190]]], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-122-6b0870240141>:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[[ 0.1159,  0.3597, -0.2397],\n",
            "         [-0.5233, -0.1846,  0.1624],\n",
            "         [-0.3675, -0.2710, -0.0697],\n",
            "         [-0.1335,  0.1616, -0.0017],\n",
            "         [-0.1494,  0.0953, -0.0379],\n",
            "         [-0.0612,  0.4984, -0.5224]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.0882,  0.4165, -0.2383],\n",
            "         [-0.5480, -0.1473,  0.1577],\n",
            "         [-0.3875, -0.2506, -0.0726],\n",
            "         [-0.1454,  0.1892, -0.0046],\n",
            "         [-0.1577,  0.1150, -0.0400],\n",
            "         [-0.0667,  0.5090, -0.5258]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.0604,  0.4709, -0.2370],\n",
            "         [-0.5725, -0.1095,  0.1530],\n",
            "         [-0.4040, -0.2283, -0.0770],\n",
            "         [-0.1573,  0.2166, -0.0075],\n",
            "         [-0.1659,  0.1346, -0.0422],\n",
            "         [-0.0722,  0.5195, -0.5293]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.0386,  0.5238, -0.2360],\n",
            "         [-0.6265,  0.0374,  0.1522],\n",
            "         [-0.4222, -0.2018, -0.0813],\n",
            "         [-0.1645,  0.2441, -0.0103],\n",
            "         [-0.1710,  0.1543, -0.0443],\n",
            "         [-0.0754,  0.5300, -0.5327]]], requires_grad=True)\n",
            "search -0.05303499847650528\n",
            "2 act:  13 reward:  0.0\n",
            "Parameter containing:\n",
            "tensor([[[-0.4187,  0.5125,  0.3652],\n",
            "         [-0.0895, -0.3124,  0.2838],\n",
            "         [ 0.5110,  0.0348,  0.4810],\n",
            "         [ 0.2394,  0.3541,  0.3941],\n",
            "         [-0.0078, -0.4405, -0.1675],\n",
            "         [ 0.4787,  0.4301, -0.0616]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.4227,  0.5797,  0.3654],\n",
            "         [-0.1152, -0.2742,  0.2790],\n",
            "         [ 0.4988,  0.0602,  0.4789],\n",
            "         [ 0.2305,  0.3823,  0.3923],\n",
            "         [-0.0144, -0.4220, -0.1712],\n",
            "         [ 0.4754,  0.4427, -0.0637]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.4404,  0.6703,  0.3629],\n",
            "         [-0.1470, -0.2317,  0.2751],\n",
            "         [ 0.4848,  0.0825,  0.4768],\n",
            "         [ 0.2201,  0.4081,  0.3905],\n",
            "         [-0.0219, -0.4046, -0.1747],\n",
            "         [ 0.4704,  0.4529, -0.0652]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.4579,  0.7523,  0.3603],\n",
            "         [-0.1785, -0.1882,  0.2712],\n",
            "         [ 0.4706,  0.1048,  0.4747],\n",
            "         [ 0.2096,  0.4334,  0.3887],\n",
            "         [-0.0296, -0.3868, -0.1783],\n",
            "         [ 0.4665,  0.4645, -0.0673]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.4751,  0.8264,  0.3578],\n",
            "         [-0.2097, -0.1440,  0.2673],\n",
            "         [ 0.4563,  0.1269,  0.4726],\n",
            "         [ 0.1994,  0.4582,  0.3869],\n",
            "         [-0.0393, -0.3709, -0.1810],\n",
            "         [ 0.4626,  0.4759, -0.0694]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.4921,  0.8937,  0.3552],\n",
            "         [-0.2406, -0.0992,  0.2634],\n",
            "         [ 0.4418,  0.1489,  0.4705],\n",
            "         [ 0.1889,  0.4825,  0.3850],\n",
            "         [-0.0469, -0.3527, -0.1846],\n",
            "         [ 0.4587,  0.4872, -0.0715]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.5088,  0.9549,  0.3526],\n",
            "         [-0.2710, -0.0539,  0.2594],\n",
            "         [ 0.4271,  0.1708,  0.4684],\n",
            "         [ 0.1783,  0.5063,  0.3832],\n",
            "         [-0.0544, -0.3343, -0.1883],\n",
            "         [ 0.4536,  0.4971, -0.0730]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.5252,  1.0109,  0.3500],\n",
            "         [-0.3010, -0.0082,  0.2554],\n",
            "         [ 0.4125,  0.1924,  0.4663],\n",
            "         [ 0.1649,  0.5268,  0.3819],\n",
            "         [-0.0620, -0.3157, -0.1920],\n",
            "         [ 0.4496,  0.5081, -0.0751]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.5414,  1.0624,  0.3474],\n",
            "         [-0.3305,  0.0376,  0.2514],\n",
            "         [ 0.3976,  0.2140,  0.4642],\n",
            "         [ 0.1542,  0.5497,  0.3800],\n",
            "         [-0.0697, -0.2968, -0.1957],\n",
            "         [ 0.4456,  0.5191, -0.0772]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.5574,  1.1099,  0.3448],\n",
            "         [-0.3595,  0.0833,  0.2473],\n",
            "         [ 0.3823,  0.2354,  0.4621],\n",
            "         [ 0.1435,  0.5720,  0.3782],\n",
            "         [-0.0773, -0.2777, -0.1994],\n",
            "         [ 0.4417,  0.5300, -0.0793]]], requires_grad=True)\n",
            "search -0.05413929373025894\n",
            "2 act:  15 reward:  0.0\n",
            "Parameter containing:\n",
            "tensor([[[-0.3287, -0.3161, -0.3700],\n",
            "         [ 0.0723,  0.3289,  0.4288],\n",
            "         [ 0.2828,  0.3421, -0.0361],\n",
            "         [ 0.2611, -0.1727, -0.0412],\n",
            "         [ 0.0893, -0.3264, -0.3744],\n",
            "         [-0.3970, -0.4027, -0.0719]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.3334, -0.2374, -0.3695],\n",
            "         [ 0.0463,  0.3668,  0.4251],\n",
            "         [ 0.2682,  0.3649, -0.0417],\n",
            "         [ 0.2522, -0.1417, -0.0454],\n",
            "         [ 0.0828, -0.3064, -0.3796],\n",
            "         [-0.4017, -0.3912, -0.0735]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.3381, -0.1554, -0.3689],\n",
            "         [ 0.0203,  0.4037,  0.4214],\n",
            "         [ 0.2535,  0.3874, -0.0473],\n",
            "         [ 0.2431, -0.1104, -0.0495],\n",
            "         [ 0.0761, -0.2860, -0.3848],\n",
            "         [-0.4053, -0.3782, -0.0756]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.3428, -0.0707, -0.3683],\n",
            "         [-0.0058,  0.4397,  0.4177],\n",
            "         [ 0.2390,  0.4094, -0.0530],\n",
            "         [ 0.2314, -0.0823, -0.0527],\n",
            "         [ 0.0694, -0.2654, -0.3901],\n",
            "         [-0.4088, -0.3651, -0.0778]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.3474,  0.0156, -0.3677],\n",
            "         [-0.0319,  0.4746,  0.4139],\n",
            "         [ 0.2240,  0.4311, -0.0588],\n",
            "         [ 0.2223, -0.0506, -0.0569],\n",
            "         [ 0.0627, -0.2446, -0.3954],\n",
            "         [-0.4123, -0.3518, -0.0799]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.3521,  0.1024, -0.3672],\n",
            "         [-0.0580,  0.5085,  0.4101],\n",
            "         [ 0.2090,  0.4524, -0.0646],\n",
            "         [ 0.2132, -0.0187, -0.0611],\n",
            "         [ 0.0560, -0.2235, -0.4007],\n",
            "         [-0.4159, -0.3384, -0.0821]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.3567,  0.1882, -0.3666],\n",
            "         [-0.0840,  0.5414,  0.4063],\n",
            "         [ 0.1938,  0.4734, -0.0705],\n",
            "         [ 0.2040,  0.0132, -0.0654],\n",
            "         [ 0.0493, -0.2022, -0.4061],\n",
            "         [-0.4194, -0.3249, -0.0843]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.3817,  0.2499, -0.3650],\n",
            "         [-0.1099,  0.5732,  0.4024],\n",
            "         [ 0.1786,  0.4940, -0.0764],\n",
            "         [ 0.1950,  0.0450, -0.0697],\n",
            "         [ 0.0406, -0.1831, -0.4102],\n",
            "         [-0.4229, -0.3113, -0.0865]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.3828,  0.3303, -0.3645],\n",
            "         [-0.1816,  0.6160,  0.4019],\n",
            "         [ 0.1626,  0.5294, -0.0823],\n",
            "         [ 0.1840,  0.0607, -0.0738],\n",
            "         [ 0.0338, -0.1630, -0.4156],\n",
            "         [-0.4277, -0.3000, -0.0881]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.4038,  0.3866, -0.3630],\n",
            "         [-0.2518,  0.6570,  0.4013],\n",
            "         [ 0.1465,  0.5637, -0.0882],\n",
            "         [ 0.1723,  0.0759, -0.0779],\n",
            "         [ 0.0268, -0.1427, -0.4210],\n",
            "         [-0.4314, -0.2871, -0.0902]]], requires_grad=True)\n",
            "search -0.05487517639994621\n",
            "2 act:  13 reward:  0.0\n",
            "Parameter containing:\n",
            "tensor([[[-0.2498,  0.3643,  0.3670],\n",
            "         [-0.2516,  0.0363, -0.0635],\n",
            "         [ 0.0721, -0.1902, -0.2535],\n",
            "         [-0.4506, -0.0371, -0.1855],\n",
            "         [-0.3959, -0.0564,  0.3963],\n",
            "         [-0.3244,  0.4894,  0.4489]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2332,  0.4597,  0.3668],\n",
            "         [-0.2701,  0.0815, -0.0750],\n",
            "         [ 0.0596, -0.1640, -0.2638],\n",
            "         [-0.4565, -0.0019, -0.1922],\n",
            "         [-0.4002, -0.0317,  0.3947],\n",
            "         [-0.3269,  0.5027,  0.4479]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2165,  0.5483,  0.3667],\n",
            "         [-0.2884,  0.1265, -0.0866],\n",
            "         [ 0.0470, -0.1375, -0.2743],\n",
            "         [-0.4623,  0.0334, -0.1990],\n",
            "         [-0.4045, -0.0070,  0.3931],\n",
            "         [-0.3294,  0.5159,  0.4469]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.1996,  0.6300,  0.3666],\n",
            "         [-0.3065,  0.1710, -0.0986],\n",
            "         [ 0.0344, -0.1108, -0.2850],\n",
            "         [-0.4681,  0.0687, -0.2058],\n",
            "         [-0.4087,  0.0177,  0.3915],\n",
            "         [-0.3320,  0.5288,  0.4458]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2128,  0.7255,  0.3627],\n",
            "         [-0.3366,  0.2154, -0.1063],\n",
            "         [ 0.0197, -0.0870, -0.2957],\n",
            "         [-0.4750,  0.1017, -0.2126],\n",
            "         [-0.4138,  0.0409,  0.3900],\n",
            "         [-0.3351,  0.5409,  0.4449]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2259,  0.8109,  0.3589],\n",
            "         [-0.3575,  0.2638, -0.1161],\n",
            "         [ 0.0050, -0.0631, -0.3066],\n",
            "         [-0.4817,  0.1345, -0.2195],\n",
            "         [-0.4206,  0.0617,  0.3887],\n",
            "         [-0.3383,  0.5527,  0.4439]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2393,  0.8863,  0.3550],\n",
            "         [-0.3782,  0.3101, -0.1260],\n",
            "         [-0.0098, -0.0402, -0.3176],\n",
            "         [-0.4887,  0.1661, -0.2264],\n",
            "         [-0.4248,  0.0844,  0.3871],\n",
            "         [-0.3516,  0.5684,  0.4436]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2526,  0.9544,  0.3511],\n",
            "         [-0.4068,  0.3507, -0.1341],\n",
            "         [-0.0283, -0.0189, -0.3265],\n",
            "         [-0.4956,  0.1974, -0.2334],\n",
            "         [-0.4289,  0.1071,  0.3856],\n",
            "         [-0.3649,  0.5839,  0.4434]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2658,  1.0159,  0.3471],\n",
            "         [-0.4267,  0.3946, -0.1444],\n",
            "         [-0.0432,  0.0041, -0.3379],\n",
            "         [-0.5025,  0.2283, -0.2405],\n",
            "         [-0.4330,  0.1297,  0.3840],\n",
            "         [-0.3780,  0.5991,  0.4431]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2789,  1.0721,  0.3431],\n",
            "         [-0.4463,  0.4382, -0.1549],\n",
            "         [-0.0580,  0.0272, -0.3495],\n",
            "         [-0.5094,  0.2588, -0.2477],\n",
            "         [-0.4371,  0.1521,  0.3824],\n",
            "         [-0.3911,  0.6127,  0.4429]]], requires_grad=True)\n",
            "search -0.054341644048690796\n",
            "2 act:  16 reward:  0.0\n",
            "Parameter containing:\n",
            "tensor([[[-0.2925,  0.2852, -0.5111],\n",
            "         [ 0.3068,  0.0243, -0.0248],\n",
            "         [ 0.1208, -0.4103,  0.0876],\n",
            "         [ 0.0024,  0.0789, -0.0316],\n",
            "         [ 0.4286, -0.0070, -0.1537],\n",
            "         [ 0.1353, -0.4887,  0.4356]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2546,  0.4055, -0.5131],\n",
            "         [ 0.2952,  0.0726, -0.0374],\n",
            "         [ 0.1120, -0.3859,  0.0809],\n",
            "         [-0.0019,  0.1174, -0.0377],\n",
            "         [ 0.4261,  0.0201, -0.1590],\n",
            "         [ 0.1340, -0.4741,  0.4344]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2159,  0.5167, -0.5151],\n",
            "         [ 0.2837,  0.1207, -0.0503],\n",
            "         [ 0.1031, -0.3609,  0.0741],\n",
            "         [-0.0063,  0.1556, -0.0438],\n",
            "         [ 0.4235,  0.0472, -0.1643],\n",
            "         [ 0.1327, -0.4593,  0.4332]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.1766,  0.6177, -0.5171],\n",
            "         [ 0.2720,  0.1684, -0.0636],\n",
            "         [ 0.0942, -0.3355,  0.0672],\n",
            "         [-0.0107,  0.1934, -0.0500],\n",
            "         [ 0.4209,  0.0742, -0.1698],\n",
            "         [ 0.1314, -0.4443,  0.4319]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.1823,  0.7243, -0.5419],\n",
            "         [ 0.2594,  0.2223, -0.0745],\n",
            "         [ 0.0832, -0.3125,  0.0603],\n",
            "         [-0.0159,  0.2285, -0.0562],\n",
            "         [ 0.4141,  0.0949, -0.1733],\n",
            "         [ 0.1294, -0.4300,  0.4307]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.1881,  0.8184, -0.5677],\n",
            "         [ 0.2467,  0.2751, -0.0855],\n",
            "         [ 0.0722, -0.2891,  0.0533],\n",
            "         [-0.0217,  0.2633, -0.0625],\n",
            "         [ 0.4107,  0.1203, -0.1787],\n",
            "         [ 0.1275, -0.4155,  0.4295]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.1938,  0.9018, -0.5946],\n",
            "         [ 0.2339,  0.3266, -0.0968],\n",
            "         [ 0.0612, -0.2654,  0.0463],\n",
            "         [-0.0275,  0.2975, -0.0689],\n",
            "         [ 0.4072,  0.1456, -0.1842],\n",
            "         [ 0.1255, -0.4009,  0.4282]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.1995,  0.9760, -0.6225],\n",
            "         [ 0.2211,  0.3766, -0.1084],\n",
            "         [ 0.0502, -0.2415,  0.0392],\n",
            "         [-0.0333,  0.3310, -0.0753],\n",
            "         [ 0.4039,  0.1706, -0.1897],\n",
            "         [ 0.1221, -0.3875,  0.4272]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2052,  1.0425, -0.6517],\n",
            "         [ 0.2082,  0.4249, -0.1202],\n",
            "         [ 0.0391, -0.2172,  0.0319],\n",
            "         [-0.0391,  0.3639, -0.0818],\n",
            "         [ 0.4005,  0.1955, -0.1953],\n",
            "         [ 0.1201, -0.3725,  0.4260]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.2109,  1.1026, -0.6821],\n",
            "         [ 0.1952,  0.4714, -0.1322],\n",
            "         [ 0.0280, -0.1926,  0.0246],\n",
            "         [-0.0449,  0.3960, -0.0883],\n",
            "         [ 0.3970,  0.2201, -0.2009],\n",
            "         [ 0.1181, -0.3573,  0.4247]]], requires_grad=True)\n",
            "search -0.05450349301099777\n",
            "2 act:  16 reward:  0.0\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5524, -0.0429, -0.1054],\n",
            "         [ 0.1585, -0.2076, -0.2323],\n",
            "         [-0.0733, -0.2202,  0.0331],\n",
            "         [-0.0553, -0.2301, -0.2017],\n",
            "         [-0.4950,  0.0683,  0.1449],\n",
            "         [-0.5273,  0.4390,  0.0451]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4944,  0.0801, -0.1219],\n",
            "         [ 0.1387, -0.1628, -0.2480],\n",
            "         [-0.0852, -0.1907,  0.0245],\n",
            "         [-0.0611, -0.1896, -0.2112],\n",
            "         [-0.5001,  0.0957,  0.1418],\n",
            "         [-0.5289,  0.4558,  0.0421]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5457,  0.2312, -0.1236],\n",
            "         [ 0.1334, -0.1127, -0.2698],\n",
            "         [-0.0904, -0.1613,  0.0157],\n",
            "         [-0.0626, -0.1489, -0.2210],\n",
            "         [-0.5008,  0.1249,  0.1382],\n",
            "         [-0.5298,  0.4708,  0.0395]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5946,  0.3754, -0.1254],\n",
            "         [ 0.1281, -0.0619, -0.2924],\n",
            "         [-0.0956, -0.1315,  0.0067],\n",
            "         [-0.0641, -0.1077, -0.2310],\n",
            "         [-0.5016,  0.1539,  0.1346],\n",
            "         [-0.5296,  0.4870,  0.0364]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5391,  0.4827, -0.1425],\n",
            "         [ 0.1081, -0.0153, -0.3097],\n",
            "         [-0.1075, -0.1011, -0.0023],\n",
            "         [-0.0703, -0.0655, -0.2410],\n",
            "         [-0.5049,  0.1831,  0.1309],\n",
            "         [-0.5322,  0.5019,  0.0338]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5883,  0.6049, -0.1443],\n",
            "         [ 0.0955,  0.0331, -0.3306],\n",
            "         [-0.1123, -0.0710, -0.0115],\n",
            "         [-0.0746, -0.0269, -0.2498],\n",
            "         [-0.5057,  0.2116,  0.1273],\n",
            "         [-0.5321,  0.5176,  0.0307]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4969,  0.6354, -0.1469],\n",
            "         [ 0.0904,  0.1420, -0.3510],\n",
            "         [-0.1195, -0.0416, -0.0212],\n",
            "         [-0.0806,  0.0135, -0.2601],\n",
            "         [-0.5091,  0.2386,  0.1236],\n",
            "         [-0.5336,  0.5324,  0.0277]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4928,  0.7400, -0.1603],\n",
            "         [ 0.0776,  0.1961, -0.3690],\n",
            "         [-0.1304, -0.0160, -0.0292],\n",
            "         [-0.0835,  0.0535, -0.2706],\n",
            "         [-0.5107,  0.2651,  0.1199],\n",
            "         [-0.5340,  0.5469,  0.0246]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4947,  0.8408, -0.1774],\n",
            "         [ 0.0739,  0.2542, -0.3905],\n",
            "         [-0.1372,  0.0113, -0.0388],\n",
            "         [-0.0895,  0.0901, -0.2798],\n",
            "         [-0.5123,  0.2912,  0.1162],\n",
            "         [-0.5344,  0.5611,  0.0215]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4963,  0.9282, -0.1951],\n",
            "         [ 0.0605,  0.3051, -0.4096],\n",
            "         [-0.1446,  0.0376, -0.0485],\n",
            "         [-0.0923,  0.1287, -0.2906],\n",
            "         [-0.5144,  0.3146,  0.1130],\n",
            "         [-0.5459,  0.5831,  0.0204]]], requires_grad=True)\n",
            "search -0.05379300191998482\n",
            "2 act:  16 reward:  0.0\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5528,  0.4375, -0.3098],\n",
            "         [-0.3998,  0.4269, -0.1367],\n",
            "         [ 0.0496, -0.0601, -0.2136],\n",
            "         [-0.4563,  0.3494, -0.4665],\n",
            "         [ 0.4650,  0.5005, -0.1798],\n",
            "         [ 0.2233,  0.1599,  0.4772]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4990,  0.5396, -0.3375],\n",
            "         [-0.4095,  0.4677, -0.1519],\n",
            "         [ 0.0377, -0.0292, -0.2270],\n",
            "         [-0.4616,  0.3874, -0.4813],\n",
            "         [ 0.4630,  0.5260, -0.1871],\n",
            "         [ 0.2227,  0.1813,  0.4757]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5686,  0.6711, -0.3411],\n",
            "         [-0.4082,  0.5119, -0.1729],\n",
            "         [ 0.0360,  0.0030, -0.2429],\n",
            "         [-0.4604,  0.4267, -0.4990],\n",
            "         [ 0.4621,  0.5486, -0.1937],\n",
            "         [ 0.2243,  0.2023,  0.4741]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4855,  0.7036, -0.3479],\n",
            "         [-0.4039,  0.6049, -0.1906],\n",
            "         [ 0.0265,  0.0322, -0.2593],\n",
            "         [-0.4596,  0.4646, -0.5171],\n",
            "         [ 0.4589,  0.5698, -0.2003],\n",
            "         [ 0.2211,  0.2191,  0.4730]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4860,  0.8024, -0.3724],\n",
            "         [-0.4751,  0.6892, -0.1980],\n",
            "         [ 0.0244,  0.0916, -0.2750],\n",
            "         [-0.4604,  0.4793, -0.5348],\n",
            "         [ 0.4405,  0.6024, -0.2033],\n",
            "         [ 0.2233,  0.2382,  0.4716]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4866,  0.8894, -0.3980],\n",
            "         [-0.5421,  0.7660, -0.2056],\n",
            "         [ 0.0179,  0.1458, -0.2891],\n",
            "         [-0.4611,  0.4939, -0.5530],\n",
            "         [ 0.4219,  0.6340, -0.2064],\n",
            "         [ 0.2255,  0.2572,  0.4702]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.4932,  0.9736, -0.4298],\n",
            "         [-0.6050,  0.8358, -0.2133],\n",
            "         [ 0.0113,  0.1993, -0.3036],\n",
            "         [-0.4619,  0.5082, -0.5717],\n",
            "         [ 0.4031,  0.6645, -0.2095],\n",
            "         [ 0.2259,  0.2743,  0.4690]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5052,  1.0463, -0.4629],\n",
            "         [-0.7333,  0.8937, -0.2303],\n",
            "         [ 0.0124,  0.2229, -0.3214],\n",
            "         [-0.4597,  0.5378, -0.5890],\n",
            "         [ 0.3872,  0.6897, -0.2119],\n",
            "         [ 0.2283,  0.2915,  0.4678]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5170,  1.1114, -0.4977],\n",
            "         [-0.8437,  0.9473, -0.2479],\n",
            "         [ 0.0090,  0.2448, -0.3375],\n",
            "         [-0.4551,  0.5693, -0.6094],\n",
            "         [ 0.3710,  0.7182, -0.2150],\n",
            "         [ 0.2287,  0.3068,  0.4667]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5285,  1.1690, -0.5346],\n",
            "         [-0.9302,  0.9893, -0.2647],\n",
            "         [ 0.0126,  0.2678, -0.3565],\n",
            "         [-0.4769,  0.6128, -0.6183],\n",
            "         [ 0.3464,  0.7436, -0.2182],\n",
            "         [ 0.2292,  0.3221,  0.4655]]], requires_grad=True)\n",
            "search -0.05426899343729019\n",
            "2 act:  16 reward:  0.0\n",
            "Parameter containing:\n",
            "tensor([[[-0.1827,  0.3194,  0.1909],\n",
            "         [ 0.1642,  0.0116,  0.2218],\n",
            "         [ 0.1603, -0.3943, -0.5187],\n",
            "         [ 0.3163, -0.1210,  0.1254],\n",
            "         [-0.2122, -0.4116,  0.3221],\n",
            "         [-0.2236,  0.2936, -0.2409]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[-0.0748,  0.4965,  0.1891],\n",
            "         [ 0.1650,  0.0662,  0.2112],\n",
            "         [ 0.1579, -0.3666, -0.5445],\n",
            "         [ 0.3197, -0.0727,  0.1186],\n",
            "         [-0.2097, -0.3825,  0.3188],\n",
            "         [-0.2207,  0.3150, -0.2474]]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[[ 0.0360,  0.6503,  0.1872],\n",
            "         [ 0.1732,  0.1235,  0.1992],\n",
            "         [ 0.1554, -0.3384, -0.5713],\n",
            "         [ 0.3230, -0.0240,  0.1116],\n",
            "         [-0.2071, -0.3529,  0.3155],\n",
            "         [-0.2192,  0.3346, -0.2533]]], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-cac91b08ec04>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# buffer = simulate(agent, buffer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-cac91b08ec04>\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(agent, buffer)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#     print(stt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-6b0870240141>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.unsqueeze(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0micost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mla\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mlact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes_to_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlact\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-6b0870240141>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, sx, T)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    agent.eval()\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # out = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        print(i, 'act: ',action, 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9cm6KjvBrnNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b29326b-7035-4252-e97e-2ea4ffce3d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "tcost icost -0.1917724609375 0.4500417113304138\n",
            "search 0.25830078125\n",
            "tcost icost -0.1258544921875 0.4571450352668762\n",
            "tcost icost -0.174560546875 0.4520290493965149\n",
            "tcost icost -0.174560546875 0.45218896865844727\n",
            "search 0.277587890625\n",
            "tcost icost -0.1385498046875 0.4564841687679291\n",
            "tcost icost -0.183349609375 0.4503477215766907\n",
            "tcost icost -0.183349609375 0.4505652189254761\n",
            "search 0.267333984375\n",
            "tcost icost -0.129638671875 0.4571947753429413\n",
            "tcost icost -0.158203125 0.4537872076034546\n",
            "tcost icost -0.158203125 0.4545557498931885\n",
            "search 0.29638671875\n",
            "tcost icost -0.1522216796875 0.4549732506275177\n",
            "tcost icost -0.1678466796875 0.452946275472641\n",
            "tcost icost -0.1678466796875 0.45447802543640137\n",
            "search 0.28662109375\n",
            "tcost icost -0.1328125 0.4565030634403229\n",
            "tcost icost -0.185791015625 0.4522371292114258\n",
            "tcost icost -0.185791015625 0.45176154375076294\n",
            "search 0.265869140625\n",
            "tcost icost -0.135986328125 0.45669934153556824\n",
            "tcost icost -0.180419921875 0.4511166214942932\n",
            "tcost icost -0.180419921875 0.4510495662689209\n",
            "search 0.2705078125\n",
            "tcost icost -0.1195068359375 0.4582979679107666\n",
            "tcost icost -0.1484375 0.45480725169181824\n",
            "tcost icost -0.1484375 0.4554913341999054\n",
            "search 0.30712890625\n",
            "tcost icost -0.13232421875 0.4559730887413025\n",
            "tcost icost -0.1715087890625 0.45109763741493225\n",
            "tcost icost -0.1715087890625 0.4516078233718872\n",
            "search 0.2802734375\n",
            "tcost icost -0.13037109375 0.45773550868034363\n",
            "tcost icost -0.1641845703125 0.4536299407482147\n",
            "tcost icost -0.1641845703125 0.45345237851142883\n",
            "search 0.2890625\n",
            "tcost icost -0.1207275390625 0.45868822932243347\n",
            "tcost icost -0.1446533203125 0.4552922248840332\n",
            "tcost icost -0.1446533203125 0.4574338495731354\n",
            "search 0.31298828125\n",
            "tcost icost -0.126953125 0.45746827125549316\n",
            "tcost icost -0.1536865234375 0.4548506438732147\n",
            "tcost icost -0.1536865234375 0.45684871077537537\n",
            "search 0.30322265625\n",
            "tcost icost -0.1295166015625 0.45746099948883057\n",
            "tcost icost -0.189208984375 0.45017173886299133\n",
            "tcost icost -0.189208984375 0.450045108795166\n",
            "search 0.2607421875\n",
            "tcost icost -0.12939453125 0.45723482966423035\n",
            "tcost icost -0.1878662109375 0.4501354992389679\n",
            "tcost icost -0.1878662109375 0.4503631889820099\n",
            "search 0.2626953125\n",
            "tcost icost -0.1258544921875 0.457701176404953\n",
            "tcost icost -0.1783447265625 0.45172303915023804\n",
            "tcost icost -0.1783447265625 0.451874315738678\n",
            "search 0.2734375\n",
            "tcost icost -0.1280517578125 0.4578351080417633\n",
            "tcost icost -0.196044921875 0.449463427066803\n",
            "tcost icost -0.196044921875 0.4490892291069031\n",
            "search 0.2529296875\n",
            "tcost icost -0.1495361328125 0.4556976556777954\n",
            "tcost icost -0.1773681640625 0.4528885781764984\n",
            "tcost icost -0.1773681640625 0.452725350856781\n",
            "search 0.275390625\n",
            "tcost icost -0.14111328125 0.458251029253006\n",
            "tcost icost -0.187255859375 0.45108622312545776\n",
            "tcost icost -0.187255859375 0.4512355327606201\n",
            "search 0.263916015625\n",
            "tcost icost -0.135986328125 0.4573563039302826\n",
            "tcost icost -0.1922607421875 0.45061540603637695\n",
            "tcost icost -0.1922607421875 0.45116448402404785\n",
            "search 0.2587890625\n",
            "tcost icost -0.12939453125 0.4607274532318115\n",
            "tcost icost -0.190185546875 0.45124879479408264\n",
            "tcost icost -0.190185546875 0.4515508711338043\n",
            "search 0.261474609375\n",
            "tcost icost -0.137939453125 0.4576600193977356\n",
            "tcost icost -0.1827392578125 0.45219114422798157\n",
            "tcost icost -0.1827392578125 0.4524862766265869\n",
            "search 0.26953125\n",
            "tcost icost -0.1241455078125 0.4584851562976837\n",
            "tcost icost -0.1826171875 0.4524085223674774\n",
            "tcost icost -0.1826171875 0.45260536670684814\n",
            "search 0.27001953125\n",
            "tcost icost -0.1707763671875 0.45384910702705383\n",
            "tcost icost -0.186279296875 0.45239439606666565\n",
            "tcost icost -0.186279296875 0.4539453983306885\n",
            "search 0.267578125\n",
            "tcost icost -0.1302490234375 0.4603346288204193\n",
            "tcost icost -0.1639404296875 0.455904096364975\n",
            "tcost icost -0.1639404296875 0.4560299813747406\n",
            "search 0.2919921875\n",
            "tcost icost -0.1304931640625 0.4589146375656128\n",
            "tcost icost -0.146728515625 0.45834842324256897\n",
            "tcost icost -0.146728515625 0.4579556882381439\n",
            "search 0.311279296875\n",
            "tcost icost -0.1324462890625 0.458191841840744\n",
            "tcost icost -0.1431884765625 0.45726484060287476\n",
            "tcost icost -0.1431884765625 0.4570212960243225\n",
            "search 0.31396484375\n",
            "tcost icost -0.1278076171875 0.45920678973197937\n",
            "tcost icost -0.15771484375 0.45565417408943176\n",
            "tcost icost -0.15771484375 0.45501211285591125\n",
            "search 0.29736328125\n",
            "tcost icost -0.12841796875 0.45842650532722473\n",
            "tcost icost -0.1575927734375 0.45565542578697205\n",
            "tcost icost -0.1575927734375 0.4552828073501587\n",
            "search 0.2978515625\n",
            "tcost icost -0.159423828125 0.45528966188430786\n",
            "tcost icost -0.1881103515625 0.4513475298881531\n",
            "tcost icost -0.1881103515625 0.45143091678619385\n",
            "search 0.26318359375\n",
            "tcost icost -0.1348876953125 0.45758795738220215\n",
            "tcost icost -0.173095703125 0.4529717266559601\n",
            "tcost icost -0.173095703125 0.45285311341285706\n",
            "search 0.27978515625\n",
            "tcost icost -0.1724853515625 0.4532346725463867\n",
            "tcost icost -0.1881103515625 0.45129695534706116\n",
            "tcost icost -0.1881103515625 0.45131972432136536\n",
            "search 0.26318359375\n",
            "tcost icost -0.1290283203125 0.4583168923854828\n",
            "tcost icost -0.149169921875 0.4558131992816925\n",
            "tcost icost -0.149169921875 0.4564491808414459\n",
            "search 0.307373046875\n",
            "tcost icost -0.1356201171875 0.45891207456588745\n",
            "tcost icost -0.1763916015625 0.4541504681110382\n",
            "tcost icost -0.1763916015625 0.45429521799087524\n",
            "search 0.27783203125\n",
            "tcost icost -0.1441650390625 0.45821186900138855\n",
            "tcost icost -0.1572265625 0.4568858742713928\n",
            "tcost icost -0.1572265625 0.4567483365535736\n",
            "search 0.299560546875\n",
            "tcost icost -0.1309814453125 0.4594759941101074\n",
            "tcost icost -0.1619873046875 0.4558771252632141\n",
            "tcost icost -0.1619873046875 0.45635685324668884\n",
            "search 0.29443359375\n",
            "tcost icost -0.12188720703125 0.459843248128891\n",
            "tcost icost -0.1407470703125 0.4576476216316223\n",
            "tcost icost -0.1407470703125 0.4573657810688019\n",
            "search 0.31640625\n",
            "tcost icost -0.146484375 0.45707979798316956\n",
            "tcost icost -0.17333984375 0.45354005694389343\n",
            "tcost icost -0.17333984375 0.4537920653820038\n",
            "search 0.280517578125\n",
            "tcost icost -0.1292724609375 0.46031665802001953\n",
            "tcost icost -0.1619873046875 0.4548754394054413\n",
            "tcost icost -0.1619873046875 0.45684683322906494\n",
            "search 0.294921875\n",
            "tcost icost -0.1632080078125 0.45381471514701843\n",
            "tcost icost -0.178466796875 0.4520741105079651\n",
            "tcost icost -0.178466796875 0.4517580568790436\n",
            "search 0.273193359375\n",
            "tcost icost -0.126220703125 0.4581817388534546\n",
            "tcost icost -0.181640625 0.45237940549850464\n",
            "tcost icost -0.181640625 0.45129865407943726\n",
            "search 0.269775390625\n",
            "tcost icost -0.1357421875 0.45692408084869385\n",
            "tcost icost -0.191162109375 0.4506516754627228\n",
            "tcost icost -0.191162109375 0.45048943161964417\n",
            "search 0.25927734375\n",
            "tcost icost -0.1732177734375 0.4529710114002228\n",
            "tcost icost -0.187744140625 0.450913667678833\n",
            "tcost icost -0.187744140625 0.45085087418556213\n",
            "search 0.26318359375\n",
            "tcost icost -0.1513671875 0.4552533030509949\n",
            "tcost icost -0.1666259765625 0.45279461145401\n",
            "tcost icost -0.1666259765625 0.45329418778419495\n",
            "search 0.28662109375\n",
            "tcost icost -0.1297607421875 0.4573538899421692\n",
            "tcost icost -0.17822265625 0.45179471373558044\n",
            "tcost icost -0.17822265625 0.4518124759197235\n",
            "search 0.273681640625\n",
            "tcost icost -0.156005859375 0.4552094340324402\n",
            "tcost icost -0.197509765625 0.44925346970558167\n",
            "tcost icost -0.197509765625 0.4495968222618103\n",
            "search 0.252197265625\n",
            "tcost icost -0.11968994140625 0.45742204785346985\n",
            "tcost icost -0.1259765625 0.45687660574913025\n",
            "tcost icost -0.1259765625 0.4591549038887024\n",
            "search 0.333251953125\n",
            "tcost icost -0.10015869140625 0.4576987028121948\n",
            "tcost icost -0.1072998046875 0.4564245939254761\n",
            "tcost icost -0.1072998046875 0.45669955015182495\n",
            "search 0.349609375\n",
            "tcost icost -0.12396240234375 0.4563211500644684\n",
            "tcost icost -0.1448974609375 0.45256999135017395\n",
            "tcost icost -0.1448974609375 0.4526514708995819\n",
            "search 0.3076171875\n",
            "tcost icost -0.12322998046875 0.45471060276031494\n",
            "tcost icost -0.1298828125 0.45407962799072266\n",
            "tcost icost -0.1298828125 0.4545351266860962\n",
            "search 0.32470703125\n",
            "tcost icost -0.12017822265625 0.4552350342273712\n",
            "tcost icost -0.1304931640625 0.45295193791389465\n",
            "tcost icost -0.1304931640625 0.4528146982192993\n",
            "search 0.322265625\n",
            "tcost icost -0.1190185546875 0.4533029794692993\n",
            "tcost icost -0.1407470703125 0.45146849751472473\n",
            "tcost icost -0.1407470703125 0.45256227254867554\n",
            "search 0.31201171875\n",
            "tcost icost -0.125 0.4521556794643402\n",
            "tcost icost -0.160400390625 0.44806620478630066\n",
            "tcost icost -0.160400390625 0.44800132513046265\n",
            "search 0.28759765625\n",
            "tcost icost -0.1297607421875 0.4521123766899109\n",
            "tcost icost -0.1689453125 0.4474334716796875\n",
            "tcost icost -0.1689453125 0.4472966194152832\n",
            "search 0.2783203125\n",
            "tcost icost -0.13525390625 0.45302316546440125\n",
            "tcost icost -0.1773681640625 0.4477558434009552\n",
            "tcost icost -0.1773681640625 0.44714000821113586\n",
            "search 0.26953125\n",
            "tcost icost -0.12457275390625 0.4533313810825348\n",
            "tcost icost -0.1480712890625 0.4507625102996826\n",
            "tcost icost -0.1480712890625 0.4510008990764618\n",
            "search 0.302734375\n",
            "tcost icost -0.11328125 0.4542665481567383\n",
            "tcost icost -0.1357421875 0.45208001136779785\n",
            "tcost icost -0.1357421875 0.45274052023887634\n",
            "search 0.31689453125\n",
            "tcost icost -0.115966796875 0.4530367851257324\n",
            "tcost icost -0.1419677734375 0.4503437280654907\n",
            "tcost icost -0.1419677734375 0.4512947201728821\n",
            "search 0.3095703125\n",
            "tcost icost -0.11785888671875 0.4531176686286926\n",
            "tcost icost -0.1346435546875 0.4515015184879303\n",
            "tcost icost -0.1346435546875 0.4519946277141571\n",
            "search 0.3173828125\n",
            "tcost icost -0.156982421875 0.4496397078037262\n",
            "tcost icost -0.156982421875 0.4485163390636444\n",
            "tcost icost -0.156982421875 0.4489791989326477\n",
            "search 0.2919921875\n",
            "tcost icost -0.111083984375 0.4523293972015381\n",
            "tcost icost -0.12359619140625 0.4511989951133728\n",
            "tcost icost -0.12359619140625 0.4516626298427582\n",
            "search 0.328125\n",
            "tcost icost -0.1192626953125 0.451643168926239\n",
            "tcost icost -0.14892578125 0.44876062870025635\n",
            "tcost icost -0.14892578125 0.4491689205169678\n",
            "search 0.30029296875\n",
            "tcost icost -0.1341552734375 0.449694961309433\n",
            "tcost icost -0.1710205078125 0.4451819956302643\n",
            "tcost icost -0.1710205078125 0.44561487436294556\n",
            "search 0.2744140625\n",
            "tcost icost -0.12384033203125 0.4528011679649353\n",
            "tcost icost -0.1414794921875 0.4491543173789978\n",
            "tcost icost -0.1414794921875 0.4494020938873291\n",
            "search 0.30810546875\n",
            "ded\n",
            "time\n",
            "20 #### train ####\n",
            "repr, std, cov, closslb 0.004742905497550964 0.474853515625 0.00020816875621676445 0.006639578379690647 0.0005656339344568551\n",
            "0.009498101386116705 0.028407063925967778 1.0\n",
            "repr, std, cov, closslb 0.0027855292428284883 0.47607421875 0.000161072239279747 4.651730250770925e-06 7.551890303147957e-05\n",
            "0.009488612773343362 0.028237216503531004 1.0\n",
            "repr, std, cov, closslb 0.00586820300668478 0.475341796875 0.00018018181435763836 0.011462045833468437 0.015634149312973022\n",
            "0.009603104467265708 0.028577932984050012 1.0\n",
            "repr, std, cov, closslb 0.00382648385129869 0.4775390625 0.0001394741702824831 0.005702572409063578 7.05966231180355e-05\n",
            "0.009728696620855067 0.029038625319532215 1.0\n",
            "repr, std, cov, closslb 0.005948936566710472 0.477294921875 0.00011645141057670116 0.008094072341918945 0.00023356771271210164\n",
            "0.009748163742793395 0.029595353056853 1.0\n",
            "repr, std, cov, closslb 0.006911762990057468 0.476806640625 0.00012450781650841236 0.01257975772023201 0.015348418615758419\n",
            "0.009806799142815846 0.0301025192222957 1.0\n",
            "repr, std, cov, closslb 0.0034177645575255156 0.477783203125 0.00013721385039389133 4.197569978714455e-06 0.015328913927078247\n",
            "0.009757911906536187 0.02977336969784112 1.0\n",
            "repr, std, cov, closslb 0.003915002569556236 0.477294921875 0.0001160742249339819 0.003917462658137083 3.132569690933451e-05\n",
            "0.009787214925749424 0.03013262174151799 1.0\n",
            "repr, std, cov, closslb 0.004678093828260899 0.476806640625 0.00012978562153875828 0.004166137892752886 0.015471182763576508\n",
            "0.009806799142815846 0.030648994909765845 1.0\n",
            "repr, std, cov, closslb 0.0030405272264033556 0.476318359375 0.00014275102876126766 3.3055871426768135e-06 0.0011466563446447253\n",
            "0.009875653023178962 0.03098782461570577 1.0\n",
            "repr, std, cov, closslb 0.0026512895710766315 0.4765625 0.00014862953685224056 6.472271707025357e-06 0.007245045620948076\n",
            "0.00973842531747592 0.030162754363259506 1.0\n",
            "repr, std, cov, closslb 0.012444514781236649 0.47412109375 0.0002490428742021322 0.011356242932379246 0.0011251990217715502\n",
            "0.009631942599584008 0.029654573358319756 1.0\n",
            "repr, std, cov, closslb 0.005141636822372675 0.47412109375 0.0002313065342605114 0.0073713865131139755 0.0002218810113845393\n",
            "0.009460203771955977 0.028721108714222333 1.0\n",
            "repr, std, cov, closslb 0.0033981259912252426 0.474365234375 0.00019656680524349213 1.411910488968715e-05 0.00903324130922556\n",
            "0.009375486090915151 0.02829371917375456 1.0\n",
            "repr, std, cov, closslb 0.003281734650954604 0.474609375 0.00020000687800347805 0.0037837435957044363 7.99996341811493e-05\n",
            "0.009263708145819038 0.027623084959454244 1.0\n",
            "repr, std, cov, closslb 0.004099999088793993 0.474853515625 0.00016104662790894508 0.007883109152317047 0.03161616995930672\n",
            "0.009263708145819038 0.0275128682989696 1.0\n",
            "repr, std, cov, closslb 0.0025445101782679558 0.477294921875 0.00011069327592849731 7.154585091484478e-06 0.0002557701664045453\n",
            "0.009403640685021654 0.02801233193212689 1.0\n",
            "repr, std, cov, closslb 0.002431931672617793 0.476318359375 0.0001372741535305977 4.263964910933282e-06 0.0002785921096801758\n",
            "0.009460203771955977 0.02832201289292831 1.0\n",
            "repr, std, cov, closslb 0.004682808183133602 0.4755859375 0.0001558968797326088 0.002168509876355529 0.0005997493281029165\n",
            "0.009574352676603518 0.029067663944851743 1.0\n",
            "repr, std, cov, closslb 0.005885259713977575 0.47607421875 0.00014723138883709908 0.012706861831247807 9.093287371797487e-05\n",
            "0.009469663975727932 0.028749829822936553 1.0\n",
            "repr, std, cov, closslb 0.00253578694537282 0.475341796875 0.00019934470765292645 1.3201066394685768e-05 0.006280958186835051\n",
            "0.009347415791944529 0.02826545372003453 1.0\n",
            "repr, std, cov, closslb 0.002015430713072419 0.47509765625 0.0001822661142796278 2.7076098376710434e-06 0.00015718527720309794\n",
            "0.009366119970944208 0.02826545372003453 1.0\n",
            "repr, std, cov, closslb 0.0029257836285978556 0.474853515625 0.0001647304743528366 0.0035668285563588142 0.0004172801272943616\n",
            "0.009180750115435038 0.027650708044413694 1.0\n",
            "repr, std, cov, closslb 0.0024393140338361263 0.475341796875 0.0001848917454481125 0.0033815132919698954 0.0003749435709323734\n",
            "0.009116741156370945 0.027403091405160767 1.0\n",
            "repr, std, cov, closslb 0.0029264637269079685 0.47412109375 0.00018507614731788635 0.006028665229678154 0.0008758017211221159\n",
            "0.008990058954248117 0.026726840528536387 1.0\n",
            "repr, std, cov, closslb 0.012541243806481361 0.47412109375 0.0002227453514933586 0.01703476905822754 1.6752012015786022e-05\n",
            "0.008909551497391642 0.026302824224470164 1.0\n",
            "repr, std, cov, closslb 0.00495955441147089 0.474853515625 0.00016967603005468845 0.007787500042468309 8.27028852654621e-05\n",
            "0.008776971339775425 0.025628097538873986 1.0\n",
            "repr, std, cov, closslb 0.0021512173116207123 0.474853515625 0.00017200387082993984 2.563019279477885e-06 0.00019226898439228535\n",
            "0.008785748311115199 0.025628097538873986 1.0\n",
            "repr, std, cov, closslb 0.0027353663463145494 0.475341796875 0.00015423167496919632 0.00405862694606185 0.00012180631165392697\n",
            "0.008856280791174221 0.025808033309575335 1.0\n",
            "repr, std, cov, closslb 0.003925098106265068 0.474853515625 0.0001763210166245699 0.004083381500095129 0.00012361496919766068\n",
            "0.008838594763053353 0.025576918125704454 1.0\n",
            "repr, std, cov, closslb 0.0029403071384876966 0.47509765625 0.00015607266686856747 0.004162149038165808 0.000655696727335453\n",
            "0.008776971339775425 0.0252215125859283 1.0\n",
            "repr, std, cov, closslb 0.0021191369742155075 0.476806640625 0.00011166278272867203 0.004878454841673374 8.820158836897463e-05\n",
            "0.008820944054001299 0.025398593709430303 1.0\n",
            "repr, std, cov, closslb 0.003098270855844021 0.476318359375 0.0001295134425163269 0.007093444000929594 0.002633704338222742\n",
            "0.008918461048889033 0.025989232415626833 1.0\n",
            "repr, std, cov, closslb 0.004267781041562557 0.476806640625 0.00011597713455557823 0.003935760352760553 0.015331914648413658\n",
            "0.008945243196337304 0.02625029737941396 1.0\n",
            "repr, std, cov, closslb 0.0019566803239285946 0.47705078125 0.00011710380204021931 3.87434283766197e-06 0.004274462815374136\n",
            "0.00909853498786024 0.026726840528536387 1.0\n",
            "repr, std, cov, closslb 0.0019358006538823247 0.476318359375 0.00012248032726347446 9.296023563365452e-06 6.167720857774839e-05\n",
            "0.009080365177140783 0.027103458193801052 1.0\n",
            "tcost icost -0.117919921875 0.43551188707351685\n",
            "tcost icost -0.1744384765625 0.4312528371810913\n",
            "tcost icost -0.1744384765625 0.43112877011299133\n",
            "search 0.2568359375\n",
            "tcost icost -0.117919921875 0.4360354542732239\n",
            "tcost icost -0.1744384765625 0.43195751309394836\n",
            "tcost icost -0.1744384765625 0.43098723888397217\n",
            "search 0.25634765625\n",
            "tcost icost -0.099609375 0.43557268381118774\n",
            "tcost icost -0.1744384765625 0.43033739924430847\n",
            "tcost icost -0.1744384765625 0.4298038184642792\n",
            "search 0.25537109375\n",
            "tcost icost -0.1734619140625 0.42911237478256226\n",
            "tcost icost -0.1734619140625 0.42903873324394226\n",
            "tcost icost -0.1734619140625 0.4289577305316925\n",
            "search 0.25537109375\n",
            "tcost icost -0.1009521484375 0.43437355756759644\n",
            "tcost icost -0.1763916015625 0.4293786883354187\n",
            "tcost icost -0.1763916015625 0.42903804779052734\n",
            "search 0.25244140625\n",
            "tcost icost -0.0694580078125 0.43589088320732117\n",
            "tcost icost -0.1739501953125 0.42897018790245056\n",
            "tcost icost -0.1739501953125 0.42885032296180725\n",
            "search 0.2548828125\n",
            "tcost icost -0.1243896484375 0.4300909638404846\n",
            "tcost icost -0.170654296875 0.42746156454086304\n",
            "tcost icost -0.170654296875 0.4278695583343506\n",
            "search 0.25732421875\n",
            "tcost icost -0.11212158203125 0.43065527081489563\n",
            "tcost icost -0.169921875 0.42783108353614807\n",
            "tcost icost -0.169921875 0.4270486831665039\n",
            "search 0.257080078125\n",
            "tcost icost -0.137451171875 0.42851415276527405\n",
            "tcost icost -0.175537109375 0.42607301473617554\n",
            "tcost icost -0.175537109375 0.42605629563331604\n",
            "search 0.25048828125\n",
            "tcost icost -0.12255859375 0.430897980928421\n",
            "tcost icost -0.1795654296875 0.4257410168647766\n",
            "tcost icost -0.1795654296875 0.42556530237197876\n",
            "search 0.2459716796875\n",
            "tcost icost -0.139892578125 0.4308494031429291\n",
            "tcost icost -0.1790771484375 0.4287406802177429\n",
            "tcost icost -0.1790771484375 0.42608967423439026\n",
            "search 0.2469482421875\n",
            "tcost icost -0.1405029296875 0.4275403320789337\n",
            "tcost icost -0.1873779296875 0.4266280233860016\n",
            "tcost icost -0.1873779296875 0.42470866441726685\n",
            "search 0.2374267578125\n",
            "tcost icost -0.1473388671875 0.42844682931900024\n",
            "tcost icost -0.187255859375 0.4249471127986908\n",
            "tcost icost -0.187255859375 0.42447927594184875\n",
            "search 0.2373046875\n",
            "tcost icost -0.148681640625 0.426774799823761\n",
            "tcost icost -0.1873779296875 0.4237613379955292\n",
            "tcost icost -0.1873779296875 0.42359331250190735\n",
            "search 0.2362060546875\n",
            "tcost icost -0.12286376953125 0.42721495032310486\n",
            "tcost icost -0.1800537109375 0.4234107434749603\n",
            "tcost icost -0.1800537109375 0.42340341210365295\n",
            "search 0.2432861328125\n",
            "tcost icost -0.1229248046875 0.4285203516483307\n",
            "tcost icost -0.1806640625 0.4229988753795624\n",
            "tcost icost -0.1806640625 0.4222545921802521\n",
            "search 0.24169921875\n",
            "tcost icost -0.144287109375 0.42507386207580566\n",
            "tcost icost -0.182861328125 0.42178213596343994\n",
            "tcost icost -0.182861328125 0.4214475154876709\n",
            "search 0.238525390625\n",
            "tcost icost -0.08343505859375 0.4279431104660034\n",
            "tcost icost -0.191650390625 0.4217742681503296\n",
            "tcost icost -0.191650390625 0.42154237627983093\n",
            "search 0.22998046875\n",
            "tcost icost -0.120361328125 0.42648667097091675\n",
            "tcost icost -0.1998291015625 0.4201856553554535\n",
            "tcost icost -0.1998291015625 0.420744389295578\n",
            "search 0.2208251953125\n",
            "tcost icost -0.1915283203125 0.42111167311668396\n",
            "tcost icost -0.1915283203125 0.4221841096878052\n",
            "tcost icost -0.1915283203125 0.4218009412288666\n",
            "search 0.2303466796875\n",
            "tcost icost -0.1561279296875 0.42378878593444824\n",
            "tcost icost -0.196533203125 0.41978850960731506\n",
            "tcost icost -0.196533203125 0.4209885597229004\n",
            "search 0.224365234375\n",
            "tcost icost -0.121337890625 0.4230208396911621\n",
            "tcost icost -0.1768798828125 0.41942542791366577\n",
            "tcost icost -0.1768798828125 0.4192052483558655\n",
            "search 0.2423095703125\n",
            "tcost icost -0.121826171875 0.4244101047515869\n",
            "tcost icost -0.179443359375 0.4204516112804413\n",
            "tcost icost -0.179443359375 0.419679194688797\n",
            "search 0.240234375\n",
            "tcost icost -0.115966796875 0.4224005341529846\n",
            "tcost icost -0.1722412109375 0.4203478991985321\n",
            "tcost icost -0.1722412109375 0.4197863042354584\n",
            "search 0.2474365234375\n",
            "tcost icost -0.12158203125 0.42321091890335083\n",
            "tcost icost -0.178955078125 0.42122310400009155\n",
            "tcost icost -0.178955078125 0.4192574918270111\n",
            "search 0.240234375\n",
            "tcost icost -0.1478271484375 0.4224652349948883\n",
            "tcost icost -0.208251953125 0.4183766841888428\n",
            "tcost icost -0.208251953125 0.4185119867324829\n",
            "search 0.210205078125\n",
            "tcost icost -0.15234375 0.42289644479751587\n",
            "tcost icost -0.2127685546875 0.4188612997531891\n",
            "tcost icost -0.2127685546875 0.4178065359592438\n",
            "search 0.2049560546875\n",
            "tcost icost -0.162841796875 0.42139604687690735\n",
            "tcost icost -0.204833984375 0.4195599853992462\n",
            "tcost icost -0.204833984375 0.41820356249809265\n",
            "search 0.21337890625\n",
            "tcost icost -0.1610107421875 0.42091044783592224\n",
            "tcost icost -0.2047119140625 0.4181362986564636\n",
            "tcost icost -0.2047119140625 0.41754865646362305\n",
            "search 0.2127685546875\n",
            "tcost icost -0.162353515625 0.4197205901145935\n",
            "tcost icost -0.1834716796875 0.4172486662864685\n",
            "tcost icost -0.1834716796875 0.4171542227268219\n",
            "search 0.2337646484375\n",
            "tcost icost -0.1312255859375 0.42182913422584534\n",
            "tcost icost -0.188720703125 0.4186135530471802\n",
            "tcost icost -0.188720703125 0.41699519753456116\n",
            "search 0.228271484375\n",
            "tcost icost -0.1214599609375 0.42076054215431213\n",
            "tcost icost -0.180419921875 0.41674911975860596\n",
            "tcost icost -0.180419921875 0.41654905676841736\n",
            "search 0.236083984375\n",
            "tcost icost -0.172607421875 0.4188198447227478\n",
            "tcost icost -0.1922607421875 0.4169580340385437\n",
            "tcost icost -0.1922607421875 0.41700851917266846\n",
            "search 0.2247314453125\n",
            "tcost icost -0.1597900390625 0.41744565963745117\n",
            "tcost icost -0.1806640625 0.41779065132141113\n",
            "tcost icost -0.1806640625 0.41658028960227966\n",
            "search 0.23583984375\n",
            "tcost icost -0.11578369140625 0.4189308285713196\n",
            "tcost icost -0.1680908203125 0.4157288670539856\n",
            "tcost icost -0.1680908203125 0.41564837098121643\n",
            "search 0.2474365234375\n",
            "tcost icost -0.10260009765625 0.4207954406738281\n",
            "tcost icost -0.177001953125 0.41635286808013916\n",
            "tcost icost -0.177001953125 0.41642338037490845\n",
            "search 0.239501953125\n",
            "tcost icost -0.1064453125 0.4200722575187683\n",
            "tcost icost -0.184326171875 0.4147501587867737\n",
            "tcost icost -0.184326171875 0.41500377655029297\n",
            "search 0.230712890625\n",
            "tcost icost -0.1253662109375 0.4200281500816345\n",
            "tcost icost -0.1832275390625 0.4162250757217407\n",
            "tcost icost -0.1832275390625 0.41466736793518066\n",
            "search 0.2313232421875\n",
            "tcost icost -0.171142578125 0.41620317101478577\n",
            "tcost icost -0.1898193359375 0.4144527018070221\n",
            "tcost icost -0.1898193359375 0.4147331118583679\n",
            "search 0.2249755859375\n",
            "tcost icost -0.1220703125 0.4183185398578644\n",
            "tcost icost -0.17919921875 0.4159010350704193\n",
            "tcost icost -0.17919921875 0.41578057408332825\n",
            "search 0.236572265625\n",
            "tcost icost -0.08184814453125 0.4210546910762787\n",
            "tcost icost -0.1749267578125 0.41453105211257935\n",
            "tcost icost -0.1749267578125 0.41521212458610535\n",
            "search 0.2403564453125\n",
            "tcost icost -0.144775390625 0.41700419783592224\n",
            "tcost icost -0.185546875 0.4135347902774811\n",
            "tcost icost -0.185546875 0.41589251160621643\n",
            "search 0.230224609375\n",
            "tcost icost -0.1427001953125 0.41657009720802307\n",
            "tcost icost -0.2020263671875 0.4120039939880371\n",
            "tcost icost -0.2020263671875 0.41323021054267883\n",
            "search 0.2113037109375\n",
            "tcost icost -0.140869140625 0.41563576459884644\n",
            "tcost icost -0.1854248046875 0.41115835309028625\n",
            "tcost icost -0.1854248046875 0.4115850329399109\n",
            "search 0.2261962890625\n",
            "tcost icost -0.156005859375 0.41364341974258423\n",
            "tcost icost -0.1968994140625 0.4120582044124603\n",
            "tcost icost -0.1968994140625 0.41153454780578613\n",
            "search 0.2147216796875\n",
            "tcost icost -0.09442138671875 0.4189658761024475\n",
            "tcost icost -0.1988525390625 0.4123401939868927\n",
            "tcost icost -0.1988525390625 0.41174957156181335\n",
            "search 0.2130126953125\n",
            "tcost icost -0.10955810546875 0.41777855157852173\n",
            "tcost icost -0.2027587890625 0.41056787967681885\n",
            "tcost icost -0.2027587890625 0.40965455770492554\n",
            "search 0.2069091796875\n",
            "tcost icost -0.09503173828125 0.41613245010375977\n",
            "tcost icost -0.1866455078125 0.4106992781162262\n",
            "tcost icost -0.1866455078125 0.41062647104263306\n",
            "search 0.2239990234375\n",
            "tcost icost -0.1832275390625 0.409636914730072\n",
            "tcost icost -0.1832275390625 0.4105207622051239\n",
            "tcost icost -0.1832275390625 0.40953442454338074\n",
            "search 0.2261962890625\n",
            "tcost icost -0.1439208984375 0.41118955612182617\n",
            "tcost icost -0.1978759765625 0.4095025062561035\n",
            "tcost icost -0.1978759765625 0.40825361013412476\n",
            "search 0.2103271484375\n",
            "tcost icost -0.10552978515625 0.41465088725090027\n",
            "tcost icost -0.2164306640625 0.4082149267196655\n",
            "tcost icost -0.2164306640625 0.40717029571533203\n",
            "search 0.1907958984375\n",
            "tcost icost -0.1507568359375 0.4132763743400574\n",
            "tcost icost -0.20703125 0.40889501571655273\n",
            "tcost icost -0.20703125 0.4085806608200073\n",
            "search 0.20166015625\n",
            "tcost icost -0.1280517578125 0.4106845557689667\n",
            "tcost icost -0.1832275390625 0.40681666135787964\n",
            "tcost icost -0.1832275390625 0.40687140822410583\n",
            "search 0.2237548828125\n",
            "tcost icost -0.12548828125 0.4095393419265747\n",
            "tcost icost -0.1798095703125 0.4055636525154114\n",
            "tcost icost -0.1798095703125 0.40540698170661926\n",
            "search 0.2257080078125\n",
            "tcost icost -0.15869140625 0.4080483913421631\n",
            "tcost icost -0.2012939453125 0.40579739212989807\n",
            "tcost icost -0.2012939453125 0.40439972281455994\n",
            "search 0.2030029296875\n",
            "tcost icost -0.1453857421875 0.408385694026947\n",
            "tcost icost -0.204833984375 0.4042174220085144\n",
            "tcost icost -0.204833984375 0.40606364607810974\n",
            "search 0.201171875\n",
            "tcost icost -0.157470703125 0.408572256565094\n",
            "tcost icost -0.21484375 0.40450409054756165\n",
            "tcost icost -0.21484375 0.40451234579086304\n",
            "search 0.189697265625\n",
            "tcost icost -0.1585693359375 0.4070581793785095\n",
            "tcost icost -0.2196044921875 0.40408456325531006\n",
            "tcost icost -0.2196044921875 0.4035360813140869\n",
            "search 0.1839599609375\n",
            "tcost icost -0.111328125 0.4079919159412384\n",
            "tcost icost -0.1866455078125 0.4030023515224457\n",
            "tcost icost -0.1866455078125 0.4030144512653351\n",
            "search 0.2164306640625\n",
            "tcost icost -0.201904296875 0.4055399000644684\n",
            "tcost icost -0.2235107421875 0.4038114845752716\n",
            "tcost icost -0.2235107421875 0.40319955348968506\n",
            "search 0.1798095703125\n",
            "tcost icost -0.1842041015625 0.4045400619506836\n",
            "tcost icost -0.2066650390625 0.40384814143180847\n",
            "tcost icost -0.2066650390625 0.40310922265052795\n",
            "search 0.1964111328125\n",
            "tcost icost -0.17236328125 0.40752458572387695\n",
            "tcost icost -0.222900390625 0.4043810963630676\n",
            "tcost icost -0.222900390625 0.4045800566673279\n",
            "search 0.181640625\n",
            "tcost icost -0.142333984375 0.40753766894340515\n",
            "tcost icost -0.203369140625 0.4036101996898651\n",
            "tcost icost -0.203369140625 0.4027857184410095\n",
            "search 0.199462890625\n",
            "tcost icost -0.136474609375 0.40715426206588745\n",
            "tcost icost -0.191650390625 0.40373462438583374\n",
            "tcost icost -0.191650390625 0.4028935730457306\n",
            "search 0.211181640625\n",
            "tcost icost -0.1468505859375 0.4098009169101715\n",
            "tcost icost -0.2281494140625 0.40301838517189026\n",
            "tcost icost -0.2281494140625 0.4024601876735687\n",
            "search 0.1741943359375\n",
            "tcost icost -0.1646728515625 0.40537160634994507\n",
            "tcost icost -0.225341796875 0.40174710750579834\n",
            "tcost icost -0.225341796875 0.40178585052490234\n",
            "search 0.176513671875\n",
            "tcost icost -0.1541748046875 0.406399130821228\n",
            "tcost icost -0.2135009765625 0.40142905712127686\n",
            "tcost icost -0.2135009765625 0.4021649658679962\n",
            "search 0.1885986328125\n",
            "tcost icost -0.220458984375 0.4021865725517273\n",
            "tcost icost -0.220458984375 0.40143176913261414\n",
            "tcost icost -0.220458984375 0.40208396315574646\n",
            "search 0.181640625\n",
            "tcost icost -0.1591796875 0.4061478078365326\n",
            "tcost icost -0.223388671875 0.4015459716320038\n",
            "tcost icost -0.223388671875 0.4024885296821594\n",
            "search 0.17919921875\n",
            "tcost icost -0.1416015625 0.40526866912841797\n",
            "tcost icost -0.204833984375 0.4022916257381439\n",
            "tcost icost -0.204833984375 0.4023519456386566\n",
            "search 0.197509765625\n",
            "tcost icost -0.1119384765625 0.4065406620502472\n",
            "tcost icost -0.2105712890625 0.40023478865623474\n",
            "tcost icost -0.2105712890625 0.4000825881958008\n",
            "search 0.1895751953125\n",
            "tcost icost -0.1181640625 0.4062395989894867\n",
            "tcost icost -0.198974609375 0.4012010991573334\n",
            "tcost icost -0.198974609375 0.4022553563117981\n",
            "search 0.203369140625\n",
            "tcost icost -0.1700439453125 0.4005349278450012\n",
            "tcost icost -0.18994140625 0.40033841133117676\n",
            "tcost icost -0.18994140625 0.39933377504348755\n",
            "search 0.20947265625\n",
            "tcost icost -0.1549072265625 0.40285927057266235\n",
            "tcost icost -0.174560546875 0.399959534406662\n",
            "tcost icost -0.174560546875 0.39968371391296387\n",
            "search 0.22509765625\n",
            "ded\n",
            "time\n",
            "21 #### train ####\n",
            "repr, std, cov, closslb 0.004046579822897911 0.4765625 0.00011088489554822445 0.008533292450010777 0.00012015362153761089\n",
            "0.009199120796416022 0.027485382916053548 1.0\n",
            "repr, std, cov, closslb 0.002671221736818552 0.475830078125 0.0001506558619439602 0.007292586378753185 0.014570031315088272\n",
            "0.009134983755424842 0.0275128682989696 1.0\n",
            "repr, std, cov, closslb 0.0029351229313760996 0.4775390625 0.00010273698717355728 0.004283816087990999 0.01510640699416399\n",
            "0.009125857897527316 0.027321046275873284 1.0\n",
            "repr, std, cov, closslb 0.0061668879352509975 0.47509765625 0.00016136490739881992 0.011158337816596031 0.0006598160252906382\n",
            "0.009107633522848099 0.027076381811989066 1.0\n",
            "repr, std, cov, closslb 0.007118141744285822 0.4736328125 0.000185021897777915 0.017671111971139908 0.00040262730908580124\n",
            "0.008945243196337304 0.026224073306107855 1.0\n",
            "repr, std, cov, closslb 0.004312531556934118 0.473388671875 0.00021092360839247704 0.007842183113098145 0.0003918962902389467\n",
            "0.008900650846545097 0.025756494563952872 1.0\n",
            "repr, std, cov, closslb 0.0035021286457777023 0.474365234375 0.00019593210890889168 0.011159456335008144 0.00029073836049064994\n",
            "0.008812131922079221 0.024970679112511524 1.0\n",
            "repr, std, cov, closslb 0.0037380228750407696 0.474609375 0.000203403877094388 1.67317884915974e-05 0.00021115882555022836\n",
            "0.008741951048897002 0.024525448556834868 1.0\n",
            "repr, std, cov, closslb 0.00904854852706194 0.475341796875 0.00019526458345353603 0.010675708763301373 0.01696264185011387\n",
            "0.008698372117560808 0.023896315560458084 1.0\n",
            "repr, std, cov, closslb 0.012344637885689735 0.475830078125 0.00017551914788782597 0.010779151692986488 0.00032262440072372556\n",
            "0.008776971339775425 0.02404005237683912 1.0\n",
            "repr, std, cov, closslb 0.004322373308241367 0.4765625 0.0001200125552713871 0.003268169006332755 0.026766160503029823\n",
            "0.008820944054001299 0.024821378026879826 1.0\n",
            "repr, std, cov, closslb 0.006093362346291542 0.476318359375 0.00012277532368898392 0.00782328937202692 0.0027406923472881317\n",
            "0.008865137071965394 0.02514599912540871 1.0\n",
            "repr, std, cov, closslb 0.003421070287004113 0.474365234375 0.0002710777334868908 1.9223234630771913e-05 0.00020514216157607734\n",
            "0.008715777560168045 0.024354455074497988 1.0\n",
            "repr, std, cov, closslb 0.003221368882805109 0.475830078125 0.0002072267234325409 2.3147949832491577e-05 0.0003484257613308728\n",
            "0.008655010428719538 0.023992044296202428 1.0\n",
            "repr, std, cov, closslb 0.003725007176399231 0.478271484375 0.000128971878439188 0.0045365397818386555 0.0006703527760691941\n",
            "0.008759443692945844 0.024722340232992388 1.0\n",
            "repr, std, cov, closslb 0.0037534553557634354 0.478271484375 0.00010558357462286949 1.0355541235185228e-05 0.014167857356369495\n",
            "0.008768203136638788 0.025171145124534115 1.0\n",
            "repr, std, cov, closslb 0.006124985869973898 0.478271484375 0.00010378379374742508 0.008354313671588898 0.008703842759132385\n",
            "0.008891759087457641 0.025989232415626833 1.0\n",
            "repr, std, cov, closslb 0.00419189315289259 0.47900390625 8.296594023704529e-05 0.004323991946876049 0.01565534435212612\n",
            "0.008909551497391642 0.026461036238631525 1.0\n",
            "repr, std, cov, closslb 0.0029121306724846363 0.479248046875 7.974216714501381e-05 0.0041062659583985806 0.00012941815657541156\n",
            "0.008927379509937921 0.027076381811989066 1.0\n",
            "repr, std, cov, closslb 0.0023020256776362658 0.477783203125 0.00012617232277989388 4.480433744902257e-06 0.028112024068832397\n",
            "0.008900650846545097 0.027293752523349937 1.0\n",
            "repr, std, cov, closslb 0.0033039606641978025 0.477783203125 0.00011429516598582268 0.004003614652901888 0.014571934938430786\n",
            "0.008847433357816405 0.02773374314832177 1.0\n",
            "repr, std, cov, closslb 0.00579023826867342 0.476318359375 0.00016642757691442966 0.011856873519718647 0.0003571729175746441\n",
            "0.008759443692945844 0.02773374314832177 1.0\n",
            "repr, std, cov, closslb 0.004070877097547054 0.476318359375 0.00017084553837776184 0.0037199407815933228 0.00022811499366071075\n",
            "0.00856037407799669 0.027184849905860488 1.0\n",
            "repr, std, cov, closslb 0.002665645442903042 0.47705078125 0.00012317951768636703 0.00410926528275013 0.0001648682082304731\n",
            "0.00852621801451714 0.027375715689471298 1.0\n",
            "repr, std, cov, closslb 0.0027307081036269665 0.4755859375 0.0001764623448252678 0.003381720744073391 4.695009920396842e-05\n",
            "0.008315808819388084 0.026673466921227015 1.0\n",
            "repr, std, cov, closslb 0.003998648840934038 0.475341796875 0.00016188900917768478 0.003751985263079405 1.4598210327676497e-05\n",
            "0.008233106277130538 0.026302824224470164 1.0\n",
            "repr, std, cov, closslb 0.007765226997435093 0.476318359375 0.00015350570902228355 0.03019256517291069 0.015100860968232155\n",
            "0.008110592082312467 0.025730763800152722 1.0\n",
            "repr, std, cov, closslb 0.004773006308823824 0.475830078125 0.0001473084557801485 0.007779749110341072 7.595105125801638e-05\n",
            "0.008094395197522228 0.02585967518422779 1.0\n",
            "repr, std, cov, closslb 0.002397964708507061 0.476806640625 0.00013646436855196953 1.0383588232798502e-05 0.00016127520939335227\n",
            "0.008134948198446242 0.026145558168781453 1.0\n",
            "repr, std, cov, closslb 0.002609224757179618 0.476318359375 0.00013839313760399818 9.475609658693429e-06 0.0154102249071002\n",
            "0.008183880074680927 0.02654049875691716 1.0\n",
            "repr, std, cov, closslb 0.002605163026601076 0.4765625 0.00015903310850262642 2.75004258583067e-05 0.007574700750410557\n",
            "0.008183880074680927 0.02662019990122467 1.0\n",
            "repr, std, cov, closslb 0.0030621367041021585 0.476806640625 0.00011738738976418972 0.00392943574115634 3.318157541798428e-05\n",
            "0.008183880074680927 0.02688760300925339 1.0\n",
            "repr, std, cov, closslb 0.0026108133606612682 0.475830078125 0.00014111120253801346 5.0018217734759673e-05 0.01564818248152733\n",
            "0.00811870267439478 0.026434601636994532 1.0\n",
            "repr, std, cov, closslb 0.002595361554995179 0.475830078125 0.00014500459656119347 3.3479693229310215e-05 0.0001022229262162\n",
            "0.00810248959271975 0.026329127048694632 1.0\n",
            "repr, std, cov, closslb 0.0022832429967820644 0.475341796875 0.0001659933477640152 3.029014987987466e-05 0.000189912534551695\n",
            "0.008045998356367975 0.025730763800152722 1.0\n",
            "repr, std, cov, closslb 0.0025835144333541393 0.474853515625 0.00018184795044362545 7.1818581091065425e-06 8.06372772785835e-05\n",
            "0.00798990098164947 0.025322550066258786 1.0\n",
            "tcost icost -0.33349609375 0.4074966609477997\n",
            "tcost icost -0.410400390625 0.4051245450973511\n",
            "tcost icost -0.410400390625 0.4043504297733307\n",
            "search -0.006103515625\n",
            "tcost icost -0.36767578125 0.4058453440666199\n",
            "tcost icost -0.410400390625 0.40362781286239624\n",
            "tcost icost -0.410400390625 0.4045637547969818\n",
            "search -0.005859375\n",
            "tcost icost -0.252197265625 0.40930330753326416\n",
            "tcost icost -0.410400390625 0.4034983217716217\n",
            "tcost icost -0.410400390625 0.4035428762435913\n",
            "search -0.0068359375\n",
            "tcost icost -0.33349609375 0.40552613139152527\n",
            "tcost icost -0.410400390625 0.4022190570831299\n",
            "tcost icost -0.410400390625 0.4027567505836487\n",
            "search -0.007568359375\n",
            "tcost icost -0.33349609375 0.40521439909935\n",
            "tcost icost -0.410400390625 0.4030614495277405\n",
            "tcost icost -0.410400390625 0.4023161232471466\n",
            "search -0.008056640625\n",
            "tcost icost -0.3330078125 0.4048318564891815\n",
            "tcost icost -0.4111328125 0.4014230966567993\n",
            "tcost icost -0.4111328125 0.40116482973098755\n",
            "search -0.010009765625\n",
            "tcost icost -0.333984375 0.40559297800064087\n",
            "tcost icost -0.412841796875 0.4018005132675171\n",
            "tcost icost -0.412841796875 0.402099072933197\n",
            "search -0.0107421875\n",
            "tcost icost -0.376220703125 0.4024221897125244\n",
            "tcost icost -0.410400390625 0.40214288234710693\n",
            "tcost icost -0.410400390625 0.4018684923648834\n",
            "search -0.008544921875\n",
            "tcost icost -0.333740234375 0.40405210852622986\n",
            "tcost icost -0.41357421875 0.4002173840999603\n",
            "tcost icost -0.41357421875 0.4012068808078766\n",
            "search -0.012451171875\n",
            "tcost icost -0.37744140625 0.40175312757492065\n",
            "tcost icost -0.413818359375 0.4000113904476166\n",
            "tcost icost -0.413818359375 0.40005818009376526\n",
            "search -0.013671875\n",
            "tcost icost -0.395751953125 0.4014531970024109\n",
            "tcost icost -0.418701171875 0.3993930220603943\n",
            "tcost icost -0.418701171875 0.40045616030693054\n",
            "search -0.018310546875\n",
            "tcost icost -0.33642578125 0.4030705392360687\n",
            "tcost icost -0.420654296875 0.4005815088748932\n",
            "tcost icost -0.420654296875 0.39907214045524597\n",
            "search -0.021484375\n",
            "tcost icost -0.279541015625 0.4049097001552582\n",
            "tcost icost -0.4228515625 0.39987310767173767\n",
            "tcost icost -0.4228515625 0.3994162976741791\n",
            "search -0.0234375\n",
            "tcost icost -0.343017578125 0.4026137888431549\n",
            "tcost icost -0.435302734375 0.398410439491272\n",
            "tcost icost -0.435302734375 0.3974560797214508\n",
            "search -0.037841796875\n",
            "tcost icost -0.339111328125 0.40237942337989807\n",
            "tcost icost -0.428466796875 0.3977331221103668\n",
            "tcost icost -0.428466796875 0.3979741930961609\n",
            "search -0.030517578125\n",
            "tcost icost -0.308349609375 0.40386316180229187\n",
            "tcost icost -0.423828125 0.39885661005973816\n",
            "tcost icost -0.423828125 0.39774662256240845\n",
            "search -0.026123046875\n",
            "tcost icost -0.310546875 0.40318337082862854\n",
            "tcost icost -0.42578125 0.3977378010749817\n",
            "tcost icost -0.42578125 0.39652663469314575\n",
            "search -0.029296875\n",
            "tcost icost -0.3349609375 0.40148311853408813\n",
            "tcost icost -0.427734375 0.3964482247829437\n",
            "tcost icost -0.427734375 0.39758211374282837\n",
            "search -0.0302734375\n",
            "tcost icost -0.343994140625 0.40150943398475647\n",
            "tcost icost -0.427978515625 0.3975852429866791\n",
            "tcost icost -0.427978515625 0.3966635465621948\n",
            "search -0.03125\n",
            "tcost icost -0.3759765625 0.39882057905197144\n",
            "tcost icost -0.423828125 0.39712515473365784\n",
            "tcost icost -0.423828125 0.39798155426979065\n",
            "search -0.02587890625\n",
            "tcost icost -0.37548828125 0.4002663195133209\n",
            "tcost icost -0.427734375 0.39668312668800354\n",
            "tcost icost -0.427734375 0.3976750671863556\n",
            "search -0.030029296875\n",
            "tcost icost -0.33349609375 0.40151849389076233\n",
            "tcost icost -0.439697265625 0.395908921957016\n",
            "tcost icost -0.439697265625 0.39683255553245544\n",
            "search -0.04296875\n",
            "tcost icost -0.309814453125 0.40373629331588745\n",
            "tcost icost -0.432373046875 0.39796772599220276\n",
            "tcost icost -0.432373046875 0.3988475501537323\n",
            "search -0.033447265625\n",
            "tcost icost -0.333984375 0.40239816904067993\n",
            "tcost icost -0.42724609375 0.39800718426704407\n",
            "tcost icost -0.42724609375 0.39726072549819946\n",
            "search -0.030029296875\n",
            "tcost icost -0.305908203125 0.40309038758277893\n",
            "tcost icost -0.430908203125 0.3965319097042084\n",
            "tcost icost -0.430908203125 0.39603662490844727\n",
            "search -0.034912109375\n",
            "tcost icost -0.251953125 0.404477059841156\n",
            "tcost icost -0.428955078125 0.39706429839134216\n",
            "tcost icost -0.428955078125 0.39681628346443176\n",
            "search -0.0322265625\n",
            "tcost icost -0.3310546875 0.400723934173584\n",
            "tcost icost -0.424072265625 0.3960026502609253\n",
            "tcost icost -0.424072265625 0.39513280987739563\n",
            "search -0.029052734375\n",
            "tcost icost -0.2320556640625 0.4039130210876465\n",
            "tcost icost -0.44091796875 0.3952036499977112\n",
            "tcost icost -0.44091796875 0.3958592414855957\n",
            "search -0.045166015625\n",
            "tcost icost -0.3154296875 0.40184253454208374\n",
            "tcost icost -0.40576171875 0.39712801575660706\n",
            "tcost icost -0.40576171875 0.398772269487381\n",
            "search -0.007080078125\n",
            "tcost icost -0.32177734375 0.40027156472206116\n",
            "tcost icost -0.407958984375 0.39696112275123596\n",
            "tcost icost -0.407958984375 0.3968491852283478\n",
            "search -0.01123046875\n",
            "tcost icost -0.271484375 0.4003453850746155\n",
            "tcost icost -0.42724609375 0.39536091685295105\n",
            "tcost icost -0.42724609375 0.393846333026886\n",
            "search -0.033447265625\n",
            "tcost icost -0.384033203125 0.39697134494781494\n",
            "tcost icost -0.42529296875 0.39477378129959106\n",
            "tcost icost -0.42529296875 0.3957974910736084\n",
            "search -0.029541015625\n",
            "tcost icost -0.248046875 0.40182241797447205\n",
            "tcost icost -0.417724609375 0.3961256444454193\n",
            "tcost icost -0.417724609375 0.3967517912387848\n",
            "search -0.02099609375\n",
            "tcost icost -0.3740234375 0.3977836072444916\n",
            "tcost icost -0.418701171875 0.3964218497276306\n",
            "tcost icost -0.418701171875 0.39603254199028015\n",
            "search -0.022705078125\n",
            "tcost icost -0.34716796875 0.39864209294319153\n",
            "tcost icost -0.435546875 0.3944567143917084\n",
            "tcost icost -0.435546875 0.3948369026184082\n",
            "search -0.040771484375\n",
            "tcost icost -0.39501953125 0.39603421092033386\n",
            "tcost icost -0.44140625 0.39347973465919495\n",
            "tcost icost -0.44140625 0.39418578147888184\n",
            "search -0.047119140625\n",
            "tcost icost -0.322021484375 0.39841145277023315\n",
            "tcost icost -0.456787109375 0.3924099802970886\n",
            "tcost icost -0.456787109375 0.39335542917251587\n",
            "search -0.0634765625\n",
            "tcost icost -0.374755859375 0.3967357575893402\n",
            "tcost icost -0.444580078125 0.3931758403778076\n",
            "tcost icost -0.444580078125 0.3946215808391571\n",
            "search -0.050048828125\n",
            "tcost icost -0.43896484375 0.3949439227581024\n",
            "tcost icost -0.453369140625 0.3924281597137451\n",
            "tcost icost -0.453369140625 0.39198794960975647\n",
            "search -0.061279296875\n",
            "tcost icost -0.462646484375 0.3914355933666229\n",
            "tcost icost -0.462646484375 0.3907785415649414\n",
            "tcost icost -0.462646484375 0.39070600271224976\n",
            "search -0.072021484375\n",
            "tcost icost -0.373046875 0.39501190185546875\n",
            "tcost icost -0.432373046875 0.39163514971733093\n",
            "tcost icost -0.432373046875 0.39219215512275696\n",
            "search -0.040283203125\n",
            "tcost icost -0.326904296875 0.3952510356903076\n",
            "tcost icost -0.44091796875 0.3921073079109192\n",
            "tcost icost -0.44091796875 0.3907974660396576\n",
            "search -0.050048828125\n",
            "tcost icost -0.419921875 0.3917493224143982\n",
            "tcost icost -0.456787109375 0.39085108041763306\n",
            "tcost icost -0.456787109375 0.3905450701713562\n",
            "search -0.066162109375\n",
            "tcost icost -0.413818359375 0.39317482709884644\n",
            "tcost icost -0.44482421875 0.3909473717212677\n",
            "tcost icost -0.44482421875 0.3913174569606781\n",
            "search -0.053466796875\n",
            "tcost icost -0.41748046875 0.39123639464378357\n",
            "tcost icost -0.4443359375 0.39033347368240356\n",
            "tcost icost -0.4443359375 0.39094778895378113\n",
            "search -0.053466796875\n",
            "tcost icost -0.415771484375 0.3913251459598541\n",
            "tcost icost -0.4521484375 0.38996124267578125\n",
            "tcost icost -0.4521484375 0.3912074863910675\n",
            "search -0.06103515625\n",
            "tcost icost -0.376953125 0.39295631647109985\n",
            "tcost icost -0.442138671875 0.3894483149051666\n",
            "tcost icost -0.442138671875 0.3897762894630432\n",
            "search -0.05224609375\n",
            "tcost icost -0.34326171875 0.3929022550582886\n",
            "tcost icost -0.4365234375 0.3907558023929596\n",
            "tcost icost -0.4365234375 0.3904554545879364\n",
            "search -0.046142578125\n",
            "tcost icost -0.383056640625 0.3927253186702728\n",
            "tcost icost -0.457275390625 0.38791424036026\n",
            "tcost icost -0.457275390625 0.38808441162109375\n",
            "search -0.069091796875\n",
            "tcost icost -0.39111328125 0.39187291264533997\n",
            "tcost icost -0.464599609375 0.38706645369529724\n",
            "tcost icost -0.464599609375 0.389157772064209\n",
            "search -0.075439453125\n",
            "tcost icost -0.39013671875 0.3935009241104126\n",
            "tcost icost -0.4658203125 0.3866632282733917\n",
            "tcost icost -0.4658203125 0.38669875264167786\n",
            "search -0.0791015625\n",
            "tcost icost -0.4150390625 0.3898577094078064\n",
            "tcost icost -0.443603515625 0.3885657489299774\n",
            "tcost icost -0.443603515625 0.38869690895080566\n",
            "search -0.054931640625\n",
            "tcost icost -0.335205078125 0.3935043513774872\n",
            "tcost icost -0.46044921875 0.3876381814479828\n",
            "tcost icost -0.46044921875 0.3881121575832367\n",
            "search -0.072265625\n",
            "tcost icost -0.338134765625 0.393451452255249\n",
            "tcost icost -0.455078125 0.387815922498703\n",
            "tcost icost -0.455078125 0.38831961154937744\n",
            "search -0.066650390625\n",
            "tcost icost -0.365478515625 0.39052534103393555\n",
            "tcost icost -0.43603515625 0.38801315426826477\n",
            "tcost icost -0.43603515625 0.38661062717437744\n",
            "search -0.04931640625\n",
            "tcost icost -0.37060546875 0.38935521245002747\n",
            "tcost icost -0.4365234375 0.38698577880859375\n",
            "tcost icost -0.4365234375 0.38605183362960815\n",
            "search -0.050537109375\n",
            "tcost icost -0.3623046875 0.39013975858688354\n",
            "tcost icost -0.435302734375 0.3860304057598114\n",
            "tcost icost -0.435302734375 0.38698846101760864\n",
            "search -0.04833984375\n",
            "tcost icost -0.39453125 0.3894079923629761\n",
            "tcost icost -0.4365234375 0.38600051403045654\n",
            "tcost icost -0.4365234375 0.38516658544540405\n",
            "search -0.05126953125\n",
            "tcost icost -0.413330078125 0.3873009979724884\n",
            "tcost icost -0.44140625 0.3857642710208893\n",
            "tcost icost -0.44140625 0.38714900612831116\n",
            "search -0.05419921875\n",
            "tcost icost -0.2978515625 0.391050785779953\n",
            "tcost icost -0.442138671875 0.3847682476043701\n",
            "tcost icost -0.442138671875 0.3876238465309143\n",
            "search -0.054443359375\n",
            "tcost icost -0.44287109375 0.3855101466178894\n",
            "tcost icost -0.447998046875 0.38488098978996277\n",
            "tcost icost -0.447998046875 0.3841164708137512\n",
            "search -0.06396484375\n",
            "tcost icost -0.38134765625 0.3878326714038849\n",
            "tcost icost -0.44189453125 0.3844146430492401\n",
            "tcost icost -0.44189453125 0.3850289583206177\n",
            "search -0.056884765625\n",
            "tcost icost -0.427001953125 0.3851805329322815\n",
            "tcost icost -0.4599609375 0.3834104835987091\n",
            "tcost icost -0.4599609375 0.3823092579841614\n",
            "search -0.07763671875\n",
            "tcost icost -0.37548828125 0.3878486752510071\n",
            "tcost icost -0.441162109375 0.38426655530929565\n",
            "tcost icost -0.441162109375 0.3837660849094391\n",
            "search -0.057373046875\n",
            "tcost icost -0.41943359375 0.3849828839302063\n",
            "tcost icost -0.45556640625 0.38243719935417175\n",
            "tcost icost -0.45556640625 0.38286858797073364\n",
            "search -0.07275390625\n",
            "tcost icost -0.4111328125 0.38448193669319153\n",
            "tcost icost -0.45166015625 0.38294273614883423\n",
            "tcost icost -0.45166015625 0.3847344219684601\n",
            "search -0.06689453125\n",
            "tcost icost -0.447265625 0.3838387727737427\n",
            "tcost icost -0.45947265625 0.383741557598114\n",
            "tcost icost -0.45947265625 0.3825586438179016\n",
            "search -0.076904296875\n",
            "tcost icost -0.352783203125 0.385668009519577\n",
            "tcost icost -0.430908203125 0.3829385042190552\n",
            "tcost icost -0.430908203125 0.3827340006828308\n",
            "search -0.048095703125\n",
            "tcost icost -0.36865234375 0.38515180349349976\n",
            "tcost icost -0.4638671875 0.38132792711257935\n",
            "tcost icost -0.4638671875 0.38048210740089417\n",
            "search -0.08349609375\n",
            "tcost icost -0.333984375 0.3873943090438843\n",
            "tcost icost -0.4521484375 0.38229459524154663\n",
            "tcost icost -0.4521484375 0.38232237100601196\n",
            "search -0.06982421875\n",
            "tcost icost -0.462158203125 0.3820347785949707\n",
            "tcost icost -0.462158203125 0.38177725672721863\n",
            "tcost icost -0.462158203125 0.3819178640842438\n",
            "search -0.080322265625\n",
            "tcost icost -0.287353515625 0.389072060585022\n",
            "tcost icost -0.455078125 0.38146910071372986\n",
            "tcost icost -0.455078125 0.3839472830295563\n",
            "search -0.071044921875\n",
            "tcost icost -0.451171875 0.38320860266685486\n",
            "tcost icost -0.451171875 0.382785826921463\n",
            "tcost icost -0.451171875 0.38266703486442566\n",
            "search -0.068603515625\n",
            "tcost icost -0.258056640625 0.3909751772880554\n",
            "tcost icost -0.440673828125 0.3835379183292389\n",
            "tcost icost -0.440673828125 0.3840091824531555\n",
            "search -0.056640625\n",
            "tcost icost -0.272705078125 0.38904955983161926\n",
            "tcost icost -0.440673828125 0.3830983340740204\n",
            "tcost icost -0.440673828125 0.382598876953125\n",
            "search -0.05810546875\n",
            "tcost icost -0.339599609375 0.38575732707977295\n",
            "tcost icost -0.433837890625 0.3833312690258026\n",
            "tcost icost -0.433837890625 0.38226017355918884\n",
            "search -0.051513671875\n",
            "tcost icost -0.2734375 0.389404296875\n",
            "tcost icost -0.4462890625 0.38168519735336304\n",
            "tcost icost -0.4462890625 0.3823702931404114\n",
            "search -0.06396484375\n",
            "tcost icost -0.270751953125 0.3870600163936615\n",
            "tcost icost -0.4375 0.381826251745224\n",
            "tcost icost -0.4375 0.38161128759384155\n",
            "search -0.055908203125\n",
            "tcost icost -0.386962890625 0.3838540315628052\n",
            "tcost icost -0.436279296875 0.38219109177589417\n",
            "tcost icost -0.436279296875 0.38251572847366333\n",
            "search -0.0537109375\n",
            "tcost icost -0.430908203125 0.3815569281578064\n",
            "tcost icost -0.430908203125 0.3816460371017456\n",
            "tcost icost -0.430908203125 0.381940096616745\n",
            "search -0.049072265625\n",
            "tcost icost -0.336669921875 0.38536980748176575\n",
            "tcost icost -0.4375 0.3822183310985565\n",
            "tcost icost -0.4375 0.38199013471603394\n",
            "search -0.055419921875\n",
            "tcost icost -0.337890625 0.3864457905292511\n",
            "tcost icost -0.438232421875 0.3816935122013092\n",
            "tcost icost -0.438232421875 0.38199612498283386\n",
            "search -0.05615234375\n",
            "tcost icost -0.3310546875 0.3854123055934906\n",
            "tcost icost -0.43359375 0.3817223310470581\n",
            "tcost icost -0.43359375 0.3811728358268738\n",
            "search -0.052490234375\n",
            "tcost icost -0.388427734375 0.3835161030292511\n",
            "tcost icost -0.451416015625 0.38071078062057495\n",
            "tcost icost -0.451416015625 0.38101187348365784\n",
            "search -0.0703125\n",
            "tcost icost -0.326416015625 0.386261522769928\n",
            "tcost icost -0.448486328125 0.38122081756591797\n",
            "tcost icost -0.448486328125 0.3809545040130615\n",
            "search -0.067626953125\n",
            "tcost icost -0.318115234375 0.3873301148414612\n",
            "tcost icost -0.427490234375 0.383059561252594\n",
            "tcost icost -0.427490234375 0.38305988907814026\n",
            "search -0.04443359375\n",
            "tcost icost -0.2666015625 0.38850802183151245\n",
            "tcost icost -0.438720703125 0.38199836015701294\n",
            "tcost icost -0.438720703125 0.38145580887794495\n",
            "search -0.057373046875\n",
            "tcost icost -0.413330078125 0.38231611251831055\n",
            "tcost icost -0.443603515625 0.38143840432167053\n",
            "tcost icost -0.443603515625 0.38090643286705017\n",
            "search -0.062744140625\n",
            "tcost icost -0.3359375 0.3865946829319\n",
            "tcost icost -0.452392578125 0.38049209117889404\n",
            "tcost icost -0.452392578125 0.38189366459846497\n",
            "search -0.070556640625\n",
            "tcost icost -0.333984375 0.38679730892181396\n",
            "tcost icost -0.447265625 0.38074740767478943\n",
            "tcost icost -0.447265625 0.38267478346824646\n",
            "search -0.064697265625\n",
            "tcost icost -0.31396484375 0.3891909718513489\n",
            "tcost icost -0.453857421875 0.3824160397052765\n",
            "tcost icost -0.453857421875 0.3821704089641571\n",
            "search -0.07177734375\n",
            "ded\n",
            "time\n",
            "22 #### train ####\n",
            "repr, std, cov, closslb 0.004492983687669039 0.4755859375 0.0001507760025560856 0.0035937943030148745 0.01453033834695816\n",
            "0.00793419472252793 0.024747062573225377 1.0\n",
            "repr, std, cov, closslb 0.004802734591066837 0.475341796875 0.00014112680219113827 0.004179793410003185 0.00046169216511771083\n",
            "0.007918350103969888 0.024599098503376483 1.0\n",
            "repr, std, cov, closslb 0.003378499299287796 0.475341796875 0.0001657954417169094 4.175023059360683e-05 0.014838876202702522\n",
            "0.007918350103969888 0.02423304726524745 1.0\n",
            "repr, std, cov, closslb 0.006416471675038338 0.4755859375 0.00013790116645395756 0.0109061598777771 0.0001421052438672632\n",
            "0.007997890882631118 0.02423304726524745 1.0\n",
            "repr, std, cov, closslb 0.0036080493591725826 0.4755859375 0.00014750496484339237 4.049357812618837e-05 0.014286178164184093\n",
            "0.00810248959271975 0.024184653773047585 1.0\n",
            "repr, std, cov, closslb 0.005645255092531443 0.4755859375 0.00013115210458636284 0.004311899188905954 6.13777810940519e-05\n",
            "0.008200256018710363 0.024257280312512695 1.0\n",
            "repr, std, cov, closslb 0.007475211750715971 0.47705078125 0.00011232984252274036 0.009025807492434978 0.01528556551784277\n",
            "0.008266088133817375 0.024599098503376483 1.0\n",
            "repr, std, cov, closslb 0.006884289439767599 0.4765625 0.00010969815775752068 0.023277025669813156 0.0003887568600475788\n",
            "0.008249580722791073 0.0245499740053917 1.0\n",
            "repr, std, cov, closslb 0.006983977742493153 0.47509765625 0.00016288552433252335 0.0182130578905344 0.015286334790289402\n",
            "0.008224881395734804 0.024378809529572484 1.0\n",
            "repr, std, cov, closslb 0.004561740905046463 0.4755859375 0.00015572342090308666 0.004687740467488766 0.030990948900580406\n",
            "0.008192063954755607 0.024525448556834868 1.0\n",
            "repr, std, cov, closslb 0.003966422751545906 0.476806640625 0.00012035202234983444 0.004545988980680704 0.014127025380730629\n",
            "0.008224881395734804 0.025196316269658646 1.0\n",
            "repr, std, cov, closslb 0.003968015778809786 0.476806640625 0.00012537185102701187 0.00515331793576479 0.00011165779869770631\n",
            "0.00828262857617314 0.02565372563641286 1.0\n",
            "repr, std, cov, closslb 0.004830010235309601 0.477294921875 0.0001227955799549818 0.0040960353799164295 0.0003843970480374992\n",
            "0.008357471104772888 0.026302824224470164 1.0\n",
            "repr, std, cov, closslb 0.011600019410252571 0.4775390625 0.00013156072236597538 0.007438776083290577 0.015201348811388016\n",
            "0.00836582857587766 0.026408193443550984 1.0\n",
            "repr, std, cov, closslb 0.007032359950244427 0.47705078125 0.00011713220737874508 0.008144429884850979 0.0005563881131820381\n",
            "0.008399342118624302 0.02688760300925339 1.0\n",
            "repr, std, cov, closslb 0.0057830121368169785 0.477294921875 0.00013877591118216515 0.004320004489272833 0.00022776609694119543\n",
            "0.008324124628207471 0.02683390835862778 1.0\n",
            "repr, std, cov, closslb 0.004973018541932106 0.476806640625 0.0001572777982801199 0.003798343939706683 0.010773302055895329\n",
            "0.008200256018710363 0.026593606294929743 1.0\n",
            "repr, std, cov, closslb 0.005220476537942886 0.4765625 0.00016471510753035545 0.003962721209973097 0.0001748077047523111\n",
            "0.00811870267439478 0.026329127048694632 1.0\n",
            "repr, std, cov, closslb 0.005993735045194626 0.47607421875 0.00017351540736854076 0.004047622438520193 9.605701052350923e-05\n",
            "0.008021908556949548 0.02619787543067718 1.0\n",
            "repr, std, cov, closslb 0.004255231469869614 0.47607421875 0.000185755779966712 4.8226662329398096e-05 0.0031362317968159914\n",
            "0.008005888773513749 0.026329127048694632 1.0\n",
            "repr, std, cov, closslb 0.005379647947847843 0.477294921875 0.0001357251312583685 0.004605891648679972 0.0005326720420271158\n",
            "0.008062098399079065 0.02713056165199485 1.0\n",
            "repr, std, cov, closslb 0.003697468899190426 0.4775390625 0.00011748098768293858 6.42200029687956e-06 0.015399542637169361\n",
            "0.008086308888633596 0.027457924991062487 1.0\n",
            "repr, std, cov, closslb 0.003596336580812931 0.47705078125 0.00014092307537794113 0.003419162007048726 0.03144950792193413\n",
            "0.00807823065797562 0.027844844634336643 1.0\n",
            "repr, std, cov, closslb 0.0029999869875609875 0.47802734375 0.00011257175356149673 1.0299952009518165e-05 0.01544342003762722\n",
            "0.008086308888633596 0.028237216503531004 1.0\n",
            "repr, std, cov, closslb 0.003457501297816634 0.477294921875 0.00011594290845096111 0.009486781433224678 0.0005882927216589451\n",
            "0.008013894662287262 0.027872689478970977 1.0\n",
            "repr, std, cov, closslb 0.0054714917205274105 0.476318359375 0.00013078446500003338 0.0035949749872088432 0.00031670357566326857\n",
            "0.00793419472252793 0.027678358752458106 1.0\n",
            "repr, std, cov, closslb 0.004364726599305868 0.47607421875 0.00015205563977360725 0.007575691677629948 0.029942240566015244\n",
            "0.007855287416146665 0.02713056165199485 1.0\n",
            "repr, std, cov, closslb 0.0034446504432708025 0.475341796875 0.0001722564920783043 0.007357654627412558 2.3921989850350656e-05\n",
            "0.007761633830935007 0.02670014038814824 1.0\n",
            "repr, std, cov, closslb 0.0038774320855736732 0.474853515625 0.00017377990297973156 0.008005235344171524 0.0005443611880764365\n",
            "0.0075625304409391225 0.025885534859412015 1.0\n",
            "repr, std, cov, closslb 0.005132904276251793 0.475341796875 0.0001668811310082674 0.004373509902507067 0.00035575442598201334\n",
            "0.007487319417339401 0.025474865711738306 1.0\n",
            "repr, std, cov, closslb 0.002664248924702406 0.474853515625 0.00021945219486951828 4.8424517444800586e-06 0.00018125830683857203\n",
            "0.00736853450135527 0.024846199404906705 1.0\n",
            "repr, std, cov, closslb 0.005551577545702457 0.474609375 0.00018505798652768135 0.008356301113963127 1.2860117749369238e-05\n",
            "0.007375903035856625 0.024895916649915917 1.0\n",
            "repr, std, cov, closslb 0.0026654412504285574 0.476806640625 0.00013939477503299713 2.8949943953193724e-05 4.86630124214571e-05\n",
            "0.007412856383862179 0.024970679112511524 1.0\n",
            "repr, std, cov, closslb 0.002550432924181223 0.47705078125 0.0001151449978351593 1.677275577094406e-05 8.224754128605127e-05\n",
            "0.007398052880049202 0.024970679112511524 1.0\n",
            "repr, std, cov, closslb 0.004171174019575119 0.47607421875 0.00014080526307225227 0.004349469672888517 0.0002793864405248314\n",
            "0.0074948067367567395 0.0252215125859283 1.0\n",
            "repr, std, cov, closslb 0.004354018718004227 0.4765625 0.00012295995838940144 0.003886320861056447 0.0002641913015395403\n",
            "0.007509803845036988 0.025602495043830158 1.0\n",
            "tcost icost -0.78955078125 0.3832520842552185\n",
            "tcost icost -0.8759765625 0.37857070565223694\n",
            "tcost icost -0.8759765625 0.3793528378009796\n",
            "search -0.49658203125\n",
            "tcost icost -0.437744140625 0.3954213559627533\n",
            "tcost icost -0.8759765625 0.3794456720352173\n",
            "tcost icost -0.8759765625 0.3805050253868103\n",
            "search -0.495361328125\n",
            "tcost icost -0.58935546875 0.39060094952583313\n",
            "tcost icost -0.87548828125 0.37904664874076843\n",
            "tcost icost -0.87548828125 0.37758690118789673\n",
            "search -0.497802734375\n",
            "tcost icost -0.53515625 0.3931661546230316\n",
            "tcost icost -0.8759765625 0.38037973642349243\n",
            "tcost icost -0.8759765625 0.37984389066696167\n",
            "search -0.49609375\n",
            "tcost icost -0.58349609375 0.39223819971084595\n",
            "tcost icost -0.87255859375 0.3802954852581024\n",
            "tcost icost -0.87255859375 0.37968260049819946\n",
            "search -0.492919921875\n",
            "tcost icost -0.5361328125 0.39416876435279846\n",
            "tcost icost -0.87646484375 0.3810741603374481\n",
            "tcost icost -0.87646484375 0.38119059801101685\n",
            "search -0.495361328125\n",
            "tcost icost -0.6064453125 0.3932284712791443\n",
            "tcost icost -0.88427734375 0.38093000650405884\n",
            "tcost icost -0.88427734375 0.38149869441986084\n",
            "search -0.5029296875\n",
            "tcost icost -0.451171875 0.39710721373558044\n",
            "tcost icost -0.8876953125 0.380618155002594\n",
            "tcost icost -0.8876953125 0.38097628951072693\n",
            "search -0.5068359375\n",
            "tcost icost -0.619140625 0.39250898361206055\n",
            "tcost icost -0.8896484375 0.38169074058532715\n",
            "tcost icost -0.8896484375 0.3812255561351776\n",
            "search -0.5087890625\n",
            "tcost icost -0.78759765625 0.3862306475639343\n",
            "tcost icost -0.87255859375 0.3824976086616516\n",
            "tcost icost -0.87255859375 0.3820079565048218\n",
            "search -0.490478515625\n",
            "tcost icost -0.55615234375 0.3954246938228607\n",
            "tcost icost -0.8388671875 0.38519078493118286\n",
            "tcost icost -0.8388671875 0.38478657603263855\n",
            "search -0.4541015625\n",
            "tcost icost -0.697265625 0.391325443983078\n",
            "tcost icost -0.85546875 0.38523048162460327\n",
            "tcost icost -0.85546875 0.3848605751991272\n",
            "search -0.470703125\n",
            "tcost icost -0.8759765625 0.3852694630622864\n",
            "tcost icost -0.8759765625 0.3858565390110016\n",
            "tcost icost -0.8759765625 0.3845048248767853\n",
            "search -0.491455078125\n",
            "tcost icost -0.7021484375 0.3941633403301239\n",
            "tcost icost -0.888671875 0.3861674964427948\n",
            "tcost icost -0.888671875 0.3861682116985321\n",
            "search -0.50244140625\n",
            "tcost icost -0.382568359375 0.4043905735015869\n",
            "tcost icost -0.86767578125 0.38700172305107117\n",
            "tcost icost -0.86767578125 0.38746652007102966\n",
            "search -0.480224609375\n",
            "tcost icost -0.5908203125 0.3988837003707886\n",
            "tcost icost -0.859375 0.38756421208381653\n",
            "tcost icost -0.859375 0.3881807327270508\n",
            "search -0.47119140625\n",
            "tcost icost -0.421142578125 0.4043189287185669\n",
            "tcost icost -0.8671875 0.3868943750858307\n",
            "tcost icost -0.8671875 0.3874626159667969\n",
            "search -0.479736328125\n",
            "tcost icost -0.65673828125 0.397569864988327\n",
            "tcost icost -0.8583984375 0.3876388967037201\n",
            "tcost icost -0.8583984375 0.38903623819351196\n",
            "search -0.469482421875\n",
            "tcost icost -0.5341796875 0.4014544188976288\n",
            "tcost icost -0.82373046875 0.39110469818115234\n",
            "tcost icost -0.82373046875 0.3897864520549774\n",
            "search -0.433837890625\n",
            "tcost icost -0.5810546875 0.40219607949256897\n",
            "tcost icost -0.85107421875 0.3887890875339508\n",
            "tcost icost -0.85107421875 0.3897850215435028\n",
            "search -0.461181640625\n",
            "tcost icost -0.61083984375 0.40133073925971985\n",
            "tcost icost -0.875 0.38965320587158203\n",
            "tcost icost -0.875 0.389327734708786\n",
            "search -0.485595703125\n",
            "tcost icost -0.50732421875 0.40406063199043274\n",
            "tcost icost -0.8427734375 0.39036503434181213\n",
            "tcost icost -0.8427734375 0.39245036244392395\n",
            "search -0.450439453125\n",
            "tcost icost -0.7626953125 0.3963867425918579\n",
            "tcost icost -0.84716796875 0.3919598460197449\n",
            "tcost icost -0.84716796875 0.39138343930244446\n",
            "search -0.455810546875\n",
            "tcost icost -0.50390625 0.40478578209877014\n",
            "tcost icost -0.84375 0.3924102783203125\n",
            "tcost icost -0.84375 0.39148426055908203\n",
            "search -0.4521484375\n",
            "tcost icost -0.355712890625 0.40756648778915405\n",
            "tcost icost -0.85546875 0.3915444612503052\n",
            "tcost icost -0.85546875 0.39239925146102905\n",
            "search -0.463134765625\n",
            "tcost icost -0.55224609375 0.40225404500961304\n",
            "tcost icost -0.70458984375 0.39501020312309265\n",
            "tcost icost -0.70458984375 0.39715078473091125\n",
            "search -0.307373046875\n",
            "tcost icost -0.46630859375 0.4068583548069\n",
            "tcost icost -0.8583984375 0.3917425274848938\n",
            "tcost icost -0.8583984375 0.39124414324760437\n",
            "search -0.467041015625\n",
            "tcost icost -0.52783203125 0.4031290113925934\n",
            "tcost icost -0.65966796875 0.39746883511543274\n",
            "tcost icost -0.65966796875 0.3986477851867676\n",
            "search -0.260986328125\n",
            "tcost icost -0.369873046875 0.408220499753952\n",
            "tcost icost -0.630859375 0.3974776566028595\n",
            "tcost icost -0.630859375 0.39738771319389343\n",
            "search -0.2333984375\n",
            "ded\n",
            "time\n",
            "23 #### train ####\n",
            "repr, std, cov, closslb 0.005877246148884296 0.4765625 0.00013689370825886726 0.00802852027118206 0.00033537292620167136\n",
            "0.0075549754654736494 0.025989232415626833 1.0\n",
            "repr, std, cov, closslb 0.028300456702709198 0.47705078125 0.0001606650184839964 0.009096522815525532 3.268568616476841e-05\n",
            "0.007570092971380061 0.026224073306107855 1.0\n",
            "repr, std, cov, closslb 0.004441590514034033 0.477294921875 0.0001202321145683527 0.003367833560332656 8.334558515343815e-06\n",
            "0.007532355793493435 0.02651398477214502 1.0\n",
            "repr, std, cov, closslb 0.0036907270550727844 0.476318359375 0.00012922193855047226 2.0989700715290383e-05 0.00047687048208899796\n",
            "0.0075549754654736494 0.026968346507977768 1.0\n",
            "repr, std, cov, closslb 0.0069149453192949295 0.4765625 0.00015663867816329002 0.012613905593752861 0.0001648802135605365\n",
            "0.0075549754654736494 0.027266486037312628 1.0\n",
            "repr, std, cov, closslb 0.00426121661439538 0.475830078125 0.00016146735288202763 1.6767799024819396e-05 0.015299215912818909\n",
            "0.007539888149286928 0.027157692213646844 1.0\n",
            "repr, std, cov, closslb 0.005897667724639177 0.476806640625 0.0001405805815011263 0.007719026412814856 0.0003504573833197355\n",
            "0.0074948067367567395 0.027375715689471298 1.0\n",
            "repr, std, cov, closslb 0.005102652125060558 0.476806640625 0.0001225871965289116 0.0049979728646576405 7.491955329896882e-05\n",
            "0.0074948067367567395 0.0275128682989696 1.0\n",
            "repr, std, cov, closslb 0.004008447751402855 0.47509765625 0.00020116055384278297 7.09909072611481e-05 0.00011368700506864116\n",
            "0.007457444863379469 0.027212034755766345 1.0\n",
            "repr, std, cov, closslb 0.005622583907097578 0.47509765625 0.00015714159235358238 0.010931724682450294 0.016176605597138405\n",
            "0.007390662217831372 0.02683390835862778 1.0\n",
            "repr, std, cov, closslb 0.0033318176865577698 0.475341796875 0.0001673540100455284 1.3180757377995178e-05 0.0007191631593741477\n",
            "0.007309850610788136 0.026355456175743325 1.0\n",
            "repr, std, cov, closslb 0.005275647155940533 0.474609375 0.00016607041470706463 0.00873885490000248 0.015143304131925106\n",
            "0.007237152543508669 0.025937331814665694 1.0\n",
            "repr, std, cov, closslb 0.007970946840941906 0.473876953125 0.00019705411978065968 0.02009597420692444 0.0003932336694560945\n",
            "0.007150868586408633 0.025246734098514224 1.0\n",
            "repr, std, cov, closslb 0.011164131574332714 0.47509765625 0.00015014642849564552 0.008150309324264526 3.892952736350708e-05\n",
            "0.007158019454995041 0.024995649791624032 1.0\n",
            "repr, std, cov, closslb 0.005891947075724602 0.47607421875 0.00013680127449333668 0.003049250226467848 0.00031915109138935804\n",
            "0.007193881204080554 0.024846199404906705 1.0\n",
            "repr, std, cov, closslb 0.005517603363841772 0.475830078125 0.00018767965957522392 0.0044685485772788525 5.6450298870913684e-05\n",
            "0.0072082761603699174 0.0247718096357986 1.0\n",
            "repr, std, cov, closslb 0.005667917430400848 0.4755859375 0.0001610259059816599 0.004249251447618008 0.015256859362125397\n",
            "0.0072082761603699174 0.02467296962078121 1.0\n",
            "repr, std, cov, closslb 0.003378906287252903 0.47607421875 0.000145449535921216 3.913813998224214e-06 0.00056261639110744\n",
            "0.007222699920966816 0.02492081256656583 1.0\n",
            "repr, std, cov, closslb 0.0029725059866905212 0.475830078125 0.0001430809497833252 0.008661158382892609 0.029344215989112854\n",
            "0.007136588273273813 0.024525448556834868 1.0\n",
            "repr, std, cov, closslb 0.002906346693634987 0.47412109375 0.00019092881120741367 4.744352918351069e-05 5.132015940034762e-05\n",
            "0.007044458821051029 0.024160493279767818 1.0\n",
            "repr, std, cov, closslb 0.003991396632045507 0.475341796875 0.00013316702097654343 0.0040003638714551926 0.00015429145423695445\n",
            "0.00705855478315195 0.024305819130418026 1.0\n",
            "repr, std, cov, closslb 0.0030712790321558714 0.476318359375 0.00011999369598925114 4.988093678548466e-06 0.011637803167104721\n",
            "0.0071010121314496214 0.024599098503376483 1.0\n",
            "repr, std, cov, closslb 0.0037977041210979223 0.476318359375 0.00011377758346498013 0.0032403909135609865 0.0002474663779139519\n",
            "0.007129458814459355 0.02487104560431161 1.0\n",
            "repr, std, cov, closslb 0.0099635636433959 0.475830078125 0.00013128109276294708 0.014774617739021778 0.008116225712001324\n",
            "0.007179514994576408 0.02514599912540871 1.0\n",
            "repr, std, cov, closslb 0.003342575393617153 0.477783203125 9.133829735219479e-05 3.0381566830328666e-05 0.0001735686237225309\n",
            "0.007295252809915497 0.025937331814665694 1.0\n",
            "repr, std, cov, closslb 0.006001757457852364 0.476806640625 0.00011363555677235126 0.016023915261030197 3.7387908378150314e-05\n",
            "0.007324477621860321 0.026329127048694632 1.0\n",
            "repr, std, cov, closslb 0.008112208917737007 0.475830078125 0.000115538714453578 0.01139577105641365 0.014924242161214352\n",
            "0.007229922620887782 0.025576918125704454 1.0\n",
            "repr, std, cov, closslb 0.0029523223638534546 0.475341796875 0.00015514087863266468 2.6396905013825744e-05 0.00010740949801402166\n",
            "0.007122336477981374 0.02514599912540871 1.0\n",
            "repr, std, cov, closslb 0.0029950356110930443 0.474853515625 0.00017915456555783749 5.614692781819031e-06 0.00035071588354185224\n",
            "0.0069674327006168475 0.024378809529572484 1.0\n",
            "repr, std, cov, closslb 0.0036483986768871546 0.474365234375 0.0001798812299966812 0.003838867414742708 0.0003258803335484117\n",
            "0.006870616338002455 0.023968076219982447 1.0\n",
            "repr, std, cov, closslb 0.002667088061571121 0.475341796875 0.00015930552035570145 1.973208327399334e-06 2.0860685253865086e-05\n",
            "0.006822713825201796 0.02349371129197447 1.0\n",
            "repr, std, cov, closslb 0.0030112373642623425 0.47509765625 0.00015511596575379372 0.00419982522726059 3.4391512599540874e-05\n",
            "0.0068022865518842035 0.023423370885781107 1.0\n",
            "repr, std, cov, closslb 0.00502417329698801 0.475341796875 0.0001360289752483368 0.012591980397701263 0.015480849891901016\n",
            "0.006815897927274522 0.02347024105092355 1.0\n",
            "repr, std, cov, closslb 0.0023952170740813017 0.4765625 0.00012003234587609768 2.763369593594689e-06 0.005678262561559677\n",
            "0.0068022865518842035 0.02358782719340845 1.0\n",
            "repr, std, cov, closslb 0.003590760286897421 0.476318359375 0.0001420907210558653 0.003596073482185602 0.0005964734591543674\n",
            "0.006795491060823381 0.02396807621998245 1.0\n",
            "repr, std, cov, closslb 0.0027530654333531857 0.476318359375 0.0001239236444234848 0.004849454388022423 0.00015969904779922217\n",
            "0.00685004564408323 0.024281537592825208 1.0\n",
            "tcost icost -0.79052734375 0.39144882559776306\n",
            "tcost icost -1.25 0.37615966796875\n",
            "tcost icost -1.25 0.37672632932662964\n",
            "search -0.873046875\n",
            "tcost icost -1.25 0.37870195508003235\n",
            "tcost icost -1.25 0.3782869875431061\n",
            "tcost icost -1.25 0.3773873746395111\n",
            "search -0.87255859375\n",
            "tcost icost -0.9267578125 0.39017254114151\n",
            "tcost icost -1.2470703125 0.3780980408191681\n",
            "tcost icost -1.2470703125 0.37902432680130005\n",
            "search -0.8681640625\n",
            "tcost icost -0.7783203125 0.3957310616970062\n",
            "tcost icost -1.2470703125 0.37980812788009644\n",
            "tcost icost -1.2470703125 0.3796974718570709\n",
            "search -0.8671875\n",
            "tcost icost -0.93505859375 0.39041727781295776\n",
            "tcost icost -1.25 0.38041263818740845\n",
            "tcost icost -1.25 0.3802049458026886\n",
            "search -0.8701171875\n",
            "tcost icost -0.69482421875 0.39874422550201416\n",
            "tcost icost -1.25 0.3804468512535095\n",
            "tcost icost -1.25 0.38001519441604614\n",
            "search -0.8701171875\n",
            "tcost icost -0.78466796875 0.3967547118663788\n",
            "tcost icost -1.248046875 0.38105612993240356\n",
            "tcost icost -1.248046875 0.3802439868450165\n",
            "search -0.8681640625\n",
            "tcost icost -0.79345703125 0.39838847517967224\n",
            "tcost icost -1.25 0.3809666633605957\n",
            "tcost icost -1.25 0.3813071846961975\n",
            "search -0.86865234375\n",
            "tcost icost -1.2529296875 0.38225001096725464\n",
            "tcost icost -1.2529296875 0.3828505575656891\n",
            "tcost icost -1.2529296875 0.3825257420539856\n",
            "search -0.8701171875\n",
            "tcost icost -0.94873046875 0.3943844437599182\n",
            "tcost icost -1.2529296875 0.3831259608268738\n",
            "tcost icost -1.2529296875 0.3819064497947693\n",
            "search -0.87109375\n",
            "tcost icost -0.95458984375 0.39321795105934143\n",
            "tcost icost -1.2548828125 0.3820725977420807\n",
            "tcost icost -1.2548828125 0.38273006677627563\n",
            "search -0.8720703125\n",
            "tcost icost -0.95556640625 0.39549365639686584\n",
            "tcost icost -1.2548828125 0.38376811146736145\n",
            "tcost icost -1.2548828125 0.3836592733860016\n",
            "search -0.87109375\n",
            "tcost icost -0.96044921875 0.39586520195007324\n",
            "tcost icost -1.2568359375 0.38514986634254456\n",
            "tcost icost -1.2568359375 0.38420915603637695\n",
            "search -0.87255859375\n",
            "tcost icost -0.96240234375 0.3945503532886505\n",
            "tcost icost -1.2568359375 0.3854292035102844\n",
            "tcost icost -1.2568359375 0.38530007004737854\n",
            "search -0.87158203125\n",
            "tcost icost -0.96923828125 0.3957934081554413\n",
            "tcost icost -1.2587890625 0.3863750398159027\n",
            "tcost icost -1.2587890625 0.38428598642349243\n",
            "search -0.87451171875\n",
            "tcost icost -1.1103515625 0.39160066843032837\n",
            "tcost icost -1.2568359375 0.3849755525588989\n",
            "tcost icost -1.2568359375 0.38714101910591125\n",
            "search -0.86962890625\n",
            "tcost icost -1.185546875 0.38873761892318726\n",
            "tcost icost -1.2568359375 0.38593003153800964\n",
            "tcost icost -1.2568359375 0.3868749737739563\n",
            "search -0.8701171875\n",
            "tcost icost -0.96630859375 0.39660459756851196\n",
            "tcost icost -1.2587890625 0.3858056962490082\n",
            "tcost icost -1.2587890625 0.3877648115158081\n",
            "search -0.87109375\n",
            "tcost icost -0.80859375 0.40252357721328735\n",
            "tcost icost -1.2568359375 0.3867220878601074\n",
            "tcost icost -1.2568359375 0.38656046986579895\n",
            "search -0.8701171875\n",
            "tcost icost -0.955078125 0.39682334661483765\n",
            "tcost icost -1.2568359375 0.38761505484580994\n",
            "tcost icost -1.2568359375 0.3867039978504181\n",
            "search -0.8701171875\n",
            "tcost icost -0.63671875 0.4079509675502777\n",
            "tcost icost -1.255859375 0.38585546612739563\n",
            "tcost icost -1.255859375 0.38630980253219604\n",
            "search -0.86962890625\n",
            "tcost icost -0.8515625 0.4015902578830719\n",
            "tcost icost -1.251953125 0.38688552379608154\n",
            "tcost icost -1.251953125 0.38690370321273804\n",
            "search -0.865234375\n",
            "tcost icost -0.93896484375 0.39992961287498474\n",
            "tcost icost -1.25 0.3884270489215851\n",
            "tcost icost -1.25 0.38902637362480164\n",
            "search -0.861328125\n",
            "tcost icost -1.1748046875 0.3912163972854614\n",
            "tcost icost -1.2509765625 0.38821643590927124\n",
            "tcost icost -1.2509765625 0.38858360052108765\n",
            "search -0.8623046875\n",
            "tcost icost -1.0498046875 0.3946012258529663\n",
            "tcost icost -1.2490234375 0.3894404470920563\n",
            "tcost icost -1.2490234375 0.38894736766815186\n",
            "search -0.8603515625\n",
            "tcost icost -0.794921875 0.40445631742477417\n",
            "tcost icost -1.2490234375 0.38959643244743347\n",
            "tcost icost -1.2490234375 0.3880338668823242\n",
            "search -0.861328125\n",
            "tcost icost -0.92919921875 0.40183305740356445\n",
            "tcost icost -1.248046875 0.39032718539237976\n",
            "tcost icost -1.248046875 0.3890511989593506\n",
            "search -0.85888671875\n",
            "tcost icost -0.841796875 0.40522488951683044\n",
            "tcost icost -1.2509765625 0.38965675234794617\n",
            "tcost icost -1.2509765625 0.3877677023410797\n",
            "search -0.86328125\n",
            "tcost icost -1.0732421875 0.39373183250427246\n",
            "tcost icost -1.259765625 0.3881188929080963\n",
            "tcost icost -1.259765625 0.38826197385787964\n",
            "search -0.87158203125\n",
            "tcost icost -0.99951171875 0.3979639708995819\n",
            "tcost icost -1.2626953125 0.388292133808136\n",
            "tcost icost -1.2626953125 0.3886185586452484\n",
            "search -0.8740234375\n",
            "ded\n",
            "time\n",
            "24 #### train ####\n",
            "repr, std, cov, closslb 0.0032864392269402742 0.477294921875 0.00010784692130982876 0.004005146212875843 0.00446245726197958\n",
            "0.006905038194596367 0.02462369760187986 1.0\n",
            "repr, std, cov, closslb 0.0026931066531687975 0.475830078125 0.00014228420332074165 4.4996286305831745e-06 6.275661871768534e-05\n",
            "0.006856895689727313 0.02462369760187986 1.0\n",
            "repr, std, cov, closslb 0.0027411107439547777 0.4765625 0.00013535842299461365 9.211135875375476e-06 0.00016851363761816174\n",
            "0.006856895689727313 0.02489591664991592 1.0\n",
            "repr, std, cov, closslb 0.0036950125358998775 0.477294921875 0.00011631450615823269 0.004255776759237051 4.114492912776768e-05\n",
            "0.006884364441294796 0.025045666086857068 1.0\n",
            "repr, std, cov, closslb 0.002965731080621481 0.4765625 0.00012924033217132092 1.9217659428250045e-05 5.989389319438487e-05\n",
            "0.006932699805230975 0.02547486571173831 1.0\n",
            "repr, std, cov, closslb 0.003977229818701744 0.47607421875 0.00015800725668668747 0.0036618399899452925 5.52461133338511e-05\n",
            "0.006843202441641589 0.025171145124534118 1.0\n",
            "repr, std, cov, closslb 0.004618678241968155 0.4755859375 0.0001579828094691038 0.010533813387155533 9.464110189583153e-05\n",
            "0.006734636320472731 0.024403188339102058 1.0\n",
            "repr, std, cov, closslb 0.005545820575207472 0.47607421875 0.00012366869486868382 0.008885336108505726 0.030594436451792717\n",
            "0.00678192043802689 0.024648321299481735 1.0\n",
            "repr, std, cov, closslb 0.003229642054066062 0.4775390625 0.00011826609261333942 0.004562399350106716 0.015398180112242699\n",
            "0.006891248805736091 0.025398593709430307 1.0\n",
            "repr, std, cov, closslb 0.0042172810062766075 0.476806640625 0.00011435500346124172 0.004735240247100592 0.0015860111452639103\n",
            "0.0069188551760237535 0.025782251058516826 1.0\n",
            "repr, std, cov, closslb 0.00481234909966588 0.477783203125 0.00013291137292981148 0.007897858507931232 9.326168219558895e-05\n",
            "0.006911943232790963 0.02596326914648036 1.0\n",
            "repr, std, cov, closslb 0.002918192185461521 0.477783203125 0.00011697318404912949 2.9264710974530317e-05 7.530604489147663e-05\n",
            "0.006925774031199776 0.02638181163191907 1.0\n",
            "repr, std, cov, closslb 0.0039572324603796005 0.4765625 0.0001417878083884716 0.005982143804430962 0.00014814459427725524\n",
            "0.006884364441294796 0.026914490612262643 1.0\n",
            "repr, std, cov, closslb 0.003548037726432085 0.47705078125 0.00010987906716763973 0.003995418082922697 4.149977758061141e-05\n",
            "0.006884364441294796 0.027157692213646847 1.0\n",
            "repr, std, cov, closslb 0.003417119849473238 0.47705078125 0.00011703837662935257 0.004159027244895697 0.00012088972289348021\n",
            "0.0069188551760237535 0.027595489469984268 1.0\n",
            "repr, std, cov, closslb 0.0060484278947114944 0.475341796875 0.00017417524941265583 0.011775021441280842 0.00012813109788112342\n",
            "0.006843202441641589 0.02723924679052211 1.0\n",
            "repr, std, cov, closslb 0.0030427484307438135 0.47509765625 0.00018444121815264225 0.00041723050526343286 9.255218901671469e-05\n",
            "0.006721187224835837 0.026434601636994536 1.0\n",
            "repr, std, cov, closslb 0.008848482742905617 0.474365234375 0.00020707142539322376 0.007164040580391884 0.0007861289195716381\n",
            "0.006667658885447463 0.02596326914648036 1.0\n",
            "repr, std, cov, closslb 0.00831657089293003 0.474365234375 0.00020283879712224007 0.00748396897688508 0.015435392037034035\n",
            "0.006607948903256501 0.025576918125704458 1.0\n",
            "repr, std, cov, closslb 0.004501477349549532 0.475341796875 0.0001863548532128334 0.027390241622924805 0.002866199240088463\n",
            "0.006542231402376352 0.025120878247161554 1.0\n",
            "repr, std, cov, closslb 0.0040739187970757484 0.477294921875 0.00013356562703847885 7.314899994526058e-05 0.0004981268430128694\n",
            "0.006627792580420927 0.025705058741411317 1.0\n",
            "repr, std, cov, closslb 0.00455980421975255 0.476806640625 0.00012287870049476624 0.0039829071611166 0.03073323890566826\n",
            "0.006667658885447463 0.026145558168781457 1.0\n",
            "repr, std, cov, closslb 0.003714833175763488 0.475830078125 0.0001532379537820816 5.32803860551212e-05 0.0007515797042287886\n",
            "0.006707764987096658 0.02635545617574333 1.0\n",
            "repr, std, cov, closslb 0.013758634217083454 0.475341796875 0.00016520917415618896 0.00803887564688921 0.015120791271328926\n",
            "0.006654343544015888 0.026145558168781457 1.0\n",
            "repr, std, cov, closslb 0.00419267825782299 0.475830078125 0.00014817039482295513 0.007814226672053337 0.00017208344070240855\n",
            "0.006542231402376352 0.025423992303139735 1.0\n",
            "repr, std, cov, closslb 0.0030931993387639523 0.47607421875 0.00014613731764256954 4.9964753998210654e-05 0.015940673649311066\n",
            "0.006529166540129554 0.025500340577450046 1.0\n",
            "repr, std, cov, closslb 0.0051034800708293915 0.474365234375 0.00022617168724536896 0.003953520208597183 0.00016905332449823618\n",
            "0.006535695706669683 0.02544941629544287 1.0\n",
            "repr, std, cov, closslb 0.007432346232235432 0.475341796875 0.0001742879394441843 0.007856015115976334 0.015383044257760048\n",
            "0.006483644643929901 0.02509578246469686 1.0\n",
            "repr, std, cov, closslb 0.005313042551279068 0.4755859375 0.00015788455493748188 0.003979384433478117 0.015095756389200687\n",
            "0.006361678712140779 0.02442759152744116 1.0\n",
            "repr, std, cov, closslb 0.0038654785603284836 0.4765625 0.00013897172175347805 0.0036729227285832167 0.01520991139113903\n",
            "0.006387163622514689 0.02472234023299239 1.0\n",
            "repr, std, cov, closslb 0.0039739832282066345 0.476806640625 0.00012646988034248352 0.004455788992345333 0.0003984383656643331\n",
            "0.006425582539543229 0.024970679112511527 1.0\n",
            "repr, std, cov, closslb 0.0033490825444459915 0.47705078125 0.00012020370922982693 3.0156124921632e-05 0.00010322082380298525\n",
            "0.006432008122082771 0.02529725281344535 1.0\n",
            "repr, std, cov, closslb 0.004929758608341217 0.4775390625 0.00010439916513860226 0.009969419799745083 0.00010522460797801614\n",
            "0.006483644643929901 0.02606727810656019 1.0\n",
            "repr, std, cov, closslb 0.0030654831789433956 0.477783203125 9.892717935144901e-05 0.004483148455619812 0.00013804752961732447\n",
            "0.006522643896233322 0.026540498756917164 1.0\n",
            "repr, std, cov, closslb 0.008565609343349934 0.4775390625 0.00010025221854448318 0.013328639790415764 0.015542430803179741\n",
            "0.006555322407412505 0.02729375252334994 1.0\n",
            "repr, std, cov, closslb 0.003962707240134478 0.4775390625 0.00010309810750186443 0.004221776500344276 0.015156255103647709\n",
            "0.006561877729819917 0.02778923836836156 1.0\n",
            "tcost icost -0.736328125 0.41356050968170166\n",
            "tcost icost -1.5888671875 0.39348524808883667\n",
            "tcost icost -1.5888671875 0.3921407461166382\n",
            "search -1.197265625\n",
            "tcost icost -0.736328125 0.41374471783638\n",
            "tcost icost -1.5888671875 0.39378422498703003\n",
            "tcost icost -1.5888671875 0.3947184383869171\n",
            "search -1.1943359375\n",
            "tcost icost -1.0576171875 0.4064429700374603\n",
            "tcost icost -1.5830078125 0.39408835768699646\n",
            "tcost icost -1.5830078125 0.3937951326370239\n",
            "search -1.189453125\n",
            "tcost icost -0.404052734375 0.4206113815307617\n",
            "tcost icost -1.5830078125 0.3933400511741638\n",
            "tcost icost -1.5830078125 0.39291396737098694\n",
            "search -1.1904296875\n",
            "tcost icost -1.0556640625 0.40737682580947876\n",
            "tcost icost -1.58984375 0.39359551668167114\n",
            "tcost icost -1.58984375 0.39364978671073914\n",
            "search -1.1962890625\n",
            "tcost icost -1.58984375 0.39439383149147034\n",
            "tcost icost -1.58984375 0.3952353000640869\n",
            "tcost icost -1.58984375 0.39431723952293396\n",
            "search -1.1953125\n",
            "tcost icost -0.7744140625 0.4162672460079193\n",
            "tcost icost -1.611328125 0.3957803249359131\n",
            "tcost icost -1.611328125 0.3954221308231354\n",
            "search -1.2158203125\n",
            "tcost icost -0.87841796875 0.41669848561286926\n",
            "tcost icost -1.6650390625 0.39506736397743225\n",
            "tcost icost -1.6650390625 0.39568430185317993\n",
            "search -1.26953125\n",
            "tcost icost -0.2449951171875 0.4273475110530853\n",
            "tcost icost -1.662109375 0.3946998715400696\n",
            "tcost icost -1.662109375 0.3962537944316864\n",
            "search -1.265625\n",
            "tcost icost -0.469970703125 0.4243435263633728\n",
            "tcost icost -1.634765625 0.39618760347366333\n",
            "tcost icost -1.634765625 0.3978707790374756\n",
            "search -1.236328125\n",
            "tcost icost -0.9111328125 0.41573411226272583\n",
            "tcost icost -1.6826171875 0.3972175717353821\n",
            "tcost icost -1.6826171875 0.3963800370693207\n",
            "search -1.2861328125\n",
            "tcost icost -0.142578125 0.42824622988700867\n",
            "tcost icost -1.658203125 0.39604440331459045\n",
            "tcost icost -1.658203125 0.39659419655799866\n",
            "search -1.26171875\n",
            "tcost icost -1.33984375 0.4065965712070465\n",
            "tcost icost -1.732421875 0.39509665966033936\n",
            "tcost icost -1.732421875 0.3947084844112396\n",
            "search -1.337890625\n",
            "tcost icost -0.92529296875 0.4148474931716919\n",
            "tcost icost -1.6640625 0.39658209681510925\n",
            "tcost icost -1.6640625 0.39656761288642883\n",
            "search -1.267578125\n",
            "tcost icost -0.493408203125 0.4251953363418579\n",
            "tcost icost -1.6494140625 0.3969031572341919\n",
            "tcost icost -1.6494140625 0.3978246748447418\n",
            "search -1.251953125\n",
            "tcost icost -1.5341796875 0.40050897002220154\n",
            "tcost icost -1.7109375 0.3974211812019348\n",
            "tcost icost -1.7109375 0.39658594131469727\n",
            "search -1.314453125\n",
            "tcost icost -1.201171875 0.4100220799446106\n",
            "tcost icost -1.6884765625 0.39798620343208313\n",
            "tcost icost -1.6884765625 0.3986152112483978\n",
            "search -1.2900390625\n",
            "tcost icost -1.23046875 0.41025492548942566\n",
            "tcost icost -1.7236328125 0.39735162258148193\n",
            "tcost icost -1.7236328125 0.3960084319114685\n",
            "search -1.328125\n",
            "tcost icost -0.62255859375 0.4228702187538147\n",
            "tcost icost -1.6748046875 0.39862489700317383\n",
            "tcost icost -1.6748046875 0.3984977602958679\n",
            "search -1.2763671875\n",
            "tcost icost -0.64111328125 0.42490848898887634\n",
            "tcost icost -1.697265625 0.3978062570095062\n",
            "tcost icost -1.697265625 0.3982870578765869\n",
            "search -1.298828125\n",
            "tcost icost -0.77685546875 0.42150336503982544\n",
            "tcost icost -1.6220703125 0.400056928396225\n",
            "tcost icost -1.6220703125 0.4002901613712311\n",
            "search -1.2216796875\n",
            "tcost icost -1.142578125 0.41214537620544434\n",
            "tcost icost -1.6328125 0.3991849422454834\n",
            "tcost icost -1.6328125 0.39948391914367676\n",
            "search -1.2333984375\n",
            "tcost icost -1.521484375 0.40038561820983887\n",
            "tcost icost -1.716796875 0.3973943293094635\n",
            "tcost icost -1.716796875 0.39605912566185\n",
            "search -1.3203125\n",
            "tcost icost -0.80615234375 0.4204082190990448\n",
            "tcost icost -1.626953125 0.3985261917114258\n",
            "tcost icost -1.626953125 0.3985316753387451\n",
            "search -1.228515625\n",
            "tcost icost -0.7783203125 0.42114993929862976\n",
            "tcost icost -1.609375 0.4006243646144867\n",
            "tcost icost -1.609375 0.40067893266677856\n",
            "search -1.208984375\n",
            "tcost icost -1.09375 0.41316184401512146\n",
            "tcost icost -1.6181640625 0.399809330701828\n",
            "tcost icost -1.6181640625 0.4000672698020935\n",
            "search -1.2177734375\n",
            "tcost icost -0.328369140625 0.4306422472000122\n",
            "tcost icost -1.498046875 0.4032547175884247\n",
            "tcost icost -1.498046875 0.40360960364341736\n",
            "search -1.0947265625\n",
            "tcost icost -1.5322265625 0.402681440114975\n",
            "tcost icost -1.5322265625 0.4042438566684723\n",
            "tcost icost -1.5322265625 0.40279415249824524\n",
            "search -1.12890625\n",
            "tcost icost -1.2177734375 0.4114348292350769\n",
            "tcost icost -1.5087890625 0.40471187233924866\n",
            "tcost icost -1.5087890625 0.4036465287208557\n",
            "search -1.10546875\n",
            "tcost icost -0.708984375 0.423641175031662\n",
            "tcost icost -1.58203125 0.40208742022514343\n",
            "tcost icost -1.58203125 0.4019138813018799\n",
            "search -1.1796875\n",
            "tcost icost -0.66650390625 0.42438068985939026\n",
            "tcost icost -1.5615234375 0.4025912880897522\n",
            "tcost icost -1.5615234375 0.4023968577384949\n",
            "search -1.1591796875\n",
            "tcost icost -1.2353515625 0.4109433889389038\n",
            "tcost icost -1.494140625 0.40345922112464905\n",
            "tcost icost -1.494140625 0.4031311273574829\n",
            "search -1.0908203125\n",
            "tcost icost -1.044921875 0.4157021939754486\n",
            "tcost icost -1.5830078125 0.403250515460968\n",
            "tcost icost -1.5830078125 0.4037723243236542\n",
            "search -1.1796875\n",
            "tcost icost -0.6796875 0.42710742354393005\n",
            "tcost icost -1.5625 0.4051453173160553\n",
            "tcost icost -1.5625 0.40438026189804077\n",
            "search -1.158203125\n",
            "tcost icost -0.71728515625 0.4254477024078369\n",
            "tcost icost -1.5771484375 0.4027966856956482\n",
            "tcost icost -1.5771484375 0.4035932123661041\n",
            "search -1.173828125\n",
            "tcost icost -0.349853515625 0.43147870898246765\n",
            "tcost icost -1.474609375 0.4051787555217743\n",
            "tcost icost -1.474609375 0.40624576807022095\n",
            "search -1.068359375\n",
            "tcost icost -0.3974609375 0.43063825368881226\n",
            "tcost icost -1.51171875 0.40429994463920593\n",
            "tcost icost -1.51171875 0.404902845621109\n",
            "search -1.107421875\n",
            "tcost icost -0.69677734375 0.4261842668056488\n",
            "tcost icost -1.5009765625 0.40593934059143066\n",
            "tcost icost -1.5009765625 0.40494656562805176\n",
            "search -1.095703125\n",
            "tcost icost -1.0712890625 0.4160607159137726\n",
            "tcost icost -1.5615234375 0.40476927161216736\n",
            "tcost icost -1.5615234375 0.4053349494934082\n",
            "search -1.15625\n",
            "tcost icost -0.1678466796875 0.4355485439300537\n",
            "tcost icost -1.515625 0.4073095917701721\n",
            "tcost icost -1.515625 0.4068831503391266\n",
            "search -1.1083984375\n",
            "tcost icost -0.64013671875 0.4286692142486572\n",
            "tcost icost -1.5126953125 0.4078141748905182\n",
            "tcost icost -1.5126953125 0.4071756601333618\n",
            "search -1.10546875\n",
            "tcost icost -0.97509765625 0.4209738075733185\n",
            "tcost icost -1.513671875 0.406802773475647\n",
            "tcost icost -1.513671875 0.4075463116168976\n",
            "search -1.1064453125\n",
            "tcost icost -1.03125 0.41806817054748535\n",
            "tcost icost -1.5537109375 0.4053402543067932\n",
            "tcost icost -1.5537109375 0.4055325388908386\n",
            "search -1.1484375\n",
            "tcost icost -0.187255859375 0.4371837377548218\n",
            "tcost icost -1.5791015625 0.40556320548057556\n",
            "tcost icost -1.5791015625 0.4056869149208069\n",
            "search -1.173828125\n",
            "tcost icost -0.63037109375 0.42794883251190186\n",
            "tcost icost -1.5078125 0.4079142212867737\n",
            "tcost icost -1.5078125 0.4065169095993042\n",
            "search -1.1015625\n",
            "tcost icost -0.6357421875 0.4267582893371582\n",
            "tcost icost -1.5048828125 0.40682998299598694\n",
            "tcost icost -1.5048828125 0.40707793831825256\n",
            "search -1.09765625\n",
            "tcost icost -0.414794921875 0.4336404502391815\n",
            "tcost icost -1.6005859375 0.4064740538597107\n",
            "tcost icost -1.6005859375 0.40454795956611633\n",
            "search -1.1962890625\n",
            "tcost icost -1.0859375 0.4197799265384674\n",
            "tcost icost -1.59765625 0.4073420763015747\n",
            "tcost icost -1.59765625 0.40660810470581055\n",
            "search -1.19140625\n",
            "tcost icost -0.69677734375 0.43124428391456604\n",
            "tcost icost -1.5791015625 0.4066169857978821\n",
            "tcost icost -1.5791015625 0.4066605269908905\n",
            "search -1.171875\n",
            "tcost icost -0.423095703125 0.43357932567596436\n",
            "tcost icost -1.6103515625 0.4058132767677307\n",
            "tcost icost -1.6103515625 0.4069882035255432\n",
            "search -1.203125\n",
            "tcost icost -0.1563720703125 0.4374987781047821\n",
            "tcost icost -1.55078125 0.40789133310317993\n",
            "tcost icost -1.55078125 0.40728944540023804\n",
            "search -1.1435546875\n",
            "tcost icost -0.7607421875 0.42902377247810364\n",
            "tcost icost -1.603515625 0.40762948989868164\n",
            "tcost icost -1.603515625 0.40784594416618347\n",
            "search -1.1953125\n",
            "tcost icost -1.4306640625 0.4117507040500641\n",
            "tcost icost -1.64453125 0.40625259280204773\n",
            "tcost icost -1.64453125 0.4063974916934967\n",
            "search -1.23828125\n",
            "tcost icost -1.390625 0.41348797082901\n",
            "tcost icost -1.609375 0.40743687748908997\n",
            "tcost icost -1.609375 0.40732327103614807\n",
            "search -1.2021484375\n",
            "tcost icost -0.40869140625 0.43573901057243347\n",
            "tcost icost -1.58984375 0.4079011380672455\n",
            "tcost icost -1.58984375 0.40873971581459045\n",
            "search -1.181640625\n",
            "tcost icost -0.8046875 0.4290831983089447\n",
            "tcost icost -1.61328125 0.408266544342041\n",
            "tcost icost -1.61328125 0.4076054096221924\n",
            "search -1.205078125\n",
            "tcost icost -0.5146484375 0.4351210296154022\n",
            "tcost icost -1.607421875 0.40778374671936035\n",
            "tcost icost -1.607421875 0.40790173411369324\n",
            "search -1.19921875\n",
            "tcost icost -1.1044921875 0.42066285014152527\n",
            "tcost icost -1.5908203125 0.40806740522384644\n",
            "tcost icost -1.5908203125 0.40870192646980286\n",
            "search -1.181640625\n",
            "tcost icost -0.1922607421875 0.4399023652076721\n",
            "tcost icost -1.5927734375 0.4095233082771301\n",
            "tcost icost -1.5927734375 0.4085591733455658\n",
            "search -1.1845703125\n",
            "tcost icost -0.391357421875 0.43839478492736816\n",
            "tcost icost -1.578125 0.4095165431499481\n",
            "tcost icost -1.578125 0.41056638956069946\n",
            "search -1.16796875\n",
            "tcost icost -0.779296875 0.43106555938720703\n",
            "tcost icost -1.599609375 0.4096686542034149\n",
            "tcost icost -1.599609375 0.4094034731388092\n",
            "search -1.1904296875\n",
            "tcost icost -0.7529296875 0.4311806261539459\n",
            "tcost icost -1.6044921875 0.409239262342453\n",
            "tcost icost -1.6044921875 0.4092942476272583\n",
            "search -1.1953125\n",
            "tcost icost -0.416015625 0.44071832299232483\n",
            "tcost icost -1.5517578125 0.41201964020729065\n",
            "tcost icost -1.5517578125 0.4118623435497284\n",
            "search -1.1396484375\n",
            "tcost icost -0.127197265625 0.44470053911209106\n",
            "tcost icost -1.490234375 0.41292133927345276\n",
            "tcost icost -1.490234375 0.41318219900131226\n",
            "search -1.0771484375\n",
            "tcost icost -1.033203125 0.42407533526420593\n",
            "tcost icost -1.5908203125 0.41055530309677124\n",
            "tcost icost -1.5908203125 0.4105048179626465\n",
            "search -1.1806640625\n",
            "tcost icost -1.3525390625 0.4166161119937897\n",
            "tcost icost -1.6064453125 0.409756064414978\n",
            "tcost icost -1.6064453125 0.40909668803215027\n",
            "search -1.197265625\n",
            "tcost icost -1.02734375 0.42403504252433777\n",
            "tcost icost -1.5908203125 0.4102499485015869\n",
            "tcost icost -1.5908203125 0.4093784689903259\n",
            "search -1.181640625\n",
            "tcost icost -0.5869140625 0.4344373047351837\n",
            "tcost icost -1.501953125 0.4107581377029419\n",
            "tcost icost -1.501953125 0.41181573271751404\n",
            "search -1.08984375\n",
            "tcost icost -1.0732421875 0.4237201511859894\n",
            "tcost icost -1.5830078125 0.41125133633613586\n",
            "tcost icost -1.5830078125 0.41024211049079895\n",
            "search -1.1728515625\n",
            "tcost icost -0.76025390625 0.4322131276130676\n",
            "tcost icost -1.5869140625 0.4109519422054291\n",
            "tcost icost -1.5869140625 0.41111132502555847\n",
            "search -1.17578125\n",
            "tcost icost -0.36767578125 0.4417121112346649\n",
            "tcost icost -1.576171875 0.4112313687801361\n",
            "tcost icost -1.576171875 0.4112284481525421\n",
            "search -1.1650390625\n",
            "tcost icost -0.64794921875 0.435821533203125\n",
            "tcost icost -1.5556640625 0.4116537272930145\n",
            "tcost icost -1.5556640625 0.41191601753234863\n",
            "search -1.1435546875\n",
            "tcost icost -1.361328125 0.4162065386772156\n",
            "tcost icost -1.6083984375 0.41091325879096985\n",
            "tcost icost -1.6083984375 0.40998855233192444\n",
            "search -1.1982421875\n",
            "tcost icost -1.3544921875 0.416337788105011\n",
            "tcost icost -1.603515625 0.4097701609134674\n",
            "tcost icost -1.603515625 0.41075316071510315\n",
            "search -1.193359375\n",
            "tcost icost -0.76318359375 0.4327296316623688\n",
            "tcost icost -1.609375 0.4093151390552521\n",
            "tcost icost -1.609375 0.4092999994754791\n",
            "search -1.2001953125\n",
            "tcost icost -1.142578125 0.4206479489803314\n",
            "tcost icost -1.583984375 0.4101838767528534\n",
            "tcost icost -1.583984375 0.4091998040676117\n",
            "search -1.1748046875\n",
            "tcost icost -0.493896484375 0.4373345375061035\n",
            "tcost icost -1.6611328125 0.408661425113678\n",
            "tcost icost -1.6611328125 0.4077584147453308\n",
            "search -1.25390625\n",
            "tcost icost -0.56298828125 0.43604567646980286\n",
            "tcost icost -1.6455078125 0.40811342000961304\n",
            "tcost icost -1.6455078125 0.4089592695236206\n",
            "search -1.236328125\n",
            "tcost icost -0.324462890625 0.4398389160633087\n",
            "tcost icost -1.6962890625 0.40818578004837036\n",
            "tcost icost -1.6962890625 0.40786227583885193\n",
            "search -1.2880859375\n",
            "tcost icost -1.1748046875 0.42095986008644104\n",
            "tcost icost -1.669921875 0.4078170657157898\n",
            "tcost icost -1.669921875 0.40811917185783386\n",
            "search -1.26171875\n",
            "tcost icost -0.11895751953125 0.44329389929771423\n",
            "tcost icost -1.66015625 0.40892913937568665\n",
            "tcost icost -1.66015625 0.40847525000572205\n",
            "search -1.251953125\n",
            "tcost icost -0.9150390625 0.42869868874549866\n",
            "tcost icost -1.697265625 0.40710005164146423\n",
            "tcost icost -1.697265625 0.40688464045524597\n",
            "search -1.2900390625\n",
            "tcost icost -1.169921875 0.4202619194984436\n",
            "tcost icost -1.638671875 0.40877464413642883\n",
            "tcost icost -1.638671875 0.4075624942779541\n",
            "search -1.2314453125\n",
            "tcost icost -1.23828125 0.41904711723327637\n",
            "tcost icost -1.66015625 0.40769949555397034\n",
            "tcost icost -1.66015625 0.40865492820739746\n",
            "search -1.251953125\n",
            "tcost icost -0.869140625 0.4288438856601715\n",
            "tcost icost -1.626953125 0.40943443775177\n",
            "tcost icost -1.626953125 0.4099334180355072\n",
            "search -1.216796875\n",
            "tcost icost -1.390625 0.4157634973526001\n",
            "tcost icost -1.5966796875 0.410093754529953\n",
            "tcost icost -1.5966796875 0.4109030067920685\n",
            "search -1.185546875\n",
            "tcost icost -0.86279296875 0.42955297231674194\n",
            "tcost icost -1.6298828125 0.4093286395072937\n",
            "tcost icost -1.6298828125 0.4095571041107178\n",
            "search -1.220703125\n",
            "tcost icost -0.873046875 0.43039023876190186\n",
            "tcost icost -1.6337890625 0.4098196029663086\n",
            "tcost icost -1.6337890625 0.41048917174339294\n",
            "search -1.2236328125\n",
            "tcost icost -0.476806640625 0.43875423073768616\n",
            "tcost icost -1.6201171875 0.41043922305107117\n",
            "tcost icost -1.6201171875 0.4102622866630554\n",
            "search -1.2099609375\n",
            "tcost icost -0.556640625 0.43670326471328735\n",
            "tcost icost -1.669921875 0.4089397192001343\n",
            "tcost icost -1.669921875 0.40875244140625\n",
            "search -1.26171875\n",
            "tcost icost -0.277587890625 0.44321271777153015\n",
            "tcost icost -1.6650390625 0.40993890166282654\n",
            "tcost icost -1.6650390625 0.4094525873661041\n",
            "search -1.255859375\n",
            "tcost icost -1.1650390625 0.4234565496444702\n",
            "tcost icost -1.65625 0.4112660884857178\n",
            "tcost icost -1.65625 0.41050803661346436\n",
            "search -1.24609375\n",
            "tcost icost -0.230224609375 0.44492217898368835\n",
            "tcost icost -1.6376953125 0.4121088683605194\n",
            "tcost icost -1.6376953125 0.4112338125705719\n",
            "search -1.2265625\n",
            "tcost icost -1.669921875 0.40996721386909485\n",
            "tcost icost -1.669921875 0.4103884696960449\n",
            "tcost icost -1.669921875 0.41027867794036865\n",
            "search -1.259765625\n",
            "tcost icost -0.81787109375 0.4327208697795868\n",
            "tcost icost -1.638671875 0.41215780377388\n",
            "tcost icost -1.638671875 0.41089341044425964\n",
            "search -1.2275390625\n",
            "tcost icost -1.388671875 0.41723763942718506\n",
            "tcost icost -1.6044921875 0.4118739664554596\n",
            "tcost icost -1.6044921875 0.41178765892982483\n",
            "search -1.1923828125\n",
            "tcost icost -0.8310546875 0.43254250288009644\n",
            "tcost icost -1.6474609375 0.41064175963401794\n",
            "tcost icost -1.6474609375 0.4114213287830353\n",
            "search -1.236328125\n",
            "tcost icost -1.4091796875 0.4174969494342804\n",
            "tcost icost -1.6435546875 0.4106472134590149\n",
            "tcost icost -1.6435546875 0.41036686301231384\n",
            "search -1.2333984375\n",
            "tcost icost -0.77978515625 0.43276333808898926\n",
            "tcost icost -1.6279296875 0.4105331003665924\n",
            "tcost icost -1.6279296875 0.41050487756729126\n",
            "search -1.2177734375\n",
            "tcost icost -1.66015625 0.4103526771068573\n",
            "tcost icost -1.66015625 0.4096651077270508\n",
            "tcost icost -1.66015625 0.41073209047317505\n",
            "search -1.25\n",
            "tcost icost -1.4580078125 0.4143930971622467\n",
            "tcost icost -1.6865234375 0.4086810052394867\n",
            "tcost icost -1.6865234375 0.40895119309425354\n",
            "search -1.27734375\n",
            "tcost icost -1.44140625 0.41441774368286133\n",
            "tcost icost -1.65625 0.4096342921257019\n",
            "tcost icost -1.65625 0.4100951850414276\n",
            "search -1.24609375\n",
            "tcost icost -0.544921875 0.4378963112831116\n",
            "tcost icost -1.69140625 0.4085314869880676\n",
            "tcost icost -1.69140625 0.4091678261756897\n",
            "search -1.2822265625\n",
            "tcost icost -0.48291015625 0.43824297189712524\n",
            "tcost icost -1.654296875 0.4094991981983185\n",
            "tcost icost -1.654296875 0.4094647467136383\n",
            "search -1.2451171875\n",
            "tcost icost -0.89697265625 0.42967161536216736\n",
            "tcost icost -1.6669921875 0.409528523683548\n",
            "tcost icost -1.6669921875 0.41072529554367065\n",
            "search -1.255859375\n",
            "tcost icost -0.2900390625 0.44291192293167114\n",
            "tcost icost -1.67578125 0.4096391499042511\n",
            "tcost icost -1.67578125 0.4101487696170807\n",
            "search -1.265625\n",
            "tcost icost -0.1605224609375 0.44646862149238586\n",
            "tcost icost -1.6923828125 0.40919509530067444\n",
            "tcost icost -1.6923828125 0.4104844331741333\n",
            "search -1.2822265625\n",
            "tcost icost -0.53125 0.43968039751052856\n",
            "tcost icost -1.6611328125 0.41037803888320923\n",
            "tcost icost -1.6611328125 0.4102266728878021\n",
            "search -1.2509765625\n",
            "tcost icost -0.485107421875 0.4403553605079651\n",
            "tcost icost -1.650390625 0.4120732247829437\n",
            "tcost icost -1.650390625 0.4113536477088928\n",
            "search -1.2392578125\n",
            "tcost icost -1.25 0.4222277104854584\n",
            "tcost icost -1.6962890625 0.4104585647583008\n",
            "tcost icost -1.6962890625 0.4105662405490875\n",
            "search -1.28515625\n",
            "tcost icost -0.2359619140625 0.4441286325454712\n",
            "tcost icost -1.6494140625 0.4112439453601837\n",
            "tcost icost -1.6494140625 0.4112398624420166\n",
            "search -1.23828125\n",
            "tcost icost -0.436767578125 0.440263956785202\n",
            "tcost icost -1.61328125 0.4131247401237488\n",
            "tcost icost -1.61328125 0.4115160405635834\n",
            "search -1.201171875\n",
            "tcost icost -0.8388671875 0.43157678842544556\n",
            "tcost icost -1.6064453125 0.4120607376098633\n",
            "tcost icost -1.6064453125 0.411992609500885\n",
            "search -1.1943359375\n",
            "tcost icost -0.09173583984375 0.44627612829208374\n",
            "tcost icost -1.607421875 0.41220417618751526\n",
            "tcost icost -1.607421875 0.4141561686992645\n",
            "search -1.193359375\n",
            "tcost icost -0.272705078125 0.44359368085861206\n",
            "tcost icost -1.6806640625 0.4105621874332428\n",
            "tcost icost -1.6806640625 0.41041600704193115\n",
            "search -1.2705078125\n",
            "tcost icost -0.88818359375 0.43185320496559143\n",
            "tcost icost -1.650390625 0.41144901514053345\n",
            "tcost icost -1.650390625 0.41153833270072937\n",
            "search -1.23828125\n",
            "tcost icost -1.2294921875 0.42341846227645874\n",
            "tcost icost -1.669921875 0.41277822852134705\n",
            "tcost icost -1.669921875 0.4118405878543854\n",
            "search -1.2578125\n",
            "tcost icost -1.1904296875 0.4250687062740326\n",
            "tcost icost -1.6220703125 0.4132690727710724\n",
            "tcost icost -1.6220703125 0.41388219594955444\n",
            "search -1.2080078125\n",
            "tcost icost -1.6875 0.41160351037979126\n",
            "tcost icost -1.6875 0.41175681352615356\n",
            "tcost icost -1.6875 0.4113961458206177\n",
            "search -1.2763671875\n",
            "tcost icost -1.2255859375 0.42428913712501526\n",
            "tcost icost -1.7041015625 0.4110187292098999\n",
            "tcost icost -1.7041015625 0.4109721779823303\n",
            "search -1.29296875\n",
            "tcost icost -0.953125 0.4310002326965332\n",
            "tcost icost -1.7021484375 0.4107600450515747\n",
            "tcost icost -1.7021484375 0.4108522832393646\n",
            "search -1.291015625\n",
            "tcost icost -0.96240234375 0.43139946460723877\n",
            "tcost icost -1.7021484375 0.4111713171005249\n",
            "tcost icost -1.7021484375 0.41105717420578003\n",
            "search -1.291015625\n",
            "tcost icost -1.4951171875 0.4165170192718506\n",
            "tcost icost -1.689453125 0.41168537735939026\n",
            "tcost icost -1.689453125 0.41247543692588806\n",
            "search -1.27734375\n",
            "tcost icost -1.6611328125 0.41209033131599426\n",
            "tcost icost -1.6611328125 0.4117083251476288\n",
            "tcost icost -1.6611328125 0.4127233624458313\n",
            "search -1.248046875\n",
            "tcost icost -0.46533203125 0.4420603811740875\n",
            "tcost icost -1.6259765625 0.41382139921188354\n",
            "tcost icost -1.6259765625 0.41365668177604675\n",
            "search -1.212890625\n",
            "tcost icost -0.83251953125 0.4341525733470917\n",
            "tcost icost -1.603515625 0.41307130455970764\n",
            "tcost icost -1.603515625 0.41388240456581116\n",
            "search -1.189453125\n",
            "tcost icost -1.4609375 0.41703516244888306\n",
            "tcost icost -1.66015625 0.4118078649044037\n",
            "tcost icost -1.66015625 0.41257283091545105\n",
            "search -1.248046875\n",
            "tcost icost -0.2412109375 0.44484105706214905\n",
            "tcost icost -1.6494140625 0.41225457191467285\n",
            "tcost icost -1.6494140625 0.412203311920166\n",
            "search -1.2373046875\n",
            "tcost icost -0.89990234375 0.43312346935272217\n",
            "tcost icost -1.68359375 0.41250279545783997\n",
            "tcost icost -1.68359375 0.4124108552932739\n",
            "search -1.271484375\n",
            "tcost icost -0.94384765625 0.4333907663822174\n",
            "tcost icost -1.701171875 0.4128452241420746\n",
            "tcost icost -1.701171875 0.4118066728115082\n",
            "search -1.2890625\n",
            "tcost icost -0.48388671875 0.44260257482528687\n",
            "tcost icost -1.6396484375 0.41358107328414917\n",
            "tcost icost -1.6396484375 0.41330406069755554\n",
            "search -1.2265625\n",
            "tcost icost -0.90087890625 0.4332526624202728\n",
            "tcost icost -1.6572265625 0.4134312570095062\n",
            "tcost icost -1.6572265625 0.4136020243167877\n",
            "search -1.244140625\n",
            "tcost icost -0.8916015625 0.4335601031780243\n",
            "tcost icost -1.6513671875 0.41492530703544617\n",
            "tcost icost -1.6513671875 0.41356760263442993\n",
            "search -1.23828125\n",
            "tcost icost -0.7724609375 0.43551912903785706\n",
            "tcost icost -1.5888671875 0.4157642424106598\n",
            "tcost icost -1.5888671875 0.4149467647075653\n",
            "search -1.173828125\n",
            "tcost icost -0.72509765625 0.4368125796318054\n",
            "tcost icost -1.564453125 0.4157561957836151\n",
            "tcost icost -1.564453125 0.41581103205680847\n",
            "search -1.1484375\n",
            "tcost icost -0.43701171875 0.444257527589798\n",
            "tcost icost -1.619140625 0.41483020782470703\n",
            "tcost icost -1.619140625 0.4148038923740387\n",
            "search -1.2041015625\n",
            "tcost icost -0.80859375 0.43556278944015503\n",
            "tcost icost -1.599609375 0.4149915874004364\n",
            "tcost icost -1.599609375 0.4146310091018677\n",
            "search -1.185546875\n",
            "tcost icost -0.25244140625 0.4472309947013855\n",
            "tcost icost -1.642578125 0.41428837180137634\n",
            "tcost icost -1.642578125 0.41420283913612366\n",
            "search -1.228515625\n",
            "tcost icost -0.83984375 0.4346446096897125\n",
            "tcost icost -1.6015625 0.41495898365974426\n",
            "tcost icost -1.6015625 0.4146999716758728\n",
            "search -1.1865234375\n",
            "tcost icost -0.09967041015625 0.4480687975883484\n",
            "tcost icost -1.6201171875 0.41383278369903564\n",
            "tcost icost -1.6201171875 0.41394951939582825\n",
            "search -1.2060546875\n",
            "tcost icost -1.22265625 0.4257434904575348\n",
            "tcost icost -1.6787109375 0.4136769473552704\n",
            "tcost icost -1.6787109375 0.41369208693504333\n",
            "search -1.265625\n",
            "tcost icost -0.1990966796875 0.44813960790634155\n",
            "tcost icost -1.626953125 0.41479188203811646\n",
            "tcost icost -1.626953125 0.4159719944000244\n",
            "search -1.2109375\n",
            "tcost icost -1.3876953125 0.420958548784256\n",
            "tcost icost -1.607421875 0.41606971621513367\n",
            "tcost icost -1.607421875 0.4152944087982178\n",
            "search -1.1923828125\n",
            "tcost icost -1.4296875 0.41985395550727844\n",
            "tcost icost -1.62890625 0.41475990414619446\n",
            "tcost icost -1.62890625 0.41510269045829773\n",
            "search -1.2138671875\n",
            "tcost icost -1.4501953125 0.4196782112121582\n",
            "tcost icost -1.6474609375 0.41580265760421753\n",
            "tcost icost -1.6474609375 0.41490504145622253\n",
            "search -1.232421875\n",
            "tcost icost -1.4033203125 0.42124059796333313\n",
            "tcost icost -1.6318359375 0.4150744378566742\n",
            "tcost icost -1.6318359375 0.4149443209171295\n",
            "search -1.216796875\n",
            "tcost icost -0.450439453125 0.44489237666130066\n",
            "tcost icost -1.654296875 0.4138270616531372\n",
            "tcost icost -1.654296875 0.4138306975364685\n",
            "search -1.240234375\n",
            "tcost icost -0.2261962890625 0.44801998138427734\n",
            "tcost icost -1.6494140625 0.41433823108673096\n",
            "tcost icost -1.6494140625 0.41491761803627014\n",
            "search -1.234375\n",
            "tcost icost -0.489501953125 0.4449467658996582\n",
            "tcost icost -1.6142578125 0.4156045615673065\n",
            "tcost icost -1.6142578125 0.41635048389434814\n",
            "search -1.1982421875\n",
            "tcost icost -0.78125 0.4397149980068207\n",
            "tcost icost -1.638671875 0.41562873125076294\n",
            "tcost icost -1.638671875 0.4152095317840576\n",
            "search -1.2236328125\n",
            "tcost icost -0.5244140625 0.4445849359035492\n",
            "tcost icost -1.66796875 0.4141887426376343\n",
            "tcost icost -1.66796875 0.4141481816768646\n",
            "search -1.25390625\n",
            "tcost icost -1.111328125 0.43016543984413147\n",
            "tcost icost -1.626953125 0.415952205657959\n",
            "tcost icost -1.626953125 0.41680824756622314\n",
            "search -1.2099609375\n",
            "tcost icost -0.465576171875 0.4469169080257416\n",
            "tcost icost -1.6533203125 0.41532403230667114\n",
            "tcost icost -1.6533203125 0.4154614806175232\n",
            "search -1.23828125\n",
            "tcost icost -1.224609375 0.4277726113796234\n",
            "tcost icost -1.6962890625 0.4140019416809082\n",
            "tcost icost -1.6962890625 0.4148326814174652\n",
            "search -1.28125\n",
            "tcost icost -1.48046875 0.4199270009994507\n",
            "tcost icost -1.6787109375 0.4143708348274231\n",
            "tcost icost -1.6787109375 0.41395774483680725\n",
            "search -1.2646484375\n",
            "tcost icost -0.105712890625 0.44947129487991333\n",
            "tcost icost -1.6298828125 0.41463178396224976\n",
            "tcost icost -1.6298828125 0.414752334356308\n",
            "search -1.21484375\n",
            "tcost icost -0.900390625 0.4345484972000122\n",
            "tcost icost -1.654296875 0.4140312969684601\n",
            "tcost icost -1.654296875 0.41476839780807495\n",
            "search -1.2392578125\n",
            "tcost icost -0.95068359375 0.4328395128250122\n",
            "tcost icost -1.693359375 0.4127279818058014\n",
            "tcost icost -1.693359375 0.41315776109695435\n",
            "search -1.2802734375\n",
            "tcost icost -0.90576171875 0.43384987115859985\n",
            "tcost icost -1.6298828125 0.41481319069862366\n",
            "tcost icost -1.6298828125 0.41492408514022827\n",
            "search -1.21484375\n",
            "tcost icost -0.223876953125 0.4473831355571747\n",
            "tcost icost -1.6123046875 0.4152696132659912\n",
            "tcost icost -1.6123046875 0.41510650515556335\n",
            "search -1.197265625\n",
            "tcost icost -0.5966796875 0.4427533745765686\n",
            "tcost icost -1.642578125 0.415178507566452\n",
            "tcost icost -1.642578125 0.4157308042049408\n",
            "search -1.2265625\n",
            "tcost icost -0.051544189453125 0.4504333734512329\n",
            "tcost icost -1.578125 0.41643139719963074\n",
            "tcost icost -1.578125 0.4164814054965973\n",
            "search -1.162109375\n",
            "tcost icost -1.32421875 0.42335769534111023\n",
            "tcost icost -1.53125 0.4175509810447693\n",
            "tcost icost -1.53125 0.41834932565689087\n",
            "search -1.11328125\n",
            "tcost icost -0.87841796875 0.4346271753311157\n",
            "tcost icost -1.63671875 0.41532638669013977\n",
            "tcost icost -1.63671875 0.4151899814605713\n",
            "search -1.2216796875\n",
            "tcost icost -0.259521484375 0.44703027606010437\n",
            "tcost icost -1.642578125 0.4146367907524109\n",
            "tcost icost -1.642578125 0.4153926372528076\n",
            "search -1.2275390625\n",
            "tcost icost -0.9345703125 0.4341973662376404\n",
            "tcost icost -1.6806640625 0.41466623544692993\n",
            "tcost icost -1.6806640625 0.4148869812488556\n",
            "search -1.265625\n",
            "tcost icost -0.93505859375 0.436055064201355\n",
            "tcost icost -1.689453125 0.4153665006160736\n",
            "tcost icost -1.689453125 0.41532784700393677\n",
            "search -1.2744140625\n",
            "tcost icost -0.224365234375 0.4511166214942932\n",
            "tcost icost -1.61328125 0.4175375998020172\n",
            "tcost icost -1.61328125 0.41762420535087585\n",
            "search -1.1953125\n",
            "tcost icost -1.1669921875 0.4286215007305145\n",
            "tcost icost -1.673828125 0.4165872037410736\n",
            "tcost icost -1.673828125 0.4159427881240845\n",
            "search -1.2578125\n",
            "tcost icost -0.9267578125 0.43671828508377075\n",
            "tcost icost -1.716796875 0.41490551829338074\n",
            "tcost icost -1.716796875 0.41476187109947205\n",
            "search -1.3017578125\n",
            "tcost icost -0.86865234375 0.4366273283958435\n",
            "tcost icost -1.66015625 0.41724419593811035\n",
            "tcost icost -1.66015625 0.4162987470626831\n",
            "search -1.244140625\n",
            "tcost icost -0.87060546875 0.43685612082481384\n",
            "tcost icost -1.6298828125 0.41735759377479553\n",
            "tcost icost -1.6298828125 0.4168054163455963\n",
            "search -1.212890625\n",
            "tcost icost -0.88671875 0.436142235994339\n",
            "tcost icost -1.6357421875 0.41684091091156006\n",
            "tcost icost -1.6357421875 0.4176996350288391\n",
            "search -1.2177734375\n",
            "tcost icost -0.474365234375 0.44585171341896057\n",
            "tcost icost -1.6064453125 0.4178631603717804\n",
            "tcost icost -1.6064453125 0.41783472895622253\n",
            "search -1.1884765625\n",
            "tcost icost -0.1646728515625 0.4518323838710785\n",
            "tcost icost -1.6826171875 0.4167827367782593\n",
            "tcost icost -1.6826171875 0.41667819023132324\n",
            "search -1.265625\n",
            "tcost icost -0.787109375 0.44071346521377563\n",
            "tcost icost -1.6181640625 0.4181259572505951\n",
            "tcost icost -1.6181640625 0.4177679717540741\n",
            "search -1.2001953125\n",
            "tcost icost -1.1591796875 0.4302327334880829\n",
            "tcost icost -1.640625 0.4170421361923218\n",
            "tcost icost -1.640625 0.4172402620315552\n",
            "search -1.2236328125\n",
            "tcost icost -0.232421875 0.4496062994003296\n",
            "tcost icost -1.61328125 0.41805127263069153\n",
            "tcost icost -1.61328125 0.4176276624202728\n",
            "search -1.1953125\n",
            "tcost icost -0.95068359375 0.4356237053871155\n",
            "tcost icost -1.6923828125 0.4157812297344208\n",
            "tcost icost -1.6923828125 0.41694626212120056\n",
            "search -1.275390625\n",
            "tcost icost -0.37939453125 0.44979187846183777\n",
            "tcost icost -1.60546875 0.41839590668678284\n",
            "tcost icost -1.60546875 0.4183804988861084\n",
            "search -1.1875\n",
            "tcost icost -1.142578125 0.4325038194656372\n",
            "tcost icost -1.6318359375 0.4180905222892761\n",
            "tcost icost -1.6318359375 0.41786009073257446\n",
            "search -1.2138671875\n",
            "ded\n",
            "time\n",
            "25 #### train ####\n",
            "repr, std, cov, closslb 0.0036602190230041742 0.47607421875 0.0001446844544261694 0.00651975255459547 0.0005268545355647802\n",
            "0.0064771674764534486 0.028209007496034975 1.0\n",
            "repr, std, cov, closslb 0.002923540771007538 0.47509765625 0.000187750905752182 9.433983905182686e-06 9.94423171505332e-05\n",
            "0.006425582539543229 0.027706037111210564 1.0\n",
            "repr, std, cov, closslb 0.0024959042202681303 0.47509765625 0.0001989088486880064 3.851573637803085e-05 6.659312202828005e-05\n",
            "0.00630470883273744 0.026887603009253393 1.0\n",
            "repr, std, cov, closslb 0.0042516980320215225 0.474609375 0.00018593226559460163 0.011573296040296555 2.668070919753518e-05\n",
            "0.00626701262635562 0.026434601636994536 1.0\n",
            "repr, std, cov, closslb 0.004228209611028433 0.474853515625 0.00019118771888315678 0.0041421945206820965 0.00024412003403995186\n",
            "0.006155270951189609 0.02567937936204927 1.0\n",
            "repr, std, cov, closslb 0.0034220910165458918 0.474853515625 0.00017780368216335773 1.2670845535467379e-05 0.0004126876301597804\n",
            "0.0061062496367282 0.025221512585928304 1.0\n",
            "repr, std, cov, closslb 0.005196250043809414 0.474609375 0.00019082543440163136 0.0036282946821302176 0.0003263824910391122\n",
            "0.006130711297204036 0.025120878247161554 1.0\n",
            "repr, std, cov, closslb 0.0040496462024748325 0.476318359375 0.00013288063928484917 0.003753776429221034 0.00012434138625394553\n",
            "0.00614297885050974 0.02519631626965865 1.0\n",
            "repr, std, cov, closslb 0.0060479892417788506 0.4755859375 0.00015582097694277763 0.0033780583180487156 7.637358794454485e-05\n",
            "0.006217101387820975 0.025171145124534118 1.0\n",
            "repr, std, cov, closslb 0.003651218954473734 0.475341796875 0.00015588244423270226 6.399889571184758e-06 0.01586126536130905\n",
            "0.0061737552360113 0.024970679112511527 1.0\n",
            "repr, std, cov, closslb 0.008939927443861961 0.4755859375 0.00015238276682794094 0.010498554445803165 7.654790533706546e-05\n",
            "0.006223318489208795 0.024846199404906708 1.0\n",
            "repr, std, cov, closslb 0.015402658842504025 0.4755859375 0.00017060642130672932 0.012689555995166302 0.0003994411090388894\n",
            "0.006235771349505701 0.024697642590401993 1.0\n",
            "repr, std, cov, closslb 0.009040911681950092 0.476318359375 0.00013498193584382534 0.01038916502147913 0.029523752629756927\n",
            "0.006254497377104036 0.02452544855683487 1.0\n",
            "repr, std, cov, closslb 0.0034373896196484566 0.4775390625 0.00010719848796725273 3.892061613441911e-06 0.0006581316702067852\n",
            "0.006292118304011115 0.024945733379132397 1.0\n",
            "repr, std, cov, closslb 0.003940174356102943 0.476318359375 0.00012361910194158554 0.004469045903533697 0.015872161835432053\n",
            "0.006329965521546523 0.02560249504383016 1.0\n",
            "repr, std, cov, closslb 0.006063270848244429 0.4755859375 0.0001291409134864807 0.0075773862190544605 0.013062414713203907\n",
            "0.00634897441433769 0.025653725636412862 1.0\n",
            "repr, std, cov, closslb 0.007882947102189064 0.475830078125 0.00011568563058972359 0.003266541752964258 0.0464790016412735\n",
            "0.006311013541570177 0.025145999125408713 1.0\n",
            "repr, std, cov, closslb 0.00673061516135931 0.47509765625 0.00013645482249557972 0.016604851931333542 0.03905665874481201\n",
            "0.00626075187448114 0.02482137802687983 1.0\n",
            "repr, std, cov, closslb 0.004446479957550764 0.475830078125 0.00013591395691037178 0.008043366484344006 0.00024600373581051826\n",
            "0.006167587648362938 0.024403188339102058 1.0\n",
            "repr, std, cov, closslb 0.002540611196309328 0.475830078125 0.00016520987264811993 2.5241906769224443e-06 0.000530734658241272\n",
            "0.006130711297204036 0.024378809529572488 1.0\n",
            "repr, std, cov, closslb 0.007270186208188534 0.4755859375 0.00013118842616677284 0.014053475111722946 0.013077223673462868\n",
            "0.006057618733881581 0.02372970844608729 1.0\n",
            "repr, std, cov, closslb 0.0051717618480324745 0.47509765625 0.0001614780630916357 0.0040703569538891315 0.015983641147613525\n",
            "0.005979418188416882 0.023329911168297943 1.0\n",
            "repr, std, cov, closslb 0.003663939656689763 0.475341796875 0.00015715928748250008 0.004127074498683214 0.028718261048197746\n",
            "0.0060274212928998 0.02335324107946624 1.0\n",
            "repr, std, cov, closslb 0.003810756839811802 0.47607421875 0.00011350610293447971 0.004231694154441357 0.00455548195168376\n",
            "0.0060636763526154625 0.02358782719340845 1.0\n",
            "repr, std, cov, closslb 0.0038819354958832264 0.4765625 0.00011403858661651611 0.0037993527948856354 0.014829408377408981\n",
            "0.005991383004211903 0.02321361074620726 1.0\n",
            "repr, std, cov, closslb 0.008657373487949371 0.476318359375 0.0001313991378992796 0.008155676536262035 0.00020143376605119556\n",
            "0.005997374387216114 0.023329911168297943 1.0\n",
            "repr, std, cov, closslb 0.007343903183937073 0.47607421875 0.00012897327542304993 0.003683855989947915 8.160608558682725e-05\n",
            "0.0060758097689970455 0.02358782719340845 1.0\n",
            "repr, std, cov, closslb 0.0029449991416186094 0.477294921875 0.00011631520465016365 7.226622255984694e-05 0.000297280348604545\n",
            "0.006130711297204036 0.023753438154533374 1.0\n",
            "repr, std, cov, closslb 0.002534066326916218 0.4775390625 0.00011289981193840504 1.0437412129249424e-05 0.013981134630739689\n",
            "0.00620468581151214 0.024184653773047588 1.0\n",
            "repr, std, cov, closslb 0.0059830257669091225 0.47607421875 0.00011337082833051682 0.004471549764275551 0.01529817096889019\n",
            "0.00626701262635562 0.02452544855683487 1.0\n",
            "repr, std, cov, closslb 0.004106117412447929 0.477294921875 0.00010443618521094322 0.004610867705196142 0.00038609004695899785\n",
            "0.00634897441433769 0.02509578246469686 1.0\n",
            "repr, std, cov, closslb 0.009617396630346775 0.4755859375 0.00013118237257003784 0.008112695068120956 0.0002181975869461894\n",
            "0.006336295487068069 0.02529725281344535 1.0\n",
            "repr, std, cov, closslb 0.0033456936944276094 0.47607421875 0.0001375274732708931 0.004767603240907192 0.015600036829710007\n",
            "0.006217101387820975 0.024574523979397092 1.0\n",
            "repr, std, cov, closslb 0.0032393329311162233 0.474853515625 0.000155268469825387 0.0030842158012092113 0.0007890210836194456\n",
            "0.006161426222140798 0.0242572803125127 1.0\n",
            "repr, std, cov, closslb 0.0025103013031184673 0.4755859375 0.00015839352272450924 1.4957729035813827e-05 0.019079208374023438\n",
            "0.006033448714192699 0.02349371129197447 1.0\n",
            "repr, std, cov, closslb 0.005457533057779074 0.474609375 0.00017261342145502567 0.007595105562359095 0.01452275738120079\n",
            "0.005943666912972825 0.02307481527051258 1.0\n",
            "tcost icost -5.74609375 0.38011863827705383\n",
            "tcost icost -3.75 0.4132768213748932\n",
            "tcost icost -3.75 0.4114972651004791\n",
            "search -3.337890625\n",
            "tcost icost -5.7578125 0.37793755531311035\n",
            "tcost icost -6.38671875 0.36772584915161133\n",
            "tcost icost -6.38671875 0.3673538863658905\n",
            "search -6.01953125\n",
            "tcost icost -4.96875 0.39249661564826965\n",
            "tcost icost -5.7890625 0.3804023861885071\n",
            "tcost icost -5.7890625 0.3798287808895111\n",
            "search -5.41015625\n",
            "tcost icost -3.763671875 0.4118156135082245\n",
            "tcost icost -5.7890625 0.37969672679901123\n",
            "tcost icost -5.7890625 0.3798139989376068\n",
            "search -5.41015625\n",
            "tcost icost -6.23046875 0.37179791927337646\n",
            "tcost icost -6.32421875 0.3690747022628784\n",
            "tcost icost -6.32421875 0.36873966455459595\n",
            "search -5.95703125\n",
            "tcost icost -5.73046875 0.38013696670532227\n",
            "tcost icost -5.7265625 0.3808763325214386\n",
            "tcost icost -5.7265625 0.3813176155090332\n",
            "search -5.34375\n",
            "tcost icost -4.5078125 0.40175294876098633\n",
            "tcost icost -5.94921875 0.37874966859817505\n",
            "tcost icost -5.94921875 0.3787086606025696\n",
            "search -5.5703125\n",
            "tcost icost -5.9765625 0.3762185573577881\n",
            "tcost icost -5.890625 0.37931570410728455\n",
            "tcost icost -5.890625 0.37945324182510376\n",
            "search -5.51171875\n",
            "tcost icost -6.015625 0.3760167956352234\n",
            "tcost icost -5.9296875 0.3791635036468506\n",
            "tcost icost -5.9296875 0.3788646459579468\n",
            "search -5.55078125\n",
            "tcost icost -6.3046875 0.3728102147579193\n",
            "tcost icost -3.794921875 0.41282060742378235\n",
            "tcost icost -3.794921875 0.4138038754463196\n",
            "search -3.380859375\n",
            "tcost icost -5.6484375 0.38314610719680786\n",
            "tcost icost -5.703125 0.38240936398506165\n",
            "tcost icost -5.703125 0.38287118077278137\n",
            "search -5.3203125\n",
            "tcost icost -4.93359375 0.39493218064308167\n",
            "tcost icost -5.6953125 0.38292646408081055\n",
            "tcost icost -5.6953125 0.38284748792648315\n",
            "search -5.3125\n",
            "tcost icost -5.7890625 0.3805965185165405\n",
            "tcost icost -5.74609375 0.38267382979393005\n",
            "tcost icost -5.74609375 0.3824138939380646\n",
            "search -5.36328125\n",
            "tcost icost -5.26953125 0.3894726037979126\n",
            "tcost icost -5.9765625 0.3793838322162628\n",
            "tcost icost -5.9765625 0.3792716860771179\n",
            "search -5.59765625\n",
            "tcost icost -4.9765625 0.3950396478176117\n",
            "tcost icost -5.734375 0.3826207220554352\n",
            "tcost icost -5.734375 0.38283464312553406\n",
            "search -5.3515625\n",
            "tcost icost -5.90234375 0.3792688250541687\n",
            "tcost icost -5.80859375 0.3818834722042084\n",
            "tcost icost -5.80859375 0.3822645843029022\n",
            "search -5.42578125\n",
            "tcost icost -5.72265625 0.3813905715942383\n",
            "tcost icost -6.26953125 0.37354254722595215\n",
            "tcost icost -6.26953125 0.37293359637260437\n",
            "search -5.8984375\n",
            "tcost icost -6.04296875 0.37848594784736633\n",
            "tcost icost -5.90625 0.3814549446105957\n",
            "tcost icost -5.90625 0.38172054290771484\n",
            "search -5.5234375\n",
            "tcost icost -5.69140625 0.3824838101863861\n",
            "tcost icost -3.904296875 0.4130578935146332\n",
            "tcost icost -3.904296875 0.41302964091300964\n",
            "search -3.4921875\n",
            "tcost icost -6.49609375 0.37163203954696655\n",
            "tcost icost -4.09765625 0.4104379117488861\n",
            "tcost icost -4.09765625 0.41098785400390625\n",
            "search -3.6875\n",
            "tcost icost -6.28125 0.3736550807952881\n",
            "tcost icost -3.8046875 0.4145358204841614\n",
            "tcost icost -3.8046875 0.4147089123725891\n",
            "search -3.390625\n",
            "tcost icost -4.01171875 0.4111296832561493\n",
            "tcost icost -5.74609375 0.38415366411209106\n",
            "tcost icost -5.74609375 0.383783757686615\n",
            "search -5.36328125\n",
            "tcost icost -4.2265625 0.408162385225296\n",
            "tcost icost -5.390625 0.38823673129081726\n",
            "tcost icost -5.390625 0.3884297311306\n",
            "search -5.00390625\n",
            "tcost icost -3.23828125 0.423831582069397\n",
            "tcost icost -4.09765625 0.4089902937412262\n",
            "tcost icost -4.09765625 0.40870270133018494\n",
            "search -3.689453125\n",
            "tcost icost -5.80859375 0.38083532452583313\n",
            "tcost icost -5.75390625 0.3819667398929596\n",
            "tcost icost -5.75390625 0.38224709033966064\n",
            "search -5.37109375\n",
            "tcost icost -5.76953125 0.3804776072502136\n",
            "tcost icost -6.40625 0.3702054023742676\n",
            "tcost icost -6.40625 0.3705423176288605\n",
            "search -6.03515625\n",
            "tcost icost -6.5 0.36852604150772095\n",
            "tcost icost -5.83984375 0.38140472769737244\n",
            "tcost icost -5.83984375 0.3812013566493988\n",
            "search -5.45703125\n",
            "tcost icost -4.19921875 0.40948382019996643\n",
            "tcost icost -5.67578125 0.38472747802734375\n",
            "tcost icost -5.67578125 0.3845005929470062\n",
            "search -5.29296875\n",
            "tcost icost -6.05078125 0.3771716058254242\n",
            "tcost icost -6.640625 0.3685971796512604\n",
            "tcost icost -6.640625 0.36769601702690125\n",
            "search -6.2734375\n",
            "tcost icost -6.57421875 0.3695773184299469\n",
            "tcost icost -3.9765625 0.41200217604637146\n",
            "tcost icost -3.9765625 0.4116276502609253\n",
            "search -3.564453125\n",
            "tcost icost -4.609375 0.40236517786979675\n",
            "tcost icost -6.21875 0.3772500455379486\n",
            "tcost icost -6.21875 0.3777177631855011\n",
            "search -5.83984375\n",
            "tcost icost -6.08984375 0.3762867748737335\n",
            "tcost icost -6.6015625 0.3678491711616516\n",
            "tcost icost -6.6015625 0.3679696023464203\n",
            "search -6.234375\n",
            "tcost icost -6.26953125 0.37309470772743225\n",
            "tcost icost -6.21484375 0.3766678273677826\n",
            "tcost icost -6.21484375 0.37685626745224\n",
            "search -5.8359375\n",
            "tcost icost -6.296875 0.37312909960746765\n",
            "tcost icost -6.1875 0.37748992443084717\n",
            "tcost icost -6.1875 0.3791780173778534\n",
            "search -5.80859375\n",
            "tcost icost -6.828125 0.36470872163772583\n",
            "tcost icost -6.3359375 0.37616991996765137\n",
            "tcost icost -6.3359375 0.3758188784122467\n",
            "search -5.9609375\n",
            "tcost icost -5.4921875 0.38703209161758423\n",
            "tcost icost -6.2734375 0.37712961435317993\n",
            "tcost icost -6.2734375 0.3771244287490845\n",
            "search -5.89453125\n",
            "tcost icost -6.73828125 0.3668803870677948\n",
            "tcost icost -6.2265625 0.3783651888370514\n",
            "tcost icost -6.2265625 0.37839600443840027\n",
            "search -5.84765625\n",
            "tcost icost -5.19921875 0.3928287625312805\n",
            "tcost icost -6.01171875 0.3812025487422943\n",
            "tcost icost -6.01171875 0.3816573917865753\n",
            "search -5.62890625\n",
            "tcost icost -5.9453125 0.3801615238189697\n",
            "tcost icost -5.97265625 0.3820178806781769\n",
            "tcost icost -5.97265625 0.38199615478515625\n",
            "search -5.58984375\n",
            "tcost icost -6.55078125 0.3693575859069824\n",
            "tcost icost -5.875 0.3829401135444641\n",
            "tcost icost -5.875 0.383059024810791\n",
            "search -5.4921875\n",
            "tcost icost -6.00390625 0.37821629643440247\n",
            "tcost icost -5.8125 0.38339126110076904\n",
            "tcost icost -5.8125 0.3828867971897125\n",
            "search -5.4296875\n",
            "tcost icost -6.1328125 0.3769853413105011\n",
            "tcost icost -6.125 0.3800157606601715\n",
            "tcost icost -6.125 0.3806994557380676\n",
            "search -5.74609375\n",
            "tcost icost -5.85546875 0.38152506947517395\n",
            "tcost icost -6.63671875 0.36842185258865356\n",
            "tcost icost -6.63671875 0.3683813214302063\n",
            "search -6.26953125\n",
            "tcost icost -6.75 0.3667323887348175\n",
            "tcost icost -6.17578125 0.3792842626571655\n",
            "tcost icost -6.17578125 0.38043898344039917\n",
            "search -5.796875\n",
            "tcost icost -6.3046875 0.3743564188480377\n",
            "tcost icost -6.24609375 0.3784393072128296\n",
            "tcost icost -6.24609375 0.37845686078071594\n",
            "search -5.8671875\n",
            "tcost icost -6.40234375 0.37216341495513916\n",
            "tcost icost -6.4140625 0.3765150010585785\n",
            "tcost icost -6.4140625 0.37652480602264404\n",
            "search -6.0390625\n",
            "tcost icost -6.578125 0.37099945545196533\n",
            "tcost icost -6.953125 0.3642396926879883\n",
            "tcost icost -6.953125 0.3649352192878723\n",
            "search -6.58984375\n",
            "tcost icost -4.734375 0.4013591706752777\n",
            "tcost icost -6.1875 0.3803700804710388\n",
            "tcost icost -6.1875 0.3803273141384125\n",
            "search -5.80859375\n",
            "tcost icost -6.625 0.370942622423172\n",
            "tcost icost -6.16796875 0.38159456849098206\n",
            "tcost icost -6.16796875 0.3816487789154053\n",
            "search -5.78515625\n",
            "tcost icost -6.58984375 0.3727928698062897\n",
            "tcost icost -4.296875 0.40845921635627747\n",
            "tcost icost -4.296875 0.4086185693740845\n",
            "search -3.888671875\n",
            "tcost icost -6.6953125 0.3699902892112732\n",
            "tcost icost -4.19921875 0.40952008962631226\n",
            "tcost icost -4.19921875 0.40947991609573364\n",
            "search -3.7890625\n",
            "tcost icost -5.4609375 0.387897253036499\n",
            "tcost icost -5.95703125 0.3820893168449402\n",
            "tcost icost -5.95703125 0.38206788897514343\n",
            "search -5.57421875\n",
            "tcost icost -6.08984375 0.3804056942462921\n",
            "tcost icost -4.1328125 0.4105907678604126\n",
            "tcost icost -4.1328125 0.4102869927883148\n",
            "search -3.72265625\n",
            "tcost icost -4.67578125 0.4019966423511505\n",
            "tcost icost -6.0703125 0.38103824853897095\n",
            "tcost icost -6.0703125 0.3812166452407837\n",
            "search -5.6875\n",
            "tcost icost -5.875 0.38080230355262756\n",
            "tcost icost -5.99609375 0.3821389973163605\n",
            "tcost icost -5.99609375 0.3821582496166229\n",
            "search -5.61328125\n",
            "tcost icost -6.2421875 0.37363380193710327\n",
            "tcost icost -6.21484375 0.3775174617767334\n",
            "tcost icost -6.21484375 0.377803236246109\n",
            "search -5.8359375\n",
            "tcost icost -6.29296875 0.3735942542552948\n",
            "tcost icost -4.01171875 0.41181591153144836\n",
            "tcost icost -4.01171875 0.4113113582134247\n",
            "search -3.599609375\n",
            "tcost icost -5.04296875 0.3941916525363922\n",
            "tcost icost -5.921875 0.38191846013069153\n",
            "tcost icost -5.921875 0.3817521333694458\n",
            "search -5.5390625\n",
            "tcost icost -6.23828125 0.37743130326271057\n",
            "tcost icost -3.984375 0.41261792182922363\n",
            "tcost icost -3.984375 0.4128754436969757\n",
            "search -3.572265625\n",
            "tcost icost -5.61328125 0.38498228788375854\n",
            "tcost icost -6.390625 0.37576600909233093\n",
            "tcost icost -6.390625 0.37590497732162476\n",
            "search -6.015625\n",
            "tcost icost -6.21875 0.3763391077518463\n",
            "tcost icost -6.2265625 0.37449660897254944\n",
            "tcost icost -6.2265625 0.37461456656455994\n",
            "search -5.8515625\n",
            "tcost icost -4.1484375 0.4104083776473999\n",
            "tcost icost -4.98828125 0.39364948868751526\n",
            "tcost icost -4.98828125 0.393619567155838\n",
            "search -4.59375\n",
            "tcost icost -4.89453125 0.396757572889328\n",
            "tcost icost -5.359375 0.38974258303642273\n",
            "tcost icost -5.359375 0.38872262835502625\n",
            "search -4.96875\n",
            "tcost icost -6.828125 0.3635067343711853\n",
            "tcost icost -6.02734375 0.3784105181694031\n",
            "tcost icost -6.02734375 0.37922802567481995\n",
            "search -5.6484375\n",
            "tcost icost -5.453125 0.38600775599479675\n",
            "tcost icost -6.05859375 0.3772174119949341\n",
            "tcost icost -6.05859375 0.37810829281806946\n",
            "search -5.6796875\n",
            "tcost icost -6.0234375 0.37600287795066833\n",
            "tcost icost -5.89453125 0.3806324303150177\n",
            "tcost icost -5.89453125 0.38041016459465027\n",
            "search -5.515625\n",
            "tcost icost -5.64453125 0.3825346827507019\n",
            "tcost icost -6.12109375 0.37640535831451416\n",
            "tcost icost -6.12109375 0.3759697675704956\n",
            "search -5.74609375\n",
            "tcost icost -6.14453125 0.3742337226867676\n",
            "tcost icost -5.859375 0.3807448148727417\n",
            "tcost icost -5.859375 0.3805689215660095\n",
            "search -5.48046875\n",
            "tcost icost -5.71484375 0.3808128237724304\n",
            "tcost icost -6.34375 0.3735562860965729\n",
            "tcost icost -6.34375 0.37352466583251953\n",
            "search -5.96875\n",
            "tcost icost -4.6015625 0.4000028669834137\n",
            "tcost icost -6.24609375 0.37465137243270874\n",
            "tcost icost -6.24609375 0.3745238482952118\n",
            "search -5.87109375\n",
            "tcost icost -5.84765625 0.3789193034172058\n",
            "tcost icost -5.828125 0.38097083568573\n",
            "tcost icost -5.828125 0.3809438645839691\n",
            "search -5.4453125\n",
            "tcost icost -6.35546875 0.37063610553741455\n",
            "tcost icost -6.1171875 0.37650415301322937\n",
            "tcost icost -6.1171875 0.3766644299030304\n",
            "search -5.7421875\n",
            "tcost icost -6.125 0.37670350074768066\n",
            "tcost icost -4.29296875 0.4055081009864807\n",
            "tcost icost -4.29296875 0.4056517481803894\n",
            "search -3.88671875\n",
            "tcost icost -6.25 0.37180352210998535\n",
            "tcost icost -4.33203125 0.40465521812438965\n",
            "tcost icost -4.33203125 0.40570691227912903\n",
            "search -3.92578125\n",
            "tcost icost -6.41796875 0.36958301067352295\n",
            "tcost icost -6.29296875 0.3750811517238617\n",
            "tcost icost -6.29296875 0.37483832240104675\n",
            "search -5.91796875\n",
            "tcost icost -6.703125 0.36776915192604065\n",
            "tcost icost -4.24609375 0.4065764546394348\n",
            "tcost icost -4.24609375 0.40673547983169556\n",
            "search -3.83984375\n",
            "tcost icost -6.28125 0.37195372581481934\n",
            "tcost icost -6.6484375 0.3669302463531494\n",
            "tcost icost -6.6484375 0.36681798100471497\n",
            "search -6.28125\n",
            "tcost icost -6.703125 0.3671228289604187\n",
            "tcost icost -4.19921875 0.40793490409851074\n",
            "tcost icost -4.19921875 0.4065452218055725\n",
            "search -3.79296875\n",
            "tcost icost -5.890625 0.37779784202575684\n",
            "tcost icost -6.1953125 0.3730870187282562\n",
            "tcost icost -6.1953125 0.3730566203594208\n",
            "search -5.8203125\n",
            "tcost icost -5.46875 0.3860640227794647\n",
            "tcost icost -5.89453125 0.3803463280200958\n",
            "tcost icost -5.89453125 0.3805411756038666\n",
            "search -5.515625\n",
            "tcost icost -4.69921875 0.3994213938713074\n",
            "tcost icost -6.015625 0.37902307510375977\n",
            "tcost icost -6.015625 0.37916263937950134\n",
            "search -5.63671875\n",
            "tcost icost -4.49609375 0.4025411903858185\n",
            "tcost icost -5.99609375 0.37929773330688477\n",
            "tcost icost -5.99609375 0.37888363003730774\n",
            "search -5.6171875\n",
            "tcost icost -6.375 0.3697216510772705\n",
            "tcost icost -6.23828125 0.3750138580799103\n",
            "tcost icost -6.23828125 0.3749021291732788\n",
            "search -5.86328125\n",
            "tcost icost -6.58203125 0.3675953149795532\n",
            "tcost icost -4.328125 0.40457549691200256\n",
            "tcost icost -4.328125 0.4043653905391693\n",
            "search -3.923828125\n",
            "tcost icost -5.3671875 0.3873097598552704\n",
            "tcost icost -6.140625 0.37689948081970215\n",
            "tcost icost -6.140625 0.37600433826446533\n",
            "search -5.765625\n",
            "tcost icost -6.28125 0.37199294567108154\n",
            "tcost icost -6.13671875 0.375980406999588\n",
            "tcost icost -6.13671875 0.3764916956424713\n",
            "search -5.76171875\n",
            "tcost icost -5.76171875 0.38057684898376465\n",
            "tcost icost -6.5 0.37165558338165283\n",
            "tcost icost -6.5 0.3714108467102051\n",
            "search -6.12890625\n",
            "tcost icost -6.43359375 0.3694625198841095\n",
            "tcost icost -4.41796875 0.402955025434494\n",
            "tcost icost -4.41796875 0.402864933013916\n",
            "search -4.015625\n",
            "tcost icost -6.42578125 0.37020203471183777\n",
            "tcost icost -4.09375 0.4081818461418152\n",
            "tcost icost -4.09375 0.40829089283943176\n",
            "search -3.685546875\n",
            "tcost icost -5.74609375 0.38070470094680786\n",
            "tcost icost -5.9296875 0.37999725341796875\n",
            "tcost icost -5.9296875 0.38025668263435364\n",
            "search -5.55078125\n",
            "tcost icost -3.970703125 0.41079840064048767\n",
            "tcost icost -5.51953125 0.38508859276771545\n",
            "tcost icost -5.51953125 0.38501909375190735\n",
            "search -5.1328125\n",
            "tcost icost -4.0703125 0.4103868901729584\n",
            "tcost icost -5.77734375 0.38278961181640625\n",
            "tcost icost -5.77734375 0.38275107741355896\n",
            "search -5.39453125\n",
            "tcost icost -6.1015625 0.3729446828365326\n",
            "tcost icost -5.51171875 0.38525688648223877\n",
            "tcost icost -5.51171875 0.38473525643348694\n",
            "search -5.125\n",
            "tcost icost -3.8125 0.41352567076683044\n",
            "tcost icost -5.42578125 0.38688984513282776\n",
            "tcost icost -5.42578125 0.38729265332221985\n",
            "search -5.0390625\n",
            "tcost icost -5.8125 0.37864285707473755\n",
            "tcost icost -5.578125 0.3816731870174408\n",
            "tcost icost -5.578125 0.3818615674972534\n",
            "search -5.1953125\n",
            "tcost icost -3.595703125 0.4161474108695984\n",
            "tcost icost -5.22265625 0.3880983591079712\n",
            "tcost icost -5.22265625 0.3884253203868866\n",
            "search -4.8359375\n",
            "tcost icost -4.29296875 0.4052051603794098\n",
            "tcost icost -5.921875 0.37847962975502014\n",
            "tcost icost -5.921875 0.3791571855545044\n",
            "search -5.54296875\n",
            "tcost icost -5.4453125 0.3837102949619293\n",
            "tcost icost -6.203125 0.3727850914001465\n",
            "tcost icost -6.203125 0.37308478355407715\n",
            "search -5.828125\n",
            "tcost icost -5.85546875 0.37707898020744324\n",
            "tcost icost -5.9375 0.378050297498703\n",
            "tcost icost -5.9375 0.37868621945381165\n",
            "search -5.55859375\n",
            "tcost icost -5.75390625 0.3812990188598633\n",
            "tcost icost -3.943359375 0.41019830107688904\n",
            "tcost icost -3.943359375 0.4103878140449524\n",
            "search -3.533203125\n",
            "tcost icost -6.1875 0.3732840120792389\n",
            "tcost icost -3.99609375 0.4093160927295685\n",
            "tcost icost -3.99609375 0.40917012095451355\n",
            "search -3.5859375\n",
            "tcost icost -6.50390625 0.36885255575180054\n",
            "tcost icost -6.66015625 0.3640064299106598\n",
            "tcost icost -6.66015625 0.364023894071579\n",
            "search -6.296875\n",
            "tcost icost -5.9609375 0.37610024213790894\n",
            "tcost icost -6.56640625 0.3661372661590576\n",
            "tcost icost -6.56640625 0.3660776615142822\n",
            "search -6.19921875\n",
            "tcost icost -5.50390625 0.38405370712280273\n",
            "tcost icost -6.23046875 0.37235426902770996\n",
            "tcost icost -6.23046875 0.3718780279159546\n",
            "search -5.859375\n",
            "tcost icost -4.01171875 0.40966227650642395\n",
            "tcost icost -5.59765625 0.3837032914161682\n",
            "tcost icost -5.59765625 0.38362738490104675\n",
            "search -5.21484375\n",
            "tcost icost -5.7109375 0.3804253339767456\n",
            "tcost icost -5.75390625 0.3815149664878845\n",
            "tcost icost -5.75390625 0.3812578618526459\n",
            "search -5.37109375\n",
            "tcost icost -6.27734375 0.3728969693183899\n",
            "tcost icost -3.9375 0.41094765067100525\n",
            "tcost icost -3.9375 0.4111759662628174\n",
            "search -3.52734375\n",
            "tcost icost -5.87109375 0.3792792558670044\n",
            "tcost icost -5.80859375 0.3819235861301422\n",
            "tcost icost -5.80859375 0.38240697979927063\n",
            "search -5.42578125\n",
            "tcost icost -5.20703125 0.3898450434207916\n",
            "tcost icost -5.94140625 0.38034069538116455\n",
            "tcost icost -5.94140625 0.3805735111236572\n",
            "search -5.5625\n",
            "tcost icost -5.71875 0.3808404803276062\n",
            "tcost icost -6.51171875 0.36777564883232117\n",
            "tcost icost -6.51171875 0.3680056035518646\n",
            "search -6.14453125\n",
            "tcost icost -5.5703125 0.38389405608177185\n",
            "tcost icost -5.59375 0.38442346453666687\n",
            "tcost icost -5.59375 0.3844052851200104\n",
            "search -5.2109375\n",
            "tcost icost -5.609375 0.3834037482738495\n",
            "tcost icost -5.63671875 0.38405078649520874\n",
            "tcost icost -5.63671875 0.38395529985427856\n",
            "search -5.25390625\n",
            "tcost icost -5.50390625 0.3845829367637634\n",
            "tcost icost -5.5625 0.3847676217556\n",
            "tcost icost -5.5625 0.3846112787723541\n",
            "search -5.1796875\n",
            "tcost icost -4.5390625 0.4005236327648163\n",
            "tcost icost -5.22265625 0.38922667503356934\n",
            "tcost icost -5.22265625 0.3894745111465454\n",
            "search -4.83203125\n",
            "tcost icost -4.0859375 0.40865758061408997\n",
            "tcost icost -5.80859375 0.38172438740730286\n",
            "tcost icost -5.80859375 0.3819705545902252\n",
            "search -5.42578125\n",
            "tcost icost -3.841796875 0.4118384122848511\n",
            "tcost icost -5.765625 0.3816074728965759\n",
            "tcost icost -5.765625 0.38163116574287415\n",
            "search -5.3828125\n",
            "tcost icost -5.80859375 0.38084420561790466\n",
            "tcost icost -3.8671875 0.4113842844963074\n",
            "tcost icost -3.8671875 0.4113287925720215\n",
            "search -3.455078125\n",
            "tcost icost -5.78515625 0.37878289818763733\n",
            "tcost icost -6.515625 0.3670526444911957\n",
            "tcost icost -6.515625 0.3662339448928833\n",
            "search -6.1484375\n",
            "tcost icost -5.8984375 0.3763401210308075\n",
            "tcost icost -5.40625 0.386037677526474\n",
            "tcost icost -5.40625 0.3866861164569855\n",
            "search -5.01953125\n",
            "tcost icost -5.109375 0.39021196961402893\n",
            "tcost icost -5.62109375 0.3827296793460846\n",
            "tcost icost -5.62109375 0.382310152053833\n",
            "search -5.23828125\n",
            "tcost icost -4.5 0.40098902583122253\n",
            "tcost icost -5.796875 0.37930741906166077\n",
            "tcost icost -5.796875 0.37903255224227905\n",
            "search -5.41796875\n",
            "tcost icost -5.57421875 0.38264214992523193\n",
            "tcost icost -5.84765625 0.3798741102218628\n",
            "tcost icost -5.84765625 0.37979522347450256\n",
            "search -5.46875\n",
            "tcost icost -3.75390625 0.4134003520011902\n",
            "tcost icost -5.08984375 0.3893008232116699\n",
            "tcost icost -5.08984375 0.38933447003364563\n",
            "search -4.69921875\n",
            "tcost icost -6.35546875 0.36952540278434753\n",
            "tcost icost -5.6484375 0.3828493356704712\n",
            "tcost icost -5.6484375 0.3835374712944031\n",
            "search -5.265625\n",
            "tcost icost -3.861328125 0.4118560552597046\n",
            "tcost icost -4.76953125 0.3953193426132202\n",
            "tcost icost -4.76953125 0.395473450422287\n",
            "search -4.375\n",
            "tcost icost -3.2421875 0.42175284028053284\n",
            "tcost icost -5.08203125 0.39168113470077515\n",
            "tcost icost -5.08203125 0.391142338514328\n",
            "search -4.69140625\n",
            "tcost icost -3.181640625 0.42260897159576416\n",
            "tcost icost -4.96484375 0.3912855088710785\n",
            "tcost icost -4.96484375 0.39130279421806335\n",
            "search -4.57421875\n",
            "tcost icost -3.021484375 0.4237106144428253\n",
            "tcost icost -3.876953125 0.4116046130657196\n",
            "tcost icost -3.876953125 0.4108157157897949\n",
            "search -3.466796875\n",
            "tcost icost -4.29296875 0.40460866689682007\n",
            "tcost icost -5.20703125 0.38696688413619995\n",
            "tcost icost -5.20703125 0.387052446603775\n",
            "search -4.8203125\n",
            "tcost icost -5.55859375 0.38240841031074524\n",
            "tcost icost -6.11328125 0.3725990951061249\n",
            "tcost icost -6.11328125 0.3726802468299866\n",
            "search -5.7421875\n",
            "tcost icost -3.91796875 0.4111681580543518\n",
            "tcost icost -5.078125 0.3890862464904785\n",
            "tcost icost -5.078125 0.3889594078063965\n",
            "search -4.6875\n",
            "tcost icost -4.4375 0.402225524187088\n",
            "tcost icost -5.6484375 0.38108593225479126\n",
            "tcost icost -5.6484375 0.38080093264579773\n",
            "search -5.265625\n",
            "tcost icost -3.236328125 0.42194443941116333\n",
            "tcost icost -5.01953125 0.3902379870414734\n",
            "tcost icost -5.01953125 0.3904079496860504\n",
            "search -4.62890625\n",
            "tcost icost -5.33203125 0.38606104254722595\n",
            "tcost icost -3.19140625 0.4218454957008362\n",
            "tcost icost -3.19140625 0.4222916066646576\n",
            "search -2.76953125\n",
            "tcost icost -5.25 0.3884035050868988\n",
            "tcost icost -6.18359375 0.3753546476364136\n",
            "tcost icost -6.18359375 0.37525200843811035\n",
            "search -5.80859375\n",
            "tcost icost -5.26953125 0.38825729489326477\n",
            "tcost icost -6.19921875 0.37583592534065247\n",
            "tcost icost -6.19921875 0.37555205821990967\n",
            "search -5.82421875\n",
            "tcost icost -3.701171875 0.41544246673583984\n",
            "tcost icost -5.71875 0.38162022829055786\n",
            "tcost icost -5.71875 0.38147449493408203\n",
            "search -5.3359375\n",
            "tcost icost -3.302734375 0.42180418968200684\n",
            "tcost icost -5.34765625 0.3866312801837921\n",
            "tcost icost -5.34765625 0.386800080537796\n",
            "search -4.9609375\n",
            "tcost icost -1.337890625 0.4450911283493042\n",
            "tcost icost -1.62890625 0.44156908988952637\n",
            "tcost icost -1.62890625 0.4415779411792755\n",
            "search -1.1875\n",
            "tcost icost -3.408203125 0.41647645831108093\n",
            "tcost icost -3.330078125 0.41862159967422485\n",
            "tcost icost -3.330078125 0.4184044599533081\n",
            "search -2.912109375\n",
            "tcost icost -2.7109375 0.42786431312561035\n",
            "tcost icost -3.41015625 0.4174334704875946\n",
            "tcost icost -3.41015625 0.4176107943058014\n",
            "search -2.9921875\n",
            "tcost icost -5.734375 0.37907832860946655\n",
            "tcost icost -6.39453125 0.3681029975414276\n",
            "tcost icost -6.39453125 0.3677712678909302\n",
            "search -6.02734375\n",
            "tcost icost -2.693359375 0.42838138341903687\n",
            "tcost icost -3.716796875 0.4110400974750519\n",
            "tcost icost -3.716796875 0.41094085574150085\n",
            "search -3.306640625\n",
            "tcost icost -2.40625 0.4316541254520416\n",
            "tcost icost -3.171875 0.41948461532592773\n",
            "tcost icost -3.171875 0.41948097944259644\n",
            "search -2.751953125\n",
            "tcost icost -5.359375 0.3850671052932739\n",
            "tcost icost -5.578125 0.38194575905799866\n",
            "tcost icost -5.578125 0.38211682438850403\n",
            "search -5.1953125\n",
            "tcost icost -3.28515625 0.4190446138381958\n",
            "tcost icost -5.1796875 0.38568052649497986\n",
            "tcost icost -5.1796875 0.3859753906726837\n",
            "search -4.79296875\n",
            "tcost icost -5.66015625 0.3796377182006836\n",
            "tcost icost -6.1484375 0.37172162532806396\n",
            "tcost icost -6.1484375 0.37135687470436096\n",
            "search -5.77734375\n",
            "tcost icost -5.046875 0.3891505300998688\n",
            "tcost icost -5.328125 0.3844728469848633\n",
            "tcost icost -5.328125 0.3841821253299713\n",
            "search -4.9453125\n",
            "tcost icost -4.640625 0.39785289764404297\n",
            "tcost icost -5.4296875 0.3820337653160095\n",
            "tcost icost -5.4296875 0.3822062313556671\n",
            "search -5.046875\n",
            "tcost icost -3.9140625 0.4099736511707306\n",
            "tcost icost -5.5546875 0.3823857605457306\n",
            "tcost icost -5.5546875 0.38204821944236755\n",
            "search -5.171875\n",
            "tcost icost -5.859375 0.37659862637519836\n",
            "tcost icost -6.0546875 0.3750067949295044\n",
            "tcost icost -6.0546875 0.3749352693557739\n",
            "search -5.6796875\n",
            "tcost icost -5.0 0.39112305641174316\n",
            "tcost icost -5.484375 0.3831070363521576\n",
            "tcost icost -5.484375 0.3833816945552826\n",
            "search -5.1015625\n",
            "tcost icost -4.54296875 0.4005531668663025\n",
            "tcost icost -6.01953125 0.3751082122325897\n",
            "tcost icost -6.01953125 0.37542879581451416\n",
            "search -5.64453125\n",
            "tcost icost -5.83203125 0.3764524459838867\n",
            "tcost icost -6.35546875 0.3675331473350525\n",
            "tcost icost -6.35546875 0.3675973415374756\n",
            "search -5.98828125\n",
            "tcost icost -5.6171875 0.3804861903190613\n",
            "tcost icost -3.70703125 0.41233572363853455\n",
            "tcost icost -3.70703125 0.41252580285072327\n",
            "search -3.294921875\n",
            "tcost icost -1.3828125 0.44165554642677307\n",
            "tcost icost -1.7099609375 0.4383341372013092\n",
            "tcost icost -1.7099609375 0.43887168169021606\n",
            "search -1.271484375\n",
            "tcost icost -4.53125 0.39904123544692993\n",
            "tcost icost -5.5234375 0.38164249062538147\n",
            "tcost icost -5.5234375 0.3809838593006134\n",
            "search -5.140625\n",
            "tcost icost -6.6171875 0.3621039390563965\n",
            "tcost icost -5.859375 0.3766564130783081\n",
            "tcost icost -5.859375 0.3765748143196106\n",
            "search -5.484375\n",
            "tcost icost -4.14453125 0.40487247705459595\n",
            "tcost icost -5.80078125 0.3768722712993622\n",
            "tcost icost -5.80078125 0.377444863319397\n",
            "search -5.421875\n",
            "tcost icost -4.7890625 0.3951612412929535\n",
            "tcost icost -5.6953125 0.3802714943885803\n",
            "tcost icost -5.6953125 0.3797883987426758\n",
            "search -5.31640625\n",
            "tcost icost -4.3515625 0.4014292359352112\n",
            "tcost icost -4.78125 0.39297929406166077\n",
            "tcost icost -4.78125 0.39348551630973816\n",
            "search -4.38671875\n",
            "tcost icost -4.87890625 0.3941497802734375\n",
            "tcost icost -5.8359375 0.3780001699924469\n",
            "tcost icost -5.8359375 0.37784895300865173\n",
            "search -5.45703125\n",
            "tcost icost -3.681640625 0.413425087928772\n",
            "tcost icost -5.359375 0.3846489489078522\n",
            "tcost icost -5.359375 0.3844483494758606\n",
            "search -4.9765625\n",
            "tcost icost -2.333984375 0.43080222606658936\n",
            "tcost icost -3.068359375 0.4215710461139679\n",
            "tcost icost -3.068359375 0.4202571511268616\n",
            "search -2.6484375\n",
            "tcost icost -6.17578125 0.3701101243495941\n",
            "tcost icost -6.125 0.36992761492729187\n",
            "tcost icost -6.125 0.3703605532646179\n",
            "search -5.75390625\n",
            "tcost icost -4.8203125 0.3930952250957489\n",
            "tcost icost -5.30859375 0.3851897120475769\n",
            "tcost icost -5.30859375 0.38507556915283203\n",
            "search -4.921875\n",
            "tcost icost -5.72265625 0.37818723917007446\n",
            "tcost icost -3.77734375 0.4102896451950073\n",
            "tcost icost -3.77734375 0.4101141691207886\n",
            "search -3.3671875\n",
            "tcost icost -5.12890625 0.3859539330005646\n",
            "tcost icost -5.17578125 0.38494572043418884\n",
            "tcost icost -5.17578125 0.3854104280471802\n",
            "search -4.7890625\n",
            "tcost icost -5.45703125 0.38249197602272034\n",
            "tcost icost -5.6875 0.37910252809524536\n",
            "tcost icost -5.6875 0.378964364528656\n",
            "search -5.30859375\n",
            "tcost icost -6.140625 0.37164783477783203\n",
            "tcost icost -5.98828125 0.374940425157547\n",
            "tcost icost -5.98828125 0.3751792907714844\n",
            "search -5.61328125\n",
            "tcost icost -4.734375 0.39502769708633423\n",
            "tcost icost -5.140625 0.38730382919311523\n",
            "tcost icost -5.140625 0.3873980641365051\n",
            "search -4.75390625\n",
            "tcost icost -5.29296875 0.38496872782707214\n",
            "tcost icost -3.58203125 0.41355445981025696\n",
            "tcost icost -3.58203125 0.41344019770622253\n",
            "search -3.16796875\n",
            "tcost icost -5.80859375 0.37652596831321716\n",
            "tcost icost -6.60546875 0.3633958399295807\n",
            "tcost icost -6.60546875 0.3630290925502777\n",
            "search -6.2421875\n",
            "tcost icost -4.66015625 0.3956295847892761\n",
            "tcost icost -5.46484375 0.3827148675918579\n",
            "tcost icost -5.46484375 0.38261014223098755\n",
            "search -5.08203125\n",
            "tcost icost -3.685546875 0.4112936556339264\n",
            "tcost icost -5.5078125 0.3816361129283905\n",
            "tcost icost -5.5078125 0.382414311170578\n",
            "search -5.125\n",
            "tcost icost -4.73046875 0.39421138167381287\n",
            "tcost icost -5.40234375 0.382938951253891\n",
            "tcost icost -5.40234375 0.38273143768310547\n",
            "search -5.01953125\n",
            "tcost icost -5.0546875 0.3903679847717285\n",
            "tcost icost -5.3046875 0.3857439458370209\n",
            "tcost icost -5.3046875 0.3855898976325989\n",
            "search -4.91796875\n",
            "tcost icost -4.78125 0.39394786953926086\n",
            "tcost icost -5.37109375 0.3848760724067688\n",
            "tcost icost -5.37109375 0.38413092494010925\n",
            "search -4.98828125\n",
            "tcost icost -5.13671875 0.38867777585983276\n",
            "tcost icost -5.75 0.3800768554210663\n",
            "tcost icost -5.75 0.3801913857460022\n",
            "search -5.37109375\n",
            "tcost icost -5.58203125 0.3800351023674011\n",
            "tcost icost -5.7109375 0.3804425001144409\n",
            "tcost icost -5.7109375 0.38069406151771545\n",
            "search -5.33203125\n",
            "tcost icost -4.828125 0.39272135496139526\n",
            "tcost icost -5.453125 0.3829072117805481\n",
            "tcost icost -5.453125 0.3827829957008362\n",
            "search -5.0703125\n",
            "tcost icost -6.05859375 0.3718423545360565\n",
            "tcost icost -6.00390625 0.376157283782959\n",
            "tcost icost -6.00390625 0.3752012848854065\n",
            "search -5.62890625\n",
            "tcost icost -5.0625 0.38906189799308777\n",
            "tcost icost -5.73046875 0.3786008358001709\n",
            "tcost icost -5.73046875 0.37853923439979553\n",
            "search -5.3515625\n",
            "tcost icost -5.99609375 0.37195491790771484\n",
            "tcost icost -6.6328125 0.3619726598262787\n",
            "tcost icost -6.6328125 0.3616201877593994\n",
            "search -6.26953125\n",
            "tcost icost -6.015625 0.3747069537639618\n",
            "tcost icost -3.806640625 0.40959498286247253\n",
            "tcost icost -3.806640625 0.40940773487091064\n",
            "search -3.396484375\n",
            "tcost icost -4.10546875 0.4051326513290405\n",
            "tcost icost -5.77734375 0.3780648708343506\n",
            "tcost icost -5.77734375 0.37798449397087097\n",
            "search -5.3984375\n",
            "tcost icost -4.2890625 0.4028199017047882\n",
            "tcost icost -5.75 0.3801019489765167\n",
            "tcost icost -5.75 0.37888798117637634\n",
            "search -5.37109375\n",
            "tcost icost -6.20703125 0.36994805932044983\n",
            "tcost icost -6.6796875 0.36175745725631714\n",
            "tcost icost -6.6796875 0.3615858554840088\n",
            "search -6.31640625\n",
            "tcost icost -5.40625 0.3828873932361603\n",
            "tcost icost -6.31640625 0.3724779188632965\n",
            "tcost icost -6.31640625 0.37293240427970886\n",
            "search -5.9453125\n",
            "tcost icost -6.51953125 0.3668173551559448\n",
            "tcost icost -6.59765625 0.36366355419158936\n",
            "tcost icost -6.59765625 0.3637932538986206\n",
            "search -6.234375\n",
            "tcost icost -6.56640625 0.3664953410625458\n",
            "tcost icost -6.69140625 0.36244532465934753\n",
            "tcost icost -6.69140625 0.3621307611465454\n",
            "search -6.328125\n",
            "tcost icost -4.1953125 0.4045531451702118\n",
            "tcost icost -5.98828125 0.377349853515625\n",
            "tcost icost -5.98828125 0.3769052028656006\n",
            "search -5.609375\n",
            "tcost icost -5.84765625 0.3790030777454376\n",
            "tcost icost -3.724609375 0.412076473236084\n",
            "tcost icost -3.724609375 0.4122881591320038\n",
            "search -3.3125\n",
            "tcost icost -5.16015625 0.38924524188041687\n",
            "tcost icost -5.6015625 0.3831871449947357\n",
            "tcost icost -5.6015625 0.38361164927482605\n",
            "search -5.21875\n",
            "tcost icost -6.4375 0.3671407103538513\n",
            "tcost icost -5.7109375 0.38047295808792114\n",
            "tcost icost -5.7109375 0.38093551993370056\n",
            "search -5.328125\n",
            "tcost icost -4.83203125 0.3946933448314667\n",
            "tcost icost -5.66796875 0.3808165192604065\n",
            "tcost icost -5.66796875 0.3807624280452728\n",
            "search -5.2890625\n",
            "tcost icost -5.33203125 0.3860678970813751\n",
            "tcost icost -5.94140625 0.37742653489112854\n",
            "tcost icost -5.94140625 0.3773932158946991\n",
            "search -5.5625\n",
            "tcost icost -2.828125 0.42651909589767456\n",
            "tcost icost -4.34375 0.40220677852630615\n",
            "tcost icost -4.34375 0.401997447013855\n",
            "search -3.94140625\n",
            "tcost icost -2.947265625 0.4247078597545624\n",
            "tcost icost -4.76171875 0.39613839983940125\n",
            "tcost icost -4.76171875 0.39500364661216736\n",
            "search -4.3671875\n",
            "tcost icost -2.751953125 0.4271447956562042\n",
            "tcost icost -3.767578125 0.40904906392097473\n",
            "tcost icost -3.767578125 0.4095112979412079\n",
            "search -3.357421875\n",
            "tcost icost -2.966796875 0.4231347441673279\n",
            "tcost icost -4.5625 0.3949366807937622\n",
            "tcost icost -4.5625 0.39493483304977417\n",
            "search -4.16796875\n",
            "tcost icost -3.01171875 0.4231101870536804\n",
            "tcost icost -4.77734375 0.39315125346183777\n",
            "tcost icost -4.77734375 0.3927192687988281\n",
            "search -4.3828125\n",
            "tcost icost -3.20703125 0.41970518231391907\n",
            "tcost icost -5.0859375 0.3886774480342865\n",
            "tcost icost -5.0859375 0.3882066309452057\n",
            "search -4.69921875\n",
            "tcost icost -6.1875 0.37038031220436096\n",
            "tcost icost -3.853515625 0.4088750183582306\n",
            "tcost icost -3.853515625 0.40861210227012634\n",
            "search -3.4453125\n",
            "tcost icost -5.1015625 0.3883548378944397\n",
            "tcost icost -5.46484375 0.3818446397781372\n",
            "tcost icost -5.46484375 0.3815819025039673\n",
            "search -5.08203125\n",
            "tcost icost -3.388671875 0.4160142242908478\n",
            "tcost icost -4.515625 0.395782470703125\n",
            "tcost icost -4.515625 0.3971182703971863\n",
            "search -4.1171875\n",
            "tcost icost -4.2265625 0.40123772621154785\n",
            "tcost icost -4.89453125 0.3910636603832245\n",
            "tcost icost -4.89453125 0.3905564546585083\n",
            "search -4.50390625\n",
            "tcost icost -2.099609375 0.43357717990875244\n",
            "tcost icost -3.216796875 0.4175472557544708\n",
            "tcost icost -3.216796875 0.4172774851322174\n",
            "search -2.798828125\n",
            "tcost icost -3.92578125 0.4065587520599365\n",
            "tcost icost -4.921875 0.3879331350326538\n",
            "tcost icost -4.921875 0.38742882013320923\n",
            "search -4.53515625\n",
            "tcost icost -5.25390625 0.38265469670295715\n",
            "tcost icost -6.07421875 0.36946001648902893\n",
            "tcost icost -6.07421875 0.3704427182674408\n",
            "search -5.703125\n",
            "tcost icost -5.75390625 0.37738361954689026\n",
            "tcost icost -4.0078125 0.4052107334136963\n",
            "tcost icost -4.0078125 0.4053879678249359\n",
            "search -3.6015625\n",
            "tcost icost -5.91015625 0.37283259630203247\n",
            "tcost icost -5.7734375 0.37595048546791077\n",
            "tcost icost -5.7734375 0.3756946921348572\n",
            "search -5.3984375\n",
            "tcost icost -4.26171875 0.4007508158683777\n",
            "tcost icost -5.15625 0.38538792729377747\n",
            "tcost icost -5.15625 0.38459616899490356\n",
            "search -4.7734375\n",
            "tcost icost -4.29296875 0.3994392156600952\n",
            "tcost icost -5.62890625 0.3775032162666321\n",
            "tcost icost -5.62890625 0.3776743710041046\n",
            "search -5.25\n",
            "tcost icost -5.54296875 0.37727779150009155\n",
            "tcost icost -5.16015625 0.3842969238758087\n",
            "tcost icost -5.16015625 0.3837989270687103\n",
            "search -4.77734375\n",
            "tcost icost -4.62890625 0.39293384552001953\n",
            "tcost icost -5.2421875 0.3826983571052551\n",
            "tcost icost -5.2421875 0.3823944330215454\n",
            "search -4.859375\n",
            "tcost icost -3.6875 0.4085039496421814\n",
            "tcost icost -5.25 0.38183146715164185\n",
            "tcost icost -5.25 0.38154351711273193\n",
            "search -4.8671875\n",
            "tcost icost -3.9609375 0.40271061658859253\n",
            "tcost icost -5.171875 0.3817308247089386\n",
            "tcost icost -5.171875 0.38146477937698364\n",
            "search -4.7890625\n",
            "tcost icost -4.265625 0.3983357846736908\n",
            "tcost icost -4.859375 0.387113481760025\n",
            "tcost icost -4.859375 0.3889458477497101\n",
            "search -4.46875\n",
            "tcost icost -5.58203125 0.37714868783950806\n",
            "tcost icost -6.01953125 0.3676595091819763\n",
            "tcost icost -6.01953125 0.36776310205459595\n",
            "search -5.65234375\n",
            "tcost icost -6.171875 0.36635223031044006\n",
            "tcost icost -6.171875 0.3689555525779724\n",
            "tcost icost -6.171875 0.3689940869808197\n",
            "search -5.8046875\n",
            "tcost icost -6.08984375 0.3681185841560364\n",
            "tcost icost -5.55859375 0.37983375787734985\n",
            "tcost icost -5.55859375 0.3784934878349304\n",
            "search -5.1796875\n",
            "tcost icost -4.84375 0.38949334621429443\n",
            "tcost icost -5.77734375 0.3752596974372864\n",
            "tcost icost -5.77734375 0.37554237246513367\n",
            "search -5.40234375\n",
            "tcost icost -5.05078125 0.38473841547966003\n",
            "tcost icost -5.1015625 0.3836960196495056\n",
            "tcost icost -5.1015625 0.3838873505592346\n",
            "search -4.71875\n",
            "tcost icost -6.34765625 0.3638181984424591\n",
            "tcost icost -6.28125 0.36902326345443726\n",
            "tcost icost -6.28125 0.3682751953601837\n",
            "search -5.9140625\n",
            "tcost icost -4.03125 0.40240588784217834\n",
            "tcost icost -6.3515625 0.3667815327644348\n",
            "tcost icost -6.3515625 0.366451233625412\n",
            "search -5.984375\n",
            "tcost icost -6.734375 0.35763272643089294\n",
            "tcost icost -6.06640625 0.37012651562690735\n",
            "tcost icost -6.06640625 0.37071776390075684\n",
            "search -5.6953125\n",
            "tcost icost -6.1953125 0.3661078214645386\n",
            "tcost icost -6.82421875 0.35730135440826416\n",
            "tcost icost -6.82421875 0.3556358218193054\n",
            "search -6.46875\n",
            "tcost icost -5.875 0.36967387795448303\n",
            "tcost icost -3.7578125 0.40627074241638184\n",
            "tcost icost -3.7578125 0.4061990976333618\n",
            "search -3.3515625\n",
            "tcost icost -6.046875 0.3686288297176361\n",
            "tcost icost -3.5625 0.4099051356315613\n",
            "tcost icost -3.5625 0.40979859232902527\n",
            "search -3.15234375\n",
            "tcost icost -3.4765625 0.41187798976898193\n",
            "tcost icost -5.359375 0.3801030218601227\n",
            "tcost icost -5.359375 0.3806244432926178\n",
            "search -4.98046875\n",
            "tcost icost -4.484375 0.39185717701911926\n",
            "tcost icost -4.92578125 0.38453274965286255\n",
            "tcost icost -4.92578125 0.3843064308166504\n",
            "search -4.54296875\n",
            "tcost icost -3.734375 0.4069717228412628\n",
            "tcost icost -5.33984375 0.3792741596698761\n",
            "tcost icost -5.33984375 0.3795498013496399\n",
            "search -4.9609375\n",
            "tcost icost -5.42578125 0.37785375118255615\n",
            "tcost icost -5.4921875 0.3780980706214905\n",
            "tcost icost -5.4921875 0.3787093460559845\n",
            "search -5.11328125\n",
            "tcost icost -3.90234375 0.40308982133865356\n",
            "tcost icost -5.46484375 0.37795087695121765\n",
            "tcost icost -5.46484375 0.3780882954597473\n",
            "search -5.0859375\n",
            "tcost icost -5.6796875 0.3727586567401886\n",
            "tcost icost -6.26953125 0.3659560978412628\n",
            "tcost icost -6.26953125 0.36570778489112854\n",
            "search -5.90234375\n",
            "tcost icost -6.2734375 0.3639942407608032\n",
            "tcost icost -3.970703125 0.4012286365032196\n",
            "tcost icost -3.970703125 0.4019916355609894\n",
            "search -3.568359375\n",
            "tcost icost -4.3359375 0.3951539397239685\n",
            "tcost icost -5.70703125 0.37299424409866333\n",
            "tcost icost -5.70703125 0.3725462555885315\n",
            "search -5.3359375\n",
            "tcost icost -6.26953125 0.3624102771282196\n",
            "tcost icost -3.958984375 0.4008479118347168\n",
            "tcost icost -3.958984375 0.4009149372577667\n",
            "search -3.55859375\n",
            "tcost icost -3.5 0.4069039821624756\n",
            "tcost icost -3.6328125 0.4025885760784149\n",
            "tcost icost -3.6328125 0.4025585651397705\n",
            "search -3.23046875\n",
            "tcost icost -2.642578125 0.4182615876197815\n",
            "tcost icost -3.455078125 0.4056631624698639\n",
            "tcost icost -3.455078125 0.4061630666255951\n",
            "search -3.048828125\n",
            "tcost icost -2.404296875 0.4231563210487366\n",
            "tcost icost -3.7421875 0.4028867483139038\n",
            "tcost icost -3.7421875 0.40254640579223633\n",
            "search -3.33984375\n",
            "ded\n",
            "time\n",
            "26 #### train ####\n",
            "repr, std, cov, closslb 0.0024518007412552834 0.4755859375 0.00014355103485286236 4.3641863157972693e-05 0.0015534465201199055\n",
            "0.0059140375257986265 0.022799708955021418 1.0\n",
            "repr, std, cov, closslb 0.0036418952513486147 0.475341796875 0.00013059284538030624 0.0048682428896427155 0.014568321406841278\n",
            "0.0058963308383945985 0.02275417784515327 1.0\n",
            "repr, std, cov, closslb 0.002595817670226097 0.4765625 0.0001286589540541172 1.5690940927015617e-05 0.00034557978506200016\n",
            "0.005884555842154449 0.022663388221262685 1.0\n",
            "repr, std, cov, closslb 0.0036599510349333286 0.4765625 0.00011508655734360218 0.00430914806202054 0.01614457555115223\n",
            "0.0058963308383945985 0.02289104468031686 1.0\n",
            "repr, std, cov, closslb 0.0029476042836904526 0.4755859375 0.0001558761578053236 6.666367698926479e-05 0.0011067362502217293\n",
            "0.005967477266406803 0.02307481527051258 1.0\n",
            "repr, std, cov, closslb 0.00990111380815506 0.47412109375 0.00018201489001512527 0.012552347034215927 0.030524911358952522\n",
            "0.005955560190465683 0.02289104468031686 1.0\n",
            "repr, std, cov, closslb 0.007728822994977236 0.475830078125 0.00014373566955327988 0.011774287559092045 0.0002969091583508998\n",
            "0.005884555842154449 0.022595533810633825 1.0\n",
            "repr, std, cov, closslb 0.006322233006358147 0.477294921875 0.000104858772829175 0.0051009273156523705 0.000527398195117712\n",
            "0.0059140375257986265 0.022822508663976438 1.0\n",
            "repr, std, cov, closslb 0.00909745879471302 0.476806640625 0.00013712397776544094 0.004689391702413559 0.0007239080732688308\n",
            "0.005955560190465683 0.02286817650381305 1.0\n",
            "repr, std, cov, closslb 0.008328179828822613 0.476806640625 0.00011709658429026604 0.00917315948754549 0.00016147131100296974\n",
            "0.006021399893006794 0.02328332124249172 1.0\n",
            "repr, std, cov, closslb 0.007772207260131836 0.47802734375 9.867176413536072e-05 0.004449228756129742 0.000231904472457245\n",
            "0.006100149487240959 0.024040052376839125 1.0\n",
            "repr, std, cov, closslb 0.0062486520037055016 0.478271484375 9.341887198388577e-05 1.888025872176513e-05 0.03180861845612526\n",
            "0.006161426222140798 0.02442759152744116 1.0\n",
            "repr, std, cov, closslb 0.006046414375305176 0.478759765625 7.750513032078743e-05 1.4377910702023655e-05 0.0022919431794434786\n",
            "0.00626075187448114 0.025221512585928304 1.0\n",
            "repr, std, cov, closslb 0.011182268150150776 0.4775390625 9.437091648578644e-05 0.0032103294506669044 0.00027998967561870813\n",
            "0.006285832471539576 0.02567937936204927 1.0\n",
            "repr, std, cov, closslb 0.006997518707066774 0.476806640625 0.00011005043052136898 0.0038877788465470076 0.0003942670300602913\n",
            "0.0062984104223151255 0.025989232415626836 1.0\n",
            "repr, std, cov, closslb 0.005699300207197666 0.477294921875 0.00010643410496413708 2.2006075596436858e-05 0.0003137217427138239\n",
            "0.006242007120855206 0.02601522164804246 1.0\n",
            "repr, std, cov, closslb 0.0055265589617192745 0.476806640625 0.00012959190644323826 1.2421874089341145e-05 7.177626685006544e-05\n",
            "0.006149121829360249 0.02537322048894137 1.0\n",
            "repr, std, cov, closslb 0.005732540041208267 0.475830078125 0.00014059850946068764 1.7568697785463883e-06 8.047417213674635e-05\n",
            "0.006069740028968078 0.024920812566565833 1.0\n",
            "repr, std, cov, closslb 0.01174065750092268 0.473388671875 0.00020952406339347363 0.00726440642029047 0.015738576650619507\n",
            "0.005961515750656148 0.02413635692284498 1.0\n",
            "repr, std, cov, closslb 0.006434311158955097 0.474609375 0.0001757203135639429 0.003419914050027728 0.00017056292563211173\n",
            "0.005890440397996603 0.02370600244364365 1.0\n",
            "repr, std, cov, closslb 0.007164848502725363 0.47509765625 0.00014106486923992634 0.0035297793801873922 0.00033997202990576625\n",
            "0.005861076346858769 0.023329911168297943 1.0\n",
            "repr, std, cov, closslb 0.00996584352105856 0.474609375 0.00015434203669428825 0.008696733973920345 0.0001872203138191253\n",
            "0.0058435282257533055 0.023051763507005576 1.0\n",
            "repr, std, cov, closslb 0.00479526212438941 0.47509765625 0.0001793201081454754 2.9361854103626683e-06 0.00017113052308559418\n",
            "0.005745076671548618 0.02246043385147152 1.0\n",
            "repr, std, cov, closslb 0.005090026650577784 0.475830078125 0.00013027735985815525 1.1676926078507677e-05 0.00015019896090961993\n",
            "0.005785413056138439 0.022437995855615907 1.0\n",
            "repr, std, cov, closslb 0.0065094782039523125 0.474365234375 0.00018604472279548645 0.0064353435300290585 0.00023168936604633927\n",
            "0.0057738595631525716 0.0221483335985264 1.0\n",
            "repr, std, cov, closslb 0.0065056681632995605 0.474853515625 0.00013976357877254486 0.004072894342243671 0.0003183876979164779\n",
            "0.005716437263664701 0.021884273133507375 1.0\n",
            "repr, std, cov, closslb 0.00481533445417881 0.475830078125 0.00014204555191099644 8.7319640442729e-06 0.01578982174396515\n",
            "0.005699322193418544 0.02175342584167476 1.0\n",
            "repr, std, cov, closslb 0.005154754500836134 0.4755859375 0.00012417673133313656 0.0038687705527991056 2.4598983145551756e-05\n",
            "0.005733603730483921 0.022015907474419266 1.0\n",
            "repr, std, cov, closslb 0.003249306231737137 0.47705078125 0.00010738382115960121 6.245963049877901e-06 0.00014870765153318644\n",
            "0.005785413056138439 0.022686051609483945 1.0\n",
            "repr, std, cov, closslb 0.006123808212578297 0.4775390625 0.00010962388478219509 0.014298981055617332 0.00021584246132988483\n",
            "0.005820212431466185 0.02307481527051258 1.0\n",
            "repr, std, cov, closslb 0.007766279857605696 0.476806640625 0.00011556153185665607 0.007858136668801308 0.00030716153560206294\n",
            "0.00587867716498946 0.02368232012352013 1.0\n",
            "repr, std, cov, closslb 0.003559867851436138 0.4775390625 0.00010192347690463066 2.2117956177680753e-05 9.366186714032665e-05\n",
            "0.0058963308383945985 0.02399204429620243 1.0\n",
            "repr, std, cov, closslb 0.006740730721503496 0.477783203125 8.951965719461441e-05 0.016132256016135216 5.574551323661581e-05\n",
            "0.005937729183789037 0.024697642590401993 1.0\n",
            "repr, std, cov, closslb 0.006688715890049934 0.47607421875 0.00014203530736267567 0.012860706076025963 0.015736911445856094\n",
            "0.005919951563324424 0.024995649791624035 1.0\n",
            "repr, std, cov, closslb 0.015751903876662254 0.4755859375 0.0001627046149224043 0.008339001797139645 3.3973323297686875e-05\n",
            "0.005779633422715724 0.02472234023299239 1.0\n",
            "repr, std, cov, closslb 0.006693913601338863 0.475830078125 0.00012819701805710793 0.00804878119379282 0.00033681176137179136\n",
            "0.005756572569968386 0.02474706257322538 1.0\n",
            "tcost icost -21.796875 0.29133570194244385\n",
            "tcost icost -23.4375 0.28871870040893555\n",
            "tcost icost -23.4375 0.28874021768569946\n",
            "search -23.15625\n",
            "tcost icost -18.21875 0.3127927780151367\n",
            "tcost icost -23.4375 0.288411945104599\n",
            "tcost icost -23.4375 0.2888473868370056\n",
            "search -23.15625\n",
            "tcost icost -16.0 0.33291879296302795\n",
            "tcost icost -23.4375 0.2893402874469757\n",
            "tcost icost -23.4375 0.29016339778900146\n",
            "search -23.140625\n",
            "tcost icost -16.0 0.33334842324256897\n",
            "tcost icost -23.4375 0.289989173412323\n",
            "tcost icost -23.4375 0.2896646559238434\n",
            "search -23.140625\n",
            "tcost icost -21.796875 0.29289719462394714\n",
            "tcost icost -23.4375 0.2899114787578583\n",
            "tcost icost -23.4375 0.28989002108573914\n",
            "search -23.140625\n",
            "tcost icost -17.21875 0.32409775257110596\n",
            "tcost icost -23.5 0.29016217589378357\n",
            "tcost icost -23.5 0.2903338670730591\n",
            "search -23.203125\n",
            "tcost icost -23.21875 0.29060307145118713\n",
            "tcost icost -23.609375 0.29051560163497925\n",
            "tcost icost -23.609375 0.29151222109794617\n",
            "search -23.3125\n",
            "tcost icost -22.0 0.29377755522727966\n",
            "tcost icost -23.609375 0.290432333946228\n",
            "tcost icost -23.609375 0.29075345396995544\n",
            "search -23.3125\n",
            "tcost icost -22.3125 0.2930569350719452\n",
            "tcost icost -23.828125 0.2905546724796295\n",
            "tcost icost -23.828125 0.29019445180892944\n",
            "search -23.53125\n",
            "tcost icost -21.765625 0.2955074608325958\n",
            "tcost icost -23.34375 0.2908377945423126\n",
            "tcost icost -23.34375 0.29169586300849915\n",
            "search -23.046875\n",
            "tcost icost -18.421875 0.31688427925109863\n",
            "tcost icost -24.21875 0.2906336486339569\n",
            "tcost icost -24.21875 0.2900151312351227\n",
            "search -23.921875\n",
            "tcost icost -16.015625 0.3364471197128296\n",
            "tcost icost -23.25 0.291991263628006\n",
            "tcost icost -23.25 0.29225727915763855\n",
            "search -22.953125\n",
            "tcost icost -22.3125 0.2938222587108612\n",
            "tcost icost -23.203125 0.2913808226585388\n",
            "tcost icost -23.203125 0.29141974449157715\n",
            "search -22.90625\n",
            "tcost icost -23.5625 0.29311230778694153\n",
            "tcost icost -23.15625 0.29176631569862366\n",
            "tcost icost -23.15625 0.29129019379615784\n",
            "search -22.859375\n",
            "tcost icost -22.875 0.29266494512557983\n",
            "tcost icost -24.453125 0.29132723808288574\n",
            "tcost icost -24.453125 0.29105469584465027\n",
            "search -24.15625\n",
            "tcost icost -16.671875 0.33233341574668884\n",
            "tcost icost -23.96875 0.29213860630989075\n",
            "tcost icost -23.96875 0.2923900783061981\n",
            "search -23.671875\n",
            "tcost icost -23.03125 0.29274433851242065\n",
            "tcost icost -23.71875 0.2918359339237213\n",
            "tcost icost -23.71875 0.2915753722190857\n",
            "search -23.421875\n",
            "tcost icost -22.296875 0.29542914032936096\n",
            "tcost icost -23.90625 0.29249727725982666\n",
            "tcost icost -23.90625 0.292709618806839\n",
            "search -23.609375\n",
            "tcost icost -22.9375 0.29382115602493286\n",
            "tcost icost -24.09375 0.2926730811595917\n",
            "tcost icost -24.09375 0.2933231592178345\n",
            "search -23.796875\n",
            "tcost icost -22.65625 0.29439371824264526\n",
            "tcost icost -23.328125 0.29280316829681396\n",
            "tcost icost -23.328125 0.292758971452713\n",
            "search -23.03125\n",
            "tcost icost -23.234375 0.2940705716609955\n",
            "tcost icost -24.34375 0.2935795187950134\n",
            "tcost icost -24.34375 0.2934637665748596\n",
            "search -24.046875\n",
            "tcost icost -22.9375 0.29537615180015564\n",
            "tcost icost -24.34375 0.2944425642490387\n",
            "tcost icost -24.34375 0.294066458940506\n",
            "search -24.046875\n",
            "tcost icost -19.515625 0.31030192971229553\n",
            "tcost icost -24.0625 0.2944987118244171\n",
            "tcost icost -24.0625 0.2945483922958374\n",
            "search -23.765625\n",
            "tcost icost -19.125 0.3154062032699585\n",
            "tcost icost -24.40625 0.2944636344909668\n",
            "tcost icost -24.40625 0.29421940445899963\n",
            "search -24.109375\n",
            "tcost icost -21.765625 0.2986751198768616\n",
            "tcost icost -23.484375 0.29597628116607666\n",
            "tcost icost -23.484375 0.29619336128234863\n",
            "search -23.1875\n",
            "tcost icost -23.4375 0.2952040731906891\n",
            "tcost icost -24.109375 0.2960192561149597\n",
            "tcost icost -24.109375 0.2964826226234436\n",
            "search -23.8125\n",
            "tcost icost -23.5 0.2950475811958313\n",
            "tcost icost -24.296875 0.29620984196662903\n",
            "tcost icost -24.296875 0.2956152558326721\n",
            "search -24.0\n",
            "tcost icost -18.5 0.3210611045360565\n",
            "tcost icost -24.125 0.2961081862449646\n",
            "tcost icost -24.125 0.29622501134872437\n",
            "search -23.828125\n",
            "tcost icost -24.484375 0.2953462600708008\n",
            "tcost icost -24.0 0.294944703578949\n",
            "tcost icost -24.0 0.29451578855514526\n",
            "search -23.703125\n",
            "tcost icost -21.953125 0.30071407556533813\n",
            "tcost icost -24.4375 0.2963973581790924\n",
            "tcost icost -24.4375 0.2959041893482208\n",
            "search -24.140625\n",
            "tcost icost -23.25 0.29570069909095764\n",
            "tcost icost -24.015625 0.29536834359169006\n",
            "tcost icost -24.015625 0.2950611412525177\n",
            "search -23.71875\n",
            "tcost icost -20.015625 0.3103008568286896\n",
            "tcost icost -24.390625 0.29581889510154724\n",
            "tcost icost -24.390625 0.29594162106513977\n",
            "search -24.09375\n",
            "tcost icost -22.640625 0.29733094573020935\n",
            "tcost icost -23.328125 0.29567503929138184\n",
            "tcost icost -23.328125 0.2957896888256073\n",
            "search -23.03125\n",
            "tcost icost -20.28125 0.311079740524292\n",
            "tcost icost -23.53125 0.29714226722717285\n",
            "tcost icost -23.53125 0.2972894012928009\n",
            "search -23.234375\n",
            "tcost icost -22.015625 0.2999657690525055\n",
            "tcost icost -23.609375 0.29787808656692505\n",
            "tcost icost -23.609375 0.2980564534664154\n",
            "search -23.3125\n",
            "tcost icost -24.296875 0.29531002044677734\n",
            "tcost icost -24.1875 0.297229140996933\n",
            "tcost icost -24.1875 0.29749956727027893\n",
            "search -23.890625\n",
            "tcost icost -21.59375 0.30277761816978455\n",
            "tcost icost -24.15625 0.29771775007247925\n",
            "tcost icost -24.15625 0.29751360416412354\n",
            "search -23.859375\n",
            "tcost icost -17.015625 0.33218568563461304\n",
            "tcost icost -23.375 0.2989078164100647\n",
            "tcost icost -23.375 0.2989245653152466\n",
            "search -23.078125\n",
            "tcost icost -24.25 0.2965371906757355\n",
            "tcost icost -24.15625 0.29847070574760437\n",
            "tcost icost -24.15625 0.2988745868206024\n",
            "search -23.859375\n",
            "tcost icost -22.421875 0.3002416491508484\n",
            "tcost icost -23.96875 0.298665851354599\n",
            "tcost icost -23.96875 0.3001355528831482\n",
            "search -23.671875\n",
            "tcost icost -23.9375 0.29837116599082947\n",
            "tcost icost -23.859375 0.29993724822998047\n",
            "tcost icost -23.859375 0.3001141846179962\n",
            "search -23.5625\n",
            "tcost icost -15.953125 0.34196749329566956\n",
            "tcost icost -23.3125 0.30098676681518555\n",
            "tcost icost -23.3125 0.30097195506095886\n",
            "search -23.015625\n",
            "tcost icost -21.484375 0.3037143647670746\n",
            "tcost icost -23.109375 0.3007012903690338\n",
            "tcost icost -23.109375 0.30049195885658264\n",
            "search -22.8125\n",
            "tcost icost -22.34375 0.3006253242492676\n",
            "tcost icost -23.28125 0.29903262853622437\n",
            "tcost icost -23.28125 0.2986312210559845\n",
            "search -22.984375\n",
            "tcost icost -21.625 0.3050582706928253\n",
            "tcost icost -24.078125 0.29966431856155396\n",
            "tcost icost -24.078125 0.3000194728374481\n",
            "search -23.78125\n",
            "tcost icost -19.234375 0.31658390164375305\n",
            "tcost icost -23.859375 0.3000183403491974\n",
            "tcost icost -23.859375 0.30004650354385376\n",
            "search -23.5625\n",
            "tcost icost -24.046875 0.2985629439353943\n",
            "tcost icost -23.9375 0.3009430468082428\n",
            "tcost icost -23.9375 0.3011179566383362\n",
            "search -23.640625\n",
            "tcost icost -18.5 0.324631005525589\n",
            "tcost icost -23.953125 0.3006308674812317\n",
            "tcost icost -23.953125 0.30031058192253113\n",
            "search -23.65625\n",
            "tcost icost -22.625 0.3022002875804901\n",
            "tcost icost -23.921875 0.3012469410896301\n",
            "tcost icost -23.921875 0.3009735643863678\n",
            "search -23.625\n",
            "tcost icost -22.640625 0.30201876163482666\n",
            "tcost icost -23.8125 0.30148550868034363\n",
            "tcost icost -23.8125 0.3016132712364197\n",
            "search -23.515625\n",
            "tcost icost -19.171875 0.319256067276001\n",
            "tcost icost -23.5625 0.3025650680065155\n",
            "tcost icost -23.5625 0.3028337359428406\n",
            "search -23.265625\n",
            "tcost icost -22.546875 0.3041342794895172\n",
            "tcost icost -23.671875 0.30277034640312195\n",
            "tcost icost -23.671875 0.30279096961021423\n",
            "search -23.375\n",
            "tcost icost -16.96875 0.33820775151252747\n",
            "tcost icost -23.4375 0.30336713790893555\n",
            "tcost icost -23.4375 0.3038012981414795\n",
            "search -23.140625\n",
            "tcost icost -22.0 0.30604279041290283\n",
            "tcost icost -23.546875 0.3037194609642029\n",
            "tcost icost -23.546875 0.3039310574531555\n",
            "search -23.25\n",
            "tcost icost -19.640625 0.32102417945861816\n",
            "tcost icost -23.09375 0.3041389584541321\n",
            "tcost icost -23.09375 0.3040678799152374\n",
            "search -22.796875\n",
            "tcost icost -23.875 0.3038441836833954\n",
            "tcost icost -23.734375 0.3027369976043701\n",
            "tcost icost -23.734375 0.3022967576980591\n",
            "search -23.4375\n",
            "tcost icost -21.984375 0.30780673027038574\n",
            "tcost icost -23.484375 0.3043173551559448\n",
            "tcost icost -23.484375 0.30455389618873596\n",
            "search -23.1875\n",
            "tcost icost -20.859375 0.3150537610054016\n",
            "tcost icost -23.625 0.3058314323425293\n",
            "tcost icost -23.625 0.30618542432785034\n",
            "search -23.3125\n",
            "tcost icost -22.578125 0.30605819821357727\n",
            "tcost icost -23.6875 0.30437779426574707\n",
            "tcost icost -23.6875 0.3044717311859131\n",
            "search -23.390625\n",
            "tcost icost -21.890625 0.3101973533630371\n",
            "tcost icost -24.015625 0.30426424741744995\n",
            "tcost icost -24.015625 0.30436110496520996\n",
            "search -23.71875\n",
            "tcost icost -22.984375 0.3043242394924164\n",
            "tcost icost -24.03125 0.30398786067962646\n",
            "tcost icost -24.03125 0.3036864995956421\n",
            "search -23.734375\n",
            "tcost icost -22.703125 0.3048297166824341\n",
            "tcost icost -23.875 0.30440613627433777\n",
            "tcost icost -23.875 0.3040522038936615\n",
            "search -23.578125\n",
            "tcost icost -24.109375 0.3048586845397949\n",
            "tcost icost -24.046875 0.3026692569255829\n",
            "tcost icost -24.046875 0.30326783657073975\n",
            "search -23.75\n",
            "tcost icost -24.125 0.30279237031936646\n",
            "tcost icost -24.078125 0.3043472468852997\n",
            "tcost icost -24.078125 0.3049299716949463\n",
            "search -23.765625\n",
            "tcost icost -24.1875 0.304613322019577\n",
            "tcost icost -24.078125 0.30344605445861816\n",
            "tcost icost -24.078125 0.3032684922218323\n",
            "search -23.78125\n",
            "tcost icost -24.46875 0.30443036556243896\n",
            "tcost icost -24.0625 0.3043668866157532\n",
            "tcost icost -24.0625 0.3045805096626282\n",
            "search -23.75\n",
            "tcost icost -20.0 0.31927791237831116\n",
            "tcost icost -24.375 0.3052136301994324\n",
            "tcost icost -24.375 0.304930180311203\n",
            "search -24.0625\n",
            "tcost icost -22.984375 0.30652618408203125\n",
            "tcost icost -24.21875 0.3057173788547516\n",
            "tcost icost -24.21875 0.30579352378845215\n",
            "search -23.90625\n",
            "tcost icost -19.75 0.3208616375923157\n",
            "tcost icost -24.171875 0.30562645196914673\n",
            "tcost icost -24.171875 0.3059544861316681\n",
            "search -23.859375\n",
            "tcost icost -17.5625 0.3363580107688904\n",
            "tcost icost -23.515625 0.3082927465438843\n",
            "tcost icost -23.515625 0.30842652916908264\n",
            "search -23.203125\n",
            "tcost icost -22.296875 0.30945754051208496\n",
            "tcost icost -23.578125 0.30613765120506287\n",
            "tcost icost -23.578125 0.3061666786670685\n",
            "search -23.265625\n",
            "tcost icost -22.734375 0.30704185366630554\n",
            "tcost icost -23.859375 0.3068978786468506\n",
            "tcost icost -23.859375 0.30704447627067566\n",
            "search -23.546875\n",
            "tcost icost -23.9375 0.3072325587272644\n",
            "tcost icost -23.953125 0.30499565601348877\n",
            "tcost icost -23.953125 0.30537205934524536\n",
            "search -23.640625\n",
            "tcost icost -23.921875 0.30548861622810364\n",
            "tcost icost -23.890625 0.30777856707572937\n",
            "tcost icost -23.890625 0.307295024394989\n",
            "search -23.578125\n",
            "tcost icost -5.1953125 0.43269556760787964\n",
            "tcost icost -5.1953125 0.43248671293258667\n",
            "tcost icost -5.1953125 0.43305274844169617\n",
            "search -4.76171875\n",
            "tcost icost 0.36572265625 0.4309149384498596\n",
            "tcost icost 0.31640625 0.42860203981399536\n",
            "tcost icost 0.31640625 0.42810508608818054\n",
            "search 0.74462890625\n",
            "ded\n",
            "time\n",
            "27 #### train ####\n",
            "repr, std, cov, closslb 0.0074026151560246944 0.476318359375 0.00012503867037594318 0.006436910014599562 0.00023224714095704257\n",
            "0.005699322193418544 0.024452019118968597 1.0\n",
            "repr, std, cov, closslb 0.0039786770939826965 0.47607421875 0.0001337970606982708 0.003242074977606535 0.01562688499689102\n",
            "0.00567658178407952 0.024452019118968597 1.0\n",
            "repr, std, cov, closslb 0.006107775494456291 0.47509765625 0.0001318154390901327 0.008775321766734123 5.643627082463354e-05\n",
            "0.0056089035174977705 0.02406409242921596 1.0\n",
            "repr, std, cov, closslb 0.004648583009839058 0.474365234375 0.000158216105774045 0.004093091003596783 0.015875833109021187\n",
            "0.005547574167888765 0.02368232012352013 1.0\n",
            "repr, std, cov, closslb 0.0037723116111010313 0.475341796875 0.00014743278734385967 0.004153222311288118 0.01550357136875391\n",
            "0.005437779180730994 0.02298274629689327 1.0\n",
            "repr, std, cov, closslb 0.003136893268674612 0.47509765625 0.00017114984802901745 1.5773355698911473e-05 0.0002640568127389997\n",
            "0.0053836992743443645 0.022686051609483945 1.0\n",
            "repr, std, cov, closslb 0.004406834952533245 0.4755859375 0.00015068682841956615 5.903290912101511e-06 0.0006227043340913951\n",
            "0.005346163671900996 0.022170481932124925 1.0\n",
            "repr, std, cov, closslb 0.004761063493788242 0.47705078125 0.00010799523442983627 0.004194548819214106 0.015237277373671532\n",
            "0.0053675804249606295 0.022393187088252317 1.0\n",
            "repr, std, cov, closslb 0.008462759666144848 0.47705078125 9.988294914364815e-05 0.008038639090955257 2.7039495762437582e-05\n",
            "0.005421498415567548 0.02309789008578309 1.0\n",
            "repr, std, cov, closslb 0.007524219341576099 0.476806640625 0.00010073790326714516 0.008371483534574509 0.027950823307037354\n",
            "0.005465022508831439 0.023517205003266442 1.0\n",
            "repr, std, cov, closslb 0.0042467741295695305 0.477294921875 9.146449156105518e-05 0.004110010340809822 0.00010454798757564276\n",
            "0.005519919316884379 0.024281537592825208 1.0\n",
            "repr, std, cov, closslb 0.0036636232398450375 0.4765625 0.00010569184087216854 3.484429498712416e-06 0.030265579000115395\n",
            "0.005553121742056653 0.024672969620781214 1.0\n",
            "repr, std, cov, closslb 0.003901171498000622 0.477294921875 0.0001065980177372694 6.468216270150151e-06 0.030487436801195145\n",
            "0.005553121742056653 0.024846199404906708 1.0\n",
            "repr, std, cov, closslb 0.005237688310444355 0.475830078125 0.00010990560986101627 0.006530426908284426 0.015274660661816597\n",
            "0.005553121742056653 0.024995649791624035 1.0\n",
            "repr, std, cov, closslb 0.0041275471448898315 0.4755859375 0.00012693298049271107 0.0031342296861112118 0.030795663595199585\n",
            "0.005454108837048506 0.024233047265247453 1.0\n",
            "repr, std, cov, closslb 0.0033889710903167725 0.47509765625 0.0001521522644907236 2.4820903490763158e-05 0.0005782839143648744\n",
            "0.005389082973618708 0.02382476975306487 1.0\n",
            "repr, std, cov, closslb 0.0039701699279248714 0.47509765625 0.00014627655036747456 0.003906633704900742 3.909735823981464e-05\n",
            "0.00531419866059381 0.023167253072808574 1.0\n",
            "repr, std, cov, closslb 0.009504524990916252 0.472900390625 0.00018765591084957123 0.003766614943742752 0.00011461190297268331\n",
            "0.005250840857024639 0.022776932022998422 1.0\n",
            "repr, std, cov, closslb 0.012351572513580322 0.473388671875 0.0001850633416324854 0.007888942956924438 0.015497427433729172\n",
            "0.005141776899543608 0.022082021266580844 1.0\n",
            "repr, std, cov, closslb 0.0050779362209141254 0.472412109375 0.0002229467500001192 0.010365056805312634 0.015317249111831188\n",
            "0.005095731441553589 0.021818751401230733 1.0\n",
            "repr, std, cov, closslb 0.005628744140267372 0.473876953125 0.00016448204405605793 0.0037725174333900213 0.015959618613123894\n",
            "0.005090640800752837 0.021472601009132886 1.0\n",
            "repr, std, cov, closslb 0.006158517673611641 0.473876953125 0.00015445658937096596 0.008378880098462105 0.0009049719665199518\n",
            "0.005095731441553589 0.021386924902404778 1.0\n",
            "repr, std, cov, closslb 0.008537054993212223 0.474609375 0.00014434335753321648 0.011438597925007343 0.01574762351810932\n",
            "0.0051572176607147125 0.021365559343061717 1.0\n",
            "repr, std, cov, closslb 0.003824982326477766 0.474853515625 0.00014855153858661652 2.6088582671945915e-06 0.00015000053099356592\n",
            "0.00518305537279286 0.021472601009132886 1.0\n",
            "repr, std, cov, closslb 0.003521227976307273 0.47607421875 0.00010215002112090588 8.841705130180344e-05 0.00026717441505752504\n",
            "0.005209022532067023 0.021731694147527236 1.0\n",
            "repr, std, cov, closslb 0.005414910148829222 0.475830078125 0.00011904723942279816 0.0038944853004068136 0.000216929882299155\n",
            "0.005245595261762876 0.021818751401230733 1.0\n",
            "repr, std, cov, closslb 0.005158097483217716 0.4765625 0.00010546902194619179 0.003969548735767603 0.030855683609843254\n",
            "0.005292994901839768 0.02212620739113527 1.0\n",
            "repr, std, cov, closslb 0.008742041885852814 0.477294921875 9.120022878050804e-05 0.011756567284464836 0.0001147307048086077\n",
            "0.005319512859254402 0.022527882556787917 1.0\n",
            "repr, std, cov, closslb 0.0029016004409641027 0.477783203125 7.848953828215599e-05 3.8034322642488405e-06 0.03183409944176674\n",
            "0.005346163671900995 0.023120987975868867 1.0\n",
            "repr, std, cov, closslb 0.008773012086749077 0.47705078125 8.366885595023632e-05 0.013718510046601295 0.0001767063804436475\n",
            "0.005367580424960629 0.023517205003266442 1.0\n",
            "repr, std, cov, closslb 0.0035848035477101803 0.476318359375 0.00010562501847743988 0.005748935975134373 0.0003347124729771167\n",
            "0.005314198660593809 0.02335324107946624 1.0\n",
            "repr, std, cov, closslb 0.005666535813361406 0.475341796875 0.00011892709881067276 0.003761613043025136 0.03066716156899929\n",
            "0.005245595261762876 0.02298274629689327 1.0\n",
            "repr, std, cov, closslb 0.003944110590964556 0.474365234375 0.00015999609604477882 0.003032187931239605 9.121376206167042e-05\n",
            "0.00513150875053379 0.02228155626842055 1.0\n",
            "repr, std, cov, closslb 0.005303278565406799 0.474365234375 0.0001362878829240799 0.008415842428803444 0.00010494319576537237\n",
            "0.005050098328214639 0.021884273133507375 1.0\n",
            "repr, std, cov, closslb 0.005171461962163448 0.474365234375 0.00014462717808783054 0.011460677720606327 0.015296687372028828\n",
            "0.004960054398101171 0.02119540147162022 1.0\n",
            "repr, std, cov, closslb 0.003028211649507284 0.47412109375 0.00014761858619749546 1.723188233881956e-06 0.00017968483734875917\n",
            "0.00490581974968694 0.02090087954901129 1.0\n",
            "tcost icost -68.625 0.25207623839378357\n",
            "tcost icost -77.25 0.25045761466026306\n",
            "tcost icost -77.25 0.25078803300857544\n",
            "search -77.0\n",
            "tcost icost -64.0625 0.2559637129306793\n",
            "tcost icost -77.25 0.2500644326210022\n",
            "tcost icost -77.25 0.2500612139701843\n",
            "search -77.0\n",
            "tcost icost -51.3125 0.2828838527202606\n",
            "tcost icost -77.25 0.25335821509361267\n",
            "tcost icost -77.25 0.2501526176929474\n",
            "search -77.0\n",
            "tcost icost -68.625 0.2513408958911896\n",
            "tcost icost -77.25 0.24976834654808044\n",
            "tcost icost -77.25 0.24941563606262207\n",
            "search -77.0\n",
            "tcost icost -64.0625 0.2551767826080322\n",
            "tcost icost -77.25 0.2501242756843567\n",
            "tcost icost -77.25 0.24966834485530853\n",
            "search -77.0\n",
            "tcost icost -74.5625 0.24957787990570068\n",
            "tcost icost -77.0625 0.24967429041862488\n",
            "tcost icost -77.0625 0.2491856962442398\n",
            "search -76.8125\n",
            "tcost icost -58.65625 0.26464220881462097\n",
            "tcost icost -76.875 0.24955472350120544\n",
            "tcost icost -76.875 0.2503282129764557\n",
            "search -76.625\n",
            "tcost icost -69.0625 0.25021985173225403\n",
            "tcost icost -77.5625 0.2494436353445053\n",
            "tcost icost -77.5625 0.2490355372428894\n",
            "search -77.3125\n",
            "tcost icost -75.25 0.25148195028305054\n",
            "tcost icost -77.625 0.25147655606269836\n",
            "tcost icost -77.625 0.25239527225494385\n",
            "search -77.375\n",
            "tcost icost -71.0625 0.2528376579284668\n",
            "tcost icost -76.9375 0.2513556480407715\n",
            "tcost icost -76.9375 0.2520778775215149\n",
            "search -76.6875\n",
            "tcost icost -77.375 0.25147077441215515\n",
            "tcost icost -77.375 0.2510348856449127\n",
            "tcost icost -77.375 0.25118523836135864\n",
            "search -77.125\n",
            "tcost icost -51.59375 0.2836335599422455\n",
            "tcost icost -77.1875 0.25099173188209534\n",
            "tcost icost -77.1875 0.25118890404701233\n",
            "search -76.9375\n",
            "tcost icost -68.875 0.25256645679473877\n",
            "tcost icost -77.5625 0.25110480189323425\n",
            "tcost icost -77.5625 0.2507181465625763\n",
            "search -77.3125\n",
            "tcost icost -75.625 0.2508755624294281\n",
            "tcost icost -77.875 0.25093716382980347\n",
            "tcost icost -77.875 0.25141653418540955\n",
            "search -77.625\n",
            "tcost icost -59.875 0.2650221884250641\n",
            "tcost icost -77.6875 0.25114351511001587\n",
            "tcost icost -77.6875 0.25126633048057556\n",
            "search -77.4375\n",
            "tcost icost -64.5625 0.25707685947418213\n",
            "tcost icost -77.375 0.25147223472595215\n",
            "tcost icost -77.375 0.2514154016971588\n",
            "search -77.125\n",
            "tcost icost -54.1875 0.277608186006546\n",
            "tcost icost -78.25 0.25149330496788025\n",
            "tcost icost -78.25 0.25146591663360596\n",
            "search -78.0\n",
            "tcost icost -70.75 0.25272905826568604\n",
            "tcost icost -78.875 0.25133180618286133\n",
            "tcost icost -78.875 0.2517373561859131\n",
            "search -78.625\n",
            "tcost icost -65.6875 0.25547170639038086\n",
            "tcost icost -78.0 0.2527352571487427\n",
            "tcost icost -78.0 0.25232234597206116\n",
            "search -77.75\n",
            "tcost icost -65.3125 0.2554548382759094\n",
            "tcost icost -77.875 0.2514844834804535\n",
            "tcost icost -77.875 0.251469224691391\n",
            "search -77.625\n",
            "tcost icost -70.25 0.25284063816070557\n",
            "tcost icost -78.5625 0.25165873765945435\n",
            "tcost icost -78.5625 0.25155210494995117\n",
            "search -78.3125\n",
            "tcost icost -70.875 0.252379447221756\n",
            "tcost icost -79.0 0.2512659728527069\n",
            "tcost icost -79.0 0.25124478340148926\n",
            "search -78.75\n",
            "tcost icost -68.3125 0.253260999917984\n",
            "tcost icost -76.6875 0.25111135840415955\n",
            "tcost icost -76.6875 0.25178295373916626\n",
            "search -76.4375\n",
            "tcost icost -69.8125 0.2537979483604431\n",
            "tcost icost -76.1875 0.2519519031047821\n",
            "tcost icost -76.1875 0.25201091170310974\n",
            "search -75.9375\n",
            "tcost icost -68.8125 0.2532152235507965\n",
            "tcost icost -77.3125 0.25158703327178955\n",
            "tcost icost -77.3125 0.25230950117111206\n",
            "search -77.0625\n",
            "tcost icost -57.0625 0.27069661021232605\n",
            "tcost icost -76.0625 0.2521306872367859\n",
            "tcost icost -76.0625 0.2514464855194092\n",
            "search -75.8125\n",
            "tcost icost -68.375 0.2537906765937805\n",
            "tcost icost -76.5 0.2518525719642639\n",
            "tcost icost -76.5 0.25184082984924316\n",
            "search -76.25\n",
            "tcost icost -57.34375 0.2697902321815491\n",
            "tcost icost -76.0 0.2524087429046631\n",
            "tcost icost -76.0 0.2522145211696625\n",
            "search -75.75\n",
            "tcost icost -67.875 0.2540079653263092\n",
            "tcost icost -74.0 0.2524506151676178\n",
            "tcost icost -74.0 0.2527061998844147\n",
            "search -73.75\n",
            "tcost icost -66.6875 0.2544986307621002\n",
            "tcost icost -75.0 0.25222331285476685\n",
            "tcost icost -75.0 0.25226879119873047\n",
            "search -74.75\n",
            "tcost icost -67.5625 0.25334322452545166\n",
            "tcost icost -76.0625 0.2517918646335602\n",
            "tcost icost -76.0625 0.25176507234573364\n",
            "search -75.8125\n",
            "tcost icost -67.625 0.2533753514289856\n",
            "tcost icost -76.3125 0.2511248290538788\n",
            "tcost icost -76.3125 0.2514730989933014\n",
            "search -76.0625\n",
            "tcost icost -57.28125 0.27022767066955566\n",
            "tcost icost -76.5 0.2510436773300171\n",
            "tcost icost -76.5 0.2513863444328308\n",
            "search -76.25\n",
            "tcost icost -72.5625 0.2513079345226288\n",
            "tcost icost -78.125 0.2513899803161621\n",
            "tcost icost -78.125 0.25139501690864563\n",
            "search -77.875\n",
            "tcost icost -71.25 0.2525072991847992\n",
            "tcost icost -76.9375 0.25276196002960205\n",
            "tcost icost -76.9375 0.2522338330745697\n",
            "search -76.6875\n",
            "tcost icost -68.8125 0.2532845139503479\n",
            "tcost icost -74.9375 0.2516992390155792\n",
            "tcost icost -74.9375 0.2518821656703949\n",
            "search -74.6875\n",
            "tcost icost -56.65625 0.2718214690685272\n",
            "tcost icost -75.6875 0.25193408131599426\n",
            "tcost icost -75.6875 0.2521476745605469\n",
            "search -75.4375\n",
            "tcost icost -56.25 0.2724226117134094\n",
            "tcost icost -75.625 0.2513516843318939\n",
            "tcost icost -75.625 0.251767635345459\n",
            "search -75.375\n",
            "tcost icost -58.0 0.2687559127807617\n",
            "tcost icost -74.9375 0.2517290413379669\n",
            "tcost icost -74.9375 0.2514183819293976\n",
            "search -74.6875\n",
            "tcost icost -66.125 0.2538933753967285\n",
            "tcost icost -74.8125 0.2509961426258087\n",
            "tcost icost -74.8125 0.2515818178653717\n",
            "search -74.5625\n",
            "tcost icost -69.0625 0.25244078040122986\n",
            "tcost icost -77.5625 0.25073912739753723\n",
            "tcost icost -77.5625 0.25036749243736267\n",
            "search -77.3125\n",
            "tcost icost -70.6875 0.2516895532608032\n",
            "tcost icost -76.5625 0.25027862191200256\n",
            "tcost icost -76.5625 0.25018754601478577\n",
            "search -76.3125\n",
            "tcost icost -74.125 0.25015392899513245\n",
            "tcost icost -76.6875 0.2502034902572632\n",
            "tcost icost -76.6875 0.250474214553833\n",
            "search -76.4375\n",
            "tcost icost -71.25 0.2512805759906769\n",
            "tcost icost -77.25 0.24997352063655853\n",
            "tcost icost -77.25 0.25089648365974426\n",
            "search -77.0\n",
            "tcost icost -72.0625 0.2504208981990814\n",
            "tcost icost -77.5625 0.2504301071166992\n",
            "tcost icost -77.5625 0.25014612078666687\n",
            "search -77.3125\n",
            "tcost icost -62.59375 0.2583453357219696\n",
            "tcost icost -76.5625 0.25065168738365173\n",
            "tcost icost -76.5625 0.2502385377883911\n",
            "search -76.3125\n",
            "tcost icost -59.15625 0.26508909463882446\n",
            "tcost icost -77.1875 0.25015658140182495\n",
            "tcost icost -77.1875 0.249968484044075\n",
            "search -76.9375\n",
            "tcost icost -65.3125 0.25416654348373413\n",
            "tcost icost -77.75 0.25058093667030334\n",
            "tcost icost -77.75 0.24994368851184845\n",
            "search -77.5\n",
            "tcost icost -70.0625 0.2513853907585144\n",
            "tcost icost -78.1875 0.25024670362472534\n",
            "tcost icost -78.1875 0.2507884204387665\n",
            "search -77.9375\n",
            "tcost icost -65.5 0.2540423274040222\n",
            "tcost icost -78.0 0.25084400177001953\n",
            "tcost icost -78.0 0.2505214810371399\n",
            "search -77.75\n",
            "tcost icost -71.125 0.25049731135368347\n",
            "tcost icost -76.375 0.2506943643093109\n",
            "tcost icost -76.375 0.2502378225326538\n",
            "search -76.125\n",
            "tcost icost -76.5 0.2502584755420685\n",
            "tcost icost -76.5 0.2502061724662781\n",
            "tcost icost -76.5 0.25001007318496704\n",
            "search -76.25\n",
            "tcost icost -74.125 0.2496492713689804\n",
            "tcost icost -76.3125 0.24994193017482758\n",
            "tcost icost -76.3125 0.2502078115940094\n",
            "search -76.0625\n",
            "tcost icost -69.5625 0.2510811686515808\n",
            "tcost icost -77.4375 0.24993649125099182\n",
            "tcost icost -77.4375 0.2499963790178299\n",
            "search -77.1875\n",
            "tcost icost -65.875 0.25362884998321533\n",
            "tcost icost -77.4375 0.2503550350666046\n",
            "tcost icost -77.4375 0.24998341500759125\n",
            "search -77.1875\n",
            "tcost icost -68.25 0.25163838267326355\n",
            "tcost icost -76.875 0.2497267723083496\n",
            "tcost icost -76.875 0.2503003776073456\n",
            "search -76.625\n",
            "ded\n",
            "time\n",
            "28 #### train ####\n",
            "repr, std, cov, closslb 0.004397707059979439 0.473388671875 0.00016469648107886314 0.004041234031319618 6.852069054730237e-05\n",
            "0.004803922142968647 0.02034438053109987 1.0\n",
            "repr, std, cov, closslb 0.0042351773008704185 0.47412109375 0.00013652699999511242 0.0076392474584281445 0.015561901964247227\n",
            "0.004789539152105337 0.02018235544992211 1.0\n",
            "repr, std, cov, closslb 0.0035758046433329582 0.474853515625 0.00013981410302221775 4.224009444442345e-06 0.015528611838817596\n",
            "0.004799123019948699 0.019862166172177523 1.0\n",
            "repr, std, cov, closslb 0.003665371797978878 0.476318359375 0.00011097430251538754 1.2426696684997296e-06 0.00016986328409984708\n",
            "0.004847330787587867 0.020162193256665444 1.0\n",
            "repr, std, cov, closslb 0.004278248175978661 0.477783203125 9.209290146827698e-05 0.004111853893846273 0.0001056827895808965\n",
            "0.004955099298802369 0.020817484622323516 1.0\n",
            "repr, std, cov, closslb 0.004319237545132637 0.478515625 7.447297684848309e-05 3.380167527211597e-06 9.071687964024022e-05\n",
            "0.004989889224581266 0.02119540147162022 1.0\n",
            "repr, std, cov, closslb 0.004449246451258659 0.47802734375 7.65463337302208e-05 7.3384580900892615e-06 0.015535185113549232\n",
            "0.005080474770736594 0.021884273133507375 1.0\n",
            "repr, std, cov, closslb 0.014145915396511555 0.477783203125 8.82619060575962e-05 0.01166690606623888 0.015265626832842827\n",
            "0.005126382368165625 0.02228155626842055 1.0\n",
            "repr, std, cov, closslb 0.004655459895730019 0.476806640625 0.00010975310578942299 0.0038700962904840708 0.00028814346296712756\n",
            "0.005105928000168136 0.022708737661093426 1.0\n",
            "repr, std, cov, closslb 0.0038745934143662453 0.474609375 0.00015810271725058556 1.2960908861714415e-05 2.3359927581623197e-05\n",
            "0.005024923411647921 0.02250537717960831 1.0\n",
            "repr, std, cov, closslb 0.004345983266830444 0.475341796875 0.00014116358943283558 0.0042622932232916355 0.00015042285667732358\n",
            "0.004915636295006062 0.02179695444678395 1.0\n",
            "repr, std, cov, closslb 0.008004048839211464 0.474609375 0.00014113239012658596 0.007748771924525499 0.015426920726895332\n",
            "0.004866749214117111 0.02140831182730718 1.0\n",
            "repr, std, cov, closslb 0.004186824429780245 0.474609375 0.0001430748961865902 0.004588539246469736 0.015230854973196983\n",
            "0.004746648041088951 0.02079668793438913 1.0\n",
            "repr, std, cov, closslb 0.0035516181960701942 0.474609375 0.00014943000860512257 2.043705990217859e-06 0.00011256707512075081\n",
            "0.004722985834803113 0.020610450172750844 1.0\n",
            "repr, std, cov, closslb 0.005250457674264908 0.474365234375 0.00015036854892969131 0.0032763986382633448 0.015588987618684769\n",
            "0.004648056551796686 0.020061684019013034 1.0\n",
            "repr, std, cov, closslb 0.006396008655428886 0.475341796875 0.00012189336121082306 0.012886329554021358 0.015579327940940857\n",
            "0.004657357312956831 0.020021620755880522 1.0\n",
            "repr, std, cov, closslb 0.004477044567465782 0.475830078125 0.0001366431824862957 0.004124999046325684 0.0011055185459554195\n",
            "0.0046434131386580285 0.019782915732601283 1.0\n",
            "repr, std, cov, closslb 0.00790109857916832 0.47509765625 0.0001282289158552885 0.007823141291737556 0.015692925080657005\n",
            "0.004652704608348482 0.0200416423766364 1.0\n",
            "repr, std, cov, closslb 0.005970261991024017 0.476318359375 0.00010811095125973225 0.007801018189638853 0.01599755883216858\n",
            "0.0047088451680546 0.020405474726179137 1.0\n",
            "repr, std, cov, closslb 0.005554135888814926 0.475341796875 0.00013217143714427948 0.008115185424685478 0.015022952109575272\n",
            "0.004732436529458552 0.02069301571860528 1.0\n",
            "repr, std, cov, closslb 0.005836465395987034 0.476806640625 9.569176472723484e-05 0.003874690504744649 0.00015993740817066282\n",
            "0.0047656631321328895 0.020672343375230055 1.0\n",
            "repr, std, cov, closslb 0.006371969822794199 0.475830078125 0.000115987379103899 0.003852714318782091 6.357282836688682e-05\n",
            "0.004808726065111615 0.02087999954946183 1.0\n",
            "repr, std, cov, closslb 0.005133816972374916 0.47705078125 9.842333383858204e-05 0.00598863884806633 0.015459316782653332\n",
            "0.004857030296493829 0.021386924902404778 1.0\n",
            "repr, std, cov, closslb 0.004240105859935284 0.476806640625 0.00010421429760754108 1.3031753042014316e-05 7.687448669457808e-05\n",
            "0.004896022808048036 0.021709984163363876 1.0\n",
            "repr, std, cov, closslb 0.005578342825174332 0.476806640625 0.00011231936514377594 0.003730465890839696 0.04560256376862526\n",
            "0.004910725569436626 0.021949991627611562 1.0\n",
            "repr, std, cov, closslb 0.007444760296493769 0.476806640625 9.648851118981838e-05 0.004290177021175623 0.0001305542973568663\n",
            "0.0049353283536713154 0.022348467804176167 1.0\n",
            "repr, std, cov, closslb 0.005750139243900776 0.47705078125 0.0001049344427883625 0.0036744181998074055 0.00011702258780132979\n",
            "0.004989889224581266 0.022913935724997177 1.0\n",
            "repr, std, cov, closslb 0.006650155410170555 0.476318359375 0.00011519994586706161 0.008246874436736107 2.1293242753017694e-05\n",
            "0.004989889224581266 0.02328332124249172 1.0\n",
            "repr, std, cov, closslb 0.014769305475056171 0.474365234375 0.00015763170085847378 0.011763297952711582 0.015229623764753342\n",
            "0.004881364066873852 0.022550410439344703 1.0\n",
            "repr, std, cov, closslb 0.0035241423174738884 0.474609375 0.00015439698472619057 2.202738642154145e-06 0.0310344398021698\n",
            "0.004803922142968647 0.0221483335985264 1.0\n",
            "repr, std, cov, closslb 0.0037976019084453583 0.473876953125 0.00016732048243284225 6.519307135022245e-06 0.030304299667477608\n",
            "0.004699441585442132 0.021451149859273617 1.0\n",
            "repr, std, cov, closslb 0.005669952370226383 0.47412109375 0.0001619379036128521 0.003760572522878647 0.015393628738820553\n",
            "0.004648056551796686 0.021089741654926164 1.0\n",
            "repr, std, cov, closslb 0.005635355599224567 0.474853515625 0.00015244679525494576 0.008298160508275032 0.03044859692454338\n",
            "0.004606432440740839 0.020755156865501265 1.0\n",
            "repr, std, cov, closslb 0.004318431485444307 0.474365234375 0.00016775401309132576 2.1565225324593484e-05 0.01563463732600212\n",
            "0.004601830610130709 0.02069301571860528 1.0\n",
            "repr, std, cov, closslb 0.009532995522022247 0.475830078125 0.00011200946755707264 0.013297811150550842 3.77817232219968e-05\n",
            "0.004624885827528781 0.020713708734323884 1.0\n",
            "repr, std, cov, closslb 0.005256162025034428 0.476318359375 0.00010948861017823219 0.004084751475602388 0.0005362739320844412\n",
            "0.004657357312956831 0.020921780428560296 1.0\n",
            "tcost icost -191.875 0.2025439739227295\n",
            "tcost icost -227.75 0.2005978524684906\n",
            "tcost icost -227.75 0.20034758746623993\n",
            "search -227.5\n",
            "tcost icost -191.875 0.20234888792037964\n",
            "tcost icost -227.75 0.20079128444194794\n",
            "tcost icost -227.75 0.20056474208831787\n",
            "search -227.5\n",
            "tcost icost -200.75 0.20144391059875488\n",
            "tcost icost -227.75 0.20074717700481415\n",
            "tcost icost -227.75 0.20051725208759308\n",
            "search -227.5\n",
            "tcost icost -175.25 0.20431257784366608\n",
            "tcost icost -227.75 0.20070312917232513\n",
            "tcost icost -227.75 0.20073626935482025\n",
            "search -227.5\n",
            "tcost icost -207.75 0.20132122933864594\n",
            "tcost icost -227.75 0.20072202384471893\n",
            "tcost icost -227.75 0.2009911835193634\n",
            "search -227.5\n",
            "tcost icost -191.875 0.20241637527942657\n",
            "tcost icost -227.75 0.20052695274353027\n",
            "tcost icost -227.75 0.20040589570999146\n",
            "search -227.5\n",
            "tcost icost -207.75 0.20081259310245514\n",
            "tcost icost -227.75 0.200577512383461\n",
            "tcost icost -227.75 0.2008267641067505\n",
            "search -227.5\n",
            "tcost icost -191.875 0.20252582430839539\n",
            "tcost icost -227.75 0.20068693161010742\n",
            "tcost icost -227.75 0.20045273005962372\n",
            "search -227.5\n",
            "tcost icost -185.125 0.20289146900177002\n",
            "tcost icost -227.75 0.200836643576622\n",
            "tcost icost -227.75 0.20115958154201508\n",
            "search -227.5\n",
            "tcost icost -200.75 0.2018681764602661\n",
            "tcost icost -227.75 0.20109307765960693\n",
            "tcost icost -227.75 0.20065419375896454\n",
            "search -227.5\n",
            "tcost icost -175.25 0.20456035435199738\n",
            "tcost icost -227.75 0.20132869482040405\n",
            "tcost icost -227.75 0.2008390873670578\n",
            "search -227.5\n",
            "tcost icost -175.25 0.20497848093509674\n",
            "tcost icost -227.75 0.2012680172920227\n",
            "tcost icost -227.75 0.2011697143316269\n",
            "search -227.5\n",
            "tcost icost -200.75 0.20175352692604065\n",
            "tcost icost -227.75 0.20127703249454498\n",
            "tcost icost -227.75 0.20088060200214386\n",
            "search -227.5\n",
            "tcost icost -200.75 0.20217476785182953\n",
            "tcost icost -227.75 0.20152679085731506\n",
            "tcost icost -227.75 0.20100264251232147\n",
            "search -227.5\n",
            "tcost icost -213.875 0.2019687294960022\n",
            "tcost icost -228.0 0.20110590755939484\n",
            "tcost icost -228.0 0.20122013986110687\n",
            "search -227.75\n",
            "tcost icost -177.75 0.20403054356575012\n",
            "tcost icost -215.125 0.20199519395828247\n",
            "tcost icost -215.125 0.20228897035121918\n",
            "search -214.875\n",
            "tcost icost -200.5 0.20221473276615143\n",
            "tcost icost -227.375 0.20100237429141998\n",
            "tcost icost -227.375 0.20107151567935944\n",
            "search -227.125\n",
            "tcost icost -192.5 0.20329596102237701\n",
            "tcost icost -228.375 0.20115265250205994\n",
            "tcost icost -228.375 0.20106390118598938\n",
            "search -228.125\n",
            "tcost icost -201.5 0.20198176801204681\n",
            "tcost icost -228.75 0.20140314102172852\n",
            "tcost icost -228.75 0.20111176371574402\n",
            "search -228.5\n",
            "tcost icost -208.75 0.2016836553812027\n",
            "tcost icost -228.5 0.20109209418296814\n",
            "tcost icost -228.5 0.20181801915168762\n",
            "search -228.25\n",
            "tcost icost -201.75 0.20177873969078064\n",
            "tcost icost -228.875 0.20129705965518951\n",
            "tcost icost -228.875 0.20119231939315796\n",
            "search -228.625\n",
            "tcost icost -175.0 0.20521487295627594\n",
            "tcost icost -227.625 0.20129629969596863\n",
            "tcost icost -227.625 0.2015238255262375\n",
            "search -227.375\n",
            "tcost icost -202.5 0.20262646675109863\n",
            "tcost icost -217.25 0.20277884602546692\n",
            "tcost icost -217.25 0.20220156013965607\n",
            "search -217.0\n",
            "tcost icost -186.5 0.20395377278327942\n",
            "tcost icost -228.625 0.202483132481575\n",
            "tcost icost -228.625 0.20193375647068024\n",
            "search -228.375\n",
            "tcost icost -192.75 0.20407605171203613\n",
            "tcost icost -228.625 0.20222432911396027\n",
            "tcost icost -228.625 0.2019565999507904\n",
            "search -228.375\n",
            "tcost icost -230.375 0.20234277844429016\n",
            "tcost icost -230.375 0.20228049159049988\n",
            "tcost icost -230.375 0.20253288745880127\n",
            "search -230.125\n",
            "tcost icost -202.375 0.20293644070625305\n",
            "tcost icost -230.0 0.20214837789535522\n",
            "tcost icost -230.0 0.20224229991436005\n",
            "search -229.75\n",
            "tcost icost -210.0 0.20257329940795898\n",
            "tcost icost -230.0 0.20182347297668457\n",
            "tcost icost -230.0 0.2016933709383011\n",
            "search -229.75\n",
            "tcost icost -210.0 0.20247089862823486\n",
            "tcost icost -230.25 0.2020527869462967\n",
            "tcost icost -230.25 0.2017832249403\n",
            "search -230.0\n",
            "tcost icost -196.875 0.20289653539657593\n",
            "tcost icost -225.375 0.20221035182476044\n",
            "tcost icost -225.375 0.20232297480106354\n",
            "search -225.125\n",
            "tcost icost -200.25 0.2026863843202591\n",
            "tcost icost -227.375 0.202096089720726\n",
            "tcost icost -227.375 0.20206528902053833\n",
            "search -227.125\n",
            "tcost icost -176.0 0.20555199682712555\n",
            "tcost icost -228.5 0.20175078511238098\n",
            "tcost icost -228.5 0.20193065702915192\n",
            "search -228.25\n",
            "tcost icost -209.625 0.20247747004032135\n",
            "tcost icost -229.875 0.20214581489562988\n",
            "tcost icost -229.875 0.20184646546840668\n",
            "search -229.625\n",
            "tcost icost -224.25 0.20223921537399292\n",
            "tcost icost -224.25 0.202155202627182\n",
            "tcost icost -224.25 0.20208142697811127\n",
            "search -224.0\n",
            "tcost icost -174.75 0.20534870028495789\n",
            "tcost icost -229.375 0.2016388326883316\n",
            "tcost icost -229.375 0.20262978971004486\n",
            "search -229.125\n",
            "tcost icost -183.0 0.2047656923532486\n",
            "tcost icost -222.375 0.20189324021339417\n",
            "tcost icost -222.375 0.20232883095741272\n",
            "search -222.125\n",
            "tcost icost -201.5 0.2028694748878479\n",
            "tcost icost -231.125 0.20185185968875885\n",
            "tcost icost -231.125 0.20188243687152863\n",
            "search -230.875\n",
            "tcost icost -127.9375 0.23018184304237366\n",
            "tcost icost -127.9375 0.23011396825313568\n",
            "tcost icost -127.9375 0.22997689247131348\n",
            "search -127.6875\n",
            "tcost icost -137.75 0.22401776909828186\n",
            "tcost icost -205.125 0.20281916856765747\n",
            "tcost icost -205.125 0.2026345580816269\n",
            "search -204.875\n",
            "tcost icost -148.375 0.21550554037094116\n",
            "tcost icost -198.75 0.20322804152965546\n",
            "tcost icost -198.75 0.20349553227424622\n",
            "search -198.5\n",
            "tcost icost -202.125 0.20213817059993744\n",
            "tcost icost -222.125 0.2019500732421875\n",
            "tcost icost -222.125 0.20189523696899414\n",
            "search -221.875\n",
            "tcost icost -194.375 0.20265580713748932\n",
            "tcost icost -220.625 0.20173567533493042\n",
            "tcost icost -220.625 0.2019054889678955\n",
            "search -220.375\n",
            "tcost icost -148.0 0.21521300077438354\n",
            "tcost icost -182.5 0.2054060697555542\n",
            "tcost icost -182.5 0.20526458323001862\n",
            "search -182.25\n",
            "tcost icost -32.4375 0.39235803484916687\n",
            "tcost icost -46.34375 0.35664233565330505\n",
            "tcost icost -46.34375 0.3563360273838043\n",
            "search -46.0\n",
            "tcost icost -38.4375 0.3733143210411072\n",
            "tcost icost -38.4375 0.37350374460220337\n",
            "tcost icost -38.4375 0.3732811510562897\n",
            "search -38.0625\n",
            "tcost icost -147.875 0.21279799938201904\n",
            "tcost icost -196.375 0.20188620686531067\n",
            "tcost icost -196.375 0.20221729576587677\n",
            "search -196.125\n",
            "tcost icost -197.125 0.20067273080348969\n",
            "tcost icost -214.625 0.20103658735752106\n",
            "tcost icost -214.625 0.20052391290664673\n",
            "search -214.375\n",
            "tcost icost -32.125 0.3923051357269287\n",
            "tcost icost -55.25 0.3362811207771301\n",
            "tcost icost -55.25 0.3362331688404083\n",
            "search -54.90625\n",
            "tcost icost -189.75 0.19991613924503326\n",
            "tcost icost -212.875 0.1998194456100464\n",
            "tcost icost -212.875 0.1998050957918167\n",
            "search -212.625\n",
            "tcost icost -137.0 0.22250545024871826\n",
            "tcost icost -201.75 0.20052139461040497\n",
            "tcost icost -201.75 0.20123448967933655\n",
            "search -201.5\n",
            "tcost icost -219.625 0.19890283048152924\n",
            "tcost icost -223.75 0.1988474726676941\n",
            "tcost icost -223.75 0.1987226903438568\n",
            "search -223.5\n",
            "tcost icost -183.875 0.20213006436824799\n",
            "tcost icost -218.0 0.2008756697177887\n",
            "tcost icost -218.0 0.19996224343776703\n",
            "search -217.75\n",
            "tcost icost -195.875 0.20039425790309906\n",
            "tcost icost -220.25 0.2009844183921814\n",
            "tcost icost -220.25 0.19979442656040192\n",
            "search -220.0\n",
            "tcost icost -202.125 0.1997341513633728\n",
            "tcost icost -216.375 0.20034585893154144\n",
            "tcost icost -216.375 0.1992722898721695\n",
            "search -216.125\n",
            "tcost icost -28.953125 0.39948394894599915\n",
            "tcost icost -39.9375 0.3721355199813843\n",
            "tcost icost -39.9375 0.3714352250099182\n",
            "search -39.5625\n",
            "tcost icost -215.75 0.19858886301517487\n",
            "tcost icost -219.125 0.19849145412445068\n",
            "tcost icost -219.125 0.19827577471733093\n",
            "search -218.875\n",
            "tcost icost -190.875 0.1999174952507019\n",
            "tcost icost -213.375 0.20033475756645203\n",
            "tcost icost -213.375 0.19917890429496765\n",
            "search -213.125\n",
            "tcost icost -161.125 0.20722703635692596\n",
            "tcost icost -214.75 0.1993933618068695\n",
            "tcost icost -214.75 0.19909356534481049\n",
            "search -214.5\n",
            "tcost icost -173.75 0.20211809873580933\n",
            "tcost icost -200.125 0.19970475137233734\n",
            "tcost icost -200.125 0.19955603778362274\n",
            "search -199.875\n",
            "tcost icost -171.0 0.20334579050540924\n",
            "tcost icost -191.875 0.2009608894586563\n",
            "tcost icost -191.875 0.20108774304389954\n",
            "search -191.625\n",
            "tcost icost -161.125 0.20741252601146698\n",
            "tcost icost -217.25 0.19943074882030487\n",
            "tcost icost -217.25 0.19951121509075165\n",
            "search -217.0\n",
            "tcost icost -197.625 0.20066781342029572\n",
            "tcost icost -218.75 0.20029035210609436\n",
            "tcost icost -218.75 0.20023077726364136\n",
            "search -218.5\n",
            "tcost icost -187.625 0.20129573345184326\n",
            "tcost icost -198.5 0.2020714432001114\n",
            "tcost icost -198.5 0.2017260640859604\n",
            "search -198.25\n",
            "tcost icost -195.25 0.20093944668769836\n",
            "tcost icost -220.5 0.20041340589523315\n",
            "tcost icost -220.5 0.2001294642686844\n",
            "search -220.25\n",
            "tcost icost -193.5 0.20109930634498596\n",
            "tcost icost -217.75 0.20070509612560272\n",
            "tcost icost -217.75 0.20025625824928284\n",
            "search -217.5\n",
            "tcost icost -197.875 0.20070365071296692\n",
            "tcost icost -222.5 0.2002934217453003\n",
            "tcost icost -222.5 0.2013527899980545\n",
            "search -222.25\n",
            "tcost icost -210.875 0.20174428820610046\n",
            "tcost icost -216.0 0.20185156166553497\n",
            "tcost icost -216.0 0.20161190629005432\n",
            "search -215.75\n",
            "tcost icost -129.875 0.23139294981956482\n",
            "tcost icost -193.625 0.20300398766994476\n",
            "tcost icost -193.625 0.2030525654554367\n",
            "search -193.375\n",
            "tcost icost -192.75 0.20343536138534546\n",
            "tcost icost -218.625 0.20264700055122375\n",
            "tcost icost -218.625 0.2026476263999939\n",
            "search -218.375\n",
            "tcost icost -215.375 0.20238423347473145\n",
            "tcost icost -222.0 0.20245234668254852\n",
            "tcost icost -222.0 0.2025599181652069\n",
            "search -221.75\n",
            "tcost icost -201.25 0.20323720574378967\n",
            "tcost icost -227.375 0.2021723985671997\n",
            "tcost icost -227.375 0.20230194926261902\n",
            "search -227.125\n",
            "tcost icost -186.0 0.20513680577278137\n",
            "tcost icost -221.875 0.20273905992507935\n",
            "tcost icost -221.875 0.20351852476596832\n",
            "search -221.625\n",
            "tcost icost -109.9375 0.25026002526283264\n",
            "tcost icost -169.875 0.20575113594532013\n",
            "tcost icost -169.875 0.20669208467006683\n",
            "search -169.625\n",
            "tcost icost -146.25 0.2161087691783905\n",
            "tcost icost -173.5 0.2072959691286087\n",
            "tcost icost -173.5 0.20719791948795319\n",
            "search -173.25\n",
            "tcost icost -185.75 0.2048017978668213\n",
            "tcost icost -211.75 0.2038981169462204\n",
            "tcost icost -211.75 0.20401787757873535\n",
            "search -211.5\n",
            "tcost icost -183.25 0.20540611445903778\n",
            "tcost icost -209.0 0.20394594967365265\n",
            "tcost icost -209.0 0.20390234887599945\n",
            "search -208.75\n",
            "tcost icost -176.375 0.2067534327507019\n",
            "tcost icost -180.625 0.2071915566921234\n",
            "tcost icost -180.625 0.2070445865392685\n",
            "search -180.375\n",
            "tcost icost -163.125 0.21019572019577026\n",
            "tcost icost -193.625 0.20554640889167786\n",
            "tcost icost -193.625 0.20506280660629272\n",
            "search -193.375\n",
            "tcost icost -189.5 0.20499871671199799\n",
            "tcost icost -226.25 0.20235411822795868\n",
            "tcost icost -226.25 0.2023516148328781\n",
            "search -226.0\n",
            "tcost icost -183.75 0.2047925740480423\n",
            "tcost icost -202.625 0.20358650386333466\n",
            "tcost icost -202.625 0.20364360511302948\n",
            "search -202.375\n",
            "tcost icost -194.0 0.2035033106803894\n",
            "tcost icost -220.5 0.20259283483028412\n",
            "tcost icost -220.5 0.2026236355304718\n",
            "search -220.25\n",
            "tcost icost -210.375 0.20242001116275787\n",
            "tcost icost -216.75 0.20269589126110077\n",
            "tcost icost -216.75 0.20379723608493805\n",
            "search -216.5\n",
            "tcost icost -202.0 0.20315255224704742\n",
            "tcost icost -219.625 0.2028801143169403\n",
            "tcost icost -219.625 0.20255529880523682\n",
            "search -219.375\n",
            "tcost icost -198.25 0.202959343791008\n",
            "tcost icost -223.875 0.2021801471710205\n",
            "tcost icost -223.875 0.20188526809215546\n",
            "search -223.625\n",
            "tcost icost -124.6875 0.23316670954227448\n",
            "tcost icost -160.0 0.20992645621299744\n",
            "tcost icost -160.0 0.20997411012649536\n",
            "search -159.75\n",
            "tcost icost -213.375 0.20222412049770355\n",
            "tcost icost -220.375 0.20218682289123535\n",
            "tcost icost -220.375 0.20239758491516113\n",
            "search -220.125\n",
            "tcost icost -216.75 0.20247985422611237\n",
            "tcost icost -223.625 0.20263297855854034\n",
            "tcost icost -223.625 0.20254714787006378\n",
            "search -223.375\n",
            "tcost icost -196.375 0.203778937458992\n",
            "tcost icost -220.625 0.20291179418563843\n",
            "tcost icost -220.625 0.204435795545578\n",
            "search -220.375\n",
            "tcost icost -192.75 0.20366282761096954\n",
            "tcost icost -217.875 0.20304639637470245\n",
            "tcost icost -217.875 0.20398619771003723\n",
            "search -217.625\n",
            "tcost icost -187.75 0.20403671264648438\n",
            "tcost icost -214.75 0.20307183265686035\n",
            "tcost icost -214.75 0.2030249387025833\n",
            "search -214.5\n",
            "tcost icost -31.59375 0.4004155695438385\n",
            "tcost icost -38.375 0.38327857851982117\n",
            "tcost icost -38.375 0.38320621848106384\n",
            "search -38.0\n",
            "tcost icost -156.0 0.21329760551452637\n",
            "tcost icost -192.125 0.20512764155864716\n",
            "tcost icost -192.125 0.2041759341955185\n",
            "search -191.875\n",
            "tcost icost -151.75 0.21437330543994904\n",
            "tcost icost -215.25 0.2019742876291275\n",
            "tcost icost -215.25 0.20176704227924347\n",
            "search -215.0\n",
            "tcost icost -187.25 0.2025277465581894\n",
            "tcost icost -213.75 0.20150382816791534\n",
            "tcost icost -213.75 0.2016335278749466\n",
            "search -213.5\n",
            "tcost icost -147.5 0.21593153476715088\n",
            "tcost icost -177.375 0.20561757683753967\n",
            "tcost icost -177.375 0.20565137267112732\n",
            "search -177.125\n",
            "tcost icost -175.75 0.20497138798236847\n",
            "tcost icost -201.5 0.20281770825386047\n",
            "tcost icost -201.5 0.2026878297328949\n",
            "search -201.25\n",
            "tcost icost -131.5 0.22999869287014008\n",
            "tcost icost -172.75 0.20744894444942474\n",
            "tcost icost -172.75 0.2081528902053833\n",
            "search -172.5\n",
            "tcost icost -77.875 0.29244717955589294\n",
            "tcost icost -134.5 0.22429484128952026\n",
            "tcost icost -134.5 0.2240665853023529\n",
            "search -134.25\n",
            "tcost icost -97.5625 0.26203590631484985\n",
            "tcost icost -168.5 0.20644636452198029\n",
            "tcost icost -168.5 0.2064540684223175\n",
            "search -168.25\n",
            "tcost icost -155.0 0.21234828233718872\n",
            "tcost icost -189.5 0.2043839991092682\n",
            "tcost icost -189.5 0.2050209492444992\n",
            "search -189.25\n",
            "tcost icost -43.1875 0.3634724020957947\n",
            "tcost icost -57.71875 0.3305530846118927\n",
            "tcost icost -57.71875 0.330780565738678\n",
            "search -57.375\n",
            "tcost icost -208.125 0.20107993483543396\n",
            "tcost icost -214.875 0.20042107999324799\n",
            "tcost icost -214.875 0.20027793943881989\n",
            "search -214.625\n",
            "tcost icost -181.25 0.20253100991249084\n",
            "tcost icost -207.125 0.20086757838726044\n",
            "tcost icost -207.125 0.20077942311763763\n",
            "search -206.875\n",
            "tcost icost -170.5 0.20337781310081482\n",
            "tcost icost -176.5 0.20406895875930786\n",
            "tcost icost -176.5 0.20391614735126495\n",
            "search -176.25\n",
            "tcost icost -205.25 0.20102941989898682\n",
            "tcost icost -205.25 0.20060385763645172\n",
            "tcost icost -205.25 0.20067338645458221\n",
            "search -205.0\n",
            "tcost icost -195.125 0.20155835151672363\n",
            "tcost icost -211.5 0.20138226449489594\n",
            "tcost icost -211.5 0.20166850090026855\n",
            "search -211.25\n",
            "tcost icost -200.125 0.20133374631404877\n",
            "tcost icost -218.5 0.2008354514837265\n",
            "tcost icost -218.5 0.20066529512405396\n",
            "search -218.25\n",
            "tcost icost -196.875 0.20275506377220154\n",
            "tcost icost -213.375 0.2027491331100464\n",
            "tcost icost -213.375 0.20303957164287567\n",
            "search -213.125\n",
            "tcost icost -202.625 0.2023405134677887\n",
            "tcost icost -219.75 0.20261074602603912\n",
            "tcost icost -219.75 0.20243048667907715\n",
            "search -219.5\n",
            "tcost icost -221.75 0.20178715884685516\n",
            "tcost icost -226.75 0.2032424509525299\n",
            "tcost icost -226.75 0.2021157592535019\n",
            "search -226.5\n",
            "tcost icost -208.0 0.20217518508434296\n",
            "tcost icost -212.875 0.2028428316116333\n",
            "tcost icost -212.875 0.2023577243089676\n",
            "search -212.625\n",
            "tcost icost -163.5 0.2097187489271164\n",
            "tcost icost -215.75 0.20457352697849274\n",
            "tcost icost -215.75 0.2031542807817459\n",
            "search -215.5\n",
            "tcost icost -160.875 0.21136000752449036\n",
            "tcost icost -214.375 0.20353956520557404\n",
            "tcost icost -214.375 0.20333488285541534\n",
            "search -214.125\n",
            "tcost icost -154.125 0.21498918533325195\n",
            "tcost icost -215.5 0.20326097309589386\n",
            "tcost icost -215.5 0.2030375748872757\n",
            "search -215.25\n",
            "tcost icost -177.625 0.205873504281044\n",
            "tcost icost -211.125 0.20344752073287964\n",
            "tcost icost -211.125 0.2036084085702896\n",
            "search -210.875\n",
            "tcost icost -203.75 0.20385992527008057\n",
            "tcost icost -215.875 0.2045133113861084\n",
            "tcost icost -215.875 0.2033994048833847\n",
            "search -215.625\n",
            "tcost icost -193.25 0.20458799600601196\n",
            "tcost icost -216.375 0.2037777304649353\n",
            "tcost icost -216.375 0.20355159044265747\n",
            "search -216.125\n",
            "tcost icost -196.5 0.20401594042778015\n",
            "tcost icost -213.0 0.20504602789878845\n",
            "tcost icost -213.0 0.2041008174419403\n",
            "search -212.75\n",
            "tcost icost -191.75 0.20412610471248627\n",
            "tcost icost -215.0 0.20441745221614838\n",
            "tcost icost -215.0 0.20340970158576965\n",
            "search -214.75\n",
            "tcost icost -142.625 0.2207178771495819\n",
            "tcost icost -202.625 0.20539525151252747\n",
            "tcost icost -202.625 0.20439454913139343\n",
            "search -202.375\n",
            "tcost icost -151.0 0.21592023968696594\n",
            "tcost icost -207.5 0.20558948814868927\n",
            "tcost icost -207.5 0.2043820321559906\n",
            "search -207.25\n",
            "tcost icost -214.125 0.20398473739624023\n",
            "tcost icost -214.125 0.20407021045684814\n",
            "tcost icost -214.125 0.20523543655872345\n",
            "search -213.875\n",
            "tcost icost -210.375 0.2047601193189621\n",
            "tcost icost -214.25 0.2055329978466034\n",
            "tcost icost -214.25 0.20555050671100616\n",
            "search -214.0\n",
            "tcost icost -192.125 0.2051374316215515\n",
            "tcost icost -214.625 0.20470906794071198\n",
            "tcost icost -214.625 0.20451593399047852\n",
            "search -214.375\n",
            "tcost icost -116.5 0.2444610595703125\n",
            "tcost icost -161.0 0.21197491884231567\n",
            "tcost icost -161.0 0.21161113679409027\n",
            "search -160.75\n",
            "tcost icost -208.875 0.20397312939167023\n",
            "tcost icost -214.75 0.20423386991024017\n",
            "tcost icost -214.75 0.20398475229740143\n",
            "search -214.5\n",
            "tcost icost -211.5 0.20459334552288055\n",
            "tcost icost -216.0 0.20464278757572174\n",
            "tcost icost -216.0 0.20466238260269165\n",
            "search -215.75\n",
            "tcost icost -170.75 0.2080039232969284\n",
            "tcost icost -213.25 0.20423422753810883\n",
            "tcost icost -213.25 0.204328715801239\n",
            "search -213.0\n",
            "tcost icost -189.375 0.2053370475769043\n",
            "tcost icost -213.625 0.20428229868412018\n",
            "tcost icost -213.625 0.20434236526489258\n",
            "search -213.375\n",
            "tcost icost -187.875 0.2062499076128006\n",
            "tcost icost -211.0 0.2044041007757187\n",
            "tcost icost -211.0 0.2046089917421341\n",
            "search -210.75\n",
            "tcost icost -143.375 0.2224922627210617\n",
            "tcost icost -206.625 0.20468977093696594\n",
            "tcost icost -206.625 0.20471808314323425\n",
            "search -206.375\n",
            "tcost icost -214.375 0.20410175621509552\n",
            "tcost icost -220.0 0.20445369184017181\n",
            "tcost icost -220.0 0.2049475759267807\n",
            "search -219.75\n",
            "tcost icost -215.5 0.20515087246894836\n",
            "tcost icost -215.5 0.20523248612880707\n",
            "tcost icost -215.5 0.20514698326587677\n",
            "search -215.25\n",
            "tcost icost -193.0 0.20572808384895325\n",
            "tcost icost -216.5 0.20501288771629333\n",
            "tcost icost -216.5 0.20531544089317322\n",
            "search -216.25\n",
            "tcost icost -192.625 0.20591597259044647\n",
            "tcost icost -216.0 0.2054203301668167\n",
            "tcost icost -216.0 0.20564617216587067\n",
            "search -215.75\n",
            "tcost icost -188.125 0.2066107541322708\n",
            "tcost icost -214.625 0.2052621990442276\n",
            "tcost icost -214.625 0.20545731484889984\n",
            "search -214.375\n",
            "tcost icost -200.875 0.20575353503227234\n",
            "tcost icost -227.625 0.20505128800868988\n",
            "tcost icost -227.625 0.20482222735881805\n",
            "search -227.375\n",
            "tcost icost -201.5 0.20640629529953003\n",
            "tcost icost -219.0 0.20564524829387665\n",
            "tcost icost -219.0 0.20554454624652863\n",
            "search -218.75\n",
            "tcost icost -203.5 0.2073410451412201\n",
            "tcost icost -218.25 0.20553800463676453\n",
            "tcost icost -218.25 0.20546197891235352\n",
            "search -218.0\n",
            "tcost icost -188.5 0.20693977177143097\n",
            "tcost icost -214.0 0.20549394190311432\n",
            "tcost icost -214.0 0.20543034374713898\n",
            "search -213.75\n",
            "tcost icost -169.125 0.21115246415138245\n",
            "tcost icost -222.375 0.2055063098669052\n",
            "tcost icost -222.375 0.20584268867969513\n",
            "search -222.125\n",
            "tcost icost -188.5 0.20722444355487823\n",
            "tcost icost -222.875 0.20592622458934784\n",
            "tcost icost -222.875 0.20577505230903625\n",
            "search -222.625\n",
            "tcost icost -177.625 0.20983487367630005\n",
            "tcost icost -212.625 0.2064751535654068\n",
            "tcost icost -212.625 0.20621122419834137\n",
            "search -212.375\n",
            "tcost icost -190.875 0.20650361478328705\n",
            "tcost icost -215.125 0.20575769245624542\n",
            "tcost icost -215.125 0.20592950284481049\n",
            "search -214.875\n",
            "tcost icost -191.25 0.20695310831069946\n",
            "tcost icost -211.625 0.20614349842071533\n",
            "tcost icost -211.625 0.20662149786949158\n",
            "search -211.375\n",
            "tcost icost -195.625 0.206983283162117\n",
            "tcost icost -219.75 0.20587258040905\n",
            "tcost icost -219.75 0.20606979727745056\n",
            "search -219.5\n",
            "tcost icost -185.125 0.20754213631153107\n",
            "tcost icost -211.375 0.20593516528606415\n",
            "tcost icost -211.375 0.20579691231250763\n",
            "search -211.125\n",
            "tcost icost -183.375 0.2083602249622345\n",
            "tcost icost -203.875 0.20627368986606598\n",
            "tcost icost -203.875 0.2061196118593216\n",
            "search -203.625\n",
            "tcost icost -181.375 0.20800207555294037\n",
            "tcost icost -215.75 0.2064560353755951\n",
            "tcost icost -215.75 0.20584052801132202\n",
            "search -215.5\n",
            "tcost icost -191.875 0.20697325468063354\n",
            "tcost icost -215.125 0.2063891440629959\n",
            "tcost icost -215.125 0.20636652410030365\n",
            "search -214.875\n",
            "tcost icost -146.375 0.22210076451301575\n",
            "tcost icost -207.5 0.2066228687763214\n",
            "tcost icost -207.5 0.20596924424171448\n",
            "search -207.25\n",
            "tcost icost -192.375 0.20722579956054688\n",
            "tcost icost -218.25 0.20630988478660583\n",
            "tcost icost -218.25 0.206395223736763\n",
            "search -218.0\n",
            "tcost icost -217.125 0.20600810647010803\n",
            "tcost icost -221.75 0.2061595916748047\n",
            "tcost icost -221.75 0.20606963336467743\n",
            "search -221.5\n",
            "tcost icost -179.625 0.20712029933929443\n",
            "tcost icost -205.25 0.20614774525165558\n",
            "tcost icost -205.25 0.2059878557920456\n",
            "search -205.0\n",
            "tcost icost -149.625 0.22046060860157013\n",
            "tcost icost -188.75 0.207472562789917\n",
            "tcost icost -188.75 0.20755590498447418\n",
            "search -188.5\n",
            "tcost icost -164.375 0.21373912692070007\n",
            "tcost icost -226.625 0.20566600561141968\n",
            "tcost icost -226.625 0.20569680631160736\n",
            "search -226.375\n",
            "tcost icost -197.875 0.20667879283428192\n",
            "tcost icost -222.25 0.2057136744260788\n",
            "tcost icost -222.25 0.2057991772890091\n",
            "search -222.0\n",
            "tcost icost -198.375 0.2066766768693924\n",
            "tcost icost -223.5 0.2059205174446106\n",
            "tcost icost -223.5 0.20664852857589722\n",
            "search -223.25\n",
            "tcost icost -192.375 0.20669110119342804\n",
            "tcost icost -216.375 0.20587219297885895\n",
            "tcost icost -216.375 0.2059933990240097\n",
            "search -216.125\n",
            "tcost icost -190.25 0.20750465989112854\n",
            "tcost icost -217.375 0.20538532733917236\n",
            "tcost icost -217.375 0.20535016059875488\n",
            "search -217.125\n",
            "tcost icost -188.75 0.2067275047302246\n",
            "tcost icost -213.625 0.20563317835330963\n",
            "tcost icost -213.625 0.20508258044719696\n",
            "search -213.375\n",
            "tcost icost -160.25 0.2121041864156723\n",
            "tcost icost -206.5 0.2055630385875702\n",
            "tcost icost -206.5 0.20570439100265503\n",
            "search -206.25\n",
            "tcost icost -184.125 0.20696088671684265\n",
            "tcost icost -210.375 0.2060357630252838\n",
            "tcost icost -210.375 0.20560066401958466\n",
            "search -210.125\n",
            "tcost icost -158.25 0.21507984399795532\n",
            "tcost icost -220.0 0.2052900195121765\n",
            "tcost icost -220.0 0.20570017397403717\n",
            "search -219.75\n",
            "tcost icost -158.625 0.2150096744298935\n",
            "tcost icost -214.25 0.205857053399086\n",
            "tcost icost -214.25 0.20653264224529266\n",
            "search -214.0\n",
            "tcost icost -213.375 0.20488141477108002\n",
            "tcost icost -217.375 0.20476694405078888\n",
            "tcost icost -217.375 0.20455312728881836\n",
            "search -217.125\n",
            "tcost icost -151.125 0.21609742939472198\n",
            "tcost icost -208.375 0.20478388667106628\n",
            "tcost icost -208.375 0.20416824519634247\n",
            "search -208.125\n",
            "tcost icost -163.625 0.20976386964321136\n",
            "tcost icost -216.25 0.203843355178833\n",
            "tcost icost -216.25 0.20388241112232208\n",
            "search -216.0\n",
            "tcost icost -188.625 0.20469480752944946\n",
            "tcost icost -213.875 0.20365555584430695\n",
            "tcost icost -213.875 0.20348569750785828\n",
            "search -213.625\n",
            "tcost icost -211.5 0.20409347116947174\n",
            "tcost icost -215.125 0.2041645646095276\n",
            "tcost icost -215.125 0.20448125898838043\n",
            "search -214.875\n",
            "tcost icost -152.875 0.2159864604473114\n",
            "tcost icost -215.625 0.2045935094356537\n",
            "tcost icost -215.625 0.20428626239299774\n",
            "search -215.375\n",
            "tcost icost -182.0 0.20575472712516785\n",
            "tcost icost -208.625 0.20447134971618652\n",
            "tcost icost -208.625 0.20427414774894714\n",
            "search -208.375\n",
            "tcost icost -154.25 0.21630772948265076\n",
            "tcost icost -211.75 0.20432378351688385\n",
            "tcost icost -211.75 0.2047339230775833\n",
            "search -211.5\n",
            "tcost icost -213.375 0.20369775593280792\n",
            "tcost icost -213.375 0.20375166833400726\n",
            "tcost icost -213.375 0.20465080440044403\n",
            "search -213.125\n",
            "tcost icost -174.0 0.2066882997751236\n",
            "tcost icost -215.875 0.20419037342071533\n",
            "tcost icost -215.875 0.20438581705093384\n",
            "search -215.625\n",
            "tcost icost -208.0 0.20385214686393738\n",
            "tcost icost -222.375 0.20365746319293976\n",
            "tcost icost -222.375 0.20420949161052704\n",
            "search -222.125\n",
            "tcost icost -198.375 0.20462095737457275\n",
            "tcost icost -216.875 0.20386454463005066\n",
            "tcost icost -216.875 0.20397081971168518\n",
            "search -216.625\n",
            "tcost icost -210.375 0.20453433692455292\n",
            "tcost icost -214.0 0.20435498654842377\n",
            "tcost icost -214.0 0.20425894856452942\n",
            "search -213.75\n",
            "tcost icost -193.625 0.20503050088882446\n",
            "tcost icost -218.125 0.20444045960903168\n",
            "tcost icost -218.125 0.20437860488891602\n",
            "search -217.875\n",
            "tcost icost -152.875 0.21731939911842346\n",
            "tcost icost -214.875 0.20540739595890045\n",
            "tcost icost -214.875 0.20562562346458435\n",
            "search -214.625\n",
            "tcost icost -169.875 0.20929594337940216\n",
            "tcost icost -212.375 0.20570588111877441\n",
            "tcost icost -212.375 0.20669451355934143\n",
            "search -212.125\n",
            "tcost icost -150.75 0.21758343279361725\n",
            "tcost icost -185.0 0.2076239287853241\n",
            "tcost icost -185.0 0.20779326558113098\n",
            "search -184.75\n",
            "tcost icost -186.0 0.20577698945999146\n",
            "tcost icost -211.5 0.20469272136688232\n",
            "tcost icost -211.5 0.20501050353050232\n",
            "search -211.25\n",
            "tcost icost -211.125 0.20554929971694946\n",
            "tcost icost -211.125 0.2055283784866333\n",
            "tcost icost -211.125 0.20577876269817352\n",
            "search -210.875\n",
            "tcost icost -155.75 0.21611759066581726\n",
            "tcost icost -214.0 0.20495986938476562\n",
            "tcost icost -214.0 0.20528173446655273\n",
            "search -213.75\n",
            "tcost icost -218.5 0.20524980127811432\n",
            "tcost icost -222.75 0.2054470330476761\n",
            "tcost icost -222.75 0.2054743766784668\n",
            "search -222.5\n",
            "tcost icost -203.125 0.2059786468744278\n",
            "tcost icost -221.375 0.20531193912029266\n",
            "tcost icost -221.375 0.20558355748653412\n",
            "search -221.125\n",
            "tcost icost -204.75 0.20703350007534027\n",
            "tcost icost -219.0 0.20572088658809662\n",
            "tcost icost -219.0 0.20556332170963287\n",
            "search -218.75\n",
            "tcost icost -163.5 0.21222464740276337\n",
            "tcost icost -198.0 0.2070668339729309\n",
            "tcost icost -198.0 0.20692676305770874\n",
            "search -197.75\n",
            "tcost icost -181.375 0.2079361230134964\n",
            "tcost icost -218.0 0.20590057969093323\n",
            "tcost icost -218.0 0.20574288070201874\n",
            "search -217.75\n",
            "tcost icost -167.125 0.21037712693214417\n",
            "tcost icost -209.125 0.20630434155464172\n",
            "tcost icost -209.125 0.2065649777650833\n",
            "search -208.875\n",
            "tcost icost -183.25 0.20799221098423004\n",
            "tcost icost -230.5 0.20504260063171387\n",
            "tcost icost -230.5 0.2048831731081009\n",
            "search -230.25\n",
            "tcost icost -201.75 0.20655818283557892\n",
            "tcost icost -205.625 0.20680581033229828\n",
            "tcost icost -205.625 0.20688925683498383\n",
            "search -205.375\n",
            "tcost icost -183.5 0.20784969627857208\n",
            "tcost icost -220.0 0.20572088658809662\n",
            "tcost icost -220.0 0.20566096901893616\n",
            "search -219.75\n",
            "tcost icost -193.0 0.20694859325885773\n",
            "tcost icost -197.875 0.2068530023097992\n",
            "tcost icost -197.875 0.20700351893901825\n",
            "search -197.625\n",
            "tcost icost -175.5 0.20758818089962006\n",
            "tcost icost -202.5 0.2062814086675644\n",
            "tcost icost -202.5 0.20636218786239624\n",
            "search -202.25\n",
            "tcost icost -227.875 0.20531779527664185\n",
            "tcost icost -227.875 0.20537368953227997\n",
            "tcost icost -227.875 0.20519743859767914\n",
            "search -227.625\n",
            "tcost icost -175.625 0.20974989235401154\n",
            "tcost icost -228.875 0.2056369185447693\n",
            "tcost icost -228.875 0.20579178631305695\n",
            "search -228.625\n",
            "tcost icost -196.5 0.20695790648460388\n",
            "tcost icost -225.625 0.20574496686458588\n",
            "tcost icost -225.625 0.20598886907100677\n",
            "search -225.375\n",
            "tcost icost -188.5 0.2079584002494812\n",
            "tcost icost -224.375 0.20696868002414703\n",
            "tcost icost -224.375 0.2058710753917694\n",
            "search -224.125\n",
            "tcost icost -190.875 0.20795632898807526\n",
            "tcost icost -226.0 0.2059745043516159\n",
            "tcost icost -226.0 0.2060837745666504\n",
            "search -225.75\n",
            "tcost icost -207.25 0.20609226822853088\n",
            "tcost icost -226.875 0.2056308388710022\n",
            "tcost icost -226.875 0.205645352602005\n",
            "search -226.625\n",
            "tcost icost -212.5 0.2061620056629181\n",
            "tcost icost -232.125 0.2066706418991089\n",
            "tcost icost -232.125 0.20554575324058533\n",
            "search -231.875\n",
            "tcost icost -233.0 0.20655693113803864\n",
            "tcost icost -233.0 0.20557166635990143\n",
            "tcost icost -233.0 0.20554737746715546\n",
            "search -232.75\n",
            "tcost icost -231.125 0.20623041689395905\n",
            "tcost icost -231.125 0.20596320927143097\n",
            "tcost icost -231.125 0.20610935986042023\n",
            "search -230.875\n",
            "tcost icost -226.625 0.20630869269371033\n",
            "tcost icost -233.75 0.20622581243515015\n",
            "tcost icost -233.75 0.20585882663726807\n",
            "search -233.5\n",
            "tcost icost -203.25 0.20698347687721252\n",
            "tcost icost -232.5 0.20618867874145508\n",
            "tcost icost -232.5 0.20711490511894226\n",
            "search -232.25\n",
            "tcost icost -175.125 0.21024033427238464\n",
            "tcost icost -228.625 0.2077825963497162\n",
            "tcost icost -228.625 0.20658276975154877\n",
            "search -228.375\n",
            "tcost icost -216.25 0.20704273879528046\n",
            "tcost icost -233.0 0.20659267902374268\n",
            "tcost icost -233.0 0.2062726616859436\n",
            "search -232.75\n",
            "tcost icost -201.375 0.2073233276605606\n",
            "tcost icost -228.25 0.20647001266479492\n",
            "tcost icost -228.25 0.20674368739128113\n",
            "search -228.0\n",
            "tcost icost -190.375 0.20954647660255432\n",
            "tcost icost -225.75 0.20634868741035461\n",
            "tcost icost -225.75 0.20747579634189606\n",
            "search -225.5\n",
            "tcost icost -200.25 0.20888599753379822\n",
            "tcost icost -225.0 0.20770733058452606\n",
            "tcost icost -225.0 0.2081698477268219\n",
            "search -224.75\n",
            "tcost icost -192.375 0.20956012606620789\n",
            "tcost icost -228.25 0.20776386559009552\n",
            "tcost icost -228.25 0.20757274329662323\n",
            "search -228.0\n",
            "tcost icost -203.375 0.20796172320842743\n",
            "tcost icost -221.875 0.20752596855163574\n",
            "tcost icost -221.875 0.20818813145160675\n",
            "search -221.625\n",
            "tcost icost -197.375 0.20837394893169403\n",
            "tcost icost -224.375 0.20745766162872314\n",
            "tcost icost -224.375 0.20861785113811493\n",
            "search -224.125\n",
            "tcost icost -205.625 0.20739567279815674\n",
            "tcost icost -210.375 0.20755913853645325\n",
            "tcost icost -210.375 0.20731227099895477\n",
            "search -210.125\n",
            "tcost icost -182.875 0.20861901342868805\n",
            "tcost icost -209.0 0.20726442337036133\n",
            "tcost icost -209.0 0.20739206671714783\n",
            "search -208.75\n",
            "tcost icost -214.5 0.2075783908367157\n",
            "tcost icost -214.5 0.2076900750398636\n",
            "tcost icost -214.5 0.2074214518070221\n",
            "search -214.25\n",
            "tcost icost -191.5 0.20769527554512024\n",
            "tcost icost -218.625 0.20675793290138245\n",
            "tcost icost -218.625 0.20704832673072815\n",
            "search -218.375\n",
            "tcost icost -139.875 0.22234857082366943\n",
            "tcost icost -142.0 0.2222481518983841\n",
            "tcost icost -142.0 0.22223085165023804\n",
            "search -141.75\n",
            "tcost icost -47.40625 0.35888996720314026\n",
            "tcost icost -69.4375 0.3136337995529175\n",
            "tcost icost -69.4375 0.3123322129249573\n",
            "search -69.125\n",
            "ded\n",
            "time\n",
            "29 #### train ####\n",
            "repr, std, cov, closslb 0.004764149431139231 0.476806640625 9.772321209311485e-05 0.009513414464890957 0.0006230245344340801\n",
            "0.004699441585442132 0.021429720139134485 1.0\n",
            "repr, std, cov, closslb 0.0037126769311726093 0.476806640625 9.519304148852825e-05 0.014430821873247623 7.822399493306875e-05\n",
            "0.00473716896598801 0.021731694147527236 1.0\n",
            "repr, std, cov, closslb 0.006522396579384804 0.47705078125 9.775534272193909e-05 0.004655113909393549 0.016500001773238182\n",
            "0.004784754397707629 0.022192652414057046 1.0\n",
            "repr, std, cov, closslb 0.00734697375446558 0.4765625 0.00010372907854616642 0.0034207513090223074 0.015369517728686333\n",
            "0.004842488299288579 0.022437995855615907 1.0\n",
            "repr, std, cov, closslb 0.008707258850336075 0.475341796875 0.0001434267032891512 0.010901977308094501 0.016116119921207428\n",
            "0.004886245430940725 0.0221483335985264 1.0\n",
            "repr, std, cov, closslb 0.011722669936716557 0.475830078125 0.00012530316598713398 0.004190608859062195 0.004379340913146734\n",
            "0.004940263682024986 0.02228155626842055 1.0\n",
            "repr, std, cov, closslb 0.009251845069229603 0.475830078125 0.00011812127195298672 3.437994746491313e-05 0.000517315580509603\n",
            "0.005014888619520263 0.022326141662513656 1.0\n",
            "repr, std, cov, closslb 0.011517725884914398 0.47509765625 0.0001417580060660839 0.004408196080476046 0.015449101105332375\n",
            "0.005050098328214639 0.02212620739113527 1.0\n",
            "repr, std, cov, closslb 0.01632694900035858 0.4765625 0.00013045500963926315 0.0035572692286223173 7.609014573972672e-05\n",
            "0.0051161449620964715 0.02203792338189368 1.0\n",
            "repr, std, cov, closslb 0.019835233688354492 0.4765625 0.00011671474203467369 0.007909556850790977 0.0001138818624895066\n",
            "0.005188238428165652 0.02230383782468897 1.0\n",
            "repr, std, cov, closslb 0.010744176805019379 0.47802734375 8.899625390768051e-05 1.4247096260078251e-05 8.912666817195714e-05\n",
            "0.00524035490685602 0.022845331172640412 1.0\n",
            "repr, std, cov, closslb 0.011133086867630482 0.476806640625 0.00010090344585478306 0.004264385439455509 0.0013714783126488328\n",
            "0.005245595261762876 0.02319042032588138 1.0\n",
            "repr, std, cov, closslb 0.011811730451881886 0.475830078125 0.00014321156777441502 1.699119275144767e-05 0.01501001138240099\n",
            "0.00520381871335367 0.02250537717960831 1.0\n",
            "repr, std, cov, closslb 0.01941053569316864 0.4755859375 0.00014777458272874355 0.004207833204418421 0.00036027177702635527\n",
            "0.005167537253253802 0.02212620739113527 1.0\n",
            "repr, std, cov, closslb 0.012298472225666046 0.47412109375 0.00018036668188869953 0.004246142692863941 0.00025497100432403386\n",
            "0.00508555524550733 0.021537083251435905 1.0\n",
            "repr, std, cov, closslb 0.015581261366605759 0.47607421875 0.00012181675992906094 0.012415438890457153 5.613575558527373e-05\n",
            "0.005029948335059569 0.02123781346996493 1.0\n",
            "repr, std, cov, closslb 0.014302028343081474 0.475830078125 0.0001436946913599968 0.004247898701578379 0.01608801633119583\n",
            "0.0051161449620964715 0.021451149859273617 1.0\n",
            "repr, std, cov, closslb 0.013937833718955517 0.47607421875 0.00012603006325662136 0.004278453998267651 0.015358678065240383\n",
            "0.005141776899543608 0.021688295867496384 1.0\n",
            "repr, std, cov, closslb 0.005575581919401884 0.4765625 0.00015002372674643993 2.530060737626627e-05 0.000968863139860332\n",
            "0.0051572176607147125 0.022326141662513656 1.0\n",
            "repr, std, cov, closslb 0.015023430809378624 0.47705078125 0.00011853943578898907 0.007951274514198303 0.00044759592856280506\n",
            "0.005167537253253802 0.022663388221262685 1.0\n",
            "repr, std, cov, closslb 0.009656626731157303 0.47607421875 0.00014042085967957973 0.00446957116946578 0.0002811183803714812\n",
            "0.0051572176607147125 0.023028734772233345 1.0\n",
            "repr, std, cov, closslb 0.010266177356243134 0.475341796875 0.00013927044346928596 0.00862904079258442 0.0159755889326334\n",
            "0.005111033928168304 0.02289104468031686 1.0\n",
            "repr, std, cov, closslb 0.0054842084646224976 0.476806640625 0.00011111516505479813 2.051332194241695e-05 0.015470213256776333\n",
            "0.005029948335059569 0.022708737661093426 1.0\n",
            "repr, std, cov, closslb 0.004049478564411402 0.475830078125 0.00014216545969247818 2.1628631657222286e-05 0.00010763445607153699\n",
            "0.004955099298802369 0.022415580275340567 1.0\n",
            "repr, std, cov, closslb 0.015551378950476646 0.475341796875 0.00013120798394083977 0.012021521106362343 0.00020046652934979647\n",
            "0.004876487579294558 0.021840570152631963 1.0\n",
            "repr, std, cov, closslb 0.00793411023914814 0.475830078125 0.00013356306590139866 0.0036522585432976484 0.00019524901290424168\n",
            "0.004852178118375454 0.021731694147527236 1.0\n",
            "repr, std, cov, closslb 0.012584454379975796 0.47607421875 0.00012141792103648186 0.00916002131998539 0.01565232127904892\n",
            "0.004866749214117111 0.021644984254004126 1.0\n",
            "repr, std, cov, closslb 0.00918231625109911 0.47509765625 0.0001378438901156187 0.004300905857235193 0.00036421007825993\n",
            "0.004857030296493829 0.021666629238258128 1.0\n",
            "repr, std, cov, closslb 0.01351539883762598 0.476318359375 0.00012241676449775696 0.007788582239300013 0.00021366000873968005\n",
            "0.004799123019948699 0.02132289223569809 1.0\n",
            "repr, std, cov, closslb 0.007207298651337624 0.4755859375 0.0001275455579161644 2.3124874132918194e-05 0.00014119441038928926\n",
            "0.0047943286912574415 0.021365559343061717 1.0\n",
            "repr, std, cov, closslb 0.007897086441516876 0.47607421875 0.00013054884038865566 3.8040590879973024e-05 0.00024116659187711775\n",
            "0.004775199224060286 0.021494073610142016 1.0\n",
            "repr, std, cov, closslb 0.006912623532116413 0.476318359375 0.00015381863340735435 2.355351170990616e-05 0.0003319907991681248\n",
            "0.004746648041088951 0.021558620334687337 1.0\n",
            "repr, std, cov, closslb 0.009280606172978878 0.47607421875 0.00012755231000483036 0.02106943540275097 0.00025132831069640815\n",
            "0.004741906134953997 0.02203792338189368 1.0\n",
            "repr, std, cov, closslb 0.008399400860071182 0.47705078125 9.657838381826878e-05 0.004224392585456371 0.0007171184988692403\n",
            "0.004727708820637915 0.02228155626842055 1.0\n",
            "repr, std, cov, closslb 0.007985643111169338 0.4775390625 9.949388913810253e-05 0.008294006809592247 0.0002846625284291804\n",
            "0.0047088451680546 0.022572960849784045 1.0\n",
            "repr, std, cov, closslb 0.005769720301032066 0.47705078125 0.00012487638741731644 0.004248587414622307 6.349667091853917e-05\n",
            "0.004727708820637915 0.02293684966072217 1.0\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(30):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "    c_loader = make_weighted(buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    # train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "52c05147-476a-4829-a858-9550917d66e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31mvideo.avi: Invalid data found when processing input\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 338
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "590a0de7-b72c-4093-d77c-ced956345207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  self.hub = sentry_sdk.Hub(client)\n",
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240825_234036-n807anhl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/n807anhl' target=\"_blank\">efficient-oath-22</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/n807anhl' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/n807anhl</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.cuda.amp.autocast():\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.cuda.amp.autocast():\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjm2kV3H7ZVR",
        "outputId": "d4040132-28f1-4347-8028-2e951476da85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6872065\n"
          ]
        }
      ],
      "source": [
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko5qJO7Et09L",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK5u500Vad2P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.cuda.amp.autocast(): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ],
      "metadata": {
        "id": "gJ3X_hQelW2x",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi4ODp-XlZoU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "QYbOgNoZn6JL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kcajtpjr7Io",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.cuda.amp.autocast(): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mhkK_9AQm8_q",
        "wUhKd009Qvk3"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}