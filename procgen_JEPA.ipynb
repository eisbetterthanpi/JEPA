{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "5d5d0854-a064-4488-e8b3-acc3f3b600b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Jx0k_ndHOEMe",
        "outputId": "b5fb2840-eeb5-45ae-f0e0-e2a15f8e7ccc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-05466a22e258>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# visualise(agent.sense,layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mvisualise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N2TGs69fnrZo"
      },
      "outputs": [],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "for name, param in agent.emb.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred[0].weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "eb552e65-521b-4921-f7c8-50e29c06a039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcOidvtW9KAH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "0a92b92f-6976-4a3e-bfc6-5c76a2c741b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2ce37b725f67>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "d96d7a2e-d4c5-4485-c06a-7ae8feb9ed66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-2e41fcea5594>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "        # self.h0 = None\n",
        "        self.h0 = torch.randn((self.jepa.pred.num_layers, d_model), device=device)\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "\n",
        "    def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # 20\n",
        "        act = lact.cpu()[0,:k].tolist()\n",
        "        # act = out[0].cpu()[0,:k].tolist()\n",
        "        self.h0=lh0[k-1].unsqueeze(0)\n",
        "        self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        # return lact, lh0, lx, lz\n",
        "        return act\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        torch.nn.init.xavier_normal_(z)\n",
        "        optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e1, (0.9, 0.95)) #\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        lh0=h0\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                out, h0 = self.jepa.pred(sxaz, h0)\n",
        "                # print(out.shape)\n",
        "                # out = out[:, -1, :]\n",
        "                sx = sx + out\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            lh0 = torch.cat([lh0, h0], dim=0)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.randn((self.jepa.pred.num_layers, batch_size, self.d_model), device=device)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0)\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    # _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    # stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + 1000*clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "c100cf8d-3790-4b40-ddb7-e5de6ad2a291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1XBDhD2efIFW9lnewGRLrb362w47a8b1q\n",
            "From (redirected): https://drive.google.com/uc?id=1XBDhD2efIFW9lnewGRLrb362w47a8b1q&confirm=t&uuid=ce202158-5f32-4222-93b3-38452b59df99\n",
            "To: /content/agentoptim.pkl\n",
            "100% 36.4M/36.4M [00:00<00:00, 49.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cce25a33b41e>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1HRwU4u7Y6YjQWmC8xlKrDe4YGcNUWxvQ\n",
            "From (redirected): https://drive.google.com/uc?id=1HRwU4u7Y6YjQWmC8xlKrDe4YGcNUWxvQ&confirm=t&uuid=f9832c0b-41b5-44e4-9005-4a9c85d4cd9e\n",
            "To: /content/buffer512.pkl\n",
            "100% 706M/706M [00:11<00:00, 59.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3 convenc4\n",
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "import pickle\n",
        "# !gdown 19VQp7UjXqH8kJjEPABOTHDV8reg8r7Zn -O buffer512down.pkl # B\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1HRwU4u7Y6YjQWmC8xlKrDe4YGcNUWxvQ -O buffer512.pkl # B\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "f52431e1-b99f-4027-d045-6746c3ac61db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-da06f6ba9e7e>:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "def save(folder, name='agent.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    # agent.mem.save(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer512.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# save(folder, name='agent_jepa753333256.pth')\n",
        "# buffer = load(folder)\n",
        "# save('/content/')\n",
        "# buffer = load('/content/')\n",
        "\n",
        "# name='agent.pth'\n",
        "# print(folder+name)\n",
        "# torch.load(folder+name, map_location='o')\n",
        "# with open(folder+'buffer512down.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "# with open(folder+'buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "optim.load_state_dict(optimsd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG4Wn3c8IN4V"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = buffer[7][80][0]\n",
        "state = transform(state).unsqueeze(0).to(device)#[0]\n",
        "act = agent(state).cpu()[:1].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.normal(mean=50, std=25)\n",
        "# torch.empty(1).uniform_(from=20, to=80)\n",
        "\n",
        "print(type(buffer[6][-1][2]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtUPaTBvYRGC",
        "outputId": "7ff48918-24b1-4f10-912b-b97cf3a2571a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        buffer = self.process(buffer)\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 80):] for episode in cleaned]\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 25 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e3fpbtNOiz1",
        "outputId": "eab2db0d-7e74-4fa3-b949-6dcfcb6b47ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1., -1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.])\n",
            "tensor([ 0., -1.,  0., -1., -1., -1., -1., -1., -1.,  0.])\n",
            "tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0., -1., -1., -1.])\n",
            "tensor([-1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0., -1.])\n",
            "tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
            "tensor([ 0., -1.,  0.,  0.,  0.,  0., -1.,  0., -1., -1.])\n",
            "tensor([ 0., -1.,  0.,  0.])\n",
            "tensor([-0.9924, -0.9929, -0.0030, -0.0045, -0.9961, -0.0020, -0.0063, -0.0030,\n",
            "        -0.0114, -0.0036])\n",
            "tensor([-0.0030, -0.9935, -0.0022, -0.9930, -0.9930, -0.9926, -0.9931, -0.9935,\n",
            "        -0.9943, -0.0058])\n",
            "tensor([-9.3046e-03,  8.6819e-04, -6.0082e-03, -9.5456e-03, -3.0059e-03,\n",
            "        -9.9295e-01, -8.5696e-03, -9.9643e-01, -9.9332e-01, -9.9283e-01])\n",
            "tensor([-0.9936, -0.0015, -0.9947, -0.0082, -0.0190, -0.0037, -0.0053, -0.0040,\n",
            "        -0.0151, -0.9929])\n",
            "tensor([-0.0025, -0.0081, -0.0039, -0.9922, -0.0027, -0.0081, -0.0026, -0.0797,\n",
            "        -0.0018, -0.0164])\n",
            "tensor([-0.0451, -0.9932, -0.0035, -0.0034, -0.0070, -0.0066, -0.9929, -0.0051,\n",
            "        -1.0660, -0.9930])\n",
            "tensor([-0.0030, -0.9934, -0.0039, -0.0043])\n",
            "tensor(0.0002)\n"
          ]
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "train_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "train_data = Datasetme(train_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    train_data=list(zip(state,reward))\n",
        "    train_data = Datasetme(train_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range((len(labels)//10)+1):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    # print(pred)\n",
        "    for x in range((len(pred)//10)+1):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(F.mse_loss(labels, pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "OksdjCeJYpYh",
        "outputId": "9ba92230-f9ed-4704-ac23-ca67aad98fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n",
            "<ipython-input-66-2e41fcea5594>:174: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repr, std, cov, closslb 0.043924480676651 0.4755859375 0.0001367940567433834 0.014443311840295792 6.589589611394331e-05\n",
            "0.06733634134735009 0.03417674375677684 1.0\n",
            "repr, std, cov, closslb 0.042442698031663895 0.474853515625 0.00015151361003518105 0.015571567229926586 0.015444489195942879\n",
            "0.06808075573369342 0.03504149441449154 1.0\n",
            "repr, std, cov, closslb 0.049822863191366196 0.47509765625 0.00015119928866624832 0.016875047236680984 0.015681009739637375\n",
            "0.06876463510904313 0.035928125263758104 1.0\n",
            "repr, std, cov, closslb 0.04448898136615753 0.47509765625 0.00014981813728809357 0.02955315262079239 0.004061547573655844\n",
            "0.06945538413492752 0.03683718992402522 1.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-5059a2eb0b0b>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# agent.train_ae(train_loader, optim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# agent.train_jepa(train_loader, optim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_jepa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# state = buffer[7][80][0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-2e41fcea5594>\u001b[0m in \u001b[0;36mtrain_jepa\u001b[0;34m(self, dataloader, c_loader, optim, bptt)\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjepa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msy_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                     \u001b[0msyaz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msy_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     \u001b[0;31m# sy_ = self.jepa.pred(syaz)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2ce37b725f67>\u001b[0m in \u001b[0;36margm\u001b[0;34m(self, sx, a, sy, lr, h0)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m# sy_ = self.pred(sxaz)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0msy_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msxaz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msy_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m   1140\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m   1141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "for i in range(30):\n",
        "    print(i)\n",
        "    # agent.train_ae(train_loader, optim)\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "# 10 epochs 15m23s\n",
        "\n",
        "\n",
        "\n",
        "# loss 0.00027325598057359457\n",
        "# loss 0.00027538512949831784\n",
        "# loss 0.000279315427178517\n",
        "# loss 0.00028544830274768174\n",
        "# loss 0.00029633755912072957\n",
        "# loss 0.0002964686427731067\n",
        "# loss 0.00030574199627153575\n",
        "# loss 0.00031030713580548763\n",
        "# loss 0.00011697990703396499\n",
        "# loss 0.00012466282350942492\n",
        "\n",
        "# loss 0.0002805441035889089\n",
        "# loss 0.0002813159371726215\n",
        "# loss 0.00028616547933779657\n",
        "# loss 0.00029815093148499727\n",
        "# loss 0.0003055527340620756\n",
        "# loss 0.0002878434315789491\n",
        "# loss 0.0002965773455798626\n",
        "# loss 0.00030164531199261546\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "f8d556e2-87df-45ff-82eb-b2281486d6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PraFUAPB3j7v",
        "outputId": "9e7efd1a-6614-47f1-9a35-7ea761a00a1f",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: early reset ignored\n",
            "search tensor([[[-0.1009,  0.0843,  0.4584],\n",
            "         [ 0.0940,  0.5125, -0.3909],\n",
            "         [ 0.1004,  0.5048, -0.0980],\n",
            "         [-0.0174, -0.0566,  0.1924],\n",
            "         [ 0.2328, -0.5044, -0.3249],\n",
            "         [-0.2888, -0.4658,  0.4933]]], device='cuda:0') tensor([[[-0.0129],\n",
            "         [ 0.7154],\n",
            "         [-0.3455],\n",
            "         [ 0.3087],\n",
            "         [-0.0574],\n",
            "         [-0.3163]]], device='cuda:0')\n",
            "tcost icost 0.048980712890625 0.0\n",
            "tcost icost 0.057586669921875 0.0\n",
            "tcost icost 0.062164306640625 0.0\n",
            "tcost icost 0.056182861328125 0.0\n",
            "tcost icost 0.0374755859375 0.0\n",
            "tcost icost 0.01025390625 0.0\n",
            "loss tensor([[0.2228]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-2.0080e-01, -1.5740e-02,  3.5796e-01],\n",
            "         [-6.0450e-03,  4.1198e-01, -4.9055e-01],\n",
            "         [ 3.2811e-04,  4.0433e-01, -1.9792e-01],\n",
            "         [-1.1737e-01, -1.5659e-01,  9.2247e-02],\n",
            "         [ 1.3253e-01, -6.0388e-01, -4.2453e-01],\n",
            "         [-3.8852e-01, -5.6537e-01,  3.9285e-01]]], device='cuda:0') tensor([-0.0127,  0.7173, -0.3441,  0.3096, -0.0571, -0.3163], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2227783203125\n",
            "tcost icost 0.0489501953125 0.0\n",
            "tcost icost 0.05755615234375 0.0\n",
            "tcost icost 0.0621337890625 0.0\n",
            "tcost icost 0.056182861328125 0.0\n",
            "tcost icost 0.037445068359375 0.0\n",
            "tcost icost 0.01021575927734375 0.0\n",
            "loss tensor([[0.2224]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.3006, -0.1156,  0.2576],\n",
            "         [-0.1058,  0.3116, -0.5901],\n",
            "         [-0.0997,  0.3040, -0.2977],\n",
            "         [-0.2173, -0.2563, -0.0078],\n",
            "         [ 0.0326, -0.7026, -0.5240],\n",
            "         [-0.4877, -0.6646,  0.2929]]], device='cuda:0') tensor([-0.0126,  0.7193, -0.3426,  0.3106, -0.0569, -0.3163], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.222412109375\n",
            "tcost icost 0.048980712890625 0.0\n",
            "tcost icost 0.0570068359375 0.0\n",
            "tcost icost 0.06207275390625 0.0\n",
            "tcost icost 0.054595947265625 0.0\n",
            "tcost icost 0.035888671875 0.0\n",
            "tcost icost 0.00539398193359375 0.0\n",
            "loss tensor([[0.2172]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.3957, -0.2108,  0.1572],\n",
            "         [-0.1972,  0.2128, -0.6896],\n",
            "         [-0.1845,  0.2061, -0.3974],\n",
            "         [-0.3075, -0.3527, -0.1075],\n",
            "         [-0.0587, -0.7846, -0.6172],\n",
            "         [-0.5833, -0.7607,  0.1930]]], device='cuda:0') tensor([-0.0129,  0.7214, -0.3409,  0.3119, -0.0563, -0.3163], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2171630859375\n",
            "tcost icost 0.046630859375 0.0\n",
            "tcost icost 0.05401611328125 0.0\n",
            "tcost icost 0.0595703125 0.0\n",
            "tcost icost 0.053070068359375 0.0\n",
            "tcost icost 0.0347900390625 0.0\n",
            "tcost icost 0.004428863525390625 0.0\n",
            "loss tensor([[0.2075]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.4874, -0.3048,  0.0571],\n",
            "         [-0.2721,  0.1142, -0.7888],\n",
            "         [-0.2446,  0.1088, -0.4967],\n",
            "         [-0.3843, -0.4463, -0.2065],\n",
            "         [-0.1208, -0.7659, -0.6315],\n",
            "         [-0.6164, -0.7824,  0.0891]]], device='cuda:0') tensor([-0.0124,  0.7234, -0.3395,  0.3130, -0.0559, -0.3162], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.20751953125\n",
            "tcost icost 0.046630859375 0.0\n",
            "tcost icost 0.05401611328125 0.0\n",
            "tcost icost 0.0595703125 0.0\n",
            "tcost icost 0.0531005859375 0.0\n",
            "tcost icost 0.034759521484375 0.0\n",
            "tcost icost 0.004436492919921875 0.0\n",
            "loss tensor([[0.2075]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.5791, -0.3969, -0.0430],\n",
            "         [-0.3227,  0.0160, -0.8880],\n",
            "         [-0.2827,  0.0121, -0.5958],\n",
            "         [-0.4532, -0.5360, -0.3048],\n",
            "         [-0.1668, -0.7467, -0.6439],\n",
            "         [-0.6266, -0.7793, -0.0038]]], device='cuda:0') tensor([-0.0119,  0.7255, -0.3380,  0.3140, -0.0554, -0.3161], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.20751953125\n",
            "tcost icost 0.046630859375 0.0\n",
            "tcost icost 0.05401611328125 0.0\n",
            "tcost icost 0.0595703125 0.0\n",
            "tcost icost 0.05303955078125 0.0\n",
            "tcost icost 0.034820556640625 0.0\n",
            "tcost icost 0.004497528076171875 0.0\n",
            "loss tensor([[0.2076]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.6711, -0.4846, -0.1430],\n",
            "         [-0.3252, -0.0778, -0.9424],\n",
            "         [-0.2977, -0.0838, -0.6947],\n",
            "         [-0.5132, -0.6196, -0.4021],\n",
            "         [-0.2003, -0.7286, -0.6550],\n",
            "         [-0.6309, -0.7712, -0.0850]]], device='cuda:0') tensor([-0.0114,  0.7276, -0.3366,  0.3151, -0.0550, -0.3161], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2076416015625\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.053985595703125 0.0\n",
            "tcost icost 0.059600830078125 0.0\n",
            "tcost icost 0.0531005859375 0.0\n",
            "tcost icost 0.0347900390625 0.0\n",
            "tcost icost 0.004421234130859375 0.0\n",
            "loss tensor([[0.2076]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.7568, -0.5585, -0.2426],\n",
            "         [-0.2842, -0.1583, -0.9456],\n",
            "         [-0.2859, -0.1785, -0.7936],\n",
            "         [-0.5453, -0.6818, -0.4876],\n",
            "         [-0.2229, -0.7124, -0.6654],\n",
            "         [-0.6315, -0.7595, -0.1559]]], device='cuda:0') tensor([-0.0108,  0.7298, -0.3351,  0.3162, -0.0545, -0.3160], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2076416015625\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.05401611328125 0.0\n",
            "tcost icost 0.059539794921875 0.0\n",
            "tcost icost 0.05303955078125 0.0\n",
            "tcost icost 0.034820556640625 0.0\n",
            "tcost icost 0.00441741943359375 0.0\n",
            "loss tensor([[0.2075]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.7612, -0.5676, -0.3137],\n",
            "         [-0.2289, -0.2296, -0.9460],\n",
            "         [-0.2523, -0.2718, -0.8927],\n",
            "         [-0.5153, -0.6777, -0.5246],\n",
            "         [-0.2379, -0.6984, -0.6750],\n",
            "         [-0.6298, -0.7456, -0.2176]]], device='cuda:0') tensor([-0.0103,  0.7319, -0.3336,  0.3173, -0.0541, -0.3159], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.20751953125\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.05401611328125 0.0\n",
            "tcost icost 0.0595703125 0.0\n",
            "tcost icost 0.0531005859375 0.0\n",
            "tcost icost 0.034759521484375 0.0\n",
            "tcost icost 0.004436492919921875 0.0\n",
            "loss tensor([[0.2075]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.7421, -0.5571, -0.3727],\n",
            "         [-0.1657, -0.2914, -0.9421],\n",
            "         [-0.1872, -0.3376, -0.9225],\n",
            "         [-0.4785, -0.6751, -0.5615],\n",
            "         [-0.2466, -0.6867, -0.6838],\n",
            "         [-0.6268, -0.7304, -0.2714]]], device='cuda:0') tensor([-0.0098,  0.7340, -0.3322,  0.3184, -0.0536, -0.3159], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.20751953125\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.05401611328125 0.0\n",
            "tcost icost 0.0595703125 0.0\n",
            "tcost icost 0.053070068359375 0.0\n",
            "tcost icost 0.034820556640625 0.0\n",
            "tcost icost 0.004436492919921875 0.0\n",
            "loss tensor([[0.2075]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.7210, -0.5443, -0.4288],\n",
            "         [-0.0996, -0.3436, -0.9338],\n",
            "         [-0.1120, -0.3827, -0.9171],\n",
            "         [-0.4353, -0.6732, -0.5977],\n",
            "         [-0.2503, -0.6772, -0.6919],\n",
            "         [-0.6232, -0.7145, -0.3180]]], device='cuda:0') tensor([-0.0093,  0.7360, -0.3307,  0.3195, -0.0532, -0.3158], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.20751953125\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.0509033203125 0.0\n",
            "tcost icost 0.05499267578125 0.0\n",
            "tcost icost 0.049224853515625 0.0\n",
            "tcost icost 0.034698486328125 0.0\n",
            "tcost icost 0.004375457763671875 0.0\n",
            "loss tensor([[0.1981]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.7046, -0.5263, -0.4759],\n",
            "         [-0.0396, -0.3865, -0.9214],\n",
            "         [-0.0415, -0.4190, -0.9071],\n",
            "         [-0.3989, -0.6685, -0.6277],\n",
            "         [-0.2566, -0.6699, -0.6967],\n",
            "         [-0.6221, -0.6975, -0.3556]]], device='cuda:0') tensor([-0.0094,  0.7380, -0.3296,  0.3203, -0.0530, -0.3160], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1981201171875\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.0509033203125 0.0\n",
            "tcost icost 0.051544189453125 0.0\n",
            "tcost icost 0.0457763671875 0.0\n",
            "tcost icost 0.033660888671875 0.0\n",
            "tcost icost 0.004291534423828125 0.0\n",
            "loss tensor([[0.1921]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.6915, -0.5075, -0.5141],\n",
            "         [ 0.0107, -0.4215, -0.9068],\n",
            "         [ 0.0199, -0.4475, -0.8941],\n",
            "         [-0.3725, -0.6612, -0.6512],\n",
            "         [-0.2682, -0.6644, -0.6976],\n",
            "         [-0.6241, -0.6812, -0.3828]]], device='cuda:0') tensor([-0.0095,  0.7400, -0.3281,  0.3212, -0.0527, -0.3160], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.192138671875\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.0509033203125 0.0\n",
            "tcost icost 0.051544189453125 0.0\n",
            "tcost icost 0.0457763671875 0.0\n",
            "tcost icost 0.033660888671875 0.0\n",
            "tcost icost 0.004238128662109375 0.0\n",
            "loss tensor([[0.1921]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.6813, -0.4895, -0.5442],\n",
            "         [ 0.0473, -0.4500, -0.8918],\n",
            "         [ 0.0691, -0.4697, -0.8801],\n",
            "         [-0.3563, -0.6524, -0.6689],\n",
            "         [-0.2835, -0.6603, -0.6955],\n",
            "         [-0.6285, -0.6662, -0.4015]]], device='cuda:0') tensor([-0.0095,  0.7420, -0.3265,  0.3222, -0.0525, -0.3161], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.192138671875\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.0509033203125 0.0\n",
            "tcost icost 0.051513671875 0.0\n",
            "tcost icost 0.045745849609375 0.0\n",
            "tcost icost 0.03369140625 0.0\n",
            "tcost icost 0.004283905029296875 0.0\n",
            "loss tensor([[0.1921]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.6726, -0.4746, -0.5678],\n",
            "         [ 0.0731, -0.4730, -0.8780],\n",
            "         [ 0.1084, -0.4869, -0.8667],\n",
            "         [-0.3460, -0.6435, -0.6828],\n",
            "         [-0.3011, -0.6570, -0.6911],\n",
            "         [-0.6349, -0.6529, -0.4130]]], device='cuda:0') tensor([-0.0096,  0.7440, -0.3251,  0.3231, -0.0522, -0.3161], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.192138671875\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.0509033203125 0.0\n",
            "tcost icost 0.051544189453125 0.0\n",
            "tcost icost 0.045745849609375 0.0\n",
            "tcost icost 0.033660888671875 0.0\n",
            "tcost icost 0.004268646240234375 0.0\n",
            "loss tensor([[0.1921]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.6656, -0.4629, -0.5855],\n",
            "         [ 0.0854, -0.4921, -0.8664],\n",
            "         [ 0.1350, -0.5005, -0.8551],\n",
            "         [-0.3423, -0.6346, -0.6929],\n",
            "         [-0.3200, -0.6545, -0.6850],\n",
            "         [-0.6427, -0.6415, -0.4187]]], device='cuda:0') tensor([-0.0096,  0.7460, -0.3235,  0.3240, -0.0519, -0.3162], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.192138671875\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.0509033203125 0.0\n",
            "tcost icost 0.051544189453125 0.0\n",
            "tcost icost 0.04571533203125 0.0\n",
            "tcost icost 0.03369140625 0.0\n",
            "tcost icost 0.00421905517578125 0.0\n",
            "loss tensor([[0.1920]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.6591, -0.4551, -0.5988],\n",
            "         [ 0.0885, -0.5077, -0.8570],\n",
            "         [ 0.1526, -0.5115, -0.8456],\n",
            "         [-0.3417, -0.6265, -0.7005],\n",
            "         [-0.3395, -0.6523, -0.6777],\n",
            "         [-0.6516, -0.6320, -0.4196]]], device='cuda:0') tensor([-0.0097,  0.7480, -0.3221,  0.3250, -0.0516, -0.3163], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1920166015625\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.0509033203125 0.0\n",
            "tcost icost 0.05157470703125 0.0\n",
            "tcost icost 0.0457763671875 0.0\n",
            "tcost icost 0.033660888671875 0.0\n",
            "tcost icost 0.004261016845703125 0.0\n",
            "loss tensor([[0.1921]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.6536, -0.4506, -0.6081],\n",
            "         [ 0.0807, -0.5208, -0.8499],\n",
            "         [ 0.1590, -0.5208, -0.8388],\n",
            "         [-0.3451, -0.6190, -0.7055],\n",
            "         [-0.3591, -0.6504, -0.6693],\n",
            "         [-0.6611, -0.6242, -0.4164]]], device='cuda:0') tensor([-0.0097,  0.7500, -0.3205,  0.3259, -0.0513, -0.3163], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.192138671875\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.050872802734375 0.0\n",
            "tcost icost 0.05157470703125 0.0\n",
            "tcost icost 0.045745849609375 0.0\n",
            "tcost icost 0.03369140625 0.0\n",
            "tcost icost 0.004245758056640625 0.0\n",
            "loss tensor([[0.1921]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.6483, -0.4494, -0.6147],\n",
            "         [ 0.0670, -0.5315, -0.8444],\n",
            "         [ 0.1586, -0.5285, -0.8340],\n",
            "         [-0.3500, -0.6125, -0.7088],\n",
            "         [-0.3784, -0.6487, -0.6603],\n",
            "         [-0.6710, -0.6179, -0.4099]]], device='cuda:0') tensor([-0.0098,  0.7520, -0.3191,  0.3268, -0.0510, -0.3164], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.192138671875\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.050872802734375 0.0\n",
            "tcost icost 0.051605224609375 0.0\n",
            "tcost icost 0.045806884765625 0.0\n",
            "tcost icost 0.033660888671875 0.0\n",
            "tcost icost 0.0041961669921875 0.0\n",
            "loss tensor([[0.1923]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.6432, -0.4507, -0.6190],\n",
            "         [ 0.0485, -0.5400, -0.8403],\n",
            "         [ 0.1525, -0.5350, -0.8310],\n",
            "         [-0.3558, -0.6070, -0.7106],\n",
            "         [-0.3972, -0.6470, -0.6509],\n",
            "         [-0.6809, -0.6129, -0.4008]]], device='cuda:0') tensor([-0.0099,  0.7540, -0.3176,  0.3277, -0.0507, -0.3165], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1922607421875\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.0509033203125 0.0\n",
            "tcost icost 0.051605224609375 0.0\n",
            "tcost icost 0.045745849609375 0.0\n",
            "tcost icost 0.033660888671875 0.0\n",
            "tcost icost 0.0042877197265625 0.0\n",
            "loss tensor([[0.1923]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.6384, -0.4540, -0.6216],\n",
            "         [ 0.0266, -0.5466, -0.8370],\n",
            "         [ 0.1417, -0.5404, -0.8294],\n",
            "         [-0.3624, -0.6025, -0.7111],\n",
            "         [-0.4152, -0.6454, -0.6412],\n",
            "         [-0.6907, -0.6091, -0.3897]]], device='cuda:0') tensor([-0.0100,  0.7559, -0.3161,  0.3287, -0.0505, -0.3165], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1922607421875\n",
            "search tensor([[[-0.4152, -0.6454, -0.6412],\n",
            "         [-0.6907, -0.6091, -0.3897],\n",
            "         [ 0.2710, -0.2626, -0.3605],\n",
            "         [-0.4958, -0.2008,  0.3296],\n",
            "         [ 0.5158,  0.1857,  0.5163],\n",
            "         [ 0.4298, -0.1263, -0.3197]]], device='cuda:0') tensor([[[-0.0505],\n",
            "         [-0.3165],\n",
            "         [-0.1804],\n",
            "         [ 1.2503],\n",
            "         [ 0.2108],\n",
            "         [-0.2692]]], device='cuda:0')\n",
            "tcost icost 0.0418701171875 0.0\n",
            "tcost icost 0.04742431640625 0.0\n",
            "tcost icost 0.04742431640625 0.0\n",
            "tcost icost 0.045684814453125 0.0\n",
            "tcost icost 0.041412353515625 0.0\n",
            "tcost icost 0.048492431640625 0.0\n",
            "loss tensor([[0.2123]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.3148, -0.5447, -0.7406],\n",
            "         [-0.7458, -0.4800, -0.4619],\n",
            "         [ 0.1707, -0.3624, -0.4602],\n",
            "         [-0.5953, -0.3006,  0.2293],\n",
            "         [ 0.4152,  0.0855,  0.4158],\n",
            "         [ 0.3294, -0.2260, -0.4194]]], device='cuda:0') tensor([-0.0499, -0.3168, -0.1817,  1.0000,  0.2097, -0.2697], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2122802734375\n",
            "tcost icost 0.0418701171875 0.0\n",
            "tcost icost 0.04742431640625 0.0\n",
            "tcost icost 0.047393798828125 0.0\n",
            "tcost icost 0.044036865234375 0.0\n",
            "tcost icost 0.0389404296875 0.0\n",
            "tcost icost 0.045684814453125 0.0\n",
            "loss tensor([[0.2075]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.2328, -0.5238, -0.8194],\n",
            "         [-0.7763, -0.3610, -0.5167],\n",
            "         [ 0.0707, -0.4604, -0.5597],\n",
            "         [-0.6946, -0.3994,  0.1291],\n",
            "         [ 0.3159, -0.0145,  0.3155],\n",
            "         [ 0.2379, -0.3254, -0.5187]]], device='cuda:0') tensor([-0.0493, -0.3173, -0.1831,  0.9986,  0.2083, -0.2703], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.20751953125\n",
            "tcost icost 0.0418701171875 0.0\n",
            "tcost icost 0.047393798828125 0.0\n",
            "tcost icost 0.04644775390625 0.0\n",
            "tcost icost 0.041839599609375 0.0\n",
            "tcost icost 0.037506103515625 0.0\n",
            "tcost icost 0.04498291015625 0.0\n",
            "loss tensor([[0.2040]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.1473, -0.5118, -0.8464],\n",
            "         [-0.7648, -0.2915, -0.5746],\n",
            "         [-0.0225, -0.5435, -0.6591],\n",
            "         [-0.7907, -0.4884,  0.0292],\n",
            "         [ 0.2229, -0.1136,  0.2157],\n",
            "         [ 0.1593, -0.4145, -0.6169]]], device='cuda:0') tensor([-0.0485, -0.3172, -0.1839,  0.9977,  0.2071, -0.2709], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2039794921875\n",
            "tcost icost 0.0418701171875 0.0\n",
            "tcost icost 0.04742431640625 0.0\n",
            "tcost icost 0.04644775390625 0.0\n",
            "tcost icost 0.0418701171875 0.0\n",
            "tcost icost 0.037567138671875 0.0\n",
            "tcost icost 0.04498291015625 0.0\n",
            "loss tensor([[0.2040]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.0589, -0.5107, -0.8577],\n",
            "         [-0.7280, -0.2646, -0.6324],\n",
            "         [-0.1075, -0.5963, -0.7583],\n",
            "         [-0.8476, -0.5264, -0.0675],\n",
            "         [ 0.1332, -0.2085,  0.1164],\n",
            "         [ 0.0926, -0.4814, -0.7151]]], device='cuda:0') tensor([-0.0476, -0.3171, -0.1846,  0.9967,  0.2059, -0.2715], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2039794921875\n",
            "tcost icost 0.0418701171875 0.0\n",
            "tcost icost 0.04742431640625 0.0\n",
            "tcost icost 0.04644775390625 0.0\n",
            "tcost icost 0.0418701171875 0.0\n",
            "tcost icost 0.03753662109375 0.0\n",
            "tcost icost 0.044769287109375 0.0\n",
            "loss tensor([[0.2039]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.0256, -0.5154, -0.8566],\n",
            "         [-0.6745, -0.2663, -0.6886],\n",
            "         [-0.1690, -0.5740, -0.8012],\n",
            "         [-0.8522, -0.5009, -0.1516],\n",
            "         [ 0.0428, -0.2980,  0.0174],\n",
            "         [ 0.0293, -0.5195, -0.8136]]], device='cuda:0') tensor([-0.0469, -0.3172, -0.1854,  0.9957,  0.2046, -0.2722], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.203857421875\n",
            "tcost icost 0.038665771484375 0.0\n",
            "tcost icost 0.042510986328125 0.0\n",
            "tcost icost 0.040069580078125 0.0\n",
            "tcost icost 0.035125732421875 0.0\n",
            "tcost icost 0.0308990478515625 0.0\n",
            "tcost icost 0.043121337890625 0.0\n",
            "loss tensor([[0.1808]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.1027, -0.5223, -0.8465],\n",
            "         [-0.6013, -0.2907, -0.7443],\n",
            "         [-0.2007, -0.5383, -0.8185],\n",
            "         [-0.8555, -0.4642, -0.2292],\n",
            "         [-0.0411, -0.3515, -0.0806],\n",
            "         [-0.0052, -0.4684, -0.8835]]], device='cuda:0') tensor([-0.0449, -0.3170, -0.1860,  0.9949,  0.2036, -0.2728], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1807861328125\n",
            "tcost icost 0.038665771484375 0.0\n",
            "tcost icost 0.042510986328125 0.0\n",
            "tcost icost 0.040069580078125 0.0\n",
            "tcost icost 0.035125732421875 0.0\n",
            "tcost icost 0.0308990478515625 0.0\n",
            "tcost icost 0.041473388671875 0.0\n",
            "loss tensor([[0.1798]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.1718, -0.5282, -0.8316],\n",
            "         [-0.5139, -0.3240, -0.7943],\n",
            "         [-0.2107, -0.5058, -0.8365],\n",
            "         [-0.8561, -0.4190, -0.3027],\n",
            "         [-0.1208, -0.3672, -0.1782],\n",
            "         [-0.0039, -0.3865, -0.9223]]], device='cuda:0') tensor([-0.0428, -0.3168, -0.1866,  0.9941,  0.2025, -0.2734], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1798095703125\n",
            "tcost icost 0.038665771484375 0.0\n",
            "tcost icost 0.042510986328125 0.0\n",
            "tcost icost 0.04010009765625 0.0\n",
            "tcost icost 0.035125732421875 0.0\n",
            "tcost icost 0.0306243896484375 0.0\n",
            "tcost icost 0.041290283203125 0.0\n",
            "loss tensor([[0.1796]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.2331, -0.5294, -0.8157],\n",
            "         [-0.4172, -0.3586, -0.8351],\n",
            "         [-0.2000, -0.4748, -0.8571],\n",
            "         [-0.8502, -0.3706, -0.3738],\n",
            "         [-0.1980, -0.3546, -0.2759],\n",
            "         [ 0.0144, -0.2977, -0.9546]]], device='cuda:0') tensor([-0.0407, -0.3168, -0.1875,  0.9930,  0.2013, -0.2740], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1795654296875\n",
            "tcost icost 0.0386962890625 0.0\n",
            "tcost icost 0.04254150390625 0.0\n",
            "tcost icost 0.040069580078125 0.0\n",
            "tcost icost 0.03509521484375 0.0\n",
            "tcost icost 0.0306396484375 0.0\n",
            "tcost icost 0.041290283203125 0.0\n",
            "loss tensor([[0.1797]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.2868, -0.5271, -0.8000],\n",
            "         [-0.3163, -0.3908, -0.8644],\n",
            "         [-0.1730, -0.4479, -0.8772],\n",
            "         [-0.8360, -0.3260, -0.4413],\n",
            "         [-0.2730, -0.3243, -0.3735],\n",
            "         [ 0.0445, -0.2071, -0.9773]]], device='cuda:0') tensor([-0.0386, -0.3168, -0.1884,  0.9918,  0.2000, -0.2746], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1796875\n",
            "tcost icost 0.038665771484375 0.0\n",
            "tcost icost 0.04254150390625 0.0\n",
            "tcost icost 0.040069580078125 0.0\n",
            "tcost icost 0.035125732421875 0.0\n",
            "tcost icost 0.030609130859375 0.0\n",
            "tcost icost 0.041290283203125 0.0\n",
            "loss tensor([[0.1794]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.3330, -0.5229, -0.7846],\n",
            "         [-0.2156, -0.4184, -0.8823],\n",
            "         [-0.1341, -0.4275, -0.8940],\n",
            "         [-0.8137, -0.2894, -0.5041],\n",
            "         [-0.3454, -0.2824, -0.4710],\n",
            "         [ 0.0814, -0.1194, -0.9895]]], device='cuda:0') tensor([-0.0365, -0.3169, -0.1894,  0.9907,  0.1988, -0.2752], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.179443359375\n",
            "tcost icost 0.038665771484375 0.0\n",
            "tcost icost 0.04254150390625 0.0\n",
            "tcost icost 0.040069580078125 0.0\n",
            "tcost icost 0.035125732421875 0.0\n",
            "tcost icost 0.030609130859375 0.0\n",
            "tcost icost 0.04132080078125 0.0\n",
            "loss tensor([[0.1794]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.3721, -0.5185, -0.7699],\n",
            "         [-0.1185, -0.4402, -0.8900],\n",
            "         [-0.0867, -0.4145, -0.9059],\n",
            "         [-0.7843, -0.2621, -0.5622],\n",
            "         [-0.4130, -0.2327, -0.5682],\n",
            "         [ 0.1205, -0.0390, -0.9919]]], device='cuda:0') tensor([-0.0345, -0.3169, -0.1903,  0.9896,  0.1976, -0.2758], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.179443359375\n",
            "tcost icost 0.038665771484375 0.0\n",
            "tcost icost 0.04254150390625 0.0\n",
            "tcost icost 0.040069580078125 0.0\n",
            "tcost icost 0.03515625 0.0\n",
            "tcost icost 0.0306549072265625 0.0\n",
            "tcost icost 0.04132080078125 0.0\n",
            "loss tensor([[0.1796]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.4045, -0.5149, -0.7558],\n",
            "         [-0.0273, -0.4560, -0.8896],\n",
            "         [-0.0345, -0.4085, -0.9121],\n",
            "         [-0.7490, -0.2439, -0.6160],\n",
            "         [-0.4719, -0.1775, -0.6652],\n",
            "         [ 0.1584,  0.0311, -0.9869]]], device='cuda:0') tensor([-0.0324, -0.3169, -0.1912,  0.9884,  0.1963, -0.2765], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1795654296875\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost 0.0445556640625 0.0\n",
            "tcost icost 0.0400390625 0.0\n",
            "tcost icost 0.033599853515625 0.0\n",
            "tcost icost 0.0285797119140625 0.0\n",
            "tcost icost 0.035491943359375 0.0\n",
            "loss tensor([[0.1791]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.4308, -0.5048, -0.7481],\n",
            "         [ 0.0560, -0.4638, -0.8842],\n",
            "         [ 0.0176, -0.4090, -0.9123],\n",
            "         [-0.7118, -0.2358, -0.6616],\n",
            "         [-0.5320, -0.1278, -0.7604],\n",
            "         [ 0.1972,  0.0871, -0.9765]]], device='cuda:0') tensor([-0.0319, -0.3167, -0.1919,  0.9874,  0.1955, -0.2769], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1790771484375\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost 0.044525146484375 0.0\n",
            "tcost icost 0.0400390625 0.0\n",
            "tcost icost 0.0335693359375 0.0\n",
            "tcost icost 0.02850341796875 0.0\n",
            "tcost icost 0.03546142578125 0.0\n",
            "loss tensor([[0.1787]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.4510, -0.4922, -0.7446],\n",
            "         [ 0.1307, -0.4647, -0.8757],\n",
            "         [ 0.0678, -0.4141, -0.9077],\n",
            "         [-0.6737, -0.2357, -0.7004],\n",
            "         [-0.5667, -0.0802, -0.8200],\n",
            "         [ 0.2354,  0.1302, -0.9631]]], device='cuda:0') tensor([-0.0313, -0.3166, -0.1926,  0.9864,  0.1946, -0.2772], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1787109375\n",
            "tcost icost 0.042205810546875 0.0\n",
            "tcost icost 0.0445556640625 0.0\n",
            "tcost icost 0.0400390625 0.0\n",
            "tcost icost 0.033599853515625 0.0\n",
            "tcost icost 0.0285797119140625 0.0\n",
            "tcost icost 0.03546142578125 0.0\n",
            "loss tensor([[0.1790]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.4652, -0.4803, -0.7436],\n",
            "         [ 0.1964, -0.4605, -0.8657],\n",
            "         [ 0.1145, -0.4219, -0.8994],\n",
            "         [-0.6352, -0.2419, -0.7335],\n",
            "         [-0.5624, -0.0373, -0.8260],\n",
            "         [ 0.2718,  0.1621, -0.9486]]], device='cuda:0') tensor([-0.0308, -0.3164, -0.1933,  0.9854,  0.1938, -0.2776], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.178955078125\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost 0.0445556640625 0.0\n",
            "tcost icost 0.0400390625 0.0\n",
            "tcost icost 0.033599853515625 0.0\n",
            "tcost icost 0.028564453125 0.0\n",
            "tcost icost 0.035491943359375 0.0\n",
            "loss tensor([[0.1790]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.4737, -0.4715, -0.7438],\n",
            "         [ 0.2528, -0.4528, -0.8550],\n",
            "         [ 0.1565, -0.4311, -0.8886],\n",
            "         [-0.5963, -0.2526, -0.7620],\n",
            "         [-0.5561, -0.0035, -0.8311],\n",
            "         [ 0.3059,  0.1846, -0.9340]]], device='cuda:0') tensor([-0.0303, -0.3162, -0.1940,  0.9843,  0.1929, -0.2780], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.178955078125\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost 0.0445556640625 0.0\n",
            "tcost icost 0.0400390625 0.0\n",
            "tcost icost 0.0335693359375 0.0\n",
            "tcost icost 0.0285491943359375 0.0\n",
            "tcost icost 0.0355224609375 0.0\n",
            "loss tensor([[0.1790]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.4771, -0.4666, -0.7447],\n",
            "         [ 0.2999, -0.4436, -0.8445],\n",
            "         [ 0.1930, -0.4406, -0.8767],\n",
            "         [-0.5571, -0.2665, -0.7865],\n",
            "         [-0.5485,  0.0221, -0.8359],\n",
            "         [ 0.3374,  0.1995, -0.9200]]], device='cuda:0') tensor([-0.0298, -0.3161, -0.1947,  0.9833,  0.1921, -0.2784], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.178955078125\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost 0.0445556640625 0.0\n",
            "tcost icost 0.0400390625 0.0\n",
            "tcost icost 0.0335693359375 0.0\n",
            "tcost icost 0.0285491943359375 0.0\n",
            "tcost icost 0.035491943359375 0.0\n",
            "loss tensor([[0.1790]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.4762, -0.4659, -0.7458],\n",
            "         [ 0.3379, -0.4344, -0.8349],\n",
            "         [ 0.2236, -0.4497, -0.8647],\n",
            "         [-0.5173, -0.2823, -0.8079],\n",
            "         [-0.5399,  0.0403, -0.8407],\n",
            "         [ 0.3661,  0.2083, -0.9070]]], device='cuda:0') tensor([-0.0293, -0.3159, -0.1954,  0.9823,  0.1913, -0.2788], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.178955078125\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost 0.0445556640625 0.0\n",
            "tcost icost 0.0400390625 0.0\n",
            "tcost icost 0.0335693359375 0.0\n",
            "tcost icost 0.0285186767578125 0.0\n",
            "tcost icost 0.0355224609375 0.0\n",
            "loss tensor([[0.1788]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.4717, -0.4683, -0.7471],\n",
            "         [ 0.3673, -0.4266, -0.8265],\n",
            "         [ 0.2482, -0.4581, -0.8535],\n",
            "         [-0.4767, -0.2990, -0.8267],\n",
            "         [-0.5308,  0.0521, -0.8459],\n",
            "         [ 0.3921,  0.2122, -0.8951]]], device='cuda:0') tensor([-0.0288, -0.3157, -0.1961,  0.9813,  0.1904, -0.2792], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1788330078125\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost 0.0445556640625 0.0\n",
            "tcost icost 0.0400390625 0.0\n",
            "tcost icost 0.033599853515625 0.0\n",
            "tcost icost 0.0285491943359375 0.0\n",
            "tcost icost 0.0355224609375 0.0\n",
            "loss tensor([[0.1790]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.4648, -0.4730, -0.7485],\n",
            "         [ 0.3885, -0.4209, -0.8197],\n",
            "         [ 0.2668, -0.4656, -0.8438],\n",
            "         [-0.4351, -0.3157, -0.8432],\n",
            "         [-0.5214,  0.0582, -0.8513],\n",
            "         [ 0.4154,  0.2124, -0.8845]]], device='cuda:0') tensor([-0.0283, -0.3156, -0.1968,  0.9802,  0.1896, -0.2796], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.178955078125\n",
            "search tensor([[[-0.5214,  0.0582, -0.8513],\n",
            "         [ 0.4154,  0.2124, -0.8845],\n",
            "         [-0.2911,  0.3180,  0.4055],\n",
            "         [-0.2176,  0.5065,  0.3058],\n",
            "         [ 0.3446,  0.2553, -0.1016],\n",
            "         [-0.4589,  0.4424,  0.3616]]], device='cuda:0') tensor([[[ 0.1896],\n",
            "         [-0.2796],\n",
            "         [-0.2332],\n",
            "         [-0.2364],\n",
            "         [-0.4416],\n",
            "         [ 0.5580]]], device='cuda:0')\n",
            "tcost icost 0.050750732421875 0.0\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.04620361328125 0.0\n",
            "tcost icost 0.045867919921875 0.0\n",
            "tcost icost 0.051666259765625 0.0\n",
            "tcost icost 0.06756591796875 0.0\n",
            "loss tensor([[0.2374]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.4003,  0.1504, -0.9040],\n",
            "         [ 0.4615,  0.1006, -0.8814],\n",
            "         [-0.1908,  0.2177,  0.3051],\n",
            "         [-0.1174,  0.4060,  0.2055],\n",
            "         [ 0.2443,  0.1551, -0.2015],\n",
            "         [-0.5583,  0.3420,  0.2612]]], device='cuda:0') tensor([ 0.1964, -0.2738, -0.2293, -0.2341, -0.4407,  0.5582], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2374267578125\n",
            "tcost icost 0.05078125 0.0\n",
            "tcost icost 0.044464111328125 0.0\n",
            "tcost icost 0.042205810546875 0.0\n",
            "tcost icost 0.042022705078125 0.0\n",
            "tcost icost 0.0482177734375 0.0\n",
            "tcost icost 0.0633544921875 0.0\n",
            "loss tensor([[0.2246]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.2787,  0.2326, -0.9318],\n",
            "         [ 0.4968,  0.0034, -0.8678],\n",
            "         [-0.0947,  0.1907,  0.2050],\n",
            "         [-0.0266,  0.3059,  0.1058],\n",
            "         [ 0.3060,  0.0549, -0.2993],\n",
            "         [-0.4929,  0.2418,  0.1609]]], device='cuda:0') tensor([ 0.2036, -0.2674, -0.2249, -0.2316, -0.4396,  0.5586], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.224609375\n",
            "tcost icost 0.05078125 0.0\n",
            "tcost icost 0.04449462890625 0.0\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost 0.042083740234375 0.0\n",
            "tcost icost 0.048248291015625 0.0\n",
            "tcost icost 0.06243896484375 0.0\n",
            "loss tensor([[0.2241]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.1626,  0.3021, -0.9393],\n",
            "         [ 0.5246, -0.0444, -0.8502],\n",
            "         [ 0.0026,  0.2051,  0.1049],\n",
            "         [ 0.0675,  0.2064,  0.0066],\n",
            "         [ 0.3837, -0.0442, -0.3971],\n",
            "         [-0.4122,  0.1417,  0.0607]]], device='cuda:0') tensor([ 0.2109, -0.2609, -0.2204, -0.2291, -0.4385,  0.5590], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.22412109375\n",
            "tcost icost 0.050811767578125 0.0\n",
            "tcost icost 0.046661376953125 0.0\n",
            "tcost icost 0.04449462890625 0.0\n",
            "tcost icost 0.044281005859375 0.0\n",
            "tcost icost 0.05047607421875 0.0\n",
            "tcost icost 0.065185546875 0.0\n",
            "loss tensor([[0.2327]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.0574,  0.3569, -0.9324],\n",
            "         [ 0.5463, -0.0590, -0.8355],\n",
            "         [ 0.0969,  0.1874,  0.0048],\n",
            "         [ 0.1536,  0.1067, -0.0928],\n",
            "         [ 0.4361, -0.1426, -0.4949],\n",
            "         [-0.3536,  0.0415, -0.0393]]], device='cuda:0') tensor([ 0.2178, -0.2550, -0.2162, -0.2268, -0.4374,  0.5594], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.232666015625\n",
            "tcost icost 0.047882080078125 0.0\n",
            "tcost icost 0.042022705078125 0.0\n",
            "tcost icost 0.039459228515625 0.0\n",
            "tcost icost 0.039306640625 0.0\n",
            "tcost icost 0.046875 0.0\n",
            "tcost icost 0.0618896484375 0.0\n",
            "loss tensor([[0.2136]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.0374,  0.3806, -0.9240],\n",
            "         [ 0.5640, -0.0935, -0.8205],\n",
            "         [ 0.1924,  0.1342, -0.0953],\n",
            "         [ 0.2422,  0.0069, -0.1924],\n",
            "         [ 0.4904, -0.2397, -0.5922],\n",
            "         [-0.2820, -0.0579, -0.1391]]], device='cuda:0') tensor([ 0.2255, -0.2489, -0.2121, -0.2246, -0.4365,  0.5597], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.213623046875\n",
            "tcost icost 0.047882080078125 0.0\n",
            "tcost icost 0.0419921875 0.0\n",
            "tcost icost 0.039459228515625 0.0\n",
            "tcost icost 0.039306640625 0.0\n",
            "tcost icost 0.046875 0.0\n",
            "tcost icost 0.061920166015625 0.0\n",
            "loss tensor([[0.2136]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.1224,  0.3770, -0.9181],\n",
            "         [ 0.5781, -0.1300, -0.8055],\n",
            "         [ 0.2886,  0.0668, -0.1954],\n",
            "         [ 0.3322, -0.0922, -0.2922],\n",
            "         [ 0.5444, -0.3352, -0.6894],\n",
            "         [-0.2031, -0.1563, -0.2385]]], device='cuda:0') tensor([ 0.2333, -0.2428, -0.2080, -0.2225, -0.4355,  0.5601], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.213623046875\n",
            "tcost icost 0.04791259765625 0.0\n",
            "tcost icost 0.04205322265625 0.0\n",
            "tcost icost 0.039520263671875 0.0\n",
            "tcost icost 0.039337158203125 0.0\n",
            "tcost icost 0.046875 0.0\n",
            "tcost icost 0.06195068359375 0.0\n",
            "loss tensor([[0.2139]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.1990,  0.3509, -0.9150],\n",
            "         [ 0.5900, -0.1569, -0.7920],\n",
            "         [ 0.3848, -0.0077, -0.2951],\n",
            "         [ 0.4221, -0.1887, -0.3920],\n",
            "         [ 0.5541, -0.3983, -0.7309],\n",
            "         [-0.1195, -0.2532, -0.3376]]], device='cuda:0') tensor([ 0.2411, -0.2368, -0.2039, -0.2203, -0.4345,  0.5605], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2138671875\n",
            "tcost icost 0.04791259765625 0.0\n",
            "tcost icost 0.042083740234375 0.0\n",
            "tcost icost 0.03955078125 0.0\n",
            "tcost icost 0.039398193359375 0.0\n",
            "tcost icost 0.046966552734375 0.0\n",
            "tcost icost 0.06195068359375 0.0\n",
            "loss tensor([[0.2140]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.2677,  0.3075, -0.9131],\n",
            "         [ 0.6013, -0.1664, -0.7815],\n",
            "         [ 0.4804, -0.0822, -0.3945],\n",
            "         [ 0.5108, -0.2814, -0.4919],\n",
            "         [ 0.5315, -0.4307, -0.7294],\n",
            "         [-0.0324, -0.3481, -0.4365]]], device='cuda:0') tensor([ 0.2489, -0.2307, -0.1998, -0.2181, -0.4336,  0.5608], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2139892578125\n",
            "tcost icost 0.047943115234375 0.0\n",
            "tcost icost 0.042083740234375 0.0\n",
            "tcost icost 0.03961181640625 0.0\n",
            "tcost icost 0.039459228515625 0.0\n",
            "tcost icost 0.04693603515625 0.0\n",
            "tcost icost 0.061981201171875 0.0\n",
            "loss tensor([[0.2141]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.3286,  0.2522, -0.9102],\n",
            "         [ 0.6126, -0.1582, -0.7744],\n",
            "         [ 0.5745, -0.1498, -0.4941],\n",
            "         [ 0.5974, -0.3696, -0.5920],\n",
            "         [ 0.5108, -0.4567, -0.7283],\n",
            "         [ 0.0572, -0.4404, -0.5356]]], device='cuda:0') tensor([ 0.2567, -0.2246, -0.1957, -0.2160, -0.4326,  0.5612], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.214111328125\n",
            "tcost icost 0.047943115234375 0.0\n",
            "tcost icost 0.0421142578125 0.0\n",
            "tcost icost 0.03961181640625 0.0\n",
            "tcost icost 0.03948974609375 0.0\n",
            "tcost icost 0.046966552734375 0.0\n",
            "tcost icost 0.06201171875 0.0\n",
            "loss tensor([[0.2141]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.3814,  0.1900, -0.9047],\n",
            "         [ 0.6236, -0.1376, -0.7695],\n",
            "         [ 0.6665, -0.2074, -0.5939],\n",
            "         [ 0.6358, -0.4227, -0.6458],\n",
            "         [ 0.4929, -0.4771, -0.7276],\n",
            "         [ 0.1488, -0.5298, -0.6350]]], device='cuda:0') tensor([ 0.2645, -0.2186, -0.1916, -0.2138, -0.4316,  0.5615], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.214111328125\n",
            "tcost icost 0.047943115234375 0.0\n",
            "tcost icost 0.0421142578125 0.0\n",
            "tcost icost 0.0396728515625 0.0\n",
            "tcost icost 0.039520263671875 0.0\n",
            "tcost icost 0.047027587890625 0.0\n",
            "tcost icost 0.062042236328125 0.0\n",
            "loss tensor([[0.2142]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.4256,  0.1237, -0.8964],\n",
            "         [ 0.6322, -0.1203, -0.7654],\n",
            "         [ 0.7135, -0.2465, -0.6559],\n",
            "         [ 0.6220, -0.4375, -0.6494],\n",
            "         [ 0.4726, -0.4946, -0.7294],\n",
            "         [ 0.2389, -0.6164, -0.7347]]], device='cuda:0') tensor([ 0.2720, -0.2127, -0.1876, -0.2118, -0.4307,  0.5618], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2142333984375\n",
            "tcost icost 0.047943115234375 0.0\n",
            "tcost icost 0.042144775390625 0.0\n",
            "tcost icost 0.0396728515625 0.0\n",
            "tcost icost 0.03955078125 0.0\n",
            "tcost icost 0.047027587890625 0.0\n",
            "tcost icost 0.06036376953125 0.0\n",
            "loss tensor([[0.2134]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.4619,  0.0611, -0.8848],\n",
            "         [ 0.6407, -0.0924, -0.7622],\n",
            "         [ 0.7032, -0.2513, -0.6651],\n",
            "         [ 0.6105, -0.4462, -0.6544],\n",
            "         [ 0.4591, -0.5067, -0.7297],\n",
            "         [ 0.2910, -0.6148, -0.7331]]], device='cuda:0') tensor([ 0.2800, -0.2065, -0.1834, -0.2096, -0.4297,  0.5622], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.21337890625\n",
            "tcost icost 0.0479736328125 0.0\n",
            "tcost icost 0.04217529296875 0.0\n",
            "tcost icost 0.03973388671875 0.0\n",
            "tcost icost 0.03955078125 0.0\n",
            "tcost icost 0.04705810546875 0.0\n",
            "tcost icost 0.062042236328125 0.0\n",
            "loss tensor([[0.2145]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 4.9030e-01,  7.5958e-04, -8.7156e-01],\n",
            "         [ 6.4643e-01, -7.8975e-02, -7.5888e-01],\n",
            "         [ 6.9265e-01, -2.5459e-01, -6.7485e-01],\n",
            "         [ 5.9868e-01, -4.5240e-01, -6.6099e-01],\n",
            "         [ 4.4410e-01, -5.1674e-01, -7.3195e-01],\n",
            "         [ 3.3109e-01, -6.0435e-01, -7.2467e-01]]], device='cuda:0') tensor([ 0.2872, -0.2009, -0.1796, -0.2075, -0.4288,  0.5625], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2144775390625\n",
            "tcost icost 0.0479736328125 0.0\n",
            "tcost icost 0.042205810546875 0.0\n",
            "tcost icost 0.039764404296875 0.0\n",
            "tcost icost 0.0396728515625 0.0\n",
            "tcost icost 0.04705810546875 0.0\n",
            "tcost icost 0.06207275390625 0.0\n",
            "loss tensor([[0.2146]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.5119, -0.0544, -0.8573],\n",
            "         [ 0.6503, -0.0785, -0.7556],\n",
            "         [ 0.6818, -0.2569, -0.6849],\n",
            "         [ 0.5869, -0.4564, -0.6687],\n",
            "         [ 0.4286, -0.5248, -0.7354],\n",
            "         [ 0.3636, -0.5933, -0.7182]]], device='cuda:0') tensor([ 0.2944, -0.1953, -0.1757, -0.2055, -0.4279,  0.5628], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.214599609375\n",
            "tcost icost 0.0479736328125 0.0\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost 0.039794921875 0.0\n",
            "tcost icost 0.039642333984375 0.0\n",
            "tcost icost 0.04705810546875 0.0\n",
            "tcost icost 0.06207275390625 0.0\n",
            "loss tensor([[0.2146]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.5279, -0.1018, -0.8432],\n",
            "         [ 0.6527, -0.0873, -0.7526],\n",
            "         [ 0.6709, -0.2587, -0.6949],\n",
            "         [ 0.5754, -0.4585, -0.6772],\n",
            "         [ 0.4134, -0.5310, -0.7397],\n",
            "         [ 0.3893, -0.5823, -0.7136]]], device='cuda:0') tensor([ 0.3017, -0.1897, -0.1718, -0.2034, -0.4269,  0.5632], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.214599609375\n",
            "tcost icost 0.050994873046875 0.0\n",
            "tcost icost 0.045928955078125 0.0\n",
            "tcost icost 0.043365478515625 0.0\n",
            "tcost icost 0.044158935546875 0.0\n",
            "tcost icost 0.05072021484375 0.0\n",
            "tcost icost 0.06683349609375 0.0\n",
            "loss tensor([[0.2323]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.5352, -0.1430, -0.8325],\n",
            "         [ 0.6505, -0.0902, -0.7541],\n",
            "         [ 0.6579, -0.2544, -0.7089],\n",
            "         [ 0.5595, -0.4581, -0.6908],\n",
            "         [ 0.3898, -0.5347, -0.7498],\n",
            "         [ 0.4052, -0.5707, -0.7142]]], device='cuda:0') tensor([ 0.3073, -0.1853, -0.1687, -0.2016, -0.4261,  0.5635], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2322998046875\n",
            "tcost icost 0.050994873046875 0.0\n",
            "tcost icost 0.04595947265625 0.0\n",
            "tcost icost 0.04339599609375 0.0\n",
            "tcost icost 0.044189453125 0.0\n",
            "tcost icost 0.05078125 0.0\n",
            "tcost icost 0.06689453125 0.0\n",
            "loss tensor([[0.2324]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.5360, -0.1764, -0.8256],\n",
            "         [ 0.6447, -0.0881, -0.7594],\n",
            "         [ 0.6427, -0.2459, -0.7256],\n",
            "         [ 0.5398, -0.4552, -0.7081],\n",
            "         [ 0.3603, -0.5355, -0.7639],\n",
            "         [ 0.4129, -0.5588, -0.7192]]], device='cuda:0') tensor([ 0.3130, -0.1809, -0.1656, -0.1997, -0.4252,  0.5638], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.232421875\n",
            "tcost icost 0.050994873046875 0.0\n",
            "tcost icost 0.04595947265625 0.0\n",
            "tcost icost 0.043365478515625 0.0\n",
            "tcost icost 0.044158935546875 0.0\n",
            "tcost icost 0.0494384765625 0.0\n",
            "tcost icost 0.063720703125 0.0\n",
            "loss tensor([[0.2297]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.5312, -0.2014, -0.8230],\n",
            "         [ 0.6346, -0.0853, -0.7681],\n",
            "         [ 0.6244, -0.2373, -0.7442],\n",
            "         [ 0.5149, -0.4515, -0.7287],\n",
            "         [ 0.3211, -0.5355, -0.7811],\n",
            "         [ 0.4092, -0.5489, -0.7289]]], device='cuda:0') tensor([ 0.3185, -0.1765, -0.1624, -0.1979, -0.4243,  0.5640], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.229736328125\n",
            "tcost icost 0.051025390625 0.0\n",
            "tcost icost 0.04595947265625 0.0\n",
            "tcost icost 0.04339599609375 0.0\n",
            "tcost icost 0.044219970703125 0.0\n",
            "tcost icost 0.049468994140625 0.0\n",
            "tcost icost 0.063720703125 0.0\n",
            "loss tensor([[0.2299]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.5221, -0.2175, -0.8247],\n",
            "         [ 0.6209, -0.0834, -0.7795],\n",
            "         [ 0.6030, -0.2300, -0.7639],\n",
            "         [ 0.4857, -0.4467, -0.7514],\n",
            "         [ 0.2760, -0.5336, -0.7994],\n",
            "         [ 0.3965, -0.5405, -0.7420]]], device='cuda:0') tensor([ 0.3238, -0.1721, -0.1593, -0.1961, -0.4234,  0.5643], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2298583984375\n",
            "tcost icost 0.051025390625 0.0\n",
            "tcost icost 0.045989990234375 0.0\n",
            "tcost icost 0.04339599609375 0.0\n",
            "tcost icost 0.04425048828125 0.0\n",
            "tcost icost 0.0494384765625 0.0\n",
            "tcost icost 0.063720703125 0.0\n",
            "loss tensor([[0.2299]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.5101, -0.2252, -0.8301],\n",
            "         [ 0.6039, -0.0844, -0.7926],\n",
            "         [ 0.5787, -0.2253, -0.7838],\n",
            "         [ 0.4528, -0.4406, -0.7751],\n",
            "         [ 0.2278, -0.5291, -0.8174],\n",
            "         [ 0.3767, -0.5331, -0.7576]]], device='cuda:0') tensor([ 0.3291, -0.1677, -0.1562, -0.1943, -0.4226,  0.5646], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2298583984375\n",
            "search tensor([[[ 0.2278, -0.5291, -0.8174],\n",
            "         [ 0.3767, -0.5331, -0.7576],\n",
            "         [-0.1262,  0.0355, -0.1795],\n",
            "         [ 0.5241, -0.0042,  0.1795],\n",
            "         [ 0.0460, -0.2520, -0.0669],\n",
            "         [ 0.2662,  0.1570, -0.0743]]], device='cuda:0') tensor([[[-0.4226],\n",
            "         [ 0.5646],\n",
            "         [ 0.2243],\n",
            "         [ 0.3172],\n",
            "         [ 0.2169],\n",
            "         [ 1.1200]]], device='cuda:0')\n",
            "tcost icost 0.04736328125 0.0\n",
            "tcost icost 0.051177978515625 0.0\n",
            "tcost icost 0.054962158203125 0.0\n",
            "tcost icost 0.0601806640625 0.0\n",
            "tcost icost 0.07330322265625 0.0\n",
            "tcost icost 0.0859375 0.0\n",
            "loss tensor([[0.2808]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.1140, -0.5619, -0.8193],\n",
            "         [ 0.4082, -0.5422, -0.7344],\n",
            "         [-0.0261, -0.0645, -0.2793],\n",
            "         [ 0.6236, -0.1042,  0.0794],\n",
            "         [ 0.1460, -0.3517,  0.0331],\n",
            "         [ 0.3660,  0.0568,  0.0257]]], device='cuda:0') tensor([-0.4187,  0.5673,  0.2262,  0.3183,  0.2175,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.28076171875\n",
            "tcost icost 0.047332763671875 0.0\n",
            "tcost icost 0.051177978515625 0.0\n",
            "tcost icost 0.054290771484375 0.0\n",
            "tcost icost 0.05999755859375 0.0\n",
            "tcost icost 0.07281494140625 0.0\n",
            "tcost icost 0.087158203125 0.0\n",
            "loss tensor([[0.2803]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.0127, -0.5815, -0.8134],\n",
            "         [ 0.3045, -0.5797, -0.7558],\n",
            "         [ 0.0740, -0.1576, -0.3786],\n",
            "         [ 0.7053, -0.1321, -0.0186],\n",
            "         [ 0.2458, -0.4057, -0.0199],\n",
            "         [ 0.4629, -0.0431, -0.0029]]], device='cuda:0') tensor([-0.4151,  0.5698,  0.2280,  0.3194,  0.2181,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2802734375\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.0518798828125 0.0\n",
            "tcost icost 0.056610107421875 0.0\n",
            "tcost icost 0.060546875 0.0\n",
            "tcost icost 0.0733642578125 0.0\n",
            "tcost icost 0.08489990234375 0.0\n",
            "loss tensor([[0.2852]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.0760, -0.5945, -0.8005],\n",
            "         [ 0.2552, -0.6007, -0.7576],\n",
            "         [ 0.1730, -0.2492, -0.4767],\n",
            "         [ 0.7911, -0.1562, -0.1009],\n",
            "         [ 0.3448, -0.4501, -0.0705],\n",
            "         [ 0.5592, -0.1412, -0.0069]]], device='cuda:0') tensor([-0.4111,  0.5735,  0.2305,  0.3209,  0.2187,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.28515625\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.051849365234375 0.0\n",
            "tcost icost 0.056671142578125 0.0\n",
            "tcost icost 0.060546875 0.0\n",
            "tcost icost 0.0733642578125 0.0\n",
            "tcost icost 0.0848388671875 0.0\n",
            "loss tensor([[0.2852]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.1524, -0.6021, -0.7838],\n",
            "         [ 0.2435, -0.6125, -0.7521],\n",
            "         [ 0.2723, -0.3380, -0.5748],\n",
            "         [ 0.8794, -0.1814, -0.1743],\n",
            "         [ 0.4429, -0.4763, -0.1163],\n",
            "         [ 0.6526, -0.2302,  0.0078]]], device='cuda:0') tensor([-0.4072,  0.5772,  0.2330,  0.3223,  0.2194,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.28515625\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.0518798828125 0.0\n",
            "tcost icost 0.056671142578125 0.0\n",
            "tcost icost 0.060577392578125 0.0\n",
            "tcost icost 0.07342529296875 0.0\n",
            "tcost icost 0.0849609375 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.2178, -0.6058, -0.7652],\n",
            "         [ 0.2513, -0.6190, -0.7441],\n",
            "         [ 0.3716, -0.4229, -0.6734],\n",
            "         [ 0.9493, -0.2044, -0.2387],\n",
            "         [ 0.5400, -0.4820, -0.1564],\n",
            "         [ 0.7433, -0.3136,  0.0282]]], device='cuda:0') tensor([-0.4032,  0.5810,  0.2355,  0.3238,  0.2201,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.0518798828125 0.0\n",
            "tcost icost 0.056640625 0.0\n",
            "tcost icost 0.060546875 0.0\n",
            "tcost icost 0.07342529296875 0.0\n",
            "tcost icost 0.08489990234375 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.2740, -0.6069, -0.7461],\n",
            "         [ 0.2685, -0.6223, -0.7353],\n",
            "         [ 0.4547, -0.4853, -0.7468],\n",
            "         [ 0.9375, -0.2109, -0.2767],\n",
            "         [ 0.6360, -0.4698, -0.1901],\n",
            "         [ 0.8319, -0.3938,  0.0533]]], device='cuda:0') tensor([-0.3992,  0.5847,  0.2380,  0.3252,  0.2208,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.051910400390625 0.0\n",
            "tcost icost 0.05670166015625 0.0\n",
            "tcost icost 0.060546875 0.0\n",
            "tcost icost 0.07342529296875 0.0\n",
            "tcost icost 0.0848388671875 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.3221, -0.6063, -0.7271],\n",
            "         [ 0.2899, -0.6239, -0.7258],\n",
            "         [ 0.4785, -0.4831, -0.7333],\n",
            "         [ 0.9246, -0.2205, -0.3106],\n",
            "         [ 0.7305, -0.4443, -0.2172],\n",
            "         [ 0.8865, -0.4556,  0.0809]]], device='cuda:0') tensor([-0.3952,  0.5885,  0.2405,  0.3267,  0.2215,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.054718017578125 0.0\n",
            "tcost icost 0.060516357421875 0.0\n",
            "tcost icost 0.06439208984375 0.0\n",
            "tcost icost 0.0745849609375 0.0\n",
            "tcost icost 0.08734130859375 0.0\n",
            "loss tensor([[0.2959]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.3580, -0.5982, -0.7169],\n",
            "         [ 0.2563, -0.6122, -0.7480],\n",
            "         [ 0.4886, -0.4714, -0.7342],\n",
            "         [ 0.9146, -0.1980, -0.3525],\n",
            "         [ 0.8213, -0.3951, -0.2225],\n",
            "         [ 0.8698, -0.4786,  0.1199]]], device='cuda:0') tensor([-0.3923,  0.5911,  0.2423,  0.3277,  0.2222,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2958984375\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.051910400390625 0.0\n",
            "tcost icost 0.056640625 0.0\n",
            "tcost icost 0.06060791015625 0.0\n",
            "tcost icost 0.073486328125 0.0\n",
            "tcost icost 0.08489990234375 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.3888, -0.5921, -0.7059],\n",
            "         [ 0.2347, -0.6031, -0.7623],\n",
            "         [ 0.4993, -0.4589, -0.7350],\n",
            "         [ 0.9029, -0.1852, -0.3879],\n",
            "         [ 0.9108, -0.3455, -0.2222],\n",
            "         [ 0.8535, -0.4977,  0.1543]]], device='cuda:0') tensor([-0.3883,  0.5948,  0.2448,  0.3292,  0.2229,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.051910400390625 0.0\n",
            "tcost icost 0.05670166015625 0.0\n",
            "tcost icost 0.06060791015625 0.0\n",
            "tcost icost 0.073486328125 0.0\n",
            "tcost icost 0.08502197265625 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.4160, -0.5878, -0.6938],\n",
            "         [ 0.2214, -0.5986, -0.7699],\n",
            "         [ 0.5089, -0.4482, -0.7349],\n",
            "         [ 0.8901, -0.1815, -0.4180],\n",
            "         [ 0.9383, -0.2804, -0.2023],\n",
            "         [ 0.8378, -0.5138,  0.1848]]], device='cuda:0') tensor([-0.3844,  0.5985,  0.2472,  0.3306,  0.2236,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.051910400390625 0.0\n",
            "tcost icost 0.05670166015625 0.0\n",
            "tcost icost 0.06060791015625 0.0\n",
            "tcost icost 0.073486328125 0.0\n",
            "tcost icost 0.0849609375 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.4383, -0.5859, -0.6817],\n",
            "         [ 0.2226, -0.5963, -0.7713],\n",
            "         [ 0.5197, -0.4384, -0.7333],\n",
            "         [ 0.8812, -0.1805, -0.4369],\n",
            "         [ 0.9654, -0.2111, -0.1531],\n",
            "         [ 0.8264, -0.5174,  0.2221]]], device='cuda:0') tensor([-0.3802,  0.6024,  0.2499,  0.3323,  0.2244,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.051910400390625 0.0\n",
            "tcost icost 0.056671142578125 0.0\n",
            "tcost icost 0.06060791015625 0.0\n",
            "tcost icost 0.07354736328125 0.0\n",
            "tcost icost 0.0849609375 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.4568, -0.5854, -0.6698],\n",
            "         [ 0.2335, -0.5959, -0.7684],\n",
            "         [ 0.5306, -0.4301, -0.7304],\n",
            "         [ 0.8757, -0.1823, -0.4471],\n",
            "         [ 0.9858, -0.1407, -0.0916],\n",
            "         [ 0.8184, -0.5112,  0.2624]]], device='cuda:0') tensor([-0.3759,  0.6064,  0.2526,  0.3340,  0.2253,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.0518798828125 0.0\n",
            "tcost icost 0.05670166015625 0.0\n",
            "tcost icost 0.060638427734375 0.0\n",
            "tcost icost 0.0736083984375 0.0\n",
            "tcost icost 0.0850830078125 0.0\n",
            "loss tensor([[0.2856]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.4726, -0.5860, -0.6582],\n",
            "         [ 0.2504, -0.5966, -0.7625],\n",
            "         [ 0.5413, -0.4234, -0.7264],\n",
            "         [ 0.8731, -0.1866, -0.4505],\n",
            "         [ 0.9970, -0.0733, -0.0245],\n",
            "         [ 0.8128, -0.4972,  0.3035]]], device='cuda:0') tensor([-0.3716,  0.6104,  0.2553,  0.3357,  0.2261,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.28564453125\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.051910400390625 0.0\n",
            "tcost icost 0.05670166015625 0.0\n",
            "tcost icost 0.06060791015625 0.0\n",
            "tcost icost 0.07354736328125 0.0\n",
            "tcost icost 0.0850830078125 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.4857, -0.5870, -0.6476],\n",
            "         [ 0.2715, -0.5971, -0.7548],\n",
            "         [ 0.5520, -0.4168, -0.7222],\n",
            "         [ 0.8731, -0.1901, -0.4490],\n",
            "         [ 0.9989, -0.0139,  0.0440],\n",
            "         [ 0.8089, -0.4769,  0.3438]]], device='cuda:0') tensor([-0.3675,  0.6142,  0.2578,  0.3373,  0.2269,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.05194091796875 0.0\n",
            "tcost icost 0.05670166015625 0.0\n",
            "tcost icost 0.060638427734375 0.0\n",
            "tcost icost 0.0736083984375 0.0\n",
            "tcost icost 0.0850830078125 0.0\n",
            "loss tensor([[0.2856]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.4970, -0.5882, -0.6380],\n",
            "         [ 0.2944, -0.5976, -0.7458],\n",
            "         [ 0.5622, -0.4109, -0.7177],\n",
            "         [ 0.8752, -0.1928, -0.4437],\n",
            "         [ 0.9933,  0.0319,  0.1110],\n",
            "         [ 0.8060, -0.4517,  0.3826]]], device='cuda:0') tensor([-0.3634,  0.6181,  0.2604,  0.3389,  0.2278,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.28564453125\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.051910400390625 0.0\n",
            "tcost icost 0.05670166015625 0.0\n",
            "tcost icost 0.0606689453125 0.0\n",
            "tcost icost 0.0736083984375 0.0\n",
            "tcost icost 0.08502197265625 0.0\n",
            "loss tensor([[0.2856]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.5069, -0.5893, -0.6292],\n",
            "         [ 0.3174, -0.5980, -0.7359],\n",
            "         [ 0.5717, -0.4057, -0.7131],\n",
            "         [ 0.8791, -0.1944, -0.4352],\n",
            "         [ 0.9828,  0.0593,  0.1748],\n",
            "         [ 0.8036, -0.4226,  0.4192]]], device='cuda:0') tensor([-0.3593,  0.6219,  0.2630,  0.3405,  0.2286,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.28564453125\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.05474853515625 0.0\n",
            "tcost icost 0.06060791015625 0.0\n",
            "tcost icost 0.06439208984375 0.0\n",
            "tcost icost 0.07476806640625 0.0\n",
            "tcost icost 0.0872802734375 0.0\n",
            "loss tensor([[0.2961]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.5188, -0.5798, -0.6283],\n",
            "         [ 0.3058, -0.5967, -0.7419],\n",
            "         [ 0.5680, -0.4014, -0.7185],\n",
            "         [ 0.8781, -0.1761, -0.4450],\n",
            "         [ 0.9696,  0.0940,  0.2257],\n",
            "         [ 0.7953, -0.4053,  0.4509]]], device='cuda:0') tensor([-0.3564,  0.6244,  0.2646,  0.3414,  0.2292,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.296142578125\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.054779052734375 0.0\n",
            "tcost icost 0.060577392578125 0.0\n",
            "tcost icost 0.06439208984375 0.0\n",
            "tcost icost 0.07464599609375 0.0\n",
            "tcost icost 0.08734130859375 0.0\n",
            "loss tensor([[0.2961]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.5337, -0.5628, -0.6312],\n",
            "         [ 0.2758, -0.5932, -0.7564],\n",
            "         [ 0.5534, -0.4000, -0.7306],\n",
            "         [ 0.8721, -0.1493, -0.4659],\n",
            "         [ 0.9544,  0.1320,  0.2679],\n",
            "         [ 0.7842, -0.3934,  0.4799]]], device='cuda:0') tensor([-0.3534,  0.6269,  0.2663,  0.3424,  0.2298,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.296142578125\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.051910400390625 0.0\n",
            "tcost icost 0.05670166015625 0.0\n",
            "tcost icost 0.0606689453125 0.0\n",
            "tcost icost 0.0733642578125 0.0\n",
            "tcost icost 0.085205078125 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.5460, -0.5503, -0.6317],\n",
            "         [ 0.2503, -0.5928, -0.7655],\n",
            "         [ 0.5402, -0.4027, -0.7389],\n",
            "         [ 0.8631, -0.1396, -0.4854],\n",
            "         [ 0.9447,  0.1281,  0.3019],\n",
            "         [ 0.7749, -0.3942,  0.4942]]], device='cuda:0') tensor([-0.3498,  0.6302,  0.2684,  0.3437,  0.2305,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.051910400390625 0.0\n",
            "tcost icost 0.05670166015625 0.0\n",
            "tcost icost 0.060699462890625 0.0\n",
            "tcost icost 0.0733642578125 0.0\n",
            "tcost icost 0.085205078125 0.0\n",
            "loss tensor([[0.2854]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.5559, -0.5420, -0.6302],\n",
            "         [ 0.2298, -0.5945, -0.7705],\n",
            "         [ 0.5284, -0.4086, -0.7442],\n",
            "         [ 0.8520, -0.1435, -0.5036],\n",
            "         [ 0.9388,  0.1021,  0.3289],\n",
            "         [ 0.7677, -0.4047,  0.4968]]], device='cuda:0') tensor([-0.3463,  0.6335,  0.2705,  0.3449,  0.2311,  1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.285400390625\n",
            "search tensor([[[ 0.9388,  0.1021,  0.3289],\n",
            "         [ 0.7677, -0.4047,  0.4968],\n",
            "         [-0.1030, -0.1608,  0.4635],\n",
            "         [-0.4816, -0.0982,  0.1162],\n",
            "         [ 0.1301,  0.3524,  0.4706],\n",
            "         [-0.4983, -0.0398,  0.0969]]], device='cuda:0') tensor([[[ 0.2311],\n",
            "         [ 1.0000],\n",
            "         [ 1.3478],\n",
            "         [ 0.2255],\n",
            "         [-0.2798],\n",
            "         [-0.8122]]], device='cuda:0')\n",
            "tcost icost 0.0487060546875 0.0\n",
            "tcost icost 0.051513671875 0.0\n",
            "tcost icost 0.051666259765625 0.0\n",
            "tcost icost 0.05108642578125 0.0\n",
            "tcost icost 0.0474853515625 0.0\n",
            "tcost icost 0.0474853515625 0.0\n",
            "loss tensor([[0.2334]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.9766,  0.0018,  0.2151],\n",
            "         [ 0.8664, -0.3041,  0.3960],\n",
            "         [-0.0029, -0.2607,  0.3630],\n",
            "         [-0.3811, -0.1981,  0.0161],\n",
            "         [ 0.2300,  0.2521,  0.5701],\n",
            "         [-0.3979, -0.1397,  0.1967]]], device='cuda:0') tensor([ 0.2388,  1.0000,  1.0000,  0.2287, -0.2784, -0.8119], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2333984375\n",
            "tcost icost 0.0487060546875 0.0\n",
            "tcost icost 0.051544189453125 0.0\n",
            "tcost icost 0.04998779296875 0.0\n",
            "tcost icost 0.047515869140625 0.0\n",
            "tcost icost 0.044708251953125 0.0\n",
            "tcost icost 0.04461669921875 0.0\n",
            "loss tensor([[0.2260]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.9902, -0.0900,  0.1063],\n",
            "         [ 0.9368, -0.1982,  0.2882],\n",
            "         [ 0.0971, -0.3596,  0.2628],\n",
            "         [-0.2807, -0.2973, -0.0822],\n",
            "         [ 0.3298,  0.1521,  0.5930],\n",
            "         [-0.2975, -0.2395,  0.2960]]], device='cuda:0') tensor([ 0.2463,  1.0000,  1.0000,  0.2316, -0.2771, -0.8116], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2259521484375\n",
            "tcost icost 0.047760009765625 0.0\n",
            "tcost icost 0.050628662109375 0.0\n",
            "tcost icost 0.049163818359375 0.0\n",
            "tcost icost 0.046966552734375 0.0\n",
            "tcost icost 0.045440673828125 0.0\n",
            "tcost icost 0.04376220703125 0.0\n",
            "loss tensor([[0.2231]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.9834, -0.1813,  0.0060],\n",
            "         [ 0.9761, -0.1238,  0.1786],\n",
            "         [ 0.1966, -0.4585,  0.1636],\n",
            "         [-0.1805, -0.3966, -0.1707],\n",
            "         [ 0.4294,  0.0524,  0.6572],\n",
            "         [-0.1971, -0.3339,  0.3755]]], device='cuda:0') tensor([ 0.2534,  1.0000,  1.0000,  0.2342, -0.2761, -0.8113], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.22314453125\n",
            "tcost icost 0.047760009765625 0.0\n",
            "tcost icost 0.050201416015625 0.0\n",
            "tcost icost 0.04864501953125 0.0\n",
            "tcost icost 0.04693603515625 0.0\n",
            "tcost icost 0.045440673828125 0.0\n",
            "tcost icost 0.04266357421875 0.0\n",
            "loss tensor([[0.2214]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9562, -0.2774, -0.0930],\n",
            "         [ 0.9932, -0.0890,  0.0748],\n",
            "         [ 0.2952, -0.5537,  0.0641],\n",
            "         [-0.0810, -0.4960, -0.2614],\n",
            "         [ 0.5283, -0.0471,  0.7282],\n",
            "         [-0.0972, -0.4177,  0.4616]]], device='cuda:0') tensor([ 0.2611,  1.0000,  1.0000,  0.2369, -0.2749, -0.8110], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.221435546875\n",
            "tcost icost 0.047760009765625 0.0\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.04864501953125 0.0\n",
            "tcost icost 0.047027587890625 0.0\n",
            "tcost icost 0.045501708984375 0.0\n",
            "tcost icost 0.042694091796875 0.0\n",
            "loss tensor([[0.2217]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9083, -0.3719, -0.1914],\n",
            "         [ 0.9961, -0.0857, -0.0223],\n",
            "         [ 0.3927, -0.6415, -0.0353],\n",
            "         [ 0.0180, -0.5950, -0.3544],\n",
            "         [ 0.6072, -0.1411,  0.7819],\n",
            "         [ 0.0025, -0.5046,  0.5515]]], device='cuda:0') tensor([ 0.2688,  1.0000,  1.0000,  0.2396, -0.2737, -0.8106], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2216796875\n",
            "tcost icost 0.047760009765625 0.0\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.048675537109375 0.0\n",
            "tcost icost 0.0469970703125 0.0\n",
            "tcost icost 0.04547119140625 0.0\n",
            "tcost icost 0.042694091796875 0.0\n",
            "loss tensor([[0.2217]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.8413, -0.4585, -0.2863],\n",
            "         [ 0.9885, -0.1018, -0.1118],\n",
            "         [ 0.4888, -0.7194, -0.1341],\n",
            "         [ 0.1163, -0.6930, -0.4489],\n",
            "         [ 0.6174, -0.2083,  0.7586],\n",
            "         [ 0.1019, -0.5942,  0.6439]]], device='cuda:0') tensor([ 0.2765,  1.0000,  1.0000,  0.2423, -0.2726, -0.8103], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2216796875\n",
            "tcost icost 0.04779052734375 0.0\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.048675537109375 0.0\n",
            "tcost icost 0.047027587890625 0.0\n",
            "tcost icost 0.0455322265625 0.0\n",
            "tcost icost 0.041107177734375 0.0\n",
            "loss tensor([[0.2209]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.7610, -0.5304, -0.3737],\n",
            "         [ 0.9732, -0.1251, -0.1928],\n",
            "         [ 0.5826, -0.7789, -0.2320],\n",
            "         [ 0.2142, -0.7882, -0.5447],\n",
            "         [ 0.6236, -0.2628,  0.7362],\n",
            "         [ 0.1960, -0.6680,  0.7179]]], device='cuda:0') tensor([ 0.2841,  1.0000,  1.0000,  0.2450, -0.2714, -0.8100], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.220947265625\n",
            "tcost icost 0.04779052734375 0.0\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.04864501953125 0.0\n",
            "tcost icost 0.04583740234375 0.0\n",
            "tcost icost 0.043914794921875 0.0\n",
            "tcost icost 0.039459228515625 0.0\n",
            "loss tensor([[0.2180]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.6823, -0.5777, -0.4480],\n",
            "         [ 0.9538, -0.1437, -0.2637],\n",
            "         [ 0.6140, -0.7306, -0.2988],\n",
            "         [ 0.2775, -0.7744, -0.5686],\n",
            "         [ 0.6278, -0.3009,  0.7178],\n",
            "         [ 0.2564, -0.6609,  0.7053]]], device='cuda:0') tensor([ 0.2918,  1.0000,  1.0000,  0.2482, -0.2699, -0.8095], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.218017578125\n",
            "tcost icost 0.04779052734375 0.0\n",
            "tcost icost 0.050262451171875 0.0\n",
            "tcost icost 0.0487060546875 0.0\n",
            "tcost icost 0.04583740234375 0.0\n",
            "tcost icost 0.043914794921875 0.0\n",
            "tcost icost 0.03948974609375 0.0\n",
            "loss tensor([[0.2180]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.6139, -0.6033, -0.5091],\n",
            "         [ 0.9322, -0.1584, -0.3255],\n",
            "         [ 0.6476, -0.6703, -0.3623],\n",
            "         [ 0.3296, -0.7424, -0.5833],\n",
            "         [ 0.6316, -0.3254,  0.7037],\n",
            "         [ 0.3076, -0.6526,  0.6925]]], device='cuda:0') tensor([ 0.2995,  1.0000,  1.0000,  0.2514, -0.2684, -0.8090], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.218017578125\n",
            "tcost icost 0.047821044921875 0.0\n",
            "tcost icost 0.050262451171875 0.0\n",
            "tcost icost 0.0487060546875 0.0\n",
            "tcost icost 0.045867919921875 0.0\n",
            "tcost icost 0.0439453125 0.0\n",
            "tcost icost 0.0394287109375 0.0\n",
            "loss tensor([[0.2180]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.5607, -0.6113, -0.5585],\n",
            "         [ 0.9095, -0.1706, -0.3790],\n",
            "         [ 0.6795, -0.6004, -0.4217],\n",
            "         [ 0.3765, -0.7068, -0.5989],\n",
            "         [ 0.6355, -0.3391,  0.6937],\n",
            "         [ 0.3508, -0.6434,  0.6804]]], device='cuda:0') tensor([ 0.3072,  1.0000,  1.0000,  0.2546, -0.2669, -0.8085], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.218017578125\n",
            "tcost icost 0.047821044921875 0.0\n",
            "tcost icost 0.050262451171875 0.0\n",
            "tcost icost 0.048736572265625 0.0\n",
            "tcost icost 0.0458984375 0.0\n",
            "tcost icost 0.0439453125 0.0\n",
            "tcost icost 0.039459228515625 0.0\n",
            "loss tensor([[0.2181]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.5225, -0.6067, -0.5990],\n",
            "         [ 0.8867, -0.1815, -0.4252],\n",
            "         [ 0.7064, -0.5245, -0.4752],\n",
            "         [ 0.4185, -0.6684, -0.6149],\n",
            "         [ 0.6397, -0.3444,  0.6871],\n",
            "         [ 0.3879, -0.6318,  0.6711]]], device='cuda:0') tensor([ 0.3147,  1.0000,  1.0000,  0.2575, -0.2655, -0.8082], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2181396484375\n",
            "tcost icost 0.047821044921875 0.0\n",
            "tcost icost 0.050262451171875 0.0\n",
            "tcost icost 0.048736572265625 0.0\n",
            "tcost icost 0.0458984375 0.0\n",
            "tcost icost 0.044036865234375 0.0\n",
            "tcost icost 0.03948974609375 0.0\n",
            "loss tensor([[0.2183]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.4981, -0.5931, -0.6325],\n",
            "         [ 0.8644, -0.1920, -0.4648],\n",
            "         [ 0.7268, -0.4462, -0.5221],\n",
            "         [ 0.4562, -0.6278, -0.6307],\n",
            "         [ 0.6443, -0.3428,  0.6836],\n",
            "         [ 0.4201, -0.6180,  0.6645]]], device='cuda:0') tensor([ 0.3222,  1.0000,  1.0000,  0.2605, -0.2642, -0.8078], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.21826171875\n",
            "tcost icost 0.0478515625 0.0\n",
            "tcost icost 0.050262451171875 0.0\n",
            "tcost icost 0.0487060546875 0.0\n",
            "tcost icost 0.0458984375 0.0\n",
            "tcost icost 0.04400634765625 0.0\n",
            "tcost icost 0.039581298828125 0.0\n",
            "loss tensor([[0.2183]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.4850, -0.5736, -0.6601],\n",
            "         [ 0.8429, -0.2026, -0.4985],\n",
            "         [ 0.7403, -0.3692, -0.5618],\n",
            "         [ 0.4898, -0.5855, -0.6460],\n",
            "         [ 0.6492, -0.3358,  0.6825],\n",
            "         [ 0.4486, -0.6019,  0.6607]]], device='cuda:0') tensor([ 0.3297,  1.0000,  1.0000,  0.2635, -0.2629, -0.8074], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.21826171875\n",
            "tcost icost 0.0478515625 0.0\n",
            "tcost icost 0.050323486328125 0.0\n",
            "tcost icost 0.04876708984375 0.0\n",
            "tcost icost 0.045989990234375 0.0\n",
            "tcost icost 0.044097900390625 0.0\n",
            "tcost icost 0.039581298828125 0.0\n",
            "loss tensor([[0.2185]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.4803, -0.5506, -0.6828],\n",
            "         [ 0.8226, -0.2138, -0.5269],\n",
            "         [ 0.7471, -0.2976, -0.5943],\n",
            "         [ 0.5195, -0.5423, -0.6603],\n",
            "         [ 0.6542, -0.3243,  0.6833],\n",
            "         [ 0.4743, -0.5832,  0.6595]]], device='cuda:0') tensor([ 0.3372,  1.0000,  1.0000,  0.2665, -0.2615, -0.8070], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.218505859375\n",
            "tcost icost 0.0478515625 0.0\n",
            "tcost icost 0.050323486328125 0.0\n",
            "tcost icost 0.04876708984375 0.0\n",
            "tcost icost 0.045989990234375 0.0\n",
            "tcost icost 0.044036865234375 0.0\n",
            "tcost icost 0.03955078125 0.0\n",
            "loss tensor([[0.2184]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.4818, -0.5258, -0.7010],\n",
            "         [ 0.8035, -0.2256, -0.5508],\n",
            "         [ 0.7483, -0.2354, -0.6201],\n",
            "         [ 0.5456, -0.4990, -0.6733],\n",
            "         [ 0.6592, -0.3094,  0.6854],\n",
            "         [ 0.4979, -0.5615,  0.6609]]], device='cuda:0') tensor([ 0.3449,  1.0000,  1.0000,  0.2695, -0.2602, -0.8066], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2183837890625\n",
            "tcost icost 0.0478515625 0.0\n",
            "tcost icost 0.050323486328125 0.0\n",
            "tcost icost 0.04876708984375 0.0\n",
            "tcost icost 0.0460205078125 0.0\n",
            "tcost icost 0.044097900390625 0.0\n",
            "tcost icost 0.039581298828125 0.0\n",
            "loss tensor([[0.2185]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.4873, -0.5013, -0.7150],\n",
            "         [ 0.7859, -0.2379, -0.5708],\n",
            "         [ 0.7453, -0.1857, -0.6403],\n",
            "         [ 0.5680, -0.4566, -0.6847],\n",
            "         [ 0.6642, -0.2918,  0.6883],\n",
            "         [ 0.5200, -0.5367,  0.6645]]], device='cuda:0') tensor([ 0.3526,  1.0000,  1.0000,  0.2725, -0.2588, -0.8062], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.218505859375\n",
            "tcost icost 0.0478515625 0.0\n",
            "tcost icost 0.05029296875 0.0\n",
            "tcost icost 0.048797607421875 0.0\n",
            "tcost icost 0.046051025390625 0.0\n",
            "tcost icost 0.044097900390625 0.0\n",
            "tcost icost 0.03887939453125 0.0\n",
            "loss tensor([[0.2181]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.4955, -0.4785, -0.7249],\n",
            "         [ 0.7695, -0.2508, -0.5873],\n",
            "         [ 0.7395, -0.1506, -0.6561],\n",
            "         [ 0.5870, -0.4158, -0.6946],\n",
            "         [ 0.6694, -0.2713,  0.6916],\n",
            "         [ 0.5423, -0.5051,  0.6714]]], device='cuda:0') tensor([ 0.3603,  1.0000,  1.0000,  0.2754, -0.2576, -0.8059], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2181396484375\n",
            "tcost icost 0.047882080078125 0.0\n",
            "tcost icost 0.05035400390625 0.0\n",
            "tcost icost 0.048797607421875 0.0\n",
            "tcost icost 0.046051025390625 0.0\n",
            "tcost icost 0.04412841796875 0.0\n",
            "tcost icost 0.038848876953125 0.0\n",
            "loss tensor([[0.2181]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.5049, -0.4589, -0.7311],\n",
            "         [ 0.7546, -0.2638, -0.6008],\n",
            "         [ 0.7322, -0.1302, -0.6685],\n",
            "         [ 0.6027, -0.3777, -0.7029],\n",
            "         [ 0.6746, -0.2486,  0.6951],\n",
            "         [ 0.5645, -0.4670,  0.6806]]], device='cuda:0') tensor([ 0.3680,  1.0000,  1.0000,  0.2783, -0.2563, -0.8055], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2181396484375\n",
            "tcost icost 0.047882080078125 0.0\n",
            "tcost icost 0.05035400390625 0.0\n",
            "tcost icost 0.048828125 0.0\n",
            "tcost icost 0.046142578125 0.0\n",
            "tcost icost 0.044189453125 0.0\n",
            "tcost icost 0.03887939453125 0.0\n",
            "loss tensor([[0.2185]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.5144, -0.4436, -0.7339],\n",
            "         [ 0.7410, -0.2766, -0.6119],\n",
            "         [ 0.7243, -0.1229, -0.6784],\n",
            "         [ 0.6154, -0.3434, -0.7095],\n",
            "         [ 0.6796, -0.2246,  0.6983],\n",
            "         [ 0.5862, -0.4229,  0.6910]]], device='cuda:0') tensor([ 0.3757,  1.0000,  1.0000,  0.2811, -0.2550, -0.8052], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.218505859375\n",
            "tcost icost 0.04791259765625 0.0\n",
            "tcost icost 0.05035400390625 0.0\n",
            "tcost icost 0.048828125 0.0\n",
            "tcost icost 0.046142578125 0.0\n",
            "tcost icost 0.044189453125 0.0\n",
            "tcost icost 0.038848876953125 0.0\n",
            "loss tensor([[0.2185]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.5235, -0.4329, -0.7339],\n",
            "         [ 0.7287, -0.2887, -0.6209],\n",
            "         [ 0.7162, -0.1263, -0.6863],\n",
            "         [ 0.6255, -0.3138, -0.7144],\n",
            "         [ 0.6844, -0.2002,  0.7010],\n",
            "         [ 0.6068, -0.3736,  0.7016]]], device='cuda:0') tensor([ 0.3834,  1.0000,  1.0000,  0.2840, -0.2538, -0.8048], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.218505859375\n",
            "search tensor([[[ 0.6844, -0.2002,  0.7010],\n",
            "         [ 0.6068, -0.3736,  0.7016],\n",
            "         [ 0.4477, -0.2532, -0.0849],\n",
            "         [-0.4185, -0.3010, -0.0359],\n",
            "         [-0.3052,  0.0846,  0.3003],\n",
            "         [ 0.1506, -0.0990, -0.0936]]], device='cuda:0') tensor([[[-0.2538],\n",
            "         [-0.8048],\n",
            "         [ 0.2777],\n",
            "         [ 0.4563],\n",
            "         [-0.5886],\n",
            "         [-0.3020]]], device='cuda:0')\n",
            "tcost icost 0.036224365234375 0.0\n",
            "tcost icost 0.033050537109375 0.0\n",
            "tcost icost 0.03021240234375 0.0\n",
            "tcost icost 0.036529541015625 0.0\n",
            "tcost icost 0.042633056640625 0.0\n",
            "tcost icost 0.03961181640625 0.0\n",
            "loss tensor([[0.1685]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.7596, -0.2906,  0.5818],\n",
            "         [ 0.7062, -0.2732,  0.6009],\n",
            "         [ 0.5473, -0.1530, -0.1848],\n",
            "         [-0.3181, -0.2007, -0.1358],\n",
            "         [-0.2049,  0.1845,  0.2000],\n",
            "         [ 0.2505,  0.0011, -0.1934]]], device='cuda:0') tensor([-0.2509, -0.8029,  0.2787,  0.4566, -0.5887, -0.3021], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.16845703125\n",
            "tcost icost 0.036224365234375 0.0\n",
            "tcost icost 0.033050537109375 0.0\n",
            "tcost icost 0.03021240234375 0.0\n",
            "tcost icost 0.03656005859375 0.0\n",
            "tcost icost 0.040496826171875 0.0\n",
            "tcost icost 0.036834716796875 0.0\n",
            "loss tensor([[0.1654]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.8517, -0.2159,  0.4775],\n",
            "         [ 0.8055, -0.1738,  0.5005],\n",
            "         [ 0.6467, -0.0549, -0.2795],\n",
            "         [-0.2185, -0.1007, -0.2354],\n",
            "         [-0.1054,  0.2193,  0.1009],\n",
            "         [ 0.3502,  0.0908, -0.1439]]], device='cuda:0') tensor([-0.2472, -0.8001,  0.2807,  0.4576, -0.5884, -0.3023], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1654052734375\n",
            "tcost icost 0.036224365234375 0.0\n",
            "tcost icost 0.0330810546875 0.0\n",
            "tcost icost 0.030242919921875 0.0\n",
            "tcost icost 0.03656005859375 0.0\n",
            "tcost icost 0.04046630859375 0.0\n",
            "tcost icost 0.036834716796875 0.0\n",
            "loss tensor([[0.1654]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.9211, -0.1287,  0.3673],\n",
            "         [ 0.9043, -0.0742,  0.4011],\n",
            "         [ 0.7450,  0.0339, -0.3755],\n",
            "         [-0.1186, -0.0023, -0.3347],\n",
            "         [-0.0057,  0.1845,  0.0021],\n",
            "         [ 0.4498,  0.1220, -0.0708]]], device='cuda:0') tensor([-0.2437, -0.7975,  0.2823,  0.4585, -0.5882, -0.3024], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1654052734375\n",
            "tcost icost 0.0362548828125 0.0\n",
            "tcost icost 0.0330810546875 0.0\n",
            "tcost icost 0.0302734375 0.0\n",
            "tcost icost 0.03656005859375 0.0\n",
            "tcost icost 0.040496826171875 0.0\n",
            "tcost icost 0.036834716796875 0.0\n",
            "loss tensor([[0.1655]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9654, -0.0429,  0.2574],\n",
            "         [ 0.9567,  0.0226,  0.2901],\n",
            "         [ 0.8422,  0.1122, -0.4726],\n",
            "         [-0.0187,  0.0845, -0.4341],\n",
            "         [ 0.0942,  0.1243, -0.0960],\n",
            "         [ 0.5489,  0.0963,  0.0117]]], device='cuda:0') tensor([-0.2403, -0.7949,  0.2840,  0.4593, -0.5880, -0.3025], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.16552734375\n",
            "tcost icost 0.0362548828125 0.0\n",
            "tcost icost 0.033050537109375 0.0\n",
            "tcost icost 0.030242919921875 0.0\n",
            "tcost icost 0.035003662109375 0.0\n",
            "tcost icost 0.03826904296875 0.0\n",
            "tcost icost 0.035125732421875 0.0\n",
            "loss tensor([[0.1619]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9881,  0.0205,  0.1525],\n",
            "         [ 0.9781,  0.1058,  0.1794],\n",
            "         [ 0.8439,  0.1611, -0.5118],\n",
            "         [ 0.0813,  0.1476, -0.5335],\n",
            "         [ 0.1937,  0.0609, -0.1948],\n",
            "         [ 0.6449,  0.0496,  0.0458]]], device='cuda:0') tensor([-0.2369, -0.7923,  0.2857,  0.4600, -0.5878, -0.3027], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.161865234375\n",
            "tcost icost 0.035003662109375 0.0\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.0293731689453125 0.0\n",
            "tcost icost 0.033905029296875 0.0\n",
            "tcost icost 0.0357666015625 0.0\n",
            "tcost icost 0.034759521484375 0.0\n",
            "loss tensor([[0.1564]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9956,  0.0775,  0.0525],\n",
            "         [ 0.9814,  0.1770,  0.0751],\n",
            "         [ 0.8227,  0.1960, -0.5337],\n",
            "         [ 0.1804,  0.2110, -0.6330],\n",
            "         [ 0.2926,  0.0192, -0.2916],\n",
            "         [ 0.7381,  0.0076,  0.0922]]], device='cuda:0') tensor([-0.2331, -0.7896,  0.2874,  0.4609, -0.5876, -0.3028], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1563720703125\n",
            "tcost icost 0.035003662109375 0.0\n",
            "tcost icost 0.031982421875 0.0\n",
            "tcost icost 0.0293731689453125 0.0\n",
            "tcost icost 0.033905029296875 0.0\n",
            "tcost icost 0.0357666015625 0.0\n",
            "tcost icost 0.03472900390625 0.0\n",
            "loss tensor([[0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9776,  0.1178, -0.0475],\n",
            "         [ 0.9718,  0.2348, -0.0206],\n",
            "         [ 0.8031,  0.2219, -0.5530],\n",
            "         [ 0.2786,  0.2462, -0.7326],\n",
            "         [ 0.3905,  0.0041, -0.3868],\n",
            "         [ 0.8289, -0.0070,  0.1477]]], device='cuda:0') tensor([-0.2295, -0.7869,  0.2891,  0.4617, -0.5873, -0.3029], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15625\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.0294036865234375 0.0\n",
            "tcost icost 0.03387451171875 0.0\n",
            "tcost icost 0.0357666015625 0.0\n",
            "tcost icost 0.034698486328125 0.0\n",
            "loss tensor([[0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9425,  0.1354, -0.1475],\n",
            "         [ 0.9541,  0.2797, -0.1068],\n",
            "         [ 0.7857,  0.2394, -0.5704],\n",
            "         [ 0.3758,  0.2382, -0.8323],\n",
            "         [ 0.4871,  0.0099, -0.4814],\n",
            "         [ 0.9181,  0.0057,  0.2116]]], device='cuda:0') tensor([-0.2259, -0.7842,  0.2909,  0.4626, -0.5871, -0.3030], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15625\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.029388427734375 0.0\n",
            "tcost icost 0.033905029296875 0.0\n",
            "tcost icost 0.035736083984375 0.0\n",
            "tcost icost 0.0347900390625 0.0\n",
            "loss tensor([[0.1564]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.8958,  0.1305, -0.2475],\n",
            "         [ 0.9320,  0.3126, -0.1835],\n",
            "         [ 0.7707,  0.2495, -0.5863],\n",
            "         [ 0.4434,  0.1887, -0.8762],\n",
            "         [ 0.5822,  0.0288, -0.5764],\n",
            "         [ 0.9620,  0.0365,  0.2705]]], device='cuda:0') tensor([-0.2222, -0.7815,  0.2926,  0.4634, -0.5868, -0.3030], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1563720703125\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.029388427734375 0.0\n",
            "tcost icost 0.033905029296875 0.0\n",
            "tcost icost 0.0357666015625 0.0\n",
            "tcost icost 0.034515380859375 0.0\n",
            "loss tensor([[0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.8428,  0.1048, -0.3475],\n",
            "         [ 0.9086,  0.3337, -0.2511],\n",
            "         [ 0.7585,  0.2517, -0.6012],\n",
            "         [ 0.4792,  0.1197, -0.8695],\n",
            "         [ 0.6753,  0.0473, -0.6724],\n",
            "         [ 0.9470,  0.0710,  0.3134]]], device='cuda:0') tensor([-0.2187, -0.7790,  0.2943,  0.4642, -0.5866, -0.3031], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15625\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.0294189453125 0.0\n",
            "tcost icost 0.033935546875 0.0\n",
            "tcost icost 0.0357666015625 0.0\n",
            "tcost icost 0.0345458984375 0.0\n",
            "loss tensor([[0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.7859,  0.0641, -0.4476],\n",
            "         [ 0.8859,  0.3443, -0.3108],\n",
            "         [ 0.7487,  0.2472, -0.6151],\n",
            "         [ 0.5078,  0.0494, -0.8601],\n",
            "         [ 0.7047,  0.0536, -0.7075],\n",
            "         [ 0.9290,  0.1065,  0.3543]]], device='cuda:0') tensor([-0.2152, -0.7764,  0.2961,  0.4651, -0.5863, -0.3032], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15625\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.0294036865234375 0.0\n",
            "tcost icost 0.033905029296875 0.0\n",
            "tcost icost 0.0357666015625 0.0\n",
            "tcost icost 0.0345458984375 0.0\n",
            "loss tensor([[0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.7294,  0.0131, -0.5479],\n",
            "         [ 0.8650,  0.3462, -0.3631],\n",
            "         [ 0.7410,  0.2372, -0.6282],\n",
            "         [ 0.5298, -0.0171, -0.8480],\n",
            "         [ 0.7012,  0.0498, -0.7112],\n",
            "         [ 0.9094,  0.1384,  0.3923]]], device='cuda:0') tensor([-0.2114, -0.7737,  0.2978,  0.4659, -0.5862, -0.3033], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15625\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.03204345703125 0.0\n",
            "tcost icost 0.0294189453125 0.0\n",
            "tcost icost 0.033966064453125 0.0\n",
            "tcost icost 0.035797119140625 0.0\n",
            "tcost icost 0.0345458984375 0.0\n",
            "loss tensor([[0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.6744, -0.0462, -0.6484],\n",
            "         [ 0.8464,  0.3406, -0.4093],\n",
            "         [ 0.7352,  0.2228, -0.6402],\n",
            "         [ 0.5461, -0.0763, -0.8342],\n",
            "         [ 0.6973,  0.0401, -0.7156],\n",
            "         [ 0.8894,  0.1632,  0.4270]]], device='cuda:0') tensor([-0.2077, -0.7710,  0.2996,  0.4667, -0.5860, -0.3034], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15625\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.03204345703125 0.0\n",
            "tcost icost 0.0294189453125 0.0\n",
            "tcost icost 0.033966064453125 0.0\n",
            "tcost icost 0.035736083984375 0.0\n",
            "tcost icost 0.034515380859375 0.0\n",
            "loss tensor([[0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.6217, -0.1122, -0.7492],\n",
            "         [ 0.8301,  0.3289, -0.4503],\n",
            "         [ 0.7306,  0.2048, -0.6514],\n",
            "         [ 0.5583, -0.1251, -0.8201],\n",
            "         [ 0.6932,  0.0260, -0.7202],\n",
            "         [ 0.8707,  0.1780,  0.4585]]], device='cuda:0') tensor([-0.2041, -0.7684,  0.3013,  0.4675, -0.5858, -0.3035], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15625\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.0294342041015625 0.0\n",
            "tcost icost 0.033935546875 0.0\n",
            "tcost icost 0.035797119140625 0.0\n",
            "tcost icost 0.034576416015625 0.0\n",
            "loss tensor([[0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.5501, -0.1764, -0.8162],\n",
            "         [ 0.8159,  0.3123, -0.4866],\n",
            "         [ 0.7269,  0.1845, -0.6615],\n",
            "         [ 0.5679, -0.1608, -0.8072],\n",
            "         [ 0.6889,  0.0095, -0.7248],\n",
            "         [ 0.8543,  0.1817,  0.4870]]], device='cuda:0') tensor([-0.2004, -0.7657,  0.3032,  0.4683, -0.5856, -0.3036], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15625\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.0294342041015625 0.0\n",
            "tcost icost 0.033966064453125 0.0\n",
            "tcost icost 0.035797119140625 0.0\n",
            "tcost icost 0.0345458984375 0.0\n",
            "loss tensor([[0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.4717, -0.2335, -0.8503],\n",
            "         [ 0.8036,  0.2914, -0.5190],\n",
            "         [ 0.7238,  0.1629, -0.6705],\n",
            "         [ 0.5763, -0.1823, -0.7967],\n",
            "         [ 0.6843, -0.0077, -0.7291],\n",
            "         [ 0.8407,  0.1748,  0.5125]]], device='cuda:0') tensor([-0.1967, -0.7630,  0.3051,  0.4691, -0.5854, -0.3036], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15625\n",
            "tcost icost 0.031524658203125 0.0\n",
            "tcost icost 0.028106689453125 0.0\n",
            "tcost icost 0.0257568359375 0.0\n",
            "tcost icost 0.03033447265625 0.0\n",
            "tcost icost 0.032501220703125 0.0\n",
            "tcost icost 0.031158447265625 0.0\n",
            "loss tensor([[0.1395]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.4143, -0.2824, -0.8652],\n",
            "         [ 0.7934,  0.2667, -0.5471],\n",
            "         [ 0.7221,  0.1361, -0.6783],\n",
            "         [ 0.5840, -0.1999, -0.7867],\n",
            "         [ 0.6805, -0.0422, -0.7315],\n",
            "         [ 0.8318,  0.1425,  0.5365]]], device='cuda:0') tensor([-0.1910, -0.7591,  0.3075,  0.4703, -0.5850, -0.3037], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1395263671875\n",
            "tcost icost 0.031524658203125 0.0\n",
            "tcost icost 0.028106689453125 0.0\n",
            "tcost icost 0.0257568359375 0.0\n",
            "tcost icost 0.030364990234375 0.0\n",
            "tcost icost 0.032440185546875 0.0\n",
            "tcost icost 0.0312347412109375 0.0\n",
            "loss tensor([[0.1396]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.3787, -0.3199, -0.8684],\n",
            "         [ 0.7846,  0.2406, -0.5715],\n",
            "         [ 0.7209,  0.1062, -0.6848],\n",
            "         [ 0.5916, -0.2124, -0.7778],\n",
            "         [ 0.6765, -0.0836, -0.7317],\n",
            "         [ 0.8243,  0.0980,  0.5575]]], device='cuda:0') tensor([-0.1854, -0.7551,  0.3099,  0.4715, -0.5847, -0.3037], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1396484375\n",
            "tcost icost 0.031524658203125 0.0\n",
            "tcost icost 0.0281219482421875 0.0\n",
            "tcost icost 0.0257568359375 0.0\n",
            "tcost icost 0.030364990234375 0.0\n",
            "tcost icost 0.032501220703125 0.0\n",
            "tcost icost 0.0312347412109375 0.0\n",
            "loss tensor([[0.1396]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.3610, -0.3461, -0.8660],\n",
            "         [ 0.7767,  0.2131, -0.5928],\n",
            "         [ 0.7197,  0.0762, -0.6900],\n",
            "         [ 0.5992, -0.2181, -0.7703],\n",
            "         [ 0.6718, -0.1260, -0.7300],\n",
            "         [ 0.8168,  0.0497,  0.5748]]], device='cuda:0') tensor([-0.1798, -0.7511,  0.3125,  0.4727, -0.5843, -0.3038], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1396484375\n",
            "tcost icost 0.031524658203125 0.0\n",
            "tcost icost 0.028106689453125 0.0\n",
            "tcost icost 0.0257568359375 0.0\n",
            "tcost icost 0.0303955078125 0.0\n",
            "tcost icost 0.03253173828125 0.0\n",
            "tcost icost 0.03125 0.0\n",
            "loss tensor([[0.1396]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.3569, -0.3610, -0.8616],\n",
            "         [ 0.7695,  0.1851, -0.6112],\n",
            "         [ 0.7183,  0.0475, -0.6941],\n",
            "         [ 0.6070, -0.2170, -0.7645],\n",
            "         [ 0.6666, -0.1660, -0.7267],\n",
            "         [ 0.8087,  0.0040,  0.5882]]], device='cuda:0') tensor([-0.1741, -0.7470,  0.3150,  0.4739, -0.5840, -0.3038], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1396484375\n",
            "ded\n",
            "time\n",
            "[3, 6, 6, 3, 11, 6, 6, 3, 6, 11, 11, 11, 3, 6, 11, 11, 11, 11, 11, 11, 6, 11, 11]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "\n",
        "env = TimeLimit(env, max_episode_steps=600)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6]) torch.Size([1, 256]\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # out = agent(state, zip(*out)[k:])\n",
        "            act = agent(state, k)\n",
        "            # act = zip(*out)[0].cpu()[0,:k].tolist()\n",
        "            # act = out[0].cpu()[0,:k].tolist()\n",
        "            # h0=lh0[k-1].unsqueeze(0)\n",
        "            # , lx, lz\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9cm6KjvBrnNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ffcf75-c2de-429e-d6da-6aa428848f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "         [ 0.8679,  0.2731,  0.4150],\n",
            "         [ 0.8303,  0.5176,  0.2065],\n",
            "         [ 0.9392,  0.3409, -0.0406],\n",
            "         [ 0.6632,  0.2170, -0.7163],\n",
            "         [ 0.7833,  0.3247,  0.5301]]], device='cuda:0') tensor([-0.2587,  0.7856, -0.3664,  0.1550,  0.0490, -0.2041], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.186279296875\n",
            "tcost icost 0.016021728515625 0.0\n",
            "tcost icost -0.2352294921875 0.0\n",
            "tcost icost -0.06414794921875 0.0\n",
            "tcost icost -0.1488037109375 0.0\n",
            "tcost icost -0.0271453857421875 0.0\n",
            "tcost icost -0.032257080078125 0.0\n",
            "loss tensor([[-0.3931]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9294, -0.2457, -0.2754],\n",
            "         [ 0.8793,  0.2521,  0.4040],\n",
            "         [ 0.8443,  0.4931,  0.2099],\n",
            "         [ 0.9498,  0.3113, -0.0323],\n",
            "         [ 0.6767,  0.2163, -0.7037],\n",
            "         [ 0.7890,  0.3353,  0.5148]]], device='cuda:0') tensor([ 0.2838,  0.6317, -0.2569,  0.0110,  0.1053, -0.2399], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.39306640625\n",
            "search tensor([[[ 0.6767,  0.2163, -0.7037],\n",
            "         [ 0.7890,  0.3353,  0.5148],\n",
            "         [ 0.3497,  0.2929, -0.2440],\n",
            "         [-0.4984,  0.4768,  0.1597],\n",
            "         [ 0.1394,  0.2813, -0.4383],\n",
            "         [-0.4733, -0.2210, -0.4673]]], device='cuda:0') tensor([[[ 0.1053],\n",
            "         [-0.2399],\n",
            "         [ 0.7562],\n",
            "         [ 0.9202],\n",
            "         [-0.8520],\n",
            "         [ 0.5724]]], device='cuda:0')\n",
            "tcost icost 0.01145172119140625 0.0\n",
            "tcost icost 0.01043701171875 0.0\n",
            "tcost icost -0.3037109375 0.0\n",
            "tcost icost -0.022125244140625 0.0\n",
            "tcost icost -0.034088134765625 0.0\n",
            "tcost icost -0.0625 0.0\n",
            "loss tensor([[-0.3005]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.7517,  0.3062, -0.5841],\n",
            "         [ 0.8813,  0.2332,  0.4110],\n",
            "         [ 0.4494,  0.3926, -0.1438],\n",
            "         [-0.5979,  0.3763,  0.0595],\n",
            "         [ 0.2393,  0.1810, -0.5379],\n",
            "         [-0.3728, -0.1208, -0.3668]]], device='cuda:0') tensor([-0.2834,  0.1471,  0.5962,  0.9683, -0.8295,  0.4968], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.300537109375\n",
            "tcost icost -0.0021648406982421875 0.0\n",
            "tcost icost -0.044708251953125 0.0\n",
            "tcost icost 0.1607666015625 0.0\n",
            "tcost icost 0.10675048828125 0.0\n",
            "tcost icost -0.055145263671875 0.0\n",
            "tcost icost -0.0270843505859375 0.0\n",
            "loss tensor([[0.1136]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.7419,  0.2868, -0.6060],\n",
            "         [ 0.9395,  0.1587,  0.3037],\n",
            "         [ 0.5487,  0.4059, -0.1560],\n",
            "         [-0.5405,  0.4331,  0.1103],\n",
            "         [ 0.3308,  0.0957, -0.6012],\n",
            "         [-0.2799, -0.0388, -0.2798]]], device='cuda:0') tensor([ 0.1724, -0.1125,  0.5742,  0.8179, -0.8117,  0.4877], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1136474609375\n",
            "tcost icost 0.01241302490234375 0.0\n",
            "tcost icost 0.01352691650390625 0.0\n",
            "tcost icost -0.255126953125 0.0\n",
            "tcost icost -0.019744873046875 0.0\n",
            "tcost icost -0.027679443359375 0.0\n",
            "tcost icost -0.0933837890625 0.0\n",
            "loss tensor([[-0.2698]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.7599,  0.3067, -0.5732],\n",
            "         [ 0.9778,  0.0727,  0.1964],\n",
            "         [ 0.6471,  0.4497, -0.1251],\n",
            "         [-0.5185,  0.4517,  0.1229],\n",
            "         [ 0.4230,  0.0049, -0.6403],\n",
            "         [-0.2022,  0.0461, -0.1985]]], device='cuda:0') tensor([-0.2851,  0.3167,  0.4088,  0.9073, -0.7815,  0.3643], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.269775390625\n",
            "tcost icost -0.0021820068359375 0.0\n",
            "tcost icost -0.0888671875 0.0\n",
            "tcost icost 0.13330078125 0.0\n",
            "tcost icost -0.0264434814453125 0.0\n",
            "tcost icost -0.060943603515625 0.0\n",
            "tcost icost -0.02978515625 0.0\n",
            "loss tensor([[-0.0510]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.7216,  0.2652, -0.6156],\n",
            "         [ 0.9775,  0.0900,  0.1909],\n",
            "         [ 0.7322,  0.4120, -0.1653],\n",
            "         [-0.4600,  0.5083,  0.1796],\n",
            "         [ 0.5164, -0.0847, -0.6523],\n",
            "         [-0.1235,  0.1264, -0.1175]]], device='cuda:0') tensor([ 1.0000, -0.8620,  1.0000,  0.6170, -0.7606,  0.3460], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.051025390625\n",
            "tcost icost -0.2183837890625 0.0\n",
            "tcost icost -0.05010986328125 0.0\n",
            "tcost icost -0.439453125 0.0\n",
            "tcost icost -0.0347900390625 0.0\n",
            "tcost icost -0.015838623046875 0.0\n",
            "tcost icost -0.1480712890625 0.0\n",
            "loss tensor([[-0.7422]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.6952,  0.2348, -0.6473],\n",
            "         [ 0.9771,  0.1038,  0.1856],\n",
            "         [ 0.8144,  0.3824, -0.1958],\n",
            "         [-0.4147,  0.5538,  0.2229],\n",
            "         [ 0.4854, -0.1607, -0.7047],\n",
            "         [-0.0404,  0.2125, -0.0313]]], device='cuda:0') tensor([ 0.7322, -0.8705,  0.8859,  0.6045, -0.6510,  0.2220], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.7421875\n",
            "tcost icost -0.1419677734375 0.0\n",
            "tcost icost -0.04052734375 0.0\n",
            "tcost icost -0.44580078125 0.0\n",
            "tcost icost -0.0340576171875 0.0\n",
            "tcost icost -0.018463134765625 0.0\n",
            "tcost icost -0.0780029296875 0.0\n",
            "loss tensor([[-0.6226]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.6789,  0.2137, -0.6704],\n",
            "         [ 0.9771,  0.1144,  0.1795],\n",
            "         [ 0.8935,  0.3593, -0.2196],\n",
            "         [-0.3802,  0.5912,  0.2548],\n",
            "         [ 0.4303, -0.2423, -0.7715],\n",
            "         [ 0.0472,  0.3018,  0.0585]]], device='cuda:0') tensor([ 0.4214, -0.8745,  0.7565,  0.5885, -0.5321,  0.0963], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.62255859375\n",
            "tcost icost -0.03082275390625 0.0\n",
            "tcost icost -0.0264434814453125 0.0\n",
            "tcost icost -0.447021484375 0.0\n",
            "tcost icost -0.0340576171875 0.0\n",
            "tcost icost -0.0212249755859375 0.0\n",
            "tcost icost -0.013885498046875 0.0\n",
            "loss tensor([[-0.4639]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.6754,  0.2040, -0.6833],\n",
            "         [ 0.9779,  0.1209,  0.1706],\n",
            "         [ 0.9185,  0.3239, -0.2270],\n",
            "         [-0.3529,  0.6219,  0.2702],\n",
            "         [ 0.3634, -0.3271, -0.8467],\n",
            "         [ 0.1373,  0.3901,  0.1444]]], device='cuda:0') tensor([-0.0544, -0.8559,  0.5902,  0.5741, -0.4198,  0.0152], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4638671875\n",
            "tcost icost 0.00795745849609375 0.0\n",
            "tcost icost -0.00555419921875 0.0\n",
            "tcost icost -0.397705078125 0.0\n",
            "tcost icost -0.03375244140625 0.0\n",
            "tcost icost -0.004436492919921875 0.0\n",
            "tcost icost 0.00939178466796875 0.0\n",
            "loss tensor([[-0.3411]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.6785,  0.2003, -0.6897],\n",
            "         [ 0.9806,  0.1195,  0.1552],\n",
            "         [ 0.9288,  0.2947, -0.2245],\n",
            "         [-0.3293,  0.6469,  0.2709],\n",
            "         [ 0.3105, -0.3651, -0.8777],\n",
            "         [ 0.2206,  0.4621,  0.1967]]], device='cuda:0') tensor([-0.1995, -0.6888,  0.4108,  0.6167, -0.4547,  0.0596], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.341064453125\n",
            "tcost icost 0.0011920928955078125 0.0\n",
            "tcost icost 0.0018186569213867188 0.0\n",
            "tcost icost -0.28076171875 0.0\n",
            "tcost icost -0.0242767333984375 0.0\n",
            "tcost icost -0.00707244873046875 0.0\n",
            "tcost icost 0.008697509765625 0.0\n",
            "loss tensor([[-0.2418]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.6886,  0.2027, -0.6901],\n",
            "         [ 0.9856,  0.1085,  0.1297],\n",
            "         [ 0.9361,  0.2729, -0.2221],\n",
            "         [-0.3024,  0.6730,  0.2735],\n",
            "         [ 0.2459, -0.3934, -0.8859],\n",
            "         [ 0.3018,  0.5234,  0.2237]]], device='cuda:0') tensor([-0.4103, -0.4761,  0.2139,  0.5245, -0.3726,  0.1002], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2418212890625\n",
            "tcost icost -0.006717681884765625 0.0\n",
            "tcost icost 0.0088043212890625 0.0\n",
            "tcost icost -0.050933837890625 0.0\n",
            "tcost icost 0.046966552734375 0.0\n",
            "tcost icost -0.0167083740234375 0.0\n",
            "tcost icost -0.028778076171875 0.0\n",
            "loss tensor([[-0.0338]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.7128,  0.2243, -0.6646],\n",
            "         [ 0.9938,  0.0736,  0.0830],\n",
            "         [ 0.9418,  0.2696, -0.2008],\n",
            "         [-0.2851,  0.6904,  0.2567],\n",
            "         [ 0.1817, -0.4228, -0.8878],\n",
            "         [ 0.3806,  0.5830,  0.2608]]], device='cuda:0') tensor([-1.0000,  0.4316, -0.2822,  0.6402, -0.3143,  0.0375], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.03375244140625\n",
            "tcost icost -0.02288818359375 0.0\n",
            "tcost icost -0.262939453125 0.0\n",
            "tcost icost -0.041595458984375 0.0\n",
            "tcost icost -0.391845703125 0.0\n",
            "tcost icost -0.07073974609375 0.0\n",
            "tcost icost -0.1654052734375 0.0\n",
            "loss tensor([[-0.7231]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.7275,  0.2371, -0.6439],\n",
            "         [ 0.9973,  0.0508,  0.0522],\n",
            "         [ 0.9477,  0.2619, -0.1826],\n",
            "         [-0.2557,  0.7130,  0.2612],\n",
            "         [ 0.1266, -0.4533, -0.8823],\n",
            "         [ 0.4602,  0.6416,  0.3117]]], device='cuda:0') tensor([-0.7234,  0.1760, -0.1338,  0.5904, -0.2640, -0.0650], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.72314453125\n",
            "tcost icost -0.0169830322265625 0.0\n",
            "tcost icost -0.15478515625 0.0\n",
            "tcost icost -0.0213470458984375 0.0\n",
            "tcost icost -0.3134765625 0.0\n",
            "tcost icost -0.0673828125 0.0\n",
            "tcost icost -0.09088134765625 0.0\n",
            "loss tensor([[-0.5000]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.7334,  0.2395, -0.6362],\n",
            "         [ 0.9984,  0.0419,  0.0380],\n",
            "         [ 0.9538,  0.2476, -0.1702],\n",
            "         [-0.2150,  0.7409,  0.2835],\n",
            "         [ 0.0790, -0.4849, -0.8710],\n",
            "         [ 0.5381,  0.6969,  0.3711]]], device='cuda:0') tensor([-0.2654, -0.2188,  0.0889,  0.5143, -0.2106, -0.1613], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5\n",
            "tcost icost -0.0014820098876953125 0.0\n",
            "tcost icost 0.02471923828125 0.0\n",
            "tcost icost 0.10955810546875 0.0\n",
            "tcost icost 0.1431884765625 0.0\n",
            "tcost icost -0.0188751220703125 0.0\n",
            "tcost icost -0.01450347900390625 0.0\n",
            "loss tensor([[0.1930]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 7.5326e-01,  2.6136e-01, -6.0356e-01],\n",
            "         [ 9.9996e-01,  8.7082e-03, -1.6765e-04],\n",
            "         [ 9.5745e-01,  2.5269e-01, -1.3945e-01],\n",
            "         [-1.7143e-01,  7.6685e-01,  2.9189e-01],\n",
            "         [ 3.3262e-02, -5.1510e-01, -8.5649e-01],\n",
            "         [ 5.7738e-01,  7.0821e-01,  4.0628e-01]]], device='cuda:0') tensor([-1.0000,  1.0000, -0.6353,  0.5552, -0.1633, -0.1933], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1929931640625\n",
            "tcost icost -0.02288818359375 0.0\n",
            "tcost icost -0.40185546875 0.0\n",
            "tcost icost -0.059478759765625 0.0\n",
            "tcost icost -0.373779296875 0.0\n",
            "tcost icost -0.06304931640625 0.0\n",
            "tcost icost -0.142333984375 0.0\n",
            "loss tensor([[-0.8306]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.7688,  0.2775, -0.5762],\n",
            "         [ 0.9995, -0.0165, -0.0284],\n",
            "         [ 0.9607,  0.2543, -0.1109],\n",
            "         [-0.1223,  0.7942,  0.3139],\n",
            "         [-0.0088, -0.5476, -0.8367],\n",
            "         [ 0.5849,  0.6888,  0.4282]]], device='cuda:0') tensor([-0.7866,  0.8376, -0.5560,  0.5030, -0.0992, -0.3188], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.83056640625\n",
            "tcost icost -0.018310546875 0.0\n",
            "tcost icost -0.345458984375 0.0\n",
            "tcost icost -0.058441162109375 0.0\n",
            "tcost icost -0.340087890625 0.0\n",
            "tcost icost -0.06158447265625 0.0\n",
            "tcost icost -0.0921630859375 0.0\n",
            "loss tensor([[-0.7192]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.7800,  0.2874, -0.5559],\n",
            "         [ 0.9983, -0.0335, -0.0468],\n",
            "         [ 0.9639,  0.2528, -0.0841],\n",
            "         [-0.0684,  0.8222,  0.3473],\n",
            "         [-0.0483, -0.5818, -0.8119],\n",
            "         [ 0.5865,  0.6711,  0.4535]]], device='cuda:0') tensor([-0.4995,  0.6511, -0.4581,  0.4444, -0.0293, -0.4396], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.71923828125\n",
            "tcost icost -0.01059722900390625 0.0\n",
            "tcost icost -0.2261962890625 0.0\n",
            "tcost icost -0.04437255859375 0.0\n",
            "tcost icost -0.30029296875 0.0\n",
            "tcost icost -0.055999755859375 0.0\n",
            "tcost icost -0.04534912109375 0.0\n",
            "loss tensor([[-0.5327]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.7851,  0.2888, -0.5479],\n",
            "         [ 0.9975, -0.0422, -0.0562],\n",
            "         [ 0.9671,  0.2469, -0.0611],\n",
            "         [-0.0093,  0.8506,  0.3893],\n",
            "         [-0.0857, -0.6155, -0.7834],\n",
            "         [ 0.5847,  0.6546,  0.4793]]], device='cuda:0') tensor([-0.0112,  0.4305, -0.3101,  0.3575,  0.0319, -0.5142], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.53271484375\n",
            "tcost icost 0.00905609130859375 0.0\n",
            "tcost icost 0.018707275390625 0.0\n",
            "tcost icost 0.018585205078125 0.0\n",
            "tcost icost -0.1383056640625 0.0\n",
            "tcost icost -0.031341552734375 0.0\n",
            "tcost icost -0.0081024169921875 0.0\n",
            "loss tensor([[-0.0852]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.7755,  0.2715, -0.5673],\n",
            "         [ 0.9982, -0.0328, -0.0496],\n",
            "         [ 0.9726,  0.2242, -0.0607],\n",
            "         [ 0.0550,  0.8798,  0.4393],\n",
            "         [-0.1215, -0.6461, -0.7535],\n",
            "         [ 0.5830,  0.6390,  0.5017]]], device='cuda:0') tensor([ 1.0000, -0.3927,  0.1977,  0.1607,  0.0846, -0.5250], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.085205078125\n",
            "tcost icost -0.2183837890625 0.0\n",
            "tcost icost -0.04278564453125 0.0\n",
            "tcost icost -0.32373046875 0.0\n",
            "tcost icost -0.03387451171875 0.0\n",
            "tcost icost -0.1483154296875 0.0\n",
            "tcost icost -0.0222320556640625 0.0\n",
            "loss tensor([[-0.6543]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.7731,  0.2607, -0.5782],\n",
            "         [ 0.9987, -0.0258, -0.0432],\n",
            "         [ 0.9765,  0.2095, -0.0508],\n",
            "         [ 0.1148,  0.8787,  0.4634],\n",
            "         [-0.1198, -0.6528, -0.7480],\n",
            "         [ 0.5842,  0.6245,  0.5184]]], device='cuda:0') tensor([ 0.7283, -0.2411,  0.0701,  0.3034, -0.0867, -0.5114], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.654296875\n",
            "tcost icost -0.140869140625 0.0\n",
            "tcost icost -0.0244140625 0.0\n",
            "tcost icost -0.2919921875 0.0\n",
            "tcost icost -0.0248565673828125 0.0\n",
            "tcost icost -0.061920166015625 0.0\n",
            "tcost icost -0.00942230224609375 0.0\n",
            "loss tensor([[-0.4636]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.7755,  0.2570, -0.5767],\n",
            "         [ 0.9990, -0.0230, -0.0387],\n",
            "         [ 0.9785,  0.2039, -0.0318],\n",
            "         [ 0.1713,  0.8667,  0.4684],\n",
            "         [-0.0955, -0.6414, -0.7613],\n",
            "         [ 0.5896,  0.6104,  0.5290]]], device='cuda:0') tensor([ 0.3276,  0.0209, -0.1122,  0.4892, -0.2837, -0.4904], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.463623046875\n",
            "tcost icost 0.0006151199340820312 0.0\n",
            "tcost icost 0.00965118408203125 0.0\n",
            "tcost icost -0.115234375 0.0\n",
            "tcost icost 0.0243988037109375 0.0\n",
            "tcost icost -0.0101165771484375 0.0\n",
            "tcost icost -2.9206275939941406e-06 0.0\n",
            "loss tensor([[-0.0729]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.7852,  0.2638, -0.5602],\n",
            "         [ 0.9985, -0.0307, -0.0456],\n",
            "         [ 0.9781,  0.2080, -0.0066],\n",
            "         [ 0.2245,  0.8550,  0.4675],\n",
            "         [-0.0757, -0.6317, -0.7715],\n",
            "         [ 0.5966,  0.5973,  0.5360]]], device='cuda:0') tensor([-0.3843,  0.5487, -0.4540,  0.5327, -0.2450, -0.4811], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.07293701171875\n",
            "search tensor([[[-0.0757, -0.6317, -0.7715],\n",
            "         [ 0.5966,  0.5973,  0.5360],\n",
            "         [ 0.1529,  0.1501, -0.2380],\n",
            "         [ 0.3861, -0.2853, -0.1573],\n",
            "         [ 0.0011,  0.2043,  0.4942],\n",
            "         [ 0.2723, -0.3002, -0.3153]]], device='cuda:0') tensor([[[-0.2450],\n",
            "         [-0.4811],\n",
            "         [ 0.3555],\n",
            "         [-0.2241],\n",
            "         [ 0.5094],\n",
            "         [ 0.3917]]], device='cuda:0')\n",
            "tcost icost 0.01568603515625 0.0\n",
            "tcost icost 0.0692138671875 0.0\n",
            "tcost icost 0.1939697265625 0.0\n",
            "tcost icost -0.01029205322265625 0.0\n",
            "tcost icost -0.130126953125 0.0\n",
            "tcost icost -0.00748443603515625 0.0\n",
            "loss tensor([[0.1379]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.1526, -0.6355, -0.7569],\n",
            "         [ 0.6960,  0.4967,  0.4355],\n",
            "         [ 0.2528,  0.2500, -0.1378],\n",
            "         [ 0.2857, -0.3850, -0.2571],\n",
            "         [ 0.1011,  0.3041,  0.5937],\n",
            "         [ 0.1721, -0.3999, -0.4150]]], device='cuda:0') tensor([-0.0374, -0.0187, -0.0200, -0.0272,  0.3537,  0.4224], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.137939453125\n",
            "tcost icost 0.02960205078125 0.0\n",
            "tcost icost 0.073974609375 0.0\n",
            "tcost icost 0.0163116455078125 0.0\n",
            "tcost icost 0.028289794921875 0.0\n",
            "tcost icost 0.289306640625 0.0\n",
            "tcost icost 0.06817626953125 0.0\n",
            "loss tensor([[0.3601]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.2280, -0.6969, -0.6799],\n",
            "         [ 0.7348,  0.5248,  0.4297],\n",
            "         [ 0.2869,  0.2347, -0.1877],\n",
            "         [ 0.3481, -0.3605, -0.3533],\n",
            "         [ 0.1944,  0.3451,  0.5810],\n",
            "         [ 0.0935, -0.4802, -0.4947]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.2730, -0.2380,  0.3498,  0.6750], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.360107421875\n",
            "tcost icost -0.099365234375 0.0\n",
            "tcost icost -0.0653076171875 0.0\n",
            "tcost icost -0.273193359375 0.0\n",
            "tcost icost -0.041351318359375 0.0\n",
            "tcost icost -0.285888671875 0.0\n",
            "tcost icost -0.01837158203125 0.0\n",
            "loss tensor([[-0.6079]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.2753, -0.7343, -0.6205],\n",
            "         [ 0.7482,  0.5196,  0.4126],\n",
            "         [ 0.3327,  0.2349, -0.2117],\n",
            "         [ 0.3987, -0.3509, -0.4267],\n",
            "         [ 0.2791,  0.4007,  0.5948],\n",
            "         [ 0.0281, -0.5471, -0.5602]]], device='cuda:0') tensor([ 0.8605, -0.8945,  0.1878, -0.1575,  0.2181,  0.6913], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.60791015625\n",
            "tcost icost -0.067626953125 0.0\n",
            "tcost icost -0.05487060546875 0.0\n",
            "tcost icost -0.271728515625 0.0\n",
            "tcost icost -0.038543701171875 0.0\n",
            "tcost icost -0.252197265625 0.0\n",
            "tcost icost -0.0116729736328125 0.0\n",
            "loss tensor([[-0.5376]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.3092, -0.7636, -0.5668],\n",
            "         [ 0.7667,  0.5044,  0.3973],\n",
            "         [ 0.3872,  0.2464, -0.2181],\n",
            "         [ 0.4390, -0.3558, -0.4875],\n",
            "         [ 0.3603,  0.4607,  0.6195],\n",
            "         [-0.0264, -0.6028, -0.6156]]], device='cuda:0') tensor([ 0.6697, -0.7482,  0.0911, -0.0576,  0.0816,  0.6908], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.53759765625\n",
            "tcost icost -0.01345062255859375 0.0\n",
            "tcost icost -0.0401611328125 0.0\n",
            "tcost icost -0.261962890625 0.0\n",
            "tcost icost -0.03594970703125 0.0\n",
            "tcost icost -0.2060546875 0.0\n",
            "tcost icost -0.0037841796875 0.0\n",
            "loss tensor([[-0.4250]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.3370, -0.7934, -0.5069],\n",
            "         [ 0.7917,  0.4787,  0.3796],\n",
            "         [ 0.4488,  0.2676, -0.2105],\n",
            "         [ 0.4697, -0.3738, -0.5417],\n",
            "         [ 0.4374,  0.5183,  0.6472],\n",
            "         [-0.0734, -0.6505, -0.6644]]], device='cuda:0') tensor([ 0.3232, -0.5317, -0.0297,  0.0601, -0.0690,  0.6899], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.425048828125\n",
            "tcost icost 0.0469970703125 0.0\n",
            "tcost icost -0.004520416259765625 0.0\n",
            "tcost icost -0.15771484375 0.0\n",
            "tcost icost -0.02349853515625 0.0\n",
            "tcost icost -0.10565185546875 0.0\n",
            "tcost icost 0.04754638671875 0.0\n",
            "loss tensor([[-0.1433]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.3656, -0.8239, -0.4331],\n",
            "         [ 0.8340,  0.4323,  0.3428],\n",
            "         [ 0.5167,  0.3026, -0.1862],\n",
            "         [ 0.4840, -0.4105, -0.6008],\n",
            "         [ 0.4977,  0.5598,  0.6626],\n",
            "         [-0.1229, -0.6952, -0.7082]]], device='cuda:0') tensor([-0.2329,  0.0587, -0.2387,  0.3091, -0.3180,  0.7727], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.143310546875\n",
            "tcost icost 0.0166778564453125 0.0\n",
            "tcost icost -0.0830078125 0.0\n",
            "tcost icost -0.0167083740234375 0.0\n",
            "tcost icost -0.1458740234375 0.0\n",
            "tcost icost -0.031707763671875 0.0\n",
            "tcost icost -0.183837890625 0.0\n",
            "loss tensor([[-0.3074]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.3767, -0.8284, -0.4145],\n",
            "         [ 0.8515,  0.4058,  0.3322],\n",
            "         [ 0.5749,  0.3210, -0.1715],\n",
            "         [ 0.5217, -0.4168, -0.6250],\n",
            "         [ 0.5304,  0.5593,  0.6371],\n",
            "         [-0.1322, -0.6892, -0.7124]]], device='cuda:0') tensor([ 0.5898, -0.3730,  0.0575,  0.1730, -0.2223,  0.6831], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.307373046875\n",
            "tcost icost 0.0097808837890625 0.0\n",
            "tcost icost -0.018035888671875 0.0\n",
            "tcost icost -0.1966552734375 0.0\n",
            "tcost icost -0.02777099609375 0.0\n",
            "tcost icost -0.064208984375 0.0\n",
            "tcost icost 0.0709228515625 0.0\n",
            "loss tensor([[-0.1863]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.3904, -0.8420, -0.3723],\n",
            "         [ 0.8785,  0.3643,  0.3090],\n",
            "         [ 0.6384,  0.3486, -0.1467],\n",
            "         [ 0.5417, -0.4385, -0.6591],\n",
            "         [ 0.5645,  0.5559,  0.6102],\n",
            "         [-0.1297, -0.6778, -0.7237]]], device='cuda:0') tensor([-0.0225,  0.1290, -0.1441,  0.3975, -0.4082,  0.6459], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.186279296875\n",
            "tcost icost 0.030609130859375 0.0\n",
            "tcost icost 0.0357666015625 0.0\n",
            "tcost icost -0.002361297607421875 0.0\n",
            "tcost icost -0.076904296875 0.0\n",
            "tcost icost -0.026641845703125 0.0\n",
            "tcost icost -0.161376953125 0.0\n",
            "loss tensor([[-0.1079]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.3871, -0.8412, -0.3776],\n",
            "         [ 0.8827,  0.3522,  0.3111],\n",
            "         [ 0.6946,  0.3583, -0.1347],\n",
            "         [ 0.5803, -0.4362, -0.6683],\n",
            "         [ 0.6022,  0.5469,  0.5815],\n",
            "         [-0.1103, -0.6616, -0.7417]]], device='cuda:0') tensor([ 1.0000, -0.5600,  0.2424,  0.2274, -0.3121,  0.5518], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.10791015625\n",
            "tcost icost -0.099365234375 0.0\n",
            "tcost icost -0.05352783203125 0.0\n",
            "tcost icost -0.26416015625 0.0\n",
            "tcost icost -0.033172607421875 0.0\n",
            "tcost icost -0.055877685546875 0.0\n",
            "tcost icost 0.036163330078125 0.0\n",
            "loss tensor([[-0.4011]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.3844, -0.8440, -0.3741],\n",
            "         [ 0.8892,  0.3353,  0.3112],\n",
            "         [ 0.7538,  0.3731, -0.1166],\n",
            "         [ 0.5951, -0.4365, -0.6748],\n",
            "         [ 0.6446,  0.5358,  0.5453],\n",
            "         [-0.0828, -0.6368, -0.7666]]], device='cuda:0') tensor([ 0.8123, -0.3721,  0.0671,  0.3839, -0.3663,  0.4678], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.401123046875\n",
            "tcost icost -0.055908203125 0.0\n",
            "tcost icost -0.03472900390625 0.0\n",
            "tcost icost -0.2149658203125 0.0\n",
            "tcost icost -0.0229949951171875 0.0\n",
            "tcost icost -0.0249176025390625 0.0\n",
            "tcost icost 0.09796142578125 0.0\n",
            "loss tensor([[-0.2366]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.3820, -0.8512, -0.3599],\n",
            "         [ 0.9008,  0.3104,  0.3036],\n",
            "         [ 0.8158,  0.3924, -0.0948],\n",
            "         [ 0.5862, -0.4359, -0.6829],\n",
            "         [ 0.6976,  0.5179,  0.4950],\n",
            "         [-0.0439, -0.5992, -0.7994]]], device='cuda:0') tensor([ 0.4114, -0.0880, -0.2146,  0.4914, -0.3309,  0.3608], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.236572265625\n",
            "tcost icost 0.040863037109375 0.0\n",
            "tcost icost 0.00959014892578125 0.0\n",
            "tcost icost 0.1060791015625 0.0\n",
            "tcost icost 0.08740234375 0.0\n",
            "tcost icost -0.004764556884765625 0.0\n",
            "tcost icost 0.03265380859375 0.0\n",
            "loss tensor([[0.2153]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.3764, -0.8676, -0.3249],\n",
            "         [ 0.9286,  0.2611,  0.2635],\n",
            "         [ 0.8854,  0.4176, -0.0689],\n",
            "         [ 0.5790, -0.4260, -0.6952],\n",
            "         [ 0.7623,  0.4799,  0.4343],\n",
            "         [ 0.0091, -0.5440, -0.8390]]], device='cuda:0') tensor([-1.0000,  1.0000, -0.6746,  0.4290,  0.1454,  0.2081], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.21533203125\n",
            "tcost icost -0.01174163818359375 0.0\n",
            "tcost icost -0.447021484375 0.0\n",
            "tcost icost -0.0692138671875 0.0\n",
            "tcost icost -0.3056640625 0.0\n",
            "tcost icost -0.039886474609375 0.0\n",
            "tcost icost -0.06671142578125 0.0\n",
            "loss tensor([[-0.7588]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.3679, -0.8816, -0.2957],\n",
            "         [ 0.9481,  0.2186,  0.2307],\n",
            "         [ 0.9073,  0.4183, -0.0422],\n",
            "         [ 0.5897, -0.4100, -0.6958],\n",
            "         [ 0.8176,  0.4374,  0.3743],\n",
            "         [ 0.0651, -0.4839, -0.8727]]], device='cuda:0') tensor([-0.7638,  0.9149, -0.6117,  0.3361,  0.2302,  0.1283], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.7587890625\n",
            "tcost icost -0.00441741943359375 0.0\n",
            "tcost icost -0.408935546875 0.0\n",
            "tcost icost -0.07098388671875 0.0\n",
            "tcost icost -0.25341796875 0.0\n",
            "tcost icost -0.03369140625 0.0\n",
            "tcost icost -0.0343017578125 0.0\n",
            "loss tensor([[-0.6567]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.3567, -0.8930, -0.2745],\n",
            "         [ 0.9614,  0.1835,  0.2051],\n",
            "         [ 0.9121,  0.4096, -0.0180],\n",
            "         [ 0.6152, -0.3870, -0.6869],\n",
            "         [ 0.8640,  0.3924,  0.3157],\n",
            "         [ 0.1204, -0.4220, -0.8986]]], device='cuda:0') tensor([-0.4301,  0.8196, -0.5461,  0.2260,  0.2980,  0.0820], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.65673828125\n",
            "tcost icost 0.005504608154296875 0.0\n",
            "tcost icost -0.310302734375 0.0\n",
            "tcost icost -0.07073974609375 0.0\n",
            "tcost icost -0.182861328125 0.0\n",
            "tcost icost -0.024078369140625 0.0\n",
            "tcost icost -0.016998291015625 0.0\n",
            "loss tensor([[-0.4902]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.3441, -0.9011, -0.2638],\n",
            "         [ 0.9699,  0.1563,  0.1865],\n",
            "         [ 0.9168,  0.3994,  0.0030],\n",
            "         [ 0.6523, -0.3557, -0.6693],\n",
            "         [ 0.9013,  0.3469,  0.2594],\n",
            "         [ 0.1715, -0.3619, -0.9163]]], device='cuda:0') tensor([ 0.0219,  0.6995, -0.4771,  0.0916,  0.3322,  0.0683], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.490234375\n",
            "tcost icost 0.03436279296875 0.0\n",
            "tcost icost -0.125732421875 0.0\n",
            "tcost icost -0.0517578125 0.0\n",
            "tcost icost -0.08538818359375 0.0\n",
            "tcost icost -0.00734710693359375 0.0\n",
            "tcost icost -0.0021228790283203125 0.0\n",
            "loss tensor([[-0.1890]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.3313, -0.9038, -0.2707],\n",
            "         [ 0.9742,  0.1405,  0.1767],\n",
            "         [ 0.9224,  0.3858,  0.0189],\n",
            "         [ 0.6991, -0.3137, -0.6425],\n",
            "         [ 0.9311,  0.3017,  0.2052],\n",
            "         [ 0.2158, -0.3072, -0.9268]]], device='cuda:0') tensor([ 0.8304,  0.4573, -0.3610, -0.1162,  0.3543,  0.1005], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.18896484375\n",
            "tcost icost -0.060577392578125 0.0\n",
            "tcost icost -0.00018131732940673828 0.0\n",
            "tcost icost -0.0018644332885742188 0.0\n",
            "tcost icost -0.002986907958984375 0.0\n",
            "tcost icost -0.0914306640625 0.0\n",
            "tcost icost -0.0089263916015625 0.0\n",
            "loss tensor([[-0.1298]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.3124, -0.9123, -0.2648],\n",
            "         [ 0.9794,  0.1190,  0.1630],\n",
            "         [ 0.9244,  0.3795,  0.0387],\n",
            "         [ 0.7234, -0.2886, -0.6272],\n",
            "         [ 0.9477,  0.2719,  0.1672],\n",
            "         [ 0.2511, -0.2612, -0.9321]]], device='cuda:0') tensor([ 0.2733,  0.7297, -0.4863,  0.1436,  0.1694,  0.1381], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1297607421875\n",
            "tcost icost 0.0496826171875 0.0\n",
            "tcost icost 0.0238494873046875 0.0\n",
            "tcost icost -0.0357666015625 0.0\n",
            "tcost icost -0.050567626953125 0.0\n",
            "tcost icost -0.004070281982421875 0.0\n",
            "tcost icost -0.0018987655639648438 0.0\n",
            "loss tensor([[0.0015]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.2938, -0.9151, -0.2760],\n",
            "         [ 0.9805,  0.1129,  0.1611],\n",
            "         [ 0.9281,  0.3682,  0.0555],\n",
            "         [ 0.7616, -0.2511, -0.5974],\n",
            "         [ 0.9658,  0.2292,  0.1214],\n",
            "         [ 0.2955, -0.2049, -0.9331]]], device='cuda:0') tensor([ 1.0000,  0.3652, -0.3192, -0.0733,  0.3981,  0.0307], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.00151824951171875\n",
            "tcost icost -0.099365234375 0.0\n",
            "tcost icost -0.016265869140625 0.0\n",
            "tcost icost 0.0034770965576171875 0.0\n",
            "tcost icost -0.004344940185546875 0.0\n",
            "tcost icost -0.1461181640625 0.0\n",
            "tcost icost -0.018341064453125 0.0\n",
            "loss tensor([[-0.2211]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.2681, -0.9227, -0.2771],\n",
            "         [ 0.9823,  0.1024,  0.1569],\n",
            "         [ 0.9289,  0.3624,  0.0760],\n",
            "         [ 0.7866, -0.2259, -0.5747],\n",
            "         [ 0.9753,  0.2013,  0.0909],\n",
            "         [ 0.3318, -0.1561, -0.9303]]], device='cuda:0') tensor([ 0.6035,  0.5791, -0.3971,  0.1247,  0.2448,  0.0536], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2210693359375\n",
            "tcost icost 0.006046295166015625 0.0\n",
            "tcost icost 0.0775146484375 0.0\n",
            "tcost icost -0.00821685791015625 0.0\n",
            "tcost icost -0.0094757080078125 0.0\n",
            "tcost icost 0.13330078125 0.0\n",
            "tcost icost 0.0576171875 0.0\n",
            "loss tensor([[0.1837]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.2287, -0.9330, -0.2778],\n",
            "         [ 0.9840,  0.0935,  0.1517],\n",
            "         [ 0.9355,  0.3443,  0.0787],\n",
            "         [ 0.8194, -0.1840, -0.5429],\n",
            "         [ 0.9837,  0.1704,  0.0571],\n",
            "         [ 0.3416, -0.1350, -0.9301]]], device='cuda:0') tensor([ 0.3186,  0.5246, -0.2010, -0.2721,  0.2747,  0.3030], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1837158203125\n",
            "search tensor([[[ 0.9837,  0.1704,  0.0571],\n",
            "         [ 0.3416, -0.1350, -0.9301],\n",
            "         [-0.4415,  0.4685, -0.4108],\n",
            "         [ 0.0203, -0.4090, -0.1996],\n",
            "         [ 0.3126,  0.0600, -0.4836],\n",
            "         [ 0.2363,  0.1087, -0.4110]]], device='cuda:0') tensor([[[ 0.2747],\n",
            "         [ 0.3030],\n",
            "         [-0.0570],\n",
            "         [ 0.5824],\n",
            "         [-0.3029],\n",
            "         [ 0.9138]]], device='cuda:0')\n",
            "tcost icost -0.007694244384765625 0.0\n",
            "tcost icost 0.0139617919921875 0.0\n",
            "tcost icost 0.2237548828125 0.0\n",
            "tcost icost 0.1541748046875 0.0\n",
            "tcost icost -0.0156707763671875 0.0\n",
            "tcost icost -0.189453125 0.0\n",
            "loss tensor([[0.1765]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.9608,  0.2398,  0.1393],\n",
            "         [ 0.2228, -0.2169, -0.9504],\n",
            "         [-0.3411,  0.5680, -0.3104],\n",
            "         [-0.0797, -0.5086, -0.2994],\n",
            "         [ 0.2123, -0.0400, -0.5831],\n",
            "         [ 0.3360,  0.2086, -0.3106]]], device='cuda:0') tensor([-1.0000,  1.0000, -1.0000,  0.8062, -0.1647,  0.8272], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.176513671875\n",
            "tcost icost -0.013885498046875 0.0\n",
            "tcost icost -0.41259765625 0.0\n",
            "tcost icost -0.0743408203125 0.0\n",
            "tcost icost -0.34228515625 0.0\n",
            "tcost icost -0.055999755859375 0.0\n",
            "tcost icost -0.26806640625 0.0\n",
            "loss tensor([[-0.8901]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.9467,  0.2676,  0.1795],\n",
            "         [ 0.1577, -0.2592, -0.9529],\n",
            "         [-0.2714,  0.6351, -0.2389],\n",
            "         [-0.0587, -0.5283, -0.3092],\n",
            "         [ 0.1300, -0.1364, -0.6384],\n",
            "         [ 0.4357,  0.3084, -0.2120]]], device='cuda:0') tensor([-0.6917,  0.8815, -0.9421,  0.7541, -0.1036,  0.7362], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.89013671875\n",
            "tcost icost -0.0076446533203125 0.0\n",
            "tcost icost -0.32421875 0.0\n",
            "tcost icost -0.07537841796875 0.0\n",
            "tcost icost -0.29248046875 0.0\n",
            "tcost icost -0.050689697265625 0.0\n",
            "tcost icost -0.227783203125 0.0\n",
            "loss tensor([[-0.7412]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.9419,  0.2729,  0.1959],\n",
            "         [ 0.1203, -0.2819, -0.9519],\n",
            "         [-0.2145,  0.6880, -0.1775],\n",
            "         [-0.0032, -0.5134, -0.2817],\n",
            "         [ 0.0432, -0.2334, -0.6772],\n",
            "         [ 0.5348,  0.4079, -0.1129]]], device='cuda:0') tensor([-0.2698,  0.7524, -0.8661,  0.7037, -0.0232,  0.6346], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.7412109375\n",
            "tcost icost 0.0014619827270507812 0.0\n",
            "tcost icost -0.144775390625 0.0\n",
            "tcost icost -0.0552978515625 0.0\n",
            "tcost icost -0.256591796875 0.0\n",
            "tcost icost -0.041259765625 0.0\n",
            "tcost icost -0.2235107421875 0.0\n",
            "loss tensor([[-0.5195]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9438,  0.2651,  0.1975],\n",
            "         [ 0.1014, -0.2917, -0.9511],\n",
            "         [-0.1653,  0.7312, -0.1228],\n",
            "         [ 0.0685, -0.4751, -0.2339],\n",
            "         [-0.0463, -0.3309, -0.7077],\n",
            "         [ 0.6318,  0.5059, -0.0136]]], device='cuda:0') tensor([ 0.1631,  0.6082, -0.7411,  0.6285,  0.0661,  0.5330], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.51953125\n",
            "tcost icost 0.01035308837890625 0.0\n",
            "tcost icost 0.086669921875 0.0\n",
            "tcost icost -0.0111083984375 0.0\n",
            "tcost icost -0.20654296875 0.0\n",
            "tcost icost -0.027557373046875 0.0\n",
            "tcost icost -0.1824951171875 0.0\n",
            "loss tensor([[-0.1970]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9474,  0.2539,  0.1946],\n",
            "         [ 0.0937, -0.2932, -0.9514],\n",
            "         [-0.1262,  0.7588, -0.1087],\n",
            "         [ 0.1488, -0.4181, -0.1733],\n",
            "         [-0.1386, -0.4290, -0.7377],\n",
            "         [ 0.7241,  0.6007,  0.0854]]], device='cuda:0') tensor([ 0.1982,  0.4619, -0.4484,  0.4992,  0.1850,  0.4328], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.197021484375\n",
            "tcost icost 0.007122039794921875 0.0\n",
            "tcost icost 0.06378173828125 0.0\n",
            "tcost icost 0.01617431640625 0.0\n",
            "tcost icost -0.04052734375 0.0\n",
            "tcost icost -0.01056671142578125 0.0\n",
            "tcost icost -0.065185546875 0.0\n",
            "loss tensor([[0.0027]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9569,  0.2310,  0.1762],\n",
            "         [ 0.1090, -0.2725, -0.9560],\n",
            "         [-0.1040,  0.7508, -0.1497],\n",
            "         [ 0.2293, -0.3494, -0.1012],\n",
            "         [-0.2311, -0.5182, -0.7709],\n",
            "         [ 0.7503,  0.6388,  0.1702]]], device='cuda:0') tensor([ 0.5785, -0.2974,  0.5106,  0.1186,  0.4713,  0.2915], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.002685546875\n",
            "tcost icost -0.0814208984375 0.0\n",
            "tcost icost -0.03314208984375 0.0\n",
            "tcost icost -0.341796875 0.0\n",
            "tcost icost -0.045745849609375 0.0\n",
            "tcost icost -0.1995849609375 0.0\n",
            "tcost icost -0.01519012451171875 0.0\n",
            "loss tensor([[-0.5615]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9600,  0.2214,  0.1713],\n",
            "         [ 0.1206, -0.2587, -0.9584],\n",
            "         [-0.0796,  0.7483, -0.1777],\n",
            "         [ 0.3014, -0.2928, -0.0376],\n",
            "         [-0.2460, -0.5534, -0.7958],\n",
            "         [ 0.7391,  0.6349,  0.2251]]], device='cuda:0') tensor([ 0.2628, -0.0532,  0.4422,  0.2233,  0.3242,  0.3262], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5615234375\n",
            "tcost icost -0.004962921142578125 0.0\n",
            "tcost icost -0.0010671615600585938 0.0\n",
            "tcost icost -0.2244873046875 0.0\n",
            "tcost icost -0.0335693359375 0.0\n",
            "tcost icost -0.08056640625 0.0\n",
            "tcost icost 0.008697509765625 0.0\n",
            "loss tensor([[-0.2600]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9558,  0.2296,  0.1835],\n",
            "         [ 0.1239, -0.2558, -0.9588],\n",
            "         [-0.0520,  0.7523, -0.1930],\n",
            "         [ 0.3660, -0.2500,  0.0129],\n",
            "         [-0.2207, -0.5482, -0.8067],\n",
            "         [ 0.7330,  0.6269,  0.2641]]], device='cuda:0') tensor([-0.3543,  0.3873,  0.3046,  0.3630,  0.1723,  0.3401], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.260009765625\n",
            "tcost icost -0.0008325576782226562 0.0\n",
            "tcost icost -0.09588623046875 0.0\n",
            "tcost icost 0.0301971435546875 0.0\n",
            "tcost icost 0.0611572265625 0.0\n",
            "tcost icost -0.020843505859375 0.0\n",
            "tcost icost -0.04498291015625 0.0\n",
            "loss tensor([[-0.0583]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9632,  0.2102,  0.1673],\n",
            "         [ 0.1523, -0.2307, -0.9610],\n",
            "         [-0.0291,  0.7382, -0.2278],\n",
            "         [ 0.4378, -0.1931,  0.0732],\n",
            "         [-0.1994, -0.5514, -0.8100],\n",
            "         [ 0.7225,  0.6215,  0.3029]]], device='cuda:0') tensor([ 0.8586, -0.5317,  0.9843,  0.1509,  0.2942,  0.2250], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.058349609375\n",
            "tcost icost -0.1357421875 0.0\n",
            "tcost icost -0.0555419921875 0.0\n",
            "tcost icost -0.39453125 0.0\n",
            "tcost icost -0.04656982421875 0.0\n",
            "tcost icost -0.150390625 0.0\n",
            "tcost icost -0.01041412353515625 0.0\n",
            "loss tensor([[-0.6440]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9675,  0.1972,  0.1584],\n",
            "         [ 0.1782, -0.2096, -0.9614],\n",
            "         [-0.0047,  0.7295, -0.2529],\n",
            "         [ 0.5045, -0.1458,  0.1268],\n",
            "         [-0.1580, -0.5389, -0.8274],\n",
            "         [ 0.7164,  0.6138,  0.3318]]], device='cuda:0') tensor([ 0.7117, -0.3936,  0.9412,  0.2654,  0.1410,  0.2432], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.64404296875\n",
            "tcost icost -0.1082763671875 0.0\n",
            "tcost icost -0.04449462890625 0.0\n",
            "tcost icost -0.395263671875 0.0\n",
            "tcost icost -0.043548583984375 0.0\n",
            "tcost icost -0.0889892578125 0.0\n",
            "tcost icost -0.0018100738525390625 0.0\n",
            "loss tensor([[-0.5596]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9692,  0.1906,  0.1562],\n",
            "         [ 0.2011, -0.1926, -0.9605],\n",
            "         [ 0.0218,  0.7256, -0.2700],\n",
            "         [ 0.5670, -0.1079,  0.1723],\n",
            "         [-0.1051, -0.5158, -0.8503],\n",
            "         [ 0.7123,  0.6060,  0.3541]]], device='cuda:0') tensor([ 0.5063, -0.2308,  0.8718,  0.3735,  0.0250,  0.2262], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5595703125\n",
            "tcost icost -0.064453125 0.0\n",
            "tcost icost -0.0256195068359375 0.0\n",
            "tcost icost -0.371337890625 0.0\n",
            "tcost icost -0.040191650390625 0.0\n",
            "tcost icost -0.040496826171875 0.0\n",
            "tcost icost 0.0030803680419921875 0.0\n",
            "loss tensor([[-0.4424]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9683,  0.1913,  0.1607],\n",
            "         [ 0.2193, -0.1800, -0.9589],\n",
            "         [ 0.0521,  0.7257, -0.2819],\n",
            "         [ 0.6256, -0.0778,  0.2087],\n",
            "         [-0.0501, -0.4894, -0.8706],\n",
            "         [ 0.7111,  0.5979,  0.3700]]], device='cuda:0') tensor([ 0.1347, -0.0317,  0.7366,  0.4368,  0.0103,  0.1987], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4423828125\n",
            "tcost icost 0.01035308837890625 0.0\n",
            "tcost icost 0.00811004638671875 0.0\n",
            "tcost icost -0.2353515625 0.0\n",
            "tcost icost -0.032745361328125 0.0\n",
            "tcost icost -0.018157958984375 0.0\n",
            "tcost icost 0.01157379150390625 0.0\n",
            "loss tensor([[-0.2019]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9646,  0.1996,  0.1723],\n",
            "         [ 0.2274, -0.1751, -0.9579],\n",
            "         [ 0.0853,  0.7289, -0.2910],\n",
            "         [ 0.6821, -0.0520,  0.2356],\n",
            "         [ 0.0012, -0.4612, -0.8873],\n",
            "         [ 0.7212,  0.5852,  0.3707]]], device='cuda:0') tensor([-0.2169,  0.2913,  0.4802,  0.4105,  0.0395,  0.2652], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.201904296875\n",
            "tcost icost 0.0028362274169921875 0.0\n",
            "tcost icost 0.0142974853515625 0.0\n",
            "tcost icost 0.192138671875 0.0\n",
            "tcost icost 0.047210693359375 0.0\n",
            "tcost icost -0.0176239013671875 0.0\n",
            "tcost icost -0.03729248046875 0.0\n",
            "loss tensor([[0.1722]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9686,  0.1882,  0.1624],\n",
            "         [ 0.2465, -0.1587, -0.9561],\n",
            "         [ 0.1264,  0.7349, -0.2976],\n",
            "         [ 0.7352, -0.0401,  0.2515],\n",
            "         [ 0.0348, -0.4416, -0.8965],\n",
            "         [ 0.7243,  0.5770,  0.3774]]], device='cuda:0') tensor([ 0.6176, -0.1750,  0.2893,  0.5941,  0.1509,  0.1663], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1722412109375\n",
            "tcost icost -0.089599609375 0.0\n",
            "tcost icost -0.0304412841796875 0.0\n",
            "tcost icost -0.27783203125 0.0\n",
            "tcost icost -0.02142333984375 0.0\n",
            "tcost icost -0.03900146484375 0.0\n",
            "tcost icost 0.0166473388671875 0.0\n",
            "loss tensor([[-0.3735]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9691,  0.1858,  0.1623],\n",
            "         [ 0.2614, -0.1481, -0.9538],\n",
            "         [ 0.1690,  0.7478, -0.2939],\n",
            "         [ 0.7865, -0.0408,  0.2551],\n",
            "         [ 0.0832, -0.4020, -0.9119],\n",
            "         [ 0.7426,  0.5586,  0.3695]]], device='cuda:0') tensor([ 0.3168,  0.1102,  0.0861,  0.7752, -0.0588,  0.2883], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.37353515625\n",
            "tcost icost -0.017913818359375 0.0\n",
            "tcost icost 0.001766204833984375 0.0\n",
            "tcost icost -0.11212158203125 0.0\n",
            "tcost icost 0.0280609130859375 0.0\n",
            "tcost icost -0.01331329345703125 0.0\n",
            "tcost icost -0.0229949951171875 0.0\n",
            "loss tensor([[-0.1090]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9654,  0.1952,  0.1730],\n",
            "         [ 0.2629, -0.1476, -0.9535],\n",
            "         [ 0.2200,  0.7663, -0.2839],\n",
            "         [ 0.8337, -0.0445,  0.2548],\n",
            "         [ 0.1117, -0.3731, -0.9210],\n",
            "         [ 0.7540,  0.5451,  0.3666]]], device='cuda:0') tensor([-0.3624,  0.5757, -0.2386,  0.8103,  0.1058,  0.1906], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1090087890625\n",
            "tcost icost -0.0010890960693359375 0.0\n",
            "tcost icost -0.1412353515625 0.0\n",
            "tcost icost -0.03289794921875 0.0\n",
            "tcost icost -0.25537109375 0.0\n",
            "tcost icost -0.044403076171875 0.0\n",
            "tcost icost -0.0584716796875 0.0\n",
            "loss tensor([[-0.4043]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9656,  0.1936,  0.1738],\n",
            "         [ 0.2751, -0.1400, -0.9512],\n",
            "         [ 0.2683,  0.7801, -0.2773],\n",
            "         [ 0.8789, -0.0371,  0.2635],\n",
            "         [ 0.1345, -0.3514, -0.9265],\n",
            "         [ 0.7590,  0.5358,  0.3699]]], device='cuda:0') tensor([ 0.1908,  0.3248, -0.0448,  0.7177,  0.1749,  0.1110], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.404296875\n",
            "tcost icost 0.0080108642578125 0.0\n",
            "tcost icost 0.0350341796875 0.0\n",
            "tcost icost 0.3037109375 0.0\n",
            "tcost icost 0.1309814453125 0.0\n",
            "tcost icost -0.0109405517578125 0.0\n",
            "tcost icost -0.008514404296875 0.0\n",
            "loss tensor([[0.3689]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9608,  0.2058,  0.1856],\n",
            "         [ 0.2660, -0.1455, -0.9529],\n",
            "         [ 0.3278,  0.7855, -0.2863],\n",
            "         [ 0.9242, -0.0186,  0.2727],\n",
            "         [ 0.1529, -0.3313, -0.9310],\n",
            "         [ 0.7688,  0.5229,  0.3680]]], device='cuda:0') tensor([-0.7970,  1.0000, -0.3607,  0.4541,  0.2092,  0.1417], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.368896484375\n",
            "tcost icost -0.010009765625 0.0\n",
            "tcost icost -0.375 0.0\n",
            "tcost icost -0.06536865234375 0.0\n",
            "tcost icost -0.276123046875 0.0\n",
            "tcost icost -0.043853759765625 0.0\n",
            "tcost icost -0.072509765625 0.0\n",
            "loss tensor([[-0.6733]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9591,  0.2095,  0.1903],\n",
            "         [ 0.2650, -0.1463, -0.9531],\n",
            "         [ 0.3849,  0.7896, -0.2941],\n",
            "         [ 0.9578,  0.0071,  0.2875],\n",
            "         [ 0.1649, -0.3190, -0.9333],\n",
            "         [ 0.7718,  0.5153,  0.3726]]], device='cuda:0') tensor([-0.3778,  0.8643, -0.2673,  0.3627,  0.2976,  0.0551], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.67333984375\n",
            "tcost icost -0.0014905929565429688 0.0\n",
            "tcost icost -0.2098388671875 0.0\n",
            "tcost icost -0.05120849609375 0.0\n",
            "tcost icost -0.1942138671875 0.0\n",
            "tcost icost -0.0306243896484375 0.0\n",
            "tcost icost -0.0308685302734375 0.0\n",
            "loss tensor([[-0.4119]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9615,  0.2029,  0.1855],\n",
            "         [ 0.2727, -0.1411, -0.9517],\n",
            "         [ 0.4390,  0.7911, -0.3030],\n",
            "         [ 0.9540,  0.0378,  0.2974],\n",
            "         [ 0.1723, -0.3113, -0.9346],\n",
            "         [ 0.7716,  0.5104,  0.3797]]], device='cuda:0') tensor([ 0.1588,  0.6594, -0.1299,  0.2378,  0.3534,  0.0136], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.411865234375\n",
            "search tensor([[[ 0.1723, -0.3113, -0.9346],\n",
            "         [ 0.7716,  0.5104,  0.3797],\n",
            "         [ 0.3819, -0.3267, -0.2870],\n",
            "         [-0.4190, -0.5025, -0.3572],\n",
            "         [ 0.4294, -0.1146, -0.2514],\n",
            "         [-0.5160, -0.5327, -0.3173]]], device='cuda:0') tensor([[[ 0.3534],\n",
            "         [ 0.0136],\n",
            "         [-0.2476],\n",
            "         [-0.4823],\n",
            "         [ 0.0914],\n",
            "         [ 0.0535]]], device='cuda:0')\n",
            "tcost icost -0.02520751953125 0.0\n",
            "tcost icost 0.0108184814453125 0.0\n",
            "tcost icost -0.01153564453125 0.0\n",
            "tcost icost -0.017913818359375 0.0\n",
            "tcost icost -0.1854248046875 0.0\n",
            "tcost icost -0.01555633544921875 0.0\n",
            "loss tensor([[-0.1687]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.2721, -0.2110, -0.8336],\n",
            "         [ 0.8689,  0.4090,  0.2787],\n",
            "         [ 0.4816, -0.2264, -0.1867],\n",
            "         [-0.3185, -0.6020, -0.4568],\n",
            "         [ 0.5290, -0.0145, -0.1512],\n",
            "         [-0.4155, -0.6321, -0.4169]]], device='cuda:0') tensor([-0.8722,  1.0000, -0.6514, -0.3245, -0.0625,  0.0745], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.168701171875\n",
            "tcost icost -0.021820068359375 0.0\n",
            "tcost icost -0.401123046875 0.0\n",
            "tcost icost -0.06268310546875 0.0\n",
            "tcost icost -0.2152099609375 0.0\n",
            "tcost icost -0.03564453125 0.0\n",
            "tcost icost -0.169677734375 0.0\n",
            "loss tensor([[-0.7139]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.3295, -0.1585, -0.7760],\n",
            "         [ 0.9125,  0.3411,  0.2260],\n",
            "         [ 0.5545, -0.1641, -0.1078],\n",
            "         [-0.3324, -0.6084, -0.4395],\n",
            "         [ 0.5872,  0.0081, -0.1042],\n",
            "         [-0.4499, -0.5770, -0.3614]]], device='cuda:0') tensor([-0.6193,  0.8812, -0.5568, -0.4577,  0.0547, -0.0480], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.7138671875\n",
            "tcost icost -0.0163421630859375 0.0\n",
            "tcost icost -0.330810546875 0.0\n",
            "tcost icost -0.0623779296875 0.0\n",
            "tcost icost -0.1590576171875 0.0\n",
            "tcost icost -0.025177001953125 0.0\n",
            "tcost icost -0.0928955078125 0.0\n",
            "loss tensor([[-0.5522]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.3574, -0.1379, -0.7595],\n",
            "         [ 0.9324,  0.2985,  0.2039],\n",
            "         [ 0.6159, -0.1222, -0.0355],\n",
            "         [-0.3815, -0.5738, -0.3841],\n",
            "         [ 0.6206, -0.0146, -0.1010],\n",
            "         [-0.4969, -0.5009, -0.2857]]], device='cuda:0') tensor([-0.2099,  0.7554, -0.4221, -0.6330,  0.2170, -0.1801], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.55224609375\n",
            "tcost icost -0.007167816162109375 0.0\n",
            "tcost icost -0.1767578125 0.0\n",
            "tcost icost -0.04730224609375 0.0\n",
            "tcost icost -0.038116455078125 0.0\n",
            "tcost icost 0.0074462890625 0.0\n",
            "tcost icost 0.061279296875 0.0\n",
            "loss tensor([[-0.1914]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.3583, -0.1412, -0.7900],\n",
            "         [ 0.9394,  0.2818,  0.1952],\n",
            "         [ 0.6613, -0.1091, -0.0167],\n",
            "         [-0.3743, -0.5274, -0.3609],\n",
            "         [ 0.6734, -0.0126, -0.1058],\n",
            "         [-0.4487, -0.5057, -0.3050]]], device='cuda:0') tensor([ 0.3365,  0.4310, -0.3309, -0.8217,  0.1302,  0.0449], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.19140625\n",
            "tcost icost -0.0203857421875 0.0\n",
            "tcost icost 0.060455322265625 0.0\n",
            "tcost icost 0.031890869140625 0.0\n",
            "tcost icost -0.007137298583984375 0.0\n",
            "tcost icost -0.1497802734375 0.0\n",
            "tcost icost -0.0124053955078125 0.0\n",
            "loss tensor([[-0.0509]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.3726, -0.1297, -0.7992],\n",
            "         [ 0.9462,  0.2636,  0.1878],\n",
            "         [ 0.6994, -0.1036, -0.0030],\n",
            "         [-0.3449, -0.5281, -0.3723],\n",
            "         [ 0.7368,  0.0164, -0.0777],\n",
            "         [-0.4054, -0.5150, -0.3255]]], device='cuda:0') tensor([-0.1325,  0.4049, -0.2055, -0.6480, -0.0348,  0.0622], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0509033203125\n",
            "tcost icost -0.006862640380859375 0.0\n",
            "tcost icost -0.0765380859375 0.0\n",
            "tcost icost -0.01044464111328125 0.0\n",
            "tcost icost -0.001163482666015625 0.0\n",
            "tcost icost 0.1119384765625 0.0\n",
            "tcost icost 0.16162109375 0.0\n",
            "loss tensor([[0.0838]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.3436, -0.1583, -0.8449],\n",
            "         [ 0.9316,  0.2919,  0.2167],\n",
            "         [ 0.7280, -0.1360, -0.0339],\n",
            "         [-0.2930, -0.4840, -0.3557],\n",
            "         [ 0.7927, -0.0160, -0.1159],\n",
            "         [-0.3455, -0.4952, -0.3427]]], device='cuda:0') tensor([ 1.0000, -0.6488,  0.2645, -1.0000,  0.6737, -0.0329], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.08380126953125\n",
            "tcost icost -0.2138671875 0.0\n",
            "tcost icost -0.047271728515625 0.0\n",
            "tcost icost -0.314453125 0.0\n",
            "tcost icost -0.05792236328125 0.0\n",
            "tcost icost -0.38525390625 0.0\n",
            "tcost icost -0.04412841796875 0.0\n",
            "loss tensor([[-0.8315]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.3257, -0.1778, -0.8776],\n",
            "         [ 0.9194,  0.3115,  0.2403],\n",
            "         [ 0.7619, -0.1553, -0.0505],\n",
            "         [-0.2443, -0.4458, -0.3403],\n",
            "         [ 0.8546, -0.0370, -0.1399],\n",
            "         [-0.2913, -0.4809, -0.3599]]], device='cuda:0') tensor([ 0.7919, -0.5506,  0.1580, -0.9816,  0.5917, -0.0184], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.83154296875\n",
            "tcost icost -0.164794921875 0.0\n",
            "tcost icost -0.035797119140625 0.0\n",
            "tcost icost -0.30224609375 0.0\n",
            "tcost icost -0.05755615234375 0.0\n",
            "tcost icost -0.37060546875 0.0\n",
            "tcost icost -0.0419921875 0.0\n",
            "loss tensor([[-0.7520]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.3184, -0.1881, -0.8986],\n",
            "         [ 0.9111,  0.3218,  0.2575],\n",
            "         [ 0.8003, -0.1633, -0.0552],\n",
            "         [-0.1978, -0.4117, -0.3241],\n",
            "         [ 0.9210, -0.0483, -0.1519],\n",
            "         [-0.2420, -0.4721, -0.3776]]], device='cuda:0') tensor([ 0.5179, -0.4026,  0.0426, -0.9679,  0.5035, -0.0034], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.751953125\n",
            "tcost icost -0.086669921875 0.0\n",
            "tcost icost -0.0157928466796875 0.0\n",
            "tcost icost -0.2607421875 0.0\n",
            "tcost icost -0.057464599609375 0.0\n",
            "tcost icost -0.342041015625 0.0\n",
            "tcost icost -0.03826904296875 0.0\n",
            "loss tensor([[-0.6006]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.3242, -0.1862, -0.9063],\n",
            "         [ 0.9091,  0.3210,  0.2656],\n",
            "         [ 0.8424, -0.1610, -0.0499],\n",
            "         [-0.1516, -0.3805, -0.3066],\n",
            "         [ 0.9868, -0.0514, -0.1537],\n",
            "         [-0.1974, -0.4692, -0.3966]]], device='cuda:0') tensor([-0.0021, -0.1645, -0.0928, -0.9529,  0.4157,  0.0144], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6005859375\n",
            "tcost icost -0.005359649658203125 0.0\n",
            "tcost icost 0.061920166015625 0.0\n",
            "tcost icost 0.08721923828125 0.0\n",
            "tcost icost -0.03021240234375 0.0\n",
            "tcost icost -0.2802734375 0.0\n",
            "tcost icost -0.0287933349609375 0.0\n",
            "loss tensor([[-0.1019]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.3570, -0.1560, -0.8846],\n",
            "         [ 0.9304,  0.2831,  0.2328],\n",
            "         [ 0.8971, -0.1319, -0.0261],\n",
            "         [-0.1058, -0.3578, -0.2980],\n",
            "         [ 0.9893, -0.0444, -0.1393],\n",
            "         [-0.1579, -0.4728, -0.4175]]], device='cuda:0') tensor([-1.0000,  1.0000, -0.7051, -0.8913,  0.3000,  0.0347], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1019287109375\n",
            "tcost icost -0.0242767333984375 0.0\n",
            "tcost icost -0.40673828125 0.0\n",
            "tcost icost -0.06121826171875 0.0\n",
            "tcost icost -0.1375732421875 0.0\n",
            "tcost icost -0.01317596435546875 0.0\n",
            "tcost icost -0.08538818359375 0.0\n",
            "loss tensor([[-0.5991]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 3.8592e-01, -1.3162e-01, -8.6499e-01],\n",
            "         [ 9.4512e-01,  2.5134e-01,  2.0875e-01],\n",
            "         [ 9.4710e-01, -1.0719e-01, -9.2076e-04],\n",
            "         [-4.6676e-02, -3.1444e-01, -2.6274e-01],\n",
            "         [ 9.8950e-01, -4.9125e-02, -1.3592e-01],\n",
            "         [-1.0209e-01, -4.3998e-01, -4.1270e-01]]], device='cuda:0') tensor([-0.8231,  0.8791, -0.5953, -1.0000,  0.5204, -0.1046], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.59912109375\n",
            "tcost icost -0.020782470703125 0.0\n",
            "tcost icost -0.36962890625 0.0\n",
            "tcost icost -0.061065673828125 0.0\n",
            "tcost icost -0.09454345703125 0.0\n",
            "tcost icost -0.000621795654296875 0.0\n",
            "tcost icost 0.0628662109375 0.0\n",
            "loss tensor([[-0.4353]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.4112, -0.1116, -0.8498],\n",
            "         [ 0.9565,  0.2239,  0.1868],\n",
            "         [ 0.9908, -0.0884,  0.0143],\n",
            "         [ 0.0133, -0.2754, -0.2505],\n",
            "         [ 0.9905, -0.0442, -0.1300],\n",
            "         [-0.0822, -0.4461, -0.4391]]], device='cuda:0') tensor([-0.7126,  0.6487, -0.5772, -1.0000,  0.2966,  0.0863], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.435302734375\n",
            "tcost icost -0.0183258056640625 0.0\n",
            "tcost icost -0.298828125 0.0\n",
            "tcost icost -0.056976318359375 0.0\n",
            "tcost icost -0.08843994140625 0.0\n",
            "tcost icost -0.004154205322265625 0.0\n",
            "tcost icost -0.0233306884765625 0.0\n",
            "loss tensor([[-0.4143]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.4289, -0.1014, -0.8405],\n",
            "         [ 0.9626,  0.2053,  0.1766],\n",
            "         [ 0.9968, -0.0746,  0.0296],\n",
            "         [ 0.0778, -0.2205, -0.2124],\n",
            "         [ 0.9883, -0.0572, -0.1412],\n",
            "         [-0.0375, -0.4231, -0.4466]]], device='cuda:0') tensor([-0.2326,  0.4636, -0.3211, -1.0000,  0.6670, -0.1074], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.414306640625\n",
            "tcost icost -0.0076141357421875 0.0\n",
            "tcost icost -0.12420654296875 0.0\n",
            "tcost icost -0.026123046875 0.0\n",
            "tcost icost -0.0108642578125 0.0\n",
            "tcost icost -0.0284576416015625 0.0\n",
            "tcost icost -0.017486572265625 0.0\n",
            "loss tensor([[-0.1776]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 4.4245e-01, -9.3422e-02, -8.3468e-01],\n",
            "         [ 9.6870e-01,  1.8681e-01,  1.6349e-01],\n",
            "         [ 9.9742e-01, -5.7769e-02,  4.2565e-02],\n",
            "         [ 1.1172e-01, -1.9999e-01, -2.1437e-01],\n",
            "         [ 9.8806e-01, -5.6402e-02, -1.4339e-01],\n",
            "         [ 4.6537e-04, -4.0482e-01, -4.5609e-01]]], device='cuda:0') tensor([-0.2526,  0.3690, -0.4813, -0.6494,  0.3994, -0.0822], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1776123046875\n",
            "tcost icost -0.00801849365234375 0.0\n",
            "tcost icost -0.1094970703125 0.0\n",
            "tcost icost -0.0270538330078125 0.0\n",
            "tcost icost -0.053070068359375 0.0\n",
            "tcost icost 0.033843994140625 0.0\n",
            "tcost icost 0.10272216796875 0.0\n",
            "loss tensor([[-0.0842]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.4445, -0.0957, -0.8435],\n",
            "         [ 0.9720,  0.1768,  0.1550],\n",
            "         [ 0.9978, -0.0539,  0.0375],\n",
            "         [ 0.1491, -0.1794, -0.2284],\n",
            "         [ 0.9895, -0.0434, -0.1382],\n",
            "         [ 0.0090, -0.4136, -0.4888]]], device='cuda:0') tensor([ 0.3543, -0.0089, -0.3668, -0.8322,  0.1203,  0.2332], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.084228515625\n",
            "tcost icost -0.0255126953125 0.0\n",
            "tcost icost 0.0092620849609375 0.0\n",
            "tcost icost -0.004673004150390625 0.0\n",
            "tcost icost -0.022552490234375 0.0\n",
            "tcost icost -0.250732421875 0.0\n",
            "tcost icost -0.0184783935546875 0.0\n",
            "loss tensor([[-0.2128]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.4661, -0.0795, -0.8273],\n",
            "         [ 0.9801,  0.1491,  0.1310],\n",
            "         [ 0.9983, -0.0343,  0.0467],\n",
            "         [ 0.1822, -0.1653, -0.2449],\n",
            "         [ 0.9918, -0.0253, -0.1256],\n",
            "         [ 0.0155, -0.4229, -0.5197]]], device='cuda:0') tensor([-0.8078,  0.9178, -0.7015, -0.7220, -0.0208,  0.2447], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2127685546875\n",
            "tcost icost -0.0204010009765625 0.0\n",
            "tcost icost -0.3759765625 0.0\n",
            "tcost icost -0.0628662109375 0.0\n",
            "tcost icost -0.1522216796875 0.0\n",
            "tcost icost -0.0240631103515625 0.0\n",
            "tcost icost -0.201416015625 0.0\n",
            "loss tensor([[-0.6558]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.4832, -0.0692, -0.8155],\n",
            "         [ 0.9851,  0.1278,  0.1149],\n",
            "         [ 0.9982, -0.0188,  0.0569],\n",
            "         [ 0.2253, -0.1409, -0.2465],\n",
            "         [ 0.9929, -0.0150, -0.1182],\n",
            "         [ 0.0382, -0.4194, -0.5418]]], device='cuda:0') tensor([-0.4989,  0.7977, -0.5914, -0.8979,  0.1147,  0.1198], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.65576171875\n",
            "tcost icost -0.01377105712890625 0.0\n",
            "tcost icost -0.28125 0.0\n",
            "tcost icost -0.059539794921875 0.0\n",
            "tcost icost -0.06646728515625 0.0\n",
            "tcost icost -0.00820159912109375 0.0\n",
            "tcost icost -0.08123779296875 0.0\n",
            "loss tensor([[-0.4165]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.4926, -0.0677, -0.8122],\n",
            "         [ 0.9875,  0.1144,  0.1083],\n",
            "         [ 0.9977, -0.0100,  0.0669],\n",
            "         [ 0.2793, -0.1039, -0.2308],\n",
            "         [ 0.9925, -0.0161, -0.1210],\n",
            "         [ 0.0746, -0.4038, -0.5570]]], device='cuda:0') tensor([ 0.0435,  0.6290, -0.3775, -1.0000,  0.3454, -0.0185], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.41650390625\n",
            "tcost icost -0.00498199462890625 0.0\n",
            "tcost icost -0.049591064453125 0.0\n",
            "tcost icost -0.01971435546875 0.0\n",
            "tcost icost -0.00982666015625 0.0\n",
            "tcost icost -0.03936767578125 0.0\n",
            "tcost icost -0.003330230712890625 0.0\n",
            "loss tensor([[-0.1005]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.4932, -0.0720, -0.8190],\n",
            "         [ 0.9892,  0.1054,  0.1020],\n",
            "         [ 0.9969,  0.0025,  0.0781],\n",
            "         [ 0.3179, -0.0841, -0.2334],\n",
            "         [ 0.9932, -0.0082, -0.1162],\n",
            "         [ 0.1049, -0.3921, -0.5732]]], device='cuda:0') tensor([ 0.3875,  0.4263, -0.5251, -0.7437,  0.1490,  0.0170], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1004638671875\n",
            "tcost icost -0.03753662109375 0.0\n",
            "tcost icost 0.05194091796875 0.0\n",
            "tcost icost 0.009002685546875 0.0\n",
            "tcost icost -0.00202178955078125 0.0\n",
            "tcost icost -0.1044921875 0.0\n",
            "tcost icost -0.01032257080078125 0.0\n",
            "loss tensor([[-0.0596]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.5085, -0.0622, -0.8074],\n",
            "         [ 0.9921,  0.0885,  0.0885],\n",
            "         [ 0.9951,  0.0225,  0.0966],\n",
            "         [ 0.3464, -0.0773, -0.2477],\n",
            "         [ 0.9946,  0.0073, -0.1033],\n",
            "         [ 0.1296, -0.3837, -0.5902]]], device='cuda:0') tensor([-0.5729,  0.8530, -0.7313, -0.4817, -0.0341,  0.0489], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.059600830078125\n",
            "search tensor([[[ 0.9946,  0.0073, -0.1033],\n",
            "         [ 0.1296, -0.3837, -0.5902],\n",
            "         [-0.0769, -0.0977,  0.4880],\n",
            "         [ 0.4185, -0.2506, -0.3078],\n",
            "         [ 0.2689,  0.4867,  0.3006],\n",
            "         [ 0.0128, -0.3942, -0.0467]]], device='cuda:0') tensor([[[-0.0341],\n",
            "         [ 0.0489],\n",
            "         [-0.4598],\n",
            "         [ 0.2582],\n",
            "         [ 0.4067],\n",
            "         [ 0.7570]]], device='cuda:0')\n",
            "tcost icost 0.0220947265625 0.0\n",
            "tcost icost 0.018157958984375 0.0\n",
            "tcost icost 0.41455078125 0.0\n",
            "tcost icost 0.158203125 0.0\n",
            "tcost icost 0.1561279296875 0.0\n",
            "tcost icost 0.11297607421875 0.0\n",
            "loss tensor([[0.6587]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.9952,  0.0976, -0.0029],\n",
            "         [ 0.0295, -0.4833, -0.6896],\n",
            "         [ 0.0231,  0.0024,  0.5875],\n",
            "         [ 0.3181, -0.3504, -0.4075],\n",
            "         [ 0.3686,  0.5862,  0.4003],\n",
            "         [-0.0872, -0.4938, -0.1467]]], device='cuda:0') tensor([-1.0000,  1.0000, -1.0000,  1.0000, -0.1748,  0.8251], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.65869140625\n",
            "tcost icost -0.00864410400390625 0.0\n",
            "tcost icost -0.379638671875 0.0\n",
            "tcost icost -0.07061767578125 0.0\n",
            "tcost icost -0.355224609375 0.0\n",
            "tcost icost -0.059173583984375 0.0\n",
            "tcost icost -0.249267578125 0.0\n",
            "loss tensor([[-0.8525]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.9872,  0.1489,  0.0567],\n",
            "         [-0.0323, -0.5465, -0.7551],\n",
            "         [ 0.0949,  0.0693,  0.6556],\n",
            "         [ 0.2689, -0.4109, -0.4639],\n",
            "         [ 0.4440,  0.6459,  0.4648],\n",
            "         [-0.0207, -0.4739, -0.1444]]], device='cuda:0') tensor([-0.6614,  0.8763, -0.9329,  0.9618, -0.1061,  0.7285], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.8525390625\n",
            "tcost icost -0.0027751922607421875 0.0\n",
            "tcost icost -0.2626953125 0.0\n",
            "tcost icost -0.0653076171875 0.0\n",
            "tcost icost -0.297607421875 0.0\n",
            "tcost icost -0.05224609375 0.0\n",
            "tcost icost -0.2138671875 0.0\n",
            "loss tensor([[-0.6699]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.9791,  0.1798,  0.0947],\n",
            "         [-0.0753, -0.5903, -0.8036],\n",
            "         [ 0.1550,  0.1204,  0.7091],\n",
            "         [ 0.2495, -0.4510, -0.4990],\n",
            "         [ 0.5104,  0.6862,  0.5124],\n",
            "         [ 0.0605, -0.4205, -0.1073]]], device='cuda:0') tensor([-0.1797,  0.7465, -0.8271,  0.9193, -0.0296,  0.6271], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.669921875\n",
            "tcost icost 0.01537322998046875 0.0\n",
            "tcost icost -0.01424407958984375 0.0\n",
            "tcost icost -0.031829833984375 0.0\n",
            "tcost icost -0.268798828125 0.0\n",
            "tcost icost -0.03826904296875 0.0\n",
            "tcost icost -0.1776123046875 0.0\n",
            "loss tensor([[-0.3491]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9746,  0.1932,  0.1136],\n",
            "         [-0.0983, -0.5889, -0.8022],\n",
            "         [ 0.2076,  0.1592,  0.7519],\n",
            "         [ 0.2534, -0.4754, -0.5202],\n",
            "         [ 0.5385,  0.6685,  0.5130],\n",
            "         [ 0.1481, -0.3509, -0.0530]]], device='cuda:0') tensor([ 0.6401,  0.4034, -0.6263,  0.8415,  0.0653,  0.5269], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.34912109375\n",
            "tcost icost -0.10235595703125 0.0\n",
            "tcost icost -0.01526641845703125 0.0\n",
            "tcost icost -0.0011034011840820312 0.0\n",
            "tcost icost 0.12030029296875 0.0\n",
            "tcost icost 0.046356201171875 0.0\n",
            "tcost icost 0.1341552734375 0.0\n",
            "loss tensor([[0.0803]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9723,  0.1991,  0.1222],\n",
            "         [-0.1162, -0.5852, -0.8025],\n",
            "         [ 0.2659,  0.1728,  0.7701],\n",
            "         [ 0.2935, -0.4611, -0.5271],\n",
            "         [ 0.5974,  0.6459,  0.4753],\n",
            "         [ 0.2374, -0.2721,  0.0050]]], device='cuda:0') tensor([ 0.6071, -0.0067, -0.2161, -0.0067,  0.7001,  0.3289], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.080322265625\n",
            "tcost icost -0.09515380859375 0.0\n",
            "tcost icost -0.03076171875 0.0\n",
            "tcost icost -0.1788330078125 0.0\n",
            "tcost icost -0.03216552734375 0.0\n",
            "tcost icost -0.3251953125 0.0\n",
            "tcost icost -0.031280517578125 0.0\n",
            "loss tensor([[-0.5229]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9687,  0.2084,  0.1346],\n",
            "         [-0.1321, -0.5836, -0.8012],\n",
            "         [ 0.3186,  0.1886,  0.7877],\n",
            "         [ 0.3275, -0.4508, -0.5337],\n",
            "         [ 0.6426,  0.6233,  0.4457],\n",
            "         [ 0.3151, -0.2055,  0.0533]]], device='cuda:0') tensor([ 0.2821,  0.2709, -0.3902,  0.0910,  0.5877,  0.3404], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.52294921875\n",
            "tcost icost -0.01166534423828125 0.0\n",
            "tcost icost 0.0033054351806640625 0.0\n",
            "tcost icost 0.021148681640625 0.0\n",
            "tcost icost -0.004520416259765625 0.0\n",
            "tcost icost -0.204345703125 0.0\n",
            "tcost icost -0.0195465087890625 0.0\n",
            "loss tensor([[-0.1405]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9602,  0.2297,  0.1590],\n",
            "         [-0.1500, -0.5874, -0.7953],\n",
            "         [ 0.3667,  0.2101,  0.8052],\n",
            "         [ 0.3507, -0.4475, -0.5446],\n",
            "         [ 0.6762,  0.6031,  0.4232],\n",
            "         [ 0.3833, -0.1497,  0.0928]]], device='cuda:0') tensor([-0.9171,  1.0000, -0.7234,  0.3185,  0.4478,  0.3556], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1405029296875\n",
            "tcost icost -0.007171630859375 0.0\n",
            "tcost icost -0.363525390625 0.0\n",
            "tcost icost -0.0653076171875 0.0\n",
            "tcost icost -0.252685546875 0.0\n",
            "tcost icost -0.02838134765625 0.0\n",
            "tcost icost -0.08428955078125 0.0\n",
            "loss tensor([[-0.6396]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9535,  0.2440,  0.1770],\n",
            "         [-0.1631, -0.5892, -0.7914],\n",
            "         [ 0.4121,  0.2285,  0.8205],\n",
            "         [ 0.3825, -0.4387, -0.5464],\n",
            "         [ 0.7114,  0.5780,  0.3997],\n",
            "         [ 0.4548, -0.0838,  0.1439]]], device='cuda:0') tensor([-0.5280,  0.8700, -0.6321,  0.2144,  0.5928,  0.2358], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6396484375\n",
            "tcost icost 0.0016689300537109375 0.0\n",
            "tcost icost -0.21142578125 0.0\n",
            "tcost icost -0.05419921875 0.0\n",
            "tcost icost -0.1748046875 0.0\n",
            "tcost icost -0.0136566162109375 0.0\n",
            "tcost icost 0.0051422119140625 0.0\n",
            "loss tensor([[-0.3660]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9502,  0.2498,  0.1865],\n",
            "         [-0.1719, -0.5884, -0.7901],\n",
            "         [ 0.4553,  0.2434,  0.8329],\n",
            "         [ 0.4204, -0.4236, -0.5446],\n",
            "         [ 0.7450,  0.5517,  0.3750],\n",
            "         [ 0.5260, -0.0264,  0.1859]]], device='cuda:0') tensor([ 0.0365,  0.6300, -0.5363,  0.0387,  0.6191,  0.2892], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.365966796875\n",
            "tcost icost 0.0254058837890625 0.0\n",
            "tcost icost 0.10882568359375 0.0\n",
            "tcost icost 0.0013475418090820312 0.0\n",
            "tcost icost -0.0222015380859375 0.0\n",
            "tcost icost 0.1988525390625 0.0\n",
            "tcost icost 0.051513671875 0.0\n",
            "loss tensor([[0.2693]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9485,  0.2518,  0.1920],\n",
            "         [-0.1762, -0.5849, -0.7917],\n",
            "         [ 0.4936,  0.2488,  0.8333],\n",
            "         [ 0.4625, -0.4021, -0.5460],\n",
            "         [ 0.7696,  0.5302,  0.3558],\n",
            "         [ 0.5957, -0.0100,  0.1938]]], device='cuda:0') tensor([ 0.2294,  0.3961, -0.2372, -0.1557,  0.3927,  0.5092], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.269287109375\n",
            "tcost icost 0.0006799697875976562 0.0\n",
            "tcost icost 0.0121307373046875 0.0\n",
            "tcost icost 0.21728515625 0.0\n",
            "tcost icost -0.009429931640625 0.0\n",
            "tcost icost -0.17822265625 0.0\n",
            "tcost icost 0.01389312744140625 0.0\n",
            "loss tensor([[0.0720]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9337,  0.2801,  0.2231],\n",
            "         [-0.1875, -0.5941, -0.7822],\n",
            "         [ 0.5144,  0.2606,  0.8170],\n",
            "         [ 0.4961, -0.3879, -0.5520],\n",
            "         [ 0.7842,  0.5154,  0.3455],\n",
            "         [ 0.6620, -0.0044,  0.1933]]], device='cuda:0') tensor([-1.0000,  1.0000, -1.0000,  0.0743,  0.2002,  0.5776], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.072021484375\n",
            "tcost icost -0.00864410400390625 0.0\n",
            "tcost icost -0.379638671875 0.0\n",
            "tcost icost -0.070068359375 0.0\n",
            "tcost icost -0.258544921875 0.0\n",
            "tcost icost -0.037384033203125 0.0\n",
            "tcost icost -0.219970703125 0.0\n",
            "loss tensor([[-0.7500]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9205,  0.3019,  0.2481],\n",
            "         [-0.1958, -0.6009, -0.7750],\n",
            "         [ 0.5341,  0.2701,  0.8011],\n",
            "         [ 0.5350, -0.3704, -0.5499],\n",
            "         [ 0.8024,  0.4945,  0.3340],\n",
            "         [ 0.7279,  0.0168,  0.2065]]], device='cuda:0') tensor([-0.6482,  0.8799, -0.9194, -0.0141,  0.3238,  0.4580], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.75\n",
            "tcost icost -0.002460479736328125 0.0\n",
            "tcost icost -0.258544921875 0.0\n",
            "tcost icost -0.065673828125 0.0\n",
            "tcost icost -0.195068359375 0.0\n",
            "tcost icost -0.0240478515625 0.0\n",
            "tcost icost -0.140869140625 0.0\n",
            "loss tensor([[-0.5298]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9105,  0.3164,  0.2661],\n",
            "         [-0.2011, -0.6054, -0.7701],\n",
            "         [ 0.5528,  0.2773,  0.7858],\n",
            "         [ 0.5805, -0.3479, -0.5389],\n",
            "         [ 0.8261,  0.4648,  0.3186],\n",
            "         [ 0.7930,  0.0506,  0.2303]]], device='cuda:0') tensor([-0.1145,  0.7347, -0.8054, -0.1531,  0.4975,  0.3365], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.52978515625\n",
            "tcost icost 0.0176849365234375 0.0\n",
            "tcost icost 0.036376953125 0.0\n",
            "tcost icost -0.025634765625 0.0\n",
            "tcost icost -0.07830810546875 0.0\n",
            "tcost icost 0.045379638671875 0.0\n",
            "tcost icost 0.06085205078125 0.0\n",
            "loss tensor([[0.0382]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9099,  0.3156,  0.2692],\n",
            "         [-0.2005, -0.6040, -0.7713],\n",
            "         [ 0.5770,  0.2776,  0.7681],\n",
            "         [ 0.6378, -0.3107, -0.5127],\n",
            "         [ 0.8631,  0.4162,  0.2860],\n",
            "         [ 0.8601,  0.0987,  0.2648]]], device='cuda:0') tensor([ 1.0000,  0.0516, -0.3484, -0.6839,  0.9137,  0.1464], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.038238525390625\n",
            "tcost icost -0.1728515625 0.0\n",
            "tcost icost -0.047882080078125 0.0\n",
            "tcost icost -0.1328125 0.0\n",
            "tcost icost -0.038726806640625 0.0\n",
            "tcost icost -0.402099609375 0.0\n",
            "tcost icost -0.038055419921875 0.0\n",
            "loss tensor([[-0.6382]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9076,  0.3172,  0.2749],\n",
            "         [-0.2002, -0.6041, -0.7714],\n",
            "         [ 0.5969,  0.2795,  0.7521],\n",
            "         [ 0.6914, -0.2769, -0.4876],\n",
            "         [ 0.8906,  0.3731,  0.2601],\n",
            "         [ 0.9217,  0.1419,  0.2956]]], device='cuda:0') tensor([ 0.8162,  0.2607, -0.5137, -0.6452,  0.8301,  0.1581], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.63818359375\n",
            "tcost icost -0.138916015625 0.0\n",
            "tcost icost -0.030853271484375 0.0\n",
            "tcost icost -0.045928955078125 0.0\n",
            "tcost icost -0.02874755859375 0.0\n",
            "tcost icost -0.39111328125 0.0\n",
            "tcost icost -0.033935546875 0.0\n",
            "loss tensor([[-0.5015]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9030,  0.3221,  0.2845],\n",
            "         [-0.2006, -0.6060, -0.7697],\n",
            "         [ 0.6117,  0.2834,  0.7386],\n",
            "         [ 0.7414, -0.2465, -0.4639],\n",
            "         [ 0.9102,  0.3369,  0.2408],\n",
            "         [ 0.9355,  0.1725,  0.3085]]], device='cuda:0') tensor([ 0.5413,  0.5522, -0.7358, -0.5883,  0.7377,  0.1724], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.50146484375\n",
            "tcost icost -0.0802001953125 0.0\n",
            "tcost icost -0.0047454833984375 0.0\n",
            "tcost icost 0.0006885528564453125 0.0\n",
            "tcost icost -0.004650115966796875 0.0\n",
            "tcost icost -0.318359375 0.0\n",
            "tcost icost -0.0301513671875 0.0\n",
            "loss tensor([[-0.3140]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.8953,  0.3312,  0.2979],\n",
            "         [-0.2014, -0.6090, -0.7672],\n",
            "         [ 0.6214,  0.2884,  0.7285],\n",
            "         [ 0.7851, -0.2226, -0.4462],\n",
            "         [ 0.9234,  0.3093,  0.2274],\n",
            "         [ 0.9297,  0.1947,  0.3126]]], device='cuda:0') tensor([ 0.1033,  0.7433, -0.8223, -0.4057,  0.6195,  0.1845], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.31396484375\n",
            "tcost icost 0.0230865478515625 0.0\n",
            "tcost icost 0.10296630859375 0.0\n",
            "tcost icost -0.0116424560546875 0.0\n",
            "tcost icost -0.01548004150390625 0.0\n",
            "tcost icost 0.09869384765625 0.0\n",
            "tcost icost 0.032928466796875 0.0\n",
            "loss tensor([[0.1792]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.8839,  0.3452,  0.3155],\n",
            "         [-0.2003, -0.6115, -0.7655],\n",
            "         [ 0.6305,  0.2942,  0.7183],\n",
            "         [ 0.8110, -0.2141, -0.4528],\n",
            "         [ 0.9257,  0.3042,  0.2249],\n",
            "         [ 0.9299,  0.2049,  0.3055]]], device='cuda:0') tensor([-0.4758,  0.9026, -0.9612,  0.0113,  0.1759,  0.3167], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.17919921875\n",
            "tcost icost 0.00341033935546875 0.0\n",
            "tcost icost -0.1988525390625 0.0\n",
            "tcost icost -0.06390380859375 0.0\n",
            "tcost icost -0.1839599609375 0.0\n",
            "tcost icost -0.0277099609375 0.0\n",
            "tcost icost -0.164306640625 0.0\n",
            "loss tensor([[-0.4766]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.8771,  0.3524,  0.3264],\n",
            "         [-0.1969, -0.6121, -0.7659],\n",
            "         [ 0.6397,  0.2981,  0.7084],\n",
            "         [ 0.8416, -0.2002, -0.4506],\n",
            "         [ 0.9313,  0.2911,  0.2190],\n",
            "         [ 0.9259,  0.2207,  0.3066]]], device='cuda:0') tensor([ 0.0574,  0.7418, -0.8457, -0.1268,  0.3217,  0.1900], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4765625\n",
            "tcost icost 0.0257720947265625 0.0\n",
            "tcost icost 0.1046142578125 0.0\n",
            "tcost icost -0.0156707763671875 0.0\n",
            "tcost icost -0.06048583984375 0.0\n",
            "tcost icost 0.047698974609375 0.0\n",
            "tcost icost 0.052520751953125 0.0\n",
            "loss tensor([[0.1254]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.8796,  0.3474,  0.3251],\n",
            "         [-0.1892, -0.6079, -0.7711],\n",
            "         [ 0.6596,  0.2940,  0.6917],\n",
            "         [ 0.8834, -0.1707, -0.4321],\n",
            "         [ 0.9473,  0.2551,  0.1940],\n",
            "         [ 0.9167,  0.2445,  0.3161]]], device='cuda:0') tensor([ 0.9265,  0.1382, -0.3071, -0.7044,  0.7814, -0.0164], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1253662109375\n",
            "search tensor([[[ 0.9473,  0.2551,  0.1940],\n",
            "         [ 0.9167,  0.2445,  0.3161],\n",
            "         [-0.1756, -0.3465, -0.2521],\n",
            "         [ 0.1402, -0.3476, -0.0261],\n",
            "         [-0.0606,  0.0855,  0.2548],\n",
            "         [ 0.0396, -0.1589, -0.2393]]], device='cuda:0') tensor([[[ 0.7814],\n",
            "         [-0.0164],\n",
            "         [-0.0738],\n",
            "         [-0.8113],\n",
            "         [ 0.1401],\n",
            "         [-0.1551]]], device='cuda:0')\n",
            "tcost icost -0.1290283203125 0.0\n",
            "tcost icost -0.024322509765625 0.0\n",
            "tcost icost -0.1690673828125 0.0\n",
            "tcost icost -0.04400634765625 0.0\n",
            "tcost icost -0.266845703125 0.0\n",
            "tcost icost -0.030548095703125 0.0\n",
            "loss tensor([[-0.5127]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.8463,  0.3548,  0.2938],\n",
            "         [ 0.9689,  0.1376,  0.2058],\n",
            "         [-0.0755, -0.2462, -0.1518],\n",
            "         [ 0.2401, -0.4472,  0.0739],\n",
            "         [ 0.0394,  0.1854,  0.3546],\n",
            "         [-0.0604, -0.2587, -0.3391]]], device='cuda:0') tensor([ 0.4386,  0.2858, -0.2087, -0.7653,  0.0250, -0.1366], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5126953125\n",
            "tcost icost -0.0491943359375 0.0\n",
            "tcost icost 0.0116729736328125 0.0\n",
            "tcost icost 0.161865234375 0.0\n",
            "tcost icost -0.0198211669921875 0.0\n",
            "tcost icost -0.201904296875 0.0\n",
            "tcost icost -0.01824951171875 0.0\n",
            "loss tensor([[-0.0653]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.7607,  0.4465,  0.3874],\n",
            "         [ 0.9930,  0.0423,  0.1098],\n",
            "         [ 0.0217, -0.1555, -0.1144],\n",
            "         [ 0.3183, -0.5326,  0.0400],\n",
            "         [ 0.1392,  0.2853,  0.4543],\n",
            "         [-0.1598, -0.3579, -0.4387]]], device='cuda:0') tensor([-0.9735,  1.0000,  0.0420, -0.6672, -0.1360, -0.1140], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0653076171875\n",
            "tcost icost -0.01264190673828125 0.0\n",
            "tcost icost -0.42919921875 0.0\n",
            "tcost icost -0.05389404296875 0.0\n",
            "tcost icost -0.09820556640625 0.0\n",
            "tcost icost -0.01812744140625 0.0\n",
            "tcost icost -0.05914306640625 0.0\n",
            "loss tensor([[-0.5605]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.7219,  0.5035,  0.4507],\n",
            "         [ 0.9979, -0.0076,  0.0637],\n",
            "         [ 0.0915, -0.1253, -0.0881],\n",
            "         [ 0.3881, -0.4860,  0.0978],\n",
            "         [ 0.2241,  0.3167,  0.4872],\n",
            "         [-0.1096, -0.3082, -0.4023]]], device='cuda:0') tensor([-0.6244,  0.8860,  0.2215, -0.8804,  0.0299, -0.2579], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.560546875\n",
            "tcost icost -0.00432586669921875 0.0\n",
            "tcost icost -0.30908203125 0.0\n",
            "tcost icost -0.04754638671875 0.0\n",
            "tcost icost -0.005474090576171875 0.0\n",
            "tcost icost 0.008270263671875 0.0\n",
            "tcost icost 0.00962066650390625 0.0\n",
            "loss tensor([[-0.3140]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.7158,  0.5191,  0.4671],\n",
            "         [ 0.9989, -0.0291,  0.0368],\n",
            "         [ 0.1421, -0.1184, -0.0943],\n",
            "         [ 0.4563, -0.4386,  0.1474],\n",
            "         [ 0.3029,  0.3420,  0.5106],\n",
            "         [-0.0725, -0.2765, -0.3908]]], device='cuda:0') tensor([-0.2313,  0.6547,  0.2765, -0.9204,  0.0298, -0.2295], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.31396484375\n",
            "tcost icost 0.004192352294921875 0.0\n",
            "tcost icost -0.1180419921875 0.0\n",
            "tcost icost -0.01461029052734375 0.0\n",
            "tcost icost -0.0131378173828125 0.0\n",
            "tcost icost -0.0113525390625 0.0\n",
            "tcost icost -0.0037326812744140625 0.0\n",
            "loss tensor([[-0.1332]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.7255,  0.5138,  0.4579],\n",
            "         [ 0.9992, -0.0333,  0.0207],\n",
            "         [ 0.1853, -0.1095, -0.0985],\n",
            "         [ 0.5081, -0.4154,  0.1728],\n",
            "         [ 0.3784,  0.3745,  0.5416],\n",
            "         [-0.0514, -0.2606, -0.3988]]], device='cuda:0') tensor([ 0.1354,  0.4510,  0.2553, -0.8324, -0.0605, -0.1954], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1331787109375\n",
            "tcost icost 0.011962890625 0.0\n",
            "tcost icost 0.1053466796875 0.0\n",
            "tcost icost 0.29736328125 0.0\n",
            "tcost icost -0.0276947021484375 0.0\n",
            "tcost icost -0.0943603515625 0.0\n",
            "tcost icost -0.01233673095703125 0.0\n",
            "loss tensor([[0.2581]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 7.3181e-01,  5.0954e-01,  4.5257e-01],\n",
            "         [ 9.9916e-01, -4.0879e-02,  3.4783e-04],\n",
            "         [ 2.4222e-01, -9.1230e-02, -1.0125e-01],\n",
            "         [ 5.5804e-01, -3.9773e-01,  1.9692e-01],\n",
            "         [ 4.5862e-01,  4.1992e-01,  5.8729e-01],\n",
            "         [-3.7167e-02, -2.5129e-01, -4.1175e-01]]], device='cuda:0') tensor([-0.1888,  0.5826,  0.2329, -0.7635, -0.2538, -0.1811], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.258056640625\n",
            "tcost icost 0.00527191162109375 0.0\n",
            "tcost icost -0.0582275390625 0.0\n",
            "tcost icost -0.0018634796142578125 0.0\n",
            "tcost icost -0.0107879638671875 0.0\n",
            "tcost icost -0.0018453598022460938 0.0\n",
            "tcost icost 0.04425048828125 0.0\n",
            "loss tensor([[-0.0316]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.7639,  0.4873,  0.4232],\n",
            "         [ 0.9995, -0.0288, -0.0089],\n",
            "         [ 0.2878, -0.0737, -0.1153],\n",
            "         [ 0.5930, -0.4085,  0.1901],\n",
            "         [ 0.5197,  0.4685,  0.6356],\n",
            "         [-0.0574, -0.2777, -0.4577]]], device='cuda:0') tensor([ 0.4870,  0.2433,  0.2038, -0.5927, -0.4191,  0.0038], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.031585693359375\n",
            "tcost icost -0.06146240234375 0.0\n",
            "tcost icost 0.005275726318359375 0.0\n",
            "tcost icost -0.0902099609375 0.0\n",
            "tcost icost -0.040435791015625 0.0\n",
            "tcost icost -0.07891845703125 0.0\n",
            "tcost icost -0.0011720657348632812 0.0\n",
            "loss tensor([[-0.2117]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.7606,  0.4903,  0.4254],\n",
            "         [ 0.9981, -0.0471, -0.0394],\n",
            "         [ 0.3487, -0.0271, -0.0807],\n",
            "         [ 0.6276, -0.4218,  0.1823],\n",
            "         [ 0.5523,  0.5021,  0.6654],\n",
            "         [-0.0754, -0.3038, -0.5026]]], device='cuda:0') tensor([-0.4056,  0.9664, -0.0050, -0.4956, -0.6051,  0.0148], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.211669921875\n",
            "tcost icost -0.00020706653594970703 0.0\n",
            "tcost icost -0.26171875 0.0\n",
            "tcost icost -0.049896240234375 0.0\n",
            "tcost icost -0.0247650146484375 0.0\n",
            "tcost icost -0.0218048095703125 0.0\n",
            "tcost icost -0.1405029296875 0.0\n",
            "loss tensor([[-0.3918]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.7714,  0.4813,  0.4164],\n",
            "         [ 0.9977, -0.0480, -0.0474],\n",
            "         [ 0.4042,  0.0017, -0.0517],\n",
            "         [ 0.6738, -0.4079,  0.1963],\n",
            "         [ 0.5655,  0.5006,  0.6555],\n",
            "         [-0.0565, -0.2995, -0.5269]]], device='cuda:0') tensor([ 0.1232,  0.7563,  0.1755, -0.6133, -0.5010, -0.1148], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.391845703125\n",
            "tcost icost 0.0111846923828125 0.0\n",
            "tcost icost 0.055084228515625 0.0\n",
            "tcost icost 0.0121002197265625 0.0\n",
            "tcost icost -0.011932373046875 0.0\n",
            "tcost icost -0.0046234130859375 0.0\n",
            "tcost icost 0.12548828125 0.0\n",
            "loss tensor([[0.1329]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.8040,  0.4520,  0.3864],\n",
            "         [ 0.9990, -0.0273, -0.0354],\n",
            "         [ 0.4531,  0.0102, -0.0501],\n",
            "         [ 0.7191, -0.3947,  0.2016],\n",
            "         [ 0.5979,  0.4927,  0.6322],\n",
            "         [-0.0188, -0.2840, -0.5616]]], device='cuda:0') tensor([ 0.9733,  0.1606,  0.3085, -0.6240, -0.3310, -0.1560], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1329345703125\n",
            "tcost icost -0.1695556640625 0.0\n",
            "tcost icost -0.0296478271484375 0.0\n",
            "tcost icost -0.232666015625 0.0\n",
            "tcost icost -0.05169677734375 0.0\n",
            "tcost icost -0.138916015625 0.0\n",
            "tcost icost -0.0162811279296875 0.0\n",
            "loss tensor([[-0.5234]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.8247,  0.4305,  0.3668],\n",
            "         [ 0.9994, -0.0164, -0.0292],\n",
            "         [ 0.5090,  0.0329, -0.0288],\n",
            "         [ 0.7621, -0.3826,  0.2073],\n",
            "         [ 0.6193,  0.4875,  0.6155],\n",
            "         [ 0.0151, -0.2711, -0.5942]]], device='cuda:0') tensor([ 0.6999,  0.4429,  0.2210, -0.5546, -0.4859, -0.1489], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5234375\n",
            "tcost icost -0.11138916015625 0.0\n",
            "tcost icost -0.003704071044921875 0.0\n",
            "tcost icost -0.08990478515625 0.0\n",
            "tcost icost -0.0406494140625 0.0\n",
            "tcost icost -0.061798095703125 0.0\n",
            "tcost icost 0.0006580352783203125 0.0\n",
            "loss tensor([[-0.2573]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.8278,  0.4262,  0.3647],\n",
            "         [ 0.9989, -0.0247, -0.0410],\n",
            "         [ 0.5729,  0.0736,  0.0104],\n",
            "         [ 0.8024, -0.3734,  0.2099],\n",
            "         [ 0.6327,  0.4846,  0.6041],\n",
            "         [ 0.0447, -0.2621, -0.6271]]], device='cuda:0') tensor([ 0.0338,  1.0000, -0.0197, -0.4716, -0.6132, -0.1291], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.25732421875\n",
            "tcost icost 0.0084991455078125 0.0\n",
            "tcost icost -0.10076904296875 0.0\n",
            "tcost icost -0.0325927734375 0.0\n",
            "tcost icost -0.00795745849609375 0.0\n",
            "tcost icost -0.007404327392578125 0.0\n",
            "tcost icost -0.09588623046875 0.0\n",
            "loss tensor([[-0.1759]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.8436,  0.4079,  0.3492],\n",
            "         [ 0.9991, -0.0188, -0.0380],\n",
            "         [ 0.6338,  0.1040,  0.0452],\n",
            "         [ 0.8460, -0.3520,  0.2242],\n",
            "         [ 0.6562,  0.4746,  0.5867],\n",
            "         [ 0.0902, -0.2365, -0.6466]]], device='cuda:0') tensor([ 0.7023,  0.6628,  0.1172, -0.5336, -0.4810, -0.2611], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1759033203125\n",
            "tcost icost -0.11187744140625 0.0\n",
            "tcost icost 0.0084686279296875 0.0\n",
            "tcost icost 0.1226806640625 0.0\n",
            "tcost icost -0.0280914306640625 0.0\n",
            "tcost icost -0.0455322265625 0.0\n",
            "tcost icost 0.0091552734375 0.0\n",
            "loss tensor([[-0.0498]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.8398,  0.4116,  0.3541],\n",
            "         [ 0.9974, -0.0371, -0.0614],\n",
            "         [ 0.7026,  0.1494,  0.0918],\n",
            "         [ 0.8855, -0.3362,  0.2316],\n",
            "         [ 0.6731,  0.4669,  0.5736],\n",
            "         [ 0.1316, -0.2149, -0.6690]]], device='cuda:0') tensor([-0.4276,  1.0000, -0.1645, -0.4484, -0.5741, -0.2324], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.049835205078125\n",
            "tcost icost -0.0006570816040039062 0.0\n",
            "tcost icost -0.276611328125 0.0\n",
            "tcost icost -0.055511474609375 0.0\n",
            "tcost icost -0.056182861328125 0.0\n",
            "tcost icost -0.0260009765625 0.0\n",
            "tcost icost -0.11334228515625 0.0\n",
            "loss tensor([[-0.4194]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.8449,  0.4042,  0.3503],\n",
            "         [ 0.9966, -0.0442, -0.0691],\n",
            "         [ 0.7674,  0.1814,  0.1332],\n",
            "         [ 0.9206, -0.2966,  0.2539],\n",
            "         [ 0.6963,  0.4528,  0.5569],\n",
            "         [ 0.1849, -0.1812, -0.6753]]], device='cuda:0') tensor([ 0.1706,  0.8285,  0.0441, -0.6431, -0.4636, -0.3591], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.41943359375\n",
            "tcost icost 0.01129913330078125 0.0\n",
            "tcost icost 0.056671142578125 0.0\n",
            "tcost icost -0.002864837646484375 0.0\n",
            "tcost icost -0.01302337646484375 0.0\n",
            "tcost icost -0.003498077392578125 0.0\n",
            "tcost icost 0.0870361328125 0.0\n",
            "loss tensor([[0.0996]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.8609,  0.3839,  0.3339],\n",
            "         [ 0.9966, -0.0416, -0.0717],\n",
            "         [ 0.8247,  0.2120,  0.1671],\n",
            "         [ 0.9301, -0.2662,  0.2530],\n",
            "         [ 0.7149,  0.4418,  0.5420],\n",
            "         [ 0.2218, -0.1641, -0.7050]]], device='cuda:0') tensor([ 0.6654,  0.4999, -0.0334, -0.5385, -0.5505, -0.1104], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.099609375\n",
            "tcost icost -0.1043701171875 0.0\n",
            "tcost icost -0.0007066726684570312 0.0\n",
            "tcost icost 0.01404571533203125 0.0\n",
            "tcost icost -0.0279998779296875 0.0\n",
            "tcost icost -0.055450439453125 0.0\n",
            "tcost icost 0.01476287841796875 0.0\n",
            "loss tensor([[-0.1417]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.8597,  0.3840,  0.3368],\n",
            "         [ 0.9941, -0.0576, -0.0923],\n",
            "         [ 0.8776,  0.2546,  0.2145],\n",
            "         [ 0.9376, -0.2453,  0.2464],\n",
            "         [ 0.7220,  0.4370,  0.5364],\n",
            "         [ 0.2536, -0.1515, -0.7367]]], device='cuda:0') tensor([-0.2823,  1.0000, -0.3256, -0.3992, -0.7154, -0.0531], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1417236328125\n",
            "tcost icost 0.00214385986328125 0.0\n",
            "tcost icost -0.2225341796875 0.0\n",
            "tcost icost -0.056732177734375 0.0\n",
            "tcost icost -0.05908203125 0.0\n",
            "tcost icost -0.0287628173828125 0.0\n",
            "tcost icost -0.1781005859375 0.0\n",
            "loss tensor([[-0.4111]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.8659,  0.3743,  0.3320],\n",
            "         [ 0.9931, -0.0634, -0.0986],\n",
            "         [ 0.9236,  0.2854,  0.2558],\n",
            "         [ 0.9447, -0.2071,  0.2544],\n",
            "         [ 0.7350,  0.4260,  0.5275],\n",
            "         [ 0.2975, -0.1283, -0.7541]]], device='cuda:0') tensor([ 0.3007,  0.8302, -0.1278, -0.5806, -0.6304, -0.1717], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4111328125\n",
            "tcost icost -0.0113983154296875 0.0\n",
            "tcost icost 0.095458984375 0.0\n",
            "tcost icost -0.0013132095336914062 0.0\n",
            "tcost icost -0.01091766357421875 0.0\n",
            "tcost icost -0.003932952880859375 0.0\n",
            "tcost icost 0.00328826904296875 0.0\n",
            "loss tensor([[0.0649]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.8807,  0.3528,  0.3161],\n",
            "         [ 0.9942, -0.0564, -0.0920],\n",
            "         [ 0.9229,  0.2817,  0.2626],\n",
            "         [ 0.9494, -0.1551,  0.2732],\n",
            "         [ 0.7747,  0.3955,  0.4933],\n",
            "         [ 0.3542, -0.0880, -0.7576]]], device='cuda:0') tensor([ 0.5922,  0.3092,  0.1576, -0.8780, -0.2132, -0.3830], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.06494140625\n",
            "tcost icost -0.0880126953125 0.0\n",
            "tcost icost -0.002716064453125 0.0\n",
            "tcost icost -0.1258544921875 0.0\n",
            "tcost icost -0.048370361328125 0.0\n",
            "tcost icost -0.1510009765625 0.0\n",
            "tcost icost -0.019256591796875 0.0\n",
            "loss tensor([[-0.3381]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.8864,  0.3426,  0.3113],\n",
            "         [ 0.9939, -0.0590, -0.0936],\n",
            "         [ 0.9182,  0.2841,  0.2761],\n",
            "         [ 0.9509, -0.1088,  0.2898],\n",
            "         [ 0.8026,  0.3712,  0.4670],\n",
            "         [ 0.4060, -0.0514, -0.7614]]], device='cuda:0') tensor([ 0.0224,  0.7440,  0.0328, -0.8349, -0.3531, -0.3745], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.338134765625\n",
            "search tensor([[[ 0.8026,  0.3712,  0.4670],\n",
            "         [ 0.4060, -0.0514, -0.7614],\n",
            "         [-0.3678,  0.1043,  0.4135],\n",
            "         [-0.0656,  0.0877,  0.1001],\n",
            "         [-0.0881, -0.1346,  0.4043],\n",
            "         [-0.2749,  0.1338, -0.0109]]], device='cuda:0') tensor([[[-0.3531],\n",
            "         [-0.3745],\n",
            "         [ 0.0241],\n",
            "         [-0.3541],\n",
            "         [-0.4026],\n",
            "         [ 0.5079]]], device='cuda:0')\n",
            "tcost icost 0.0077972412109375 0.0\n",
            "tcost icost 0.039794921875 0.0\n",
            "tcost icost 0.20556640625 0.0\n",
            "tcost icost -0.0088348388671875 0.0\n",
            "tcost icost -0.0035533905029296875 0.0\n",
            "tcost icost 0.06707763671875 0.0\n",
            "loss tensor([[0.2410]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 8.9250e-01,  2.6803e-01,  3.6278e-01],\n",
            "         [ 3.0558e-01, -1.5132e-01, -8.6067e-01],\n",
            "         [-2.6746e-01,  2.0424e-01,  3.1314e-01],\n",
            "         [ 3.4505e-02, -1.2363e-02, -4.9546e-06],\n",
            "         [-1.8804e-01, -2.3451e-01,  3.0391e-01],\n",
            "         [-1.7464e-01,  2.3366e-01,  8.9074e-02]]], device='cuda:0') tensor([ 0.0461, -0.2813, -0.1848, -0.3577, -0.1676,  0.3215], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.240966796875\n",
            "tcost icost 0.0214996337890625 0.0\n",
            "tcost icost 0.004756927490234375 0.0\n",
            "tcost icost -0.05419921875 0.0\n",
            "tcost icost -0.0260772705078125 0.0\n",
            "tcost icost -0.1185302734375 0.0\n",
            "tcost icost 0.0101318359375 0.0\n",
            "loss tensor([[-0.1089]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.8737,  0.2996,  0.3833],\n",
            "         [ 0.2091, -0.2268, -0.9512],\n",
            "         [-0.1686,  0.3011,  0.3747],\n",
            "         [ 0.0945, -0.1034, -0.0998],\n",
            "         [-0.1392, -0.2477,  0.2802],\n",
            "         [-0.1115,  0.2801,  0.0941]]], device='cuda:0') tensor([-0.7903,  0.5219, -0.4048, -0.1908, -0.3819,  0.3721], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.10888671875\n",
            "tcost icost -0.003948211669921875 0.0\n",
            "tcost icost -0.32177734375 0.0\n",
            "tcost icost -0.059722900390625 0.0\n",
            "tcost icost -0.15185546875 0.0\n",
            "tcost icost -0.035614013671875 0.0\n",
            "tcost icost -0.2191162109375 0.0\n",
            "loss tensor([[-0.6055]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.8743,  0.3001,  0.3816],\n",
            "         [ 0.1621, -0.2475, -0.9552],\n",
            "         [-0.0924,  0.3565,  0.4073],\n",
            "         [ 0.1605, -0.0868, -0.0840],\n",
            "         [-0.1022, -0.2678,  0.2569],\n",
            "         [-0.0348,  0.3440,  0.1475]]], device='cuda:0') tensor([-0.3541,  0.3421, -0.2435, -0.3331, -0.2911,  0.2711], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.60546875\n",
            "tcost icost 0.007740020751953125 0.0\n",
            "tcost icost -0.1348876953125 0.0\n",
            "tcost icost -0.0262298583984375 0.0\n",
            "tcost icost -0.013397216796875 0.0\n",
            "tcost icost -0.0016698837280273438 0.0\n",
            "tcost icost -0.137939453125 0.0\n",
            "loss tensor([[-0.2273]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.8959,  0.2708,  0.3523],\n",
            "         [ 0.1621, -0.2334, -0.9588],\n",
            "         [-0.0268,  0.3797,  0.4051],\n",
            "         [ 0.2386, -0.0412, -0.0399],\n",
            "         [-0.0879, -0.3036,  0.2180],\n",
            "         [ 0.0487,  0.4171,  0.2175]]], device='cuda:0') tensor([ 0.3134, -0.0419, -0.0501, -0.4805, -0.1140,  0.1519], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.227294921875\n",
            "tcost icost 0.0015783309936523438 0.0\n",
            "tcost icost -0.0041656494140625 0.0\n",
            "tcost icost -0.09735107421875 0.0\n",
            "tcost icost -0.03570556640625 0.0\n",
            "tcost icost -0.133544921875 0.0\n",
            "tcost icost 0.003536224365234375 0.0\n",
            "loss tensor([[-0.1926]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.8917,  0.2766,  0.3582],\n",
            "         [ 0.1418, -0.2445, -0.9592],\n",
            "         [ 0.0381,  0.4174,  0.4344],\n",
            "         [ 0.3125, -0.0113, -0.0032],\n",
            "         [-0.0439, -0.3106,  0.2102],\n",
            "         [ 0.1216,  0.4738,  0.2647]]], device='cuda:0') tensor([-0.3731,  0.5020, -0.1902, -0.3676, -0.2972,  0.1888], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.192626953125\n",
            "tcost icost 0.00702667236328125 0.0\n",
            "tcost icost -0.1798095703125 0.0\n",
            "tcost icost -0.0357666015625 0.0\n",
            "tcost icost -0.020751953125 0.0\n",
            "tcost icost -0.00936126708984375 0.0\n",
            "tcost icost -0.1524658203125 0.0\n",
            "loss tensor([[-0.2949]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9016,  0.2616,  0.3446],\n",
            "         [ 0.1481, -0.2361, -0.9604],\n",
            "         [ 0.1019,  0.4398,  0.4481],\n",
            "         [ 0.3922,  0.0345,  0.0499],\n",
            "         [-0.0149, -0.3293,  0.1907],\n",
            "         [ 0.1985,  0.5352,  0.3256]]], device='cuda:0') tensor([ 0.2246,  0.1927, -0.0201, -0.4937, -0.1565,  0.0618], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.294921875\n",
            "tcost icost 0.0125274658203125 0.0\n",
            "tcost icost 0.023284912109375 0.0\n",
            "tcost icost 0.212158203125 0.0\n",
            "tcost icost -0.0213470458984375 0.0\n",
            "tcost icost -0.06854248046875 0.0\n",
            "tcost icost 0.0157470703125 0.0\n",
            "loss tensor([[0.1541]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.8791,  0.2931,  0.3758],\n",
            "         [ 0.1118, -0.2605, -0.9590],\n",
            "         [ 0.1749,  0.4831,  0.4864],\n",
            "         [ 0.4681,  0.0591,  0.0887],\n",
            "         [ 0.0345, -0.3242,  0.1971],\n",
            "         [ 0.2714,  0.5847,  0.3668]]], device='cuda:0') tensor([-1.0000,  1.0000, -0.5880, -0.3290, -0.3769,  0.1191], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.154052734375\n",
            "tcost icost -0.00811004638671875 0.0\n",
            "tcost icost -0.447998046875 0.0\n",
            "tcost icost -0.072265625 0.0\n",
            "tcost icost -0.1741943359375 0.0\n",
            "tcost icost -0.03741455078125 0.0\n",
            "tcost icost -0.204833984375 0.0\n",
            "loss tensor([[-0.7422]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.8614,  0.3149,  0.3986],\n",
            "         [ 0.0881, -0.2781, -0.9565],\n",
            "         [ 0.2473,  0.5181,  0.5188],\n",
            "         [ 0.5469,  0.0984,  0.1420],\n",
            "         [ 0.0779, -0.3246,  0.2005],\n",
            "         [ 0.3453,  0.6333,  0.4182]]], device='cuda:0') tensor([-0.6782,  0.9166, -0.4757, -0.4595, -0.2870,  0.0100], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.7421875\n",
            "tcost icost -0.00201416015625 0.0\n",
            "tcost icost -0.368896484375 0.0\n",
            "tcost icost -0.07073974609375 0.0\n",
            "tcost icost -0.09326171875 0.0\n",
            "tcost icost -0.025421142578125 0.0\n",
            "tcost icost -0.14453125 0.0\n",
            "loss tensor([[-0.5615]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.8514,  0.3257,  0.4112],\n",
            "         [ 0.0755, -0.2897, -0.9541],\n",
            "         [ 0.3196,  0.5454,  0.5465],\n",
            "         [ 0.6275,  0.1498,  0.2058],\n",
            "         [ 0.1133, -0.3326,  0.1980],\n",
            "         [ 0.4174,  0.6793,  0.4769]]], device='cuda:0') tensor([-0.2112,  0.8022, -0.3325, -0.6389, -0.1620, -0.1156], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5615234375\n",
            "tcost icost 0.0160064697265625 0.0\n",
            "tcost icost -0.182373046875 0.0\n",
            "tcost icost -0.049652099609375 0.0\n",
            "tcost icost -0.016204833984375 0.0\n",
            "tcost icost -0.0011205673217773438 0.0\n",
            "tcost icost -0.0258026123046875 0.0\n",
            "loss tensor([[-0.2163]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.8524,  0.3238,  0.4105],\n",
            "         [ 0.0751, -0.2924, -0.9534],\n",
            "         [ 0.3924,  0.5668,  0.5681],\n",
            "         [ 0.7069,  0.2081,  0.2749],\n",
            "         [ 0.1335, -0.3553,  0.1775],\n",
            "         [ 0.4753,  0.7052,  0.5261]]], device='cuda:0') tensor([ 0.3997,  0.5161, -0.1958, -0.7905,  0.0673, -0.2388], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.21630859375\n",
            "tcost icost -0.0168914794921875 0.0\n",
            "tcost icost 0.0307464599609375 0.0\n",
            "tcost icost 0.02935791015625 0.0\n",
            "tcost icost -0.016387939453125 0.0\n",
            "tcost icost -0.1363525390625 0.0\n",
            "tcost icost -0.01129150390625 0.0\n",
            "loss tensor([[-0.0735]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.8487,  0.3280,  0.4150],\n",
            "         [ 0.0802, -0.2922, -0.9530],\n",
            "         [ 0.4575,  0.5845,  0.5856],\n",
            "         [ 0.7848,  0.2528,  0.3340],\n",
            "         [ 0.1698, -0.3617,  0.1728],\n",
            "         [ 0.4973,  0.6842,  0.5335]]], device='cuda:0') tensor([-0.0815,  0.5500,  0.0474, -0.6928, -0.1274, -0.2171], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.07354736328125\n",
            "tcost icost 0.02056884765625 0.0\n",
            "tcost icost -0.052276611328125 0.0\n",
            "tcost icost -0.00952911376953125 0.0\n",
            "tcost icost -0.01348114013671875 0.0\n",
            "tcost icost -0.00463104248046875 0.0\n",
            "tcost icost 0.043487548828125 0.0\n",
            "loss tensor([[-0.0214]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.8559,  0.3207,  0.4057],\n",
            "         [ 0.0942, -0.2833, -0.9544],\n",
            "         [ 0.5087,  0.6027,  0.6026],\n",
            "         [ 0.8620,  0.2826,  0.3752],\n",
            "         [ 0.2165, -0.3554,  0.1798],\n",
            "         [ 0.5294,  0.6653,  0.5264]]], device='cuda:0') tensor([ 0.4849,  0.2851, -0.0181, -0.5433, -0.2700, -0.0614], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0214080810546875\n",
            "tcost icost -0.036712646484375 0.0\n",
            "tcost icost -0.00390625 0.0\n",
            "tcost icost -0.00927734375 0.0\n",
            "tcost icost -0.0285797119140625 0.0\n",
            "tcost icost -0.08624267578125 0.0\n",
            "tcost icost 0.00263214111328125 0.0\n",
            "loss tensor([[-0.1236]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.8468,  0.3316,  0.4159],\n",
            "         [ 0.0914, -0.2882, -0.9532],\n",
            "         [ 0.5262,  0.5987,  0.6039],\n",
            "         [ 0.8780,  0.2847,  0.3847],\n",
            "         [ 0.2715, -0.3382,  0.1975],\n",
            "         [ 0.5596,  0.6474,  0.5175]]], device='cuda:0') tensor([-0.5288,  1.0000, -0.1934, -0.4356, -0.4389, -0.0356], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.12359619140625\n",
            "tcost icost 0.0014410018920898438 0.0\n",
            "tcost icost -0.337158203125 0.0\n",
            "tcost icost -0.062744140625 0.0\n",
            "tcost icost -0.036041259765625 0.0\n",
            "tcost icost -0.0203704833984375 0.0\n",
            "tcost icost -0.1357421875 0.0\n",
            "loss tensor([[-0.4727]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.8456,  0.3331,  0.4172],\n",
            "         [ 0.0975, -0.2880, -0.9526],\n",
            "         [ 0.5441,  0.5872,  0.5993],\n",
            "         [ 0.8724,  0.2885,  0.3946],\n",
            "         [ 0.3190, -0.3290,  0.2084],\n",
            "         [ 0.5812,  0.6301,  0.5150]]], device='cuda:0') tensor([-0.0461,  0.8528, -0.0508, -0.5645, -0.3253, -0.1672], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.47265625\n",
            "tcost icost 0.021209716796875 0.0\n",
            "tcost icost -0.11962890625 0.0\n",
            "tcost icost -0.0301361083984375 0.0\n",
            "tcost icost -0.01126861572265625 0.0\n",
            "tcost icost -0.0021457672119140625 0.0\n",
            "tcost icost -0.0221099853515625 0.0\n",
            "loss tensor([[-0.1335]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.8553,  0.3226,  0.4054],\n",
            "         [ 0.1136, -0.2798, -0.9533],\n",
            "         [ 0.5681,  0.5729,  0.5908],\n",
            "         [ 0.8628,  0.2977,  0.4087],\n",
            "         [ 0.3550, -0.3348,  0.2020],\n",
            "         [ 0.5951,  0.6151,  0.5173]]], device='cuda:0') tensor([ 0.6125,  0.4866,  0.0574, -0.6918, -0.0901, -0.2952], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.133544921875\n",
            "tcost icost -0.06854248046875 0.0\n",
            "tcost icost -0.005115509033203125 0.0\n",
            "tcost icost 0.0248870849609375 0.0\n",
            "tcost icost -0.03155517578125 0.0\n",
            "tcost icost -0.1141357421875 0.0\n",
            "tcost icost -0.01056671142578125 0.0\n",
            "loss tensor([[-0.1571]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.8517,  0.3268,  0.4096],\n",
            "         [ 0.1159, -0.2824, -0.9523],\n",
            "         [ 0.5805,  0.5615,  0.5898],\n",
            "         [ 0.8564,  0.3006,  0.4197],\n",
            "         [ 0.4005, -0.3273,  0.2082],\n",
            "         [ 0.6091,  0.6009,  0.5176]]], device='cuda:0') tensor([-0.2649,  1.0000, -0.0462, -0.6077, -0.2816, -0.2803], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1571044921875\n",
            "tcost icost 0.0127105712890625 0.0\n",
            "tcost icost -0.24609375 0.0\n",
            "tcost icost -0.050537109375 0.0\n",
            "tcost icost -0.00955963134765625 0.0\n",
            "tcost icost -0.00384521484375 0.0\n",
            "tcost icost -0.021209716796875 0.0\n",
            "loss tensor([[-0.2715]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.8561,  0.3222,  0.4041],\n",
            "         [ 0.1263, -0.2789, -0.9520],\n",
            "         [ 0.5958,  0.5489,  0.5863],\n",
            "         [ 0.8490,  0.3053,  0.4314],\n",
            "         [ 0.4415, -0.3244,  0.2089],\n",
            "         [ 0.6207,  0.5878,  0.5188]]], device='cuda:0') tensor([ 0.2336,  0.7633,  0.0239, -0.6581, -0.2030, -0.3146], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.271484375\n",
            "tcost icost 0.01143646240234375 0.0\n",
            "tcost icost 0.04925537109375 0.0\n",
            "tcost icost -0.0028533935546875 0.0\n",
            "tcost icost -0.0175628662109375 0.0\n",
            "tcost icost -0.010284423828125 0.0\n",
            "tcost icost 0.019073486328125 0.0\n",
            "loss tensor([[0.0452]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.8658,  0.3117,  0.3914],\n",
            "         [ 0.1450, -0.2682, -0.9524],\n",
            "         [ 0.6050,  0.5397,  0.5854],\n",
            "         [ 0.8485,  0.3022,  0.4344],\n",
            "         [ 0.4869, -0.3137,  0.2162],\n",
            "         [ 0.6413,  0.5720,  0.5115]]], device='cuda:0') tensor([ 0.4789,  0.5293, -0.0030, -0.5389, -0.3105, -0.2262], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.045166015625\n",
            "tcost icost -0.035308837890625 0.0\n",
            "tcost icost 0.016204833984375 0.0\n",
            "tcost icost 0.151611328125 0.0\n",
            "tcost icost -0.0190277099609375 0.0\n",
            "tcost icost -0.042205810546875 0.0\n",
            "tcost icost 0.01187896728515625 0.0\n",
            "loss tensor([[0.0675]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.8701,  0.3097,  0.3835],\n",
            "         [ 0.1695, -0.2518, -0.9528],\n",
            "         [ 0.6331,  0.5237,  0.5701],\n",
            "         [ 0.8532,  0.2928,  0.4317],\n",
            "         [ 0.5363, -0.2958,  0.2295],\n",
            "         [ 0.6652,  0.5545,  0.5000]]], device='cuda:0') tensor([-0.2911,  0.5013,  0.4166, -0.4345, -0.4282, -0.1765], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0675048828125\n",
            "tcost icost 0.01084136962890625 0.0\n",
            "tcost icost -0.1458740234375 0.0\n",
            "tcost icost -0.005496978759765625 0.0\n",
            "tcost icost -0.0085906982421875 0.0\n",
            "tcost icost -0.00760650634765625 0.0\n",
            "tcost icost -0.02545166015625 0.0\n",
            "loss tensor([[-0.1512]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.8830,  0.2964,  0.3640],\n",
            "         [ 0.2041, -0.2280, -0.9520],\n",
            "         [ 0.6668,  0.5036,  0.5493],\n",
            "         [ 0.8547,  0.2877,  0.4321],\n",
            "         [ 0.5807, -0.2864,  0.2328],\n",
            "         [ 0.6812,  0.5409,  0.4933]]], device='cuda:0') tensor([ 0.3768,  0.0780,  0.5538, -0.5006, -0.3018, -0.2679], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1512451171875\n",
            "search tensor([[[ 0.5807, -0.2864,  0.2328],\n",
            "         [ 0.6812,  0.5409,  0.4933],\n",
            "         [-0.2228, -0.3054,  0.0686],\n",
            "         [ 0.4789,  0.2820, -0.3540],\n",
            "         [ 0.4123,  0.5229,  0.4540],\n",
            "         [-0.5342,  0.5103,  0.3476]]], device='cuda:0') tensor([[[-0.3018],\n",
            "         [-0.2679],\n",
            "         [ 0.0388],\n",
            "         [ 0.7991],\n",
            "         [-0.4013],\n",
            "         [-0.2913]]], device='cuda:0')\n",
            "tcost icost 0.0009007453918457031 0.0\n",
            "tcost icost 0.0185394287109375 0.0\n",
            "tcost icost 0.12091064453125 0.0\n",
            "tcost icost -0.1640625 0.0\n",
            "tcost icost -0.051422119140625 0.0\n",
            "tcost icost -0.0226593017578125 0.0\n",
            "loss tensor([[-0.0512]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.4801, -0.3861,  0.1325],\n",
            "         [ 0.5539,  0.6110,  0.5656],\n",
            "         [-0.3226, -0.4051, -0.0315],\n",
            "         [ 0.5785,  0.3817, -0.2537],\n",
            "         [ 0.5119,  0.4224,  0.3535],\n",
            "         [-0.4336,  0.6098,  0.4473]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.7869,  0.5875, -0.3710, -0.2983], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.05120849609375\n",
            "tcost icost -0.1456298828125 0.0\n",
            "tcost icost -0.05694580078125 0.0\n",
            "tcost icost -0.378662109375 0.0\n",
            "tcost icost -0.035186767578125 0.0\n",
            "tcost icost -0.07330322265625 0.0\n",
            "tcost icost -0.0068359375 0.0\n",
            "loss tensor([[-0.5811]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.4343, -0.4444,  0.0743],\n",
            "         [ 0.4810,  0.6352,  0.6043],\n",
            "         [-0.3692, -0.4600, -0.0803],\n",
            "         [ 0.6507,  0.4151, -0.2051],\n",
            "         [ 0.6106,  0.4858,  0.4169],\n",
            "         [-0.4143,  0.6409,  0.4285]]], device='cuda:0') tensor([ 0.8560, -0.9285,  0.7133,  0.7290, -0.5372, -0.2860], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5810546875\n",
            "tcost icost -0.1143798828125 0.0\n",
            "tcost icost -0.0489501953125 0.0\n",
            "tcost icost -0.383544921875 0.0\n",
            "tcost icost -0.029876708984375 0.0\n",
            "tcost icost -0.029327392578125 0.0\n",
            "tcost icost -0.0002472400665283203 0.0\n",
            "loss tensor([[-0.5103]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.4175, -0.4827,  0.0359],\n",
            "         [ 0.4384,  0.6433,  0.6277],\n",
            "         [-0.3889, -0.4944, -0.1096],\n",
            "         [ 0.7080,  0.4264, -0.1835],\n",
            "         [ 0.6991,  0.5365,  0.4727],\n",
            "         [-0.4255,  0.6465,  0.3752]]], device='cuda:0') tensor([ 0.6653, -0.8945,  0.5951,  0.7941, -0.5831, -0.2735], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.51025390625\n",
            "tcost icost -0.07080078125 0.0\n",
            "tcost icost -0.037384033203125 0.0\n",
            "tcost icost -0.37841796875 0.0\n",
            "tcost icost -0.027984619140625 0.0\n",
            "tcost icost -0.0195465087890625 0.0\n",
            "tcost icost 0.0017490386962890625 0.0\n",
            "loss tensor([[-0.4436]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.4193, -0.5082,  0.0108],\n",
            "         [ 0.4184,  0.6428,  0.6418],\n",
            "         [-0.3907, -0.5153, -0.1265],\n",
            "         [ 0.7565,  0.4228, -0.1809],\n",
            "         [ 0.7081,  0.5240,  0.4733],\n",
            "         [-0.4679,  0.6168,  0.3036]]], device='cuda:0') tensor([ 0.4274, -0.8311,  0.4547,  0.8536, -0.6295, -0.2526], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.443603515625\n",
            "tcost icost -0.0117340087890625 0.0\n",
            "tcost icost -0.0198822021484375 0.0\n",
            "tcost icost -0.358642578125 0.0\n",
            "tcost icost -0.0260772705078125 0.0\n",
            "tcost icost -0.00786590576171875 0.0\n",
            "tcost icost 0.0036983489990234375 0.0\n",
            "loss tensor([[-0.3420]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.4397, -0.5220, -0.0027],\n",
            "         [ 0.4180,  0.6374,  0.6473],\n",
            "         [-0.3799, -0.5263, -0.1358],\n",
            "         [ 0.7987,  0.4132, -0.1882],\n",
            "         [ 0.7239,  0.5059,  0.4690],\n",
            "         [-0.5285,  0.5614,  0.2233]]], device='cuda:0') tensor([ 0.0775, -0.7196,  0.2818,  0.8724, -0.6417, -0.2276], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.342041015625\n",
            "tcost icost 0.01320648193359375 0.0\n",
            "tcost icost 0.00240325927734375 0.0\n",
            "tcost icost -0.2344970703125 0.0\n",
            "tcost icost -0.009918212890625 0.0\n",
            "tcost icost -0.0129852294921875 0.0\n",
            "tcost icost -0.00775909423828125 0.0\n",
            "loss tensor([[-0.1949]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.4727, -0.5264, -0.0051],\n",
            "         [ 0.4410,  0.6283,  0.6408],\n",
            "         [-0.3559, -0.5278, -0.1385],\n",
            "         [ 0.8368,  0.4036, -0.1967],\n",
            "         [ 0.7490,  0.4802,  0.4566],\n",
            "         [-0.5663,  0.5298,  0.1552]]], device='cuda:0') tensor([-0.2418, -0.4052,  0.0803,  0.8619, -0.6103, -0.2325], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1949462890625\n",
            "tcost icost 0.0026645660400390625 0.0\n",
            "tcost icost 0.07122802734375 0.0\n",
            "tcost icost 0.2919921875 0.0\n",
            "tcost icost -0.045379638671875 0.0\n",
            "tcost icost -0.05364990234375 0.0\n",
            "tcost icost -0.02655029296875 0.0\n",
            "loss tensor([[0.2192]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.4452, -0.5602, -0.0412],\n",
            "         [ 0.4468,  0.6190,  0.6459],\n",
            "         [-0.3225, -0.5570, -0.1765],\n",
            "         [ 0.8890,  0.4278, -0.1634],\n",
            "         [ 0.7785,  0.4465,  0.4411],\n",
            "         [-0.5568,  0.5515,  0.1272]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.7429,  0.5526, -0.5773, -0.2542], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.21923828125\n",
            "tcost icost -0.1456298828125 0.0\n",
            "tcost icost -0.05694580078125 0.0\n",
            "tcost icost -0.36669921875 0.0\n",
            "tcost icost -0.03326416015625 0.0\n",
            "tcost icost -0.035797119140625 0.0\n",
            "tcost icost -0.000423431396484375 0.0\n",
            "loss tensor([[-0.5425]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.4270, -0.5868, -0.0699],\n",
            "         [ 0.4596,  0.6061,  0.6492],\n",
            "         [-0.2845, -0.5790, -0.2054],\n",
            "         [ 0.8963,  0.4226, -0.1346],\n",
            "         [ 0.8029,  0.4169,  0.4261],\n",
            "         [-0.5468,  0.5611,  0.0868]]], device='cuda:0') tensor([ 0.8300, -0.9785,  0.6300,  0.6179, -0.6251, -0.2460], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.54248046875\n",
            "tcost icost -0.108642578125 0.0\n",
            "tcost icost -0.048675537109375 0.0\n",
            "tcost icost -0.367919921875 0.0\n",
            "tcost icost -0.03009033203125 0.0\n",
            "tcost icost -0.029205322265625 0.0\n",
            "tcost icost 0.0022907257080078125 0.0\n",
            "loss tensor([[-0.4902]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.4173, -0.6073, -0.0920],\n",
            "         [ 0.4788,  0.5897,  0.6504],\n",
            "         [-0.2417, -0.5946, -0.2259],\n",
            "         [ 0.9043,  0.4108, -0.1165],\n",
            "         [ 0.8231,  0.3916,  0.4114],\n",
            "         [-0.5385,  0.5515,  0.0326]]], device='cuda:0') tensor([ 0.6407, -0.9462,  0.5003,  0.6801, -0.6737, -0.2245], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.490234375\n",
            "tcost icost -0.0650634765625 0.0\n",
            "tcost icost -0.037200927734375 0.0\n",
            "tcost icost -0.36328125 0.0\n",
            "tcost icost -0.027191162109375 0.0\n",
            "tcost icost -0.0204925537109375 0.0\n",
            "tcost icost 0.004978179931640625 0.0\n",
            "loss tensor([[-0.4229]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.4149, -0.6223, -0.1081],\n",
            "         [ 0.5039,  0.5701,  0.6488],\n",
            "         [-0.1931, -0.6047, -0.2395],\n",
            "         [ 0.9126,  0.3944, -0.1075],\n",
            "         [ 0.8452,  0.3633,  0.3921],\n",
            "         [-0.5182,  0.5404, -0.0289]]], device='cuda:0') tensor([ 0.3895, -0.8940,  0.3464,  0.7231, -0.6910, -0.2153], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4228515625\n",
            "tcost icost -0.0010690689086914062 0.0\n",
            "tcost icost -0.019744873046875 0.0\n",
            "tcost icost -0.339599609375 0.0\n",
            "tcost icost -0.0247650146484375 0.0\n",
            "tcost icost -0.009918212890625 0.0\n",
            "tcost icost 0.004730224609375 0.0\n",
            "loss tensor([[-0.3159]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.4223, -0.6307, -0.1177],\n",
            "         [ 0.5365,  0.5466,  0.6429],\n",
            "         [-0.1377, -0.6096, -0.2476],\n",
            "         [ 0.9206,  0.3760, -0.1054],\n",
            "         [ 0.8744,  0.3226,  0.3625],\n",
            "         [-0.4833,  0.5323, -0.0965]]], device='cuda:0') tensor([-0.0057, -0.7940,  0.1580,  0.7333, -0.6670, -0.2080], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.31591796875\n",
            "tcost icost 0.009918212890625 0.0\n",
            "tcost icost 0.0033016204833984375 0.0\n",
            "tcost icost -0.201904296875 0.0\n",
            "tcost icost -0.00615692138671875 0.0\n",
            "tcost icost -0.0113983154296875 0.0\n",
            "tcost icost -0.00846099853515625 0.0\n",
            "loss tensor([[-0.1676]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.4373, -0.6337, -0.1202],\n",
            "         [ 0.5835,  0.5139,  0.6288],\n",
            "         [-0.0738, -0.6086, -0.2495],\n",
            "         [ 0.9276,  0.3587, -0.1047],\n",
            "         [ 0.9026,  0.2774,  0.3292],\n",
            "         [-0.4464,  0.5287, -0.1585]]], device='cuda:0') tensor([-0.3353, -0.4632, -0.0487,  0.7197, -0.6450, -0.2071], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1676025390625\n",
            "tcost icost 8.976459503173828e-05 0.0\n",
            "tcost icost 0.032257080078125 0.0\n",
            "tcost icost 0.1771240234375 0.0\n",
            "tcost icost -0.15869140625 0.0\n",
            "tcost icost -0.0606689453125 0.0\n",
            "tcost icost -0.037261962890625 0.0\n",
            "loss tensor([[-0.0049]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.4152, -0.6575, -0.1473],\n",
            "         [ 0.5818,  0.5133,  0.6309],\n",
            "         [-0.0709, -0.6247, -0.2767],\n",
            "         [ 0.9307,  0.3565, -0.0814],\n",
            "         [ 0.9274,  0.2278,  0.2966],\n",
            "         [-0.3944,  0.5590, -0.1890]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.6827,  0.5206, -0.6120, -0.2385], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.004852294921875\n",
            "tcost icost -0.12139892578125 0.0\n",
            "tcost icost -0.05279541015625 0.0\n",
            "tcost icost -0.36181640625 0.0\n",
            "tcost icost -0.032470703125 0.0\n",
            "tcost icost -0.032989501953125 0.0\n",
            "tcost icost 0.0028820037841796875 0.0\n",
            "loss tensor([[-0.5059]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.3997, -0.6766, -0.1691],\n",
            "         [ 0.5850,  0.5086,  0.6317],\n",
            "         [-0.0571, -0.6364, -0.2968],\n",
            "         [ 0.9352,  0.3478, -0.0663],\n",
            "         [ 0.9430,  0.1946,  0.2699],\n",
            "         [-0.3398,  0.5727, -0.2308]]], device='cuda:0') tensor([ 0.8242, -0.9709,  0.5626,  0.5814, -0.6602, -0.2204], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.505859375\n",
            "tcost icost -0.08349609375 0.0\n",
            "tcost icost -0.04296875 0.0\n",
            "tcost icost -0.361083984375 0.0\n",
            "tcost icost -0.0294342041015625 0.0\n",
            "tcost icost -0.02587890625 0.0\n",
            "tcost icost -1.4841556549072266e-05 0.0\n",
            "loss tensor([[-0.4531]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.3902, -0.6916, -0.1860],\n",
            "         [ 0.5937,  0.4994,  0.6310],\n",
            "         [-0.0331, -0.6441, -0.3104],\n",
            "         [ 0.9409,  0.3335, -0.0584],\n",
            "         [ 0.9520,  0.1770,  0.2497],\n",
            "         [-0.2826,  0.5756, -0.2797]]], device='cuda:0') tensor([ 0.6346, -0.9182,  0.4265,  0.6434, -0.7052, -0.2100], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.453125\n",
            "tcost icost -0.040802001953125 0.0\n",
            "tcost icost -0.0297698974609375 0.0\n",
            "tcost icost -0.349609375 0.0\n",
            "tcost icost -0.0263824462890625 0.0\n",
            "tcost icost -0.016571044921875 0.0\n",
            "tcost icost -0.0019073486328125 0.0\n",
            "loss tensor([[-0.3823]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 3.8724e-01, -7.0199e-01, -1.9782e-01],\n",
            "         [ 6.0823e-01,  4.8573e-01,  6.2779e-01],\n",
            "         [-2.1948e-04, -6.4792e-01, -3.1900e-01],\n",
            "         [ 9.4687e-01,  3.1665e-01, -5.6246e-02],\n",
            "         [ 9.6199e-01,  1.5562e-01,  2.2439e-01],\n",
            "         [-2.1976e-01,  5.7113e-01, -3.3535e-01]]], device='cuda:0') tensor([ 0.3326, -0.8458,  0.2628,  0.6684, -0.7019, -0.2040], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.38232421875\n",
            "tcost icost 0.027374267578125 0.0\n",
            "tcost icost -0.007511138916015625 0.0\n",
            "tcost icost -0.2978515625 0.0\n",
            "tcost icost -0.0222320556640625 0.0\n",
            "tcost icost -0.004985809326171875 0.0\n",
            "tcost icost -0.001491546630859375 0.0\n",
            "loss tensor([[-0.2410]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.3889, -0.7092, -0.2057],\n",
            "         [ 0.6331,  0.4644,  0.6194],\n",
            "         [ 0.0405, -0.6477, -0.3238],\n",
            "         [ 0.9520,  0.3009, -0.0568],\n",
            "         [ 0.9734,  0.1250,  0.1922],\n",
            "         [-0.1550,  0.5635, -0.3923]]], device='cuda:0') tensor([ 0.2146, -0.6708,  0.0701,  0.6528, -0.6773, -0.2004], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.240966796875\n",
            "tcost icost 0.0180511474609375 0.0\n",
            "tcost icost 0.003566741943359375 0.0\n",
            "tcost icost -0.1812744140625 0.0\n",
            "tcost icost -0.0014667510986328125 0.0\n",
            "tcost icost -0.00991058349609375 0.0\n",
            "tcost icost -0.00800323486328125 0.0\n",
            "loss tensor([[-0.1379]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.3981, -0.7115, -0.2071],\n",
            "         [ 0.6729,  0.4302,  0.6018],\n",
            "         [ 0.0908, -0.6418, -0.3241],\n",
            "         [ 0.9567,  0.2848, -0.0597],\n",
            "         [ 0.9832,  0.0911,  0.1581],\n",
            "         [-0.0920,  0.5539, -0.4475]]], device='cuda:0') tensor([-0.1443, -0.2868, -0.1515,  0.6518, -0.6595, -0.1952], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.137939453125\n",
            "tcost icost 0.003780364990234375 0.0\n",
            "tcost icost 0.0572509765625 0.0\n",
            "tcost icost 0.0445556640625 0.0\n",
            "tcost icost -0.2066650390625 0.0\n",
            "tcost icost -0.060882568359375 0.0\n",
            "tcost icost -0.048431396484375 0.0\n",
            "loss tensor([[-0.1277]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.3805, -0.7288, -0.2282],\n",
            "         [ 0.6734,  0.4322,  0.5997],\n",
            "         [ 0.0990, -0.6500, -0.3394],\n",
            "         [ 0.9583,  0.2824, -0.0433],\n",
            "         [ 0.9902,  0.0530,  0.1292],\n",
            "         [-0.0210,  0.5807, -0.4640]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.4647,  0.5244, -0.6209, -0.2496], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.127685546875\n",
            "tcost icost -0.12139892578125 0.0\n",
            "tcost icost -0.05279541015625 0.0\n",
            "tcost icost -0.333740234375 0.0\n",
            "tcost icost -0.0287628173828125 0.0\n",
            "tcost icost -0.0304107666015625 0.0\n",
            "tcost icost -0.0008721351623535156 0.0\n",
            "loss tensor([[-0.4807]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.3687, -0.7424, -0.2448],\n",
            "         [ 0.6769,  0.4310,  0.5967],\n",
            "         [ 0.1164, -0.6537, -0.3494],\n",
            "         [ 0.9613,  0.2735, -0.0339],\n",
            "         [ 0.9935,  0.0339,  0.1091],\n",
            "         [ 0.0499,  0.6000, -0.4850]]], device='cuda:0') tensor([ 0.8158, -0.9741,  0.3217,  0.5863, -0.6561, -0.2456], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.480712890625\n",
            "search tensor([[[ 0.9935,  0.0339,  0.1091],\n",
            "         [ 0.0499,  0.6000, -0.4850],\n",
            "         [ 0.0829,  0.0098, -0.4760],\n",
            "         [-0.4164,  0.3251,  0.4362],\n",
            "         [-0.4694, -0.3878, -0.0016],\n",
            "         [-0.3443,  0.4598,  0.1098]]], device='cuda:0') tensor([[[-0.6561],\n",
            "         [-0.2456],\n",
            "         [ 0.3922],\n",
            "         [ 0.6254],\n",
            "         [ 0.3968],\n",
            "         [-1.0004]]], device='cuda:0')\n",
            "tcost icost -0.005878448486328125 0.0\n",
            "tcost icost -0.1597900390625 0.0\n",
            "tcost icost -0.005741119384765625 0.0\n",
            "tcost icost -0.1463623046875 0.0\n",
            "tcost icost -0.027252197265625 0.0\n",
            "tcost icost -0.0245208740234375 0.0\n",
            "loss tensor([[-0.2932]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.8925, -0.0662,  0.0090],\n",
            "         [ 0.1498,  0.6994, -0.3846],\n",
            "         [-0.0172, -0.0902, -0.5755],\n",
            "         [-0.3160,  0.4248,  0.5358],\n",
            "         [-0.5689, -0.4874, -0.1016],\n",
            "         [-0.4440,  0.3593,  0.0097]]], device='cuda:0') tensor([-0.0961, -0.7154,  0.6429,  0.4470,  0.4057, -0.9906], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.293212890625\n",
            "tcost icost 0.00864410400390625 0.0\n",
            "tcost icost 0.0123443603515625 0.0\n",
            "tcost icost -0.1939697265625 0.0\n",
            "tcost icost -0.0191192626953125 0.0\n",
            "tcost icost 0.00423431396484375 0.0\n",
            "tcost icost -0.0292205810546875 0.0\n",
            "loss tensor([[-0.1656]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.9271, -0.0777,  0.0014],\n",
            "         [ 0.1914,  0.7028, -0.3949],\n",
            "         [ 0.0227, -0.0819, -0.6200],\n",
            "         [-0.2633,  0.4746,  0.5536],\n",
            "         [-0.5030, -0.4923, -0.1942],\n",
            "         [-0.5433,  0.2608, -0.0902]]], device='cuda:0') tensor([-0.4850, -0.3482,  0.3998,  0.5095,  0.4026, -0.9776], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1656494140625\n",
            "tcost icost -0.0022525787353515625 0.0\n",
            "tcost icost -0.100830078125 0.0\n",
            "tcost icost 0.01421356201171875 0.0\n",
            "tcost icost 0.010772705078125 0.0\n",
            "tcost icost -0.0148773193359375 0.0\n",
            "tcost icost -0.0263824462890625 0.0\n",
            "loss tensor([[-0.0989]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.8782, -0.1355, -0.0550],\n",
            "         [ 0.2614,  0.7505, -0.3519],\n",
            "         [ 0.0139, -0.1290, -0.6910],\n",
            "         [-0.1887,  0.5453,  0.6112],\n",
            "         [-0.4401, -0.5335, -0.2895],\n",
            "         [-0.6383,  0.1619, -0.1887]]], device='cuda:0') tensor([ 0.5638, -1.0000,  0.9110,  0.2105,  0.4186, -0.9672], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.09893798828125\n",
            "tcost icost -0.003971099853515625 0.0\n",
            "tcost icost -0.038238525390625 0.0\n",
            "tcost icost -0.407958984375 0.0\n",
            "tcost icost -0.03118896484375 0.0\n",
            "tcost icost -0.12054443359375 0.0\n",
            "tcost icost -0.03729248046875 0.0\n",
            "loss tensor([[-0.4927]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.8597, -0.1707, -0.0829],\n",
            "         [ 0.3242,  0.7867, -0.3169],\n",
            "         [ 0.0147, -0.1602, -0.7337],\n",
            "         [-0.1240,  0.6041,  0.6512],\n",
            "         [-0.3753, -0.4850, -0.2560],\n",
            "         [-0.7266,  0.0626, -0.2869]]], device='cuda:0') tensor([ 0.3953, -0.8237,  0.8735,  0.3251,  0.2573, -0.9533], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.49267578125\n",
            "tcost icost 0.00795745849609375 0.0\n",
            "tcost icost -0.02020263671875 0.0\n",
            "tcost icost -0.391845703125 0.0\n",
            "tcost icost -0.029327392578125 0.0\n",
            "tcost icost -0.036468505859375 0.0\n",
            "tcost icost -0.029388427734375 0.0\n",
            "loss tensor([[-0.3904]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.8614, -0.1905, -0.0932],\n",
            "         [ 0.3807,  0.8095, -0.2958],\n",
            "         [ 0.0333, -0.1756, -0.7621],\n",
            "         [-0.0633,  0.6566,  0.6799],\n",
            "         [-0.3000, -0.4205, -0.2040],\n",
            "         [-0.8004, -0.0370, -0.3845]]], device='cuda:0') tensor([ 0.2075, -0.6156,  0.7767,  0.4003,  0.1638, -0.9391], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.390380859375\n",
            "tcost icost 0.0116119384765625 0.0\n",
            "tcost icost -0.0018930435180664062 0.0\n",
            "tcost icost -0.293212890625 0.0\n",
            "tcost icost -0.02691650390625 0.0\n",
            "tcost icost -0.01416015625 0.0\n",
            "tcost icost -0.0267791748046875 0.0\n",
            "loss tensor([[-0.2725]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.8833, -0.1936, -0.0867],\n",
            "         [ 0.4286,  0.8150, -0.2961],\n",
            "         [ 0.0717, -0.1740, -0.7819],\n",
            "         [-0.0067,  0.7037,  0.7016],\n",
            "         [-0.2180, -0.3526, -0.1481],\n",
            "         [-0.8530, -0.1368, -0.4812]]], device='cuda:0') tensor([-0.1270, -0.2958,  0.6140,  0.4373,  0.1237, -0.9230], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2724609375\n",
            "tcost icost 0.007843017578125 0.0\n",
            "tcost icost 0.0288848876953125 0.0\n",
            "tcost icost 0.037322998046875 0.0\n",
            "tcost icost 0.01473236083984375 0.0\n",
            "tcost icost -0.01016998291015625 0.0\n",
            "tcost icost -0.0219879150390625 0.0\n",
            "loss tensor([[0.0552]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9033, -0.1960, -0.0870],\n",
            "         [ 0.4752,  0.7983, -0.3237],\n",
            "         [ 0.1293, -0.1474, -0.7957],\n",
            "         [ 0.0450,  0.7217,  0.6908],\n",
            "         [-0.1449, -0.2955, -0.1047],\n",
            "         [-0.8198, -0.2167, -0.5301]]], device='cuda:0') tensor([-0.1684,  0.0377,  0.2256,  0.5045,  0.1337, -0.9090], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.05517578125\n",
            "tcost icost 0.006740570068359375 0.0\n",
            "tcost icost -0.0902099609375 0.0\n",
            "tcost icost -0.00110626220703125 0.0\n",
            "tcost icost -0.0443115234375 0.0\n",
            "tcost icost -0.022308349609375 0.0\n",
            "tcost icost -0.0192108154296875 0.0\n",
            "loss tensor([[-0.1337]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.8842, -0.2234, -0.1130],\n",
            "         [ 0.5250,  0.7884, -0.3207],\n",
            "         [ 0.1626, -0.1423, -0.8236],\n",
            "         [ 0.0981,  0.7267,  0.6799],\n",
            "         [-0.0862, -0.2485, -0.0730],\n",
            "         [-0.7725, -0.2864, -0.5668]]], device='cuda:0') tensor([ 0.7071, -0.5155,  0.6162,  0.2646,  0.1450, -0.8966], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1336669921875\n",
            "tcost icost -0.0200958251953125 0.0\n",
            "tcost icost -0.036407470703125 0.0\n",
            "tcost icost -0.32275390625 0.0\n",
            "tcost icost -0.0258026123046875 0.0\n",
            "tcost icost -0.040863037109375 0.0\n",
            "tcost icost -0.0286102294921875 0.0\n",
            "loss tensor([[-0.3770]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.8825, -0.2381, -0.1235],\n",
            "         [ 0.5628,  0.7634, -0.3171],\n",
            "         [ 0.2039, -0.1306, -0.8405],\n",
            "         [ 0.1504,  0.7308,  0.6658],\n",
            "         [-0.0270, -0.1873, -0.0224],\n",
            "         [-0.7110, -0.3550, -0.6070]]], device='cuda:0') tensor([ 0.4659, -0.2937,  0.5033,  0.3654,  0.0446, -0.8814], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.376953125\n",
            "tcost icost 0.00469207763671875 0.0\n",
            "tcost icost -0.007572174072265625 0.0\n",
            "tcost icost -0.2166748046875 0.0\n",
            "tcost icost -0.01739501953125 0.0\n",
            "tcost icost -0.01076507568359375 0.0\n",
            "tcost icost -0.02313232421875 0.0\n",
            "loss tensor([[-0.2111]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.8975, -0.2393, -0.1197],\n",
            "         [ 0.5962,  0.7338, -0.3257],\n",
            "         [ 0.2536, -0.1107, -0.8500],\n",
            "         [ 0.1993,  0.7339,  0.6494],\n",
            "         [ 0.0374, -0.1227,  0.0314],\n",
            "         [-0.6361, -0.4210, -0.6466]]], device='cuda:0') tensor([ 0.0782,  0.0369,  0.3254,  0.4059,  0.0051, -0.8645], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2110595703125\n",
            "tcost icost 0.01107025146484375 0.0\n",
            "tcost icost 0.04400634765625 0.0\n",
            "tcost icost 0.117919921875 0.0\n",
            "tcost icost 0.07550048828125 0.0\n",
            "tcost icost -0.01448822021484375 0.0\n",
            "tcost icost -0.0199127197265625 0.0\n",
            "loss tensor([[0.1801]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.8778, -0.2634, -0.1446],\n",
            "         [ 0.6323,  0.7044, -0.3224],\n",
            "         [ 0.3037, -0.0878, -0.8632],\n",
            "         [ 0.2544,  0.7413,  0.6210],\n",
            "         [ 0.0935, -0.0688,  0.0735],\n",
            "         [-0.5561, -0.4799, -0.6786]]], device='cuda:0') tensor([ 0.8785, -0.6199,  0.2920,  0.6117,  0.0198, -0.8489], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1800537109375\n",
            "tcost icost -0.044708251953125 0.0\n",
            "tcost icost -0.05242919921875 0.0\n",
            "tcost icost -0.2705078125 0.0\n",
            "tcost icost -0.01241302490234375 0.0\n",
            "tcost icost -0.01500701904296875 0.0\n",
            "tcost icost -0.0225067138671875 0.0\n",
            "loss tensor([[-0.3430]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.8664, -0.2810, -0.1615],\n",
            "         [ 0.6648,  0.6748, -0.3205],\n",
            "         [ 0.3614, -0.0591, -0.8678],\n",
            "         [ 0.3066,  0.7462,  0.5909],\n",
            "         [ 0.1573, -0.0102,  0.1182],\n",
            "         [-0.4666, -0.5336, -0.7054]]], device='cuda:0') tensor([ 0.7183, -0.5185,  0.1081,  0.6503, -0.0070, -0.8317], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.343017578125\n",
            "tcost icost -0.021514892578125 0.0\n",
            "tcost icost -0.037384033203125 0.0\n",
            "tcost icost -0.2283935546875 0.0\n",
            "tcost icost -0.005405426025390625 0.0\n",
            "tcost icost -0.003597259521484375 0.0\n",
            "tcost icost -0.0238037109375 0.0\n",
            "loss tensor([[-0.2605]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.8645, -0.2910, -0.1698],\n",
            "         [ 0.6932,  0.6434, -0.3248],\n",
            "         [ 0.4261, -0.0234, -0.8633],\n",
            "         [ 0.3569,  0.7488,  0.5584],\n",
            "         [ 0.2239,  0.0459,  0.1579],\n",
            "         [-0.3731, -0.5805, -0.7238]]], device='cuda:0') tensor([ 0.4985, -0.3290, -0.1364,  0.6778, -0.0037, -0.8149], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.260498046875\n",
            "tcost icost 0.0026988983154296875 0.0\n",
            "tcost icost -0.01160430908203125 0.0\n",
            "tcost icost -0.09539794921875 0.0\n",
            "tcost icost 0.05426025390625 0.0\n",
            "tcost icost -0.0026569366455078125 0.0\n",
            "tcost icost -0.020538330078125 0.0\n",
            "loss tensor([[-0.0593]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.8802, -0.2865, -0.1639],\n",
            "         [ 0.7151,  0.6061, -0.3482],\n",
            "         [ 0.4982,  0.0226, -0.8441],\n",
            "         [ 0.4084,  0.7504,  0.5197],\n",
            "         [ 0.2873,  0.0973,  0.1919],\n",
            "         [-0.2819, -0.6196, -0.7325]]], device='cuda:0') tensor([-0.0240,  0.2051, -0.5402,  0.7669, -0.0106, -0.7988], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.059326171875\n",
            "tcost icost 0.0101165771484375 0.0\n",
            "tcost icost -0.066650390625 0.0\n",
            "tcost icost -0.03521728515625 0.0\n",
            "tcost icost -0.25390625 0.0\n",
            "tcost icost -0.03253173828125 0.0\n",
            "tcost icost -0.01374053955078125 0.0\n",
            "loss tensor([[-0.2927]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.8770, -0.2970, -0.1732],\n",
            "         [ 0.7374,  0.5749, -0.3546],\n",
            "         [ 0.5620,  0.0622, -0.8248],\n",
            "         [ 0.4540,  0.7432,  0.4914],\n",
            "         [ 0.3460,  0.1395,  0.2172],\n",
            "         [-0.2028, -0.6492, -0.7331]]], device='cuda:0') tensor([ 0.7177, -0.1044, -0.4453,  0.6423,  0.0062, -0.7927], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.292724609375\n",
            "tcost icost -0.021453857421875 0.0\n",
            "tcost icost -0.018798828125 0.0\n",
            "tcost icost -0.0103912353515625 0.0\n",
            "tcost icost 0.0462646484375 0.0\n",
            "tcost icost -0.00557708740234375 0.0\n",
            "tcost icost -0.01605224609375 0.0\n",
            "loss tensor([[-0.0262]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.8789, -0.3032, -0.1815],\n",
            "         [ 0.7557,  0.5442, -0.3643],\n",
            "         [ 0.5804,  0.0856, -0.8098],\n",
            "         [ 0.4926,  0.7324,  0.4701],\n",
            "         [ 0.4007,  0.1736,  0.2344],\n",
            "         [-0.1445, -0.6726, -0.7257]]], device='cuda:0') tensor([ 0.5405, -0.1259, -0.2057,  0.4133,  0.0238, -0.7763], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0262298583984375\n",
            "tcost icost -0.0014495849609375 0.0\n",
            "tcost icost -0.00635528564453125 0.0\n",
            "tcost icost -0.0156402587890625 0.0\n",
            "tcost icost 0.08447265625 0.0\n",
            "tcost icost 0.0167083740234375 0.0\n",
            "tcost icost -0.0185699462890625 0.0\n",
            "loss tensor([[0.0417]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.8988, -0.2934, -0.1745],\n",
            "         [ 0.7660,  0.5066, -0.3958],\n",
            "         [ 0.6116,  0.1201, -0.7820],\n",
            "         [ 0.5387,  0.7204,  0.4368],\n",
            "         [ 0.4546,  0.1821,  0.2254],\n",
            "         [-0.1157, -0.6912, -0.7134]]], device='cuda:0') tensor([-0.1676,  0.4415, -0.6486,  0.5169,  0.1215, -0.7590], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.041748046875\n",
            "tcost icost 0.006763458251953125 0.0\n",
            "tcost icost -0.1846923828125 0.0\n",
            "tcost icost -0.060211181640625 0.0\n",
            "tcost icost -0.218505859375 0.0\n",
            "tcost icost -0.02740478515625 0.0\n",
            "tcost icost -0.01287078857421875 0.0\n",
            "loss tensor([[-0.3933]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9077, -0.2931, -0.1763],\n",
            "         [ 0.7777,  0.4736, -0.4134],\n",
            "         [ 0.6394,  0.1499, -0.7541],\n",
            "         [ 0.5777,  0.7026,  0.4155],\n",
            "         [ 0.5054,  0.1868,  0.2123],\n",
            "         [-0.0998, -0.7065, -0.7007]]], device='cuda:0') tensor([ 0.3349,  0.2764, -0.5919,  0.3975,  0.1294, -0.7511], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.393310546875\n",
            "tcost icost 0.00977325439453125 0.0\n",
            "tcost icost 0.0869140625 0.0\n",
            "tcost icost -0.017242431640625 0.0\n",
            "tcost icost -0.148193359375 0.0\n",
            "tcost icost -0.01259613037109375 0.0\n",
            "tcost icost -0.00872039794921875 0.0\n",
            "loss tensor([[-0.0474]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9127, -0.2967, -0.1834],\n",
            "         [ 0.7901,  0.4438, -0.4228],\n",
            "         [ 0.6589,  0.1713, -0.7325],\n",
            "         [ 0.6064,  0.6832,  0.4069],\n",
            "         [ 0.5534,  0.1886,  0.1966],\n",
            "         [-0.1019, -0.7201, -0.6864]]], device='cuda:0') tensor([ 0.5710,  0.1789, -0.4616,  0.2019,  0.1362, -0.7369], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.04742431640625\n",
            "tcost icost -0.0047149658203125 0.0\n",
            "tcost icost 0.01015472412109375 0.0\n",
            "tcost icost -0.0005393028259277344 0.0\n",
            "tcost icost -0.007282257080078125 0.0\n",
            "tcost icost 0.032806396484375 0.0\n",
            "tcost icost -0.01483917236328125 0.0\n",
            "loss tensor([[0.0114]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9183, -0.2998, -0.1918],\n",
            "         [ 0.8009,  0.4185, -0.4283],\n",
            "         [ 0.6574,  0.1770, -0.7324],\n",
            "         [ 0.6262,  0.6644,  0.4080],\n",
            "         [ 0.6055,  0.1699,  0.1619],\n",
            "         [-0.1205, -0.7300, -0.6727]]], device='cuda:0') tensor([ 0.5367,  0.0199, -0.1721, -0.0445,  0.2452, -0.7211], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.01143646240234375\n",
            "search tensor([[[ 0.6055,  0.1699,  0.1619],\n",
            "         [-0.1205, -0.7300, -0.6727],\n",
            "         [ 0.1630,  0.4619, -0.0542],\n",
            "         [ 0.3163, -0.3280,  0.2320],\n",
            "         [-0.2005,  0.4309,  0.4368],\n",
            "         [-0.0164,  0.1154, -0.3243]]], device='cuda:0') tensor([[[ 0.2452],\n",
            "         [-0.7211],\n",
            "         [-0.4294],\n",
            "         [ 0.5155],\n",
            "         [ 0.4738],\n",
            "         [-1.0408]]], device='cuda:0')\n",
            "tcost icost 0.017913818359375 0.0\n",
            "tcost icost -0.024993896484375 0.0\n",
            "tcost icost -0.169677734375 0.0\n",
            "tcost icost -0.00876617431640625 0.0\n",
            "tcost icost -0.2305908203125 0.0\n",
            "tcost icost -0.0621337890625 0.0\n",
            "loss tensor([[-0.3364]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.5049,  0.2697,  0.2618],\n",
            "         [-0.2137, -0.8041, -0.5547],\n",
            "         [ 0.2628,  0.5614,  0.0459],\n",
            "         [ 0.2160, -0.4277,  0.1318],\n",
            "         [-0.1003,  0.5305,  0.5363],\n",
            "         [-0.1163,  0.0153, -0.4240]]], device='cuda:0') tensor([-0.2509, -0.4672, -0.7119,  0.7014,  0.3342, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.33642578125\n",
            "tcost icost 0.01299285888671875 0.0\n",
            "tcost icost 0.01473236083984375 0.0\n",
            "tcost icost 0.25244140625 0.0\n",
            "tcost icost -0.038360595703125 0.0\n",
            "tcost icost 0.0224456787109375 0.0\n",
            "tcost icost -0.0260009765625 0.0\n",
            "loss tensor([[0.2021]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.5777,  0.3493,  0.2926],\n",
            "         [-0.1673, -0.7369, -0.6267],\n",
            "         [ 0.3551,  0.5138, -0.0100],\n",
            "         [ 0.2829, -0.3794,  0.1749],\n",
            "         [-0.0328,  0.5843,  0.5465],\n",
            "         [-0.2112, -0.0826, -0.5225]]], device='cuda:0') tensor([-0.5712, -0.0441,  0.9238,  0.2383,  0.4458, -0.9873], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2021484375\n",
            "tcost icost 0.0015048980712890625 0.0\n",
            "tcost icost -0.06982421875 0.0\n",
            "tcost icost 0.1175537109375 0.0\n",
            "tcost icost -0.0109100341796875 0.0\n",
            "tcost icost -0.0028705596923828125 0.0\n",
            "tcost icost -0.0274810791015625 0.0\n",
            "loss tensor([[0.0078]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.6444,  0.3748,  0.2756],\n",
            "         [-0.0992, -0.6643, -0.7018],\n",
            "         [ 0.4388,  0.4824, -0.0518],\n",
            "         [ 0.3348, -0.3493,  0.1950],\n",
            "         [ 0.0316,  0.6318,  0.5618],\n",
            "         [-0.3007, -0.1812, -0.6207]]], device='cuda:0') tensor([-0.2602, -0.2294,  0.7524,  0.3024,  0.4039, -0.9757], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0077972412109375\n",
            "tcost icost 0.012542724609375 0.0\n",
            "tcost icost 0.059051513671875 0.0\n",
            "tcost icost -0.12078857421875 0.0\n",
            "tcost icost -0.0321044921875 0.0\n",
            "tcost icost -0.04248046875 0.0\n",
            "tcost icost -0.033660888671875 0.0\n",
            "loss tensor([[-0.1033]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.6829,  0.4307,  0.3116],\n",
            "         [-0.1174, -0.6512, -0.7498],\n",
            "         [ 0.5210,  0.4722, -0.0745],\n",
            "         [ 0.3781, -0.3339,  0.1993],\n",
            "         [ 0.1017,  0.6776,  0.5922],\n",
            "         [-0.3820, -0.2802, -0.7144]]], device='cuda:0') tensor([-1.0000,  0.7989,  0.5238,  0.4291,  0.2993, -0.9646], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.10333251953125\n",
            "tcost icost -0.00688934326171875 0.0\n",
            "tcost icost -0.384765625 0.0\n",
            "tcost icost -0.0439453125 0.0\n",
            "tcost icost -0.200927734375 0.0\n",
            "tcost icost -0.03973388671875 0.0\n",
            "tcost icost -0.0222625732421875 0.0\n",
            "loss tensor([[-0.5742]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.7265,  0.4649,  0.3308],\n",
            "         [-0.1122, -0.6118, -0.7830],\n",
            "         [ 0.5918,  0.4563, -0.0986],\n",
            "         [ 0.4296, -0.3025,  0.2209],\n",
            "         [ 0.1622,  0.7172,  0.6150],\n",
            "         [-0.4462, -0.3788, -0.8009]]], device='cuda:0') tensor([-0.6907,  0.5800,  0.6671,  0.2803,  0.3034, -0.9517], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.57421875\n",
            "tcost icost -0.0005960464477539062 0.0\n",
            "tcost icost -0.27001953125 0.0\n",
            "tcost icost -0.0241241455078125 0.0\n",
            "tcost icost 0.00023233890533447266 0.0\n",
            "tcost icost -0.0240020751953125 0.0\n",
            "tcost icost -0.0227203369140625 0.0\n",
            "loss tensor([[-0.2922]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.7824,  0.4694,  0.3223],\n",
            "         [-0.0820, -0.5539, -0.8286],\n",
            "         [ 0.6549,  0.4248, -0.1324],\n",
            "         [ 0.4932, -0.2519,  0.2614],\n",
            "         [ 0.2151,  0.7474,  0.6285],\n",
            "         [-0.4423, -0.4267, -0.7889]]], device='cuda:0') tensor([ 0.1008,  0.1728,  1.0000,  0.0357,  0.3139, -0.9378], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.292236328125\n",
            "tcost icost 0.03729248046875 0.0\n",
            "tcost icost 0.03717041015625 0.0\n",
            "tcost icost -0.181884765625 0.0\n",
            "tcost icost -0.04937744140625 0.0\n",
            "tcost icost -0.0401611328125 0.0\n",
            "tcost icost -0.029449462890625 0.0\n",
            "loss tensor([[-0.1562]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.8048,  0.4895,  0.3356],\n",
            "         [-0.0752, -0.5330, -0.8428],\n",
            "         [ 0.7207,  0.4069, -0.1547],\n",
            "         [ 0.5511, -0.2084,  0.2954],\n",
            "         [ 0.2567,  0.7419,  0.6194],\n",
            "         [-0.3982, -0.4764, -0.7839]]], device='cuda:0') tensor([-0.7175,  0.9277,  0.7782,  0.0784,  0.2692, -0.9244], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.15625\n",
            "tcost icost -0.0010271072387695312 0.0\n",
            "tcost icost -0.3466796875 0.0\n",
            "tcost icost -0.03643798828125 0.0\n",
            "tcost icost 0.033660888671875 0.0\n",
            "tcost icost -0.0206451416015625 0.0\n",
            "tcost icost -0.021728515625 0.0\n",
            "loss tensor([[-0.3442]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.8145,  0.4805,  0.3251],\n",
            "         [-0.0542, -0.4987, -0.8651],\n",
            "         [ 0.7795,  0.3809, -0.1818],\n",
            "         [ 0.6150, -0.1537,  0.3390],\n",
            "         [ 0.2937,  0.7370,  0.6088],\n",
            "         [-0.3553, -0.5227, -0.7750]]], device='cuda:0') tensor([-0.1702,  0.6572,  1.0000, -0.0900,  0.2848, -0.9094], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.34423828125\n",
            "tcost icost 0.0178680419921875 0.0\n",
            "tcost icost -0.07977294921875 0.0\n",
            "tcost icost 0.1175537109375 0.0\n",
            "tcost icost -0.0206756591796875 0.0\n",
            "tcost icost -0.01006317138671875 0.0\n",
            "tcost icost -0.0214691162109375 0.0\n",
            "loss tensor([[0.0070]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.8391,  0.4555,  0.2974],\n",
            "         [-0.0204, -0.4502, -0.8927],\n",
            "         [ 0.8343,  0.3573, -0.2074],\n",
            "         [ 0.6741, -0.1068,  0.3730],\n",
            "         [ 0.3278,  0.7304,  0.5992],\n",
            "         [-0.3020, -0.5681, -0.7656]]], device='cuda:0') tensor([ 0.6252,  0.2866,  0.9385, -0.0488,  0.2624, -0.8957], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0069580078125\n",
            "tcost icost -0.08056640625 0.0\n",
            "tcost icost -0.01263427734375 0.0\n",
            "tcost icost -0.28369140625 0.0\n",
            "tcost icost -0.0526123046875 0.0\n",
            "tcost icost -0.0872802734375 0.0\n",
            "tcost icost -0.0335693359375 0.0\n",
            "loss tensor([[-0.4373]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.8515,  0.4414,  0.2832],\n",
            "         [ 0.0041, -0.4169, -0.9089],\n",
            "         [ 0.8900,  0.3418, -0.2243],\n",
            "         [ 0.7295, -0.0657,  0.4028],\n",
            "         [ 0.3606,  0.7149,  0.5991],\n",
            "         [-0.2496, -0.6111, -0.7511]]], device='cuda:0') tensor([ 0.1509,  0.6288,  0.8786,  0.0221,  0.1260, -0.8834], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.437255859375\n",
            "tcost icost 0.03497314453125 0.0\n",
            "tcost icost 0.09857177734375 0.0\n",
            "tcost icost 0.148681640625 0.0\n",
            "tcost icost -0.0282440185546875 0.0\n",
            "tcost icost -0.0196075439453125 0.0\n",
            "tcost icost -0.0191192626953125 0.0\n",
            "loss tensor([[0.1995]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.8570,  0.4345,  0.2770],\n",
            "         [ 0.0214, -0.3952, -0.9184],\n",
            "         [ 0.9179,  0.3249, -0.2276],\n",
            "         [ 0.7810, -0.0302,  0.4261],\n",
            "         [ 0.3909,  0.6987,  0.5992],\n",
            "         [-0.2133, -0.6489, -0.7304]]], device='cuda:0') tensor([-0.4194,  0.9377,  0.5876,  0.0574,  0.1002, -0.8677], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.199462890625\n",
            "tcost icost 0.006092071533203125 0.0\n",
            "tcost icost -0.24462890625 0.0\n",
            "tcost icost -0.031280517578125 0.0\n",
            "tcost icost 0.0262451171875 0.0\n",
            "tcost icost -0.018096923828125 0.0\n",
            "tcost icost -0.0187530517578125 0.0\n",
            "loss tensor([[-0.2433]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.8680,  0.4211,  0.2631],\n",
            "         [ 0.0470, -0.3624, -0.9308],\n",
            "         [ 0.9254,  0.3027, -0.2281],\n",
            "         [ 0.8325,  0.0020,  0.4390],\n",
            "         [ 0.4197,  0.6840,  0.5967],\n",
            "         [-0.2023, -0.6803, -0.7045]]], device='cuda:0') tensor([-0.1084,  0.6618,  0.6072,  0.1291,  0.1085, -0.8554], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2432861328125\n",
            "tcost icost 0.02044677734375 0.0\n",
            "tcost icost -0.038604736328125 0.0\n",
            "tcost icost 0.0875244140625 0.0\n",
            "tcost icost -0.002872467041015625 0.0\n",
            "tcost icost -0.00769805908203125 0.0\n",
            "tcost icost -0.01776123046875 0.0\n",
            "loss tensor([[0.0390]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.8933,  0.3867,  0.2291],\n",
            "         [ 0.0883, -0.3106, -0.9464],\n",
            "         [ 0.9316,  0.2755, -0.2370],\n",
            "         [ 0.8803,  0.0314,  0.4492],\n",
            "         [ 0.4475,  0.6704,  0.5919],\n",
            "         [-0.2137, -0.7058, -0.6754]]], device='cuda:0') tensor([ 1.0000, -0.0994,  0.8318,  0.1251,  0.1111, -0.8399], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.03900146484375\n",
            "tcost icost -0.1531982421875 0.0\n",
            "tcost icost -0.046844482421875 0.0\n",
            "tcost icost -0.33935546875 0.0\n",
            "tcost icost -0.047576904296875 0.0\n",
            "tcost icost -0.11090087890625 0.0\n",
            "tcost icost -0.0350341796875 0.0\n",
            "loss tensor([[-0.5981]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9109,  0.3590,  0.2035],\n",
            "         [ 0.1231, -0.2677, -0.9556],\n",
            "         [ 0.9374,  0.2546, -0.2376],\n",
            "         [ 0.8960,  0.0529,  0.4409],\n",
            "         [ 0.4685,  0.6519,  0.5962],\n",
            "         [-0.2309, -0.7276, -0.6459]]], device='cuda:0') tensor([ 0.7909,  0.1115,  0.7850,  0.2319, -0.0292, -0.8283], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.59814453125\n",
            "tcost icost -0.11407470703125 0.0\n",
            "tcost icost -0.026336669921875 0.0\n",
            "tcost icost -0.302001953125 0.0\n",
            "tcost icost -0.04302978515625 0.0\n",
            "tcost icost -0.051239013671875 0.0\n",
            "tcost icost -0.026214599609375 0.0\n",
            "loss tensor([[-0.4626]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9216,  0.3400,  0.1870],\n",
            "         [ 0.1499, -0.2349, -0.9604],\n",
            "         [ 0.9427,  0.2405, -0.2312],\n",
            "         [ 0.9019,  0.0682,  0.4265],\n",
            "         [ 0.4841,  0.6322,  0.6050],\n",
            "         [-0.2549, -0.7441, -0.6175]]], device='cuda:0') tensor([ 0.4745,  0.3730,  0.6823,  0.3051, -0.1320, -0.8126], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.462646484375\n",
            "tcost icost -0.0430908203125 0.0\n",
            "tcost icost 0.0028476715087890625 0.0\n",
            "tcost icost -0.15380859375 0.0\n",
            "tcost icost -0.031280517578125 0.0\n",
            "tcost icost -0.0187225341796875 0.0\n",
            "tcost icost -0.017913818359375 0.0\n",
            "loss tensor([[-0.2108]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9240,  0.3355,  0.1838],\n",
            "         [ 0.1625, -0.2168, -0.9626],\n",
            "         [ 0.9472,  0.2350, -0.2181],\n",
            "         [ 0.9083,  0.0795,  0.4107],\n",
            "         [ 0.4970,  0.6136,  0.6136],\n",
            "         [-0.2780, -0.7535, -0.5959]]], device='cuda:0') tensor([-0.2325,  0.8210,  0.4434,  0.3447, -0.1656, -0.7951], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2108154296875\n",
            "tcost icost 0.01392364501953125 0.0\n",
            "tcost icost -0.1728515625 0.0\n",
            "tcost icost -0.02337646484375 0.0\n",
            "tcost icost -0.00841522216796875 0.0\n",
            "tcost icost -0.02911376953125 0.0\n",
            "tcost icost -0.0187835693359375 0.0\n",
            "loss tensor([[-0.1969]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9326,  0.3183,  0.1699],\n",
            "         [ 0.1874, -0.1892, -0.9639],\n",
            "         [ 0.9515,  0.2208, -0.2141],\n",
            "         [ 0.9104,  0.1004,  0.4013],\n",
            "         [ 0.5151,  0.5946,  0.6174],\n",
            "         [-0.2899, -0.7631, -0.5777]]], device='cuda:0') tensor([ 0.4247,  0.4526,  0.7186,  0.1863, -0.1433, -0.7901], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1968994140625\n",
            "tcost icost -0.029632568359375 0.0\n",
            "tcost icost 0.018218994140625 0.0\n",
            "tcost icost -0.08428955078125 0.0\n",
            "tcost icost -0.03173828125 0.0\n",
            "tcost icost -0.018829345703125 0.0\n",
            "tcost icost -0.015228271484375 0.0\n",
            "loss tensor([[-0.1260]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9315,  0.3199,  0.1732],\n",
            "         [ 0.1927, -0.1802, -0.9646],\n",
            "         [ 0.9550,  0.2172, -0.2018],\n",
            "         [ 0.9134,  0.1168,  0.3899],\n",
            "         [ 0.5302,  0.5766,  0.6216],\n",
            "         [-0.2907, -0.7676, -0.5712]]], device='cuda:0') tensor([-0.5929,  1.0000,  0.4185,  0.2303, -0.1731, -0.7733], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1259765625\n",
            "tcost icost 0.0011043548583984375 0.0\n",
            "tcost icost -0.340087890625 0.0\n",
            "tcost icost -0.048248291015625 0.0\n",
            "tcost icost -0.1016845703125 0.0\n",
            "tcost icost -0.03778076171875 0.0\n",
            "tcost icost -0.0214385986328125 0.0\n",
            "loss tensor([[-0.4558]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9352,  0.3112,  0.1688],\n",
            "         [ 0.2067, -0.1660, -0.9642],\n",
            "         [ 0.9585,  0.2086, -0.1942],\n",
            "         [ 0.9101,  0.1414,  0.3896],\n",
            "         [ 0.5509,  0.5566,  0.6219],\n",
            "         [-0.3004, -0.7726, -0.5593]]], device='cuda:0') tensor([-0.0358,  0.8125,  0.5964,  0.0529, -0.1417, -0.7649], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.455810546875\n",
            "tcost icost 0.0243682861328125 0.0\n",
            "tcost icost -0.0770263671875 0.0\n",
            "tcost icost 0.0142822265625 0.0\n",
            "tcost icost -0.004238128662109375 0.0\n",
            "tcost icost -0.00995635986328125 0.0\n",
            "tcost icost -0.01303863525390625 0.0\n",
            "loss tensor([[-0.0507]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9454,  0.2886,  0.1513],\n",
            "         [ 0.2320, -0.1410, -0.9624],\n",
            "         [ 0.9614,  0.1979, -0.1910],\n",
            "         [ 0.9075,  0.1629,  0.3871],\n",
            "         [ 0.5712,  0.5379,  0.6200],\n",
            "         [-0.3113, -0.7728, -0.5531]]], device='cuda:0') tensor([ 0.7445,  0.3748,  0.6743,  0.0501, -0.1367, -0.7481], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.05072021484375\n",
            "search tensor([[[ 5.7122e-01,  5.3788e-01,  6.1999e-01],\n",
            "         [-3.1128e-01, -7.7278e-01, -5.5309e-01],\n",
            "         [ 3.4861e-02,  1.4232e-01,  3.8441e-01],\n",
            "         [-3.6499e-01,  1.6706e-01, -3.4272e-01],\n",
            "         [ 3.5218e-01,  4.4521e-04, -4.5296e-03],\n",
            "         [-4.7904e-01,  8.1211e-02,  5.0476e-01]]], device='cuda:0') tensor([[[-0.1367],\n",
            "         [-0.7481],\n",
            "         [ 0.6156],\n",
            "         [-0.8860],\n",
            "         [-0.2380],\n",
            "         [-0.3454]]], device='cuda:0')\n",
            "tcost icost 0.005779266357421875 0.0\n",
            "tcost icost -0.006256103515625 0.0\n",
            "tcost icost -0.374755859375 0.0\n",
            "tcost icost -0.06768798828125 0.0\n",
            "tcost icost -0.1959228515625 0.0\n",
            "tcost icost -0.02935791015625 0.0\n",
            "loss tensor([[-0.4983]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.4398,  0.5956,  0.6722],\n",
            "         [-0.2110, -0.6720, -0.6525],\n",
            "         [ 0.1348,  0.2422,  0.4840],\n",
            "         [-0.2646,  0.0669, -0.2424],\n",
            "         [ 0.4518,  0.1004,  0.0955],\n",
            "         [-0.5786, -0.0189,  0.4043]]], device='cuda:0') tensor([-0.3049, -0.4691,  0.5388, -0.8452, -0.3657, -0.3333], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.498291015625\n",
            "tcost icost 0.0018587112426757812 0.0\n",
            "tcost icost 0.0203399658203125 0.0\n",
            "tcost icost -0.2403564453125 0.0\n",
            "tcost icost -0.07037353515625 0.0\n",
            "tcost icost -0.1036376953125 0.0\n",
            "tcost icost -0.0206756591796875 0.0\n",
            "loss tensor([[-0.3062]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.3127,  0.6363,  0.7052],\n",
            "         [-0.2054, -0.7262, -0.6561],\n",
            "         [ 0.2342,  0.3416,  0.5834],\n",
            "         [-0.1651, -0.0304, -0.1422],\n",
            "         [ 0.5501,  0.2003,  0.1954],\n",
            "         [-0.6668, -0.1140,  0.3094]]], device='cuda:0') tensor([-0.8630,  0.1452,  0.4314, -0.7891, -0.5110, -0.3263], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.30615234375\n",
            "tcost icost -0.0113525390625 0.0\n",
            "tcost icost -0.146484375 0.0\n",
            "tcost icost -0.0009074211120605469 0.0\n",
            "tcost icost -0.0029582977294921875 0.0\n",
            "tcost icost -0.0037860870361328125 0.0\n",
            "tcost icost 0.021575927734375 0.0\n",
            "loss tensor([[-0.1359]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.2773,  0.6419,  0.7149],\n",
            "         [-0.1484, -0.7322, -0.6648],\n",
            "         [ 0.3197,  0.3828,  0.6248],\n",
            "         [-0.0947,  0.0190, -0.0774],\n",
            "         [ 0.6437,  0.2300,  0.2175],\n",
            "         [-0.6641, -0.1616,  0.2234]]], device='cuda:0') tensor([-0.4722, -0.3079,  0.5269, -0.9427, -0.3591, -0.2828], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1358642578125\n",
            "tcost icost -0.0024166107177734375 0.0\n",
            "tcost icost 0.056884765625 0.0\n",
            "tcost icost -0.086669921875 0.0\n",
            "tcost icost -0.063720703125 0.0\n",
            "tcost icost -0.0574951171875 0.0\n",
            "tcost icost -0.0157470703125 0.0\n",
            "loss tensor([[-0.1149]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.2444,  0.6492,  0.7203],\n",
            "         [-0.1730, -0.7098, -0.6829],\n",
            "         [ 0.4099,  0.4439,  0.6735],\n",
            "         [-0.0290,  0.0495, -0.0385],\n",
            "         [ 0.7294,  0.2675,  0.2509],\n",
            "         [-0.6680, -0.2224,  0.1406]]], device='cuda:0') tensor([-0.9370,  0.6545,  0.2422, -0.8995, -0.4417, -0.2742], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1148681640625\n",
            "tcost icost -0.01293182373046875 0.0\n",
            "tcost icost -0.295166015625 0.0\n",
            "tcost icost -0.03533935546875 0.0\n",
            "tcost icost -0.002582550048828125 0.0\n",
            "tcost icost -0.003520965576171875 0.0\n",
            "tcost icost -0.040985107421875 0.0\n",
            "loss tensor([[-0.3354]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.2355,  0.6492,  0.7233],\n",
            "         [-0.1711, -0.6781, -0.7148],\n",
            "         [ 0.5010,  0.4843,  0.7116],\n",
            "         [ 0.0459,  0.1061,  0.0209],\n",
            "         [ 0.8127,  0.2749,  0.2606],\n",
            "         [-0.6190, -0.1790,  0.1621]]], device='cuda:0') tensor([-0.6121,  0.3888,  0.3904, -0.9997, -0.3049, -0.3875], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.33544921875\n",
            "tcost icost -0.005649566650390625 0.0\n",
            "tcost icost -0.141845703125 0.0\n",
            "tcost icost 0.00026869773864746094 0.0\n",
            "tcost icost -0.01215362548828125 0.0\n",
            "tcost icost -0.0055694580078125 0.0\n",
            "tcost icost 0.0041046142578125 0.0\n",
            "loss tensor([[-0.1432]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.2572,  0.6489,  0.7160],\n",
            "         [-0.1489, -0.6237, -0.7674],\n",
            "         [ 0.5456,  0.4795,  0.6873],\n",
            "         [ 0.1063,  0.1428,  0.0541],\n",
            "         [ 0.8850,  0.2845,  0.2729],\n",
            "         [-0.5898, -0.1560,  0.1609]]], device='cuda:0') tensor([-0.1143,  0.0695,  0.4458, -0.9395, -0.3337, -0.3575], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1431884765625\n",
            "tcost icost 0.00691986083984375 0.0\n",
            "tcost icost 0.11767578125 0.0\n",
            "tcost icost -0.058074951171875 0.0\n",
            "tcost icost -0.0595703125 0.0\n",
            "tcost icost -0.061279296875 0.0\n",
            "tcost icost -0.015869140625 0.0\n",
            "loss tensor([[-0.0272]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.2547,  0.6505,  0.7155],\n",
            "         [-0.1576, -0.6282, -0.7619],\n",
            "         [ 0.5581,  0.4863,  0.6724],\n",
            "         [ 0.1663,  0.1700,  0.0817],\n",
            "         [ 0.9141,  0.2901,  0.2832],\n",
            "         [-0.5665, -0.1396,  0.1549]]], device='cuda:0') tensor([-1.0000,  1.0000,  0.1579, -0.8936, -0.4180, -0.3471], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.027191162109375\n",
            "tcost icost -0.01448822021484375 0.0\n",
            "tcost icost -0.37744140625 0.0\n",
            "tcost icost -0.046234130859375 0.0\n",
            "tcost icost -0.018951416015625 0.0\n",
            "tcost icost -0.01192474365234375 0.0\n",
            "tcost icost -0.060302734375 0.0\n",
            "loss tensor([[-0.4487]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.2658,  0.6462,  0.7153],\n",
            "         [-0.1540, -0.6239, -0.7662],\n",
            "         [ 0.5804,  0.4823,  0.6562],\n",
            "         [ 0.2365,  0.2168,  0.1326],\n",
            "         [ 0.9229,  0.2713,  0.2733],\n",
            "         [-0.5162, -0.0950,  0.1867]]], device='cuda:0') tensor([-0.6863,  0.8328,  0.3075, -1.0000, -0.2747, -0.4773], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.44873046875\n",
            "tcost icost -0.00740814208984375 0.0\n",
            "tcost icost -0.26708984375 0.0\n",
            "tcost icost -0.033905029296875 0.0\n",
            "tcost icost -0.006244659423828125 0.0\n",
            "tcost icost 0.0011568069458007812 0.0\n",
            "tcost icost 0.0167999267578125 0.0\n",
            "loss tensor([[-0.2693]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.2956,  0.6392,  0.7100],\n",
            "         [-0.1426, -0.6084, -0.7807],\n",
            "         [ 0.6019,  0.4794,  0.6387],\n",
            "         [ 0.3034,  0.2562,  0.1693],\n",
            "         [ 0.9322,  0.2520,  0.2599],\n",
            "         [-0.4747, -0.0658,  0.1964]]], device='cuda:0') tensor([-0.3665,  0.6111,  0.2803, -1.0000, -0.2418, -0.4146], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.269287109375\n",
            "tcost icost 0.00022900104522705078 0.0\n",
            "tcost icost -0.10858154296875 0.0\n",
            "tcost icost 0.002689361572265625 0.0\n",
            "tcost icost -0.01409912109375 0.0\n",
            "tcost icost -0.00595855712890625 0.0\n",
            "tcost icost 0.00296783447265625 0.0\n",
            "loss tensor([[-0.1077]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.3412,  0.6282,  0.6993],\n",
            "         [-0.1164, -0.5769, -0.8085],\n",
            "         [ 0.6278,  0.4738,  0.6175],\n",
            "         [ 0.3639,  0.2823,  0.1902],\n",
            "         [ 0.9376,  0.2394,  0.2521],\n",
            "         [-0.4462, -0.0481,  0.1937]]], device='cuda:0') tensor([ 0.1208,  0.2852,  0.3396, -0.9236, -0.2843, -0.3855], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.107666015625\n",
            "tcost icost 0.0254364013671875 0.0\n",
            "tcost icost 0.0548095703125 0.0\n",
            "tcost icost -0.035369873046875 0.0\n",
            "tcost icost -0.05279541015625 0.0\n",
            "tcost icost -0.07818603515625 0.0\n",
            "tcost icost -0.01557159423828125 0.0\n",
            "loss tensor([[-0.0529]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.3446,  0.6293,  0.6966],\n",
            "         [-0.1208, -0.5812, -0.8048],\n",
            "         [ 0.6189,  0.4835,  0.6190],\n",
            "         [ 0.4273,  0.3016,  0.2108],\n",
            "         [ 0.9360,  0.2397,  0.2578],\n",
            "         [-0.4240, -0.0356,  0.1873]]], device='cuda:0') tensor([-1.0000,  1.0000,  0.0247, -0.8731, -0.4129, -0.3718], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0528564453125\n",
            "tcost icost -0.01448822021484375 0.0\n",
            "tcost icost -0.37744140625 0.0\n",
            "tcost icost -0.04864501953125 0.0\n",
            "tcost icost -0.032958984375 0.0\n",
            "tcost icost -0.015960693359375 0.0\n",
            "tcost icost -0.077392578125 0.0\n",
            "loss tensor([[-0.4734]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 3.6026e-01,  6.2304e-01,  6.9428e-01],\n",
            "         [-1.1617e-01, -5.8039e-01, -8.0601e-01],\n",
            "         [ 6.2439e-01,  4.8073e-01,  6.1566e-01],\n",
            "         [ 4.9593e-01,  3.3810e-01,  2.5332e-01],\n",
            "         [ 9.4111e-01,  2.2463e-01,  2.5269e-01],\n",
            "         [-3.7789e-01,  8.3423e-04,  2.1136e-01]]], device='cuda:0') tensor([-0.6265,  0.8584,  0.2091, -1.0000, -0.2631, -0.5058], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.473388671875\n",
            "tcost icost -0.006031036376953125 0.0\n",
            "tcost icost -0.251708984375 0.0\n",
            "tcost icost -0.035552978515625 0.0\n",
            "tcost icost -0.005207061767578125 0.0\n",
            "tcost icost 0.003078460693359375 0.0\n",
            "tcost icost 0.0126190185546875 0.0\n",
            "loss tensor([[-0.2556]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.3913,  0.6113,  0.6879],\n",
            "         [-0.1041, -0.5706, -0.8146],\n",
            "         [ 0.6344,  0.4770,  0.6082],\n",
            "         [ 0.5627,  0.3741,  0.2936],\n",
            "         [ 0.9485,  0.2055,  0.2411],\n",
            "         [-0.3338,  0.0308,  0.2255]]], device='cuda:0') tensor([-0.2546,  0.6127,  0.2213, -1.0000, -0.1947, -0.4642], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.255615234375\n",
            "tcost icost 0.002994537353515625 0.0\n",
            "tcost icost -0.0419921875 0.0\n",
            "tcost icost 0.021820068359375 0.0\n",
            "tcost icost -0.01617431640625 0.0\n",
            "tcost icost -0.01739501953125 0.0\n",
            "tcost icost -0.00569915771484375 0.0\n",
            "loss tensor([[-0.0437]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.4430,  0.5915,  0.6737],\n",
            "         [-0.0757, -0.5432, -0.8362],\n",
            "         [ 0.6497,  0.4708,  0.5968],\n",
            "         [ 0.6306,  0.3998,  0.3223],\n",
            "         [ 0.9520,  0.1948,  0.2359],\n",
            "         [-0.2969,  0.0540,  0.2323]]], device='cuda:0') tensor([ 0.5618,  0.1720,  0.2884, -0.9221, -0.2649, -0.4483], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.043670654296875\n",
            "tcost icost -0.092041015625 0.0\n",
            "tcost icost -0.0167999267578125 0.0\n",
            "tcost icost -0.2213134765625 0.0\n",
            "tcost icost -0.05718994140625 0.0\n",
            "tcost icost -0.1533203125 0.0\n",
            "tcost icost -0.023406982421875 0.0\n",
            "loss tensor([[-0.4429]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.4811,  0.5740,  0.6626],\n",
            "         [-0.0546, -0.5257, -0.8489],\n",
            "         [ 0.6495,  0.4690,  0.5984],\n",
            "         [ 0.6987,  0.4194,  0.3499],\n",
            "         [ 0.9493,  0.1980,  0.2442],\n",
            "         [-0.2663,  0.0715,  0.2338]]], device='cuda:0') tensor([ 0.1638,  0.5074,  0.1960, -0.8804, -0.4027, -0.4331], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.44287109375\n",
            "tcost icost 0.0171051025390625 0.0\n",
            "tcost icost 0.10015869140625 0.0\n",
            "tcost icost 0.2607421875 0.0\n",
            "tcost icost -0.035675048828125 0.0\n",
            "tcost icost -0.048553466796875 0.0\n",
            "tcost icost -0.0087738037109375 0.0\n",
            "loss tensor([[0.2556]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.4693,  0.5804,  0.6655],\n",
            "         [-0.0654, -0.5309, -0.8449],\n",
            "         [ 0.6188,  0.4869,  0.6165],\n",
            "         [ 0.7686,  0.4311,  0.3713],\n",
            "         [ 0.9431,  0.2088,  0.2589],\n",
            "         [-0.2420,  0.0835,  0.2292]]], device='cuda:0') tensor([-1.0000,  1.0000, -0.5105, -0.8210, -0.4993, -0.4138], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.255615234375\n",
            "tcost icost -0.01448822021484375 0.0\n",
            "tcost icost -0.37744140625 0.0\n",
            "tcost icost -0.06005859375 0.0\n",
            "tcost icost -0.11871337890625 0.0\n",
            "tcost icost -0.03021240234375 0.0\n",
            "tcost icost -0.1280517578125 0.0\n",
            "loss tensor([[-0.5850]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.4683,  0.5789,  0.6675],\n",
            "         [-0.0685, -0.5334, -0.8431],\n",
            "         [ 0.5991,  0.4940,  0.6300],\n",
            "         [ 0.8045,  0.4419,  0.3970],\n",
            "         [ 0.9413,  0.2075,  0.2663],\n",
            "         [-0.1966,  0.1162,  0.2522]]], device='cuda:0') tensor([-0.6519,  0.9001, -0.3778, -1.0000, -0.3934, -0.5457], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5849609375\n",
            "tcost icost -0.006622314453125 0.0\n",
            "tcost icost -0.268798828125 0.0\n",
            "tcost icost -0.055023193359375 0.0\n",
            "tcost icost -0.022125244140625 0.0\n",
            "tcost icost -0.0119781494140625 0.0\n",
            "tcost icost -0.048858642578125 0.0\n",
            "loss tensor([[-0.3457]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.4797,  0.5707,  0.6664],\n",
            "         [-0.0631, -0.5298, -0.8458],\n",
            "         [ 0.5896,  0.4937,  0.6392],\n",
            "         [ 0.7968,  0.4408,  0.4133],\n",
            "         [ 0.9454,  0.1932,  0.2625],\n",
            "         [-0.1405,  0.1601,  0.2890]]], device='cuda:0') tensor([-0.2004,  0.7055, -0.2466, -1.0000, -0.2607, -0.6467], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.345703125\n",
            "tcost icost 0.00426483154296875 0.0\n",
            "tcost icost -0.0328369140625 0.0\n",
            "tcost icost -0.0172576904296875 0.0\n",
            "tcost icost -0.01044464111328125 0.0\n",
            "tcost icost 0.006801605224609375 0.0\n",
            "tcost icost 0.0124969482421875 0.0\n",
            "loss tensor([[-0.0350]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.5124,  0.5527,  0.6572],\n",
            "         [-0.0479, -0.5138, -0.8566],\n",
            "         [ 0.5815,  0.4946,  0.6459],\n",
            "         [ 0.7964,  0.4344,  0.4209],\n",
            "         [ 0.9479,  0.1835,  0.2603],\n",
            "         [-0.0919,  0.1940,  0.3124]]], device='cuda:0') tensor([ 0.5261,  0.3412, -0.2963, -0.9281, -0.2891, -0.6056], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0350341796875\n",
            "tcost icost -0.08343505859375 0.0\n",
            "tcost icost -0.00928497314453125 0.0\n",
            "tcost icost -0.0310516357421875 0.0\n",
            "tcost icost -0.033050537109375 0.0\n",
            "tcost icost -0.1800537109375 0.0\n",
            "tcost icost -0.0243988037109375 0.0\n",
            "loss tensor([[-0.2734]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.5266,  0.5432,  0.6539],\n",
            "         [-0.0427, -0.5090, -0.8597],\n",
            "         [ 0.5575,  0.5013,  0.6618],\n",
            "         [ 0.8004,  0.4225,  0.4252],\n",
            "         [ 0.9442,  0.1882,  0.2702],\n",
            "         [-0.0495,  0.2213,  0.3287]]], device='cuda:0') tensor([-0.1116,  0.9169, -0.5356, -0.8732, -0.4627, -0.5857], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2734375\n",
            "search tensor([[[ 9.4422e-01,  1.8818e-01,  2.7025e-01],\n",
            "         [-4.9451e-02,  2.2132e-01,  3.2868e-01],\n",
            "         [ 1.0311e-01, -4.7102e-02,  1.3113e-01],\n",
            "         [ 1.6606e-01, -1.2101e-01,  4.2049e-04],\n",
            "         [-4.3368e-01,  5.1134e-01,  4.1064e-01],\n",
            "         [ 4.3445e-01, -2.5464e-02,  1.0219e-01]]], device='cuda:0') tensor([[[-0.4627],\n",
            "         [-0.5857],\n",
            "         [ 1.5140],\n",
            "         [ 0.0784],\n",
            "         [ 0.6489],\n",
            "         [-0.1508]]], device='cuda:0')\n",
            "tcost icost 0.0072021484375 0.0\n",
            "tcost icost 0.0184173583984375 0.0\n",
            "tcost icost -0.297607421875 0.0\n",
            "tcost icost -0.052398681640625 0.0\n",
            "tcost icost -0.09716796875 0.0\n",
            "tcost icost -0.0295562744140625 0.0\n",
            "loss tensor([[-0.3364]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.8433,  0.2880,  0.3700],\n",
            "         [-0.1494,  0.1211,  0.2283],\n",
            "         [ 0.2030,  0.0529,  0.2310],\n",
            "         [ 0.2659, -0.2209, -0.0996],\n",
            "         [-0.3332,  0.6108,  0.5102],\n",
            "         [ 0.5339, -0.1254,  0.0021]]], device='cuda:0') tensor([-0.7186, -0.1070,  1.0000,  0.1646,  0.5200, -0.1446], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.33642578125\n",
            "tcost icost -0.0010175704956054688 0.0\n",
            "tcost icost -0.10797119140625 0.0\n",
            "tcost icost 0.11334228515625 0.0\n",
            "tcost icost -0.00507354736328125 0.0\n",
            "tcost icost 0.005767822265625 0.0\n",
            "tcost icost -0.0159759521484375 0.0\n",
            "loss tensor([[-0.0157]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.8730,  0.2881,  0.3607],\n",
            "         [-0.0949,  0.0567,  0.1372],\n",
            "         [ 0.2993,  0.1527,  0.3215],\n",
            "         [ 0.2244, -0.3084, -0.1766],\n",
            "         [-0.2407,  0.6965,  0.5923],\n",
            "         [ 0.5129, -0.2253, -0.0948]]], device='cuda:0') tensor([-0.4554, -0.2956,  0.7906,  0.3466,  0.4736, -0.1358], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0157470703125\n",
            "tcost icost 0.0077056884765625 0.0\n",
            "tcost icost 0.06805419921875 0.0\n",
            "tcost icost -0.073974609375 0.0\n",
            "tcost icost -0.02587890625 0.0\n",
            "tcost icost -0.060791015625 0.0\n",
            "tcost icost -0.0207061767578125 0.0\n",
            "loss tensor([[-0.0619]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.8524,  0.3284,  0.3857],\n",
            "         [-0.1142, -0.0207,  0.0516],\n",
            "         [ 0.3970,  0.2494,  0.4137],\n",
            "         [ 0.1618, -0.3998, -0.2576],\n",
            "         [-0.1403,  0.7428,  0.6546],\n",
            "         [ 0.4482, -0.3248, -0.1927]]], device='cuda:0') tensor([-0.7889,  0.6170,  0.5338,  0.5144,  0.3234, -0.1260], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.061920166015625\n",
            "tcost icost -0.002681732177734375 0.0\n",
            "tcost icost -0.27978515625 0.0\n",
            "tcost icost -0.026702880859375 0.0\n",
            "tcost icost -0.09100341796875 0.0\n",
            "tcost icost -0.02850341796875 0.0\n",
            "tcost icost -0.00954437255859375 0.0\n",
            "loss tensor([[-0.3669]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.8818,  0.3081,  0.3570],\n",
            "         [-0.0814, -0.0508,  0.0073],\n",
            "         [ 0.4916,  0.2838,  0.4408],\n",
            "         [ 0.2060, -0.3991, -0.2680],\n",
            "         [-0.0569,  0.7442,  0.6655],\n",
            "         [ 0.4857, -0.3811, -0.2539]]], device='cuda:0') tensor([-0.1092,  0.2630,  0.8627,  0.2849,  0.3467, -0.1334], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.366943359375\n",
            "tcost icost 0.06353759765625 0.0\n",
            "tcost icost 0.148193359375 0.0\n",
            "tcost icost 0.05047607421875 0.0\n",
            "tcost icost -0.02630615234375 0.0\n",
            "tcost icost -0.0238189697265625 0.0\n",
            "tcost icost -0.0099639892578125 0.0\n",
            "loss tensor([[0.1973]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.8843,  0.3114,  0.3479],\n",
            "         [-0.0758, -0.1029, -0.0556],\n",
            "         [ 0.5791,  0.3338,  0.4844],\n",
            "         [ 0.2408, -0.4053, -0.2836],\n",
            "         [ 0.0161,  0.7385,  0.6741],\n",
            "         [ 0.4935, -0.4425, -0.3209]]], device='cuda:0') tensor([-0.5335,  0.9788,  0.5280,  0.3553,  0.2839, -0.1282], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.197265625\n",
            "tcost icost 0.00424957275390625 0.0\n",
            "tcost icost -0.268798828125 0.0\n",
            "tcost icost -0.0313720703125 0.0\n",
            "tcost icost 0.035430908203125 0.0\n",
            "tcost icost -0.01983642578125 0.0\n",
            "tcost icost -0.00641632080078125 0.0\n",
            "loss tensor([[-0.2542]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9086,  0.2817,  0.3085],\n",
            "         [-0.0405, -0.1279, -0.0957],\n",
            "         [ 0.6687,  0.3553,  0.4971],\n",
            "         [ 0.2981, -0.3743, -0.2631],\n",
            "         [ 0.0776,  0.7339,  0.6749],\n",
            "         [ 0.5279, -0.4817, -0.3660]]], device='cuda:0') tensor([ 0.1950,  0.6541,  0.8693,  0.0977,  0.3125, -0.1326], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.254150390625\n",
            "tcost icost 0.1044921875 0.0\n",
            "tcost icost 0.10833740234375 0.0\n",
            "tcost icost 0.10662841796875 0.0\n",
            "tcost icost -0.0291595458984375 0.0\n",
            "tcost icost -0.028533935546875 0.0\n",
            "tcost icost -0.007717132568359375 0.0\n",
            "loss tensor([[0.2439]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9016,  0.2952,  0.3162],\n",
            "         [-0.0418, -0.1734, -0.1520],\n",
            "         [ 0.7364,  0.3884,  0.5201],\n",
            "         [ 0.3486, -0.3498, -0.2476],\n",
            "         [ 0.1322,  0.7250,  0.6760],\n",
            "         [ 0.5368, -0.5282, -0.4181]]], device='cuda:0') tensor([-0.9974,  1.0000,  0.4777,  0.1510,  0.2624, -0.1245], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.243896484375\n",
            "tcost icost -0.00688934326171875 0.0\n",
            "tcost icost -0.39892578125 0.0\n",
            "tcost icost -0.044036865234375 0.0\n",
            "tcost icost -0.11358642578125 0.0\n",
            "tcost icost -0.025726318359375 0.0\n",
            "tcost icost -0.0098114013671875 0.0\n",
            "loss tensor([[-0.5073]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9030,  0.2952,  0.3123],\n",
            "         [-0.0272, -0.2060, -0.1960],\n",
            "         [ 0.7678,  0.3904,  0.5080],\n",
            "         [ 0.4089, -0.3100, -0.2143],\n",
            "         [ 0.1820,  0.7179,  0.6719],\n",
            "         [ 0.5791, -0.5364, -0.4314]]], device='cuda:0') tensor([-0.5999,  0.8187,  0.6676, -0.0384,  0.3014, -0.1407], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.50732421875\n",
            "tcost icost 0.002269744873046875 0.0\n",
            "tcost icost -0.2587890625 0.0\n",
            "tcost icost -0.02197265625 0.0\n",
            "tcost icost 0.02716064453125 0.0\n",
            "tcost icost -0.003559112548828125 0.0\n",
            "tcost icost -0.003509521484375 0.0\n",
            "loss tensor([[-0.2330]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9118,  0.2849,  0.2956],\n",
            "         [-0.0020, -0.2301, -0.2354],\n",
            "         [ 0.7837,  0.3857,  0.4869],\n",
            "         [ 0.4650, -0.2763, -0.1914],\n",
            "         [ 0.2257,  0.7119,  0.6650],\n",
            "         [ 0.5987, -0.5559, -0.4591]]], device='cuda:0') tensor([-0.3240,  0.5551,  0.6433,  0.0591,  0.3131, -0.1292], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2330322265625\n",
            "tcost icost 0.0178375244140625 0.0\n",
            "tcost icost -0.0892333984375 0.0\n",
            "tcost icost 0.09844970703125 0.0\n",
            "tcost icost -0.002124786376953125 0.0\n",
            "tcost icost 0.005802154541015625 0.0\n",
            "tcost icost -0.00677490234375 0.0\n",
            "loss tensor([[0.0155]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9362,  0.2462,  0.2506],\n",
            "         [ 0.0448, -0.2316, -0.2603],\n",
            "         [ 0.8093,  0.3719,  0.4547],\n",
            "         [ 0.5153, -0.2470, -0.1741],\n",
            "         [ 0.2654,  0.7058,  0.6568],\n",
            "         [ 0.6040, -0.5816, -0.4970]]], device='cuda:0') tensor([ 1.0000, -0.2667,  0.8762,  0.0754,  0.3150, -0.1220], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.015533447265625\n",
            "tcost icost -0.135009765625 0.0\n",
            "tcost icost -0.06146240234375 0.0\n",
            "tcost icost -0.327392578125 0.0\n",
            "tcost icost -0.043487548828125 0.0\n",
            "tcost icost -0.1910400390625 0.0\n",
            "tcost icost -0.030670166015625 0.0\n",
            "loss tensor([[-0.6309]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9524,  0.2153,  0.2157],\n",
            "         [ 0.0888, -0.2342, -0.2832],\n",
            "         [ 0.8253,  0.3620,  0.4334],\n",
            "         [ 0.5626, -0.2240, -0.1566],\n",
            "         [ 0.3015,  0.6925,  0.6554],\n",
            "         [ 0.5867, -0.6076, -0.5353]]], device='cuda:0') tensor([ 0.8794, -0.1028,  0.8443,  0.1763,  0.1588, -0.1110], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.630859375\n",
            "tcost icost -0.11114501953125 0.0\n",
            "tcost icost -0.050689697265625 0.0\n",
            "tcost icost -0.322021484375 0.0\n",
            "tcost icost -0.040191650390625 0.0\n",
            "tcost icost -0.1298828125 0.0\n",
            "tcost icost -0.0223388671875 0.0\n",
            "loss tensor([[-0.5454]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9623,  0.1933,  0.1916],\n",
            "         [ 0.1296, -0.2385, -0.3051],\n",
            "         [ 0.8334,  0.3565,  0.4223],\n",
            "         [ 0.6071, -0.2074, -0.1401],\n",
            "         [ 0.3338,  0.6740,  0.6590],\n",
            "         [ 0.5415, -0.6232, -0.5643]]], device='cuda:0') tensor([ 0.6875,  0.0983,  0.7985,  0.2919,  0.0085, -0.1004], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.54541015625\n",
            "tcost icost -0.06243896484375 0.0\n",
            "tcost icost -0.0308990478515625 0.0\n",
            "tcost icost -0.295654296875 0.0\n",
            "tcost icost -0.036376953125 0.0\n",
            "tcost icost -0.061676025390625 0.0\n",
            "tcost icost -0.01381683349609375 0.0\n",
            "loss tensor([[-0.4048]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9664,  0.1836,  0.1797],\n",
            "         [ 0.1650, -0.2459, -0.3279],\n",
            "         [ 0.8347,  0.3560,  0.4200],\n",
            "         [ 0.6488, -0.1960, -0.1255],\n",
            "         [ 0.3631,  0.6532,  0.6644],\n",
            "         [ 0.4895, -0.6387, -0.5937]]], device='cuda:0') tensor([ 0.2861,  0.3605,  0.7069,  0.3867, -0.1033, -0.0890], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.40478515625\n",
            "tcost icost 0.07977294921875 0.0\n",
            "tcost icost 0.0095977783203125 0.0\n",
            "tcost icost -0.1207275390625 0.0\n",
            "tcost icost -0.0267181396484375 0.0\n",
            "tcost icost -0.0157012939453125 0.0\n",
            "tcost icost -0.0018901824951171875 0.0\n",
            "loss tensor([[-0.0403]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9623,  0.1960,  0.1884],\n",
            "         [ 0.1819, -0.2641, -0.3610],\n",
            "         [ 0.8284,  0.3638,  0.4259],\n",
            "         [ 0.6870, -0.1881, -0.1145],\n",
            "         [ 0.3896,  0.6330,  0.6690],\n",
            "         [ 0.4275, -0.6547, -0.6235]]], device='cuda:0') tensor([-0.7764,  1.0000,  0.4337,  0.4296, -0.1437, -0.0751], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.040313720703125\n",
            "tcost icost -0.00238800048828125 0.0\n",
            "tcost icost -0.34814453125 0.0\n",
            "tcost icost -0.04364013671875 0.0\n",
            "tcost icost -0.1407470703125 0.0\n",
            "tcost icost -0.0440673828125 0.0\n",
            "tcost icost -0.032501220703125 0.0\n",
            "loss tensor([[-0.5015]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9619,  0.1980,  0.1884],\n",
            "         [ 0.2075, -0.2751, -0.3859],\n",
            "         [ 0.8286,  0.3638,  0.4256],\n",
            "         [ 0.7313, -0.1655, -0.0877],\n",
            "         [ 0.4195,  0.6120,  0.6704],\n",
            "         [ 0.4274, -0.6480, -0.6304]]], device='cuda:0') tensor([-0.2744,  0.8134,  0.6006,  0.2792, -0.0994, -0.1035], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.50146484375\n",
            "tcost icost 0.0242919921875 0.0\n",
            "tcost icost -0.1341552734375 0.0\n",
            "tcost icost 0.0145263671875 0.0\n",
            "tcost icost 0.0262908935546875 0.0\n",
            "tcost icost -0.01447296142578125 0.0\n",
            "tcost icost -0.01171112060546875 0.0\n",
            "loss tensor([[-0.0819]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9674,  0.1853,  0.1726],\n",
            "         [ 0.2443, -0.2762, -0.4045],\n",
            "         [ 0.8341,  0.3592,  0.4186],\n",
            "         [ 0.7734, -0.1457, -0.0648],\n",
            "         [ 0.4529,  0.5911,  0.6674],\n",
            "         [ 0.4464, -0.6330, -0.6325]]], device='cuda:0') tensor([ 0.4424,  0.3658,  0.6831,  0.3377, -0.0584, -0.1175], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0819091796875\n",
            "tcost icost 0.017852783203125 0.0\n",
            "tcost icost -0.0032806396484375 0.0\n",
            "tcost icost -0.178466796875 0.0\n",
            "tcost icost -0.0287322998046875 0.0\n",
            "tcost icost -0.023681640625 0.0\n",
            "tcost icost -0.005828857421875 0.0\n",
            "loss tensor([[-0.1696]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9675,  0.1869,  0.1705],\n",
            "         [ 0.2709, -0.2827, -0.4270],\n",
            "         [ 0.8324,  0.3614,  0.4202],\n",
            "         [ 0.8120, -0.1308, -0.0454],\n",
            "         [ 0.4803,  0.5717,  0.6652],\n",
            "         [ 0.4518, -0.6222, -0.6393]]], device='cuda:0') tensor([-0.2549,  0.7281,  0.5047,  0.4027, -0.1231, -0.1042], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1695556640625\n",
            "tcost icost 0.027130126953125 0.0\n",
            "tcost icost -0.11468505859375 0.0\n",
            "tcost icost 0.012847900390625 0.0\n",
            "tcost icost 0.051483154296875 0.0\n",
            "tcost icost -0.01861572265625 0.0\n",
            "tcost icost -0.01361846923828125 0.0\n",
            "loss tensor([[-0.0484]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9725,  0.1750,  0.1539],\n",
            "         [ 0.3081, -0.2795, -0.4447],\n",
            "         [ 0.8361,  0.3593,  0.4145],\n",
            "         [ 0.8496, -0.1199, -0.0311],\n",
            "         [ 0.5123,  0.5504,  0.6593],\n",
            "         [ 0.4745, -0.6038, -0.6406]]], device='cuda:0') tensor([ 0.4146,  0.2389,  0.5536,  0.5376, -0.0836, -0.1173], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.04840087890625\n",
            "tcost icost 0.0289764404296875 0.0\n",
            "tcost icost -0.006130218505859375 0.0\n",
            "tcost icost -0.1875 0.0\n",
            "tcost icost -0.022308349609375 0.0\n",
            "tcost icost -0.0169525146484375 0.0\n",
            "tcost icost -0.005847930908203125 0.0\n",
            "loss tensor([[-0.1592]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9728,  0.1765,  0.1498],\n",
            "         [ 0.3361, -0.2814, -0.4664],\n",
            "         [ 0.8337,  0.3633,  0.4160],\n",
            "         [ 0.8839, -0.1126, -0.0202],\n",
            "         [ 0.5390,  0.5314,  0.6536],\n",
            "         [ 0.4802, -0.5908, -0.6484]]], device='cuda:0') tensor([-0.2944,  0.5773,  0.3643,  0.5823, -0.1269, -0.1029], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1591796875\n",
            "tcost icost 0.0216064453125 0.0\n",
            "tcost icost -0.0936279296875 0.0\n",
            "tcost icost 0.01561737060546875 0.0\n",
            "tcost icost 0.0467529296875 0.0\n",
            "tcost icost -0.0275421142578125 0.0\n",
            "tcost icost -0.01520538330078125 0.0\n",
            "loss tensor([[-0.0430]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9795,  0.1571,  0.1260],\n",
            "         [ 0.3795, -0.2691, -0.4747],\n",
            "         [ 0.8513,  0.3466,  0.3938],\n",
            "         [ 0.9240, -0.0882,  0.0048],\n",
            "         [ 0.5704,  0.5086,  0.6449],\n",
            "         [ 0.5038, -0.5707, -0.6484]]], device='cuda:0') tensor([ 0.9878, -0.1805,  0.8066,  0.3945, -0.0830, -0.1166], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.042999267578125\n",
            "search tensor([[[ 0.5704,  0.5086,  0.6449],\n",
            "         [ 0.5038, -0.5707, -0.6484],\n",
            "         [ 0.5142,  0.0415, -0.2153],\n",
            "         [ 0.4862, -0.0712,  0.4190],\n",
            "         [-0.1401,  0.0465, -0.2909],\n",
            "         [-0.4215,  0.3595, -0.0538]]], device='cuda:0') tensor([[[-0.0830],\n",
            "         [-0.1166],\n",
            "         [-0.3368],\n",
            "         [-0.2330],\n",
            "         [-0.4626],\n",
            "         [-0.1415]]], device='cuda:0')\n",
            "tcost icost 0.007007598876953125 0.0\n",
            "tcost icost 0.0938720703125 0.0\n",
            "tcost icost 0.2568359375 0.0\n",
            "tcost icost 0.1898193359375 0.0\n",
            "tcost icost 0.003582000732421875 0.0\n",
            "tcost icost -0.0157928466796875 0.0\n",
            "loss tensor([[0.4312]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.4699,  0.6081,  0.5443],\n",
            "         [ 0.3727, -0.6193, -0.6911],\n",
            "         [ 0.6137, -0.0586, -0.3150],\n",
            "         [ 0.5857, -0.1711,  0.3186],\n",
            "         [-0.2400, -0.0536, -0.3906],\n",
            "         [-0.3210,  0.4591,  0.0463]]], device='cuda:0') tensor([-0.8852,  0.0468, -0.0753, -0.0240,  0.0030, -0.3287], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.43115234375\n",
            "tcost icost -0.01438140869140625 0.0\n",
            "tcost icost -0.1309814453125 0.0\n",
            "tcost icost -0.0272979736328125 0.0\n",
            "tcost icost -0.25390625 0.0\n",
            "tcost icost -0.045166015625 0.0\n",
            "tcost icost -0.06805419921875 0.0\n",
            "loss tensor([[-0.4092]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.5176,  0.6448,  0.4638],\n",
            "         [ 0.3767, -0.5709, -0.7292],\n",
            "         [ 0.6749, -0.1581, -0.3869],\n",
            "         [ 0.6756, -0.1346,  0.3217],\n",
            "         [-0.3164, -0.1305, -0.4612],\n",
            "         [-0.2247,  0.5524,  0.1457]]], device='cuda:0') tensor([-0.4895, -0.2975,  0.1439, -0.1413,  0.0974, -0.4429], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4091796875\n",
            "tcost icost -0.0037078857421875 0.0\n",
            "tcost icost 0.0293121337890625 0.0\n",
            "tcost icost 0.14013671875 0.0\n",
            "tcost icost 0.1363525390625 0.0\n",
            "tcost icost 0.024383544921875 0.0\n",
            "tcost icost 0.000576019287109375 0.0\n",
            "loss tensor([[0.2517]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.5807,  0.6839,  0.4115],\n",
            "         [ 0.3163, -0.5682, -0.7597],\n",
            "         [ 0.7493, -0.1160, -0.3874],\n",
            "         [ 0.7670, -0.1819,  0.2636],\n",
            "         [-0.3781, -0.2074, -0.5293],\n",
            "         [-0.1511,  0.6229,  0.2173]]], device='cuda:0') tensor([-0.9221,  0.0556, -0.8527,  0.5379,  0.2802, -0.4316], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.251708984375\n",
            "tcost icost -0.015228271484375 0.0\n",
            "tcost icost -0.138916015625 0.0\n",
            "tcost icost -0.053009033203125 0.0\n",
            "tcost icost -0.404296875 0.0\n",
            "tcost icost -0.050445556640625 0.0\n",
            "tcost icost -0.08367919921875 0.0\n",
            "loss tensor([[-0.5605]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.6475,  0.6832,  0.3377],\n",
            "         [ 0.2889, -0.5488, -0.7845],\n",
            "         [ 0.8155, -0.0826, -0.3818],\n",
            "         [ 0.8509, -0.2104,  0.2259],\n",
            "         [-0.4327, -0.2768, -0.5860],\n",
            "         [-0.0714,  0.6945,  0.2984]]], device='cuda:0') tensor([-0.6805, -0.1628, -0.7706,  0.4776,  0.3712, -0.5650], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.560546875\n",
            "tcost icost -0.00881195068359375 0.0\n",
            "tcost icost -0.0343017578125 0.0\n",
            "tcost icost -0.029510498046875 0.0\n",
            "tcost icost -0.3916015625 0.0\n",
            "tcost icost -0.04571533203125 0.0\n",
            "tcost icost -0.038818359375 0.0\n",
            "loss tensor([[-0.4021]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.7142,  0.6524,  0.2538],\n",
            "         [ 0.2857, -0.5151, -0.8081],\n",
            "         [ 0.8727, -0.0580, -0.3770],\n",
            "         [ 0.9316, -0.2247,  0.2014],\n",
            "         [-0.4775, -0.3402, -0.6351],\n",
            "         [ 0.0069,  0.7608,  0.3808]]], device='cuda:0') tensor([-0.3538, -0.4558, -0.6396,  0.3833,  0.4382, -0.6443], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.402099609375\n",
            "tcost icost -0.0012941360473632812 0.0\n",
            "tcost icost 0.0311737060546875 0.0\n",
            "tcost icost 0.027618408203125 0.0\n",
            "tcost icost -0.1932373046875 0.0\n",
            "tcost icost -0.03094482421875 0.0\n",
            "tcost icost -0.00446319580078125 0.0\n",
            "loss tensor([[-0.1147]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.7853,  0.5934,  0.1769],\n",
            "         [ 0.3047, -0.4655, -0.8267],\n",
            "         [ 0.9047, -0.0569, -0.3951],\n",
            "         [ 0.9610, -0.2126,  0.1769],\n",
            "         [-0.5114, -0.3977, -0.6793],\n",
            "         [ 0.0747,  0.8183,  0.4515]]], device='cuda:0') tensor([ 0.5260, -0.7983, -0.1850,  0.1868,  0.4596, -0.6419], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.11468505859375\n",
            "tcost icost -0.050140380859375 0.0\n",
            "tcost icost -0.0411376953125 0.0\n",
            "tcost icost -0.2235107421875 0.0\n",
            "tcost icost -0.02508544921875 0.0\n",
            "tcost icost -0.2822265625 0.0\n",
            "tcost icost -0.05615234375 0.0\n",
            "loss tensor([[-0.5049]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.8245,  0.5502,  0.1321],\n",
            "         [ 0.3213, -0.4289, -0.8421],\n",
            "         [ 0.9202, -0.0457, -0.3888],\n",
            "         [ 0.9670, -0.2055,  0.1506],\n",
            "         [-0.5431, -0.4363, -0.7168],\n",
            "         [ 0.1331,  0.8541,  0.5028]]], device='cuda:0') tensor([ 0.1965, -0.6665, -0.3984,  0.3272,  0.3230, -0.6312], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5048828125\n",
            "tcost icost 0.039703369140625 0.0\n",
            "tcost icost -0.01279449462890625 0.0\n",
            "tcost icost -0.1229248046875 0.0\n",
            "tcost icost -0.0001552104949951172 0.0\n",
            "tcost icost -0.1146240234375 0.0\n",
            "tcost icost -0.042999267578125 0.0\n",
            "loss tensor([[-0.1721]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.8344,  0.5371,  0.1236],\n",
            "         [ 0.3175, -0.4216, -0.8494],\n",
            "         [ 0.9337, -0.0186, -0.3575],\n",
            "         [ 0.9691, -0.2159,  0.1197],\n",
            "         [-0.5467, -0.4355, -0.7152],\n",
            "         [ 0.1751,  0.8385,  0.5160]]], device='cuda:0') tensor([-0.1929, -0.1672, -0.8718,  0.6301,  0.1286, -0.6206], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.172119140625\n",
            "tcost icost 0.002307891845703125 0.0\n",
            "tcost icost 0.1531982421875 0.0\n",
            "tcost icost -0.007778167724609375 0.0\n",
            "tcost icost -0.350341796875 0.0\n",
            "tcost icost -0.049591064453125 0.0\n",
            "tcost icost -0.03533935546875 0.0\n",
            "loss tensor([[-0.1749]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.8635,  0.4956,  0.0933],\n",
            "         [ 0.3259, -0.3962, -0.8584],\n",
            "         [ 0.9423, -0.0011, -0.3347],\n",
            "         [ 0.9716, -0.2152,  0.0983],\n",
            "         [-0.5451, -0.4375, -0.7151],\n",
            "         [ 0.2132,  0.8208,  0.5300]]], device='cuda:0') tensor([ 0.4672, -0.3684, -0.6684,  0.5153,  0.1856, -0.6705], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1749267578125\n",
            "tcost icost -0.032806396484375 0.0\n",
            "tcost icost -0.0206756591796875 0.0\n",
            "tcost icost -0.02978515625 0.0\n",
            "tcost icost 0.0406494140625 0.0\n",
            "tcost icost 0.12322998046875 0.0\n",
            "tcost icost -0.01113128662109375 0.0\n",
            "loss tensor([[0.0284]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.8843,  0.4614,  0.0709],\n",
            "         [ 0.3265, -0.3731, -0.8685],\n",
            "         [ 0.9439,  0.0068, -0.3301],\n",
            "         [ 0.9769, -0.1968,  0.0837],\n",
            "         [-0.5077, -0.4534, -0.7326],\n",
            "         [ 0.2464,  0.8059,  0.5384]]], device='cuda:0') tensor([ 0.0888, -0.3479, -0.6050,  0.1779,  0.3410, -0.6553], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0283660888671875\n",
            "tcost icost 0.0172119140625 0.0\n",
            "tcost icost 0.006893157958984375 0.0\n",
            "tcost icost 0.01021575927734375 0.0\n",
            "tcost icost 0.1531982421875 0.0\n",
            "tcost icost 0.1513671875 0.0\n",
            "tcost icost -0.01291656494140625 0.0\n",
            "loss tensor([[0.2351]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9079,  0.4174,  0.0391],\n",
            "         [ 0.3311, -0.3408, -0.8799],\n",
            "         [ 0.9364, -0.0057, -0.3510],\n",
            "         [ 0.9840, -0.1591,  0.0800],\n",
            "         [-0.4523, -0.4767, -0.7538],\n",
            "         [ 0.2760,  0.7933,  0.5427]]], device='cuda:0') tensor([ 0.4736, -0.5149, -0.1307, -0.3116,  0.5271, -0.6416], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.235107421875\n",
            "tcost icost -0.034759521484375 0.0\n",
            "tcost icost -0.02667236328125 0.0\n",
            "tcost icost -0.2039794921875 0.0\n",
            "tcost icost -0.038604736328125 0.0\n",
            "tcost icost -0.31591796875 0.0\n",
            "tcost icost -0.056427001953125 0.0\n",
            "loss tensor([[-0.4927]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9201,  0.3907,  0.0270],\n",
            "         [ 0.3300, -0.3206, -0.8879],\n",
            "         [ 0.9329, -0.0100, -0.3599],\n",
            "         [ 0.9887, -0.1288,  0.0760],\n",
            "         [-0.4029, -0.4898, -0.7732],\n",
            "         [ 0.3025,  0.7820,  0.5450]]], device='cuda:0') tensor([ 0.0199, -0.2671, -0.3190, -0.2243,  0.4137, -0.6315], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.49267578125\n",
            "tcost icost 0.0107574462890625 0.0\n",
            "tcost icost 0.0260162353515625 0.0\n",
            "tcost icost 0.15185546875 0.0\n",
            "tcost icost 0.02728271484375 0.0\n",
            "tcost icost -0.08685302734375 0.0\n",
            "tcost icost -0.03662109375 0.0\n",
            "loss tensor([[0.0985]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9122,  0.4058,  0.0567],\n",
            "         [ 0.2849, -0.3395, -0.8964],\n",
            "         [ 0.9407,  0.0144, -0.3391],\n",
            "         [ 0.9902, -0.1303,  0.0506],\n",
            "         [-0.3521, -0.4889, -0.7982],\n",
            "         [ 0.3271,  0.7723,  0.5446]]], device='cuda:0') tensor([-1.0000,  1.0000, -1.0000,  0.4329,  0.1774, -0.6154], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0985107421875\n",
            "tcost icost -0.0179595947265625 0.0\n",
            "tcost icost -0.414306640625 0.0\n",
            "tcost icost -0.07110595703125 0.0\n",
            "tcost icost -0.343994140625 0.0\n",
            "tcost icost -0.0496826171875 0.0\n",
            "tcost icost -0.048980712890625 0.0\n",
            "loss tensor([[-0.7607]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9064,  0.4146,  0.0807],\n",
            "         [ 0.2487, -0.3532, -0.9019],\n",
            "         [ 0.9471,  0.0357, -0.3188],\n",
            "         [ 0.9914, -0.1263,  0.0336],\n",
            "         [-0.3041, -0.4885, -0.8179],\n",
            "         [ 0.3500,  0.7606,  0.5467]]], device='cuda:0') tensor([-0.8058,  0.8765, -0.9732,  0.3433,  0.2309, -0.6581], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.7607421875\n",
            "tcost icost -0.01348114013671875 0.0\n",
            "tcost icost -0.365478515625 0.0\n",
            "tcost icost -0.073486328125 0.0\n",
            "tcost icost -0.311767578125 0.0\n",
            "tcost icost -0.046234130859375 0.0\n",
            "tcost icost -0.039581298828125 0.0\n",
            "loss tensor([[-0.6826]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9038,  0.4166,  0.0982],\n",
            "         [ 0.2203, -0.3625, -0.9056],\n",
            "         [ 0.9527,  0.0545, -0.2991],\n",
            "         [ 0.9928, -0.1174,  0.0242],\n",
            "         [-0.2594, -0.4887, -0.8330],\n",
            "         [ 0.3711,  0.7481,  0.5502]]], device='cuda:0') tensor([-0.4960,  0.7595, -0.9452,  0.2480,  0.2802, -0.6853], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6826171875\n",
            "tcost icost -0.004505157470703125 0.0\n",
            "tcost icost -0.2548828125 0.0\n",
            "tcost icost -0.07391357421875 0.0\n",
            "tcost icost -0.26953125 0.0\n",
            "tcost icost -0.040283203125 0.0\n",
            "tcost icost -0.0310821533203125 0.0\n",
            "loss tensor([[-0.5352]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9054,  0.4106,  0.1081],\n",
            "         [ 0.1994, -0.3676, -0.9084],\n",
            "         [ 0.9576,  0.0709, -0.2792],\n",
            "         [ 0.9944, -0.1034,  0.0222],\n",
            "         [-0.2183, -0.4898, -0.8441],\n",
            "         [ 0.3902,  0.7347,  0.5549]]], device='cuda:0') tensor([-0.0573,  0.6396, -0.9101,  0.1416,  0.3324, -0.7181], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.53515625\n",
            "tcost icost 0.006870269775390625 0.0\n",
            "tcost icost -0.0516357421875 0.0\n",
            "tcost icost -0.05316162109375 0.0\n",
            "tcost icost -0.2235107421875 0.0\n",
            "tcost icost -0.0268707275390625 0.0\n",
            "tcost icost -0.025482177734375 0.0\n",
            "loss tensor([[-0.2783]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9144,  0.3908,  0.1056],\n",
            "         [ 0.1874, -0.3661, -0.9115],\n",
            "         [ 0.9621,  0.0848, -0.2593],\n",
            "         [ 0.9961, -0.0833,  0.0279],\n",
            "         [-0.1812, -0.4920, -0.8515],\n",
            "         [ 0.4076,  0.7209,  0.5605]]], device='cuda:0') tensor([ 0.8065,  0.3859, -0.8522, -0.0254,  0.3938, -0.7447], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2783203125\n",
            "tcost icost -0.1170654296875 0.0\n",
            "tcost icost -0.01383209228515625 0.0\n",
            "tcost icost -0.0082855224609375 0.0\n",
            "tcost icost 0.0218353271484375 0.0\n",
            "tcost icost 0.1358642578125 0.0\n",
            "tcost icost -0.0158843994140625 0.0\n",
            "loss tensor([[-0.0406]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9246,  0.3681,  0.0982],\n",
            "         [ 0.1838, -0.3561, -0.9162],\n",
            "         [ 0.9612,  0.0807, -0.2637],\n",
            "         [ 0.9978, -0.0563,  0.0362],\n",
            "         [-0.1376, -0.4928, -0.8592],\n",
            "         [ 0.4257,  0.7089,  0.5623]]], device='cuda:0') tensor([ 0.8721, -0.0630, -0.2613, -0.2463,  0.3989, -0.7302], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.040557861328125\n",
            "tcost icost -0.1307373046875 0.0\n",
            "tcost icost -0.0343017578125 0.0\n",
            "tcost icost -0.11712646484375 0.0\n",
            "tcost icost -0.0281219482421875 0.0\n",
            "tcost icost -0.2467041015625 0.0\n",
            "tcost icost -0.05157470703125 0.0\n",
            "loss tensor([[-0.4692]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9305,  0.3530,  0.0982],\n",
            "         [ 0.1772, -0.3523, -0.9190],\n",
            "         [ 0.9628,  0.0844, -0.2566],\n",
            "         [ 0.9985, -0.0363,  0.0418],\n",
            "         [-0.0836, -0.4857, -0.8701],\n",
            "         [ 0.4434,  0.6981,  0.5622]]], device='cuda:0') tensor([ 0.5770,  0.2741, -0.5316, -0.1176,  0.2638, -0.7195], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.46923828125\n",
            "tcost icost -0.0640869140625 0.0\n",
            "tcost icost -0.0018911361694335938 0.0\n",
            "tcost icost 0.0018720626831054688 0.0\n",
            "tcost icost 0.046966552734375 0.0\n",
            "tcost icost 0.07135009765625 0.0\n",
            "tcost icost -0.022857666015625 0.0\n",
            "loss tensor([[0.0033]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9225,  0.3668,  0.1202],\n",
            "         [ 0.1550, -0.3636, -0.9186],\n",
            "         [ 0.9684,  0.1087, -0.2245],\n",
            "         [ 0.9984, -0.0545,  0.0142],\n",
            "         [-0.0241, -0.4553, -0.8900],\n",
            "         [ 0.4624,  0.6886,  0.5586]]], device='cuda:0') tensor([-1.0000,  1.0000, -1.0000,  1.0000, -0.1918, -0.7042], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.00328826904296875\n",
            "search tensor([[[-0.0241, -0.4553, -0.8900],\n",
            "         [ 0.4624,  0.6886,  0.5586],\n",
            "         [-0.3233,  0.3254, -0.2081],\n",
            "         [ 0.3327, -0.1556, -0.4501],\n",
            "         [ 0.0948, -0.1180, -0.1690],\n",
            "         [-0.4829,  0.4921,  0.5157]]], device='cuda:0') tensor([[[-0.1918],\n",
            "         [-0.7042],\n",
            "         [ 0.4563],\n",
            "         [ 0.3399],\n",
            "         [ 0.4199],\n",
            "         [-0.0650]]], device='cuda:0')\n",
            "tcost icost 0.01641845703125 0.0\n",
            "tcost icost 0.0028209686279296875 0.0\n",
            "tcost icost -0.2093505859375 0.0\n",
            "tcost icost -0.030914306640625 0.0\n",
            "tcost icost -0.13671875 0.0\n",
            "tcost icost -0.0196990966796875 0.0\n",
            "loss tensor([[-0.2747]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.0759, -0.3548, -0.7891],\n",
            "         [ 0.5619,  0.5879,  0.4581],\n",
            "         [-0.2230,  0.4251, -0.1078],\n",
            "         [ 0.2324, -0.2554, -0.5496],\n",
            "         [ 0.1947, -0.0179, -0.0689],\n",
            "         [-0.5824,  0.3916,  0.4152]]], device='cuda:0') tensor([-0.6085, -0.2215,  0.3169,  0.4969,  0.2711, -0.0564], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.274658203125\n",
            "tcost icost 0.0010385513305664062 0.0\n",
            "tcost icost -0.053924560546875 0.0\n",
            "tcost icost 0.13330078125 0.0\n",
            "tcost icost 0.1387939453125 0.0\n",
            "tcost icost -0.01019287109375 0.0\n",
            "tcost icost 0.0013284683227539062 0.0\n",
            "loss tensor([[0.1556]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.0190, -0.4091, -0.8589],\n",
            "         [ 0.6418,  0.5907,  0.4890],\n",
            "         [-0.2186,  0.3817, -0.1609],\n",
            "         [ 0.2982, -0.2029, -0.5257],\n",
            "         [ 0.2605,  0.0474, -0.0079],\n",
            "         [-0.6788,  0.2989,  0.3191]]], device='cuda:0') tensor([ 0.8768, -1.0000,  0.8603,  0.1974,  0.2724, -0.0327], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1556396484375\n",
            "tcost icost -0.1123046875 0.0\n",
            "tcost icost -0.06427001953125 0.0\n",
            "tcost icost -0.361328125 0.0\n",
            "tcost icost -0.042572021484375 0.0\n",
            "tcost icost -0.2247314453125 0.0\n",
            "tcost icost -0.025390625 0.0\n",
            "loss tensor([[-0.6567]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.0164, -0.4400, -0.8979],\n",
            "         [ 0.6873,  0.5355,  0.4907],\n",
            "         [-0.1880,  0.3607, -0.1887],\n",
            "         [ 0.3479, -0.1752, -0.5053],\n",
            "         [ 0.3390,  0.1269,  0.0703],\n",
            "         [-0.7659,  0.2077,  0.2266]]], device='cuda:0') tensor([ 0.7650, -0.9219,  0.8119,  0.3057,  0.1294, -0.0222], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.65673828125\n",
            "tcost icost -0.0916748046875 0.0\n",
            "tcost icost -0.057342529296875 0.0\n",
            "tcost icost -0.367431640625 0.0\n",
            "tcost icost -0.039306640625 0.0\n",
            "tcost icost -0.179931640625 0.0\n",
            "tcost icost -0.0211639404296875 0.0\n",
            "loss tensor([[-0.6001]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.0361, -0.4432, -0.8957],\n",
            "         [ 0.7364,  0.4657,  0.4907],\n",
            "         [-0.1419,  0.3552, -0.1993],\n",
            "         [ 0.3873, -0.1655, -0.4886],\n",
            "         [ 0.4225,  0.2127,  0.1558],\n",
            "         [-0.8445,  0.1162,  0.1343]]], device='cuda:0') tensor([ 0.6279, -0.8242,  0.7604,  0.4270, -0.0271, -0.0146], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.60009765625\n",
            "tcost icost -0.06280517578125 0.0\n",
            "tcost icost -0.04779052734375 0.0\n",
            "tcost icost -0.3720703125 0.0\n",
            "tcost icost -0.035552978515625 0.0\n",
            "tcost icost -0.12054443359375 0.0\n",
            "tcost icost -0.01165771484375 0.0\n",
            "loss tensor([[-0.5190]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.0437, -0.4397, -0.8971],\n",
            "         [ 0.7855,  0.3846,  0.4849],\n",
            "         [-0.0861,  0.3620, -0.1962],\n",
            "         [ 0.4190, -0.1699, -0.4770],\n",
            "         [ 0.5073,  0.3015,  0.2454],\n",
            "         [-0.9150,  0.0237,  0.0416]]], device='cuda:0') tensor([ 0.4265, -0.6859,  0.7052,  0.5608, -0.1999, -0.0088], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.51904296875\n",
            "tcost icost -0.00443267822265625 0.0\n",
            "tcost icost -0.033416748046875 0.0\n",
            "tcost icost -0.372802734375 0.0\n",
            "tcost icost -0.032135009765625 0.0\n",
            "tcost icost -0.05242919921875 0.0\n",
            "tcost icost -0.00423431396484375 0.0\n",
            "loss tensor([[-0.3967]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.0372, -0.4244, -0.9047],\n",
            "         [ 0.8328,  0.2962,  0.4677],\n",
            "         [-0.0205,  0.3788, -0.1833],\n",
            "         [ 0.4445, -0.1826, -0.4788],\n",
            "         [ 0.5886,  0.3883,  0.3348],\n",
            "         [-0.9743, -0.0695, -0.0523]]], device='cuda:0') tensor([ 2.1952e-02, -5.1508e-01,  6.1245e-01,  6.6995e-01, -3.1625e-01,\n",
            "         2.2741e-04], device='cuda:0', grad_fn=<SqueezeBackward0>) -0.396728515625\n",
            "tcost icost 0.04168701171875 0.0\n",
            "tcost icost -0.0013942718505859375 0.0\n",
            "tcost icost -0.2802734375 0.0\n",
            "tcost icost -0.0286712646484375 0.0\n",
            "tcost icost -0.0099945068359375 0.0\n",
            "tcost icost 0.004001617431640625 0.0\n",
            "loss tensor([[-0.2118]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.0267, -0.4051, -0.9139],\n",
            "         [ 0.8785,  0.2068,  0.4307],\n",
            "         [ 0.0536,  0.4037, -0.1649],\n",
            "         [ 0.4657, -0.1954, -0.4910],\n",
            "         [ 0.6623,  0.4655,  0.4150],\n",
            "         [-0.9775, -0.1569, -0.1409]]], device='cuda:0') tensor([ 0.0329, -0.2346,  0.4398,  0.6863, -0.3368,  0.0139], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2117919921875\n",
            "tcost icost 0.04461669921875 0.0\n",
            "tcost icost 0.009002685546875 0.0\n",
            "tcost icost -0.15625 0.0\n",
            "tcost icost -0.0152130126953125 0.0\n",
            "tcost icost -0.0104217529296875 0.0\n",
            "tcost icost 0.0036067962646484375 0.0\n",
            "loss tensor([[-0.0897]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.0090, -0.3786, -0.9255],\n",
            "         [ 0.9188,  0.1180,  0.3766],\n",
            "         [ 0.1339,  0.4353, -0.1429],\n",
            "         [ 0.4839, -0.2055, -0.5119],\n",
            "         [ 0.7154,  0.5182,  0.4686],\n",
            "         [-0.9507, -0.2181, -0.2205]]], device='cuda:0') tensor([-0.1131,  0.1199,  0.2334,  0.6667, -0.3068,  0.0077], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.08966064453125\n",
            "tcost icost 0.0232696533203125 0.0\n",
            "tcost icost 0.10467529296875 0.0\n",
            "tcost icost 0.27587890625 0.0\n",
            "tcost icost 0.143310546875 0.0\n",
            "tcost icost -0.018829345703125 0.0\n",
            "tcost icost -0.01486968994140625 0.0\n",
            "loss tensor([[0.4243]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.0040, -0.3639, -0.9314],\n",
            "         [ 0.9484,  0.0521,  0.3129],\n",
            "         [ 0.2155,  0.4635, -0.1331],\n",
            "         [ 0.5011, -0.2133, -0.5570],\n",
            "         [ 0.7108,  0.5202,  0.4735],\n",
            "         [-0.9321, -0.2331, -0.2771]]], device='cuda:0') tensor([ 0.4846, -0.1723,  0.0475,  0.7311, -0.2598, -0.0141], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.42431640625\n",
            "tcost icost -0.0231781005859375 0.0\n",
            "tcost icost -0.0181732177734375 0.0\n",
            "tcost icost -0.1839599609375 0.0\n",
            "tcost icost -0.00408172607421875 0.0\n",
            "tcost icost -0.002838134765625 0.0\n",
            "tcost icost 0.016876220703125 0.0\n",
            "loss tensor([[-0.1833]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.0141, -0.3384, -0.9409],\n",
            "         [ 0.9696, -0.0177,  0.2442],\n",
            "         [ 0.2991,  0.4987, -0.1149],\n",
            "         [ 0.5155, -0.2201, -0.6081],\n",
            "         [ 0.7090,  0.5204,  0.4759],\n",
            "         [-0.8982, -0.2775, -0.3410]]], device='cuda:0') tensor([-0.0471,  0.1329, -0.1994,  0.7406, -0.2900,  0.0633], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.183349609375\n",
            "tcost icost 0.0289154052734375 0.0\n",
            "tcost icost 0.108154296875 0.0\n",
            "tcost icost 0.1259765625 0.0\n",
            "tcost icost -0.06243896484375 0.0\n",
            "tcost icost -0.0268402099609375 0.0\n",
            "tcost icost -0.022705078125 0.0\n",
            "loss tensor([[0.1517]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.0012, -0.3387, -0.9409],\n",
            "         [ 0.9728,  0.0028,  0.2316],\n",
            "         [ 0.3572,  0.4783, -0.1461],\n",
            "         [ 0.5550, -0.1958, -0.6050],\n",
            "         [ 0.7135,  0.5167,  0.4732],\n",
            "         [-0.8745, -0.2899, -0.3889]]], device='cuda:0') tensor([ 1.0000, -1.0000,  1.0000,  0.4498, -0.2240,  0.0274], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1517333984375\n",
            "tcost icost -0.1346435546875 0.0\n",
            "tcost icost -0.07122802734375 0.0\n",
            "tcost icost -0.364013671875 0.0\n",
            "tcost icost -0.03741455078125 0.0\n",
            "tcost icost -0.071044921875 0.0\n",
            "tcost icost -0.0007457733154296875 0.0\n",
            "loss tensor([[-0.5679]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.0064, -0.3374, -0.9413],\n",
            "         [ 0.9747,  0.0206,  0.2227],\n",
            "         [ 0.4180,  0.4630, -0.1693],\n",
            "         [ 0.5904, -0.1804, -0.6041],\n",
            "         [ 0.7113,  0.5161,  0.4771],\n",
            "         [-0.8400, -0.3119, -0.4439]]], device='cuda:0') tensor([ 0.9178, -0.9498,  0.9412,  0.5732, -0.3765,  0.0560], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.56787109375\n",
            "tcost icost -0.1197509765625 0.0\n",
            "tcost icost -0.0670166015625 0.0\n",
            "tcost icost -0.368896484375 0.0\n",
            "tcost icost -0.0325927734375 0.0\n",
            "tcost icost -0.039031982421875 0.0\n",
            "tcost icost 0.010345458984375 0.0\n",
            "loss tensor([[-0.5220]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.0099, -0.3345, -0.9424],\n",
            "         [ 0.9757,  0.0361,  0.2161],\n",
            "         [ 0.4817,  0.4521, -0.1865],\n",
            "         [ 0.6210, -0.1698, -0.6091],\n",
            "         [ 0.7101,  0.5149,  0.4803],\n",
            "         [-0.7887, -0.3455, -0.5085]]], device='cuda:0') tensor([ 0.8049, -0.9355,  0.8441,  0.6484, -0.4457,  0.0962], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.52197265625\n",
            "tcost icost -0.09893798828125 0.0\n",
            "tcost icost -0.06121826171875 0.0\n",
            "tcost icost -0.37158203125 0.0\n",
            "tcost icost -0.02899169921875 0.0\n",
            "tcost icost -0.031585693359375 0.0\n",
            "tcost icost 0.01263427734375 0.0\n",
            "loss tensor([[-0.4895]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.0097, -0.3297, -0.9440],\n",
            "         [ 0.9762,  0.0495,  0.2110],\n",
            "         [ 0.5476,  0.4444, -0.1991],\n",
            "         [ 0.6448, -0.1638, -0.6238],\n",
            "         [ 0.7154,  0.5104,  0.4771],\n",
            "         [-0.7238, -0.3760, -0.5785]]], device='cuda:0') tensor([ 0.6701, -0.9197,  0.7313,  0.7187, -0.4790,  0.0967], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.489501953125\n",
            "tcost icost -0.072021484375 0.0\n",
            "tcost icost -0.05389404296875 0.0\n",
            "tcost icost -0.381103515625 0.0\n",
            "tcost icost -0.0280609130859375 0.0\n",
            "tcost icost -0.03045654296875 0.0\n",
            "tcost icost 0.010955810546875 0.0\n",
            "loss tensor([[-0.4634]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.0053, -0.3223, -0.9466],\n",
            "         [ 0.9766,  0.0606,  0.2063],\n",
            "         [ 0.6145,  0.4397, -0.2078],\n",
            "         [ 0.6638, -0.1615, -0.6445],\n",
            "         [ 0.7234,  0.5044,  0.4714],\n",
            "         [-0.6446, -0.4078, -0.6467]]], device='cuda:0') tensor([ 0.4853, -0.8857,  0.6130,  0.7807, -0.5214,  0.1219], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.46337890625\n",
            "tcost icost -0.023468017578125 0.0\n",
            "tcost icost -0.04254150390625 0.0\n",
            "tcost icost -0.380615234375 0.0\n",
            "tcost icost -0.02606201171875 0.0\n",
            "tcost icost -0.0229644775390625 0.0\n",
            "tcost icost 0.016632080078125 0.0\n",
            "loss tensor([[-0.3945]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.0057, -0.3097, -0.9508],\n",
            "         [ 0.9772,  0.0686,  0.2009],\n",
            "         [ 0.6815,  0.4375, -0.2132],\n",
            "         [ 0.6774, -0.1629, -0.6724],\n",
            "         [ 0.7348,  0.4966,  0.4621],\n",
            "         [-0.5526, -0.4406, -0.7075]]], device='cuda:0') tensor([ 0.1640, -0.8100,  0.4780,  0.8476, -0.5869,  0.1617], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.39453125\n",
            "tcost icost 0.061614990234375 0.0\n",
            "tcost icost -0.0186614990234375 0.0\n",
            "tcost icost -0.35693359375 0.0\n",
            "tcost icost -0.0257568359375 0.0\n",
            "tcost icost -0.007080078125 0.0\n",
            "tcost icost 0.019256591796875 0.0\n",
            "loss tensor([[-0.2561]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.0210, -0.2930, -0.9559],\n",
            "         [ 0.9789,  0.0720,  0.1910],\n",
            "         [ 0.7460,  0.4371, -0.2177],\n",
            "         [ 0.6863, -0.1626, -0.7074],\n",
            "         [ 0.7554,  0.4838,  0.4420],\n",
            "         [-0.4530, -0.4716, -0.7565]]], device='cuda:0') tensor([ 0.0023, -0.7090,  0.2809,  0.8260, -0.5659,  0.2201], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.256103515625\n",
            "tcost icost 0.037139892578125 0.0\n",
            "tcost icost -0.01113128662109375 0.0\n",
            "tcost icost -0.29443359375 0.0\n",
            "tcost icost -0.021240234375 0.0\n",
            "tcost icost -0.004413604736328125 0.0\n",
            "tcost icost 0.01360321044921875 0.0\n",
            "loss tensor([[-0.2218]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.0382, -0.2749, -0.9607],\n",
            "         [ 0.9821,  0.0696,  0.1748],\n",
            "         [ 0.8095,  0.4371, -0.2230],\n",
            "         [ 0.6706, -0.1523, -0.7260],\n",
            "         [ 0.7910,  0.4596,  0.4039],\n",
            "         [-0.3528, -0.4781, -0.8043]]], device='cuda:0') tensor([-0.0761, -0.5685,  0.0612,  0.7231, -0.4040,  0.1784], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2218017578125\n",
            "tcost icost 0.026702880859375 0.0\n",
            "tcost icost -0.0032863616943359375 0.0\n",
            "tcost icost -0.19287109375 0.0\n",
            "tcost icost -0.0124053955078125 0.0\n",
            "tcost icost 0.0004596710205078125 0.0\n",
            "tcost icost 0.058074951171875 0.0\n",
            "loss tensor([[-0.1071]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.0586, -0.2547, -0.9653],\n",
            "         [ 0.9870,  0.0589,  0.1496],\n",
            "         [ 0.8668,  0.4399, -0.2276],\n",
            "         [ 0.6498, -0.1369, -0.7477],\n",
            "         [ 0.8269,  0.4315,  0.3607],\n",
            "         [-0.2492, -0.4916, -0.8344]]], device='cuda:0') tensor([-0.2409, -0.2709, -0.2350,  0.6591, -0.4133,  0.3888], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1070556640625\n",
            "tcost icost 0.015625 0.0\n",
            "tcost icost 0.024017333984375 0.0\n",
            "tcost icost 0.3369140625 0.0\n",
            "tcost icost 0.1575927734375 0.0\n",
            "tcost icost -0.01319122314453125 0.0\n",
            "tcost icost -0.07232666015625 0.0\n",
            "loss tensor([[0.3735]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.0984, -0.2176, -0.9600],\n",
            "         [ 0.9944,  0.0237,  0.1034],\n",
            "         [ 0.8800,  0.4255, -0.2112],\n",
            "         [ 0.6442, -0.1121, -0.7566],\n",
            "         [ 0.8653,  0.3931,  0.3110],\n",
            "         [-0.1512, -0.4731, -0.8680]]], device='cuda:0') tensor([-1.0000,  1.0000, -0.9713,  0.5432, -0.2476,  0.2638], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.37353515625\n",
            "search tensor([[[ 0.8653,  0.3931,  0.3110],\n",
            "         [-0.1512, -0.4731, -0.8680],\n",
            "         [ 0.0359,  0.3924,  0.3567],\n",
            "         [-0.3165, -0.1519, -0.3911],\n",
            "         [-0.3866,  0.2117,  0.4405],\n",
            "         [-0.2084,  0.4704,  0.1324]]], device='cuda:0') tensor([[[-0.2476],\n",
            "         [ 0.2638],\n",
            "         [ 0.5764],\n",
            "         [ 0.3455],\n",
            "         [ 0.4885],\n",
            "         [ 0.4229]]], device='cuda:0')\n",
            "tcost icost 0.014739990234375 0.0\n",
            "tcost icost -0.11090087890625 0.0\n",
            "tcost icost 0.0111083984375 0.0\n",
            "tcost icost 0.08624267578125 0.0\n",
            "tcost icost 0.005390167236328125 0.0\n",
            "tcost icost 0.043243408203125 0.0\n",
            "loss tensor([[0.0158]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.9366,  0.2843,  0.2047],\n",
            "         [-0.0492, -0.3591, -0.9320],\n",
            "         [ 0.1358,  0.2920,  0.2563],\n",
            "         [-0.2162, -0.2517, -0.4907],\n",
            "         [-0.4862,  0.1115,  0.3401],\n",
            "         [-0.3081,  0.3699,  0.0323]]], device='cuda:0') tensor([ 0.2695, -0.2347,  0.4639,  0.3896,  0.5748,  0.5821], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0158233642578125\n",
            "tcost icost 0.05108642578125 0.0\n",
            "tcost icost 0.007720947265625 0.0\n",
            "tcost icost -0.1690673828125 0.0\n",
            "tcost icost -0.0311431884765625 0.0\n",
            "tcost icost -0.11663818359375 0.0\n",
            "tcost icost 0.0001347064971923828 0.0\n",
            "loss tensor([[-0.1781]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.9345,  0.2896,  0.2071],\n",
            "         [-0.0575, -0.3572, -0.9323],\n",
            "         [ 0.2351,  0.3550,  0.2749],\n",
            "         [-0.1494, -0.3501, -0.5713],\n",
            "         [-0.4303,  0.1519,  0.3495],\n",
            "         [-0.4011,  0.2814, -0.0495]]], device='cuda:0') tensor([-0.4249,  0.3307,  0.2908,  0.5497,  0.4139,  0.6199], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1781005859375\n",
            "tcost icost 0.006473541259765625 0.0\n",
            "tcost icost -0.184326171875 0.0\n",
            "tcost icost -0.028717041015625 0.0\n",
            "tcost icost -0.1546630859375 0.0\n",
            "tcost icost -0.0260009765625 0.0\n",
            "tcost icost -0.0460205078125 0.0\n",
            "loss tensor([[-0.3396]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.9498,  0.2567,  0.1786],\n",
            "         [-0.0222, -0.3282, -0.9444],\n",
            "         [ 0.3289,  0.3592,  0.2642],\n",
            "         [-0.0686, -0.3346, -0.6123],\n",
            "         [-0.3970,  0.1577,  0.3335],\n",
            "         [-0.3448,  0.2877, -0.0813]]], device='cuda:0') tensor([0.2094, 0.0568, 0.5433, 0.4130, 0.5252, 0.5187], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.339599609375\n",
            "tcost icost 0.05389404296875 0.0\n",
            "tcost icost 0.10235595703125 0.0\n",
            "tcost icost 0.0136260986328125 0.0\n",
            "tcost icost -0.01922607421875 0.0\n",
            "tcost icost -0.0379638671875 0.0\n",
            "tcost icost 0.0045166015625 0.0\n",
            "loss tensor([[0.1207]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9344,  0.2868,  0.2110],\n",
            "         [-0.0494, -0.3431, -0.9380],\n",
            "         [ 0.4246,  0.4032,  0.2977],\n",
            "         [-0.0109, -0.3450, -0.6565],\n",
            "         [-0.3372,  0.1976,  0.3548],\n",
            "         [-0.3101,  0.2719, -0.1226]]], device='cuda:0') tensor([-1.0000,  1.0000,  0.1224,  0.5670,  0.3620,  0.5822], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1207275390625\n",
            "tcost icost -0.00775909423828125 0.0\n",
            "tcost icost -0.451904296875 0.0\n",
            "tcost icost -0.054168701171875 0.0\n",
            "tcost icost -0.26318359375 0.0\n",
            "tcost icost -0.03546142578125 0.0\n",
            "tcost icost -0.06475830078125 0.0\n",
            "loss tensor([[-0.7119]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9238,  0.3037,  0.2331],\n",
            "         [-0.0618, -0.3520, -0.9340],\n",
            "         [ 0.5188,  0.4310,  0.3225],\n",
            "         [ 0.0601, -0.3274, -0.6763],\n",
            "         [-0.2906,  0.2185,  0.3617],\n",
            "         [-0.2530,  0.2928, -0.1314]]], device='cuda:0') tensor([-0.7295,  0.9032,  0.2480,  0.4675,  0.4601,  0.4834], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.7119140625\n",
            "tcost icost -0.0019207000732421875 0.0\n",
            "tcost icost -0.394775390625 0.0\n",
            "tcost icost -0.0538330078125 0.0\n",
            "tcost icost -0.186279296875 0.0\n",
            "tcost icost -0.027008056640625 0.0\n",
            "tcost icost -0.0240020751953125 0.0\n",
            "loss tensor([[-0.5684]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9216,  0.3042,  0.2411],\n",
            "         [-0.0620, -0.3541, -0.9331],\n",
            "         [ 0.6115,  0.4465,  0.3386],\n",
            "         [ 0.1391, -0.2914, -0.6805],\n",
            "         [-0.2526,  0.2276,  0.3561],\n",
            "         [-0.1890,  0.3217, -0.1323]]], device='cuda:0') tensor([-0.2964,  0.7950,  0.3720,  0.3180,  0.5186,  0.4525], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.568359375\n",
            "tcost icost 0.012054443359375 0.0\n",
            "tcost icost -0.2374267578125 0.0\n",
            "tcost icost -0.03973388671875 0.0\n",
            "tcost icost -0.07122802734375 0.0\n",
            "tcost icost -0.01263427734375 0.0\n",
            "tcost icost -0.01216888427734375 0.0\n",
            "loss tensor([[-0.3010]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9288,  0.2877,  0.2336],\n",
            "         [-0.0488, -0.3482, -0.9362],\n",
            "         [ 0.7053,  0.4474,  0.3420],\n",
            "         [ 0.2232, -0.2409, -0.6682],\n",
            "         [-0.2224,  0.2271,  0.3397],\n",
            "         [-0.1233,  0.3513, -0.1315]]], device='cuda:0') tensor([0.3154, 0.5933, 0.5797, 0.1122, 0.5742, 0.4409], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.301025390625\n",
            "tcost icost 0.043182373046875 0.0\n",
            "tcost icost 0.1390380859375 0.0\n",
            "tcost icost 0.11383056640625 0.0\n",
            "tcost icost -0.0144195556640625 0.0\n",
            "tcost icost -0.045013427734375 0.0\n",
            "tcost icost 0.01190185546875 0.0\n",
            "loss tensor([[0.2275]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9372,  0.2692,  0.2217],\n",
            "         [-0.0161, -0.3271, -0.9448],\n",
            "         [ 0.7971,  0.4403,  0.3371],\n",
            "         [ 0.2891, -0.2078, -0.6728],\n",
            "         [-0.1712,  0.2550,  0.3573],\n",
            "         [-0.0718,  0.3579, -0.1493]]], device='cuda:0') tensor([0.1802, 0.2844, 0.7607, 0.2935, 0.3527, 0.5375], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2275390625\n",
            "tcost icost 0.0543212890625 0.0\n",
            "tcost icost 0.166259765625 0.0\n",
            "tcost icost 0.1385498046875 0.0\n",
            "tcost icost -0.01708984375 0.0\n",
            "tcost icost -0.0186767578125 0.0\n",
            "tcost icost 0.0458984375 0.0\n",
            "loss tensor([[0.3186]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9390,  0.2635,  0.2212],\n",
            "         [ 0.0109, -0.3141, -0.9493],\n",
            "         [ 0.8423,  0.4286,  0.3269],\n",
            "         [ 0.3482, -0.1784, -0.6902],\n",
            "         [-0.1089,  0.2962,  0.3886],\n",
            "         [-0.0356,  0.3355, -0.1907]]], device='cuda:0') tensor([-0.4297,  0.6702,  0.3942,  0.3482,  0.1847,  0.7184], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.318603515625\n",
            "tcost icost 0.006305694580078125 0.0\n",
            "tcost icost -0.25732421875 0.0\n",
            "tcost icost -0.041351318359375 0.0\n",
            "tcost icost -0.11700439453125 0.0\n",
            "tcost icost -0.0268096923828125 0.0\n",
            "tcost icost -0.09051513671875 0.0\n",
            "loss tensor([[-0.4150]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9467,  0.2447,  0.2094],\n",
            "         [ 0.0459, -0.2968, -0.9538],\n",
            "         [ 0.8657,  0.3969,  0.3051],\n",
            "         [ 0.4161, -0.1405, -0.6830],\n",
            "         [-0.0497,  0.3238,  0.4046],\n",
            "         [ 0.0171,  0.3340, -0.2115]]], device='cuda:0') tensor([0.1577, 0.4840, 0.6102, 0.2110, 0.2964, 0.6164], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4150390625\n",
            "tcost icost 0.05450439453125 0.0\n",
            "tcost icost 0.055908203125 0.0\n",
            "tcost icost 0.04583740234375 0.0\n",
            "tcost icost -0.006565093994140625 0.0\n",
            "tcost icost 0.00516510009765625 0.0\n",
            "tcost icost 0.1395263671875 0.0\n",
            "loss tensor([[0.2230]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9634,  0.2041,  0.1741],\n",
            "         [ 0.0956, -0.2619, -0.9603],\n",
            "         [ 0.8948,  0.3567,  0.2685],\n",
            "         [ 0.4700, -0.1007, -0.6967],\n",
            "         [ 0.0180,  0.3495,  0.4056],\n",
            "         [ 0.0752,  0.3107, -0.2539]]], device='cuda:0') tensor([ 1.0000, -0.2792,  0.7777,  0.1510,  0.1831,  0.7265], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2230224609375\n",
            "tcost icost -0.1304931640625 0.0\n",
            "tcost icost -0.045196533203125 0.0\n",
            "tcost icost -0.315185546875 0.0\n",
            "tcost icost -0.04803466796875 0.0\n",
            "tcost icost -0.1142578125 0.0\n",
            "tcost icost -0.001964569091796875 0.0\n",
            "loss tensor([[-0.5371]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9737,  0.1728,  0.1485],\n",
            "         [ 0.1370, -0.2337, -0.9626],\n",
            "         [ 0.9140,  0.3244,  0.2436],\n",
            "         [ 0.5179, -0.0698, -0.7111],\n",
            "         [ 0.0882,  0.3794,  0.4144],\n",
            "         [ 0.1344,  0.2919, -0.2925]]], device='cuda:0') tensor([ 0.7799, -0.0604,  0.7077,  0.2517,  0.0753,  0.6848], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.537109375\n",
            "tcost icost -0.087158203125 0.0\n",
            "tcost icost -0.021453857421875 0.0\n",
            "tcost icost -0.261962890625 0.0\n",
            "tcost icost -0.04254150390625 0.0\n",
            "tcost icost -0.0582275390625 0.0\n",
            "tcost icost 0.0201263427734375 0.0\n",
            "loss tensor([[-0.3757]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9787,  0.1543,  0.1355],\n",
            "         [ 0.1664, -0.2139, -0.9626],\n",
            "         [ 0.9246,  0.3030,  0.2307],\n",
            "         [ 0.5595, -0.0464, -0.7288],\n",
            "         [ 0.1625,  0.4139,  0.4302],\n",
            "         [ 0.1936,  0.2683, -0.3367]]], device='cuda:0') tensor([ 0.3683,  0.2679,  0.5638,  0.3560, -0.0650,  0.7175], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.375732421875\n",
            "tcost icost 0.029541015625 0.0\n",
            "tcost icost 0.09906005859375 0.0\n",
            "tcost icost 0.1318359375 0.0\n",
            "tcost icost -0.0139312744140625 0.0\n",
            "tcost icost -0.00691986083984375 0.0\n",
            "tcost icost 0.096435546875 0.0\n",
            "loss tensor([[0.2678]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9772,  0.1584,  0.1410],\n",
            "         [ 0.1606, -0.2131, -0.9638],\n",
            "         [ 0.9327,  0.2877,  0.2173],\n",
            "         [ 0.5841, -0.0233, -0.7570],\n",
            "         [ 0.2409,  0.4202,  0.4043],\n",
            "         [ 0.2592,  0.2686, -0.3729]]], device='cuda:0') tensor([-0.9100,  1.0000,  0.1004,  0.3046,  0.2763,  0.4552], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.267822265625\n",
            "tcost icost -0.00576019287109375 0.0\n",
            "tcost icost -0.45947265625 0.0\n",
            "tcost icost -0.056915283203125 0.0\n",
            "tcost icost -0.228271484375 0.0\n",
            "tcost icost -0.033172607421875 0.0\n",
            "tcost icost -0.0673828125 0.0\n",
            "loss tensor([[-0.6934]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9769,  0.1581,  0.1436],\n",
            "         [ 0.1622, -0.2107, -0.9640],\n",
            "         [ 0.9412,  0.2683,  0.2051],\n",
            "         [ 0.6223,  0.0075, -0.7634],\n",
            "         [ 0.3168,  0.4213,  0.3763],\n",
            "         [ 0.3301,  0.2803, -0.3939]]], device='cuda:0') tensor([-0.6141,  0.9172,  0.2194,  0.1882,  0.3865,  0.3525], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.693359375\n",
            "tcost icost 0.0009703636169433594 0.0\n",
            "tcost icost -0.3837890625 0.0\n",
            "tcost icost -0.056793212890625 0.0\n",
            "tcost icost -0.141845703125 0.0\n",
            "tcost icost -0.02227783203125 0.0\n",
            "tcost icost -0.021484375 0.0\n",
            "loss tensor([[-0.5210]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9787,  0.1503,  0.1400],\n",
            "         [ 0.1705, -0.2057, -0.9637],\n",
            "         [ 0.9505,  0.2446,  0.1914],\n",
            "         [ 0.6625,  0.0456, -0.7477],\n",
            "         [ 0.3893,  0.4201,  0.3474],\n",
            "         [ 0.3995,  0.2929, -0.4120]]], device='cuda:0') tensor([-0.1595,  0.8116,  0.3433,  0.0244,  0.4297,  0.3422], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.52099609375\n",
            "tcost icost 0.0214385986328125 0.0\n",
            "tcost icost -0.212646484375 0.0\n",
            "tcost icost -0.041534423828125 0.0\n",
            "tcost icost -0.0208740234375 0.0\n",
            "tcost icost -0.0005035400390625 0.0\n",
            "tcost icost 0.004974365234375 0.0\n",
            "loss tensor([[-0.2163]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9827,  0.1336,  0.1279],\n",
            "         [ 0.1858, -0.1962, -0.9628],\n",
            "         [ 0.9606,  0.2170,  0.1735],\n",
            "         [ 0.6931,  0.0860, -0.7157],\n",
            "         [ 0.4610,  0.4165,  0.3158],\n",
            "         [ 0.4637,  0.2996, -0.4346]]], device='cuda:0') tensor([ 0.4025,  0.5502,  0.4239, -0.1139,  0.4709,  0.3979], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.21630859375\n",
            "tcost icost 0.0199432373046875 0.0\n",
            "tcost icost 0.1199951171875 0.0\n",
            "tcost icost 0.062286376953125 0.0\n",
            "tcost icost -0.017059326171875 0.0\n",
            "tcost icost -0.0933837890625 0.0\n",
            "tcost icost 0.007755279541015625 0.0\n",
            "loss tensor([[0.1092]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9856,  0.1215,  0.1176],\n",
            "         [ 0.2135, -0.1786, -0.9605],\n",
            "         [ 0.9722,  0.1803,  0.1497],\n",
            "         [ 0.7143,  0.1134, -0.6906],\n",
            "         [ 0.5305,  0.4213,  0.3011],\n",
            "         [ 0.5192,  0.3005, -0.4602]]], device='cuda:0') tensor([0.0614, 0.2809, 0.7479, 0.0426, 0.2449, 0.4623], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.10919189453125\n",
            "tcost icost 0.0400390625 0.0\n",
            "tcost icost 0.069091796875 0.0\n",
            "tcost icost 0.1224365234375 0.0\n",
            "tcost icost -0.01558685302734375 0.0\n",
            "tcost icost -0.013031005859375 0.0\n",
            "tcost icost 0.0799560546875 0.0\n",
            "loss tensor([[0.2288]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9912,  0.0949,  0.0923],\n",
            "         [ 0.2534, -0.1502, -0.9556],\n",
            "         [ 0.9807,  0.1489,  0.1271],\n",
            "         [ 0.7221,  0.1278, -0.6799],\n",
            "         [ 0.6007,  0.4315,  0.2969],\n",
            "         [ 0.5580,  0.2877, -0.4971]]], device='cuda:0') tensor([ 0.9989, -0.2067,  0.6947,  0.1755,  0.0220,  0.5929], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.228759765625\n",
            "tcost icost -0.1302490234375 0.0\n",
            "tcost icost -0.042236328125 0.0\n",
            "tcost icost -0.293701171875 0.0\n",
            "tcost icost -0.04608154296875 0.0\n",
            "tcost icost -0.074462890625 0.0\n",
            "tcost icost -0.0015621185302734375 0.0\n",
            "loss tensor([[-0.4897]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9942,  0.0762,  0.0755],\n",
            "         [ 0.2866, -0.1275, -0.9495],\n",
            "         [ 0.9845,  0.1301,  0.1175],\n",
            "         [ 0.7280,  0.1359, -0.6720],\n",
            "         [ 0.6694,  0.4437,  0.2975],\n",
            "         [ 0.5904,  0.2733, -0.5353]]], device='cuda:0') tensor([ 0.7631,  0.0317,  0.5987,  0.2784, -0.1130,  0.6002], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.48974609375\n",
            "search tensor([[[ 0.6694,  0.4437,  0.2975],\n",
            "         [ 0.5904,  0.2733, -0.5353],\n",
            "         [ 0.2530,  0.1561, -0.3194],\n",
            "         [-0.0215,  0.1444,  0.3374],\n",
            "         [ 0.3640, -0.4189,  0.3561],\n",
            "         [-0.4259, -0.4932,  0.2928]]], device='cuda:0') tensor([[[-0.1130],\n",
            "         [ 0.6002],\n",
            "         [ 0.4299],\n",
            "         [ 0.1074],\n",
            "         [-0.5357],\n",
            "         [-0.3555]]], device='cuda:0')\n",
            "tcost icost 0.01435089111328125 0.0\n",
            "tcost icost -0.0931396484375 0.0\n",
            "tcost icost -0.010711669921875 0.0\n",
            "tcost icost -0.0003407001495361328 0.0\n",
            "tcost icost -0.017913818359375 0.0\n",
            "tcost icost -0.033538818359375 0.0\n",
            "loss tensor([[-0.1100]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.7687,  0.3432,  0.1972],\n",
            "         [ 0.6898,  0.3730, -0.4348],\n",
            "         [ 0.1527,  0.0560, -0.4191],\n",
            "         [ 0.0785,  0.2443,  0.4371],\n",
            "         [ 0.2636, -0.5184,  0.2558],\n",
            "         [-0.3255, -0.3927,  0.3925]]], device='cuda:0') tensor([ 0.5789,  0.1994,  0.4964,  0.0727, -0.4779, -0.3858], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1099853515625\n",
            "tcost icost -0.0215301513671875 0.0\n",
            "tcost icost -0.0147857666015625 0.0\n",
            "tcost icost -0.168212890625 0.0\n",
            "tcost icost -0.026947021484375 0.0\n",
            "tcost icost -0.01459503173828125 0.0\n",
            "tcost icost 0.007678985595703125 0.0\n",
            "loss tensor([[-0.1958]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.8224,  0.3265,  0.1758],\n",
            "         [ 0.7147,  0.3733, -0.4461],\n",
            "         [ 0.2214,  0.1074, -0.3892],\n",
            "         [ 0.1719,  0.2500,  0.4204],\n",
            "         [ 0.3146, -0.5615,  0.1745],\n",
            "         [-0.2951, -0.4192,  0.3738]]], device='cuda:0') tensor([ 0.1629,  0.5439,  0.2701,  0.1025, -0.4907, -0.3539], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.19580078125\n",
            "tcost icost 0.01294708251953125 0.0\n",
            "tcost icost 0.06854248046875 0.0\n",
            "tcost icost 0.03668212890625 0.0\n",
            "tcost icost -0.0021953582763671875 0.0\n",
            "tcost icost -0.00966644287109375 0.0\n",
            "tcost icost -0.0228729248046875 0.0\n",
            "loss tensor([[0.0829]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.8926,  0.2694,  0.1187],\n",
            "         [ 0.7745,  0.4179, -0.4060],\n",
            "         [ 0.2443,  0.0922, -0.4343],\n",
            "         [ 0.2630,  0.2783,  0.4290],\n",
            "         [ 0.3161, -0.6283,  0.0866],\n",
            "         [-0.2322, -0.3955,  0.3930]]], device='cuda:0') tensor([ 0.8636, -0.0973,  0.5294,  0.0746, -0.4286, -0.3898], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.08294677734375\n",
            "tcost icost -0.08184814453125 0.0\n",
            "tcost icost -0.04608154296875 0.0\n",
            "tcost icost -0.273681640625 0.0\n",
            "tcost icost -0.0357666015625 0.0\n",
            "tcost icost -0.0360107421875 0.0\n",
            "tcost icost -0.0036449432373046875 0.0\n",
            "loss tensor([[-0.3970]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9500,  0.2372,  0.0867],\n",
            "         [ 0.8149,  0.4386, -0.3790],\n",
            "         [ 0.2929,  0.1051, -0.4426],\n",
            "         [ 0.3562,  0.2627,  0.4018],\n",
            "         [ 0.3637, -0.6446,  0.0367],\n",
            "         [-0.1889, -0.3999,  0.3824]]], device='cuda:0') tensor([ 0.5960,  0.0915,  0.3811,  0.1308, -0.4815, -0.3760], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.39697265625\n",
            "tcost icost -0.0244293212890625 0.0\n",
            "tcost icost -0.0168914794921875 0.0\n",
            "tcost icost -0.1630859375 0.0\n",
            "tcost icost -0.02520751953125 0.0\n",
            "tcost icost -0.0173492431640625 0.0\n",
            "tcost icost 0.005035400390625 0.0\n",
            "loss tensor([[-0.1986]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 9.7095e-01,  2.2659e-01,  7.6847e-02],\n",
            "         [ 8.2337e-01,  4.2532e-01, -3.7571e-01],\n",
            "         [ 3.5708e-01,  1.4038e-01, -4.2849e-01],\n",
            "         [ 4.5070e-01,  2.2505e-01,  3.5251e-01],\n",
            "         [ 4.2614e-01, -6.3897e-01, -6.9321e-04],\n",
            "         [-1.6778e-01, -4.3149e-01,  3.4561e-01]]], device='cuda:0') tensor([ 0.1211,  0.4530,  0.1675,  0.1773, -0.5161, -0.3471], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1986083984375\n",
            "tcost icost 0.014739990234375 0.0\n",
            "tcost icost 0.0660400390625 0.0\n",
            "tcost icost 0.00809478759765625 0.0\n",
            "tcost icost 0.01242828369140625 0.0\n",
            "tcost icost -0.0133514404296875 0.0\n",
            "tcost icost -0.03704833984375 0.0\n",
            "loss tensor([[0.0592]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9811,  0.1885,  0.0443],\n",
            "         [ 0.8361,  0.4293, -0.3415],\n",
            "         [ 0.3949,  0.1477, -0.4451],\n",
            "         [ 0.5453,  0.2172,  0.3347],\n",
            "         [ 0.4579, -0.6584, -0.0577],\n",
            "         [-0.1200, -0.4260,  0.3364]]], device='cuda:0') tensor([ 0.7794, -0.1085,  0.4021,  0.1293, -0.4440, -0.3946], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.059173583984375\n",
            "tcost icost -0.06329345703125 0.0\n",
            "tcost icost -0.0372314453125 0.0\n",
            "tcost icost -0.2451171875 0.0\n",
            "tcost icost -0.032073974609375 0.0\n",
            "tcost icost -0.034027099609375 0.0\n",
            "tcost icost -0.00015723705291748047 0.0\n",
            "loss tensor([[-0.3411]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9854,  0.1680,  0.0282],\n",
            "         [ 0.8467,  0.4223, -0.3237],\n",
            "         [ 0.4473,  0.1705, -0.4409],\n",
            "         [ 0.6403,  0.1891,  0.3012],\n",
            "         [ 0.5077, -0.6521, -0.0878],\n",
            "         [-0.0790, -0.4311,  0.3159]]], device='cuda:0') tensor([ 0.4236,  0.1532,  0.2429,  0.1962, -0.5025, -0.3849], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.341064453125\n",
            "tcost icost -0.006504058837890625 0.0\n",
            "tcost icost -0.001743316650390625 0.0\n",
            "tcost icost -0.02886962890625 0.0\n",
            "tcost icost -0.01160430908203125 0.0\n",
            "tcost icost -0.007335662841796875 0.0\n",
            "tcost icost 0.00603485107421875 0.0\n",
            "loss tensor([[-0.0412]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9846,  0.1715,  0.0341],\n",
            "         [ 0.8511,  0.3953, -0.3456],\n",
            "         [ 0.5097,  0.2091, -0.4269],\n",
            "         [ 0.7325,  0.1606,  0.2612],\n",
            "         [ 0.5525, -0.6482, -0.1254],\n",
            "         [-0.0447, -0.4459,  0.2839]]], device='cuda:0') tensor([-0.3518,  0.8148, -0.0481,  0.1906, -0.4871, -0.3669], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.041168212890625\n",
            "tcost icost 0.007511138916015625 0.0\n",
            "tcost icost -0.2529296875 0.0\n",
            "tcost icost -0.0562744140625 0.0\n",
            "tcost icost -0.125 0.0\n",
            "tcost icost -0.0379638671875 0.0\n",
            "tcost icost -0.05157470703125 0.0\n",
            "loss tensor([[-0.4121]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9864,  0.1618,  0.0295],\n",
            "         [ 0.8587,  0.3767, -0.3476],\n",
            "         [ 0.5661,  0.2366, -0.4171],\n",
            "         [ 0.8264,  0.1781,  0.2740],\n",
            "         [ 0.5870, -0.6544, -0.1670],\n",
            "         [ 0.0080, -0.4309,  0.2806]]], device='cuda:0') tensor([ 0.1907,  0.6206,  0.1055,  0.0496, -0.4311, -0.4126], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.412109375\n",
            "tcost icost 0.0116729736328125 0.0\n",
            "tcost icost 0.05364990234375 0.0\n",
            "tcost icost -0.0081787109375 0.0\n",
            "tcost icost -0.003681182861328125 0.0\n",
            "tcost icost -0.0079193115234375 0.0\n",
            "tcost icost -0.0257415771484375 0.0\n",
            "loss tensor([[0.0302]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9901,  0.1399,  0.0126],\n",
            "         [ 0.8676,  0.3672, -0.3353],\n",
            "         [ 0.6126,  0.2534, -0.4174],\n",
            "         [ 0.9103,  0.2041,  0.2935],\n",
            "         [ 0.6016, -0.6749, -0.2209],\n",
            "         [ 0.0707, -0.4003,  0.2896]]], device='cuda:0') tensor([ 0.6592,  0.2578,  0.2070, -0.0203, -0.3653, -0.4532], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.030242919921875\n",
            "tcost icost -0.037109375 0.0\n",
            "tcost icost -0.01459503173828125 0.0\n",
            "tcost icost -0.0814208984375 0.0\n",
            "tcost icost -0.020751953125 0.0\n",
            "tcost icost -0.0275115966796875 0.0\n",
            "tcost icost 0.0023956298828125 0.0\n",
            "loss tensor([[-0.1479]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9908,  0.1349,  0.0116],\n",
            "         [ 0.8736,  0.3475, -0.3406],\n",
            "         [ 0.6681,  0.2830, -0.4005],\n",
            "         [ 0.9373,  0.2029,  0.2832],\n",
            "         [ 0.6344, -0.6735, -0.2531],\n",
            "         [ 0.1233, -0.3849,  0.2845]]], device='cuda:0') tensor([ 0.0816,  0.6739, -0.0342,  0.0496, -0.4295, -0.4322], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.14794921875\n",
            "tcost icost 0.015960693359375 0.0\n",
            "tcost icost -0.010223388671875 0.0\n",
            "tcost icost -0.02203369140625 0.0\n",
            "tcost icost -0.025421142578125 0.0\n",
            "tcost icost -0.01535797119140625 0.0\n",
            "tcost icost -0.03460693359375 0.0\n",
            "loss tensor([[-0.0601]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9931,  0.1175, -0.0016],\n",
            "         [ 0.8815,  0.3375, -0.3304],\n",
            "         [ 0.7159,  0.3027, -0.3915],\n",
            "         [ 0.9359,  0.2104,  0.2826],\n",
            "         [ 0.6514, -0.6841, -0.2954],\n",
            "         [ 0.1853, -0.3531,  0.2931]]], device='cuda:0') tensor([ 0.6426,  0.3184,  0.0919, -0.0543, -0.3626, -0.4749], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.06011962890625\n",
            "tcost icost -0.0333251953125 0.0\n",
            "tcost icost -0.01080322265625 0.0\n",
            "tcost icost -0.0136871337890625 0.0\n",
            "tcost icost -0.013275146484375 0.0\n",
            "tcost icost -0.0216217041015625 0.0\n",
            "tcost icost 0.0040130615234375 0.0\n",
            "loss tensor([[-0.0756]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9928,  0.1195,  0.0036],\n",
            "         [ 0.8852,  0.3132, -0.3439],\n",
            "         [ 0.7716,  0.3336, -0.3684],\n",
            "         [ 0.9407,  0.2046,  0.2705],\n",
            "         [ 0.6738, -0.6677, -0.3163],\n",
            "         [ 0.2399, -0.3344,  0.2892]]], device='cuda:0') tensor([-0.1914,  0.9424, -0.1203,  0.0227, -0.4265, -0.4552], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0755615234375\n",
            "tcost icost 0.0119171142578125 0.0\n",
            "tcost icost -0.2186279296875 0.0\n",
            "tcost icost -0.0582275390625 0.0\n",
            "tcost icost -0.065673828125 0.0\n",
            "tcost icost -0.0261688232421875 0.0\n",
            "tcost icost -0.04071044921875 0.0\n",
            "loss tensor([[-0.3208]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 9.9388e-01,  1.1042e-01, -5.0587e-04],\n",
            "         [ 8.9180e-01,  2.9706e-01, -3.4124e-01],\n",
            "         [ 8.2349e-01,  3.5654e-01, -3.4897e-01],\n",
            "         [ 9.3369e-01,  2.1970e-01,  2.8276e-01],\n",
            "         [ 6.7831e-01, -6.5232e-01, -3.3818e-01],\n",
            "         [ 3.0156e-01, -3.0243e-01,  2.9733e-01]]], device='cuda:0') tensor([ 0.4366,  0.7482,  0.0173, -0.1684, -0.3715, -0.4852], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.32080078125\n",
            "tcost icost -0.00780487060546875 0.0\n",
            "tcost icost 0.06268310546875 0.0\n",
            "tcost icost -0.003963470458984375 0.0\n",
            "tcost icost -0.00848388671875 0.0\n",
            "tcost icost 0.0009870529174804688 0.0\n",
            "tcost icost 0.01837158203125 0.0\n",
            "loss tensor([[0.0507]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9946,  0.1040, -0.0024],\n",
            "         [ 0.8971,  0.2821, -0.3400],\n",
            "         [ 0.8635,  0.3709, -0.3417],\n",
            "         [ 0.9240,  0.2410,  0.2968],\n",
            "         [ 0.6709, -0.6416, -0.3719],\n",
            "         [ 0.3649, -0.2906,  0.2809]]], device='cuda:0') tensor([ 0.2425,  0.7224,  0.0614, -0.2717, -0.2932, -0.3985], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.05072021484375\n",
            "tcost icost 0.00878143310546875 0.0\n",
            "tcost icost 0.0504150390625 0.0\n",
            "tcost icost -0.0100860595703125 0.0\n",
            "tcost icost -0.00922393798828125 0.0\n",
            "tcost icost 0.0011501312255859375 0.0\n",
            "tcost icost 0.01456451416015625 0.0\n",
            "loss tensor([[0.0486]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9960,  0.0887, -0.0140],\n",
            "         [ 0.9031,  0.2751, -0.3297],\n",
            "         [ 0.8691,  0.3683, -0.3301],\n",
            "         [ 0.9133,  0.2637,  0.3105],\n",
            "         [ 0.6540, -0.6347, -0.4116],\n",
            "         [ 0.4317, -0.2821,  0.2567]]], device='cuda:0') tensor([ 0.6617,  0.3997,  0.0925, -0.3261, -0.2306, -0.3634], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.04864501953125\n",
            "tcost icost -0.037689208984375 0.0\n",
            "tcost icost -0.00861358642578125 0.0\n",
            "tcost icost 0.01654052734375 0.0\n",
            "tcost icost -0.0212860107421875 0.0\n",
            "tcost icost -0.05999755859375 0.0\n",
            "tcost icost -0.0072174072265625 0.0\n",
            "loss tensor([[-0.0912]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9957,  0.0929, -0.0065],\n",
            "         [ 0.9053,  0.2528, -0.3412],\n",
            "         [ 0.8765,  0.3749, -0.3020],\n",
            "         [ 0.9116,  0.2675,  0.3121],\n",
            "         [ 0.6736, -0.6071, -0.4214],\n",
            "         [ 0.4916, -0.2835,  0.2261]]], device='cuda:0') tensor([-0.2397,  1.0000, -0.0681, -0.2074, -0.3853, -0.3395], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0911865234375\n",
            "tcost icost 0.0106048583984375 0.0\n",
            "tcost icost -0.250732421875 0.0\n",
            "tcost icost -0.057861328125 0.0\n",
            "tcost icost -0.031341552734375 0.0\n",
            "tcost icost -0.0191192626953125 0.0\n",
            "tcost icost -0.050079345703125 0.0\n",
            "loss tensor([[-0.3269]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9960,  0.0885, -0.0073],\n",
            "         [ 0.9099,  0.2379, -0.3398],\n",
            "         [ 0.8839,  0.3767, -0.2773],\n",
            "         [ 0.9017,  0.2814,  0.3281],\n",
            "         [ 0.6785, -0.5907, -0.4366],\n",
            "         [ 0.5589, -0.2618,  0.2162]]], device='cuda:0') tensor([ 0.2584,  0.7888,  0.0442, -0.3081, -0.2950, -0.4111], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.326904296875\n",
            "tcost icost 0.007762908935546875 0.0\n",
            "tcost icost 0.0408935546875 0.0\n",
            "tcost icost -0.01312255859375 0.0\n",
            "tcost icost -0.01025390625 0.0\n",
            "tcost icost 0.0006594657897949219 0.0\n",
            "tcost icost 0.01320648193359375 0.0\n",
            "loss tensor([[0.0347]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9970,  0.0761, -0.0172],\n",
            "         [ 0.9153,  0.2306, -0.3302],\n",
            "         [ 0.8895,  0.3755, -0.2604],\n",
            "         [ 0.8914,  0.2963,  0.3430],\n",
            "         [ 0.6751, -0.5782, -0.4582],\n",
            "         [ 0.6281, -0.2406,  0.2011]]], device='cuda:0') tensor([ 0.6666,  0.4746,  0.0753, -0.3496, -0.2363, -0.3850], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.03466796875\n",
            "tcost icost -0.03875732421875 0.0\n",
            "tcost icost -0.00574493408203125 0.0\n",
            "tcost icost 0.047454833984375 0.0\n",
            "tcost icost -0.0179290771484375 0.0\n",
            "tcost icost -0.052978515625 0.0\n",
            "tcost icost -0.00569915771484375 0.0\n",
            "loss tensor([[-0.0566]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9968,  0.0793, -0.0133],\n",
            "         [ 0.9176,  0.2130, -0.3355],\n",
            "         [ 0.8950,  0.3743, -0.2427],\n",
            "         [ 0.8909,  0.2953,  0.3452],\n",
            "         [ 0.6950, -0.5511, -0.4617],\n",
            "         [ 0.6920, -0.2267,  0.1801]]], device='cuda:0') tensor([-0.2167,  0.8938,  0.1643, -0.2472, -0.3555, -0.3623], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.056640625\n",
            "ded\n",
            "time\n",
            "[11, 11, 6, 11, 14, 11, 14, 11, 14, 0, 9, 14, 14, 11, 14, 12, 14, 14, 7, 12, 14, 14, 2, 0, 14, 0, 11, 11, 14, 11, 14, 14, 14, 0, 10, 11, 11, 14, 14, 14, 12, 11, 11, 14, 0, 14, 6, 6, 0, 14, 14, 10, 14, 14, 14, 14, 14, 14, 11, 14, 14, 11, 14, 0, 14, 11, 0, 11, 14, 14, 14, 14, 14, 14, 14, 14, 11, 11, 14, 10, 14, 14, 10, 14, 2, 14, 0, 11, 14, 14, 14, 14, 11, 0, 14, 11, 14, 0, 9, 14, 14, 14, 14, 0, 0, 0, 4, 14, 0, 14, 2, 12, 0, 14, 14, 0, 14, 14, 14, 11, 11, 14, 14, 0, 11, 14, 11, 14, 11, 14, 14, 0, 14, 11, 0, 14, 10, 14, 14, 14, 11, 14, 11, 14, 14, 11, 12, 0, 12, 0, 14, 0, 14, 14, 10, 14, 14, 14, 7, 6, 10, 14, 14, 11, 14, 10, 12, 11, 14, 11, 11, 14, 14, 10, 11, 11, 14, 14, 14, 11, 11, 14, 6, 0, 14, 14, 14, 14, 14, 14, 11, 14, 12, 10, 6, 11, 11, 11, 10, 14, 11, 14, 0, 14, 0, 14, 14, 0, 14, 11, 14, 0, 14, 14, 14, 12, 12, 0, 4, 0, 12, 10, 14, 14, 14, 14, 14, 4, 14, 14, 14, 14, 14, 0, 14, 11, 14, 11, 6, 14, 14, 11, 14, 14, 14, 11, 11, 14, 10, 14, 10, 14, 11, 14, 14, 11, 11, 10, 4, 0, 6, 14, 11, 14, 14, 14, 14, 11, 14, 11, 14, 10, 0, 11, 11, 14, 14, 0, 14, 14, 6, 11, 14, 14, 11, 0, 14, 10, 0, 14, 14, 14, 11, 14, 0, 11, 10, 12, 0, 11, 11, 14, 14, 0, 0, 14, 12, 14, 11, 14, 14, 10, 0, 11, 11, 14, 10, 2, 4, 14, 14, 11, 14, 6, 10, 14, 11, 14, 11, 14, 14, 11, 14, 14, 9, 0, 12, 10, 14, 14, 14, 14, 12, 4, 14, 14, 0, 11, 14, 11, 0, 11, 14, 14, 14, 12, 14, 0, 14, 14, 12, 14, 11, 0, 14, 0, 14, 0, 14, 11, 14, 10, 12, 10, 14, 14, 0, 10, 14, 12, 12, 14, 14, 14, 14, 10, 14, 9, 0, 14, 14, 14, 11, 0, 14, 12, 10, 14, 14, 11, 14, 11, 11, 14, 11, 14, 14, 11, 14, 10, 9, 0, 14, 0, 14, 0, 14, 11, 12, 14, 10, 12, 10, 14, 0, 11, 11, 12, 14, 11, 14, 14, 12, 10, 12, 14, 14, 11, 14, 0, 14, 11, 14, 0, 11, 14, 14, 11, 14, 11, 14, 11, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 #### train ####\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-2e41fcea5594>:174: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "search tensor([[[ 0.9995,  0.0281,  0.0145],\n",
            "         [ 0.3488,  0.5092,  0.7868],\n",
            "         [-0.5181,  0.2279, -0.1115],\n",
            "         [-0.1667, -0.5219, -0.1042],\n",
            "         [-0.2860,  0.3147, -0.0772],\n",
            "         [-0.3771,  0.3000,  0.1789]]], device='cuda:0') tensor([[[-0.6281],\n",
            "         [-1.0000],\n",
            "         [-0.2177],\n",
            "         [ 0.3713],\n",
            "         [ 0.6333],\n",
            "         [-0.7040]]], device='cuda:0')\n",
            "tcost icost -0.01543426513671875 0.0\n",
            "tcost icost -0.0132598876953125 0.0\n",
            "tcost icost 0.11297607421875 0.0\n",
            "tcost icost -0.0557861328125 0.0\n",
            "tcost icost -0.0240478515625 0.0\n",
            "tcost icost 0.037628173828125 0.0\n",
            "loss tensor([[0.0298]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.8985, -0.0720, -0.0855],\n",
            "         [ 0.2252,  0.5517,  0.8031],\n",
            "         [-0.6175,  0.1277, -0.2113],\n",
            "         [-0.0666, -0.4213, -0.0041],\n",
            "         [-0.3857,  0.2144, -0.1772],\n",
            "         [-0.2767,  0.3997,  0.2787]]], device='cuda:0') tensor([ 0.8123, -1.0000,  0.3922,  0.0916,  0.7026, -0.7170], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.02984619140625\n",
            "tcost icost -0.043548583984375 0.0\n",
            "tcost icost -0.0189056396484375 0.0\n",
            "tcost icost -0.311767578125 0.0\n",
            "tcost icost -0.03985595703125 0.0\n",
            "tcost icost -0.380859375 0.0\n",
            "tcost icost -0.0623779296875 0.0\n",
            "loss tensor([[-0.6284]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 8.6005e-01, -1.2644e-01, -1.4195e-01],\n",
            "         [ 1.9520e-01,  5.7432e-01,  7.9502e-01],\n",
            "         [-6.5658e-01,  7.0561e-02, -2.7270e-01],\n",
            "         [-8.5958e-04, -3.6214e-01,  5.6745e-02],\n",
            "         [-3.2363e-01,  2.4403e-01, -1.7436e-01],\n",
            "         [-2.1688e-01,  4.5032e-01,  3.1358e-01]]], device='cuda:0') tensor([ 0.4683, -0.9383,  0.2911,  0.1302,  0.6213, -0.7124], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.62841796875\n",
            "tcost icost -0.0010766983032226562 0.0\n",
            "tcost icost -0.00341796875 0.0\n",
            "tcost icost -0.291259765625 0.0\n",
            "tcost icost -0.0321044921875 0.0\n",
            "tcost icost -0.33642578125 0.0\n",
            "tcost icost -0.0574951171875 0.0\n",
            "loss tensor([[-0.5181]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.8383, -0.1604, -0.1756],\n",
            "         [ 0.2181,  0.5956,  0.7731],\n",
            "         [-0.6702,  0.0384, -0.3139],\n",
            "         [ 0.0471, -0.3324,  0.0842],\n",
            "         [-0.2446,  0.3027, -0.1363],\n",
            "         [-0.1772,  0.4764,  0.3098]]], device='cuda:0') tensor([ 0.2398, -0.7858,  0.1564,  0.2038,  0.5429, -0.7048], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.51806640625\n",
            "tcost icost -0.0008373260498046875 0.0\n",
            "tcost icost 0.009490966796875 0.0\n",
            "tcost icost -0.22265625 0.0\n",
            "tcost icost -0.0168609619140625 0.0\n",
            "tcost icost -0.2291259765625 0.0\n",
            "tcost icost -0.04693603515625 0.0\n",
            "loss tensor([[-0.3628]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.8557, -0.1678, -0.1810],\n",
            "         [ 0.2764,  0.6108,  0.7281],\n",
            "         [-0.6751,  0.0286, -0.3379],\n",
            "         [ 0.0803, -0.3335,  0.0773],\n",
            "         [-0.1584,  0.3754, -0.0768],\n",
            "         [-0.1498,  0.4855,  0.2785]]], device='cuda:0') tensor([-0.3451, -0.3844, -0.0200,  0.3234,  0.4396, -0.6964], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.36279296875\n",
            "tcost icost -0.008636474609375 0.0\n",
            "tcost icost 0.0030727386474609375 0.0\n",
            "tcost icost 0.112060546875 0.0\n",
            "tcost icost -0.08026123046875 0.0\n",
            "tcost icost -0.034088134765625 0.0\n",
            "tcost icost 0.006793975830078125 0.0\n",
            "loss tensor([[0.0080]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.8123, -0.2076, -0.2213],\n",
            "         [ 0.3073,  0.6230,  0.7193],\n",
            "         [-0.6540, -0.0105, -0.3822],\n",
            "         [ 0.1367, -0.2977,  0.0999],\n",
            "         [-0.0866,  0.4249, -0.0481],\n",
            "         [-0.0973,  0.5184,  0.2916]]], device='cuda:0') tensor([ 0.9908, -1.0000,  0.5167,  0.1267,  0.4879, -0.7164], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0080413818359375\n",
            "tcost icost -0.10009765625 0.0\n",
            "tcost icost -0.0272369384765625 0.0\n",
            "tcost icost -0.328857421875 0.0\n",
            "tcost icost -0.042816162109375 0.0\n",
            "tcost icost -0.36767578125 0.0\n",
            "tcost icost -0.06329345703125 0.0\n",
            "loss tensor([[-0.7012]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.7842, -0.2362, -0.2513],\n",
            "         [ 0.3423,  0.6255,  0.7011],\n",
            "         [-0.6299, -0.0398, -0.4179],\n",
            "         [ 0.1853, -0.2709,  0.1176],\n",
            "         [-0.0076,  0.4834, -0.0014],\n",
            "         [-0.0548,  0.5403,  0.2922]]], device='cuda:0') tensor([ 0.6478, -0.9572,  0.4293,  0.1654,  0.4010, -0.7107], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.701171875\n",
            "tcost icost -0.007762908935546875 0.0\n",
            "tcost icost -0.01129913330078125 0.0\n",
            "tcost icost -0.322509765625 0.0\n",
            "tcost icost -0.0369873046875 0.0\n",
            "tcost icost -0.34033203125 0.0\n",
            "tcost icost -0.060272216796875 0.0\n",
            "loss tensor([[-0.5649]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.7642, -0.2577, -0.2730],\n",
            "         [ 0.3867,  0.6250,  0.6781],\n",
            "         [-0.6050, -0.0600, -0.4462],\n",
            "         [ 0.2271, -0.2532,  0.1294],\n",
            "         [ 0.0760,  0.5476,  0.0575],\n",
            "         [-0.0196,  0.5538,  0.2818]]], device='cuda:0') tensor([ 0.4414, -0.8658,  0.3235,  0.2140,  0.3067, -0.7058], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.56494140625\n",
            "tcost icost -0.0010614395141601562 0.0\n",
            "tcost icost -0.0004019737243652344 0.0\n",
            "tcost icost -0.296875 0.0\n",
            "tcost icost -0.028045654296875 0.0\n",
            "tcost icost -0.28271484375 0.0\n",
            "tcost icost -0.0560302734375 0.0\n",
            "loss tensor([[-0.4810]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.7513, -0.2723, -0.2854],\n",
            "         [ 0.4440,  0.6210,  0.6459],\n",
            "         [-0.5832, -0.0704, -0.4668],\n",
            "         [ 0.2618, -0.2491,  0.1287],\n",
            "         [ 0.1630,  0.6156,  0.1250],\n",
            "         [ 0.0099,  0.5597,  0.2605]]], device='cuda:0') tensor([ 0.1938, -0.6782,  0.1855,  0.3048,  0.2010, -0.6972], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.48095703125\n",
            "tcost icost -0.00030875205993652344 0.0\n",
            "tcost icost 0.01398468017578125 0.0\n",
            "tcost icost -0.1917724609375 0.0\n",
            "tcost icost -0.0074005126953125 0.0\n",
            "tcost icost -0.1141357421875 0.0\n",
            "tcost icost -0.037567138671875 0.0\n",
            "loss tensor([[-0.2456]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.7636, -0.2684, -0.2775],\n",
            "         [ 0.5153,  0.6037,  0.5974],\n",
            "         [-0.5732, -0.0654, -0.4760],\n",
            "         [ 0.2860, -0.2668,  0.1051],\n",
            "         [ 0.2514,  0.6864,  0.1994],\n",
            "         [ 0.0344,  0.5555,  0.2211]]], device='cuda:0') tensor([-0.6724, -0.1069, -0.0524,  0.4715,  0.0644, -0.6842], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.24560546875\n",
            "tcost icost -0.016845703125 0.0\n",
            "tcost icost -0.05938720703125 0.0\n",
            "tcost icost -0.0101776123046875 0.0\n",
            "tcost icost -0.33349609375 0.0\n",
            "tcost icost -0.0653076171875 0.0\n",
            "tcost icost -0.0615234375 0.0\n",
            "loss tensor([[-0.4011]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.7559, -0.2753, -0.2807],\n",
            "         [ 0.5766,  0.5898,  0.5653],\n",
            "         [-0.5579, -0.0691, -0.4893],\n",
            "         [ 0.3211, -0.2674,  0.0974],\n",
            "         [ 0.3321,  0.7480,  0.2629],\n",
            "         [ 0.0798,  0.5791,  0.2366]]], device='cuda:0') tensor([-0.2513, -0.4663,  0.1227,  0.3904,  0.0940, -0.7378], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.401123046875\n",
            "tcost icost -0.00670623779296875 0.0\n",
            "tcost icost 0.00980377197265625 0.0\n",
            "tcost icost 0.1812744140625 0.0\n",
            "tcost icost 0.17822265625 0.0\n",
            "tcost icost -0.008697509765625 0.0\n",
            "tcost icost -0.002201080322265625 0.0\n",
            "loss tensor([[0.2720]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.7266, -0.2969, -0.3026],\n",
            "         [ 0.6151,  0.5763,  0.5380],\n",
            "         [-0.5125, -0.0897, -0.5208],\n",
            "         [ 0.3721, -0.2407,  0.1116],\n",
            "         [ 0.4084,  0.8015,  0.3112],\n",
            "         [ 0.1319,  0.6111,  0.2648]]], device='cuda:0') tensor([ 0.6057, -0.7808,  0.4582,  0.1515,  0.1356, -0.7641], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.27197265625\n",
            "tcost icost -0.00400543212890625 0.0\n",
            "tcost icost -0.004451751708984375 0.0\n",
            "tcost icost -0.327392578125 0.0\n",
            "tcost icost -0.03631591796875 0.0\n",
            "tcost icost -0.273193359375 0.0\n",
            "tcost icost -0.055938720703125 0.0\n",
            "loss tensor([[-0.5117]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.7045, -0.3129, -0.3176],\n",
            "         [ 0.6614,  0.5543,  0.5053],\n",
            "         [-0.4719, -0.1030, -0.5461],\n",
            "         [ 0.4175, -0.2210,  0.1213],\n",
            "         [ 0.4624,  0.8148,  0.3496],\n",
            "         [ 0.1780,  0.6364,  0.2853]]], device='cuda:0') tensor([ 0.3716, -0.6367,  0.3503,  0.2152,  0.0192, -0.7581], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.51171875\n",
            "tcost icost -0.0011453628540039062 0.0\n",
            "tcost icost 0.01031494140625 0.0\n",
            "tcost icost -0.2685546875 0.0\n",
            "tcost icost -0.0254669189453125 0.0\n",
            "tcost icost -0.175048828125 0.0\n",
            "tcost icost -0.046722412109375 0.0\n",
            "loss tensor([[-0.3704]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.6960, -0.3188, -0.3210],\n",
            "         [ 0.7216,  0.5176,  0.4598],\n",
            "         [-0.4358, -0.1067, -0.5645],\n",
            "         [ 0.4581, -0.2118,  0.1206],\n",
            "         [ 0.4889,  0.7883,  0.3736],\n",
            "         [ 0.2199,  0.6556,  0.2966]]], device='cuda:0') tensor([-0.1386, -0.2720,  0.2074,  0.3266, -0.1032, -0.7502], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.370361328125\n",
            "tcost icost -0.005718231201171875 0.0\n",
            "tcost icost 0.0194244384765625 0.0\n",
            "tcost icost 0.2015380859375 0.0\n",
            "tcost icost 0.2122802734375 0.0\n",
            "tcost icost -0.007053375244140625 0.0\n",
            "tcost icost -0.01082611083984375 0.0\n",
            "loss tensor([[0.3186]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.6719, -0.3337, -0.3368],\n",
            "         [ 0.7711,  0.4837,  0.4140],\n",
            "         [-0.3833, -0.1152, -0.5922],\n",
            "         [ 0.5069, -0.1928,  0.1249],\n",
            "         [ 0.5160,  0.7641,  0.3872],\n",
            "         [ 0.2672,  0.6792,  0.3190]]], device='cuda:0') tensor([ 0.3804, -0.4286,  0.3044,  0.2645, -0.0568, -0.7758], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.318603515625\n",
            "tcost icost -0.0011119842529296875 0.0\n",
            "tcost icost 0.0159454345703125 0.0\n",
            "tcost icost -0.2332763671875 0.0\n",
            "tcost icost -0.0214080810546875 0.0\n",
            "tcost icost -0.1220703125 0.0\n",
            "tcost icost -0.04205322265625 0.0\n",
            "loss tensor([[-0.2964]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.6661, -0.3360, -0.3382],\n",
            "         [ 0.8290,  0.4320,  0.3551],\n",
            "         [-0.3347, -0.1126, -0.6123],\n",
            "         [ 0.5515, -0.1852,  0.1184],\n",
            "         [ 0.5366,  0.7402,  0.4051],\n",
            "         [ 0.3106,  0.6979,  0.3312]]], device='cuda:0') tensor([-0.2861,  0.0551,  0.1447,  0.3958, -0.1907, -0.7661], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.29638671875\n",
            "tcost icost -0.007122039794921875 0.0\n",
            "tcost icost -0.0187530517578125 0.0\n",
            "tcost icost 0.03302001953125 0.0\n",
            "tcost icost -0.1669921875 0.0\n",
            "tcost icost -0.05633544921875 0.0\n",
            "tcost icost -0.0287933349609375 0.0\n",
            "loss tensor([[-0.1730]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.6360, -0.3552, -0.3572],\n",
            "         [ 0.8556,  0.4039,  0.3238],\n",
            "         [-0.2918, -0.1278, -0.6391],\n",
            "         [ 0.6010, -0.1646,  0.1252],\n",
            "         [ 0.5564,  0.7182,  0.4179],\n",
            "         [ 0.3599,  0.7180,  0.3547]]], device='cuda:0') tensor([ 0.6177, -0.5446,  0.5217,  0.2601, -0.1653, -0.7929], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1729736328125\n",
            "tcost icost -0.00496673583984375 0.0\n",
            "tcost icost 0.0017728805541992188 0.0\n",
            "tcost icost -0.32666015625 0.0\n",
            "tcost icost -0.03436279296875 0.0\n",
            "tcost icost -0.1787109375 0.0\n",
            "tcost icost -0.048370361328125 0.0\n",
            "loss tensor([[-0.4390]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.6149, -0.3678, -0.3685],\n",
            "         [ 0.8854,  0.3657,  0.2868],\n",
            "         [-0.2512, -0.1359, -0.6602],\n",
            "         [ 0.6466, -0.1510,  0.1282],\n",
            "         [ 0.5695,  0.6970,  0.4357],\n",
            "         [ 0.4058,  0.7342,  0.3713]]], device='cuda:0') tensor([ 0.2833, -0.3184,  0.4215,  0.3454, -0.3005, -0.7847], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.43896484375\n",
            "tcost icost -0.0010051727294921875 0.0\n",
            "tcost icost 0.027862548828125 0.0\n",
            "tcost icost -0.1766357421875 0.0\n",
            "tcost icost -0.0103302001953125 0.0\n",
            "tcost icost -0.00583648681640625 0.0\n",
            "tcost icost -0.02166748046875 0.0\n",
            "loss tensor([[-0.1432]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.6113, -0.3682, -0.3666],\n",
            "         [ 0.9200,  0.3124,  0.2364],\n",
            "         [-0.2062, -0.1323, -0.6755],\n",
            "         [ 0.6897, -0.1449,  0.1248],\n",
            "         [ 0.5765,  0.6784,  0.4554],\n",
            "         [ 0.4500,  0.7470,  0.3764]]], device='cuda:0') tensor([-0.4140,  0.1689,  0.1936,  0.4370, -0.3670, -0.7723], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1431884765625\n",
            "tcost icost -0.01056671142578125 0.0\n",
            "tcost icost -0.0745849609375 0.0\n",
            "tcost icost -0.005462646484375 0.0\n",
            "tcost icost -0.2392578125 0.0\n",
            "tcost icost -0.06634521484375 0.0\n",
            "tcost icost -0.042724609375 0.0\n",
            "loss tensor([[-0.3252]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.5905, -0.3804, -0.3766],\n",
            "         [ 0.9392,  0.2764,  0.2036],\n",
            "         [-0.1706, -0.1390, -0.6941],\n",
            "         [ 0.7360, -0.1280,  0.1315],\n",
            "         [ 0.5851,  0.6605,  0.4705],\n",
            "         [ 0.4982,  0.7588,  0.3922]]], device='cuda:0') tensor([ 0.2276, -0.2375,  0.4276,  0.3244, -0.3442, -0.8014], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.3251953125\n",
            "tcost icost -0.0007143020629882812 0.0\n",
            "tcost icost 0.037841796875 0.0\n",
            "tcost icost -0.1368408203125 0.0\n",
            "tcost icost -0.0005965232849121094 0.0\n",
            "tcost icost 0.0150604248046875 0.0\n",
            "tcost icost -0.0054779052734375 0.0\n",
            "loss tensor([[-0.0713]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.5860, -0.3811, -0.3744],\n",
            "         [ 0.9604,  0.2282,  0.1598],\n",
            "         [-0.1284, -0.1354, -0.7076],\n",
            "         [ 0.7775, -0.1183,  0.1322],\n",
            "         [ 0.5889,  0.6451,  0.4868],\n",
            "         [ 0.5342,  0.7512,  0.3877]]], device='cuda:0') tensor([-0.4194,  0.2114,  0.1949,  0.4066, -0.3837, -0.7866], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0712890625\n",
            "search tensor([[[ 0.5889,  0.6451,  0.4868],\n",
            "         [ 0.5342,  0.7512,  0.3877],\n",
            "         [ 0.3131,  0.4034,  0.1189],\n",
            "         [ 0.1903,  0.3624, -0.4355],\n",
            "         [-0.4130, -0.0785, -0.2320],\n",
            "         [ 0.3364,  0.2978, -0.1713]]], device='cuda:0') tensor([[[-0.3837],\n",
            "         [-0.7866],\n",
            "         [-0.6594],\n",
            "         [ 0.6456],\n",
            "         [ 0.3026],\n",
            "         [-0.9156]]], device='cuda:0')\n",
            "tcost icost 0.010650634765625 0.0\n",
            "tcost icost -0.0042266845703125 0.0\n",
            "tcost icost -0.0311126708984375 0.0\n",
            "tcost icost -0.3740234375 0.0\n",
            "tcost icost -0.05377197265625 0.0\n",
            "tcost icost -0.05804443359375 0.0\n",
            "loss tensor([[-0.3604]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.6883,  0.5445,  0.3863],\n",
            "         [ 0.5429,  0.7286,  0.4175],\n",
            "         [ 0.4128,  0.3030,  0.0187],\n",
            "         [ 0.2901,  0.4620, -0.3351],\n",
            "         [-0.5126, -0.1784, -0.3318],\n",
            "         [ 0.4360,  0.3975, -0.0711]]], device='cuda:0') tensor([-0.1049, -0.9090, -0.6146,  0.5712,  0.3406, -0.9555], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.3603515625\n",
            "tcost icost 0.0253448486328125 0.0\n",
            "tcost icost 0.027099609375 0.0\n",
            "tcost icost 0.0009169578552246094 0.0\n",
            "tcost icost -0.340087890625 0.0\n",
            "tcost icost -0.0487060546875 0.0\n",
            "tcost icost -0.045135498046875 0.0\n",
            "loss tensor([[-0.2561]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.7866,  0.4519,  0.2891],\n",
            "         [ 0.4800,  0.7401,  0.4710],\n",
            "         [ 0.4947,  0.2150, -0.0647],\n",
            "         [ 0.3896,  0.5616, -0.2348],\n",
            "         [-0.5348, -0.2782, -0.4313],\n",
            "         [ 0.5340,  0.4955,  0.0285]]], device='cuda:0') tensor([ 0.3616, -1.0000, -0.4465,  0.4782,  0.3806, -0.9924], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.256103515625\n",
            "tcost icost 0.057647705078125 0.0\n",
            "tcost icost 0.0274810791015625 0.0\n",
            "tcost icost 0.0212554931640625 0.0\n",
            "tcost icost 0.075927734375 0.0\n",
            "tcost icost 0.009613037109375 0.0\n",
            "tcost icost 0.019775390625 0.0\n",
            "loss tensor([[0.1731]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.8748,  0.3728,  0.2036],\n",
            "         [ 0.3896,  0.7611,  0.5186],\n",
            "         [ 0.4545,  0.1385, -0.1390],\n",
            "         [ 0.4851,  0.6564, -0.1405],\n",
            "         [-0.4840, -0.3745, -0.5308],\n",
            "         [ 0.6116,  0.5508,  0.0763]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.1590,  0.1897,  0.3992, -0.9696], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.173095703125\n",
            "tcost icost 0.07086181640625 0.0\n",
            "tcost icost -0.01287078857421875 0.0\n",
            "tcost icost -0.1348876953125 0.0\n",
            "tcost icost -0.02447509765625 0.0\n",
            "tcost icost -0.267333984375 0.0\n",
            "tcost icost -0.06390380859375 0.0\n",
            "loss tensor([[-0.2808]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9326,  0.3279,  0.1507],\n",
            "         [ 0.3604,  0.7590,  0.5423],\n",
            "         [ 0.4678,  0.0895, -0.1872],\n",
            "         [ 0.5671,  0.7214, -0.0786],\n",
            "         [-0.4139, -0.3597, -0.5743],\n",
            "         [ 0.6754,  0.5907,  0.1088]]], device='cuda:0') tensor([ 0.6184, -0.7808, -0.0019,  0.2705,  0.2963, -0.9637], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.28076171875\n",
            "tcost icost 0.07110595703125 0.0\n",
            "tcost icost 0.0237884521484375 0.0\n",
            "tcost icost 0.0198974609375 0.0\n",
            "tcost icost 0.044036865234375 0.0\n",
            "tcost icost 0.072998046875 0.0\n",
            "tcost icost -0.0318603515625 0.0\n",
            "loss tensor([[0.1698]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9202,  0.3531,  0.1689],\n",
            "         [ 0.4053,  0.7365,  0.5130],\n",
            "         [ 0.5201,  0.1029, -0.1778],\n",
            "         [ 0.6537,  0.7067, -0.1087],\n",
            "         [-0.3473, -0.3077, -0.6116],\n",
            "         [ 0.7324,  0.6121,  0.1141]]], device='cuda:0') tensor([-1.0000,  0.5698, -0.7168,  0.8262,  0.1422, -0.9451], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1697998046875\n",
            "tcost icost -0.0117950439453125 0.0\n",
            "tcost icost -0.33251953125 0.0\n",
            "tcost icost -0.06231689453125 0.0\n",
            "tcost icost -0.38623046875 0.0\n",
            "tcost icost -0.061981201171875 0.0\n",
            "tcost icost -0.07623291015625 0.0\n",
            "loss tensor([[-0.7285]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9096,  0.3727,  0.1834],\n",
            "         [ 0.4458,  0.7147,  0.4890],\n",
            "         [ 0.5697,  0.1138, -0.1694],\n",
            "         [ 0.7198,  0.6824, -0.1271],\n",
            "         [-0.2865, -0.2677, -0.6485],\n",
            "         [ 0.7688,  0.6246,  0.1374]]], device='cuda:0') tensor([-0.9618,  0.4501, -0.7099,  0.7764,  0.1760, -0.9909], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.728515625\n",
            "tcost icost -0.0105133056640625 0.0\n",
            "tcost icost -0.31591796875 0.0\n",
            "tcost icost -0.062744140625 0.0\n",
            "tcost icost -0.380859375 0.0\n",
            "tcost icost -0.061553955078125 0.0\n",
            "tcost icost -0.07275390625 0.0\n",
            "loss tensor([[-0.7065]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9010,  0.3876,  0.1948],\n",
            "         [ 0.4829,  0.6932,  0.4695],\n",
            "         [ 0.6173,  0.1230, -0.1617],\n",
            "         [ 0.7556,  0.6413, -0.1334],\n",
            "         [-0.2333, -0.2379, -0.6846],\n",
            "         [ 0.7703,  0.6158,  0.1659]]], device='cuda:0') tensor([-0.9173,  0.3202, -0.7035,  0.7253,  0.2099, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.70654296875\n",
            "tcost icost -0.009002685546875 0.0\n",
            "tcost icost -0.295654296875 0.0\n",
            "tcost icost -0.062347412109375 0.0\n",
            "tcost icost -0.375732421875 0.0\n",
            "tcost icost -0.06109619140625 0.0\n",
            "tcost icost -0.0711669921875 0.0\n",
            "loss tensor([[-0.6816]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.8944,  0.3985,  0.2032],\n",
            "         [ 0.5174,  0.6722,  0.4541],\n",
            "         [ 0.6634,  0.1304, -0.1544],\n",
            "         [ 0.7853,  0.6046, -0.1330],\n",
            "         [-0.1892, -0.2168, -0.7198],\n",
            "         [ 0.7672,  0.6092,  0.2007]]], device='cuda:0') tensor([-0.8589,  0.1826, -0.6961,  0.6733,  0.2441, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.681640625\n",
            "tcost icost -0.0070343017578125 0.0\n",
            "tcost icost -0.2724609375 0.0\n",
            "tcost icost -0.06268310546875 0.0\n",
            "tcost icost -0.369384765625 0.0\n",
            "tcost icost -0.0606689453125 0.0\n",
            "tcost icost -0.07061767578125 0.0\n",
            "loss tensor([[-0.6538]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.8898,  0.4057,  0.2090],\n",
            "         [ 0.5493,  0.6528,  0.4426],\n",
            "         [ 0.7081,  0.1365, -0.1474],\n",
            "         [ 0.8104,  0.5720, -0.1270],\n",
            "         [-0.1551, -0.2035, -0.7548],\n",
            "         [ 0.7596,  0.6048,  0.2392]]], device='cuda:0') tensor([-0.7824,  0.0308, -0.6871,  0.6189,  0.2812, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.65380859375\n",
            "tcost icost -0.004482269287109375 0.0\n",
            "tcost icost -0.2435302734375 0.0\n",
            "tcost icost -0.062744140625 0.0\n",
            "tcost icost -0.361083984375 0.0\n",
            "tcost icost -0.06011962890625 0.0\n",
            "tcost icost -0.06951904296875 0.0\n",
            "loss tensor([[-0.6187]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.8874,  0.4094,  0.2118],\n",
            "         [ 0.5786,  0.6362,  0.4348],\n",
            "         [ 0.7516,  0.1415, -0.1404],\n",
            "         [ 0.8317,  0.5430, -0.1161],\n",
            "         [-0.1316, -0.1973, -0.7898],\n",
            "         [ 0.7480,  0.6021,  0.2793]]], device='cuda:0') tensor([-0.6733, -0.1306, -0.6764,  0.5616,  0.3188, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.61865234375\n",
            "tcost icost -0.0009350776672363281 0.0\n",
            "tcost icost -0.2069091796875 0.0\n",
            "tcost icost -0.0621337890625 0.0\n",
            "tcost icost -0.350830078125 0.0\n",
            "tcost icost -0.05926513671875 0.0\n",
            "tcost icost -0.06781005859375 0.0\n",
            "loss tensor([[-0.5723]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.8875,  0.4095,  0.2114],\n",
            "         [ 0.6052,  0.6241,  0.4310],\n",
            "         [ 0.7937,  0.1455, -0.1332],\n",
            "         [ 0.8497,  0.5176, -0.1009],\n",
            "         [-0.1198, -0.1975, -0.8239],\n",
            "         [ 0.7320,  0.6014,  0.3202]]], device='cuda:0') tensor([-0.5219, -0.3052, -0.6676,  0.4982,  0.3566, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.572265625\n",
            "tcost icost 0.00412750244140625 0.0\n",
            "tcost icost -0.1568603515625 0.0\n",
            "tcost icost -0.059112548828125 0.0\n",
            "tcost icost -0.3388671875 0.0\n",
            "tcost icost -0.0576171875 0.0\n",
            "tcost icost -0.0650634765625 0.0\n",
            "loss tensor([[-0.5083]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.8902,  0.4056,  0.2072],\n",
            "         [ 0.6292,  0.6181,  0.4308],\n",
            "         [ 0.8346,  0.1488, -0.1257],\n",
            "         [ 0.8647,  0.4955, -0.0822],\n",
            "         [-0.1179, -0.2036, -0.8591],\n",
            "         [ 0.7124,  0.6022,  0.3604]]], device='cuda:0') tensor([-0.3267, -0.4966, -0.6577,  0.4295,  0.3973, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.50830078125\n",
            "tcost icost 0.010894775390625 0.0\n",
            "tcost icost -0.07684326171875 0.0\n",
            "tcost icost -0.049072265625 0.0\n",
            "tcost icost -0.327880859375 0.0\n",
            "tcost icost -0.05438232421875 0.0\n",
            "tcost icost -0.06402587890625 0.0\n",
            "loss tensor([[-0.4104]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.8969,  0.3960,  0.1970],\n",
            "         [ 0.6511,  0.6190,  0.4351],\n",
            "         [ 0.8740,  0.1509, -0.1179],\n",
            "         [ 0.8771,  0.4765, -0.0603],\n",
            "         [-0.1252, -0.2151, -0.8930],\n",
            "         [ 0.6884,  0.6048,  0.4004]]], device='cuda:0') tensor([ 0.0099, -0.7158, -0.6331,  0.3498,  0.4379, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.410400390625\n",
            "tcost icost 0.0299224853515625 0.0\n",
            "tcost icost 0.044647216796875 0.0\n",
            "tcost icost -0.00945281982421875 0.0\n",
            "tcost icost -0.2998046875 0.0\n",
            "tcost icost -0.046630859375 0.0\n",
            "tcost icost -0.054840087890625 0.0\n",
            "loss tensor([[-0.2190]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9082,  0.3786,  0.1786],\n",
            "         [ 0.6578,  0.6144,  0.4357],\n",
            "         [ 0.9081,  0.1487, -0.1136],\n",
            "         [ 0.8865,  0.4615, -0.0351],\n",
            "         [-0.1412, -0.2322, -0.9281],\n",
            "         [ 0.6631,  0.6069,  0.4381]]], device='cuda:0') tensor([ 0.5378, -0.7831, -0.5270,  0.2316,  0.4798, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.218994140625\n",
            "tcost icost 0.06884765625 0.0\n",
            "tcost icost 0.0282745361328125 0.0\n",
            "tcost icost 0.0157012939453125 0.0\n",
            "tcost icost 0.1434326171875 0.0\n",
            "tcost icost 0.0989990234375 0.0\n",
            "tcost icost 0.00981903076171875 0.0\n",
            "loss tensor([[0.2822]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9320,  0.3357,  0.1370],\n",
            "         [ 0.6219,  0.6367,  0.4559],\n",
            "         [ 0.9193,  0.1205, -0.1371],\n",
            "         [ 0.8880,  0.4599,  0.0032],\n",
            "         [-0.1710, -0.2589, -0.9506],\n",
            "         [ 0.6498,  0.6028,  0.4629]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.2401, -0.1768,  0.6318, -0.9718], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2822265625\n",
            "tcost icost 0.07086181640625 0.0\n",
            "tcost icost -0.01287078857421875 0.0\n",
            "tcost icost -0.16064453125 0.0\n",
            "tcost icost -0.037628173828125 0.0\n",
            "tcost icost -0.312255859375 0.0\n",
            "tcost icost -0.0648193359375 0.0\n",
            "loss tensor([[-0.3416]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9480,  0.3007,  0.1041],\n",
            "         [ 0.5975,  0.6497,  0.4699],\n",
            "         [ 0.9345,  0.0982, -0.1552],\n",
            "         [ 0.8897,  0.4552,  0.0352],\n",
            "         [-0.1546, -0.2619, -0.9526],\n",
            "         [ 0.6409,  0.5972,  0.4823]]], device='cuda:0') tensor([ 0.6702, -0.8066,  0.1057, -0.1387,  0.5472, -0.9686], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.341552734375\n",
            "tcost icost 0.0721435546875 0.0\n",
            "tcost icost 0.0202178955078125 0.0\n",
            "tcost icost -0.0311431884765625 0.0\n",
            "tcost icost -0.004253387451171875 0.0\n",
            "tcost icost -0.2149658203125 0.0\n",
            "tcost icost -0.05609130859375 0.0\n",
            "loss tensor([[-0.1121]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9556,  0.2816,  0.0864],\n",
            "         [ 0.5970,  0.6491,  0.4714],\n",
            "         [ 0.9572,  0.0860, -0.1645],\n",
            "         [ 0.8960,  0.4405,  0.0559],\n",
            "         [-0.1161, -0.2481, -0.9618],\n",
            "         [ 0.6387,  0.5889,  0.4952]]], device='cuda:0') tensor([-0.0745, -0.2612, -0.1529, -0.0060,  0.4357, -0.9594], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.112060546875\n",
            "tcost icost 0.0240478515625 0.0\n",
            "tcost icost -0.07244873046875 0.0\n",
            "tcost icost -0.030426025390625 0.0\n",
            "tcost icost -0.2138671875 0.0\n",
            "tcost icost -0.0443115234375 0.0\n",
            "tcost icost -0.0499267578125 0.0\n",
            "loss tensor([[-0.2803]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9645,  0.2564,  0.0625],\n",
            "         [ 0.5930,  0.6493,  0.4762],\n",
            "         [ 0.9762,  0.0717, -0.1747],\n",
            "         [ 0.8996,  0.4294,  0.0792],\n",
            "         [-0.0866, -0.2400, -0.9669],\n",
            "         [ 0.6305,  0.5840,  0.5113]]], device='cuda:0') tensor([ 0.5505, -0.5122, -0.0294, -0.1114,  0.4787, -0.9986], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2802734375\n",
            "tcost icost 0.069091796875 0.0\n",
            "tcost icost 0.07330322265625 0.0\n",
            "tcost icost 0.087158203125 0.0\n",
            "tcost icost 0.0751953125 0.0\n",
            "tcost icost 0.285888671875 0.0\n",
            "tcost icost 0.00860595703125 0.0\n",
            "loss tensor([[0.4529]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9723,  0.2308,  0.0375],\n",
            "         [ 0.5851,  0.6538,  0.4797],\n",
            "         [ 0.9798,  0.0518, -0.1932],\n",
            "         [ 0.9072,  0.4100,  0.0942],\n",
            "         [-0.0511, -0.2433, -0.9686],\n",
            "         [ 0.6317,  0.5752,  0.5198]]], device='cuda:0') tensor([ 0.5546, -0.6387,  0.2226, -0.1215,  0.6942, -0.9805], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.452880859375\n",
            "tcost icost 0.0692138671875 0.0\n",
            "tcost icost 0.046356201171875 0.0\n",
            "tcost icost 0.1239013671875 0.0\n",
            "tcost icost 0.022674560546875 0.0\n",
            "tcost icost -0.10809326171875 0.0\n",
            "tcost icost -0.041351318359375 0.0\n",
            "loss tensor([[0.1324]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9709,  0.2357,  0.0426],\n",
            "         [ 0.6166,  0.6353,  0.4585],\n",
            "         [ 0.9799,  0.0477, -0.1939],\n",
            "         [ 0.9217,  0.3763,  0.0943],\n",
            "         [-0.0016, -0.2299, -0.9732],\n",
            "         [ 0.6419,  0.5624,  0.5212]]], device='cuda:0') tensor([-1.0000,  0.8135, -0.0919,  0.1006,  0.5614, -0.9670], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1324462890625\n",
            "search tensor([[[-0.0016, -0.2299, -0.9732],\n",
            "         [ 0.6419,  0.5624,  0.5212],\n",
            "         [ 0.3559, -0.1520, -0.1355],\n",
            "         [ 0.4501,  0.2472, -0.0355],\n",
            "         [ 0.4047,  0.2258,  0.2298],\n",
            "         [-0.3995,  0.3855,  0.0264]]], device='cuda:0') tensor([[[ 0.5614],\n",
            "         [-0.9670],\n",
            "         [-0.5514],\n",
            "         [ 0.0238],\n",
            "         [-0.2028],\n",
            "         [-0.5618]]], device='cuda:0')\n",
            "tcost icost 0.085205078125 0.0\n",
            "tcost icost 0.016387939453125 0.0\n",
            "tcost icost 0.00753021240234375 0.0\n",
            "tcost icost 0.052947998046875 0.0\n",
            "tcost icost 0.13427734375 0.0\n",
            "tcost icost -0.0003478527069091797 0.0\n",
            "loss tensor([[0.2324]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.0984, -0.1296, -0.8723],\n",
            "         [ 0.7413,  0.4618,  0.4207],\n",
            "         [ 0.4556, -0.0519, -0.0354],\n",
            "         [ 0.5496,  0.1470, -0.1354],\n",
            "         [ 0.5042,  0.3256,  0.3296],\n",
            "         [-0.4991,  0.2852, -0.0736]]], device='cuda:0') tensor([-1.0000,  0.5720, -1.0000,  0.7367, -0.3077, -0.5387], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.232421875\n",
            "tcost icost -0.01102447509765625 0.0\n",
            "tcost icost -0.326416015625 0.0\n",
            "tcost icost -0.06658935546875 0.0\n",
            "tcost icost -0.37646484375 0.0\n",
            "tcost icost -0.06787109375 0.0\n",
            "tcost icost -0.1453857421875 0.0\n",
            "loss tensor([[-0.7632]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.1679, -0.0631, -0.8061],\n",
            "         [ 0.8081,  0.3954,  0.3558],\n",
            "         [ 0.5272,  0.0150,  0.0334],\n",
            "         [ 0.6295,  0.0838, -0.1956],\n",
            "         [ 0.5763,  0.3828,  0.3941],\n",
            "         [-0.4403,  0.3380, -0.0413]]], device='cuda:0') tensor([-0.9467,  0.4495, -0.9907,  0.6882, -0.2821, -0.6295], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.76318359375\n",
            "tcost icost -0.00917816162109375 0.0\n",
            "tcost icost -0.308837890625 0.0\n",
            "tcost icost -0.06744384765625 0.0\n",
            "tcost icost -0.36572265625 0.0\n",
            "tcost icost -0.067626953125 0.0\n",
            "tcost icost -0.1343994140625 0.0\n",
            "loss tensor([[-0.7324]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.2220, -0.0129, -0.7576],\n",
            "         [ 0.8600,  0.3460,  0.3085],\n",
            "         [ 0.5866,  0.0663,  0.0882],\n",
            "         [ 0.7173,  0.0386, -0.2363],\n",
            "         [ 0.6380,  0.4191,  0.4399],\n",
            "         [-0.3634,  0.4101,  0.0192]]], device='cuda:0') tensor([-0.8809,  0.3148, -0.9807,  0.6377, -0.2536, -0.7154], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.732421875\n",
            "tcost icost -0.00690460205078125 0.0\n",
            "tcost icost -0.286376953125 0.0\n",
            "tcost icost -0.06829833984375 0.0\n",
            "tcost icost -0.357177734375 0.0\n",
            "tcost icost -0.06744384765625 0.0\n",
            "tcost icost -0.135986328125 0.0\n",
            "loss tensor([[-0.7051]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.2645,  0.0265, -0.7219],\n",
            "         [ 0.9025,  0.3087,  0.2729],\n",
            "         [ 0.6387,  0.1078,  0.1346],\n",
            "         [ 0.8089,  0.0051, -0.2645],\n",
            "         [ 0.6942,  0.4420,  0.4737],\n",
            "         [-0.2786,  0.4900,  0.0924]]], device='cuda:0') tensor([-0.7992,  0.1636, -0.9726,  0.5853, -0.2239, -0.8015], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.705078125\n",
            "tcost icost -0.003948211669921875 0.0\n",
            "tcost icost -0.2578125 0.0\n",
            "tcost icost -0.0682373046875 0.0\n",
            "tcost icost -0.348388671875 0.0\n",
            "tcost icost -0.06695556640625 0.0\n",
            "tcost icost -0.1219482421875 0.0\n",
            "loss tensor([[-0.6611]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.2964,  0.0576, -0.6977],\n",
            "         [ 0.9290,  0.2784,  0.2439],\n",
            "         [ 0.6857,  0.1424,  0.1753],\n",
            "         [ 0.9025, -0.0200, -0.2837],\n",
            "         [ 0.7417,  0.4522,  0.4953],\n",
            "         [-0.1895,  0.5734,  0.1730]]], device='cuda:0') tensor([-0.6871,  0.0010, -0.9682,  0.5295, -0.1931, -0.8871], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6611328125\n",
            "tcost icost -8.404254913330078e-05 0.0\n",
            "tcost icost -0.220947265625 0.0\n",
            "tcost icost -0.06829833984375 0.0\n",
            "tcost icost -0.3359375 0.0\n",
            "tcost icost -0.066162109375 0.0\n",
            "tcost icost -0.109619140625 0.0\n",
            "loss tensor([[-0.6074]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.3164,  0.0815, -0.6855],\n",
            "         [ 0.9415,  0.2548,  0.2204],\n",
            "         [ 0.7286,  0.1720,  0.2118],\n",
            "         [ 0.9580, -0.0366, -0.2845],\n",
            "         [ 0.7551,  0.4368,  0.4889],\n",
            "         [-0.0988,  0.6558,  0.2569]]], device='cuda:0') tensor([-0.5313, -0.1763, -0.9669,  0.4657, -0.1646, -0.9568], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.607421875\n",
            "tcost icost 0.004604339599609375 0.0\n",
            "tcost icost -0.1680908203125 0.0\n",
            "tcost icost -0.06634521484375 0.0\n",
            "tcost icost -0.321044921875 0.0\n",
            "tcost icost -0.0648193359375 0.0\n",
            "tcost icost -0.099853515625 0.0\n",
            "loss tensor([[-0.5356]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.3207,  0.0981, -0.6873],\n",
            "         [ 0.9493,  0.2396,  0.2034],\n",
            "         [ 0.7683,  0.1975,  0.2450],\n",
            "         [ 0.9627, -0.0450, -0.2667],\n",
            "         [ 0.7714,  0.4186,  0.4793],\n",
            "         [-0.0079,  0.7351,  0.3425]]], device='cuda:0') tensor([-0.3221, -0.3694, -0.9661,  0.3996, -0.1367, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.53564453125\n",
            "tcost icost 0.0111541748046875 0.0\n",
            "tcost icost -0.08514404296875 0.0\n",
            "tcost icost -0.05682373046875 0.0\n",
            "tcost icost -0.305908203125 0.0\n",
            "tcost icost -0.0626220703125 0.0\n",
            "tcost icost -0.09503173828125 0.0\n",
            "loss tensor([[-0.4316]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.3035,  0.1057, -0.7067],\n",
            "         [ 0.9528,  0.2340,  0.1933],\n",
            "         [ 0.8044,  0.2194,  0.2753],\n",
            "         [ 0.9680, -0.0477, -0.2463],\n",
            "         [ 0.7898,  0.3978,  0.4668],\n",
            "         [ 0.0824,  0.8094,  0.4285]]], device='cuda:0') tensor([ 0.0309, -0.5944, -0.9595,  0.3203, -0.1088, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.431640625\n",
            "tcost icost 0.033843994140625 0.0\n",
            "tcost icost 0.04888916015625 0.0\n",
            "tcost icost -0.0172882080078125 0.0\n",
            "tcost icost -0.282470703125 0.0\n",
            "tcost icost -0.053558349609375 0.0\n",
            "tcost icost -0.09033203125 0.0\n",
            "loss tensor([[-0.2306]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.2671,  0.1026, -0.7427],\n",
            "         [ 0.9535,  0.2348,  0.1891],\n",
            "         [ 0.8386,  0.2353,  0.2998],\n",
            "         [ 0.9736, -0.0445, -0.2238],\n",
            "         [ 0.8120,  0.3735,  0.4484],\n",
            "         [ 0.1659,  0.8516,  0.4973]]], device='cuda:0') tensor([ 0.5910, -0.6465, -0.8848,  0.2048, -0.0749, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2305908203125\n",
            "tcost icost 0.0860595703125 0.0\n",
            "tcost icost 0.025115966796875 0.0\n",
            "tcost icost 0.007747650146484375 0.0\n",
            "tcost icost 0.039886474609375 0.0\n",
            "tcost icost 0.0188751220703125 0.0\n",
            "tcost icost 0.021484375 0.0\n",
            "loss tensor([[0.1691]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.2167,  0.0809, -0.7959],\n",
            "         [ 0.9426,  0.2630,  0.2057],\n",
            "         [ 0.8841,  0.2258,  0.2952],\n",
            "         [ 0.9802, -0.0281, -0.1961],\n",
            "         [ 0.8459,  0.3396,  0.4111],\n",
            "         [ 0.2243,  0.8256,  0.5177]]], device='cuda:0') tensor([ 1.0000, -1.0000, -0.3409, -0.0632,  0.0129, -0.9971], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1690673828125\n",
            "tcost icost 0.057342529296875 0.0\n",
            "tcost icost -0.0199127197265625 0.0\n",
            "tcost icost -0.0675048828125 0.0\n",
            "tcost icost -0.0262603759765625 0.0\n",
            "tcost icost -0.293212890625 0.0\n",
            "tcost icost -0.06536865234375 0.0\n",
            "loss tensor([[-0.2654]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.1852,  0.0704, -0.8324],\n",
            "         [ 0.9361,  0.2784,  0.2151],\n",
            "         [ 0.9230,  0.2211,  0.2951],\n",
            "         [ 0.9846, -0.0167, -0.1743],\n",
            "         [ 0.8656,  0.3175,  0.3872],\n",
            "         [ 0.2719,  0.8021,  0.5318]]], device='cuda:0') tensor([ 0.5781, -0.8164, -0.4988, -0.0093, -0.1006, -0.9933], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.265380859375\n",
            "tcost icost 0.0855712890625 0.0\n",
            "tcost icost 0.01910400390625 0.0\n",
            "tcost icost 0.01091766357421875 0.0\n",
            "tcost icost 0.0364990234375 0.0\n",
            "tcost icost 0.046173095703125 0.0\n",
            "tcost icost -0.027557373046875 0.0\n",
            "loss tensor([[0.1522]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.1782,  0.0940, -0.8194],\n",
            "         [ 0.9496,  0.2500,  0.1890],\n",
            "         [ 0.9198,  0.2378,  0.3121],\n",
            "         [ 0.9820, -0.0331, -0.1859],\n",
            "         [ 0.8678,  0.3185,  0.3815],\n",
            "         [ 0.3128,  0.7825,  0.5384]]], device='cuda:0') tensor([-1.0000,  0.5654, -1.0000,  0.6216, -0.2707, -0.9750], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1522216796875\n",
            "tcost icost -0.01102447509765625 0.0\n",
            "tcost icost -0.325439453125 0.0\n",
            "tcost icost -0.0660400390625 0.0\n",
            "tcost icost -0.362060546875 0.0\n",
            "tcost icost -0.0673828125 0.0\n",
            "tcost icost -0.105712890625 0.0\n",
            "loss tensor([[-0.7280]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.1730,  0.1149, -0.8082],\n",
            "         [ 0.9599,  0.2252,  0.1669],\n",
            "         [ 0.9118,  0.2506,  0.3254],\n",
            "         [ 0.9803, -0.0450, -0.1922],\n",
            "         [ 0.8721,  0.3152,  0.3744],\n",
            "         [ 0.3499,  0.7607,  0.5467]]], device='cuda:0') tensor([-0.9559,  0.4386, -0.9992,  0.5618, -0.2419, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.72802734375\n",
            "tcost icost -0.00952911376953125 0.0\n",
            "tcost icost -0.306396484375 0.0\n",
            "tcost icost -0.06671142578125 0.0\n",
            "tcost icost -0.35400390625 0.0\n",
            "tcost icost -0.06707763671875 0.0\n",
            "tcost icost -0.10418701171875 0.0\n",
            "loss tensor([[-0.7031]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.1690,  0.1332, -0.7987],\n",
            "         [ 0.9676,  0.2040,  0.1486],\n",
            "         [ 0.9046,  0.2612,  0.3370],\n",
            "         [ 0.9797, -0.0528, -0.1936],\n",
            "         [ 0.8783,  0.3080,  0.3657],\n",
            "         [ 0.3834,  0.7373,  0.5562]]], device='cuda:0') tensor([-0.9079,  0.2943, -0.9984,  0.5017, -0.2123, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.703125\n",
            "tcost icost -0.0078125 0.0\n",
            "tcost icost -0.281494140625 0.0\n",
            "tcost icost -0.0673828125 0.0\n",
            "tcost icost -0.345947265625 0.0\n",
            "tcost icost -0.0667724609375 0.0\n",
            "tcost icost -0.10577392578125 0.0\n",
            "loss tensor([[-0.6748]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.1656,  0.1487, -0.7912],\n",
            "         [ 0.9732,  0.1865,  0.1342],\n",
            "         [ 0.8981,  0.2699,  0.3473],\n",
            "         [ 0.9800, -0.0568, -0.1907],\n",
            "         [ 0.8861,  0.2970,  0.3559],\n",
            "         [ 0.4134,  0.7127,  0.5667]]], device='cuda:0') tensor([-0.8384,  0.1440, -0.9963,  0.4432, -0.1817, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6748046875\n",
            "tcost icost -0.005340576171875 0.0\n",
            "tcost icost -0.253662109375 0.0\n",
            "tcost icost -0.06787109375 0.0\n",
            "tcost icost -0.33642578125 0.0\n",
            "tcost icost -0.0662841796875 0.0\n",
            "tcost icost -0.1065673828125 0.0\n",
            "loss tensor([[-0.6401]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.1618,  0.1610, -0.7861],\n",
            "         [ 0.9772,  0.1727,  0.1233],\n",
            "         [ 0.8923,  0.2769,  0.3565],\n",
            "         [ 0.9813, -0.0573, -0.1839],\n",
            "         [ 0.8954,  0.2825,  0.3442],\n",
            "         [ 0.4400,  0.6874,  0.5778]]], device='cuda:0') tensor([-0.7377, -0.0187, -0.9984,  0.3802, -0.1495, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.64013671875\n",
            "tcost icost -0.0018177032470703125 0.0\n",
            "tcost icost -0.2183837890625 0.0\n",
            "tcost icost -0.06793212890625 0.0\n",
            "tcost icost -0.323974609375 0.0\n",
            "tcost icost -0.06549072265625 0.0\n",
            "tcost icost -0.1063232421875 0.0\n",
            "loss tensor([[-0.5957]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.1572,  0.1698, -0.7840],\n",
            "         [ 0.9799,  0.1625,  0.1161],\n",
            "         [ 0.8872,  0.2825,  0.3649],\n",
            "         [ 0.9833, -0.0545, -0.1736],\n",
            "         [ 0.9059,  0.2646,  0.3306],\n",
            "         [ 0.4636,  0.6620,  0.5889]]], device='cuda:0') tensor([-0.5919, -0.1915, -0.9963,  0.3144, -0.1191, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.595703125\n",
            "tcost icost 0.002960205078125 0.0\n",
            "tcost icost -0.1707763671875 0.0\n",
            "tcost icost -0.0665283203125 0.0\n",
            "tcost icost -0.30859375 0.0\n",
            "tcost icost -0.06414794921875 0.0\n",
            "tcost icost -0.10491943359375 0.0\n",
            "loss tensor([[-0.5337]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.1503,  0.1741, -0.7860],\n",
            "         [ 0.9814,  0.1561,  0.1120],\n",
            "         [ 0.8825,  0.2868,  0.3726],\n",
            "         [ 0.9859, -0.0486, -0.1603],\n",
            "         [ 0.9175,  0.2432,  0.3147],\n",
            "         [ 0.4841,  0.6371,  0.5998]]], device='cuda:0') tensor([-0.3945, -0.3859, -0.9980,  0.2433, -0.0878, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.53369140625\n",
            "tcost icost 0.00861358642578125 0.0\n",
            "tcost icost -0.0958251953125 0.0\n",
            "tcost icost -0.059234619140625 0.0\n",
            "tcost icost -0.291259765625 0.0\n",
            "tcost icost -0.061614990234375 0.0\n",
            "tcost icost -0.10321044921875 0.0\n",
            "loss tensor([[-0.4395]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.1397,  0.1723, -0.7937],\n",
            "         [ 0.9818,  0.1538,  0.1117],\n",
            "         [ 0.8784,  0.2900,  0.3798],\n",
            "         [ 0.9888, -0.0396, -0.1442],\n",
            "         [ 0.9298,  0.2184,  0.2963],\n",
            "         [ 0.5017,  0.6130,  0.6103]]], device='cuda:0') tensor([-0.0896, -0.5859, -0.9934,  0.1599, -0.0575, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.439453125\n",
            "tcost icost 0.0225982666015625 0.0\n",
            "tcost icost 0.0174713134765625 0.0\n",
            "tcost icost -0.0304412841796875 0.0\n",
            "tcost icost -0.26708984375 0.0\n",
            "tcost icost -0.05389404296875 0.0\n",
            "tcost icost -0.10101318359375 0.0\n",
            "loss tensor([[-0.2761]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.1237,  0.1640, -0.8076],\n",
            "         [ 0.9813,  0.1546,  0.1146],\n",
            "         [ 0.8757,  0.2910,  0.3854],\n",
            "         [ 0.9917, -0.0270, -0.1256],\n",
            "         [ 0.9430,  0.1895,  0.2735],\n",
            "         [ 0.5167,  0.5906,  0.6199]]], device='cuda:0') tensor([ 0.3044, -0.6905, -0.9572,  0.0473, -0.0240, -1.0000], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.276123046875\n",
            "search tensor([[[ 0.9430,  0.1895,  0.2735],\n",
            "         [ 0.5167,  0.5906,  0.6199],\n",
            "         [-0.0311, -0.2342,  0.4661],\n",
            "         [-0.0437,  0.3392, -0.3575],\n",
            "         [-0.2897, -0.3477, -0.4234],\n",
            "         [ 0.0829,  0.0714, -0.4173]]], device='cuda:0') tensor([[[-0.0240],\n",
            "         [-1.0000],\n",
            "         [ 0.1331],\n",
            "         [-0.1669],\n",
            "         [ 0.1437],\n",
            "         [ 0.4014]]], device='cuda:0')\n",
            "tcost icost 0.013397216796875 0.0\n",
            "tcost icost 0.0196075439453125 0.0\n",
            "tcost icost -0.07373046875 0.0\n",
            "tcost icost -0.0017375946044921875 0.0\n",
            "tcost icost -0.053314208984375 0.0\n",
            "tcost icost 0.00498199462890625 0.0\n",
            "loss tensor([[-0.0620]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.8421,  0.2893,  0.3732],\n",
            "         [ 0.6161,  0.4900,  0.5193],\n",
            "         [ 0.0689, -0.1339,  0.5657],\n",
            "         [-0.1436,  0.2388, -0.4571],\n",
            "         [-0.1895, -0.2474, -0.3230],\n",
            "         [-0.0172, -0.0287, -0.5169]]], device='cuda:0') tensor([-1.0000, -0.2192, -0.1865,  0.0222, -0.0104,  0.4242], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.06201171875\n",
            "tcost icost -0.01139068603515625 0.0\n",
            "tcost icost -0.103759765625 0.0\n",
            "tcost icost -0.0361328125 0.0\n",
            "tcost icost -0.347900390625 0.0\n",
            "tcost icost -0.06494140625 0.0\n",
            "tcost icost -0.277099609375 0.0\n",
            "loss tensor([[-0.5938]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.8009,  0.3436,  0.4277],\n",
            "         [ 0.6822,  0.4350,  0.4640],\n",
            "         [ 0.1258, -0.0863,  0.6209],\n",
            "         [-0.0839,  0.2120, -0.4984],\n",
            "         [-0.1338, -0.1850, -0.2460],\n",
            "         [ 0.0453,  0.0223, -0.4832]]], device='cuda:0') tensor([-0.8392, -0.4596, -0.1090, -0.0559,  0.0024,  0.3517], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.59375\n",
            "tcost icost -0.007904052734375 0.0\n",
            "tcost icost -0.041839599609375 0.0\n",
            "tcost icost -0.019195556640625 0.0\n",
            "tcost icost -0.311279296875 0.0\n",
            "tcost icost -0.062744140625 0.0\n",
            "tcost icost -0.2359619140625 0.0\n",
            "loss tensor([[-0.4688]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.8091,  0.3683,  0.4512],\n",
            "         [ 0.7293,  0.4199,  0.4408],\n",
            "         [ 0.1484, -0.0818,  0.6411],\n",
            "         [-0.0061,  0.2251, -0.5098],\n",
            "         [-0.0978, -0.1406, -0.1775],\n",
            "         [ 0.1239,  0.0936, -0.4200]]], device='cuda:0') tensor([-0.4945, -0.7142,  0.0558, -0.1505,  0.0130,  0.2838], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.46875\n",
            "tcost icost -6.139278411865234e-06 0.0\n",
            "tcost icost 0.0012598037719726562 0.0\n",
            "tcost icost 0.045928955078125 0.0\n",
            "tcost icost -0.03955078125 0.0\n",
            "tcost icost -0.033721923828125 0.0\n",
            "tcost icost -0.1531982421875 0.0\n",
            "loss tensor([[-0.1031]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.8422,  0.3407,  0.4179],\n",
            "         [ 0.7414,  0.4599,  0.4679],\n",
            "         [ 0.1222, -0.1262,  0.6083],\n",
            "         [ 0.0749,  0.2676, -0.4989],\n",
            "         [-0.0738, -0.1172, -0.1468],\n",
            "         [ 0.2097,  0.1738, -0.3454]]], device='cuda:0') tensor([ 0.6696, -1.0000,  0.5431, -0.3083,  0.0431,  0.2177], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.10308837890625\n",
            "tcost icost 0.0188446044921875 0.0\n",
            "tcost icost -0.006160736083984375 0.0\n",
            "tcost icost -0.3173828125 0.0\n",
            "tcost icost -0.051727294921875 0.0\n",
            "tcost icost -0.254150390625 0.0\n",
            "tcost icost -0.04010009765625 0.0\n",
            "loss tensor([[-0.4719]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.8587,  0.3243,  0.3969],\n",
            "         [ 0.7457,  0.4702,  0.4721],\n",
            "         [ 0.1131, -0.1577,  0.5868],\n",
            "         [ 0.1419,  0.3007, -0.4896],\n",
            "         [-0.0179, -0.0679, -0.0892],\n",
            "         [ 0.2796,  0.2369, -0.2883]]], device='cuda:0') tensor([ 0.4382, -0.8789,  0.4492, -0.2781, -0.0639,  0.2312], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.471923828125\n",
            "tcost icost 0.0195159912109375 0.0\n",
            "tcost icost 0.00974273681640625 0.0\n",
            "tcost icost -0.266357421875 0.0\n",
            "tcost icost -0.046356201171875 0.0\n",
            "tcost icost -0.1966552734375 0.0\n",
            "tcost icost -0.033203125 0.0\n",
            "loss tensor([[-0.3696]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.8600,  0.3246,  0.3938],\n",
            "         [ 0.7643,  0.4550,  0.4570],\n",
            "         [ 0.1179, -0.1763,  0.5767],\n",
            "         [ 0.1986,  0.3238, -0.4853],\n",
            "         [ 0.0513, -0.0048, -0.0190],\n",
            "         [ 0.3371,  0.2867, -0.2461]]], device='cuda:0') tensor([ 0.0837, -0.5845,  0.3507, -0.2294, -0.1773,  0.2449], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.36962890625\n",
            "tcost icost 0.0178375244140625 0.0\n",
            "tcost icost 0.05218505859375 0.0\n",
            "tcost icost -0.01898193359375 0.0\n",
            "tcost icost -0.004993438720703125 0.0\n",
            "tcost icost -0.0078887939453125 0.0\n",
            "tcost icost 0.0163726806640625 0.0\n",
            "loss tensor([[0.0503]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.8317,  0.3577,  0.4247],\n",
            "         [ 0.8130,  0.4100,  0.4135],\n",
            "         [ 0.1565, -0.1632,  0.5916],\n",
            "         [ 0.2473,  0.3244, -0.5047],\n",
            "         [ 0.1258,  0.0647,  0.0587],\n",
            "         [ 0.3837,  0.3214, -0.2315]]], device='cuda:0') tensor([-1.0000,  0.4458, -0.0358, -0.1164, -0.2610,  0.2708], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.05029296875\n",
            "tcost icost -0.01139068603515625 0.0\n",
            "tcost icost -0.252685546875 0.0\n",
            "tcost icost -0.045440673828125 0.0\n",
            "tcost icost -0.30908203125 0.0\n",
            "tcost icost -0.06512451171875 0.0\n",
            "tcost icost -0.2548828125 0.0\n",
            "loss tensor([[-0.6943]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.8084,  0.3824,  0.4475],\n",
            "         [ 0.8497,  0.3704,  0.3753],\n",
            "         [ 0.1885, -0.1565,  0.6006],\n",
            "         [ 0.3045,  0.3380, -0.5054],\n",
            "         [ 0.1925,  0.1250,  0.1304],\n",
            "         [ 0.4417,  0.3673, -0.1908]]], device='cuda:0') tensor([-0.8694,  0.2690,  0.0476, -0.1988, -0.2516,  0.1988], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6943359375\n",
            "tcost icost -0.00856781005859375 0.0\n",
            "tcost icost -0.213623046875 0.0\n",
            "tcost icost -0.040771484375 0.0\n",
            "tcost icost -0.282958984375 0.0\n",
            "tcost icost -0.0638427734375 0.0\n",
            "tcost icost -0.2296142578125 0.0\n",
            "loss tensor([[-0.6177]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.7923,  0.3985,  0.4620],\n",
            "         [ 0.8762,  0.3377,  0.3438],\n",
            "         [ 0.2125, -0.1571,  0.6032],\n",
            "         [ 0.3694,  0.3630, -0.4897],\n",
            "         [ 0.2531,  0.1775,  0.1975],\n",
            "         [ 0.5071,  0.4205, -0.1354]]], device='cuda:0') tensor([-0.6522,  0.0593,  0.1603, -0.2941, -0.2454,  0.1255], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.61767578125\n",
            "tcost icost -0.0038967132568359375 0.0\n",
            "tcost icost -0.148681640625 0.0\n",
            "tcost icost -0.0263519287109375 0.0\n",
            "tcost icost -0.2059326171875 0.0\n",
            "tcost icost -0.058441162109375 0.0\n",
            "tcost icost -0.1954345703125 0.0\n",
            "loss tensor([[-0.4629]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.7871,  0.4044,  0.4657],\n",
            "         [ 0.8929,  0.3154,  0.3213],\n",
            "         [ 0.2259, -0.1676,  0.5963],\n",
            "         [ 0.4411,  0.3976, -0.4607],\n",
            "         [ 0.3090,  0.2229,  0.2598],\n",
            "         [ 0.5770,  0.4778, -0.0709]]], device='cuda:0') tensor([-0.2667, -0.2527,  0.3414, -0.3982, -0.2402,  0.0522], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.462890625\n",
            "tcost icost 0.006092071533203125 0.0\n",
            "tcost icost 0.006786346435546875 0.0\n",
            "tcost icost 0.062469482421875 0.0\n",
            "tcost icost 0.0849609375 0.0\n",
            "tcost icost -0.01531219482421875 0.0\n",
            "tcost icost -0.1572265625 0.0\n",
            "loss tensor([[0.0219]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.8037,  0.3916,  0.4481],\n",
            "         [ 0.8943,  0.3161,  0.3169],\n",
            "         [ 0.2366, -0.1954,  0.5666],\n",
            "         [ 0.5170,  0.4272, -0.4369],\n",
            "         [ 0.3644,  0.2524,  0.2915],\n",
            "         [ 0.6500,  0.5367, -0.0017]]], device='cuda:0') tensor([ 0.7401, -1.0000,  0.7769, -0.3152, -0.1853, -0.0239], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0218505859375\n",
            "tcost icost 0.00426483154296875 0.0\n",
            "tcost icost -0.013214111328125 0.0\n",
            "tcost icost -0.357666015625 0.0\n",
            "tcost icost -0.05377197265625 0.0\n",
            "tcost icost -0.22607421875 0.0\n",
            "tcost icost -0.04351806640625 0.0\n",
            "loss tensor([[-0.5107]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.8131,  0.3848,  0.4368],\n",
            "         [ 0.8985,  0.3113,  0.3094],\n",
            "         [ 0.2523, -0.2176,  0.5432],\n",
            "         [ 0.5874,  0.4522, -0.4152],\n",
            "         [ 0.4266,  0.2912,  0.3376],\n",
            "         [ 0.7159,  0.5882,  0.0578]]], device='cuda:0') tensor([ 0.3910, -0.8981,  0.7061, -0.2831, -0.2984, -0.0125], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5107421875\n",
            "tcost icost 0.0196380615234375 0.0\n",
            "tcost icost 0.00930023193359375 0.0\n",
            "tcost icost -0.316162109375 0.0\n",
            "tcost icost -0.04949951171875 0.0\n",
            "tcost icost -0.1632080078125 0.0\n",
            "tcost icost -0.037109375 0.0\n",
            "loss tensor([[-0.3931]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.8127,  0.3873,  0.4354],\n",
            "         [ 0.9091,  0.2953,  0.2938],\n",
            "         [ 0.2726, -0.2335,  0.5271],\n",
            "         [ 0.6521,  0.4714, -0.3978],\n",
            "         [ 0.4917,  0.3372,  0.3942],\n",
            "         [ 0.7703,  0.6286,  0.1077]]], device='cuda:0') tensor([ 0.0338, -0.6088,  0.6283, -0.2326, -0.4201, -0.0008], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.39306640625\n",
            "tcost icost 0.0158233642578125 0.0\n",
            "tcost icost 0.05145263671875 0.0\n",
            "tcost icost -0.07647705078125 0.0\n",
            "tcost icost -0.015777587890625 0.0\n",
            "tcost icost -0.003139495849609375 0.0\n",
            "tcost icost 0.00768280029296875 0.0\n",
            "loss tensor([[-0.0089]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.7942,  0.4066,  0.4516],\n",
            "         [ 0.9305,  0.2588,  0.2591],\n",
            "         [ 0.3106, -0.2326,  0.5256],\n",
            "         [ 0.7108,  0.4812, -0.3920],\n",
            "         [ 0.5509,  0.3843,  0.4539],\n",
            "         [ 0.7707,  0.6222,  0.1374]]], device='cuda:0') tensor([-0.8778,  0.1754,  0.3309, -0.1574, -0.4683,  0.0182], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.00885009765625\n",
            "tcost icost -0.0087432861328125 0.0\n",
            "tcost icost -0.1912841796875 0.0\n",
            "tcost icost -0.030181884765625 0.0\n",
            "tcost icost -0.26123046875 0.0\n",
            "tcost icost -0.06488037109375 0.0\n",
            "tcost icost -0.1820068359375 0.0\n",
            "loss tensor([[-0.5459]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.7826,  0.4188,  0.4606],\n",
            "         [ 0.9451,  0.2303,  0.2318],\n",
            "         [ 0.3398, -0.2385,  0.5171],\n",
            "         [ 0.7741,  0.4983, -0.3723],\n",
            "         [ 0.6067,  0.4257,  0.5095],\n",
            "         [ 0.7696,  0.6147,  0.1727]]], device='cuda:0') tensor([-0.6044, -0.0780,  0.4939, -0.2497, -0.4619, -0.0542], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5458984375\n",
            "tcost icost -0.0028324127197265625 0.0\n",
            "tcost icost -0.104736328125 0.0\n",
            "tcost icost -0.00818634033203125 0.0\n",
            "tcost icost -0.1212158203125 0.0\n",
            "tcost icost -0.05450439453125 0.0\n",
            "tcost icost -0.1334228515625 0.0\n",
            "loss tensor([[-0.3066]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.7842,  0.4194,  0.4574],\n",
            "         [ 0.9524,  0.2151,  0.2161],\n",
            "         [ 0.3555, -0.2553,  0.4965],\n",
            "         [ 0.8035,  0.4992, -0.3244],\n",
            "         [ 0.6596,  0.4618,  0.5607],\n",
            "         [ 0.7667,  0.6063,  0.2109]]], device='cuda:0') tensor([-0.0077, -0.5016,  0.7649, -0.3751, -0.4604, -0.1261], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.306640625\n",
            "tcost icost 0.01409912109375 0.0\n",
            "tcost icost 0.05731201171875 0.0\n",
            "tcost icost -0.029296875 0.0\n",
            "tcost icost -0.01270294189453125 0.0\n",
            "tcost icost 0.00495147705078125 0.0\n",
            "tcost icost 0.01654052734375 0.0\n",
            "loss tensor([[0.0457]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.7675,  0.4355,  0.4704],\n",
            "         [ 0.9663,  0.1810,  0.1829],\n",
            "         [ 0.3904, -0.2546,  0.4918],\n",
            "         [ 0.8229,  0.4896, -0.2885],\n",
            "         [ 0.6658,  0.4708,  0.5788],\n",
            "         [ 0.7660,  0.5970,  0.2385]]], device='cuda:0') tensor([-0.9301,  0.3070,  0.4236, -0.3028, -0.5013, -0.1057], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.045654296875\n",
            "tcost icost -0.00988006591796875 0.0\n",
            "tcost icost -0.22314453125 0.0\n",
            "tcost icost -0.031463623046875 0.0\n",
            "tcost icost -0.243408203125 0.0\n",
            "tcost icost -0.06378173828125 0.0\n",
            "tcost icost -0.1776123046875 0.0\n",
            "loss tensor([[-0.5601]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.7570,  0.4459,  0.4776],\n",
            "         [ 0.9755,  0.1547,  0.1568],\n",
            "         [ 0.4185, -0.2598,  0.4805],\n",
            "         [ 0.8404,  0.4838, -0.2442],\n",
            "         [ 0.6611,  0.4694,  0.5853],\n",
            "         [ 0.7634,  0.5868,  0.2700]]], device='cuda:0') tensor([-0.6931,  0.0851,  0.5819, -0.4047, -0.4993, -0.1758], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.56005859375\n",
            "tcost icost -0.004444122314453125 0.0\n",
            "tcost icost -0.1473388671875 0.0\n",
            "tcost icost -0.0140533447265625 0.0\n",
            "tcost icost -0.104736328125 0.0\n",
            "tcost icost -0.05401611328125 0.0\n",
            "tcost icost -0.13525390625 0.0\n",
            "loss tensor([[-0.3398]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.7582,  0.4470,  0.4748],\n",
            "         [ 0.9801,  0.1402,  0.1408],\n",
            "         [ 0.4374, -0.2739,  0.4587],\n",
            "         [ 0.8550,  0.4816, -0.1922],\n",
            "         [ 0.6588,  0.4662,  0.5904],\n",
            "         [ 0.7594,  0.5758,  0.3030]]], device='cuda:0') tensor([-0.1799, -0.2990,  0.8270, -0.5320, -0.4965, -0.2449], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.33984375\n",
            "tcost icost 0.0093231201171875 0.0\n",
            "tcost icost 0.041168212890625 0.0\n",
            "tcost icost 0.1756591796875 0.0\n",
            "tcost icost 0.0113983154296875 0.0\n",
            "tcost icost 0.004840850830078125 0.0\n",
            "tcost icost 0.1328125 0.0\n",
            "loss tensor([[0.2786]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.7729,  0.4372,  0.4600],\n",
            "         [ 0.9837,  0.1285,  0.1255],\n",
            "         [ 0.4718, -0.2868,  0.4268],\n",
            "         [ 0.8627,  0.4861, -0.1397],\n",
            "         [ 0.6805,  0.4558,  0.5737],\n",
            "         [ 0.7636,  0.5608,  0.3200]]], device='cuda:0') tensor([ 0.4465, -0.3982,  0.7759, -0.6676, -0.3322, -0.1918], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.278564453125\n",
            "search tensor([[[ 0.6805,  0.4558,  0.5737],\n",
            "         [ 0.7636,  0.5608,  0.3200],\n",
            "         [ 0.3503, -0.4029, -0.5113],\n",
            "         [-0.4990,  0.4680,  0.2538],\n",
            "         [-0.3828,  0.4395,  0.3886],\n",
            "         [ 0.4537, -0.3354,  0.2146]]], device='cuda:0') tensor([[[-0.3322],\n",
            "         [-0.1918],\n",
            "         [ 0.4001],\n",
            "         [-0.7984],\n",
            "         [-0.6631],\n",
            "         [-0.0377]]], device='cuda:0')\n",
            "tcost icost 0.015838623046875 0.0\n",
            "tcost icost -0.08331298828125 0.0\n",
            "tcost icost -0.01506805419921875 0.0\n",
            "tcost icost -0.024871826171875 0.0\n",
            "tcost icost -0.04095458984375 0.0\n",
            "tcost icost -0.1651611328125 0.0\n",
            "loss tensor([[-0.2139]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.7798,  0.3554,  0.4731],\n",
            "         [ 0.6465,  0.6438,  0.4093],\n",
            "         [ 0.2500, -0.5025, -0.6108],\n",
            "         [-0.3985,  0.5676,  0.3536],\n",
            "         [-0.4824,  0.3391,  0.2882],\n",
            "         [ 0.5532, -0.2351,  0.3144]]], device='cuda:0') tensor([ 0.1639, -0.5700,  0.5748, -0.8500, -0.6668, -0.1101], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2138671875\n",
            "tcost icost 0.044921875 0.0\n",
            "tcost icost 0.10491943359375 0.0\n",
            "tcost icost 0.259521484375 0.0\n",
            "tcost icost 0.012054443359375 0.0\n",
            "tcost icost 0.007175445556640625 0.0\n",
            "tcost icost 0.12158203125 0.0\n",
            "loss tensor([[0.4348]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.8708,  0.2667,  0.3839],\n",
            "         [ 0.5323,  0.6979,  0.4792],\n",
            "         [ 0.1497, -0.5930, -0.7010],\n",
            "         [-0.2982,  0.6662,  0.4403],\n",
            "         [-0.5581,  0.2621,  0.2126],\n",
            "         [ 0.6511, -0.1352,  0.3892]]], device='cuda:0') tensor([ 1.0000, -0.9985,  0.8027, -1.0000, -0.4701, -0.1397], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.434814453125\n",
            "tcost icost -0.0031299591064453125 0.0\n",
            "tcost icost -0.02227783203125 0.0\n",
            "tcost icost -0.294677734375 0.0\n",
            "tcost icost -0.057373046875 0.0\n",
            "tcost icost -0.157958984375 0.0\n",
            "tcost icost -0.0423583984375 0.0\n",
            "loss tensor([[-0.4321]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.9230,  0.2112,  0.3216],\n",
            "         [ 0.4762,  0.7169,  0.5091],\n",
            "         [ 0.1123, -0.6374, -0.7623],\n",
            "         [-0.2167,  0.7425,  0.5068],\n",
            "         [-0.5702,  0.2404,  0.1734],\n",
            "         [ 0.7241, -0.0699,  0.4300]]], device='cuda:0') tensor([ 0.6599, -0.8450,  0.7333, -0.9889, -0.5740, -0.1286], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.43212890625\n",
            "tcost icost 0.07550048828125 0.0\n",
            "tcost icost 0.01090240478515625 0.0\n",
            "tcost icost -0.23291015625 0.0\n",
            "tcost icost -0.054656982421875 0.0\n",
            "tcost icost -0.09521484375 0.0\n",
            "tcost icost -0.03179931640625 0.0\n",
            "loss tensor([[-0.2244]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9402,  0.1876,  0.2842],\n",
            "         [ 0.4747,  0.7211,  0.5046],\n",
            "         [ 0.1050, -0.6247, -0.7738],\n",
            "         [-0.1451,  0.8071,  0.5559],\n",
            "         [-0.5531,  0.2486,  0.1566],\n",
            "         [ 0.7832, -0.0278,  0.4460]]], device='cuda:0') tensor([ 0.1497, -0.5391,  0.6524, -0.9590, -0.6706, -0.1171], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.224365234375\n",
            "tcost icost 0.044036865234375 0.0\n",
            "tcost icost 0.10528564453125 0.0\n",
            "tcost icost 0.252685546875 0.0\n",
            "tcost icost 0.00792694091796875 0.0\n",
            "tcost icost 0.005901336669921875 0.0\n",
            "tcost icost 0.09814453125 0.0\n",
            "loss tensor([[0.4111]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9662,  0.1327,  0.2211],\n",
            "         [ 0.4244,  0.7356,  0.5279],\n",
            "         [ 0.0623, -0.6230, -0.7797],\n",
            "         [-0.0801,  0.8059,  0.5866],\n",
            "         [-0.5672,  0.2165,  0.1092],\n",
            "         [ 0.8454,  0.0319,  0.4666]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.9859, -1.0000, -0.4594, -0.1627], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.4111328125\n",
            "tcost icost -0.0031299591064453125 0.0\n",
            "tcost icost -0.0223236083984375 0.0\n",
            "tcost icost -0.318359375 0.0\n",
            "tcost icost -0.05810546875 0.0\n",
            "tcost icost -0.1356201171875 0.0\n",
            "tcost icost -0.03765869140625 0.0\n",
            "loss tensor([[-0.4343]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9806,  0.0936,  0.1724],\n",
            "         [ 0.3955,  0.7421,  0.5413],\n",
            "         [ 0.0403, -0.6149, -0.7876],\n",
            "         [-0.0199,  0.7963,  0.6046],\n",
            "         [-0.5592,  0.2061,  0.0777],\n",
            "         [ 0.8824,  0.0757,  0.4645]]], device='cuda:0') tensor([ 0.6599, -0.8539,  0.9270, -0.9820, -0.5597, -0.1516], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.434326171875\n",
            "tcost icost 0.07550048828125 0.0\n",
            "tcost icost 0.01055908203125 0.0\n",
            "tcost icost -0.262939453125 0.0\n",
            "tcost icost -0.055877685546875 0.0\n",
            "tcost icost -0.08563232421875 0.0\n",
            "tcost icost -0.0311126708984375 0.0\n",
            "loss tensor([[-0.2432]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9877,  0.0719,  0.1391],\n",
            "         [ 0.3902,  0.7444,  0.5419],\n",
            "         [ 0.0350, -0.5986, -0.8003],\n",
            "         [ 0.0379,  0.7872,  0.6155],\n",
            "         [-0.5390,  0.2092,  0.0573],\n",
            "         [ 0.8908,  0.1053,  0.4421]]], device='cuda:0') tensor([ 0.1848, -0.5760,  0.8494, -0.9513, -0.6355, -0.1416], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2431640625\n",
            "tcost icost 0.04632568359375 0.0\n",
            "tcost icost 0.10235595703125 0.0\n",
            "tcost icost 0.1817626953125 0.0\n",
            "tcost icost -0.00792694091796875 0.0\n",
            "tcost icost 0.0013608932495117188 0.0\n",
            "tcost icost 0.050750732421875 0.0\n",
            "loss tensor([[0.3108]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9869,  0.0832,  0.1384],\n",
            "         [ 0.4290,  0.7438,  0.5114],\n",
            "         [ 0.0703, -0.5476, -0.8338],\n",
            "         [ 0.1014,  0.7870,  0.6085],\n",
            "         [-0.5107,  0.2239,  0.0482],\n",
            "         [ 0.9133,  0.1097,  0.3921]]], device='cuda:0') tensor([-1.0000,  0.7706,  0.2786, -0.8526, -0.6896, -0.0639], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.310791015625\n",
            "tcost icost -0.006084442138671875 0.0\n",
            "tcost icost -0.35693359375 0.0\n",
            "tcost icost -0.0501708984375 0.0\n",
            "tcost icost -0.1405029296875 0.0\n",
            "tcost icost -0.0546875 0.0\n",
            "tcost icost -0.216796875 0.0\n",
            "loss tensor([[-0.6348]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9864,  0.0914,  0.1367],\n",
            "         [ 0.4639,  0.7413,  0.4851],\n",
            "         [ 0.0960, -0.5068, -0.8567],\n",
            "         [ 0.1652,  0.7791,  0.6047],\n",
            "         [-0.4838,  0.2381,  0.0419],\n",
            "         [ 0.9246,  0.1272,  0.3592]]], device='cuda:0') tensor([-0.8961,  0.6742,  0.3729, -0.9751, -0.7073, -0.1425], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.634765625\n",
            "tcost icost -0.0026073455810546875 0.0\n",
            "tcost icost -0.34521484375 0.0\n",
            "tcost icost -0.04901123046875 0.0\n",
            "tcost icost -0.10260009765625 0.0\n",
            "tcost icost -0.051513671875 0.0\n",
            "tcost icost -0.1988525390625 0.0\n",
            "loss tensor([[-0.5791]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9863,  0.0965,  0.1336],\n",
            "         [ 0.4941,  0.7362,  0.4624],\n",
            "         [ 0.1125, -0.4768, -0.8718],\n",
            "         [ 0.2270,  0.7643,  0.6035],\n",
            "         [-0.4585,  0.2511,  0.0373],\n",
            "         [ 0.9280,  0.1531,  0.3398]]], device='cuda:0') tensor([-0.7505,  0.5668,  0.4803, -1.0000, -0.7185, -0.2114], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5791015625\n",
            "tcost icost 0.0017251968383789062 0.0\n",
            "tcost icost -0.326416015625 0.0\n",
            "tcost icost -0.047454833984375 0.0\n",
            "tcost icost -0.07568359375 0.0\n",
            "tcost icost -0.049072265625 0.0\n",
            "tcost icost -0.1781005859375 0.0\n",
            "loss tensor([[-0.5229]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9869,  0.0979,  0.1281],\n",
            "         [ 0.5201,  0.7299,  0.4435],\n",
            "         [ 0.1208, -0.4576, -0.8809],\n",
            "         [ 0.2854,  0.7437,  0.6046],\n",
            "         [-0.4354,  0.2622,  0.0337],\n",
            "         [ 0.9246,  0.1853,  0.3329]]], device='cuda:0') tensor([-0.5423,  0.4396,  0.5981, -1.0000, -0.7225, -0.2820], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.52294921875\n",
            "tcost icost 0.00795745849609375 0.0\n",
            "tcost icost -0.287353515625 0.0\n",
            "tcost icost -0.04327392578125 0.0\n",
            "tcost icost -0.040130615234375 0.0\n",
            "tcost icost -0.046051025390625 0.0\n",
            "tcost icost -0.1512451171875 0.0\n",
            "loss tensor([[-0.4346]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9884,  0.0947,  0.1191],\n",
            "         [ 0.5417,  0.7230,  0.4287],\n",
            "         [ 0.1223, -0.4472, -0.8860],\n",
            "         [ 0.3406,  0.7192,  0.6056],\n",
            "         [-0.4148,  0.2714,  0.0305],\n",
            "         [ 0.9155,  0.2213,  0.3361]]], device='cuda:0') tensor([-0.2728,  0.2600,  0.7112, -1.0000, -0.7235, -0.3518], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4345703125\n",
            "tcost icost 0.01654052734375 0.0\n",
            "tcost icost -0.199462890625 0.0\n",
            "tcost icost -0.0299224853515625 0.0\n",
            "tcost icost 0.0001512765884399414 0.0\n",
            "tcost icost -0.039947509765625 0.0\n",
            "tcost icost -0.126953125 0.0\n",
            "loss tensor([[-0.2883]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9909,  0.0851,  0.1044],\n",
            "         [ 0.5585,  0.7165,  0.4181],\n",
            "         [ 0.1193, -0.4436, -0.8883],\n",
            "         [ 0.3930,  0.6920,  0.6055],\n",
            "         [-0.3963,  0.2777,  0.0264],\n",
            "         [ 0.9020,  0.2588,  0.3457]]], device='cuda:0') tensor([ 0.1476,  0.0388,  0.8138, -1.0000, -0.7173, -0.4139], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.288330078125\n",
            "tcost icost 0.043914794921875 0.0\n",
            "tcost icost 0.01371002197265625 0.0\n",
            "tcost icost 0.03741455078125 0.0\n",
            "tcost icost 0.01079559326171875 0.0\n",
            "tcost icost -0.004886627197265625 0.0\n",
            "tcost icost -0.11859130859375 0.0\n",
            "loss tensor([[0.0212]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9951,  0.0620,  0.0765],\n",
            "         [ 0.5644,  0.7131,  0.4158],\n",
            "         [ 0.1092, -0.4483, -0.8872],\n",
            "         [ 0.4417,  0.6618,  0.6057],\n",
            "         [-0.3857,  0.2726,  0.0118],\n",
            "         [ 0.8821,  0.2996,  0.3635]]], device='cuda:0') tensor([ 1.0000, -0.4915,  1.0000, -0.9940, -0.6648, -0.5220], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.02117919921875\n",
            "tcost icost -0.0031299591064453125 0.0\n",
            "tcost icost -0.002452850341796875 0.0\n",
            "tcost icost -0.2587890625 0.0\n",
            "tcost icost -0.05712890625 0.0\n",
            "tcost icost -0.074951171875 0.0\n",
            "tcost icost -0.033721923828125 0.0\n",
            "loss tensor([[-0.3257]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9972,  0.0485,  0.0574],\n",
            "         [ 0.5812,  0.7029,  0.4100],\n",
            "         [ 0.1088, -0.4451, -0.8889],\n",
            "         [ 0.4891,  0.6301,  0.6031],\n",
            "         [-0.3662,  0.2756,  0.0045],\n",
            "         [ 0.8660,  0.3326,  0.3735]]], device='cuda:0') tensor([ 0.5386, -0.2649,  0.9117, -0.9676, -0.7087, -0.5119], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.32568359375\n",
            "tcost icost 0.0787353515625 0.0\n",
            "tcost icost 0.07745361328125 0.0\n",
            "tcost icost 0.0186767578125 0.0\n",
            "tcost icost -0.03436279296875 0.0\n",
            "tcost icost -0.0256500244140625 0.0\n",
            "tcost icost -0.00879669189453125 0.0\n",
            "loss tensor([[0.1165]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9967,  0.0561,  0.0591],\n",
            "         [ 0.6219,  0.6724,  0.3853],\n",
            "         [ 0.1285, -0.4202, -0.8983],\n",
            "         [ 0.5393,  0.5953,  0.5957],\n",
            "         [-0.3413,  0.2849,  0.0037],\n",
            "         [ 0.8564,  0.3578,  0.3722]]], device='cuda:0') tensor([-0.7280,  0.6804,  0.6009, -0.9204, -0.7423, -0.4944], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.11651611328125\n",
            "tcost icost 0.002353668212890625 0.0\n",
            "tcost icost -0.345458984375 0.0\n",
            "tcost icost -0.04608154296875 0.0\n",
            "tcost icost -0.07037353515625 0.0\n",
            "tcost icost -0.048919677734375 0.0\n",
            "tcost icost -0.134765625 0.0\n",
            "loss tensor([[-0.5088]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9965,  0.0603,  0.0583],\n",
            "         [ 0.6582,  0.6462,  0.3654],\n",
            "         [ 0.1404, -0.4043, -0.9038],\n",
            "         [ 0.5738,  0.5651,  0.5929],\n",
            "         [-0.3181,  0.2921,  0.0025],\n",
            "         [ 0.8432,  0.3818,  0.3784]]], device='cuda:0') tensor([-0.5123,  0.5608,  0.7232, -1.0000, -0.7369, -0.5581], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5087890625\n",
            "tcost icost 0.00905609130859375 0.0\n",
            "tcost icost -0.306640625 0.0\n",
            "tcost icost -0.04205322265625 0.0\n",
            "tcost icost -0.02398681640625 0.0\n",
            "tcost icost -0.0438232421875 0.0\n",
            "tcost icost -0.11676025390625 0.0\n",
            "loss tensor([[-0.4160]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 9.9674e-01,  6.0144e-02,  5.3815e-02],\n",
            "         [ 6.9027e-01,  6.2608e-01,  3.5034e-01],\n",
            "         [ 1.4651e-01, -3.9527e-01, -9.0680e-01],\n",
            "         [ 5.9938e-01,  5.3819e-01,  5.9254e-01],\n",
            "         [-2.9681e-01,  2.9684e-01,  1.4435e-04],\n",
            "         [ 8.2765e-01,  4.0397e-01,  3.8961e-01]]], device='cuda:0') tensor([-0.2267,  0.4003,  0.8269, -1.0000, -0.7266, -0.6131], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.416015625\n",
            "tcost icost 0.01812744140625 0.0\n",
            "tcost icost -0.2119140625 0.0\n",
            "tcost icost -0.030517578125 0.0\n",
            "tcost icost 0.0107421875 0.0\n",
            "tcost icost -0.036865234375 0.0\n",
            "tcost icost -0.099365234375 0.0\n",
            "loss tensor([[-0.2722]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9976,  0.0539,  0.0435],\n",
            "         [ 0.7157,  0.6107,  0.3390],\n",
            "         [ 0.1484, -0.3915, -0.9081],\n",
            "         [ 0.6211,  0.5127,  0.5928],\n",
            "         [-0.2779,  0.2982, -0.0046],\n",
            "         [ 0.8106,  0.4240,  0.4040]]], device='cuda:0') tensor([ 0.2125,  0.1942,  0.9192, -1.0000, -0.7102, -0.6630], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.272216796875\n",
            "tcost icost 0.04852294921875 0.0\n",
            "tcost icost 0.00768280029296875 0.0\n",
            "tcost icost 0.038177490234375 0.0\n",
            "tcost icost 0.0088653564453125 0.0\n",
            "tcost icost -0.0046234130859375 0.0\n",
            "tcost icost -0.07769775390625 0.0\n",
            "loss tensor([[0.0439]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9993,  0.0335,  0.0186],\n",
            "         [ 0.7199,  0.6079,  0.3350],\n",
            "         [ 0.1412, -0.3948, -0.9078],\n",
            "         [ 0.6311,  0.4952,  0.5970],\n",
            "         [-0.2639,  0.2900, -0.0202],\n",
            "         [ 0.7943,  0.4412,  0.4176]]], device='cuda:0') tensor([ 1.0000, -0.3932,  1.0000, -1.0000, -0.6440, -0.6982], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.043914794921875\n",
            "search tensor([[[-0.2639,  0.2900, -0.0202],\n",
            "         [ 0.7943,  0.4412,  0.4176],\n",
            "         [-0.2353,  0.2096,  0.2127],\n",
            "         [ 0.2341,  0.1850,  0.4737],\n",
            "         [ 0.3456,  0.0412,  0.0846],\n",
            "         [-0.0675,  0.4693, -0.2807]]], device='cuda:0') tensor([[[-0.6440],\n",
            "         [-0.6982],\n",
            "         [ 0.6012],\n",
            "         [ 0.6777],\n",
            "         [ 0.3314],\n",
            "         [-0.6220]]], device='cuda:0')\n",
            "tcost icost -0.0002567768096923828 0.0\n",
            "tcost icost -0.0189361572265625 0.0\n",
            "tcost icost 0.0167388916015625 0.0\n",
            "tcost icost -0.2374267578125 0.0\n",
            "tcost icost -0.054901123046875 0.0\n",
            "tcost icost -0.006748199462890625 0.0\n",
            "loss tensor([[-0.2169]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[-0.3637,  0.1897, -0.1202],\n",
            "         [ 0.6798,  0.5300,  0.5069],\n",
            "         [-0.3351,  0.1093,  0.1125],\n",
            "         [ 0.3339,  0.2848,  0.5732],\n",
            "         [ 0.4452, -0.0589, -0.0155],\n",
            "         [ 0.0326,  0.5688, -0.1805]]], device='cuda:0') tensor([-0.0805, -1.0000,  0.8429,  0.5477,  0.3677, -0.6325], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2169189453125\n",
            "tcost icost 0.016937255859375 0.0\n",
            "tcost icost 0.02545166015625 0.0\n",
            "tcost icost -0.048919677734375 0.0\n",
            "tcost icost 0.06011962890625 0.0\n",
            "tcost icost 0.0263671875 0.0\n",
            "tcost icost 0.0018215179443359375 0.0\n",
            "loss tensor([[0.0624]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.3603,  0.2156, -0.1033],\n",
            "         [ 0.7347,  0.4969,  0.4620],\n",
            "         [-0.2951,  0.1338,  0.1049],\n",
            "         [ 0.4287,  0.3195,  0.5825],\n",
            "         [ 0.3738, -0.1544, -0.1111],\n",
            "         [ 0.0850,  0.5939, -0.1978]]], device='cuda:0') tensor([-0.9233, -0.3105,  0.4554,  0.6211,  0.4593, -0.6209], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.06243896484375\n",
            "tcost icost -0.00688934326171875 0.0\n",
            "tcost icost -0.1253662109375 0.0\n",
            "tcost icost -0.023651123046875 0.0\n",
            "tcost icost -0.379150390625 0.0\n",
            "tcost icost -0.057769775390625 0.0\n",
            "tcost icost -0.047271728515625 0.0\n",
            "loss tensor([[-0.4810]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.3664,  0.2204, -0.1061],\n",
            "         [ 0.7661,  0.4764,  0.4315],\n",
            "         [-0.2707,  0.1379,  0.0800],\n",
            "         [ 0.5137,  0.3665,  0.6023],\n",
            "         [ 0.3008, -0.2459, -0.1961],\n",
            "         [ 0.1590,  0.6563, -0.1468]]], device='cuda:0') tensor([-0.7126, -0.5519,  0.5720,  0.5568,  0.4928, -0.6604], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.48095703125\n",
            "tcost icost -0.0017786026000976562 0.0\n",
            "tcost icost -0.053924560546875 0.0\n",
            "tcost icost -0.0032672882080078125 0.0\n",
            "tcost icost -0.303466796875 0.0\n",
            "tcost icost -0.056182861328125 0.0\n",
            "tcost icost -0.019744873046875 0.0\n",
            "loss tensor([[-0.3228]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.3875,  0.2034, -0.1294],\n",
            "         [ 0.7743,  0.4743,  0.4189],\n",
            "         [-0.2647,  0.1197,  0.0359],\n",
            "         [ 0.5769,  0.4206,  0.6305],\n",
            "         [ 0.2256, -0.3345, -0.2743],\n",
            "         [ 0.2385,  0.7277, -0.0795]]], device='cuda:0') tensor([-0.3445, -0.9039,  0.7556,  0.4609,  0.5215, -0.6880], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.32275390625\n",
            "tcost icost 0.00714874267578125 0.0\n",
            "tcost icost 0.00981903076171875 0.0\n",
            "tcost icost 0.130615234375 0.0\n",
            "tcost icost 0.1932373046875 0.0\n",
            "tcost icost -0.007152557373046875 0.0\n",
            "tcost icost 0.006103515625 0.0\n",
            "loss tensor([[0.2617]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.4262,  0.1626, -0.1748],\n",
            "         [ 0.7766,  0.4780,  0.4105],\n",
            "         [-0.2495,  0.0843, -0.0274],\n",
            "         [ 0.6178,  0.4645,  0.6345],\n",
            "         [ 0.1449, -0.4255, -0.3578],\n",
            "         [ 0.3178,  0.7980, -0.0096]]], device='cuda:0') tensor([ 0.3338, -1.0000,  0.8939,  0.3061,  0.5680, -0.7045], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.26171875\n",
            "tcost icost 0.034820556640625 0.0\n",
            "tcost icost 0.0194244384765625 0.0\n",
            "tcost icost -0.279296875 0.0\n",
            "tcost icost -0.02880859375 0.0\n",
            "tcost icost -0.14111328125 0.0\n",
            "tcost icost -0.0438232421875 0.0\n",
            "loss tensor([[-0.3132]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.4564,  0.1512, -0.1905],\n",
            "         [ 0.8068,  0.4514,  0.3812],\n",
            "         [-0.2263,  0.0682, -0.0675],\n",
            "         [ 0.6465,  0.4720,  0.5994],\n",
            "         [ 0.1769, -0.4278, -0.3701],\n",
            "         [ 0.3864,  0.8532,  0.0397]]], device='cuda:0') tensor([-0.1868, -0.5662,  0.7890,  0.4042,  0.4551, -0.6930], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.313232421875\n",
            "tcost icost 0.012908935546875 0.0\n",
            "tcost icost 0.022857666015625 0.0\n",
            "tcost icost 0.14404296875 0.0\n",
            "tcost icost 0.1907958984375 0.0\n",
            "tcost icost -0.0129852294921875 0.0\n",
            "tcost icost -0.0013685226440429688 0.0\n",
            "loss tensor([[0.2798]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.5029,  0.1150, -0.2279],\n",
            "         [ 0.8172,  0.4432,  0.3684],\n",
            "         [-0.2019,  0.0339, -0.1234],\n",
            "         [ 0.6748,  0.4770,  0.5631],\n",
            "         [ 0.1979, -0.4380, -0.3911],\n",
            "         [ 0.4465,  0.8900,  0.0926]]], device='cuda:0') tensor([ 0.7337, -1.0000,  0.9873,  0.3135,  0.4899, -0.7104], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.27978515625\n",
            "tcost icost 0.03741455078125 0.0\n",
            "tcost icost -0.0049285888671875 0.0\n",
            "tcost icost -0.364990234375 0.0\n",
            "tcost icost -0.03741455078125 0.0\n",
            "tcost icost -0.219482421875 0.0\n",
            "tcost icost -0.0518798828125 0.0\n",
            "loss tensor([[-0.4648]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.5275,  0.0959, -0.2524],\n",
            "         [ 0.8384,  0.4209,  0.3464],\n",
            "         [-0.1735,  0.0098, -0.1678],\n",
            "         [ 0.7088,  0.4735,  0.5229],\n",
            "         [ 0.2426, -0.4179, -0.3812],\n",
            "         [ 0.4739,  0.8715,  0.1263]]], device='cuda:0') tensor([ 0.2913, -0.7843,  0.9409,  0.3755,  0.3859, -0.7056], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.46484375\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.034912109375 0.0\n",
            "tcost icost -0.210205078125 0.0\n",
            "tcost icost -0.0203857421875 0.0\n",
            "tcost icost -0.0244598388671875 0.0\n",
            "tcost icost -0.03436279296875 0.0\n",
            "loss tensor([[-0.1580]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.5546,  0.0986, -0.2555],\n",
            "         [ 0.8740,  0.3774,  0.3061],\n",
            "         [-0.1365,  0.0044, -0.1945],\n",
            "         [ 0.7549,  0.4583,  0.4691],\n",
            "         [ 0.3001, -0.3796, -0.3531],\n",
            "         [ 0.4992,  0.8543,  0.1448]]], device='cuda:0') tensor([-0.4455, -0.2618,  0.7801,  0.4955,  0.2903, -0.6934], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.157958984375\n",
            "tcost icost 0.00429534912109375 0.0\n",
            "tcost icost -0.08660888671875 0.0\n",
            "tcost icost 0.0005164146423339844 0.0\n",
            "tcost icost -0.1927490234375 0.0\n",
            "tcost icost -0.05255126953125 0.0\n",
            "tcost icost -0.0162200927734375 0.0\n",
            "loss tensor([[-0.2578]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.5825,  0.0855, -0.2716],\n",
            "         [ 0.8920,  0.3523,  0.2831],\n",
            "         [-0.1193, -0.0176, -0.2312],\n",
            "         [ 0.7790,  0.4494,  0.4373],\n",
            "         [ 0.3502, -0.3488, -0.3297],\n",
            "         [ 0.5234,  0.8349,  0.1705]]], device='cuda:0') tensor([ 0.2014, -0.7091,  1.0000,  0.3651,  0.3152, -0.7126], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2578125\n",
            "tcost icost 0.028778076171875 0.0\n",
            "tcost icost 0.05242919921875 0.0\n",
            "tcost icost -0.1641845703125 0.0\n",
            "tcost icost -0.0122833251953125 0.0\n",
            "tcost icost 0.007656097412109375 0.0\n",
            "tcost icost -0.02557373046875 0.0\n",
            "loss tensor([[-0.0760]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.6077,  0.0913, -0.2706],\n",
            "         [ 0.9197,  0.3084,  0.2428],\n",
            "         [-0.0877, -0.0221, -0.2550],\n",
            "         [ 0.8136,  0.4295,  0.3920],\n",
            "         [ 0.4049, -0.3108, -0.2994],\n",
            "         [ 0.5476,  0.8169,  0.1813]]], device='cuda:0') tensor([-0.5769, -0.1090,  0.7853,  0.4647,  0.2855, -0.6993], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0760498046875\n",
            "tcost icost 0.001251220703125 0.0\n",
            "tcost icost -0.1513671875 0.0\n",
            "tcost icost -0.013824462890625 0.0\n",
            "tcost icost -0.23974609375 0.0\n",
            "tcost icost -0.054901123046875 0.0\n",
            "tcost icost -0.0262908935546875 0.0\n",
            "loss tensor([[-0.3726]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.6298,  0.0870, -0.2781],\n",
            "         [ 0.9365,  0.2775,  0.2143],\n",
            "         [-0.0693, -0.0373, -0.2851],\n",
            "         [ 0.8329,  0.4164,  0.3647],\n",
            "         [ 0.4535, -0.2800, -0.2732],\n",
            "         [ 0.5694,  0.7968,  0.2020]]], device='cuda:0') tensor([-0.1313, -0.4388,  0.9703,  0.3565,  0.3126, -0.7250], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.37255859375\n",
            "tcost icost 0.014862060546875 0.0\n",
            "tcost icost 0.0198211669921875 0.0\n",
            "tcost icost 0.1455078125 0.0\n",
            "tcost icost 0.1749267578125 0.0\n",
            "tcost icost -0.00476837158203125 0.0\n",
            "tcost icost -0.00469207763671875 0.0\n",
            "loss tensor([[0.2725]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.6597,  0.0764, -0.2902],\n",
            "         [ 0.9516,  0.2461,  0.1840],\n",
            "         [-0.0336, -0.0447, -0.3160],\n",
            "         [ 0.8633,  0.3894,  0.3210],\n",
            "         [ 0.4936, -0.2591, -0.2618],\n",
            "         [ 0.5893,  0.7761,  0.2245]]], device='cuda:0') tensor([ 0.1074, -0.5951,  0.8292,  0.4653,  0.3573, -0.7392], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2724609375\n",
            "tcost icost 0.0252532958984375 0.0\n",
            "tcost icost 0.0772705078125 0.0\n",
            "tcost icost 0.0224761962890625 0.0\n",
            "tcost icost 0.038421630859375 0.0\n",
            "tcost icost 0.026153564453125 0.0\n",
            "tcost icost 0.002422332763671875 0.0\n",
            "loss tensor([[0.1597]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.6831,  0.0851, -0.2863],\n",
            "         [ 0.9712,  0.1959,  0.1356],\n",
            "         [ 0.0189, -0.0300, -0.3385],\n",
            "         [ 0.8934,  0.3577,  0.2719],\n",
            "         [ 0.5244, -0.2489, -0.2648],\n",
            "         [ 0.6096,  0.7568,  0.2360]]], device='cuda:0') tensor([-0.8276,  0.2155,  0.4113,  0.5140,  0.4349, -0.7294], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.15966796875\n",
            "tcost icost -0.0046234130859375 0.0\n",
            "tcost icost -0.2412109375 0.0\n",
            "tcost icost -0.03619384765625 0.0\n",
            "tcost icost -0.341552734375 0.0\n",
            "tcost icost -0.055419921875 0.0\n",
            "tcost icost -0.050567626953125 0.0\n",
            "loss tensor([[-0.5659]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.6988,  0.0898, -0.2862],\n",
            "         [ 0.9833,  0.1546,  0.0956],\n",
            "         [ 0.0651, -0.0208, -0.3635],\n",
            "         [ 0.9117,  0.3338,  0.2393],\n",
            "         [ 0.5512, -0.2443, -0.2704],\n",
            "         [ 0.6269,  0.7354,  0.2572]]], device='cuda:0') tensor([-0.6664,  0.0135,  0.5095,  0.4455,  0.4693, -0.7671], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.56591796875\n",
            "tcost icost -0.0009570121765136719 0.0\n",
            "tcost icost -0.1888427734375 0.0\n",
            "tcost icost -0.0270843505859375 0.0\n",
            "tcost icost -0.299072265625 0.0\n",
            "tcost icost -0.053192138671875 0.0\n",
            "tcost icost -0.036376953125 0.0\n",
            "loss tensor([[-0.4673]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.7087,  0.0888, -0.2913],\n",
            "         [ 0.9902,  0.1236,  0.0647],\n",
            "         [ 0.1032, -0.0181, -0.3919],\n",
            "         [ 0.9212,  0.3189,  0.2228],\n",
            "         [ 0.5740, -0.2447, -0.2788],\n",
            "         [ 0.6410,  0.7135,  0.2830]]], device='cuda:0') tensor([-0.3763, -0.2311,  0.6364,  0.3581,  0.5035, -0.7928], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.46728515625\n",
            "tcost icost 0.006031036376953125 0.0\n",
            "tcost icost -0.076416015625 0.0\n",
            "tcost icost -0.0003695487976074219 0.0\n",
            "tcost icost -0.1646728515625 0.0\n",
            "tcost icost -0.042724609375 0.0\n",
            "tcost icost -0.01554107666015625 0.0\n",
            "loss tensor([[-0.2203]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.7124,  0.0759, -0.3069],\n",
            "         [ 0.9929,  0.1088,  0.0476],\n",
            "         [ 0.1273, -0.0261, -0.4285],\n",
            "         [ 0.9217,  0.3168,  0.2240],\n",
            "         [ 0.5932, -0.2497, -0.2899],\n",
            "         [ 0.6520,  0.6912,  0.3118]]], device='cuda:0') tensor([ 0.2833, -0.6581,  0.8681,  0.2204,  0.5358, -0.8161], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2203369140625\n",
            "tcost icost 0.032012939453125 0.0\n",
            "tcost icost 0.050933837890625 0.0\n",
            "tcost icost -0.169921875 0.0\n",
            "tcost icost -0.0196380615234375 0.0\n",
            "tcost icost -0.059967041015625 0.0\n",
            "tcost icost -0.038970947265625 0.0\n",
            "loss tensor([[-0.1365]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.7259,  0.0795, -0.3067],\n",
            "         [ 0.9966,  0.0802,  0.0192],\n",
            "         [ 0.1568, -0.0255, -0.4510],\n",
            "         [ 0.9300,  0.3003,  0.2120],\n",
            "         [ 0.6252, -0.2347, -0.2785],\n",
            "         [ 0.6669,  0.6683,  0.3297]]], device='cuda:0') tensor([-0.5536, -0.0355,  0.7088,  0.3374,  0.4372, -0.8023], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.136474609375\n",
            "tcost icost 0.0016469955444335938 0.0\n",
            "tcost icost -0.1632080078125 0.0\n",
            "tcost icost -0.0170135498046875 0.0\n",
            "tcost icost -0.227294921875 0.0\n",
            "tcost icost -0.0496826171875 0.0\n",
            "tcost icost -0.0259857177734375 0.0\n",
            "loss tensor([[-0.3728]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-7.3156e-01,  7.5388e-02, -3.1363e-01],\n",
            "         [ 9.9802e-01,  6.2837e-02,  7.2998e-04],\n",
            "         [ 1.7729e-01, -3.1858e-02, -4.7915e-01],\n",
            "         [ 9.3153e-01,  2.9429e-01,  2.1362e-01],\n",
            "         [ 6.5330e-01, -2.2493e-01, -2.7024e-01],\n",
            "         [ 6.7632e-01,  6.4699e-01,  3.5213e-01]]], device='cuda:0') tensor([-0.1161, -0.3446,  0.8882,  0.2294,  0.4693, -0.8298], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.372802734375\n",
            "tcost icost 0.0152587890625 0.0\n",
            "tcost icost 0.01229095458984375 0.0\n",
            "tcost icost 0.1300048828125 0.0\n",
            "tcost icost 0.1553955078125 0.0\n",
            "tcost icost 9.137392044067383e-05 0.0\n",
            "tcost icost 0.0020389556884765625 0.0\n",
            "loss tensor([[0.2461]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.7376,  0.0613, -0.3289],\n",
            "         [ 0.9985,  0.0535, -0.0132],\n",
            "         [ 0.2061, -0.0343, -0.5118],\n",
            "         [ 0.9381,  0.2787,  0.2054],\n",
            "         [ 0.6751, -0.2228, -0.2741],\n",
            "         [ 0.6824,  0.6278,  0.3744]]], device='cuda:0') tensor([ 0.4122, -0.6700,  0.8549,  0.3044,  0.5172, -0.8444], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.24609375\n",
            "search tensor([[[ 0.6751, -0.2228, -0.2741],\n",
            "         [ 0.6824,  0.6278,  0.3744],\n",
            "         [-0.1707, -0.0053,  0.5260],\n",
            "         [-0.1289, -0.1786, -0.5067],\n",
            "         [-0.3066, -0.4536, -0.2747],\n",
            "         [ 0.2866, -0.5289, -0.2053]]], device='cuda:0') tensor([[[ 0.5172],\n",
            "         [-0.8444],\n",
            "         [ 0.0623],\n",
            "         [-0.4222],\n",
            "         [-0.6459],\n",
            "         [ 0.3287]]], device='cuda:0')\n",
            "tcost icost 0.13818359375 0.0\n",
            "tcost icost 0.0182037353515625 0.0\n",
            "tcost icost -0.1121826171875 0.0\n",
            "tcost icost -0.036895751953125 0.0\n",
            "tcost icost -0.0928955078125 0.0\n",
            "tcost icost -0.0190582275390625 0.0\n",
            "loss tensor([[-0.0355]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.7744, -0.1225, -0.1738],\n",
            "         [ 0.7817,  0.5272,  0.2740],\n",
            "         [-0.0705,  0.0947,  0.6255],\n",
            "         [-0.2287, -0.2784, -0.6061],\n",
            "         [-0.2063, -0.3531, -0.1744],\n",
            "         [ 0.1863, -0.6284, -0.3051]]], device='cuda:0') tensor([ 0.0638, -0.4194, -0.1252, -0.3507, -0.7622,  0.3499], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.03546142578125\n",
            "tcost icost 0.036163330078125 0.0\n",
            "tcost icost 0.0860595703125 0.0\n",
            "tcost icost 0.059417724609375 0.0\n",
            "tcost icost 0.052703857421875 0.0\n",
            "tcost icost -0.0190582275390625 0.0\n",
            "tcost icost -0.2259521484375 0.0\n",
            "loss tensor([[0.0543]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.7622, -0.1454, -0.2000],\n",
            "         [ 0.8092,  0.5184,  0.2767],\n",
            "         [-0.0385,  0.0690,  0.6086],\n",
            "         [-0.1799, -0.3145, -0.6857],\n",
            "         [-0.1436, -0.3085, -0.1371],\n",
            "         [ 0.2444, -0.5807, -0.2625]]], device='cuda:0') tensor([ 1.0000, -0.8962,  0.1846, -0.3211, -0.7457,  0.2650], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0543212890625\n",
            "tcost icost 0.032135009765625 0.0\n",
            "tcost icost -0.0225067138671875 0.0\n",
            "tcost icost -0.2296142578125 0.0\n",
            "tcost icost -0.05322265625 0.0\n",
            "tcost icost -0.1334228515625 0.0\n",
            "tcost icost -0.03106689453125 0.0\n",
            "loss tensor([[-0.3188]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.7781, -0.1381, -0.2007],\n",
            "         [ 0.8331,  0.4853,  0.2655],\n",
            "         [ 0.0135,  0.0643,  0.6114],\n",
            "         [-0.1493, -0.3602, -0.7338],\n",
            "         [-0.0687, -0.2399, -0.0719],\n",
            "         [ 0.2768, -0.5611, -0.2502]]], device='cuda:0') tensor([ 0.5559, -0.7881,  0.0707, -0.2930, -0.8701,  0.2836], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.31884765625\n",
            "tcost icost 0.138671875 0.0\n",
            "tcost icost 0.016815185546875 0.0\n",
            "tcost icost -0.149169921875 0.0\n",
            "tcost icost -0.042572021484375 0.0\n",
            "tcost icost -0.072509765625 0.0\n",
            "tcost icost -0.018798828125 0.0\n",
            "loss tensor([[-0.0568]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.8160, -0.1045, -0.1753],\n",
            "         [ 0.8748,  0.4267,  0.2297],\n",
            "         [ 0.0753,  0.0834,  0.6341],\n",
            "         [-0.1399, -0.4228, -0.7787],\n",
            "         [ 0.0149, -0.1608,  0.0049],\n",
            "         [ 0.2903, -0.5625, -0.2627]]], device='cuda:0') tensor([ 0.0486, -0.4456, -0.0962, -0.2278, -0.9657,  0.3051], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0567626953125\n",
            "tcost icost 0.034149169921875 0.0\n",
            "tcost icost 0.0826416015625 0.0\n",
            "tcost icost 0.06561279296875 0.0\n",
            "tcost icost 0.07098388671875 0.0\n",
            "tcost icost -0.0306854248046875 0.0\n",
            "tcost icost -0.1981201171875 0.0\n",
            "loss tensor([[0.0762]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.8098, -0.1166, -0.1947],\n",
            "         [ 0.8779,  0.4165,  0.2362],\n",
            "         [ 0.1407,  0.0581,  0.6108],\n",
            "         [-0.0929, -0.4663, -0.8401],\n",
            "         [ 0.0881, -0.0958,  0.0702],\n",
            "         [ 0.3328, -0.5267, -0.2372]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.3335, -0.1499, -0.9780,  0.2260], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.07623291015625\n",
            "tcost icost 0.032135009765625 0.0\n",
            "tcost icost -0.0246124267578125 0.0\n",
            "tcost icost -0.25439453125 0.0\n",
            "tcost icost -0.051605224609375 0.0\n",
            "tcost icost -0.0941162109375 0.0\n",
            "tcost icost -0.023712158203125 0.0\n",
            "loss tensor([[-0.3093]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.8162, -0.1152, -0.2022],\n",
            "         [ 0.8857,  0.3990,  0.2376],\n",
            "         [ 0.2112,  0.0443,  0.5987],\n",
            "         [-0.0549, -0.5020, -0.8631],\n",
            "         [ 0.1687, -0.0208,  0.1450],\n",
            "         [ 0.3611, -0.5079, -0.2306]]], device='cuda:0') tensor([ 0.5806, -0.9300,  0.2091, -0.1121, -1.0000,  0.2492], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.309326171875\n",
            "tcost icost 0.1353759765625 0.0\n",
            "tcost icost 0.00946807861328125 0.0\n",
            "tcost icost -0.2059326171875 0.0\n",
            "tcost icost -0.0433349609375 0.0\n",
            "tcost icost -0.0677490234375 0.0\n",
            "tcost icost -0.02056884765625 0.0\n",
            "loss tensor([[-0.1110]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.8367, -0.0982, -0.1942],\n",
            "         [ 0.9022,  0.3670,  0.2264],\n",
            "         [ 0.2822,  0.0427,  0.5979],\n",
            "         [-0.0344, -0.5278, -0.8487],\n",
            "         [ 0.2518,  0.0590,  0.2241],\n",
            "         [ 0.3779, -0.5029, -0.2387]]], device='cuda:0') tensor([ 0.1335, -0.7086,  0.0412, -0.0491, -1.0000,  0.2710], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.11102294921875\n",
            "tcost icost 0.043731689453125 0.0\n",
            "tcost icost 0.07135009765625 0.0\n",
            "tcost icost 0.220947265625 0.0\n",
            "tcost icost 0.045806884765625 0.0\n",
            "tcost icost 0.004940032958984375 0.0\n",
            "tcost icost -0.07818603515625 0.0\n",
            "loss tensor([[0.2773]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.8588, -0.0850, -0.1928],\n",
            "         [ 0.9217,  0.3301,  0.2038],\n",
            "         [ 0.3456,  0.0485,  0.5835],\n",
            "         [-0.0031, -0.5410, -0.8410],\n",
            "         [ 0.3218,  0.0783,  0.2308],\n",
            "         [ 0.4191, -0.4647, -0.2185]]], device='cuda:0') tensor([ 0.3909, -0.4581,  0.0246, -0.0972, -0.8451,  0.1552], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.27734375\n",
            "tcost icost 0.11114501953125 0.0\n",
            "tcost icost 0.06524658203125 0.0\n",
            "tcost icost 0.17578125 0.0\n",
            "tcost icost 0.01505279541015625 0.0\n",
            "tcost icost 0.01012420654296875 0.0\n",
            "tcost icost 0.1031494140625 0.0\n",
            "loss tensor([[0.3906]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9039, -0.0416, -0.1559],\n",
            "         [ 0.9504,  0.2711,  0.1522],\n",
            "         [ 0.3977,  0.0858,  0.6079],\n",
            "         [-0.0207, -0.5578, -0.8297],\n",
            "         [ 0.3970,  0.1187,  0.2617],\n",
            "         [ 0.4342, -0.4630, -0.2440]]], device='cuda:0') tensor([-1.0000,  1.0000, -0.5916,  0.1398, -1.0000,  0.3059], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.390625\n",
            "tcost icost -0.00801849365234375 0.0\n",
            "tcost icost -0.364501953125 0.0\n",
            "tcost icost -0.05682373046875 0.0\n",
            "tcost icost -0.30078125 0.0\n",
            "tcost icost -0.06500244140625 0.0\n",
            "tcost icost -0.263916015625 0.0\n",
            "loss tensor([[-0.7998]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9454, -0.0029, -0.1227],\n",
            "         [ 0.9698,  0.2187,  0.1076],\n",
            "         [ 0.4464,  0.1177,  0.6286],\n",
            "         [-0.0092, -0.5582, -0.8297],\n",
            "         [ 0.4669,  0.1559,  0.2921],\n",
            "         [ 0.4641, -0.4453, -0.2502]]], device='cuda:0') tensor([-0.9420,  0.9026, -0.5615,  0.0436, -1.0000,  0.2370], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.7998046875\n",
            "tcost icost -0.006374359130859375 0.0\n",
            "tcost icost -0.352783203125 0.0\n",
            "tcost icost -0.05755615234375 0.0\n",
            "tcost icost -0.284423828125 0.0\n",
            "tcost icost -0.06488037109375 0.0\n",
            "tcost icost -0.25537109375 0.0\n",
            "loss tensor([[-0.7710]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9834,  0.0314, -0.0927],\n",
            "         [ 0.9825,  0.1728,  0.0697],\n",
            "         [ 0.4926,  0.1447,  0.6461],\n",
            "         [ 0.0205, -0.5459, -0.8376],\n",
            "         [ 0.5316,  0.1904,  0.3225],\n",
            "         [ 0.5050, -0.4149, -0.2408]]], device='cuda:0') tensor([-0.8634,  0.7990, -0.5263, -0.0528, -1.0000,  0.1663], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.77099609375\n",
            "tcost icost -0.0028209686279296875 0.0\n",
            "tcost icost -0.338134765625 0.0\n",
            "tcost icost -0.0589599609375 0.0\n",
            "tcost icost -0.262451171875 0.0\n",
            "tcost icost -0.06475830078125 0.0\n",
            "tcost icost -0.24365234375 0.0\n",
            "loss tensor([[-0.7329]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9961,  0.0598, -0.0651],\n",
            "         [ 0.9903,  0.1334,  0.0380],\n",
            "         [ 0.5369,  0.1669,  0.6605],\n",
            "         [ 0.0633, -0.5219, -0.8506],\n",
            "         [ 0.5910,  0.2220,  0.3523],\n",
            "         [ 0.5545, -0.3738, -0.2193]]], device='cuda:0') tensor([-0.7342,  0.6857, -0.4790, -0.1550, -1.0000,  0.0941], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.73291015625\n",
            "tcost icost 0.0008783340454101562 0.0\n",
            "tcost icost -0.316650390625 0.0\n",
            "tcost icost -0.059478759765625 0.0\n",
            "tcost icost -0.234375 0.0\n",
            "tcost icost -0.06427001953125 0.0\n",
            "tcost icost -0.2279052734375 0.0\n",
            "loss tensor([[-0.6797]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9958,  0.0819, -0.0416],\n",
            "         [ 0.9949,  0.1002,  0.0123],\n",
            "         [ 0.5800,  0.1847,  0.6720],\n",
            "         [ 0.1161, -0.4882, -0.8650],\n",
            "         [ 0.6454,  0.2507,  0.3811],\n",
            "         [ 0.6104, -0.3241, -0.1879]]], device='cuda:0') tensor([-0.5456,  0.5657, -0.4219, -0.2594, -1.0000,  0.0228], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6796875\n",
            "tcost icost 0.005870819091796875 0.0\n",
            "tcost icost -0.280517578125 0.0\n",
            "tcost icost -0.05908203125 0.0\n",
            "tcost icost -0.1949462890625 0.0\n",
            "tcost icost -0.06292724609375 0.0\n",
            "tcost icost -0.2098388671875 0.0\n",
            "loss tensor([[-0.6016]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9949,  0.0980, -0.0238],\n",
            "         [ 0.9973,  0.0732, -0.0077],\n",
            "         [ 0.6224,  0.1981,  0.6807],\n",
            "         [ 0.1771, -0.4472, -0.8767],\n",
            "         [ 0.6950,  0.2764,  0.4089],\n",
            "         [ 0.6707, -0.2677, -0.1483]]], device='cuda:0') tensor([-0.2614,  0.4306, -0.3450, -0.3650, -1.0000, -0.0458], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6015625\n",
            "tcost icost 0.0156097412109375 0.0\n",
            "tcost icost -0.1949462890625 0.0\n",
            "tcost icost -0.05242919921875 0.0\n",
            "tcost icost -0.1331787109375 0.0\n",
            "tcost icost -0.058197021484375 0.0\n",
            "tcost icost -0.1910400390625 0.0\n",
            "loss tensor([[-0.4502]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9943,  0.1058, -0.0143],\n",
            "         [ 0.9983,  0.0534, -0.0211],\n",
            "         [ 0.6651,  0.2067,  0.6862],\n",
            "         [ 0.2459, -0.3996, -0.8831],\n",
            "         [ 0.7406,  0.2993,  0.4350],\n",
            "         [ 0.7338, -0.2059, -0.1028]]], device='cuda:0') tensor([ 0.2445,  0.2623, -0.2394, -0.4795, -1.0000, -0.1103], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4501953125\n",
            "tcost icost 0.06488037109375 0.0\n",
            "tcost icost 0.08392333984375 0.0\n",
            "tcost icost -0.0012416839599609375 0.0\n",
            "tcost icost -0.01540374755859375 0.0\n",
            "tcost icost -0.0390625 0.0\n",
            "tcost icost -0.192138671875 0.0\n",
            "loss tensor([[-0.0109]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9949,  0.0985, -0.0214],\n",
            "         [ 0.9986,  0.0475, -0.0235],\n",
            "         [ 0.7036,  0.2075,  0.6796],\n",
            "         [ 0.3167, -0.3504, -0.8814],\n",
            "         [ 0.7831,  0.3192,  0.4588],\n",
            "         [ 0.7986, -0.1395, -0.0524]]], device='cuda:0') tensor([ 1.0000, -0.1923, -0.0726, -0.5167, -1.0000, -0.1824], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.01092529296875\n",
            "tcost icost 0.01450347900390625 0.0\n",
            "tcost icost 0.0034389495849609375 0.0\n",
            "tcost icost -0.0771484375 0.0\n",
            "tcost icost -0.040771484375 0.0\n",
            "tcost icost -0.08172607421875 0.0\n",
            "tcost icost -0.03021240234375 0.0\n",
            "loss tensor([[-0.1461]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9948,  0.1004, -0.0199],\n",
            "         [ 0.9989,  0.0360, -0.0312],\n",
            "         [ 0.7185,  0.2090,  0.6633],\n",
            "         [ 0.3736, -0.3066, -0.8754],\n",
            "         [ 0.8114,  0.3368,  0.4778],\n",
            "         [ 0.8569, -0.0817, -0.0096]]], device='cuda:0') tensor([ 0.3481,  0.1229, -0.2580, -0.4767, -1.0000, -0.1698], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1461181640625\n",
            "tcost icost 0.1016845703125 0.0\n",
            "tcost icost 0.158935546875 0.0\n",
            "tcost icost 0.0230865478515625 0.0\n",
            "tcost icost 0.01294708251953125 0.0\n",
            "tcost icost -0.0206146240234375 0.0\n",
            "tcost icost -0.2000732421875 0.0\n",
            "loss tensor([[0.1414]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9950,  0.0969, -0.0243],\n",
            "         [ 0.9990,  0.0309, -0.0337],\n",
            "         [ 0.7379,  0.2057,  0.6428],\n",
            "         [ 0.4367, -0.2653, -0.8596],\n",
            "         [ 0.8121,  0.3380,  0.4756],\n",
            "         [ 0.9157, -0.0180,  0.0387]]], device='cuda:0') tensor([ 0.9097, -0.0320, -0.0970, -0.4880, -0.9879, -0.2623], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.141357421875\n",
            "tcost icost 0.04583740234375 0.0\n",
            "tcost icost 0.0167999267578125 0.0\n",
            "tcost icost 0.0018262863159179688 0.0\n",
            "tcost icost -0.027069091796875 0.0\n",
            "tcost icost -0.06756591796875 0.0\n",
            "tcost icost -0.025787353515625 0.0\n",
            "loss tensor([[-0.0168]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9944,  0.1044, -0.0179],\n",
            "         [ 0.9989,  0.0188, -0.0429],\n",
            "         [ 0.7478,  0.2084,  0.6304],\n",
            "         [ 0.4809, -0.2311, -0.8458],\n",
            "         [ 0.8093,  0.3427,  0.4770],\n",
            "         [ 0.9689,  0.0366,  0.0787]]], device='cuda:0') tensor([ 0.0708,  0.3591, -0.2736, -0.4231, -1.0000, -0.2435], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.016845703125\n",
            "tcost icost 0.03948974609375 0.0\n",
            "tcost icost -0.0667724609375 0.0\n",
            "tcost icost -0.0282135009765625 0.0\n",
            "tcost icost -0.06549072265625 0.0\n",
            "tcost icost -0.04913330078125 0.0\n",
            "tcost icost -0.1663818359375 0.0\n",
            "loss tensor([[-0.2217]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9945,  0.1027, -0.0208],\n",
            "         [ 0.9989,  0.0148, -0.0447],\n",
            "         [ 0.7622,  0.2060,  0.6136],\n",
            "         [ 0.5409, -0.1910, -0.8191],\n",
            "         [ 0.8070,  0.3462,  0.4784],\n",
            "         [ 0.9886,  0.0918,  0.1193]]], device='cuda:0') tensor([ 0.7847,  0.0773, -0.1206, -0.5256, -1.0000, -0.3077], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2216796875\n",
            "search tensor([[[ 0.8070,  0.3462,  0.4784],\n",
            "         [ 0.9886,  0.0918,  0.1193],\n",
            "         [ 0.0925,  0.4860, -0.0059],\n",
            "         [ 0.0170, -0.2080,  0.0708],\n",
            "         [-0.4329,  0.4207,  0.0038],\n",
            "         [-0.2921, -0.0307, -0.5221]]], device='cuda:0') tensor([[[-1.0000],\n",
            "         [-0.3077],\n",
            "         [ 1.1486],\n",
            "         [-0.3114],\n",
            "         [-0.0801],\n",
            "         [ 0.5330]]], device='cuda:0')\n",
            "tcost icost -0.01389312744140625 0.0\n",
            "tcost icost -0.088623046875 0.0\n",
            "tcost icost 0.02130126953125 0.0\n",
            "tcost icost 0.0232391357421875 0.0\n",
            "tcost icost -0.0328369140625 0.0\n",
            "tcost icost -0.1639404296875 0.0\n",
            "loss tensor([[-0.1779]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.8953,  0.2429,  0.3734],\n",
            "         [ 0.9660,  0.1703,  0.1947],\n",
            "         [-0.0076,  0.3856, -0.1059],\n",
            "         [ 0.1170, -0.1078,  0.1707],\n",
            "         [-0.3325,  0.3203, -0.0962],\n",
            "         [-0.1918,  0.0693, -0.4216]]], device='cuda:0') tensor([-0.2881, -0.9039,  1.0000, -0.4593, -0.0632,  0.4705], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1778564453125\n",
            "tcost icost 0.0033740997314453125 0.0\n",
            "tcost icost 0.016387939453125 0.0\n",
            "tcost icost -0.155029296875 0.0\n",
            "tcost icost -0.01383209228515625 0.0\n",
            "tcost icost 0.00876617431640625 0.0\n",
            "tcost icost 0.0521240234375 0.0\n",
            "loss tensor([[-0.0811]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.8860,  0.2590,  0.3847],\n",
            "         [ 0.9756,  0.1466,  0.1637],\n",
            "         [ 0.0021,  0.3779, -0.1281],\n",
            "         [ 0.1885, -0.0822,  0.1728],\n",
            "         [-0.2531,  0.3871, -0.0313],\n",
            "         [-0.1569,  0.0557, -0.4752]]], device='cuda:0') tensor([-1.0000, -0.2735,  0.7036, -0.3432, -0.1630,  0.5591], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0810546875\n",
            "tcost icost -0.01389312744140625 0.0\n",
            "tcost icost -0.09674072265625 0.0\n",
            "tcost icost -0.0028438568115234375 0.0\n",
            "tcost icost -0.154052734375 0.0\n",
            "tcost icost -0.052734375 0.0\n",
            "tcost icost -0.193115234375 0.0\n",
            "loss tensor([[-0.3643]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.8953,  0.2477,  0.3702],\n",
            "         [ 0.9751,  0.1508,  0.1624],\n",
            "         [-0.0327,  0.3425, -0.1689],\n",
            "         [ 0.2680, -0.0316,  0.2043],\n",
            "         [-0.1847,  0.4348,  0.0158],\n",
            "         [-0.0922,  0.0788, -0.4912]]], device='cuda:0') tensor([-0.5894, -0.6612,  0.9546, -0.4688, -0.1541,  0.5038], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.3642578125\n",
            "tcost icost -0.003910064697265625 0.0\n",
            "tcost icost -0.0021152496337890625 0.0\n",
            "tcost icost 0.11090087890625 0.0\n",
            "tcost icost 0.0794677734375 0.0\n",
            "tcost icost 0.004596710205078125 0.0\n",
            "tcost icost -0.161376953125 0.0\n",
            "loss tensor([[0.0497]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9006,  0.2431,  0.3603],\n",
            "         [ 0.9796,  0.1396,  0.1444],\n",
            "         [-0.0017,  0.3308, -0.1971],\n",
            "         [ 0.3390, -0.0064,  0.2120],\n",
            "         [-0.1458,  0.4343, -0.0016],\n",
            "         [-0.0147,  0.1279, -0.4843]]], device='cuda:0') tensor([-0.6683, -0.4889,  0.8090, -0.3486, -0.0714,  0.4078], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.04974365234375\n",
            "tcost icost -0.006046295166015625 0.0\n",
            "tcost icost -0.0170135498046875 0.0\n",
            "tcost icost 0.07391357421875 0.0\n",
            "tcost icost 0.14306640625 0.0\n",
            "tcost icost -0.019439697265625 0.0\n",
            "tcost icost -0.14453125 0.0\n",
            "loss tensor([[0.0448]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9265,  0.2037,  0.3164],\n",
            "         [ 0.9751,  0.1575,  0.1561],\n",
            "         [ 0.0098,  0.2877, -0.2500],\n",
            "         [ 0.4183,  0.0328,  0.2309],\n",
            "         [-0.1066,  0.4233, -0.0296],\n",
            "         [ 0.0687,  0.1869, -0.4599]]], device='cuda:0') tensor([ 0.3600, -1.0000,  1.0000, -0.3952, -0.0387,  0.3425], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0447998046875\n",
            "tcost icost 0.01593017578125 0.0\n",
            "tcost icost 0.00463104248046875 0.0\n",
            "tcost icost -0.378662109375 0.0\n",
            "tcost icost -0.043670654296875 0.0\n",
            "tcost icost -0.1842041015625 0.0\n",
            "tcost icost -0.0280914306640625 0.0\n",
            "loss tensor([[-0.4558]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9402,  0.1805,  0.2890],\n",
            "         [ 0.9748,  0.1607,  0.1549],\n",
            "         [ 0.0248,  0.2562, -0.2896],\n",
            "         [ 0.4902,  0.0612,  0.2437],\n",
            "         [-0.0467,  0.4425, -0.0252],\n",
            "         [ 0.1384,  0.2340, -0.4450]]], device='cuda:0') tensor([ 0.0629, -0.7454,  0.9463, -0.3439, -0.1601,  0.3567], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.455810546875\n",
            "tcost icost 0.0133056640625 0.0\n",
            "tcost icost 0.027130126953125 0.0\n",
            "tcost icost -0.23828125 0.0\n",
            "tcost icost -0.031707763671875 0.0\n",
            "tcost icost -0.035064697265625 0.0\n",
            "tcost icost -0.002750396728515625 0.0\n",
            "loss tensor([[-0.2030]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9424,  0.1785,  0.2828],\n",
            "         [ 0.9798,  0.1462,  0.1366],\n",
            "         [ 0.0483,  0.2405, -0.3147],\n",
            "         [ 0.5588,  0.0771,  0.2446],\n",
            "         [ 0.0209,  0.4751, -0.0037],\n",
            "         [ 0.1966,  0.2684, -0.4475]]], device='cuda:0') tensor([-0.5133, -0.3337,  0.7992, -0.2731, -0.2417,  0.3802], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2030029296875\n",
            "tcost icost -0.0017147064208984375 0.0\n",
            "tcost icost -0.014251708984375 0.0\n",
            "tcost icost 0.09466552734375 0.0\n",
            "tcost icost 0.1492919921875 0.0\n",
            "tcost icost -0.0243072509765625 0.0\n",
            "tcost icost -0.13671875 0.0\n",
            "loss tensor([[0.0742]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9545,  0.1543,  0.2553],\n",
            "         [ 0.9792,  0.1501,  0.1364],\n",
            "         [ 0.0751,  0.2087, -0.3546],\n",
            "         [ 0.6328,  0.0944,  0.2444],\n",
            "         [ 0.0874,  0.4984,  0.0074],\n",
            "         [ 0.2632,  0.3111, -0.4304]]], device='cuda:0') tensor([ 0.3011, -0.9246,  1.0000, -0.2568, -0.2160,  0.3157], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.07415771484375\n",
            "tcost icost 0.01557159423828125 0.0\n",
            "tcost icost 0.00978851318359375 0.0\n",
            "tcost icost -0.35888671875 0.0\n",
            "tcost icost -0.0400390625 0.0\n",
            "tcost icost -0.1165771484375 0.0\n",
            "tcost icost -0.02197265625 0.0\n",
            "loss tensor([[-0.3850]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9599,  0.1433,  0.2410],\n",
            "         [ 0.9818,  0.1424,  0.1260],\n",
            "         [ 0.1001,  0.1861, -0.3834],\n",
            "         [ 0.7026,  0.1024,  0.2393],\n",
            "         [ 0.1618,  0.5369,  0.0389],\n",
            "         [ 0.3198,  0.3447, -0.4251]]], device='cuda:0') tensor([-0.0861, -0.6035,  0.9417, -0.1878, -0.3636,  0.3331], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.385009765625\n",
            "tcost icost 0.00995635986328125 0.0\n",
            "tcost icost 0.0374755859375 0.0\n",
            "tcost icost -0.1385498046875 0.0\n",
            "tcost icost -0.00785064697265625 0.0\n",
            "tcost icost 0.0113677978515625 0.0\n",
            "tcost icost 0.07757568359375 0.0\n",
            "loss tensor([[-0.0211]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9552,  0.1571,  0.2507],\n",
            "         [ 0.9895,  0.1117,  0.0921],\n",
            "         [ 0.1430,  0.1872, -0.3959],\n",
            "         [ 0.7736,  0.0963,  0.2154],\n",
            "         [ 0.2388,  0.5839,  0.0830],\n",
            "         [ 0.3613,  0.3514, -0.4571]]], device='cuda:0') tensor([-1.0000,  0.2129,  0.5821, -0.0866, -0.4567,  0.4404], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.021087646484375\n",
            "tcost icost -0.01389312744140625 0.0\n",
            "tcost icost -0.211669921875 0.0\n",
            "tcost icost -0.0278472900390625 0.0\n",
            "tcost icost -0.2587890625 0.0\n",
            "tcost icost -0.06707763671875 0.0\n",
            "tcost icost -0.195556640625 0.0\n",
            "loss tensor([[-0.5752]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9535,  0.1632,  0.2534],\n",
            "         [ 0.9934,  0.0912,  0.0693],\n",
            "         [ 0.1712,  0.1799, -0.4128],\n",
            "         [ 0.8415,  0.1043,  0.2082],\n",
            "         [ 0.3113,  0.6260,  0.1251],\n",
            "         [ 0.4133,  0.3688, -0.4656]]], device='cuda:0') tensor([-0.7366, -0.0474,  0.7537, -0.1794, -0.4583,  0.3771], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5751953125\n",
            "tcost icost -0.00786590576171875 0.0\n",
            "tcost icost -0.1282958984375 0.0\n",
            "tcost icost -0.0040283203125 0.0\n",
            "tcost icost -0.11627197265625 0.0\n",
            "tcost icost -0.056427001953125 0.0\n",
            "tcost icost -0.1395263671875 0.0\n",
            "loss tensor([[-0.3308]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9574,  0.1556,  0.2432],\n",
            "         [ 0.9945,  0.0845,  0.0611],\n",
            "         [ 0.1776,  0.1601, -0.4377],\n",
            "         [ 0.9043,  0.1252,  0.2166],\n",
            "         [ 0.3803,  0.6637,  0.1657],\n",
            "         [ 0.4728,  0.3944, -0.4571]]], device='cuda:0') tensor([-0.1565, -0.4841,  1.0000, -0.3017, -0.4664,  0.3134], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.330810546875\n",
            "tcost icost 0.007904052734375 0.0\n",
            "tcost icost 0.03399658203125 0.0\n",
            "tcost icost -0.032562255859375 0.0\n",
            "tcost icost 0.00347900390625 0.0\n",
            "tcost icost 0.0041961669921875 0.0\n",
            "tcost icost 0.10968017578125 0.0\n",
            "loss tensor([[0.0822]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9629,  0.1442,  0.2280],\n",
            "         [ 0.9960,  0.0757,  0.0472],\n",
            "         [ 0.2081,  0.1397, -0.4748],\n",
            "         [ 0.9598,  0.1555,  0.2336],\n",
            "         [ 0.4558,  0.6769,  0.1582],\n",
            "         [ 0.5337,  0.4196, -0.4558]]], device='cuda:0') tensor([ 0.0565, -0.3987,  0.9202, -0.4573, -0.2522,  0.2811], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0821533203125\n",
            "tcost icost 0.0133209228515625 0.0\n",
            "tcost icost 0.053314208984375 0.0\n",
            "tcost icost -0.1217041015625 0.0\n",
            "tcost icost -0.0183868408203125 0.0\n",
            "tcost icost -0.00028586387634277344 0.0\n",
            "tcost icost 0.0246124267578125 0.0\n",
            "loss tensor([[-0.0363]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9612,  0.1513,  0.2306],\n",
            "         [ 0.9985,  0.0523,  0.0184],\n",
            "         [ 0.2520,  0.1349, -0.5008],\n",
            "         [ 0.9586,  0.1683,  0.2298],\n",
            "         [ 0.5260,  0.6926,  0.1576],\n",
            "         [ 0.5849,  0.4367, -0.4634]]], device='cuda:0') tensor([-0.8160,  0.3005,  0.6578, -0.3876, -0.3070,  0.3187], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.036346435546875\n",
            "tcost icost -0.009918212890625 0.0\n",
            "tcost icost -0.22607421875 0.0\n",
            "tcost icost -0.0265045166015625 0.0\n",
            "tcost icost -0.153564453125 0.0\n",
            "tcost icost -0.055694580078125 0.0\n",
            "tcost icost -0.177978515625 0.0\n",
            "loss tensor([[-0.4883]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 9.6262e-01,  1.4996e-01,  2.2555e-01],\n",
            "         [ 9.9926e-01,  3.8500e-02,  5.1425e-04],\n",
            "         [ 2.8164e-01,  1.2224e-01, -5.3021e-01],\n",
            "         [ 9.5375e-01,  1.8688e-01,  2.3544e-01],\n",
            "         [ 5.9258e-01,  7.0584e-01,  1.5777e-01],\n",
            "         [ 6.4245e-01,  4.6017e-01, -4.5588e-01]]], device='cuda:0') tensor([-0.4371,  0.0156,  0.8624, -0.4991, -0.3059,  0.2550], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.48828125\n",
            "tcost icost 0.00024271011352539062 0.0\n",
            "tcost icost -0.08209228515625 0.0\n",
            "tcost icost 0.042877197265625 0.0\n",
            "tcost icost 0.10125732421875 0.0\n",
            "tcost icost -0.024261474609375 0.0\n",
            "tcost icost -0.1461181640625 0.0\n",
            "loss tensor([[-0.0673]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9681,  0.1377,  0.2094],\n",
            "         [ 0.9993,  0.0368, -0.0061],\n",
            "         [ 0.3074,  0.1066, -0.5636],\n",
            "         [ 0.9519,  0.1969,  0.2348],\n",
            "         [ 0.6572,  0.7158,  0.1569],\n",
            "         [ 0.7042,  0.4891, -0.4367]]], device='cuda:0') tensor([ 0.0575, -0.4732,  1.0000, -0.3777, -0.3053,  0.1792], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.06732177734375\n",
            "tcost icost 0.01331329345703125 0.0\n",
            "tcost icost 0.04974365234375 0.0\n",
            "tcost icost -0.1756591796875 0.0\n",
            "tcost icost -0.023406982421875 0.0\n",
            "tcost icost -0.0002359151840209961 0.0\n",
            "tcost icost 0.014923095703125 0.0\n",
            "loss tensor([[-0.0927]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9684,  0.1394,  0.2071],\n",
            "         [ 0.9994,  0.0243, -0.0240],\n",
            "         [ 0.3434,  0.1004, -0.5887],\n",
            "         [ 0.9522,  0.2016,  0.2294],\n",
            "         [ 0.6934,  0.7037,  0.1547],\n",
            "         [ 0.7520,  0.5078, -0.4204]]], device='cuda:0') tensor([-0.6261,  0.0273,  0.8027, -0.3218, -0.3455,  0.2015], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.09271240234375\n",
            "tcost icost -0.00492095947265625 0.0\n",
            "tcost icost -0.130126953125 0.0\n",
            "tcost icost 0.0015125274658203125 0.0\n",
            "tcost icost -0.0408935546875 0.0\n",
            "tcost icost -0.04656982421875 0.0\n",
            "tcost icost -0.1239013671875 0.0\n",
            "loss tensor([[-0.2544]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9731,  0.1276,  0.1919],\n",
            "         [ 0.9993,  0.0249, -0.0274],\n",
            "         [ 0.3529,  0.0842, -0.6203],\n",
            "         [ 0.9481,  0.2138,  0.2353],\n",
            "         [ 0.7165,  0.6810,  0.1514],\n",
            "         [ 0.7726,  0.5105, -0.3775]]], device='cuda:0') tensor([ 0.0687, -0.4473,  1.0000, -0.4387, -0.3496,  0.1338], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.25439453125\n",
            "tcost icost 0.01329803466796875 0.0\n",
            "tcost icost 0.050811767578125 0.0\n",
            "tcost icost -0.17138671875 0.0\n",
            "tcost icost -0.0247650146484375 0.0\n",
            "tcost icost -0.0006666183471679688 0.0\n",
            "tcost icost 0.014129638671875 0.0\n",
            "loss tensor([[-0.0899]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9735,  0.1285,  0.1892],\n",
            "         [ 0.9990,  0.0150, -0.0422],\n",
            "         [ 0.3746,  0.0775, -0.6445],\n",
            "         [ 0.9464,  0.2203,  0.2360],\n",
            "         [ 0.7345,  0.6615,  0.1518],\n",
            "         [ 0.7887,  0.5098, -0.3436]]], device='cuda:0') tensor([-0.5831,  0.0368,  0.8016, -0.3839, -0.3907,  0.1539], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.08990478515625\n",
            "tcost icost -0.0037021636962890625 0.0\n",
            "tcost icost -0.12353515625 0.0\n",
            "tcost icost 0.006412506103515625 0.0\n",
            "tcost icost -0.00867462158203125 0.0\n",
            "tcost icost -0.04425048828125 0.0\n",
            "tcost icost -0.1275634765625 0.0\n",
            "loss tensor([[-0.2205]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9785,  0.1143,  0.1718],\n",
            "         [ 0.9990,  0.0197, -0.0410],\n",
            "         [ 0.3690,  0.0593, -0.6760],\n",
            "         [ 0.9403,  0.2343,  0.2470],\n",
            "         [ 0.7515,  0.6419,  0.1526],\n",
            "         [ 0.8048,  0.5114, -0.3013]]], device='cuda:0') tensor([ 0.2582, -0.5355,  1.0000, -0.5119, -0.3963,  0.0846], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.220458984375\n",
            "search tensor([[[ 0.7515,  0.6419,  0.1526],\n",
            "         [ 0.8048,  0.5114, -0.3013],\n",
            "         [-0.0266, -0.3943, -0.5285],\n",
            "         [-0.2197, -0.4603,  0.0814],\n",
            "         [-0.0422, -0.1197,  0.5298],\n",
            "         [-0.4598,  0.3698,  0.3143]]], device='cuda:0') tensor([[[-0.3963],\n",
            "         [ 0.0846],\n",
            "         [-0.4606],\n",
            "         [-0.0346],\n",
            "         [ 0.3244],\n",
            "         [-0.6104]]], device='cuda:0')\n",
            "tcost icost 0.0061187744140625 0.0\n",
            "tcost icost -0.1248779296875 0.0\n",
            "tcost icost -0.047607421875 0.0\n",
            "tcost icost -0.2437744140625 0.0\n",
            "tcost icost -0.048248291015625 0.0\n",
            "tcost icost -0.1126708984375 0.0\n",
            "loss tensor([[-0.4209]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.8426,  0.5361,  0.0520],\n",
            "         [ 0.8149,  0.5506, -0.1811],\n",
            "         [-0.1265, -0.4939, -0.6280],\n",
            "         [-0.1195, -0.3599,  0.1813],\n",
            "         [-0.1422, -0.2196,  0.4293],\n",
            "         [-0.3593,  0.4694,  0.4140]]], device='cuda:0') tensor([ 0.0088, -0.1521, -0.3563, -0.1182,  0.3702, -0.7060], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4208984375\n",
            "tcost icost 0.0206756591796875 0.0\n",
            "tcost icost 0.0457763671875 0.0\n",
            "tcost icost 0.0097808837890625 0.0\n",
            "tcost icost -0.08038330078125 0.0\n",
            "tcost icost -0.01556396484375 0.0\n",
            "tcost icost -0.053192138671875 0.0\n",
            "loss tensor([[-0.0304]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.9022,  0.4297, -0.0379],\n",
            "         [ 0.8149,  0.5742, -0.0793],\n",
            "         [-0.2122, -0.5808, -0.6990],\n",
            "         [-0.0236, -0.2677,  0.2791],\n",
            "         [-0.2332, -0.3162,  0.3429],\n",
            "         [-0.2595,  0.5655,  0.5117]]], device='cuda:0') tensor([ 1.0000, -0.6750,  0.0046, -0.3106,  0.4634, -0.7832], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0303802490234375\n",
            "tcost icost -0.06182861328125 0.0\n",
            "tcost icost -0.0243988037109375 0.0\n",
            "tcost icost -0.1988525390625 0.0\n",
            "tcost icost -0.05169677734375 0.0\n",
            "tcost icost -0.313232421875 0.0\n",
            "tcost icost -0.06817626953125 0.0\n",
            "loss tensor([[-0.5283]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.9297,  0.3575, -0.0890],\n",
            "         [ 0.8221,  0.5691, -0.0175],\n",
            "         [-0.2498, -0.6130, -0.7495],\n",
            "         [ 0.0527, -0.1980,  0.3594],\n",
            "         [-0.2031, -0.3452,  0.3309],\n",
            "         [-0.1860,  0.6383,  0.5817]]], device='cuda:0') tensor([ 0.6946, -0.5286, -0.1310, -0.2984,  0.3822, -0.7746], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5283203125\n",
            "tcost icost 0.01248931884765625 0.0\n",
            "tcost icost 0.005352020263671875 0.0\n",
            "tcost icost -0.1265869140625 0.0\n",
            "tcost icost -0.039703369140625 0.0\n",
            "tcost icost -0.288818359375 0.0\n",
            "tcost icost -0.0638427734375 0.0\n",
            "loss tensor([[-0.3413]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9415,  0.3200, -0.1054],\n",
            "         [ 0.8426,  0.5385,  0.0060],\n",
            "         [-0.2517, -0.5916, -0.7659],\n",
            "         [ 0.1160, -0.1520,  0.4206],\n",
            "         [-0.1463, -0.3395,  0.3507],\n",
            "         [-0.1278,  0.6980,  0.6359]]], device='cuda:0') tensor([ 0.2007, -0.2403, -0.3088, -0.2564,  0.2986, -0.7672], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.34130859375\n",
            "tcost icost 0.02435302734375 0.0\n",
            "tcost icost 0.0894775390625 0.0\n",
            "tcost icost 0.05859375 0.0\n",
            "tcost icost 0.08758544921875 0.0\n",
            "tcost icost 0.11572265625 0.0\n",
            "tcost icost 0.037078857421875 0.0\n",
            "loss tensor([[0.3140]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9541,  0.2584, -0.1517],\n",
            "         [ 0.8226,  0.5657,  0.0566],\n",
            "         [-0.2439, -0.5933, -0.7672],\n",
            "         [ 0.1905, -0.0868,  0.4858],\n",
            "         [-0.1181, -0.3693,  0.3206],\n",
            "         [-0.0821,  0.7497,  0.6566]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.2254, -0.3374,  0.4836, -0.6877], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.31396484375\n",
            "tcost icost -0.06182861328125 0.0\n",
            "tcost icost -0.033294677734375 0.0\n",
            "tcost icost -0.246826171875 0.0\n",
            "tcost icost -0.0552978515625 0.0\n",
            "tcost icost -0.331787109375 0.0\n",
            "tcost icost -0.06744384765625 0.0\n",
            "loss tensor([[-0.5898]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9604,  0.2097, -0.1837],\n",
            "         [ 0.8099,  0.5788,  0.0958],\n",
            "         [-0.2260, -0.5855, -0.7786],\n",
            "         [ 0.2562, -0.0313,  0.5442],\n",
            "         [-0.0715, -0.3770,  0.3126],\n",
            "         [-0.0416,  0.7625,  0.6456]]], device='cuda:0') tensor([ 0.7600, -0.9337,  0.1092, -0.3294,  0.4040, -0.6823], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.58984375\n",
            "tcost icost -0.004669189453125 0.0\n",
            "tcost icost -0.01531982421875 0.0\n",
            "tcost icost -0.2266845703125 0.0\n",
            "tcost icost -0.052642822265625 0.0\n",
            "tcost icost -0.321044921875 0.0\n",
            "tcost icost -0.065673828125 0.0\n",
            "loss tensor([[-0.4900]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9639,  0.1751, -0.2004],\n",
            "         [ 0.8070,  0.5778,  0.1221],\n",
            "         [-0.1989, -0.5692, -0.7978],\n",
            "         [ 0.3148,  0.0148,  0.5954],\n",
            "         [-0.0123, -0.3669,  0.3208],\n",
            "         [-0.0083,  0.7733,  0.6340]]], device='cuda:0') tensor([ 0.3809, -0.8018, -0.0255, -0.3132,  0.3189, -0.6733], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.489990234375\n",
            "tcost icost 0.0269622802734375 0.0\n",
            "tcost icost 0.01519012451171875 0.0\n",
            "tcost icost -0.14013671875 0.0\n",
            "tcost icost -0.035369873046875 0.0\n",
            "tcost icost -0.264892578125 0.0\n",
            "tcost icost -0.0596923828125 0.0\n",
            "loss tensor([[-0.3076]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9670,  0.1567, -0.2010],\n",
            "         [ 0.8195,  0.5586,  0.1279],\n",
            "         [-0.1599, -0.5426, -0.8246],\n",
            "         [ 0.3695,  0.0444,  0.6316],\n",
            "         [ 0.0559, -0.3413,  0.3409],\n",
            "         [ 0.0196,  0.7827,  0.6221]]], device='cuda:0') tensor([-0.0698, -0.4565, -0.2178, -0.2446,  0.2259, -0.6655], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.3076171875\n",
            "tcost icost 0.017303466796875 0.0\n",
            "tcost icost 0.047515869140625 0.0\n",
            "tcost icost 0.046600341796875 0.0\n",
            "tcost icost 0.0309295654296875 0.0\n",
            "tcost icost 0.0022563934326171875 0.0\n",
            "tcost icost -0.021270751953125 0.0\n",
            "loss tensor([[0.1093]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9637,  0.1188, -0.2391],\n",
            "         [ 0.7976,  0.5806,  0.1632],\n",
            "         [-0.1600, -0.5490, -0.8204],\n",
            "         [ 0.4314,  0.0942,  0.6774],\n",
            "         [ 0.1084, -0.3369,  0.3381],\n",
            "         [ 0.0531,  0.7882,  0.6132]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.4360, -0.4484,  0.3229, -0.7110], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1092529296875\n",
            "tcost icost -0.06182861328125 0.0\n",
            "tcost icost -0.033294677734375 0.0\n",
            "tcost icost -0.2841796875 0.0\n",
            "tcost icost -0.05816650390625 0.0\n",
            "tcost icost -0.302978515625 0.0\n",
            "tcost icost -0.06549072265625 0.0\n",
            "loss tensor([[-0.6021]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9594,  0.0876, -0.2679],\n",
            "         [ 0.7811,  0.5941,  0.1920],\n",
            "         [-0.1505, -0.5488, -0.8223],\n",
            "         [ 0.4876,  0.1380,  0.7192],\n",
            "         [ 0.1701, -0.3177,  0.3476],\n",
            "         [ 0.0820,  0.7930,  0.6037]]], device='cuda:0') tensor([ 0.7709, -0.9286,  0.3344, -0.4388,  0.2314, -0.7049], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.60205078125\n",
            "tcost icost -0.00762176513671875 0.0\n",
            "tcost icost -0.016082763671875 0.0\n",
            "tcost icost -0.27294921875 0.0\n",
            "tcost icost -0.056549072265625 0.0\n",
            "tcost icost -0.284912109375 0.0\n",
            "tcost icost -0.0635986328125 0.0\n",
            "loss tensor([[-0.5088]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9561,  0.0648, -0.2856],\n",
            "         [ 0.7725,  0.5983,  0.2128],\n",
            "         [-0.1324, -0.5423, -0.8297],\n",
            "         [ 0.5389,  0.1760,  0.7569],\n",
            "         [ 0.2386, -0.2859,  0.3675],\n",
            "         [ 0.1069,  0.7976,  0.5937]]], device='cuda:0') tensor([ 0.4213, -0.8010,  0.2169, -0.4261,  0.1327, -0.6999], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5087890625\n",
            "tcost icost 0.02716064453125 0.0\n",
            "tcost icost 0.0135040283203125 0.0\n",
            "tcost icost -0.2060546875 0.0\n",
            "tcost icost -0.048187255859375 0.0\n",
            "tcost icost -0.2412109375 0.0\n",
            "tcost icost -0.059173583984375 0.0\n",
            "loss tensor([[-0.3557]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9551,  0.0514, -0.2918],\n",
            "         [ 0.7781,  0.5883,  0.2200],\n",
            "         [-0.1058, -0.5286, -0.8423],\n",
            "         [ 0.5847,  0.2059,  0.7847],\n",
            "         [ 0.3118, -0.2438,  0.3959],\n",
            "         [ 0.1286,  0.8023,  0.5829]]], device='cuda:0') tensor([ 0.0341, -0.4802,  0.0962, -0.3905,  0.0336, -0.6922], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.355712890625\n",
            "tcost icost 0.0218353271484375 0.0\n",
            "tcost icost 0.0684814453125 0.0\n",
            "tcost icost 0.2061767578125 0.0\n",
            "tcost icost 0.03515625 0.0\n",
            "tcost icost 0.053070068359375 0.0\n",
            "tcost icost 0.017822265625 0.0\n",
            "loss tensor([[0.3215]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9523,  0.0378, -0.3029],\n",
            "         [ 0.7923,  0.5703,  0.2167],\n",
            "         [-0.0620, -0.5019, -0.8627],\n",
            "         [ 0.6082,  0.2162,  0.7638],\n",
            "         [ 0.3816, -0.2123,  0.4109],\n",
            "         [ 0.1488,  0.8086,  0.5692]]], device='cuda:0') tensor([ 0.1375, -0.3552, -0.0267, -0.2767,  0.1460, -0.6782], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.321533203125\n",
            "tcost icost 0.0241546630859375 0.0\n",
            "tcost icost 0.0816650390625 0.0\n",
            "tcost icost 0.1927490234375 0.0\n",
            "tcost icost 0.054229736328125 0.0\n",
            "tcost icost 0.1322021484375 0.0\n",
            "tcost icost 0.016510009765625 0.0\n",
            "loss tensor([[0.3899]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9487,  0.0261, -0.3150],\n",
            "         [ 0.8107,  0.5488,  0.2041],\n",
            "         [-0.0068, -0.4606, -0.8876],\n",
            "         [ 0.6488,  0.2145,  0.7301],\n",
            "         [ 0.4547, -0.2013,  0.3991],\n",
            "         [ 0.1683,  0.8164,  0.5525]]], device='cuda:0') tensor([ 0.0786, -0.3179, -0.2986, -0.0707,  0.4202, -0.6623], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.389892578125\n",
            "tcost icost 0.0232391357421875 0.0\n",
            "tcost icost 0.07830810546875 0.0\n",
            "tcost icost 0.045623779296875 0.0\n",
            "tcost icost 0.037353515625 0.0\n",
            "tcost icost 0.012847900390625 0.0\n",
            "tcost icost 0.01143646240234375 0.0\n",
            "loss tensor([[0.1731]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9194, -0.0073, -0.3517],\n",
            "         [ 0.7995,  0.5590,  0.2199],\n",
            "         [ 0.0039, -0.4411, -0.8974],\n",
            "         [ 0.6761,  0.2282,  0.7006],\n",
            "         [ 0.5224, -0.2028,  0.3766],\n",
            "         [ 0.1936,  0.8224,  0.5350]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.3513, -0.3094,  0.5116, -0.6672], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.173095703125\n",
            "tcost icost -0.0526123046875 0.0\n",
            "tcost icost -0.030914306640625 0.0\n",
            "tcost icost -0.27099609375 0.0\n",
            "tcost icost -0.055755615234375 0.0\n",
            "tcost icost -0.334716796875 0.0\n",
            "tcost icost -0.0665283203125 0.0\n",
            "loss tensor([[-0.5991]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.8994, -0.0342, -0.3820],\n",
            "         [ 0.7923,  0.5643,  0.2321],\n",
            "         [ 0.0194, -0.4208, -0.9070],\n",
            "         [ 0.6996,  0.2390,  0.6733],\n",
            "         [ 0.5911, -0.1930,  0.3640],\n",
            "         [ 0.2165,  0.8280,  0.5172]]], device='cuda:0') tensor([ 0.7375, -0.9326,  0.2374, -0.2993,  0.4331, -0.6617], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.59912109375\n",
            "tcost icost 0.01309967041015625 0.0\n",
            "tcost icost -0.01029205322265625 0.0\n",
            "tcost icost -0.252197265625 0.0\n",
            "tcost icost -0.053192138671875 0.0\n",
            "tcost icost -0.31787109375 0.0\n",
            "tcost icost -0.06451416015625 0.0\n",
            "loss tensor([[-0.4858]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.8916, -0.0533, -0.4051],\n",
            "         [ 0.7911,  0.5631,  0.2391],\n",
            "         [ 0.0397, -0.3986, -0.9163],\n",
            "         [ 0.7206,  0.2470,  0.6479],\n",
            "         [ 0.6607, -0.1732,  0.3601],\n",
            "         [ 0.2370,  0.8337,  0.4988]]], device='cuda:0') tensor([ 0.3674, -0.7866,  0.1022, -0.2806,  0.3488, -0.6528], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.48583984375\n",
            "tcost icost 0.0274658203125 0.0\n",
            "tcost icost 0.019622802734375 0.0\n",
            "tcost icost -0.146484375 0.0\n",
            "tcost icost -0.0340576171875 0.0\n",
            "tcost icost -0.25 0.0\n",
            "tcost icost -0.05731201171875 0.0\n",
            "loss tensor([[-0.2964]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.8991, -0.0620, -0.4175],\n",
            "         [ 0.8030,  0.5484,  0.2332],\n",
            "         [ 0.0678, -0.3716, -0.9259],\n",
            "         [ 0.7444,  0.2495,  0.6194],\n",
            "         [ 0.7304, -0.1445,  0.3638],\n",
            "         [ 0.2558,  0.8394,  0.4795]]], device='cuda:0') tensor([-0.2185, -0.3459, -0.0743, -0.2084,  0.2566, -0.6458], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.29638671875\n",
            "tcost icost 0.0111236572265625 0.0\n",
            "tcost icost 0.003917694091796875 0.0\n",
            "tcost icost 0.005001068115234375 0.0\n",
            "tcost icost -0.1083984375 0.0\n",
            "tcost icost -0.03204345703125 0.0\n",
            "tcost icost -0.082275390625 0.0\n",
            "loss tensor([[-0.1299]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.8863, -0.0807, -0.4392],\n",
            "         [ 0.8041,  0.5445,  0.2386],\n",
            "         [ 0.0791, -0.3562, -0.9311],\n",
            "         [ 0.7582,  0.2591,  0.5983],\n",
            "         [ 0.7937, -0.1236,  0.3625],\n",
            "         [ 0.2791,  0.8340,  0.4760]]], device='cuda:0') tensor([ 0.5789, -0.7688,  0.2346, -0.3611,  0.3037, -0.7359], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1298828125\n",
            "tcost icost 0.033111572265625 0.0\n",
            "tcost icost 0.00829315185546875 0.0\n",
            "tcost icost -0.2255859375 0.0\n",
            "tcost icost -0.050628662109375 0.0\n",
            "tcost icost -0.26953125 0.0\n",
            "tcost icost -0.0618896484375 0.0\n",
            "loss tensor([[-0.3926]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.8845, -0.0927, -0.4541],\n",
            "         [ 0.8123,  0.5329,  0.2371],\n",
            "         [ 0.0967, -0.3383, -0.9360],\n",
            "         [ 0.7719,  0.2655,  0.5777],\n",
            "         [ 0.8556, -0.0953,  0.3679],\n",
            "         [ 0.3005,  0.8297,  0.4704]]], device='cuda:0') tensor([ 0.3323, -0.5151,  0.0934, -0.3328,  0.2186, -0.7282], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.392578125\n",
            "search tensor([[[ 0.8556, -0.0953,  0.3679],\n",
            "         [ 0.3005,  0.8297,  0.4704],\n",
            "         [-0.1627,  0.4338, -0.4410],\n",
            "         [-0.2346, -0.2286,  0.4949],\n",
            "         [-0.1631, -0.1459,  0.5071],\n",
            "         [-0.1742,  0.0223, -0.2896]]], device='cuda:0') tensor([[[ 0.2186],\n",
            "         [-0.7282],\n",
            "         [ 0.5876],\n",
            "         [ 0.5020],\n",
            "         [ 0.5555],\n",
            "         [-0.2597]]], device='cuda:0')\n",
            "tcost icost 0.0305328369140625 0.0\n",
            "tcost icost 0.061920166015625 0.0\n",
            "tcost icost 0.130859375 0.0\n",
            "tcost icost -0.004161834716796875 0.0\n",
            "tcost icost -0.0218048095703125 0.0\n",
            "tcost icost 0.021331787109375 0.0\n",
            "loss tensor([[0.1875]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.9448, -0.1932,  0.2648],\n",
            "         [ 0.3447,  0.8001,  0.4909],\n",
            "         [-0.2626,  0.3334, -0.5406],\n",
            "         [-0.1344, -0.1283,  0.5944],\n",
            "         [-0.2630, -0.2457,  0.4065],\n",
            "         [-0.0740,  0.1223, -0.1893]]], device='cuda:0') tensor([ 1.0000, -1.0000,  1.0000,  0.3059,  0.6059, -0.2723], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1875\n",
            "tcost icost 0.02117919921875 0.0\n",
            "tcost icost -0.004840850830078125 0.0\n",
            "tcost icost -0.251220703125 0.0\n",
            "tcost icost -0.0227203369140625 0.0\n",
            "tcost icost -0.18212890625 0.0\n",
            "tcost icost -0.043212890625 0.0\n",
            "loss tensor([[-0.3481]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.9548, -0.2228,  0.1968],\n",
            "         [ 0.4229,  0.7412,  0.5213],\n",
            "         [-0.3331,  0.2791, -0.5957],\n",
            "         [-0.0614, -0.0851,  0.6421],\n",
            "         [-0.1915, -0.2031,  0.4498],\n",
            "         [-0.0194,  0.1589, -0.1997]]], device='cuda:0') tensor([ 0.5854, -0.6956,  0.9145,  0.3941,  0.4902, -0.2627], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.34814453125\n",
            "tcost icost 0.03643798828125 0.0\n",
            "tcost icost 0.04345703125 0.0\n",
            "tcost icost 0.017364501953125 0.0\n",
            "tcost icost 0.046844482421875 0.0\n",
            "tcost icost 0.05023193359375 0.0\n",
            "tcost icost 0.0173797607421875 0.0\n",
            "loss tensor([[0.1670]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 9.6147e-01, -2.1251e-01,  1.7441e-01],\n",
            "         [ 5.0908e-01,  6.6637e-01,  4.9454e-01],\n",
            "         [-2.8311e-01,  2.7348e-01, -6.2612e-01],\n",
            "         [-3.2444e-05, -6.9812e-02,  6.5574e-01],\n",
            "         [-1.5221e-01, -1.9928e-01,  4.5270e-01],\n",
            "         [ 4.8292e-03,  1.5202e-01, -2.5660e-01]]], device='cuda:0') tensor([-0.4058,  0.0779,  0.4870,  0.4713,  0.5784, -0.2403], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1669921875\n",
            "tcost icost 0.002666473388671875 0.0\n",
            "tcost icost -0.255859375 0.0\n",
            "tcost icost -0.046844482421875 0.0\n",
            "tcost icost -0.263427734375 0.0\n",
            "tcost icost -0.04876708984375 0.0\n",
            "tcost icost -0.05462646484375 0.0\n",
            "loss tensor([[-0.5220]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9646, -0.2148,  0.1528],\n",
            "         [ 0.5804,  0.6050,  0.4789],\n",
            "         [-0.2451,  0.2640, -0.6535],\n",
            "         [ 0.0703, -0.0339,  0.6857],\n",
            "         [-0.1230, -0.2117,  0.4461],\n",
            "         [ 0.0627,  0.1955, -0.2284]]], device='cuda:0') tensor([-0.0957, -0.1011,  0.5859,  0.4021,  0.6189, -0.2935], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.52197265625\n",
            "tcost icost 0.017852783203125 0.0\n",
            "tcost icost -0.158203125 0.0\n",
            "tcost icost -0.02972412109375 0.0\n",
            "tcost icost -0.197509765625 0.0\n",
            "tcost icost -0.04229736328125 0.0\n",
            "tcost icost -0.032745361328125 0.0\n",
            "loss tensor([[-0.3396]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9639, -0.2337,  0.1279],\n",
            "         [ 0.6355,  0.5815,  0.4749],\n",
            "         [-0.2164,  0.2479, -0.6816],\n",
            "         [ 0.1471,  0.0172,  0.7252],\n",
            "         [-0.1023, -0.2374,  0.4269],\n",
            "         [ 0.1341,  0.2557, -0.1781]]], device='cuda:0') tensor([ 0.4102, -0.3523,  0.7310,  0.3081,  0.6637, -0.3327], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.339599609375\n",
            "tcost icost 0.03631591796875 0.0\n",
            "tcost icost 0.112060546875 0.0\n",
            "tcost icost 0.0972900390625 0.0\n",
            "tcost icost 0.0662841796875 0.0\n",
            "tcost icost -0.00824737548828125 0.0\n",
            "tcost icost 0.01264190673828125 0.0\n",
            "loss tensor([[0.2666]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9301, -0.2789,  0.0816],\n",
            "         [ 0.6256,  0.6080,  0.4888],\n",
            "         [-0.1844,  0.2037, -0.7307],\n",
            "         [ 0.2298,  0.0823,  0.7720],\n",
            "         [-0.0877, -0.2761,  0.3910],\n",
            "         [ 0.2111,  0.3189, -0.1242]]], device='cuda:0') tensor([ 1.0000, -1.0000,  1.0000,  0.1264,  0.7277, -0.3455], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2666015625\n",
            "tcost icost 0.02117919921875 0.0\n",
            "tcost icost -0.004840850830078125 0.0\n",
            "tcost icost -0.251220703125 0.0\n",
            "tcost icost -0.0244140625 0.0\n",
            "tcost icost -0.212158203125 0.0\n",
            "tcost icost -0.046112060546875 0.0\n",
            "loss tensor([[-0.3711]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9120, -0.3117,  0.0480],\n",
            "         [ 0.6331,  0.6016,  0.4871],\n",
            "         [-0.1556,  0.1698, -0.7698],\n",
            "         [ 0.3090,  0.1315,  0.8066],\n",
            "         [-0.0454, -0.2757,  0.3898],\n",
            "         [ 0.2774,  0.3701, -0.0834]]], device='cuda:0') tensor([ 0.5989, -0.7039,  0.9153,  0.2044,  0.6215, -0.3376], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.37109375\n",
            "tcost icost 0.036468505859375 0.0\n",
            "tcost icost 0.040283203125 0.0\n",
            "tcost icost 0.0009331703186035156 0.0\n",
            "tcost icost 0.0291290283203125 0.0\n",
            "tcost icost 0.06658935546875 0.0\n",
            "tcost icost 0.00836181640625 0.0\n",
            "loss tensor([[0.1433]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9282, -0.3194,  0.0405],\n",
            "         [ 0.6778,  0.5699,  0.4646],\n",
            "         [-0.1041,  0.1618, -0.7954],\n",
            "         [ 0.3913,  0.1564,  0.8178],\n",
            "         [ 0.0009, -0.2724,  0.3836],\n",
            "         [ 0.3331,  0.4060, -0.0684]]], device='cuda:0') tensor([-0.6951,  0.2332,  0.4226,  0.3850,  0.6630, -0.3194], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.143310546875\n",
            "tcost icost -0.00815582275390625 0.0\n",
            "tcost icost -0.30419921875 0.0\n",
            "tcost icost -0.05108642578125 0.0\n",
            "tcost icost -0.3095703125 0.0\n",
            "tcost icost -0.049407958984375 0.0\n",
            "tcost icost -0.072998046875 0.0\n",
            "loss tensor([[-0.6240]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9391, -0.3286,  0.0313],\n",
            "         [ 0.7134,  0.5411,  0.4452],\n",
            "         [-0.0605,  0.1527, -0.8188],\n",
            "         [ 0.4707,  0.1867,  0.8279],\n",
            "         [ 0.0405, -0.2786,  0.3737],\n",
            "         [ 0.3981,  0.4557, -0.0265]]], device='cuda:0') tensor([-0.5096,  0.0891,  0.5018,  0.3170,  0.7128, -0.3913], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6240234375\n",
            "tcost icost -0.0009179115295410156 0.0\n",
            "tcost icost -0.26953125 0.0\n",
            "tcost icost -0.0474853515625 0.0\n",
            "tcost icost -0.273681640625 0.0\n",
            "tcost icost -0.04632568359375 0.0\n",
            "tcost icost -0.053680419921875 0.0\n",
            "loss tensor([[-0.5435]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9404, -0.3396,  0.0195],\n",
            "         [ 0.7409,  0.5164,  0.4294],\n",
            "         [-0.0248,  0.1419, -0.8410],\n",
            "         [ 0.5344,  0.2162,  0.8171],\n",
            "         [ 0.0736, -0.2928,  0.3593],\n",
            "         [ 0.4679,  0.5117,  0.0277]]], device='cuda:0') tensor([-0.2426, -0.0820,  0.5946,  0.2396,  0.7557, -0.4474], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.54345703125\n",
            "tcost icost 0.01025390625 0.0\n",
            "tcost icost -0.1953125 0.0\n",
            "tcost icost -0.0357666015625 0.0\n",
            "tcost icost -0.2159423828125 0.0\n",
            "tcost icost -0.041107177734375 0.0\n",
            "tcost icost -0.0316162109375 0.0\n",
            "loss tensor([[-0.3975]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9333, -0.3565,  0.0031],\n",
            "         [ 0.7589,  0.4992,  0.4181],\n",
            "         [ 0.0027,  0.1283, -0.8628],\n",
            "         [ 0.5759,  0.2419,  0.7809],\n",
            "         [ 0.0998, -0.3151,  0.3356],\n",
            "         [ 0.5407,  0.5693,  0.0881]]], device='cuda:0') tensor([ 0.1769, -0.3083,  0.7197,  0.1380,  0.8077, -0.4843], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.3974609375\n",
            "tcost icost 0.0289764404296875 0.0\n",
            "tcost icost 0.004852294921875 0.0\n",
            "tcost icost 0.0108795166015625 0.0\n",
            "tcost icost -0.04925537109375 0.0\n",
            "tcost icost -0.019775390625 0.0\n",
            "tcost icost 0.00815582275390625 0.0\n",
            "loss tensor([[-0.0019]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9050, -0.3853, -0.0261],\n",
            "         [ 0.7586,  0.5011,  0.4165],\n",
            "         [ 0.0207,  0.1053, -0.8893],\n",
            "         [ 0.6098,  0.2717,  0.7445],\n",
            "         [ 0.1219, -0.3448,  0.3009],\n",
            "         [ 0.6140,  0.6236,  0.1488]]], device='cuda:0') tensor([ 1.0000, -0.8655,  1.0000, -0.0229,  0.8631, -0.4995], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0019073486328125\n",
            "tcost icost 0.02117919921875 0.0\n",
            "tcost icost -0.0013828277587890625 0.0\n",
            "tcost icost -0.2427978515625 0.0\n",
            "tcost icost -0.028045654296875 0.0\n",
            "tcost icost -0.2318115234375 0.0\n",
            "tcost icost -0.050262451171875 0.0\n",
            "loss tensor([[-0.3792]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.8892, -0.4054, -0.0463],\n",
            "         [ 0.7728,  0.4862,  0.4078],\n",
            "         [ 0.0414,  0.0884, -0.9110],\n",
            "         [ 0.6436,  0.2938,  0.7068],\n",
            "         [ 0.1633, -0.3498,  0.2922],\n",
            "         [ 0.6816,  0.6715,  0.2014]]], device='cuda:0') tensor([ 0.5759, -0.5430,  0.9189,  0.0384,  0.7674, -0.4917], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.379150390625\n",
            "tcost icost 0.03643798828125 0.0\n",
            "tcost icost 0.06298828125 0.0\n",
            "tcost icost 0.09222412109375 0.0\n",
            "tcost icost 0.0308685302734375 0.0\n",
            "tcost icost 0.084716796875 0.0\n",
            "tcost icost 0.0081787109375 0.0\n",
            "loss tensor([[0.2507]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9090, -0.3993, -0.0396],\n",
            "         [ 0.8143,  0.4440,  0.3739],\n",
            "         [ 0.0867,  0.0970, -0.9204],\n",
            "         [ 0.6909,  0.3032,  0.6563],\n",
            "         [ 0.2077, -0.3556,  0.2740],\n",
            "         [ 0.7041,  0.6724,  0.2283]]], device='cuda:0') tensor([-1.0000,  0.8086,  0.2026,  0.2403,  0.8449, -0.4750], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.250732421875\n",
            "tcost icost -0.018310546875 0.0\n",
            "tcost icost -0.381591796875 0.0\n",
            "tcost icost -0.05731201171875 0.0\n",
            "tcost icost -0.313720703125 0.0\n",
            "tcost icost -0.04290771484375 0.0\n",
            "tcost icost -0.0635986328125 0.0\n",
            "loss tensor([[-0.7026]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9200, -0.3905, -0.0332],\n",
            "         [ 0.8478,  0.4045,  0.3430],\n",
            "         [ 0.1272,  0.1038, -0.9294],\n",
            "         [ 0.7251,  0.3136,  0.6131],\n",
            "         [ 0.2454, -0.3682,  0.2512],\n",
            "         [ 0.7056,  0.6604,  0.2569]]], device='cuda:0') tensor([-0.9391,  0.7222,  0.2540,  0.1546,  0.9036, -0.5420], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.70263671875\n",
            "tcost icost -0.0163116455078125 0.0\n",
            "tcost icost -0.374267578125 0.0\n",
            "tcost icost -0.056671142578125 0.0\n",
            "tcost icost -0.298095703125 0.0\n",
            "tcost icost -0.0400390625 0.0\n",
            "tcost icost -0.051239013671875 0.0\n",
            "loss tensor([[-0.6729]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9246, -0.3801, -0.0273],\n",
            "         [ 0.8745,  0.3684,  0.3154],\n",
            "         [ 0.1631,  0.1089, -0.9381],\n",
            "         [ 0.7479,  0.3251,  0.5788],\n",
            "         [ 0.2766, -0.3870,  0.2235],\n",
            "         [ 0.7026,  0.6500,  0.2895]]], device='cuda:0') tensor([-0.8675,  0.6301,  0.3065,  0.0616,  0.9608, -0.6056], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6728515625\n",
            "tcost icost -0.01404571533203125 0.0\n",
            "tcost icost -0.365234375 0.0\n",
            "tcost icost -0.055908203125 0.0\n",
            "tcost icost -0.277587890625 0.0\n",
            "tcost icost -0.036102294921875 0.0\n",
            "tcost icost -0.041717529296875 0.0\n",
            "loss tensor([[-0.6387]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9284, -0.3710, -0.0226],\n",
            "         [ 0.8955,  0.3363,  0.2915],\n",
            "         [ 0.1947,  0.1122, -0.9466],\n",
            "         [ 0.7610,  0.3381,  0.5537],\n",
            "         [ 0.3008, -0.4116,  0.1906],\n",
            "         [ 0.6961,  0.6407,  0.3240]]], device='cuda:0') tensor([-0.7719,  0.5290,  0.3640, -0.0460,  1.0000, -0.6568], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.638671875\n",
            "tcost icost -0.01093292236328125 0.0\n",
            "tcost icost -0.352783203125 0.0\n",
            "tcost icost -0.054962158203125 0.0\n",
            "tcost icost -0.251220703125 0.0\n",
            "tcost icost -0.03314208984375 0.0\n",
            "tcost icost -0.032135009765625 0.0\n",
            "loss tensor([[-0.5967]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9312, -0.3639, -0.0194],\n",
            "         [ 0.9117,  0.3086,  0.2712],\n",
            "         [ 0.2223,  0.1136, -0.9553],\n",
            "         [ 0.7657,  0.3524,  0.5380],\n",
            "         [ 0.3178, -0.4417,  0.1519],\n",
            "         [ 0.6859,  0.6327,  0.3595]]], device='cuda:0') tensor([-0.6292,  0.4224,  0.4291, -0.1608,  1.0000, -0.7080], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5966796875\n",
            "tcost icost -0.005565643310546875 0.0\n",
            "tcost icost -0.333740234375 0.0\n",
            "tcost icost -0.0537109375 0.0\n",
            "tcost icost -0.216796875 0.0\n",
            "tcost icost -0.0305328369140625 0.0\n",
            "tcost icost -0.0246734619140625 0.0\n",
            "loss tensor([[-0.5420]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9329, -0.3597, -0.0184],\n",
            "         [ 0.9240,  0.2854,  0.2545],\n",
            "         [ 0.2456,  0.1129, -0.9628],\n",
            "         [ 0.7636,  0.3681,  0.5305],\n",
            "         [ 0.3285, -0.4755,  0.1085],\n",
            "         [ 0.6741,  0.6249,  0.3939]]], device='cuda:0') tensor([-0.4318,  0.3097,  0.4941, -0.2846,  1.0000, -0.7396], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5419921875\n",
            "tcost icost 0.0017652511596679688 0.0\n",
            "tcost icost -0.2998046875 0.0\n",
            "tcost icost -0.050750732421875 0.0\n",
            "tcost icost -0.167724609375 0.0\n",
            "tcost icost -0.0255126953125 0.0\n",
            "tcost icost -0.0166473388671875 0.0\n",
            "loss tensor([[-0.4580]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9329, -0.3597, -0.0208],\n",
            "         [ 0.9327,  0.2676,  0.2416],\n",
            "         [ 0.2612,  0.1084, -0.9592],\n",
            "         [ 0.7558,  0.3848,  0.5298],\n",
            "         [ 0.3330, -0.5129,  0.0600],\n",
            "         [ 0.6606,  0.6174,  0.4271]]], device='cuda:0') tensor([-0.1417,  0.1794,  0.5880, -0.4133,  1.0000, -0.7722], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4580078125\n",
            "search tensor([[[ 0.3330, -0.5129,  0.0600],\n",
            "         [ 0.6606,  0.6174,  0.4271],\n",
            "         [ 0.1822,  0.3604, -0.2233],\n",
            "         [-0.4345,  0.1724,  0.1222],\n",
            "         [-0.2894,  0.1953,  0.0109],\n",
            "         [ 0.5285, -0.3362,  0.0591]]], device='cuda:0') tensor([[[ 1.0000],\n",
            "         [-0.7722],\n",
            "         [-0.6679],\n",
            "         [-1.2575],\n",
            "         [-0.1739],\n",
            "         [ 0.4555]]], device='cuda:0')\n",
            "tcost icost -0.0831298828125 0.0\n",
            "tcost icost -0.0234832763671875 0.0\n",
            "tcost icost -0.125732421875 0.0\n",
            "tcost icost -0.052978515625 0.0\n",
            "tcost icost -0.27587890625 0.0\n",
            "tcost icost -0.0308990478515625 0.0\n",
            "loss tensor([[-0.4441]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.4327, -0.4124,  0.1599],\n",
            "         [ 0.7599,  0.5168,  0.3267],\n",
            "         [ 0.2820,  0.4601, -0.1231],\n",
            "         [-0.3341,  0.2722,  0.2221],\n",
            "         [-0.1891,  0.2951,  0.1109],\n",
            "         [ 0.4280, -0.4359, -0.0409]]], device='cuda:0') tensor([ 0.6169, -0.6569, -0.8201, -1.0000, -0.2673,  0.4752], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.444091796875\n",
            "tcost icost 0.007061004638671875 0.0\n",
            "tcost icost 0.003940582275390625 0.0\n",
            "tcost icost -0.053070068359375 0.0\n",
            "tcost icost -0.033782958984375 0.0\n",
            "tcost icost -0.283203125 0.0\n",
            "tcost icost -0.03179931640625 0.0\n",
            "loss tensor([[-0.2615]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.5223, -0.3119,  0.2596],\n",
            "         [ 0.8577,  0.4233,  0.2312],\n",
            "         [ 0.3790,  0.5597, -0.0238],\n",
            "         [-0.2442,  0.3076,  0.2699],\n",
            "         [-0.0891,  0.3948,  0.2105],\n",
            "         [ 0.3278, -0.5350, -0.1408]]], device='cuda:0') tensor([ 0.2947, -0.4033, -0.9927, -1.0000, -0.3953,  0.4920], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.261474609375\n",
            "tcost icost 0.0075225830078125 0.0\n",
            "tcost icost 0.033966064453125 0.0\n",
            "tcost icost 0.0259552001953125 0.0\n",
            "tcost icost 0.0167694091796875 0.0\n",
            "tcost icost -0.049346923828125 0.0\n",
            "tcost icost 0.035675048828125 0.0\n",
            "loss tensor([[0.0601]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.5300, -0.2328,  0.3370],\n",
            "         [ 0.9277,  0.3415,  0.1507],\n",
            "         [ 0.3698,  0.6446,  0.0569],\n",
            "         [-0.2958,  0.2469,  0.2093],\n",
            "         [ 0.0088,  0.4900,  0.2988],\n",
            "         [ 0.2475, -0.6204, -0.2187]]], device='cuda:0') tensor([-1.0000,  0.7403, -1.0000, -0.6580, -0.6477,  0.5844], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.06005859375\n",
            "tcost icost -0.01727294921875 0.0\n",
            "tcost icost -0.30859375 0.0\n",
            "tcost icost -0.05999755859375 0.0\n",
            "tcost icost -0.2349853515625 0.0\n",
            "tcost icost -0.062164306640625 0.0\n",
            "tcost icost -0.348388671875 0.0\n",
            "loss tensor([[-0.7612]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.5424, -0.1693,  0.3991],\n",
            "         [ 0.9594,  0.2686,  0.0859],\n",
            "         [ 0.3634,  0.7118,  0.1214],\n",
            "         [-0.3020,  0.2101,  0.1665],\n",
            "         [ 0.0907,  0.5688,  0.3741],\n",
            "         [ 0.2442, -0.6545, -0.2525]]], device='cuda:0') tensor([-0.9679,  0.5739, -0.9817, -0.7700, -0.6656,  0.5267], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.76123046875\n",
            "tcost icost -0.016571044921875 0.0\n",
            "tcost icost -0.275146484375 0.0\n",
            "tcost icost -0.05950927734375 0.0\n",
            "tcost icost -0.21875 0.0\n",
            "tcost icost -0.061767578125 0.0\n",
            "tcost icost -0.3427734375 0.0\n",
            "loss tensor([[-0.7148]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.5589, -0.1185,  0.4496],\n",
            "         [ 0.9773,  0.2088,  0.0360],\n",
            "         [ 0.3596,  0.7667,  0.1738],\n",
            "         [-0.2818,  0.1911,  0.1361],\n",
            "         [ 0.1615,  0.6359,  0.4399],\n",
            "         [ 0.2737, -0.6586, -0.2587]]], device='cuda:0') tensor([-0.9094,  0.3956, -0.9595, -0.8827, -0.6851,  0.4684], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.71484375\n",
            "tcost icost -0.0151214599609375 0.0\n",
            "tcost icost -0.230224609375 0.0\n",
            "tcost icost -0.058258056640625 0.0\n",
            "tcost icost -0.1971435546875 0.0\n",
            "tcost icost -0.061004638671875 0.0\n",
            "tcost icost -0.334228515625 0.0\n",
            "loss tensor([[-0.6504]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 5.7924e-01, -8.0675e-02,  4.8940e-01],\n",
            "         [ 9.8681e-01,  1.6191e-01, -8.5053e-04],\n",
            "         [ 3.5717e-01,  8.1182e-01,  2.1561e-01],\n",
            "         [-2.4264e-01,  1.8717e-01,  1.1539e-01],\n",
            "         [ 2.2294e-01,  6.9426e-01,  4.9836e-01],\n",
            "         [ 3.2091e-01, -6.4232e-01, -2.4732e-01]]], device='cuda:0') tensor([-0.7806,  0.1906, -0.9185, -1.0000, -0.7055,  0.4101], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.650390625\n",
            "tcost icost -0.01248931884765625 0.0\n",
            "tcost icost -0.173095703125 0.0\n",
            "tcost icost -0.0540771484375 0.0\n",
            "tcost icost -0.15380859375 0.0\n",
            "tcost icost -0.057373046875 0.0\n",
            "tcost icost -0.323486328125 0.0\n",
            "loss tensor([[-0.5527]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.6046, -0.0570,  0.5189],\n",
            "         [ 0.9915,  0.1276, -0.0254],\n",
            "         [ 0.3559,  0.8490,  0.2476],\n",
            "         [-0.1897,  0.1962,  0.1035],\n",
            "         [ 0.2768,  0.7455,  0.5508],\n",
            "         [ 0.3795, -0.6100, -0.2226]]], device='cuda:0') tensor([-0.5701, -0.0573, -0.8569, -1.0000, -0.7270,  0.3508], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.552734375\n",
            "tcost icost -0.00738525390625 0.0\n",
            "tcost icost -0.0830078125 0.0\n",
            "tcost icost -0.038238525390625 0.0\n",
            "tcost icost -0.11956787109375 0.0\n",
            "tcost icost -0.05328369140625 0.0\n",
            "tcost icost -0.29833984375 0.0\n",
            "loss tensor([[-0.4111]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.6384, -0.0533,  0.5352],\n",
            "         [ 0.9935,  0.1078, -0.0364],\n",
            "         [ 0.3575,  0.8788,  0.2683],\n",
            "         [-0.1269,  0.2162,  0.0997],\n",
            "         [ 0.3100,  0.7582,  0.5737],\n",
            "         [ 0.4467, -0.5634, -0.1866]]], device='cuda:0') tensor([-0.1480, -0.3725, -0.7383, -1.0000, -0.7431,  0.2836], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4111328125\n",
            "tcost icost 0.002071380615234375 0.0\n",
            "tcost icost 0.0301513671875 0.0\n",
            "tcost icost 0.03399658203125 0.0\n",
            "tcost icost 0.0125579833984375 0.0\n",
            "tcost icost 0.003276824951171875 0.0\n",
            "tcost icost -0.281982421875 0.0\n",
            "loss tensor([[-0.0984]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.6860, -0.0707,  0.5361],\n",
            "         [ 0.9939,  0.1054, -0.0315],\n",
            "         [ 0.3710,  0.8886,  0.2697],\n",
            "         [-0.0602,  0.2413,  0.1030],\n",
            "         [ 0.3387,  0.7499,  0.5683],\n",
            "         [ 0.5210, -0.5031, -0.1401]]], device='cuda:0') tensor([ 0.5312, -0.7509, -0.4976, -1.0000, -0.6654,  0.1812], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.09844970703125\n",
            "tcost icost 0.00733184814453125 0.0\n",
            "tcost icost 0.0021877288818359375 0.0\n",
            "tcost icost -0.152099609375 0.0\n",
            "tcost icost -0.05224609375 0.0\n",
            "tcost icost -0.20751953125 0.0\n",
            "tcost icost -0.036590576171875 0.0\n",
            "loss tensor([[-0.3096]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.7139, -0.0799,  0.5422],\n",
            "         [ 0.9948,  0.0958, -0.0333],\n",
            "         [ 0.3764,  0.8856,  0.2720],\n",
            "         [ 0.0030,  0.2632,  0.1055],\n",
            "         [ 0.3665,  0.7386,  0.5657],\n",
            "         [ 0.5843, -0.4547, -0.1036]]], device='cuda:0') tensor([ 0.3058, -0.5524, -0.6725, -1.0000, -0.7875,  0.1990], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.3095703125\n",
            "tcost icost 0.00672149658203125 0.0\n",
            "tcost icost 0.0215301513671875 0.0\n",
            "tcost icost -0.035491943359375 0.0\n",
            "tcost icost -0.0252685546875 0.0\n",
            "tcost icost -0.1259765625 0.0\n",
            "tcost icost -0.0207672119140625 0.0\n",
            "loss tensor([[-0.1160]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.7134, -0.0733,  0.5594],\n",
            "         [ 0.9961,  0.0745, -0.0468],\n",
            "         [ 0.3678,  0.8859,  0.2827],\n",
            "         [ 0.0621,  0.2756,  0.0996],\n",
            "         [ 0.3915,  0.7252,  0.5664],\n",
            "         [ 0.6374, -0.4183, -0.0778]]], device='cuda:0') tensor([-0.2845, -0.1601, -0.8854, -0.9521, -0.9263,  0.2207], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.115966796875\n",
            "tcost icost 0.00015115737915039062 0.0\n",
            "tcost icost -0.0014476776123046875 0.0\n",
            "tcost icost -0.01032257080078125 0.0\n",
            "tcost icost -0.05242919921875 0.0\n",
            "tcost icost -0.047332763671875 0.0\n",
            "tcost icost -0.2802734375 0.0\n",
            "loss tensor([[-0.2444]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.7243, -0.0808,  0.5658],\n",
            "         [ 0.9967,  0.0665, -0.0474],\n",
            "         [ 0.3668,  0.8862,  0.2831],\n",
            "         [ 0.1284,  0.2961,  0.1016],\n",
            "         [ 0.4110,  0.7132,  0.5678],\n",
            "         [ 0.6958, -0.3702, -0.0431]]], device='cuda:0') tensor([ 0.2341, -0.4733, -0.7024, -1.0000, -0.9555,  0.1474], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.244384765625\n",
            "tcost icost 0.007274627685546875 0.0\n",
            "tcost icost 0.030670166015625 0.0\n",
            "tcost icost 0.023223876953125 0.0\n",
            "tcost icost -0.00662994384765625 0.0\n",
            "tcost icost -0.045501708984375 0.0\n",
            "tcost icost 0.0026302337646484375 0.0\n",
            "loss tensor([[0.0206]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.7064, -0.0660,  0.5879],\n",
            "         [ 0.9969,  0.0410, -0.0669],\n",
            "         [ 0.3524,  0.8882,  0.2947],\n",
            "         [ 0.1900,  0.3044,  0.0910],\n",
            "         [ 0.4245,  0.7015,  0.5725],\n",
            "         [ 0.7458, -0.3333, -0.0187]]], device='cuda:0') tensor([-0.7952,  0.2088, -0.9605, -0.9075, -1.0000,  0.1691], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.020599365234375\n",
            "tcost icost -0.01277923583984375 0.0\n",
            "tcost icost -0.178955078125 0.0\n",
            "tcost icost -0.055450439453125 0.0\n",
            "tcost icost -0.1802978515625 0.0\n",
            "tcost icost -0.06268310546875 0.0\n",
            "tcost icost -0.295654296875 0.0\n",
            "loss tensor([[-0.5654]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.6951, -0.0576,  0.6045],\n",
            "         [ 0.9966,  0.0235, -0.0792],\n",
            "         [ 0.3420,  0.8899,  0.3020],\n",
            "         [ 0.2573,  0.3184,  0.0869],\n",
            "         [ 0.4359,  0.6907,  0.5770],\n",
            "         [ 0.7991, -0.2871,  0.0140]]], device='cuda:0') tensor([-0.5969, -0.0382, -0.9068, -1.0000, -1.0000,  0.1073], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5654296875\n",
            "tcost icost -0.00803375244140625 0.0\n",
            "tcost icost -0.09307861328125 0.0\n",
            "tcost icost -0.041900634765625 0.0\n",
            "tcost icost -0.1356201171875 0.0\n",
            "tcost icost -0.059173583984375 0.0\n",
            "tcost icost -0.274169921875 0.0\n",
            "loss tensor([[-0.4253]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.6939, -0.0597,  0.6132],\n",
            "         [ 0.9965,  0.0161, -0.0826],\n",
            "         [ 0.3372,  0.8913,  0.3030],\n",
            "         [ 0.3292,  0.3379,  0.0896],\n",
            "         [ 0.4447,  0.6811,  0.5816],\n",
            "         [ 0.8552, -0.2322,  0.0540]]], device='cuda:0') tensor([-0.1999, -0.3399, -0.8042, -1.0000, -1.0000,  0.0379], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.42529296875\n",
            "tcost icost 0.001953125 0.0\n",
            "tcost icost 0.0218658447265625 0.0\n",
            "tcost icost 0.0162353515625 0.0\n",
            "tcost icost 0.0003380775451660156 0.0\n",
            "tcost icost -0.02850341796875 0.0\n",
            "tcost icost -0.260986328125 0.0\n",
            "loss tensor([[-0.1377]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.6994, -0.0708,  0.6154],\n",
            "         [ 0.9968,  0.0174, -0.0776],\n",
            "         [ 0.3426,  0.8913,  0.2971],\n",
            "         [ 0.4022,  0.3568,  0.0939],\n",
            "         [ 0.4556,  0.6723,  0.5835],\n",
            "         [ 0.9135, -0.1693,  0.1004]]], device='cuda:0') tensor([ 0.1944, -0.5911, -0.6485, -1.0000, -0.9998, -0.0473], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1376953125\n",
            "tcost icost 0.007305145263671875 0.0\n",
            "tcost icost 0.0255279541015625 0.0\n",
            "tcost icost -0.01117706298828125 0.0\n",
            "tcost icost -0.0162506103515625 0.0\n",
            "tcost icost -0.055450439453125 0.0\n",
            "tcost icost -0.01230621337890625 0.0\n",
            "loss tensor([[-0.0342]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.6827, -0.0645,  0.6299],\n",
            "         [ 0.9961,  0.0040, -0.0883],\n",
            "         [ 0.3352,  0.8927,  0.3013],\n",
            "         [ 0.4724,  0.3664,  0.0882],\n",
            "         [ 0.4613,  0.6638,  0.5887],\n",
            "         [ 0.9653, -0.1175,  0.1367]]], device='cuda:0') tensor([-0.6176, -0.0667, -0.8908, -0.9208, -1.0000, -0.0250], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.03424072265625\n",
            "tcost icost -0.00865936279296875 0.0\n",
            "tcost icost -0.0887451171875 0.0\n",
            "tcost icost -0.040863037109375 0.0\n",
            "tcost icost -0.156494140625 0.0\n",
            "tcost icost -0.060791015625 0.0\n",
            "tcost icost -0.2578125 0.0\n",
            "loss tensor([[-0.4277]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 6.7429e-01, -6.6983e-02,  6.3781e-01],\n",
            "         [ 9.9588e-01, -4.1546e-04, -9.0716e-02],\n",
            "         [ 3.3340e-01,  8.9388e-01,  2.9969e-01],\n",
            "         [ 5.4500e-01,  3.8009e-01,  8.9201e-02],\n",
            "         [ 4.6540e-01,  6.5616e-01,  5.9402e-01],\n",
            "         [ 9.8327e-01, -5.7434e-02,  1.7287e-01]]], device='cuda:0') tensor([-0.2656, -0.3687, -0.7922, -1.0000, -1.0000, -0.0957], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.427734375\n",
            "tcost icost 0.0006341934204101562 0.0\n",
            "tcost icost 0.015899658203125 0.0\n",
            "tcost icost 0.0121612548828125 0.0\n",
            "tcost icost -0.00913238525390625 0.0\n",
            "tcost icost -0.035736083984375 0.0\n",
            "tcost icost -0.2373046875 0.0\n",
            "loss tensor([[-0.1455]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.6724, -0.0778,  0.6396],\n",
            "         [ 0.9963,  0.0026, -0.0856],\n",
            "         [ 0.3404,  0.8939,  0.2915],\n",
            "         [ 0.6177,  0.3936,  0.0928],\n",
            "         [ 0.4717,  0.6487,  0.5972],\n",
            "         [ 0.9781,  0.0065,  0.2081]]], device='cuda:0') tensor([ 0.1297, -0.5804, -0.6403, -1.0000, -1.0000, -0.1776], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1455078125\n",
            "tcost icost 0.006946563720703125 0.0\n",
            "tcost icost 0.03143310546875 0.0\n",
            "tcost icost 0.0266876220703125 0.0\n",
            "tcost icost -0.0059356689453125 0.0\n",
            "tcost icost -0.0283050537109375 0.0\n",
            "tcost icost 0.00018858909606933594 0.0\n",
            "loss tensor([[0.0341]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.6469, -0.0654,  0.6578],\n",
            "         [ 0.9947, -0.0144, -0.1018],\n",
            "         [ 0.3334,  0.8947,  0.2972],\n",
            "         [ 0.6904,  0.3970,  0.0856],\n",
            "         [ 0.4702,  0.6425,  0.6051],\n",
            "         [ 0.9712,  0.0560,  0.2315]]], device='cuda:0') tensor([-1.0000,  0.2028, -0.9516, -0.8985, -1.0000, -0.1522], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.03411865234375\n",
            "search tensor([[[ 0.4702,  0.6425,  0.6051],\n",
            "         [ 0.9712,  0.0560,  0.2315],\n",
            "         [-0.1583, -0.0790, -0.2357],\n",
            "         [-0.2940, -0.1944, -0.1458],\n",
            "         [-0.2349, -0.2063, -0.2339],\n",
            "         [-0.0083, -0.4469, -0.4242]]], device='cuda:0') tensor([[[-1.0000],\n",
            "         [-0.1522],\n",
            "         [-0.3075],\n",
            "         [-0.8124],\n",
            "         [ 0.0649],\n",
            "         [ 0.1842]]], device='cuda:0')\n",
            "tcost icost -0.006317138671875 0.0\n",
            "tcost icost -0.2132568359375 0.0\n",
            "tcost icost -0.05572509765625 0.0\n",
            "tcost icost -0.18359375 0.0\n",
            "tcost icost -0.049835205078125 0.0\n",
            "tcost icost -0.27392578125 0.0\n",
            "loss tensor([[-0.5718]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.5697,  0.5419,  0.5044],\n",
            "         [ 0.9462,  0.1378,  0.2928],\n",
            "         [-0.2582, -0.1789, -0.3355],\n",
            "         [-0.1937, -0.0942, -0.0457],\n",
            "         [-0.3347, -0.3061, -0.3337],\n",
            "         [ 0.0917, -0.3464, -0.3238]]], device='cuda:0') tensor([-0.8816, -0.3444, -0.2481, -0.9281,  0.0877,  0.1044], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.57177734375\n",
            "tcost icost -0.002410888671875 0.0\n",
            "tcost icost -0.1683349609375 0.0\n",
            "tcost icost -0.051727294921875 0.0\n",
            "tcost icost -0.1553955078125 0.0\n",
            "tcost icost -0.046173095703125 0.0\n",
            "tcost icost -0.26025390625 0.0\n",
            "loss tensor([[-0.4932]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.6686,  0.4474,  0.4069],\n",
            "         [ 0.8914,  0.2347,  0.3877],\n",
            "         [-0.3579, -0.2785, -0.4352],\n",
            "         [-0.0951,  0.0056,  0.0530],\n",
            "         [-0.4321, -0.4058, -0.4180],\n",
            "         [ 0.1915, -0.2462, -0.2235]]], device='cuda:0') tensor([-0.7002, -0.5732, -0.1727, -1.0000,  0.1134,  0.0258], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4931640625\n",
            "tcost icost 0.0023059844970703125 0.0\n",
            "tcost icost -0.10064697265625 0.0\n",
            "tcost icost -0.043487548828125 0.0\n",
            "tcost icost -0.1253662109375 0.0\n",
            "tcost icost -0.043212890625 0.0\n",
            "tcost icost -0.232177734375 0.0\n",
            "loss tensor([[-0.3804]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.7669,  0.3598,  0.3133],\n",
            "         [ 0.8111,  0.3310,  0.4822],\n",
            "         [-0.4513, -0.3776, -0.5344],\n",
            "         [ 0.0032,  0.1055,  0.1519],\n",
            "         [-0.5008, -0.5053, -0.5073],\n",
            "         [ 0.2909, -0.1462, -0.1233]]], device='cuda:0') tensor([-0.4065, -0.8415, -0.0722, -1.0000,  0.1374, -0.0539], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.38037109375\n",
            "tcost icost 0.00946807861328125 0.0\n",
            "tcost icost -0.005596160888671875 0.0\n",
            "tcost icost -0.01515960693359375 0.0\n",
            "tcost icost -0.06951904296875 0.0\n",
            "tcost icost -0.031707763671875 0.0\n",
            "tcost icost -0.1875 0.0\n",
            "loss tensor([[-0.1901]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.8655,  0.2752,  0.2233],\n",
            "         [ 0.7145,  0.4169,  0.5618],\n",
            "         [-0.4585, -0.4726, -0.6263],\n",
            "         [ 0.1013,  0.2054,  0.2511],\n",
            "         [-0.4751, -0.6049, -0.5853],\n",
            "         [ 0.3902, -0.0463, -0.0232]]], device='cuda:0') tensor([ 0.1335, -1.0000,  0.1201, -1.0000,  0.1769, -0.1317], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1900634765625\n",
            "tcost icost 0.039306640625 0.0\n",
            "tcost icost 0.04461669921875 0.0\n",
            "tcost icost 0.189208984375 0.0\n",
            "tcost icost 0.017791748046875 0.0\n",
            "tcost icost 0.0723876953125 0.0\n",
            "tcost icost 0.1400146484375 0.0\n",
            "loss tensor([[0.3760]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9598,  0.1988,  0.1415],\n",
            "         [ 0.6169,  0.4838,  0.6208],\n",
            "         [-0.4079, -0.5687, -0.6996],\n",
            "         [ 0.1981,  0.3033,  0.3402],\n",
            "         [-0.4056, -0.6648, -0.6274],\n",
            "         [ 0.4881,  0.0510,  0.0670]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.2968, -1.0000,  0.5992, -0.1358], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.3759765625\n",
            "tcost icost 0.005130767822265625 0.0\n",
            "tcost icost -0.01386260986328125 0.0\n",
            "tcost icost -0.17138671875 0.0\n",
            "tcost icost -0.047149658203125 0.0\n",
            "tcost icost -0.301513671875 0.0\n",
            "tcost icost -0.051513671875 0.0\n",
            "loss tensor([[-0.4089]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9869,  0.1403,  0.0799],\n",
            "         [ 0.5586,  0.5212,  0.6452],\n",
            "         [-0.3444, -0.6032, -0.7194],\n",
            "         [ 0.2868,  0.3891,  0.4210],\n",
            "         [-0.3364, -0.6865, -0.6446],\n",
            "         [ 0.5719,  0.1316,  0.1399]]], device='cuda:0') tensor([ 0.6069, -0.8125,  0.1592, -1.0000,  0.5189, -0.1247], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.408935546875\n",
            "tcost icost 0.044219970703125 0.0\n",
            "tcost icost 0.022796630859375 0.0\n",
            "tcost icost -0.01345062255859375 0.0\n",
            "tcost icost -0.0150299072265625 0.0\n",
            "tcost icost -0.2376708984375 0.0\n",
            "tcost icost -0.0305633544921875 0.0\n",
            "loss tensor([[-0.1310]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9910,  0.1216,  0.0566],\n",
            "         [ 0.5668,  0.5302,  0.6306],\n",
            "         [-0.2945, -0.6094, -0.7362],\n",
            "         [ 0.3751,  0.4561,  0.4698],\n",
            "         [-0.2763, -0.6966, -0.6621],\n",
            "         [ 0.6454,  0.1977,  0.1955]]], device='cuda:0') tensor([-0.3062, -0.1719, -0.0610, -0.9326,  0.4171, -0.1096], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1309814453125\n",
            "tcost icost 0.01329803466796875 0.0\n",
            "tcost icost -0.16015625 0.0\n",
            "tcost icost -0.048797607421875 0.0\n",
            "tcost icost -0.09375 0.0\n",
            "tcost icost -0.03363037109375 0.0\n",
            "tcost icost -0.1866455078125 0.0\n",
            "loss tensor([[-0.3711]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9951,  0.0953,  0.0272],\n",
            "         [ 0.5686,  0.5389,  0.6215],\n",
            "         [-0.2520, -0.6198, -0.7432],\n",
            "         [ 0.4623,  0.5195,  0.5285],\n",
            "         [-0.2248, -0.7064, -0.6712],\n",
            "         [ 0.7186,  0.2689,  0.2622]]], device='cuda:0') tensor([ 0.1521, -0.4407,  0.0604, -1.0000,  0.4599, -0.2018], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.37109375\n",
            "tcost icost 0.04052734375 0.0\n",
            "tcost icost 0.0843505859375 0.0\n",
            "tcost icost 0.00817108154296875 0.0\n",
            "tcost icost 0.00611114501953125 0.0\n",
            "tcost icost 0.005062103271484375 0.0\n",
            "tcost icost -0.1103515625 0.0\n",
            "loss tensor([[0.0656]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9983,  0.0562, -0.0143],\n",
            "         [ 0.5572,  0.5509,  0.6213],\n",
            "         [-0.2145, -0.6341, -0.7429],\n",
            "         [ 0.5426,  0.5765,  0.5888],\n",
            "         [-0.1805, -0.7145, -0.6760],\n",
            "         [ 0.7900,  0.3418,  0.3348]]], device='cuda:0') tensor([ 1.0000, -0.7932,  0.2161, -1.0000,  0.5456, -0.3011], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0655517578125\n",
            "tcost icost 0.005130767822265625 0.0\n",
            "tcost icost -0.0054473876953125 0.0\n",
            "tcost icost -0.12237548828125 0.0\n",
            "tcost icost -0.041412353515625 0.0\n",
            "tcost icost -0.28564453125 0.0\n",
            "tcost icost -0.050445556640625 0.0\n",
            "loss tensor([[-0.3462]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9986,  0.0340, -0.0393],\n",
            "         [ 0.5652,  0.5548,  0.6106],\n",
            "         [-0.1775, -0.6403, -0.7474],\n",
            "         [ 0.5667,  0.5749,  0.5902],\n",
            "         [-0.1319, -0.7174, -0.6840],\n",
            "         [ 0.8327,  0.3955,  0.3877]]], device='cuda:0') tensor([ 0.5110, -0.5227,  0.0842, -1.0000,  0.4632, -0.2913], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.34619140625\n",
            "tcost icost 0.04864501953125 0.0\n",
            "tcost icost 0.06732177734375 0.0\n",
            "tcost icost 0.1527099609375 0.0\n",
            "tcost icost 0.010223388671875 0.0\n",
            "tcost icost -0.01525115966796875 0.0\n",
            "tcost icost 0.0296173095703125 0.0\n",
            "loss tensor([[0.2478]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9984,  0.0352, -0.0449],\n",
            "         [ 0.6038,  0.5449,  0.5819],\n",
            "         [-0.1477, -0.6451, -0.7497],\n",
            "         [ 0.6040,  0.5668,  0.5603],\n",
            "         [-0.0737, -0.7102, -0.7001],\n",
            "         [ 0.8246,  0.4084,  0.3915]]], device='cuda:0') tensor([-0.6619, -0.1514,  0.2720, -0.7026,  0.2951, -0.2369], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.247802734375\n",
            "tcost icost 0.0020694732666015625 0.0\n",
            "tcost icost -0.2142333984375 0.0\n",
            "tcost icost -0.046539306640625 0.0\n",
            "tcost icost -0.17041015625 0.0\n",
            "tcost icost -0.047943115234375 0.0\n",
            "tcost icost -0.1688232421875 0.0\n",
            "loss tensor([[-0.4839]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9980,  0.0311, -0.0548],\n",
            "         [ 0.6340,  0.5352,  0.5582],\n",
            "         [-0.1270, -0.6542, -0.7456],\n",
            "         [ 0.6317,  0.5582,  0.5379],\n",
            "         [-0.0269, -0.7051, -0.7086],\n",
            "         [ 0.8125,  0.4222,  0.4019]]], device='cuda:0') tensor([-0.3865, -0.3594,  0.3796, -0.8323,  0.3272, -0.3220], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.48388671875\n",
            "tcost icost 0.01013946533203125 0.0\n",
            "tcost icost -0.12548828125 0.0\n",
            "tcost icost -0.0305023193359375 0.0\n",
            "tcost icost -0.08367919921875 0.0\n",
            "tcost icost -0.034454345703125 0.0\n",
            "tcost icost -0.1334228515625 0.0\n",
            "loss tensor([[-0.2898]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9971,  0.0181, -0.0734],\n",
            "         [ 0.6528,  0.5282,  0.5430],\n",
            "         [-0.1182, -0.6682, -0.7345],\n",
            "         [ 0.6509,  0.5503,  0.5230],\n",
            "         [ 0.0082, -0.7021, -0.7121],\n",
            "         [ 0.7976,  0.4365,  0.4163]]], device='cuda:0') tensor([ 0.1121, -0.6985,  0.5490, -0.9792,  0.3608, -0.4051], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.289794921875\n",
            "tcost icost 0.038116455078125 0.0\n",
            "tcost icost 0.08331298828125 0.0\n",
            "tcost icost 0.1622314453125 0.0\n",
            "tcost icost 0.0196533203125 0.0\n",
            "tcost icost 0.02459716796875 0.0\n",
            "tcost icost 0.05615234375 0.0\n",
            "loss tensor([[0.3081]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9933, -0.0162, -0.1145],\n",
            "         [ 0.6407,  0.5367,  0.5490],\n",
            "         [-0.1285, -0.6853, -0.7169],\n",
            "         [ 0.6554,  0.5484,  0.5193],\n",
            "         [ 0.0151, -0.6991, -0.7148],\n",
            "         [ 0.7854,  0.4480,  0.4272]]], device='cuda:0') tensor([ 1.0000, -1.0000,  0.9721, -1.0000,  0.5493, -0.4118], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.30810546875\n",
            "tcost icost 0.01554107666015625 0.0\n",
            "tcost icost -0.0107269287109375 0.0\n",
            "tcost icost -0.282470703125 0.0\n",
            "tcost icost -0.052459716796875 0.0\n",
            "tcost icost -0.2484130859375 0.0\n",
            "tcost icost -0.052032470703125 0.0\n",
            "loss tensor([[-0.4551]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9886, -0.0403, -0.1448],\n",
            "         [ 0.6408,  0.5381,  0.5476],\n",
            "         [-0.1308, -0.6982, -0.7038],\n",
            "         [ 0.6602,  0.5453,  0.5165],\n",
            "         [ 0.0373, -0.6915, -0.7214],\n",
            "         [ 0.7760,  0.4567,  0.4351]]], device='cuda:0') tensor([ 0.6199, -0.7794,  0.9012, -0.9903,  0.4607, -0.4038], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.455078125\n",
            "tcost icost 0.047607421875 0.0\n",
            "tcost icost 0.0256500244140625 0.0\n",
            "tcost icost -0.1375732421875 0.0\n",
            "tcost icost -0.037750244140625 0.0\n",
            "tcost icost -0.158447265625 0.0\n",
            "tcost icost -0.03656005859375 0.0\n",
            "loss tensor([[-0.1938]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9857, -0.0518, -0.1606],\n",
            "         [ 0.6581,  0.5293,  0.5355],\n",
            "         [-0.1271, -0.7089, -0.6938],\n",
            "         [ 0.6685,  0.5391,  0.5123],\n",
            "         [ 0.0727, -0.6782, -0.7313],\n",
            "         [ 0.7698,  0.4630,  0.4394]]], device='cuda:0') tensor([ 0.0598, -0.3715,  0.7971, -0.9549,  0.3545, -0.3924], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.19384765625\n",
            "tcost icost 0.034210205078125 0.0\n",
            "tcost icost 0.004291534423828125 0.0\n",
            "tcost icost 0.028961181640625 0.0\n",
            "tcost icost 0.0298004150390625 0.0\n",
            "tcost icost 0.0060882568359375 0.0\n",
            "tcost icost -0.0557861328125 0.0\n",
            "loss tensor([[0.0543]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9785, -0.0760, -0.1917],\n",
            "         [ 0.6612,  0.5283,  0.5326],\n",
            "         [-0.1309, -0.7191, -0.6824],\n",
            "         [ 0.6772,  0.5319,  0.5084],\n",
            "         [ 0.0917, -0.6675, -0.7389],\n",
            "         [ 0.7607,  0.4704,  0.4473]]], device='cuda:0') tensor([ 0.9016, -0.9159,  0.9941, -0.9398,  0.4416, -0.4654], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.054290771484375\n",
            "tcost icost 0.029998779296875 0.0\n",
            "tcost icost 0.0006012916564941406 0.0\n",
            "tcost icost -0.2607421875 0.0\n",
            "tcost icost -0.050140380859375 0.0\n",
            "tcost icost -0.2138671875 0.0\n",
            "tcost icost -0.049102783203125 0.0\n",
            "loss tensor([[-0.3865]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9732, -0.0901, -0.2116],\n",
            "         [ 0.6770,  0.5193,  0.5216],\n",
            "         [-0.1286, -0.7280, -0.6734],\n",
            "         [ 0.6861,  0.5235,  0.5052],\n",
            "         [ 0.1239, -0.6511, -0.7488],\n",
            "         [ 0.7541,  0.4757,  0.4528]]], device='cuda:0') tensor([ 0.4668, -0.6276,  0.9345, -0.9235,  0.3466, -0.4571], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.386474609375\n",
            "tcost icost 0.052276611328125 0.0\n",
            "tcost icost 0.0703125 0.0\n",
            "tcost icost 0.042236328125 0.0\n",
            "tcost icost -0.01502227783203125 0.0\n",
            "tcost icost -0.0472412109375 0.0\n",
            "tcost icost -0.012451171875 0.0\n",
            "loss tensor([[0.1005]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9759, -0.0794, -0.2031],\n",
            "         [ 0.7211,  0.4898,  0.4886],\n",
            "         [-0.1026, -0.7276, -0.6783],\n",
            "         [ 0.7024,  0.5108,  0.4957],\n",
            "         [ 0.1627, -0.6308, -0.7587],\n",
            "         [ 0.7513,  0.4791,  0.4538]]], device='cuda:0') tensor([-1.0000,  0.4974,  0.5653, -0.8568,  0.2781, -0.4422], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1004638671875\n",
            "tcost icost -0.010040283203125 0.0\n",
            "tcost icost -0.345703125 0.0\n",
            "tcost icost -0.047149658203125 0.0\n",
            "tcost icost -0.146484375 0.0\n",
            "tcost icost -0.04229736328125 0.0\n",
            "tcost icost -0.15625 0.0\n",
            "loss tensor([[-0.5859]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9781, -0.0707, -0.1957],\n",
            "         [ 0.7585,  0.4623,  0.4594],\n",
            "         [-0.0842, -0.7300, -0.6782],\n",
            "         [ 0.7092,  0.5018,  0.4952],\n",
            "         [ 0.1929, -0.6170, -0.7630],\n",
            "         [ 0.7421,  0.4850,  0.4627]]], device='cuda:0') tensor([-0.8840,  0.3975,  0.6764, -1.0000,  0.3172, -0.5412], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.5859375\n",
            "search tensor([[[ 0.1929, -0.6170, -0.7630],\n",
            "         [ 0.7421,  0.4850,  0.4627],\n",
            "         [ 0.1639,  0.3593,  0.1509],\n",
            "         [ 0.5055, -0.1182, -0.2134],\n",
            "         [ 0.1026,  0.4329,  0.0714],\n",
            "         [ 0.2294,  0.0043,  0.3432]]], device='cuda:0') tensor([[[ 0.3172],\n",
            "         [-0.5412],\n",
            "         [ 0.1524],\n",
            "         [-0.1282],\n",
            "         [-0.1785],\n",
            "         [ 0.3391]]], device='cuda:0')\n",
            "tcost icost 0.00955963134765625 0.0\n",
            "tcost icost 0.03460693359375 0.0\n",
            "tcost icost -0.102294921875 0.0\n",
            "tcost icost -0.003963470458984375 0.0\n",
            "tcost icost -0.018218994140625 0.0\n",
            "tcost icost 0.020599365234375 0.0\n",
            "loss tensor([[-0.0449]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.2927, -0.5164, -0.6622],\n",
            "         [ 0.8414,  0.3845,  0.3622],\n",
            "         [ 0.2637,  0.4589,  0.2507],\n",
            "         [ 0.4050, -0.2181, -0.3132],\n",
            "         [ 0.2025,  0.5324,  0.1714],\n",
            "         [ 0.1292, -0.0957,  0.2428]]], device='cuda:0') tensor([-0.6393,  0.1190, -0.1281, -0.0076, -0.2717,  0.3609], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.04486083984375\n",
            "tcost icost -0.0120391845703125 0.0\n",
            "tcost icost -0.164794921875 0.0\n",
            "tcost icost -0.0305633544921875 0.0\n",
            "tcost icost -0.298095703125 0.0\n",
            "tcost icost -0.06561279296875 0.0\n",
            "tcost icost -0.2305908203125 0.0\n",
            "loss tensor([[-0.5815]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.3209, -0.4951, -0.6456],\n",
            "         [ 0.8909,  0.3318,  0.3102],\n",
            "         [ 0.3611,  0.4948,  0.2823],\n",
            "         [ 0.4625, -0.2045, -0.3104],\n",
            "         [ 0.2760,  0.5928,  0.2433],\n",
            "         [ 0.1963, -0.0466,  0.2874]]], device='cuda:0') tensor([-0.3002, -0.1642,  0.0274, -0.0996, -0.2661,  0.2920], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.58154296875\n",
            "tcost icost -0.00315093994140625 0.0\n",
            "tcost icost -0.04071044921875 0.0\n",
            "tcost icost 0.008209228515625 0.0\n",
            "tcost icost -0.1162109375 0.0\n",
            "tcost icost -0.047332763671875 0.0\n",
            "tcost icost -0.163330078125 0.0\n",
            "loss tensor([[-0.2454]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.2900, -0.5328, -0.6911],\n",
            "         [ 0.8949,  0.3282,  0.3025],\n",
            "         [ 0.4424,  0.4814,  0.2541],\n",
            "         [ 0.5389, -0.1522, -0.2713],\n",
            "         [ 0.3497,  0.6288,  0.2898],\n",
            "         [ 0.2771,  0.0233,  0.3522]]], device='cuda:0') tensor([ 0.5235, -0.7155,  0.3604, -0.2272, -0.2516,  0.2295], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.245361328125\n",
            "tcost icost 0.01198577880859375 0.0\n",
            "tcost icost 0.01392364501953125 0.0\n",
            "tcost icost -0.289306640625 0.0\n",
            "tcost icost -0.041595458984375 0.0\n",
            "tcost icost -0.1580810546875 0.0\n",
            "tcost icost -0.0258026123046875 0.0\n",
            "loss tensor([[-0.3589]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.2812, -0.5455, -0.7064],\n",
            "         [ 0.9146,  0.2972,  0.2741],\n",
            "         [ 0.4782,  0.4812,  0.2460],\n",
            "         [ 0.5996, -0.1220, -0.2463],\n",
            "         [ 0.4323,  0.6773,  0.3567],\n",
            "         [ 0.3396,  0.0660,  0.3889]]], device='cuda:0') tensor([ 0.0826, -0.3784,  0.2538, -0.1596, -0.3807,  0.2512], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.35888671875\n",
            "tcost icost 0.004695892333984375 0.0\n",
            "tcost icost 0.0322265625 0.0\n",
            "tcost icost 0.11163330078125 0.0\n",
            "tcost icost 0.029815673828125 0.0\n",
            "tcost icost 0.00939178466796875 0.0\n",
            "tcost icost 0.020263671875 0.0\n",
            "loss tensor([[0.1638]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.2453, -0.5837, -0.7517],\n",
            "         [ 0.9132,  0.3031,  0.2724],\n",
            "         [ 0.5338,  0.4494,  0.2043],\n",
            "         [ 0.6702, -0.0653, -0.1908],\n",
            "         [ 0.5056,  0.6672,  0.3303],\n",
            "         [ 0.4134,  0.1294,  0.4468]]], device='cuda:0') tensor([ 1.0000, -0.7912,  0.4496, -0.4625, -0.0831,  0.1239], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.163818359375\n",
            "tcost icost -0.045684814453125 0.0\n",
            "tcost icost -0.01540374755859375 0.0\n",
            "tcost icost -0.345458984375 0.0\n",
            "tcost icost -0.053192138671875 0.0\n",
            "tcost icost -0.264404296875 0.0\n",
            "tcost icost -0.0401611328125 0.0\n",
            "loss tensor([[-0.5757]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.2221, -0.5948, -0.7726],\n",
            "         [ 0.9164,  0.3000,  0.2651],\n",
            "         [ 0.5825,  0.4261,  0.1743],\n",
            "         [ 0.7325, -0.0186, -0.1411],\n",
            "         [ 0.5760,  0.6621,  0.3172],\n",
            "         [ 0.4770,  0.1792,  0.4909]]], device='cuda:0') tensor([ 0.5833, -0.6876,  0.3592, -0.4316, -0.1887,  0.1399], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.57568359375\n",
            "tcost icost 0.0119781494140625 0.0\n",
            "tcost icost 0.0125579833984375 0.0\n",
            "tcost icost -0.293212890625 0.0\n",
            "tcost icost -0.046783447265625 0.0\n",
            "tcost icost -0.1868896484375 0.0\n",
            "tcost icost -0.0308074951171875 0.0\n",
            "loss tensor([[-0.3892]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.2107, -0.5914, -0.7784],\n",
            "         [ 0.9289,  0.2793,  0.2432],\n",
            "         [ 0.6226,  0.4141,  0.1562],\n",
            "         [ 0.7877,  0.0173, -0.0980],\n",
            "         [ 0.6416,  0.6639,  0.3161],\n",
            "         [ 0.5337,  0.2179,  0.5231]]], device='cuda:0') tensor([ 0.1892, -0.3690,  0.2569, -0.3818, -0.3133,  0.1572], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.38916015625\n",
            "tcost icost 0.00829315185546875 0.0\n",
            "tcost icost 0.042327880859375 0.0\n",
            "tcost icost 0.02227783203125 0.0\n",
            "tcost icost 0.00777435302734375 0.0\n",
            "tcost icost 0.01189422607421875 0.0\n",
            "tcost icost 0.1190185546875 0.0\n",
            "loss tensor([[0.1482]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.2388, -0.5647, -0.7702],\n",
            "         [ 0.9545,  0.2281,  0.1919],\n",
            "         [ 0.6381,  0.4406,  0.1714],\n",
            "         [ 0.8340,  0.0214, -0.0900],\n",
            "         [ 0.6749,  0.6640,  0.3219],\n",
            "         [ 0.5999,  0.2242,  0.5138]]], device='cuda:0') tensor([-1.0000,  0.8761, -0.3544, -0.1703, -0.4632,  0.2879], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.148193359375\n",
            "tcost icost -0.0198211669921875 0.0\n",
            "tcost icost -0.333984375 0.0\n",
            "tcost icost -0.049163818359375 0.0\n",
            "tcost icost -0.31787109375 0.0\n",
            "tcost icost -0.065185546875 0.0\n",
            "tcost icost -0.296630859375 0.0\n",
            "loss tensor([[-0.8101]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.2629, -0.5422, -0.7636],\n",
            "         [ 0.9717,  0.1838,  0.1483],\n",
            "         [ 0.6533,  0.4612,  0.1830],\n",
            "         [ 0.8834,  0.0340, -0.0727],\n",
            "         [ 0.6903,  0.6484,  0.3209],\n",
            "         [ 0.6628,  0.2376,  0.5130]]], device='cuda:0') tensor([-0.9524,  0.7255, -0.3024, -0.2704, -0.4660,  0.2144], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.81005859375\n",
            "tcost icost -0.0189056396484375 0.0\n",
            "tcost icost -0.306884765625 0.0\n",
            "tcost icost -0.048187255859375 0.0\n",
            "tcost icost -0.30712890625 0.0\n",
            "tcost icost -0.06561279296875 0.0\n",
            "tcost icost -0.281982421875 0.0\n",
            "loss tensor([[-0.7671]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.2829, -0.5242, -0.7591],\n",
            "         [ 0.9828,  0.1468,  0.1122],\n",
            "         [ 0.6682,  0.4763,  0.1910],\n",
            "         [ 0.9346,  0.0540, -0.0483],\n",
            "         [ 0.7047,  0.6330,  0.3206],\n",
            "         [ 0.7219,  0.2567,  0.5178]]], device='cuda:0') tensor([-0.8681,  0.5603, -0.2418, -0.3778, -0.4675,  0.1420], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.76708984375\n",
            "tcost icost -0.0171661376953125 0.0\n",
            "tcost icost -0.275146484375 0.0\n",
            "tcost icost -0.046356201171875 0.0\n",
            "tcost icost -0.284423828125 0.0\n",
            "tcost icost -0.06512451171875 0.0\n",
            "tcost icost -0.2646484375 0.0\n",
            "loss tensor([[-0.7090]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.2981, -0.5114, -0.7573],\n",
            "         [ 0.9896,  0.1171,  0.0835],\n",
            "         [ 0.6832,  0.4863,  0.1952],\n",
            "         [ 0.9863,  0.0803, -0.0177],\n",
            "         [ 0.7181,  0.6175,  0.3210],\n",
            "         [ 0.7767,  0.2799,  0.5265]]], device='cuda:0') tensor([-0.7409,  0.3749, -0.1669, -0.4916, -0.4726,  0.0681], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.708984375\n",
            "tcost icost -0.0142822265625 0.0\n",
            "tcost icost -0.232666015625 0.0\n",
            "tcost icost -0.041595458984375 0.0\n",
            "tcost icost -0.2498779296875 0.0\n",
            "tcost icost -0.0638427734375 0.0\n",
            "tcost icost -0.2401123046875 0.0\n",
            "loss tensor([[-0.6230]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.3071, -0.5055, -0.7597],\n",
            "         [ 0.9934,  0.0957,  0.0629],\n",
            "         [ 0.6985,  0.4904,  0.1948],\n",
            "         [ 0.9939,  0.1086,  0.0180],\n",
            "         [ 0.7314,  0.6015,  0.3213],\n",
            "         [ 0.8007,  0.2965,  0.5205]]], device='cuda:0') tensor([-0.5205,  0.1451, -0.0593, -0.6201, -0.4745, -0.0042], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.623046875\n",
            "tcost icost -0.00891876220703125 0.0\n",
            "tcost icost -0.1605224609375 0.0\n",
            "tcost icost -0.028411865234375 0.0\n",
            "tcost icost -0.170654296875 0.0\n",
            "tcost icost -0.0589599609375 0.0\n",
            "tcost icost -0.2039794921875 0.0\n",
            "loss tensor([[-0.4597]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.3065, -0.5100, -0.7694],\n",
            "         [ 0.9950,  0.0845,  0.0522],\n",
            "         [ 0.7162,  0.4867,  0.1875],\n",
            "         [ 0.9884,  0.1403,  0.0574],\n",
            "         [ 0.7446,  0.5848,  0.3217],\n",
            "         [ 0.8042,  0.3091,  0.5077]]], device='cuda:0') tensor([-0.1309, -0.1783,  0.1065, -0.7643, -0.4778, -0.0772], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.459716796875\n",
            "tcost icost 0.0003204345703125 0.0\n",
            "tcost icost -0.0040435791015625 0.0\n",
            "tcost icost 0.049102783203125 0.0\n",
            "tcost icost 0.04339599609375 0.0\n",
            "tcost icost -0.0115814208984375 0.0\n",
            "tcost icost -0.1773681640625 0.0\n",
            "loss tensor([[-0.0442]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.2909, -0.5284, -0.7915],\n",
            "         [ 0.9948,  0.0864,  0.0532],\n",
            "         [ 0.7415,  0.4723,  0.1705],\n",
            "         [ 0.9815,  0.1672,  0.0932],\n",
            "         [ 0.7659,  0.5596,  0.3166],\n",
            "         [ 0.8037,  0.3248,  0.4985]]], device='cuda:0') tensor([ 0.4936, -0.6329,  0.3644, -0.7314, -0.4465, -0.1685], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.044219970703125\n",
            "tcost icost 0.0117950439453125 0.0\n",
            "tcost icost 0.017608642578125 0.0\n",
            "tcost icost -0.25830078125 0.0\n",
            "tcost icost -0.04864501953125 0.0\n",
            "tcost icost -0.12091064453125 0.0\n",
            "tcost icost -0.029754638671875 0.0\n",
            "loss tensor([[-0.3140]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.2836, -0.5324, -0.7976],\n",
            "         [ 0.9959,  0.0785,  0.0456],\n",
            "         [ 0.7603,  0.4651,  0.1611],\n",
            "         [ 0.9750,  0.1857,  0.1221],\n",
            "         [ 0.7768,  0.5435,  0.3182],\n",
            "         [ 0.8050,  0.3370,  0.4883]]], device='cuda:0') tensor([-0.0279, -0.2413,  0.2519, -0.6744, -0.5712, -0.1544], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.31396484375\n",
            "tcost icost 0.0014352798461914062 0.0\n",
            "tcost icost 0.0164794921875 0.0\n",
            "tcost icost 0.14208984375 0.0\n",
            "tcost icost 0.033477783203125 0.0\n",
            "tcost icost 0.0034313201904296875 0.0\n",
            "tcost icost -0.131103515625 0.0\n",
            "loss tensor([[0.0806]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.2613, -0.5386, -0.8010],\n",
            "         [ 0.9959,  0.0787,  0.0448],\n",
            "         [ 0.7882,  0.4505,  0.1465],\n",
            "         [ 0.9677,  0.2026,  0.1501],\n",
            "         [ 0.8038,  0.5093,  0.3076],\n",
            "         [ 0.8027,  0.3525,  0.4810]]], device='cuda:0') tensor([ 0.5454, -0.5758,  0.3460, -0.6614, -0.4728, -0.2506], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.08056640625\n",
            "tcost icost 0.012054443359375 0.0\n",
            "tcost icost 0.0178070068359375 0.0\n",
            "tcost icost -0.257568359375 0.0\n",
            "tcost icost -0.04766845703125 0.0\n",
            "tcost icost -0.1160888671875 0.0\n",
            "tcost icost -0.0311279296875 0.0\n",
            "loss tensor([[-0.3098]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.2516, -0.5400, -0.8032],\n",
            "         [ 0.9969,  0.0697,  0.0357],\n",
            "         [ 0.8099,  0.4433,  0.1393],\n",
            "         [ 0.9620,  0.2121,  0.1720],\n",
            "         [ 0.8191,  0.4863,  0.3042],\n",
            "         [ 0.8026,  0.3646,  0.4722]]], device='cuda:0') tensor([ 0.0307, -0.1857,  0.2371, -0.6005, -0.5978, -0.2367], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.309814453125\n",
            "tcost icost 0.0030384063720703125 0.0\n",
            "tcost icost 0.0209197998046875 0.0\n",
            "tcost icost 0.1510009765625 0.0\n",
            "tcost icost 0.034332275390625 0.0\n",
            "tcost icost 0.0031719207763671875 0.0\n",
            "tcost icost -0.115966796875 0.0\n",
            "loss tensor([[0.1028]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.2275, -0.5466, -0.8059],\n",
            "         [ 0.9970,  0.0699,  0.0345],\n",
            "         [ 0.8401,  0.4277,  0.1264],\n",
            "         [ 0.9557,  0.2213,  0.1940],\n",
            "         [ 0.8467,  0.4468,  0.2889],\n",
            "         [ 0.7988,  0.3796,  0.4668]]], device='cuda:0') tensor([ 0.6870, -0.5624,  0.3414, -0.5864, -0.4950, -0.3352], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.102783203125\n",
            "tcost icost 0.0115966796875 0.0\n",
            "tcost icost 0.01309967041015625 0.0\n",
            "tcost icost -0.29052734375 0.0\n",
            "tcost icost -0.04974365234375 0.0\n",
            "tcost icost -0.13720703125 0.0\n",
            "tcost icost -0.035491943359375 0.0\n",
            "loss tensor([[-0.3591]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.2132, -0.5493, -0.8079],\n",
            "         [ 0.9976,  0.0631,  0.0272],\n",
            "         [ 0.8655,  0.4184,  0.1199],\n",
            "         [ 0.9512,  0.2246,  0.2115],\n",
            "         [ 0.8635,  0.4189,  0.2810],\n",
            "         [ 0.7968,  0.3917,  0.4602]]], device='cuda:0') tensor([ 0.2903, -0.2614,  0.2400, -0.5363, -0.6169, -0.3258], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.359130859375\n",
            "tcost icost 0.0087890625 0.0\n",
            "tcost icost 0.0462646484375 0.0\n",
            "tcost icost 0.01385498046875 0.0\n",
            "tcost icost -0.0017480850219726562 0.0\n",
            "tcost icost 0.006717681884765625 0.0\n",
            "tcost icost 0.032135009765625 0.0\n",
            "loss tensor([[0.0837]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.2202, -0.5389, -0.8067],\n",
            "         [ 0.9993,  0.0381,  0.0011],\n",
            "         [ 0.8849,  0.4258,  0.1269],\n",
            "         [ 0.9510,  0.2190,  0.2183],\n",
            "         [ 0.8737,  0.3997,  0.2773],\n",
            "         [ 0.8010,  0.3985,  0.4468]]], device='cuda:0') tensor([-0.7840,  0.5458, -0.1607, -0.4564, -0.6504, -0.2863], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.083740234375\n",
            "search tensor([[[ 0.8737,  0.3997,  0.2773],\n",
            "         [ 0.8010,  0.3985,  0.4468],\n",
            "         [ 0.4381,  0.4839,  0.1902],\n",
            "         [-0.3773, -0.0732, -0.4091],\n",
            "         [-0.1376, -0.3937, -0.4554],\n",
            "         [ 0.4304, -0.1728, -0.3281]]], device='cuda:0') tensor([[[-0.6504],\n",
            "         [-0.2863],\n",
            "         [-0.3533],\n",
            "         [-0.2695],\n",
            "         [-0.6784],\n",
            "         [ 0.1203]]], device='cuda:0')\n",
            "tcost icost 0.0032291412353515625 0.0\n",
            "tcost icost -0.1641845703125 0.0\n",
            "tcost icost -0.050079345703125 0.0\n",
            "tcost icost -0.2421875 0.0\n",
            "tcost icost -0.06707763671875 0.0\n",
            "tcost icost -0.2269287109375 0.0\n",
            "loss tensor([[-0.5396]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.9417,  0.2897,  0.1714],\n",
            "         [ 0.6876,  0.4892,  0.5366],\n",
            "         [ 0.5377,  0.3834,  0.0900],\n",
            "         [-0.2769,  0.0269, -0.3087],\n",
            "         [-0.0375, -0.2933, -0.3550],\n",
            "         [ 0.5300, -0.0726, -0.2278]]], device='cuda:0') tensor([-0.4290, -0.5054, -0.2797, -0.3768, -0.6869,  0.0496], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.53955078125\n",
            "tcost icost 0.010009765625 0.0\n",
            "tcost icost -0.08294677734375 0.0\n",
            "tcost icost -0.03753662109375 0.0\n",
            "tcost icost -0.201171875 0.0\n",
            "tcost icost -0.06329345703125 0.0\n",
            "tcost icost -0.2069091796875 0.0\n",
            "loss tensor([[-0.4053]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.9809,  0.1815,  0.0702],\n",
            "         [ 0.5614,  0.5619,  0.6075],\n",
            "         [ 0.5897,  0.2856, -0.0077],\n",
            "         [-0.1790,  0.1267, -0.2087],\n",
            "         [-0.0118, -0.3296, -0.2578],\n",
            "         [ 0.6282,  0.0271, -0.1279]]], device='cuda:0') tensor([-0.0179, -0.7723, -0.1688, -0.5016, -0.6969, -0.0255], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4052734375\n",
            "tcost icost 0.034210205078125 0.0\n",
            "tcost icost 0.0506591796875 0.0\n",
            "tcost icost 0.020355224609375 0.0\n",
            "tcost icost -0.01030731201171875 0.0\n",
            "tcost icost -0.038238525390625 0.0\n",
            "tcost icost -0.1767578125 0.0\n",
            "loss tensor([[-0.0406]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.9965,  0.0808, -0.0230],\n",
            "         [ 0.4318,  0.6144,  0.6603],\n",
            "         [ 0.6635,  0.1918, -0.0988],\n",
            "         [-0.0801,  0.2177, -0.1340],\n",
            "         [-0.0631, -0.3974, -0.2392],\n",
            "         [ 0.7251,  0.1266, -0.0280]]], device='cuda:0') tensor([ 0.5837, -0.9594,  0.0300, -0.4996, -0.6918, -0.1030], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0406494140625\n",
            "tcost icost 0.0692138671875 0.0\n",
            "tcost icost 0.016265869140625 0.0\n",
            "tcost icost -0.06982421875 0.0\n",
            "tcost icost -0.0267486572265625 0.0\n",
            "tcost icost -0.07672119140625 0.0\n",
            "tcost icost -0.02044677734375 0.0\n",
            "loss tensor([[-0.0546]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9973,  0.0609, -0.0416],\n",
            "         [ 0.4685,  0.6056,  0.6432],\n",
            "         [ 0.7452,  0.1799, -0.1082],\n",
            "         [ 0.0037,  0.2616, -0.1414],\n",
            "         [-0.0088, -0.3516, -0.1810],\n",
            "         [ 0.8020,  0.1961,  0.0397]]], device='cuda:0') tensor([-0.1038, -0.4696, -0.1971, -0.4137, -0.8271, -0.0827], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.054595947265625\n",
            "tcost icost 0.0277099609375 0.0\n",
            "tcost icost -0.01087188720703125 0.0\n",
            "tcost icost -0.014434814453125 0.0\n",
            "tcost icost -0.11224365234375 0.0\n",
            "tcost icost -0.0556640625 0.0\n",
            "tcost icost -0.17041015625 0.0\n",
            "loss tensor([[-0.2129]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9961,  0.0200, -0.0858],\n",
            "         [ 0.4905,  0.5958,  0.6359],\n",
            "         [ 0.7422,  0.1442, -0.1382],\n",
            "         [ 0.0915,  0.3217, -0.1091],\n",
            "         [ 0.0391, -0.3142, -0.1266],\n",
            "         [ 0.8770,  0.2688,  0.1154]]], device='cuda:0') tensor([ 0.6242, -0.8136, -0.0112, -0.5559, -0.8396, -0.1503], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.212890625\n",
            "tcost icost 0.06884765625 0.0\n",
            "tcost icost 0.0191802978515625 0.0\n",
            "tcost icost -0.0298919677734375 0.0\n",
            "tcost icost -0.0206298828125 0.0\n",
            "tcost icost -0.054046630859375 0.0\n",
            "tcost icost -0.011688232421875 0.0\n",
            "loss tensor([[0.0045]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9956,  0.0146, -0.0926],\n",
            "         [ 0.5465,  0.5661,  0.6089],\n",
            "         [ 0.7791,  0.1466, -0.1325],\n",
            "         [ 0.1712,  0.3578, -0.1105],\n",
            "         [ 0.1020, -0.2594, -0.0597],\n",
            "         [ 0.9322,  0.3204,  0.1682]]], device='cuda:0') tensor([-0.0564, -0.3305, -0.2805, -0.4826, -0.9181, -0.1285], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.004482269287109375\n",
            "tcost icost 0.031341552734375 0.0\n",
            "tcost icost -0.031219482421875 0.0\n",
            "tcost icost -0.0237884521484375 0.0\n",
            "tcost icost -0.1104736328125 0.0\n",
            "tcost icost -0.054534912109375 0.0\n",
            "tcost icost -0.1717529296875 0.0\n",
            "loss tensor([[-0.2338]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9925, -0.0105, -0.1222],\n",
            "         [ 0.5884,  0.5467,  0.5957],\n",
            "         [ 0.7849,  0.1311, -0.1421],\n",
            "         [ 0.2559,  0.4056, -0.0853],\n",
            "         [ 0.1586, -0.2128,  0.0026],\n",
            "         [ 0.9144,  0.3460,  0.2103]]], device='cuda:0') tensor([ 0.6589, -0.6598, -0.1123, -0.6213, -0.9265, -0.1918], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2337646484375\n",
            "tcost icost 0.06793212890625 0.0\n",
            "tcost icost 0.0232391357421875 0.0\n",
            "tcost icost 0.01849365234375 0.0\n",
            "tcost icost -0.0109100341796875 0.0\n",
            "tcost icost -0.03460693359375 0.0\n",
            "tcost icost 0.0007691383361816406 0.0\n",
            "loss tensor([[0.0736]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.9918, -0.0122, -0.1274],\n",
            "         [ 0.6470,  0.5047,  0.5625],\n",
            "         [ 0.8008,  0.1317, -0.1369],\n",
            "         [ 0.3360,  0.4344, -0.0864],\n",
            "         [ 0.2238, -0.1535,  0.0737],\n",
            "         [ 0.9025,  0.3607,  0.2356]]], device='cuda:0') tensor([ 0.0026, -0.2643, -0.2198, -0.5338, -0.9934, -0.1666], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0736083984375\n",
            "tcost icost 0.0355224609375 0.0\n",
            "tcost icost -0.0250701904296875 0.0\n",
            "tcost icost -0.0206146240234375 0.0\n",
            "tcost icost -0.0853271484375 0.0\n",
            "tcost icost -0.052490234375 0.0\n",
            "tcost icost -0.1673583984375 0.0\n",
            "loss tensor([[-0.1992]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.9842, -0.0322, -0.1534],\n",
            "         [ 0.6884,  0.4812,  0.5428],\n",
            "         [ 0.7928,  0.1165, -0.1461],\n",
            "         [ 0.4196,  0.4706, -0.0700],\n",
            "         [ 0.2833, -0.1013,  0.1400],\n",
            "         [ 0.8884,  0.3752,  0.2646]]], device='cuda:0') tensor([ 0.7678, -0.6149, -0.0472, -0.6598, -1.0000, -0.2334], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.19921875\n",
            "tcost icost 0.06927490234375 0.0\n",
            "tcost icost 0.02044677734375 0.0\n",
            "tcost icost 0.00647735595703125 0.0\n",
            "tcost icost -0.0163726806640625 0.0\n",
            "tcost icost -0.0482177734375 0.0\n",
            "tcost icost -0.01122283935546875 0.0\n",
            "loss tensor([[0.0427]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.9868, -0.0327, -0.1587],\n",
            "         [ 0.7439,  0.4368,  0.5058],\n",
            "         [ 0.8024,  0.1182, -0.1395],\n",
            "         [ 0.4995,  0.4927, -0.0714],\n",
            "         [ 0.3484, -0.0389,  0.2129],\n",
            "         [ 0.8802,  0.3821,  0.2816]]], device='cuda:0') tensor([ 0.1081, -0.2201, -0.2103, -0.5822, -1.0000, -0.2103], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.04266357421875\n",
            "tcost icost 0.0406494140625 0.0\n",
            "tcost icost -0.00731658935546875 0.0\n",
            "tcost icost -0.018096923828125 0.0\n",
            "tcost icost -0.061370849609375 0.0\n",
            "tcost icost -0.049957275390625 0.0\n",
            "tcost icost -0.1656494140625 0.0\n",
            "loss tensor([[-0.1560]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.9736, -0.0517, -0.1841],\n",
            "         [ 0.7763,  0.4107,  0.4782],\n",
            "         [ 0.7939,  0.1062, -0.1456],\n",
            "         [ 0.5811,  0.5193, -0.0598],\n",
            "         [ 0.4084,  0.0160,  0.2797],\n",
            "         [ 0.8698,  0.3898,  0.3026]]], device='cuda:0') tensor([ 1.0000, -0.5793, -0.0440, -0.6884, -1.0000, -0.2762], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.156005859375\n",
            "tcost icost 0.038055419921875 0.0\n",
            "tcost icost 0.0003204345703125 0.0\n",
            "tcost icost -0.0675048828125 0.0\n",
            "tcost icost -0.03582763671875 0.0\n",
            "tcost icost -0.08380126953125 0.0\n",
            "tcost icost -0.030731201171875 0.0\n",
            "loss tensor([[-0.1156]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.9789, -0.0562, -0.1948],\n",
            "         [ 0.8188,  0.3692,  0.4397],\n",
            "         [ 0.8111,  0.1108, -0.1358],\n",
            "         [ 0.6578,  0.5386, -0.0555],\n",
            "         [ 0.4712,  0.0780,  0.3504],\n",
            "         [ 0.8636,  0.3929,  0.3158]]], device='cuda:0') tensor([ 0.4272, -0.2604, -0.2618, -0.6494, -1.0000, -0.2622], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1156005859375\n",
            "tcost icost 0.07098388671875 0.0\n",
            "tcost icost 0.1607666015625 0.0\n",
            "tcost icost 0.0194854736328125 0.0\n",
            "tcost icost 0.01020050048828125 0.0\n",
            "tcost icost -0.0186004638671875 0.0\n",
            "tcost icost -0.1943359375 0.0\n",
            "loss tensor([[0.1118]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.9757, -0.0639, -0.2095],\n",
            "         [ 0.8515,  0.3335,  0.4046],\n",
            "         [ 0.8218,  0.1060, -0.1338],\n",
            "         [ 0.7337,  0.5565, -0.0449],\n",
            "         [ 0.5303,  0.1174,  0.3973],\n",
            "         [ 0.8535,  0.4002,  0.3338]]], device='cuda:0') tensor([ 0.6456, -0.3109, -0.1192, -0.6513, -0.9740, -0.3598], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.11181640625\n",
            "tcost icost 0.072509765625 0.0\n",
            "tcost icost 0.07452392578125 0.0\n",
            "tcost icost 0.05621337890625 0.0\n",
            "tcost icost 0.01248931884765625 0.0\n",
            "tcost icost 0.007595062255859375 0.0\n",
            "tcost icost 0.15185546875 0.0\n",
            "loss tensor([[0.2891]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.9716, -0.0711, -0.2259],\n",
            "         [ 0.8698,  0.3149,  0.3798],\n",
            "         [ 0.8255,  0.0862, -0.1508],\n",
            "         [ 0.7990,  0.5771, -0.0344],\n",
            "         [ 0.5980,  0.1248,  0.4020],\n",
            "         [ 0.8493,  0.4017,  0.3424]]], device='cuda:0') tensor([ 0.7173, -0.3987,  0.1467, -0.6877, -0.8603, -0.3331], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2890625\n",
            "tcost icost 0.071533203125 0.0\n",
            "tcost icost 0.038543701171875 0.0\n",
            "tcost icost 0.06304931640625 0.0\n",
            "tcost icost -0.01219940185546875 0.0\n",
            "tcost icost -0.03460693359375 0.0\n",
            "tcost icost -0.007061004638671875 0.0\n",
            "loss tensor([[0.1215]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.9761, -0.0521, -0.2111],\n",
            "         [ 0.9043,  0.2690,  0.3315],\n",
            "         [ 0.8553,  0.0881, -0.1496],\n",
            "         [ 0.8265,  0.5620, -0.0342],\n",
            "         [ 0.6600,  0.1431,  0.4161],\n",
            "         [ 0.8513,  0.3974,  0.3425]]], device='cuda:0') tensor([-0.6274,  0.5027, -0.0720, -0.6069, -0.9204, -0.3076], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1214599609375\n",
            "tcost icost 0.0025806427001953125 0.0\n",
            "tcost icost -0.334716796875 0.0\n",
            "tcost icost -0.0572509765625 0.0\n",
            "tcost icost -0.156005859375 0.0\n",
            "tcost icost -0.05859375 0.0\n",
            "tcost icost -0.1728515625 0.0\n",
            "loss tensor([[-0.5991]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.9790, -0.0378, -0.2004],\n",
            "         [ 0.9293,  0.2291,  0.2895],\n",
            "         [ 0.8791,  0.0848, -0.1509],\n",
            "         [ 0.8394,  0.5431, -0.0204],\n",
            "         [ 0.7175,  0.1584,  0.4298],\n",
            "         [ 0.8494,  0.3960,  0.3488]]], device='cuda:0') tensor([-0.4308,  0.3857,  0.0109, -0.7226, -0.9234, -0.3736], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.59912109375\n",
            "tcost icost 0.008575439453125 0.0\n",
            "tcost icost -0.29541015625 0.0\n",
            "tcost icost -0.054656982421875 0.0\n",
            "tcost icost -0.1072998046875 0.0\n",
            "tcost icost -0.054412841796875 0.0\n",
            "tcost icost -0.1553955078125 0.0\n",
            "loss tensor([[-0.5073]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.9802, -0.0299, -0.1959],\n",
            "         [ 0.9467,  0.1969,  0.2550],\n",
            "         [ 0.8961,  0.0757, -0.1555],\n",
            "         [ 0.8491,  0.5282,  0.0036],\n",
            "         [ 0.7714,  0.1701,  0.4421],\n",
            "         [ 0.8447,  0.3970,  0.3591]]], device='cuda:0') tensor([-0.1149,  0.2564,  0.1151, -0.8374, -0.9211, -0.4345], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.50732421875\n",
            "tcost icost 0.0224761962890625 0.0\n",
            "tcost icost -0.1937255859375 0.0\n",
            "tcost icost -0.043426513671875 0.0\n",
            "tcost icost -0.038482666015625 0.0\n",
            "tcost icost -0.04669189453125 0.0\n",
            "tcost icost -0.136474609375 0.0\n",
            "loss tensor([[-0.3262]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.9788, -0.0314, -0.2024],\n",
            "         [ 0.9578,  0.1746,  0.2284],\n",
            "         [ 0.9071,  0.0617, -0.1634],\n",
            "         [ 0.8570,  0.5144,  0.0308],\n",
            "         [ 0.8218,  0.1782,  0.4525],\n",
            "         [ 0.8373,  0.4004,  0.3723]]], device='cuda:0') tensor([ 0.3885,  0.0709,  0.2245, -0.8805, -0.9141, -0.4950], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.326171875\n",
            "tcost icost 0.06634521484375 0.0\n",
            "tcost icost 0.10809326171875 0.0\n",
            "tcost icost 0.01129913330078125 0.0\n",
            "tcost icost 0.0106353759765625 0.0\n",
            "tcost icost -0.00913238525390625 0.0\n",
            "tcost icost -0.1414794921875 0.0\n",
            "loss tensor([[0.0909]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.9683, -0.0487, -0.2276],\n",
            "         [ 0.9614,  0.1714,  0.2153],\n",
            "         [ 0.9140,  0.0430, -0.1745],\n",
            "         [ 0.8641,  0.4999,  0.0591],\n",
            "         [ 0.8736,  0.1754,  0.4524],\n",
            "         [ 0.8257,  0.4087,  0.3888]]], device='cuda:0') tensor([ 1.0000, -0.3474,  0.3394, -0.8789, -0.8725, -0.5915], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.09088134765625\n",
            "tcost icost 0.038055419921875 0.0\n",
            "tcost icost 0.00971221923828125 0.0\n",
            "tcost icost -0.0892333984375 0.0\n",
            "tcost icost -0.04388427734375 0.0\n",
            "tcost icost -0.0791015625 0.0\n",
            "tcost icost -0.03558349609375 0.0\n",
            "loss tensor([[-0.1304]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.9695, -0.0533, -0.2394],\n",
            "         [ 0.9688,  0.1557,  0.1926],\n",
            "         [ 0.9368,  0.0375, -0.1736],\n",
            "         [ 0.8722,  0.4823,  0.0811],\n",
            "         [ 0.8823,  0.1717,  0.4382],\n",
            "         [ 0.8184,  0.4131,  0.3995]]], device='cuda:0') tensor([ 0.3901,  0.0096,  0.1643, -0.8454, -0.9189, -0.5803], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.13037109375\n",
            "search tensor([[[ 0.8823,  0.1717,  0.4382],\n",
            "         [ 0.8184,  0.4131,  0.3995],\n",
            "         [ 0.0558,  0.4815, -0.1197],\n",
            "         [-0.3041, -0.4110,  0.2543],\n",
            "         [-0.1803, -0.2735, -0.4310],\n",
            "         [ 0.3974, -0.3283, -0.1955]]], device='cuda:0') tensor([[[-0.9189],\n",
            "         [-0.5803],\n",
            "         [ 0.3380],\n",
            "         [ 1.0083],\n",
            "         [-0.6923],\n",
            "         [ 0.4499]]], device='cuda:0')\n",
            "tcost icost -0.00885772705078125 0.0\n",
            "tcost icost -0.1309814453125 0.0\n",
            "tcost icost -0.035064697265625 0.0\n",
            "tcost icost -0.41748046875 0.0\n",
            "tcost icost -0.07659912109375 0.0\n",
            "tcost icost -0.1962890625 0.0\n",
            "loss tensor([[-0.6255]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.9433,  0.0688,  0.3247],\n",
            "         [ 0.7081,  0.5060,  0.4925],\n",
            "         [-0.0443,  0.3810, -0.2196],\n",
            "         [-0.2038, -0.3106,  0.3540],\n",
            "         [-0.2799, -0.3733, -0.3305],\n",
            "         [ 0.4970, -0.2280, -0.0953]]], device='cuda:0') tensor([-0.7308, -0.7790,  0.4595,  0.9672, -0.6819,  0.3769], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.62548828125\n",
            "tcost icost -0.00274658203125 0.0\n",
            "tcost icost -0.08380126953125 0.0\n",
            "tcost icost -0.022430419921875 0.0\n",
            "tcost icost -0.385498046875 0.0\n",
            "tcost icost -0.07568359375 0.0\n",
            "tcost icost -0.160888671875 0.0\n",
            "loss tensor([[-0.5220]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[ 0.9767, -0.0282,  0.2127],\n",
            "         [ 0.5868,  0.5790,  0.5661],\n",
            "         [-0.1291,  0.2816, -0.3194],\n",
            "         [-0.1038, -0.2118,  0.4537],\n",
            "         [-0.2343, -0.4715, -0.2337],\n",
            "         [ 0.5953, -0.1279,  0.0046]]], device='cuda:0') tensor([-0.4537, -1.0000,  0.5990,  0.9135, -0.6723,  0.3111], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.52197265625\n",
            "tcost icost 0.006298065185546875 0.0\n",
            "tcost icost -0.00946807861328125 0.0\n",
            "tcost icost 0.007293701171875 0.0\n",
            "tcost icost -0.30712890625 0.0\n",
            "tcost icost -0.07177734375 0.0\n",
            "tcost icost -0.1021728515625 0.0\n",
            "loss tensor([[-0.3276]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[ 0.9862, -0.1210,  0.1126],\n",
            "         [ 0.4629,  0.6318,  0.6217],\n",
            "         [-0.2125,  0.1887, -0.4142],\n",
            "         [-0.0042, -0.1168,  0.5533],\n",
            "         [-0.1681, -0.5691, -0.1796],\n",
            "         [ 0.6915, -0.0285,  0.1035]]], device='cuda:0') tensor([ 0.0219, -1.0000,  0.8220,  0.8222, -0.6578,  0.2606], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.32763671875\n",
            "tcost icost 0.02703857421875 0.0\n",
            "tcost icost 0.0264129638671875 0.0\n",
            "tcost icost 0.18115234375 0.0\n",
            "tcost icost 0.0252685546875 0.0\n",
            "tcost icost -0.058349609375 0.0\n",
            "tcost icost -0.0205535888671875 0.0\n",
            "loss tensor([[0.1656]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[ 0.9417, -0.2051,  0.0304],\n",
            "         [ 0.3468,  0.6704,  0.6560],\n",
            "         [-0.2419,  0.1085, -0.4925],\n",
            "         [ 0.0906, -0.0314,  0.6524],\n",
            "         [-0.0905, -0.6673, -0.2007],\n",
            "         [ 0.7787,  0.0599,  0.1908]]], device='cuda:0') tensor([ 1.0000, -1.0000,  1.0000,  0.5986, -0.6281,  0.2580], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1656494140625\n",
            "tcost icost 0.0157623291015625 0.0\n",
            "tcost icost -0.0050506591796875 0.0\n",
            "tcost icost -0.260986328125 0.0\n",
            "tcost icost -0.01861572265625 0.0\n",
            "tcost icost -0.00547027587890625 0.0\n",
            "tcost icost 0.0146636962890625 0.0\n",
            "loss tensor([[-0.2086]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[ 0.9236, -0.2633, -0.0277],\n",
            "         [ 0.3249,  0.6789,  0.6584],\n",
            "         [-0.2390,  0.0518, -0.5524],\n",
            "         [ 0.1743,  0.0317,  0.7272],\n",
            "         [-0.0310, -0.6714, -0.1607],\n",
            "         [ 0.8532,  0.1240,  0.2544]]], device='cuda:0') tensor([ 0.6421, -0.7770,  0.8740,  0.6614, -0.6785,  0.2811], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2086181640625\n",
            "tcost icost 0.039337158203125 0.0\n",
            "tcost icost 0.0296783447265625 0.0\n",
            "tcost icost -0.07684326171875 0.0\n",
            "tcost icost 0.038116455078125 0.0\n",
            "tcost icost -0.0017766952514648438 0.0\n",
            "tcost icost -0.0268402099609375 0.0\n",
            "loss tensor([[0.0145]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[ 0.9355, -0.2982, -0.0646],\n",
            "         [ 0.3281,  0.6950,  0.6398],\n",
            "         [-0.1911,  0.0183, -0.6071],\n",
            "         [ 0.2415,  0.0894,  0.7956],\n",
            "         [-0.0370, -0.7145, -0.1947],\n",
            "         [ 0.9188,  0.1929,  0.3151]]], device='cuda:0') tensor([ 0.2239, -0.3834,  0.7037,  0.6478, -0.5594,  0.2343], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.0145416259765625\n",
            "tcost icost 0.035614013671875 0.0\n",
            "tcost icost 0.04010009765625 0.0\n",
            "tcost icost 0.034332275390625 0.0\n",
            "tcost icost -0.122314453125 0.0\n",
            "tcost icost -0.057769775390625 0.0\n",
            "tcost icost -0.051177978515625 0.0\n",
            "loss tensor([[-0.0578]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[ 0.9044, -0.3499, -0.1192],\n",
            "         [ 0.3225,  0.7035,  0.6332],\n",
            "         [-0.1604, -0.0308, -0.6678],\n",
            "         [ 0.3116,  0.1535,  0.8592],\n",
            "         [-0.0395, -0.7559, -0.2269],\n",
            "         [ 0.9059,  0.2420,  0.3476]]], device='cuda:0') tensor([ 1.0000, -0.9532,  1.0000,  0.5309, -0.5424,  0.2023], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.05780029296875\n",
            "tcost icost 0.0157623291015625 0.0\n",
            "tcost icost -0.003040313720703125 0.0\n",
            "tcost icost -0.260009765625 0.0\n",
            "tcost icost -0.018310546875 0.0\n",
            "tcost icost -0.007465362548828125 0.0\n",
            "tcost icost 0.0113525390625 0.0\n",
            "loss tensor([[-0.2090]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[ 0.8897, -0.3868, -0.1588],\n",
            "         [ 0.3469,  0.7052,  0.6184],\n",
            "         [-0.1263, -0.0662, -0.7177],\n",
            "         [ 0.3759,  0.2028,  0.9042],\n",
            "         [-0.0141, -0.7785, -0.2403],\n",
            "         [ 0.8905,  0.2736,  0.3635]]], device='cuda:0') tensor([ 0.6248, -0.7133,  0.8762,  0.5947, -0.5948,  0.2287], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.208984375\n",
            "tcost icost 0.039642333984375 0.0\n",
            "tcost icost 0.03759765625 0.0\n",
            "tcost icost -0.042755126953125 0.0\n",
            "tcost icost 0.05255126953125 0.0\n",
            "tcost icost -0.003650665283203125 0.0\n",
            "tcost icost -0.047119140625 0.0\n",
            "loss tensor([[0.0469]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[ 0.8988, -0.4012, -0.1765],\n",
            "         [ 0.3909,  0.7108,  0.5848],\n",
            "         [-0.0727, -0.0792, -0.7613],\n",
            "         [ 0.4048,  0.2284,  0.8854],\n",
            "         [-0.0174, -0.8071, -0.2764],\n",
            "         [ 0.8732,  0.3065,  0.3790]]], device='cuda:0') tensor([-0.0676, -0.1405,  0.6082,  0.6441, -0.4935,  0.1714], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.04693603515625\n",
            "tcost icost 0.02252197265625 0.0\n",
            "tcost icost -0.1427001953125 0.0\n",
            "tcost icost -0.0255889892578125 0.0\n",
            "tcost icost -0.23681640625 0.0\n",
            "tcost icost -0.06414794921875 0.0\n",
            "tcost icost -0.0953369140625 0.0\n",
            "loss tensor([[-0.3977]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[ 0.8834, -0.4226, -0.2026],\n",
            "         [ 0.4302,  0.7070,  0.5614],\n",
            "         [-0.0312, -0.0979, -0.8027],\n",
            "         [ 0.4323,  0.2554,  0.8648],\n",
            "         [-0.0181, -0.8376, -0.3062],\n",
            "         [ 0.8513,  0.3397,  0.3998]]], device='cuda:0') tensor([ 0.4836, -0.4252,  0.7803,  0.5731, -0.4846,  0.1072], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.397705078125\n",
            "tcost icost 0.040618896484375 0.0\n",
            "tcost icost 0.11871337890625 0.0\n",
            "tcost icost 0.2291259765625 0.0\n",
            "tcost icost 0.1729736328125 0.0\n",
            "tcost icost -0.03765869140625 0.0\n",
            "tcost icost -0.058624267578125 0.0\n",
            "loss tensor([[0.3997]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[ 0.8538, -0.4593, -0.2451],\n",
            "         [ 0.4594,  0.7005,  0.5461],\n",
            "         [ 0.0174, -0.1254, -0.8511],\n",
            "         [ 0.4642,  0.2824,  0.8395],\n",
            "         [-0.0185, -0.8664, -0.3343],\n",
            "         [ 0.8292,  0.3691,  0.4197]]], device='cuda:0') tensor([ 1.0000, -0.8832,  1.0000,  0.5255, -0.4647,  0.0821], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.399658203125\n",
            "tcost icost 0.0157623291015625 0.0\n",
            "tcost icost -0.0002263784408569336 0.0\n",
            "tcost icost -0.2479248046875 0.0\n",
            "tcost icost -0.017059326171875 0.0\n",
            "tcost icost -0.007061004638671875 0.0\n",
            "tcost icost 0.0041656494140625 0.0\n",
            "loss tensor([[-0.2000]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[ 0.8328, -0.4813, -0.2736],\n",
            "         [ 0.5087,  0.6805,  0.5273],\n",
            "         [ 0.0682, -0.1443, -0.8926],\n",
            "         [ 0.4969,  0.3010,  0.8139],\n",
            "         [ 0.0024, -0.8920, -0.3499],\n",
            "         [ 0.8157,  0.3883,  0.4288]]], device='cuda:0') tensor([ 0.5845, -0.6210,  0.8719,  0.5926, -0.5210,  0.1119], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.199951171875\n",
            "tcost icost 0.040130615234375 0.0\n",
            "tcost icost 0.058197021484375 0.0\n",
            "tcost icost 0.05780029296875 0.0\n",
            "tcost icost 0.0999755859375 0.0\n",
            "tcost icost -0.01056671142578125 0.0\n",
            "tcost icost -0.067138671875 0.0\n",
            "loss tensor([[0.1656]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[ 0.8380, -0.4732, -0.2718],\n",
            "         [ 0.5747,  0.6424,  0.4895],\n",
            "         [ 0.1303, -0.1394, -0.9258],\n",
            "         [ 0.5312,  0.3092,  0.7888],\n",
            "         [ 0.0142, -0.9119, -0.3717],\n",
            "         [ 0.7975,  0.4099,  0.4427]]], device='cuda:0') tensor([-0.6812,  0.3395,  0.4396,  0.7371, -0.4782,  0.0494], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.1656494140625\n",
            "tcost icost -0.002655029296875 0.0\n",
            "tcost icost -0.330078125 0.0\n",
            "tcost icost -0.0526123046875 0.0\n",
            "tcost icost -0.341796875 0.0\n",
            "tcost icost -0.07269287109375 0.0\n",
            "tcost icost -0.135986328125 0.0\n",
            "loss tensor([[-0.7197]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[ 0.8401, -0.4687, -0.2732],\n",
            "         [ 0.6348,  0.6081,  0.4581],\n",
            "         [ 0.1848, -0.1378, -0.9569],\n",
            "         [ 0.5605,  0.3190,  0.7642],\n",
            "         [ 0.0266, -0.9231, -0.3837],\n",
            "         [ 0.7747,  0.4321,  0.4617]]], device='cuda:0') tensor([-0.4965,  0.2001,  0.5335,  0.6945, -0.4649, -0.0257], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.7197265625\n",
            "tcost icost 0.0036182403564453125 0.0\n",
            "tcost icost -0.29541015625 0.0\n",
            "tcost icost -0.049072265625 0.0\n",
            "tcost icost -0.309326171875 0.0\n",
            "tcost icost -0.07135009765625 0.0\n",
            "tcost icost -0.11041259765625 0.0\n",
            "loss tensor([[-0.6396]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[ 0.8376, -0.4693, -0.2796],\n",
            "         [ 0.6888,  0.5810,  0.4333],\n",
            "         [ 0.2270, -0.1362, -0.9643],\n",
            "         [ 0.5858,  0.3305,  0.7400],\n",
            "         [ 0.0385, -0.9215, -0.3864],\n",
            "         [ 0.7489,  0.4536,  0.4831]]], device='cuda:0') tensor([-0.2145,  0.0454,  0.6371,  0.6393, -0.4496, -0.0863], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.6396484375\n",
            "tcost icost 0.0155487060546875 0.0\n",
            "tcost icost -0.22314453125 0.0\n",
            "tcost icost -0.03826904296875 0.0\n",
            "tcost icost -0.2548828125 0.0\n",
            "tcost icost -0.06683349609375 0.0\n",
            "tcost icost -0.08294677734375 0.0\n",
            "loss tensor([[-0.4951]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[ 0.8282, -0.4778, -0.2929],\n",
            "         [ 0.7231,  0.5563,  0.4094],\n",
            "         [ 0.2572, -0.1360, -0.9567],\n",
            "         [ 0.6070,  0.3435,  0.7167],\n",
            "         [ 0.0500, -0.9210, -0.3863],\n",
            "         [ 0.7213,  0.4733,  0.5057]]], device='cuda:0') tensor([ 0.2140, -0.1583,  0.7647,  0.5750, -0.4349, -0.1373], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.4951171875\n",
            "tcost icost 0.03533935546875 0.0\n",
            "tcost icost -0.0261688232421875 0.0\n",
            "tcost icost 0.0022754669189453125 0.0\n",
            "tcost icost -0.1309814453125 0.0\n",
            "tcost icost -0.055267333984375 0.0\n",
            "tcost icost -0.045440673828125 0.0\n",
            "loss tensor([[-0.1449]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[ 0.8047, -0.5001, -0.3199],\n",
            "         [ 0.7365,  0.5504,  0.3933],\n",
            "         [ 0.2744, -0.1415, -0.9512],\n",
            "         [ 0.6235,  0.3598,  0.6941],\n",
            "         [ 0.0589, -0.9205, -0.3862],\n",
            "         [ 0.6950,  0.4897,  0.5265]]], device='cuda:0') tensor([ 1.0000, -0.5702,  0.9694,  0.4646, -0.4189, -0.1607], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1448974609375\n",
            "tcost icost 0.0157623291015625 0.0\n",
            "tcost icost 0.01296234130859375 0.0\n",
            "tcost icost -0.170166015625 0.0\n",
            "tcost icost -0.007488250732421875 0.0\n",
            "tcost icost 0.005886077880859375 0.0\n",
            "tcost icost 0.00839996337890625 0.0\n",
            "loss tensor([[-0.1070]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[ 0.7923, -0.5100, -0.3347],\n",
            "         [ 0.7691,  0.5208,  0.3706],\n",
            "         [ 0.2999, -0.1400, -0.9436],\n",
            "         [ 0.6441,  0.3707,  0.6691],\n",
            "         [ 0.0797, -0.9209, -0.3816],\n",
            "         [ 0.6754,  0.5016,  0.5406]]], device='cuda:0') tensor([ 0.5056, -0.2279,  0.7822,  0.5215, -0.4527, -0.1487], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.10699462890625\n",
            "tcost icost 0.04046630859375 0.0\n",
            "tcost icost 0.1292724609375 0.0\n",
            "tcost icost 0.1580810546875 0.0\n",
            "tcost icost 0.11273193359375 0.0\n",
            "tcost icost -0.039581298828125 0.0\n",
            "tcost icost -0.04620361328125 0.0\n",
            "loss tensor([[0.3135]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[ 0.7549, -0.5404, -0.3716],\n",
            "         [ 0.7634,  0.5276,  0.3726],\n",
            "         [ 0.2870, -0.1558, -0.9452],\n",
            "         [ 0.6587,  0.3866,  0.6455],\n",
            "         [ 0.0980, -0.9208, -0.3776],\n",
            "         [ 0.6564,  0.5117,  0.5544]]], device='cuda:0') tensor([ 1.0000, -1.0000,  1.0000,  0.3750, -0.4327, -0.1732], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.3134765625\n",
            "tcost icost 0.0157623291015625 0.0\n",
            "tcost icost -0.007568359375 0.0\n",
            "tcost icost -0.27978515625 0.0\n",
            "tcost icost -0.0244293212890625 0.0\n",
            "tcost icost -0.0293121337890625 0.0\n",
            "tcost icost -0.0115509033203125 0.0\n",
            "loss tensor([[-0.2617]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[ 0.7242, -0.5618, -0.3999],\n",
            "         [ 0.7692,  0.5222,  0.3684],\n",
            "         [ 0.2803, -0.1670, -0.9453],\n",
            "         [ 0.6785,  0.3968,  0.6182],\n",
            "         [ 0.1333, -0.9203, -0.3677],\n",
            "         [ 0.6430,  0.5188,  0.5634]]], device='cuda:0') tensor([ 0.6636, -0.7888,  0.8925,  0.4414, -0.4992, -0.1638], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.26171875\n",
            "search tensor([[[ 0.1333, -0.9203, -0.3677],\n",
            "         [ 0.6430,  0.5188,  0.5634],\n",
            "         [-0.1512,  0.0838,  0.3861],\n",
            "         [ 0.0068,  0.5044, -0.3753],\n",
            "         [ 0.3475, -0.4246,  0.0397],\n",
            "         [ 0.1846,  0.3942,  0.3179]]], device='cuda:0') tensor([[[-0.4992],\n",
            "         [-0.1638],\n",
            "         [ 0.1965],\n",
            "         [-0.3807],\n",
            "         [ 0.4457],\n",
            "         [ 0.1230]]], device='cuda:0')\n",
            "tcost icost -0.01172637939453125 0.0\n",
            "tcost icost -0.27783203125 0.0\n",
            "tcost icost -0.0513916015625 0.0\n",
            "tcost icost -0.1998291015625 0.0\n",
            "tcost icost -0.0357666015625 0.0\n",
            "tcost icost -0.1849365234375 0.0\n",
            "loss tensor([[-0.5815]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "0 tensor([[[ 0.0296, -0.9086, -0.4166],\n",
            "         [ 0.5134,  0.5854,  0.6275],\n",
            "         [-0.2511, -0.0163,  0.2857],\n",
            "         [ 0.1068,  0.6039, -0.2749],\n",
            "         [ 0.2471, -0.5242, -0.0603],\n",
            "         [ 0.2844,  0.4938,  0.4176]]], device='cuda:0') tensor([-0.3312, -0.3079,  0.3014, -0.4863,  0.4967,  0.0292], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.58154296875\n",
            "tcost icost -0.005340576171875 0.0\n",
            "tcost icost -0.2452392578125 0.0\n",
            "tcost icost -0.04913330078125 0.0\n",
            "tcost icost -0.163330078125 0.0\n",
            "tcost icost -0.0305938720703125 0.0\n",
            "tcost icost -0.1602783203125 0.0\n",
            "loss tensor([[-0.4998]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "1 tensor([[[-0.0666, -0.8527, -0.5138],\n",
            "         [ 0.3839,  0.6294,  0.6757],\n",
            "         [-0.3468, -0.1163,  0.1854],\n",
            "         [ 0.2067,  0.7031, -0.1747],\n",
            "         [ 0.1504, -0.6233, -0.1454],\n",
            "         [ 0.3828,  0.5863,  0.5159]]], device='cuda:0') tensor([-0.0763, -0.4691,  0.4256, -0.6087,  0.5657, -0.0573], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.499755859375\n",
            "tcost icost 0.005138397216796875 0.0\n",
            "tcost icost -0.180419921875 0.0\n",
            "tcost icost -0.035308837890625 0.0\n",
            "tcost icost -0.09375 0.0\n",
            "tcost icost -0.021453857421875 0.0\n",
            "tcost icost -0.1317138671875 0.0\n",
            "loss tensor([[-0.3462]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "2 tensor([[[-0.1590, -0.7773, -0.6088],\n",
            "         [ 0.2584,  0.6589,  0.7065],\n",
            "         [-0.4429, -0.2162,  0.0864],\n",
            "         [ 0.3064,  0.8006, -0.0745],\n",
            "         [ 0.0541, -0.7225, -0.2339],\n",
            "         [ 0.4664,  0.6507,  0.5992]]], device='cuda:0') tensor([ 0.3111, -0.6992,  0.5734, -0.7426,  0.6363, -0.1441], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.34619140625\n",
            "tcost icost 0.0205230712890625 0.0\n",
            "tcost icost -0.0156707763671875 0.0\n",
            "tcost icost 0.00447845458984375 0.0\n",
            "tcost icost 0.0107421875 0.0\n",
            "tcost icost 0.007099151611328125 0.0\n",
            "tcost icost -0.0712890625 0.0\n",
            "loss tensor([[-0.0196]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "3 tensor([[[-0.2464, -0.6812, -0.6894],\n",
            "         [ 0.1475,  0.6731,  0.7247],\n",
            "         [-0.5384, -0.3154, -0.0034],\n",
            "         [ 0.4029,  0.8911,  0.0227],\n",
            "         [-0.0320, -0.8216, -0.3148],\n",
            "         [ 0.4825,  0.6328,  0.6056]]], device='cuda:0') tensor([ 0.9893, -1.0000,  0.7822, -0.7989,  0.7449, -0.2345], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.019561767578125\n",
            "tcost icost 0.0254974365234375 0.0\n",
            "tcost icost 0.0215911865234375 0.0\n",
            "tcost icost -0.0986328125 0.0\n",
            "tcost icost -0.033599853515625 0.0\n",
            "tcost icost -0.19873046875 0.0\n",
            "tcost icost -0.04058837890625 0.0\n",
            "loss tensor([[-0.2137]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "4 tensor([[[-0.3117, -0.5769, -0.7550],\n",
            "         [ 0.1447,  0.6936,  0.7056],\n",
            "         [-0.5451, -0.3612, -0.0468],\n",
            "         [ 0.4547,  0.8855,  0.0956],\n",
            "         [-0.0424, -0.9038, -0.3502],\n",
            "         [ 0.4972,  0.6181,  0.6089]]], device='cuda:0') tensor([ 0.3907, -0.5015,  0.6515, -0.7627,  0.6657, -0.2222], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2137451171875\n",
            "tcost icost 0.0233306884765625 0.0\n",
            "tcost icost -0.031951904296875 0.0\n",
            "tcost icost 8.07642936706543e-05 0.0\n",
            "tcost icost 0.01335906982421875 0.0\n",
            "tcost icost 0.0091705322265625 0.0\n",
            "tcost icost -0.049713134765625 0.0\n",
            "loss tensor([[-0.0190]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "5 tensor([[[-0.3334, -0.5462, -0.7685],\n",
            "         [ 0.1261,  0.7039,  0.6991],\n",
            "         [-0.5454, -0.4161, -0.1026],\n",
            "         [ 0.4857,  0.8590,  0.1620],\n",
            "         [-0.0707, -0.9197, -0.3862],\n",
            "         [ 0.5033,  0.6073,  0.6147]]], device='cuda:0') tensor([ 1.0000, -0.9070,  0.8341, -0.8551,  0.7963, -0.3190], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.01898193359375\n",
            "tcost icost 0.025238037109375 0.0\n",
            "tcost icost 0.0243682861328125 0.0\n",
            "tcost icost -0.07684326171875 0.0\n",
            "tcost icost -0.03314208984375 0.0\n",
            "tcost icost -0.18896484375 0.0\n",
            "tcost icost -0.040618896484375 0.0\n",
            "loss tensor([[-0.1873]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "6 tensor([[[-0.3635, -0.4888, -0.7930],\n",
            "         [ 0.1447,  0.7344,  0.6631],\n",
            "         [-0.5231, -0.4421, -0.1295],\n",
            "         [ 0.5163,  0.8294,  0.2133],\n",
            "         [-0.0664, -0.9181, -0.3906],\n",
            "         [ 0.5108,  0.5975,  0.6181]]], device='cuda:0') tensor([ 0.2837, -0.3015,  0.6866, -0.8179,  0.7162, -0.3072], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.187255859375\n",
            "tcost icost 0.0196685791015625 0.0\n",
            "tcost icost -0.11529541015625 0.0\n",
            "tcost icost -0.019561767578125 0.0\n",
            "tcost icost 0.0012798309326171875 0.0\n",
            "tcost icost 0.001148223876953125 0.0\n",
            "tcost icost -0.0504150390625 0.0\n",
            "loss tensor([[-0.1281]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "7 tensor([[[-0.3665, -0.4616, -0.8078],\n",
            "         [ 0.1562,  0.7488,  0.6441],\n",
            "         [-0.4942, -0.4750, -0.1651],\n",
            "         [ 0.5391,  0.8000,  0.2634],\n",
            "         [-0.0772, -0.9078, -0.4121],\n",
            "         [ 0.5097,  0.5923,  0.6240]]], device='cuda:0') tensor([ 0.8784, -0.5754,  0.8442, -0.9092,  0.8367, -0.4020], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.1280517578125\n",
            "tcost icost 0.027435302734375 0.0\n",
            "tcost icost 0.1163330078125 0.0\n",
            "tcost icost 0.246337890625 0.0\n",
            "tcost icost -0.00229644775390625 0.0\n",
            "tcost icost -0.096923828125 0.0\n",
            "tcost icost -0.017669677734375 0.0\n",
            "loss tensor([[0.2556]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "8 tensor([[[-0.3739, -0.4188, -0.8275],\n",
            "         [ 0.1932,  0.7768,  0.5994],\n",
            "         [-0.4544, -0.5006, -0.1978],\n",
            "         [ 0.5708,  0.7674,  0.2921],\n",
            "         [-0.0642, -0.9076, -0.4150],\n",
            "         [ 0.5138,  0.5863,  0.6264]]], device='cuda:0') tensor([ 0.2856, -0.2427,  0.8471, -0.8055,  0.7267, -0.3886], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.255615234375\n",
            "tcost icost 0.019744873046875 0.0\n",
            "tcost icost -0.12939453125 0.0\n",
            "tcost icost -0.0200347900390625 0.0\n",
            "tcost icost 0.00624847412109375 0.0\n",
            "tcost icost 0.002994537353515625 0.0\n",
            "tcost icost -0.02490234375 0.0\n",
            "loss tensor([[-0.1211]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "9 tensor([[[-0.3630, -0.3981, -0.8425],\n",
            "         [ 0.2217,  0.7914,  0.5697],\n",
            "         [-0.4088, -0.5307, -0.2371],\n",
            "         [ 0.5920,  0.7382,  0.3235],\n",
            "         [-0.0639, -0.8991, -0.4330],\n",
            "         [ 0.5125,  0.5835,  0.6300]]], device='cuda:0') tensor([ 0.8301, -0.5012,  0.9728, -0.8823,  0.8341, -0.4617], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.12109375\n",
            "tcost icost 0.0282745361328125 0.0\n",
            "tcost icost 0.1263427734375 0.0\n",
            "tcost icost 0.1890869140625 0.0\n",
            "tcost icost 0.001972198486328125 0.0\n",
            "tcost icost -0.040496826171875 0.0\n",
            "tcost icost -0.0048675537109375 0.0\n",
            "loss tensor([[0.2671]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "10 tensor([[[-0.3541, -0.3762, -0.8562],\n",
            "         [ 0.2635,  0.8072,  0.5283],\n",
            "         [-0.3567, -0.5562, -0.2768],\n",
            "         [ 0.6238,  0.7053,  0.3368],\n",
            "         [-0.0470, -0.8978, -0.4378],\n",
            "         [ 0.5177,  0.5790,  0.6299]]], device='cuda:0') tensor([ 0.6466, -0.4213,  0.9792, -0.7628,  0.7426, -0.4447], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.26708984375\n",
            "tcost icost 0.0279541015625 0.0\n",
            "tcost icost 0.07403564453125 0.0\n",
            "tcost icost 0.065185546875 0.0\n",
            "tcost icost 0.0136871337890625 0.0\n",
            "tcost icost 0.059478759765625 0.0\n",
            "tcost icost 0.10284423828125 0.0\n",
            "loss tensor([[0.2573]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "11 tensor([[[-0.3310, -0.3766, -0.8652],\n",
            "         [ 0.2891,  0.8191,  0.4955],\n",
            "         [-0.2931, -0.5853, -0.3321],\n",
            "         [ 0.6574,  0.6743,  0.3365],\n",
            "         [-0.0353, -0.8880, -0.4585],\n",
            "         [ 0.5459,  0.5654,  0.6184]]], device='cuda:0') tensor([ 1.0000, -0.9039,  0.9816, -0.7584,  0.8651, -0.3402], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.25732421875\n",
            "tcost icost 0.025238037109375 0.0\n",
            "tcost icost 0.024444580078125 0.0\n",
            "tcost icost -0.106201171875 0.0\n",
            "tcost icost -0.0333251953125 0.0\n",
            "tcost icost -0.176025390625 0.0\n",
            "tcost icost -0.040130615234375 0.0\n",
            "loss tensor([[-0.2021]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "12 tensor([[[-0.3159, -0.3606, -0.8776],\n",
            "         [ 0.3276,  0.8291,  0.4530],\n",
            "         [-0.2390, -0.6002, -0.3572],\n",
            "         [ 0.6912,  0.6405,  0.3347],\n",
            "         [-0.0079, -0.8846, -0.4663],\n",
            "         [ 0.5725,  0.5514,  0.6067]]], device='cuda:0') tensor([ 0.3779, -0.3961,  0.8618, -0.7201,  0.7838, -0.3293], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.2021484375\n",
            "tcost icost 0.022857666015625 0.0\n",
            "tcost icost -0.0631103515625 0.0\n",
            "tcost icost -0.0037021636962890625 0.0\n",
            "tcost icost 0.01561737060546875 0.0\n",
            "tcost icost 0.01140594482421875 0.0\n",
            "tcost icost -0.004375457763671875 0.0\n",
            "loss tensor([[-0.0206]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "13 tensor([[[-0.2956, -0.3581, -0.8857],\n",
            "         [ 0.3501,  0.8346,  0.4253],\n",
            "         [-0.1799, -0.6206, -0.3934],\n",
            "         [ 0.7108,  0.6145,  0.3422],\n",
            "         [ 0.0016, -0.8728, -0.4880],\n",
            "         [ 0.5877,  0.5446,  0.5983]]], device='cuda:0') tensor([ 0.9907, -0.7299,  1.0000, -0.8099,  0.9105, -0.4009], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.0206451416015625\n",
            "tcost icost 0.0254669189453125 0.0\n",
            "tcost icost 0.04693603515625 0.0\n",
            "tcost icost -0.0127410888671875 0.0\n",
            "tcost icost -0.02581787109375 0.0\n",
            "tcost icost -0.15087890625 0.0\n",
            "tcost icost -0.035491943359375 0.0\n",
            "loss tensor([[-0.0814]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "14 tensor([[[-0.2717, -0.3349, -0.9022],\n",
            "         [ 0.3998,  0.8295,  0.3901],\n",
            "         [-0.1118, -0.6338, -0.3950],\n",
            "         [ 0.7348,  0.5832,  0.3463],\n",
            "         [ 0.0264, -0.8663, -0.4988],\n",
            "         [ 0.6041,  0.5364,  0.5893]]], device='cuda:0') tensor([-0.0386,  0.0948,  0.8042, -0.7575,  0.8300, -0.3902], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.08135986328125\n",
            "tcost icost 0.00685882568359375 0.0\n",
            "tcost icost -0.28515625 0.0\n",
            "tcost icost -0.043426513671875 0.0\n",
            "tcost icost -0.0301513671875 0.0\n",
            "tcost icost -0.0052490234375 0.0\n",
            "tcost icost -0.0310821533203125 0.0\n",
            "loss tensor([[-0.3286]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "15 tensor([[[-0.2437, -0.3187, -0.9160],\n",
            "         [ 0.4399,  0.8224,  0.3608],\n",
            "         [-0.0630, -0.6485, -0.4015],\n",
            "         [ 0.7459,  0.5608,  0.3593],\n",
            "         [ 0.0385, -0.8555, -0.5163],\n",
            "         [ 0.6127,  0.5332,  0.5834]]], device='cuda:0') tensor([ 0.3381, -0.0323,  0.9167, -0.8756,  0.9290, -0.4462], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.32861328125\n",
            "tcost icost 0.021453857421875 0.0\n",
            "tcost icost -0.154296875 0.0\n",
            "tcost icost -0.02508544921875 0.0\n",
            "tcost icost 0.01311492919921875 0.0\n",
            "tcost icost 0.016998291015625 0.0\n",
            "tcost icost 0.020660400390625 0.0\n",
            "loss tensor([[-0.1048]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "16 tensor([[[-0.2162, -0.3112, -0.9254],\n",
            "         [ 0.4655,  0.8181,  0.3377],\n",
            "         [-0.0317, -0.6639, -0.4130],\n",
            "         [ 0.7465,  0.5477,  0.3778],\n",
            "         [ 0.0371, -0.8395, -0.5421],\n",
            "         [ 0.6178,  0.5329,  0.5783]]], device='cuda:0') tensor([ 0.8542, -0.2841,  1.0000, -0.9577,  1.0000, -0.4732], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) -0.10479736328125\n",
            "tcost icost 0.0278472900390625 0.0\n",
            "tcost icost 0.1318359375 0.0\n",
            "tcost icost 0.11407470703125 0.0\n",
            "tcost icost 0.000881195068359375 0.0\n",
            "tcost icost -0.029754638671875 0.0\n",
            "tcost icost 0.002685546875 0.0\n",
            "loss tensor([[0.2216]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "17 tensor([[[-0.2006, -0.3066, -0.9305],\n",
            "         [ 0.5042,  0.8027,  0.3185],\n",
            "         [-0.0230, -0.6800, -0.4254],\n",
            "         [ 0.7648,  0.5199,  0.3805],\n",
            "         [ 0.0526, -0.8257, -0.5617],\n",
            "         [ 0.6293,  0.5285,  0.5699]]], device='cuda:0') tensor([ 0.9273, -0.4052,  1.0000, -0.8210,  0.8992, -0.4573], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2215576171875\n",
            "tcost icost 0.026611328125 0.0\n",
            "tcost icost 0.1307373046875 0.0\n",
            "tcost icost 0.2034912109375 0.0\n",
            "tcost icost -0.002643585205078125 0.0\n",
            "tcost icost -0.0697021484375 0.0\n",
            "tcost icost -0.014404296875 0.0\n",
            "loss tensor([[0.2529]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "18 tensor([[[-0.1816, -0.2999, -0.9365],\n",
            "         [ 0.5607,  0.7727,  0.2978],\n",
            "         [-0.0181, -0.6939, -0.4412],\n",
            "         [ 0.7919,  0.4821,  0.3749],\n",
            "         [ 0.0824, -0.8127, -0.5768],\n",
            "         [ 0.6450,  0.5213,  0.5588]]], device='cuda:0') tensor([ 0.8002, -0.3344,  1.0000, -0.7271,  0.8054, -0.4430], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2529296875\n",
            "tcost icost 0.02862548828125 0.0\n",
            "tcost icost 0.12322998046875 0.0\n",
            "tcost icost 0.107421875 0.0\n",
            "tcost icost 0.008056640625 0.0\n",
            "tcost icost 0.0294036865234375 0.0\n",
            "tcost icost 0.025390625 0.0\n",
            "loss tensor([[0.2666]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<AddBackward0>)\n",
            "19 tensor([[[-0.1757, -0.2985, -0.9381],\n",
            "         [ 0.6174,  0.7364,  0.2766],\n",
            "         [-0.0129, -0.7042, -0.4584],\n",
            "         [ 0.8285,  0.4338,  0.3542],\n",
            "         [ 0.1158, -0.7940, -0.5919],\n",
            "         [ 0.6697,  0.5077,  0.5421]]], device='cuda:0') tensor([ 1.0000, -0.5375,  0.9662, -0.5854,  0.7926, -0.4115], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>) 0.2666015625\n",
            "ded\n",
            "time\n",
            "[0, 12, 0, 14, 10, 14, 10, 14, 0, 14, 11, 0, 14, 12, 14, 14, 11, 14, 14, 0, 14, 14, 0, 14, 14, 12, 10, 12, 13, 14, 11, 14, 14, 14, 6, 11, 14, 14, 11, 14, 0, 14, 11, 6, 0, 14, 11, 12, 6, 14, 12, 14, 0, 14, 10, 12, 10, 14, 14, 14, 0, 14, 14, 14, 0, 14, 11, 12, 10, 12, 10, 14]\n",
            "7 #### train ####\n",
            "repr, std, cov, closslb 0.8643196821212769 0.4736328125 0.0003361243288964033 0.06069700047373772 0.001585931982845068\n",
            "0.1340691632840448 0.04344194658833656 1.0\n",
            "repr, std, cov, closslb 0.8702312111854553 0.474365234375 0.00026186276227235794 0.015402351506054401 0.01611626334488392\n",
            "0.13460624488857134 0.04288113481253842 1.0\n",
            "repr, std, cov, closslb 0.8754358291625977 0.474609375 0.00025986297987401485 0.015344294719398022 0.0300038680434227\n",
            "0.13595838079952752 0.042624746111542974 1.0\n",
            "repr, std, cov, closslb 0.8749006986618042 0.474365234375 0.00026678433641791344 0.060171522200107574 0.015912402421236038\n",
            "0.13732409907824986 0.0423698903729163 1.0\n",
            "repr, std, cov, closslb 0.8715431690216064 0.47314453125 0.0003183681983500719 0.015513461083173752 0.015651319175958633\n",
            "0.13856497119006525 0.04215867498942174 1.0\n",
            "repr, std, cov, closslb 0.865685224533081 0.473876953125 0.00030738417990505695 0.0005522367428056896 0.029148492962121964\n",
            "0.13898108193711395 0.04165604397780317 1.0\n",
            "repr, std, cov, closslb 0.8678172826766968 0.4736328125 0.0003170599229633808 0.04499904438853264 0.04805949330329895\n",
            "0.1402369266854376 0.04140698016335963 1.0\n",
            "repr, std, cov, closslb 0.8643189668655396 0.474609375 0.0002335661556571722 0.015581452287733555 0.014500658959150314\n",
            "0.14065805831651085 0.04091331113005149 1.0\n",
            "repr, std, cov, closslb 0.875812828540802 0.47265625 0.0003439153078943491 0.07497768849134445 0.0003895875415764749\n",
            "0.14122153506089966 0.040465953325780256 1.0\n",
            "repr, std, cov, closslb 0.842742919921875 0.475341796875 0.00019883527420461178 0.015495200641453266 0.030724182724952698\n",
            "0.14221305640626156 0.04014367763090117 1.0\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(10):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n",
        "\n",
        "    # state = buffer[7][80][0]\n",
        "    # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    # sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "    # out= agent.deconv(sx_).squeeze(0)\n",
        "    # # print(out.shape)\n",
        "    # imshow(state.detach().cpu())\n",
        "    # imshow(out.detach().cpu())\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "    c_loader = make_weighted(buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    # train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    # agent.train_jepa(train_loader, optim)\n",
        "    agent.train_jepa(train_loader, c_loader, optim)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# print(optim.param_groups[0][\"lr\"])\n",
        "optim.param_groups[0][\"lr\"] = 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "b8zxYU9jpE8K",
        "outputId": "52a584bc-735a-44a3-8778-f2262034f77e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "256c75e8-e0c1-446e-a588-6de3dee18e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  self.hub = sentry_sdk.Hub(client)\n",
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240902_034119-3gthhome</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/3gthhome' target=\"_blank\">smooth-elevator-27</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/3gthhome' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/3gthhome</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wUhKd009Qvk3"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}