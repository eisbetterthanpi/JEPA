{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "040ffac5-871d-46fe-de42-178150b5c74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "6482e941-aa5a-4c53-9e3c-3ff390ab27c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model=256, drop=0.5):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Dropout(p=drop),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Bos81kQf1dwh"
      },
      "outputs": [],
      "source": [
        "# @title transfer_sd store_sd load_sd\n",
        "\n",
        "def transfer_sd(tgt_sd, src_sd): #\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            # print(wht_name, tgt_wht.shape, src_wht.shape)\n",
        "            if tgt_wht.shape==src_wht.shape:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "                continue\n",
        "            if tgt_wht.shape[0] != src_wht.shape[0]: continue # output dim diff\n",
        "            if len(tgt_wht.shape)==2: tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "    return tgt_sd\n",
        "\n",
        "def store_sd(all_sd, new_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in new_sd.keys():\n",
        "            if not wht_name in all_sd.keys():\n",
        "                # print(wht_name, new_sd[wht_name].shape)\n",
        "                all_sd[wht_name] = (new_sd[wht_name],)\n",
        "                continue\n",
        "            all_tpl, new_wht = all_sd[wht_name], new_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                print(wht_name, all_wht.shape, new_wht.shape)\n",
        "                if all_wht.shape==new_wht.shape:\n",
        "                    all_wht = new_wht\n",
        "                    break\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: continue # diff output shape\n",
        "                if len(all_wht.shape)==2: all_wht[:, :new_wht.shape[1]] = new_wht[:, :all_wht.shape[1]]\n",
        "                break\n",
        "            if len(all_wht.shape)>=2 and len(all_wht.shape)>=2:\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: all_tpl = all_tpl + (new_wht,) # wht not in all_wht\n",
        "    return all_sd\n",
        "\n",
        "def load_sd(tgt_sd, all_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in all_sd.keys(): continue\n",
        "            tgt_wht, all_tpl = tgt_sd[wht_name], all_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                # try: print(wht_name, tgt_wht.shape, all_wht.shape)\n",
        "                # except: print(wht_name, tgt_wht, all_wht)\n",
        "                if tgt_wht.shape==all_wht.shape:\n",
        "                    tgt_wht.copy_(all_wht)\n",
        "                    break\n",
        "                if tgt_wht.shape[0] != all_wht.shape[0]: continue # output dim diff\n",
        "                if len(tgt_wht.shape)==2: tgt_wht[:, :all_wht.shape[1]].copy_(all_wht[:, :tgt_wht.shape[1]])\n",
        "                break\n",
        "    return tgt_sd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# modelsd = torch.load('agent.pkl', map_location=device).values()\n",
        "# tgt_sd = transfer_sd(agent.state_dict(), modelsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = {}\n",
        "# all_sd = store_sd(all_sd, agent1.state_dict())\n",
        "# print(all_sd.keys())\n",
        "# checkpoint = {'model': all_sd}\n",
        "# torch.save(checkpoint, 'all_sd.pkl')\n",
        "\n",
        "# agent3 = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "# agent3.tcost = tcost3\n",
        "# tgt_sd = load_sd(agent3.state_dict(), all_sd)\n",
        "# agent3.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "# for x,y in zip(agent1.state_dict().values(), agent3.state_dict().values()):\n",
        "#     print((x==y).all())\n",
        "\n",
        "# print(agent1.jepa.enc.cnn[1].num_batches_tracked)\n",
        "# jepa.enc.cnn.0.weight\n",
        "# print(agent1.jepa.enc.cnn[0].weight.shape)\n",
        "# print(agent1.jepa.enc.cnn[0].weight[0][0])\n",
        "# print(agent3.jepa.enc.cnn[0].weight[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "SFVbGqMDqcDR"
      },
      "outputs": [],
      "source": [
        "# @title rename_sd\n",
        "def rename_sd(agent_sd):\n",
        "    sd_={}\n",
        "    convert={}\n",
        "    na_=''\n",
        "    for wht_name, wht in agent_sd.items():\n",
        "        o=wht_name.split('.')\n",
        "        # print(\"####\", wht_name)\n",
        "        name=wht_name\n",
        "        for i in range(len(o)):\n",
        "            c = o[i]\n",
        "            if c.isnumeric():\n",
        "                na, me = '.'.join(o[:i]), '.'.join(o[i+1:])\n",
        "                c=int(c)\n",
        "                if na!=na_: # param name diff\n",
        "                    j=0 # reset num\n",
        "                    c_=c # track wht_name num\n",
        "                    na_=na # track param name\n",
        "                elif c_<c: # same param name, diff num\n",
        "                    j+=1\n",
        "                    c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "        # print(name)\n",
        "        sd_[name] = wht\n",
        "        convert[name] = wht_name\n",
        "    return sd_, convert\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# # modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, _ = rename_sd(modelsd)\n",
        "\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "riBHnAAkkzrd"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim me\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# def transfer_optim(tgt_sd, src_sd, tgt_optim, src_optim): #\n",
        "def transfer_optim(tgt_sd, src_sd, tgt_optim_sd, src_optim_sd): #\n",
        "    non_lst = ['running_mean', 'running_var', 'num_batches_tracked', 'num_batches_tracked', 'loss_fn']\n",
        "    tgt_lst, src_lst = [], []\n",
        "    for i, (k,v) in enumerate(tgt_sd.items()):\n",
        "        # print(i, k, v.shape, any(s in k for s in non_lst))\n",
        "        if not any(s in k for s in non_lst): tgt_lst.append(k)\n",
        "    for i, (k,v) in enumerate(src_sd.items()):\n",
        "        if not any(s in k for s in non_lst): src_lst.append(k)\n",
        "\n",
        "    # tgt_optim_st, src_optim_st = tgt_optim.state_dict()['state'], src_optim.state_dict()['state']\n",
        "    tgt_optim_st, src_optim_st = tgt_optim_sd['state'], src_optim_sd['state']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, wht_name in enumerate(tgt_lst):\n",
        "            if not wht_name in src_lst: continue\n",
        "            tgt_wht, src_wht = tgt_optim_st[tgt_lst.index(wht_name)], src_optim_st[src_lst.index(wht_name)]\n",
        "            # print(wht_name, tgt_wht, src_wht)\n",
        "            tgt_shp, src_shp = tgt_wht['exp_avg'].shape, src_wht['exp_avg'].shape\n",
        "            if tgt_shp==src_shp:\n",
        "                tgt_wht = src_wht\n",
        "                continue\n",
        "            if tgt_shp[0] != src_shp[0]: continue # output dim diff\n",
        "            if len(tgt_shp)==2:\n",
        "                tgt_wht['step'] = src_wht['step']\n",
        "                tgt_wht['exp_avg'][:, :src_shp[1]] = src_wht['exp_avg'][:, :tgt_shp[1]]\n",
        "                tgt_wht['exp_avg_sq'][:, :src_shp[1]] = src_wht['exp_avg_sq'][:, :tgt_shp[1]]\n",
        "    # return tgt_optim.state_dict()\n",
        "    return tgt_optim_sd\n",
        "\n",
        "# model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "# model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "# source_optimizer = optim.AdamW(model_src.parameters())\n",
        "# target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "# dummy_input = torch.randn(3, 10)\n",
        "# dummy_target = torch.randn(3, 5)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "# output = model_src(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# source_optimizer.step()\n",
        "\n",
        "# dummy_input = torch.randn(3, 20)\n",
        "# output = model_tgt(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# target_optimizer.step()\n",
        "\n",
        "\n",
        "# print(source_optimizer.state_dict())\n",
        "# print(target_optimizer.state_dict())\n",
        "\n",
        "# optimsd = transfer_optim(model_tgt.state_dict(), model_src.state_dict(), target_optimizer, source_optimizer)\n",
        "# target_optimizer.load_state_dict(optimsd)\n",
        "# print(target_optimizer.state_dict())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "AfjFbveH64Io"
      },
      "outputs": [],
      "source": [
        "# @title TCost\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TCost(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=256): # in_dim=(1+self.jepa.pred.num_layers)*d_model\n",
        "        super().__init__()\n",
        "        self.tc = torch.tensor([-1., 0.], device=device).unsqueeze(-1) # unsqueeze(0).T\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Dropout(p=0.),\n",
        "            nn.Linear(in_dim, 2, bias=False), #nn.Softmax(dim=-1),\n",
        "            # nn.Linear(in_dim, d_model), nn.LeakyReLU(),\n",
        "            # # nn.Dropout(p=0.5),\n",
        "            # nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "            # nn.Linear(d_model, 2), nn.Softmax(),\n",
        "            )\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def update_loss_weight(self, train_data):\n",
        "        a = len(buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "        # self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device), reduction='none')\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return self.tcost(x)@self.tc\n",
        "        return F.softmax(self.tcost(x), dim=-1)@self.tc\n",
        "\n",
        "    def loss(self, x, y, reduction='mean'):\n",
        "        out = self.tcost(x)\n",
        "        y = torch.where(y < -0.5, 0, 1)\n",
        "        if reduction=='mean': return self.loss_fn(out, y).mean()\n",
        "        return self.loss_fn(out, y)\n",
        "\n",
        "\n",
        "# tcost=TCost(1024).to(device)\n",
        "# x=torch.randn((256,1024), device=device)\n",
        "# # import time\n",
        "# # start = time.time()\n",
        "# out=tcost(x).squeeze()\n",
        "# # out=tcost.tcost(x).squeeze()\n",
        "# print(out)\n",
        "# # out=F.gumbel_softmax(out)\n",
        "# print(time.time()-start)\n",
        "# # nn.AdaptiveLogSoftmaxWithLoss(in_features=2, n_classes=2, cutoffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viimAIpYSJq_",
        "outputId": "2377ee32-2d6a-4c90-a91f-93ff4b885315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.6931, 0.3133, 0.6931])\n",
            "tensor([[0.6931, 0.6931],\n",
            "        [0.3133, 1.3133],\n",
            "        [0.6931, 0.6931]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "labels = torch.tensor([0,0,0])\n",
        "pred = torch.tensor([[50.,50.],[1.,0.],[1.,1.]])\n",
        "\n",
        "a=10\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "# loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)]))\n",
        "print(loss_fn(pred, labels))\n",
        "\n",
        "# weight=torch.tensor([1/a, 1/(1-a)])\n",
        "weight=torch.tensor([1, 1])\n",
        "# weight=torch.tensor([1/2, 1/2])\n",
        "\n",
        "\n",
        "# print((pred@torch.log(pred).T).sum())\n",
        "# print(nn.Softmax(dim=-1)(pred))\n",
        "# print(-(weight*torch.log(nn.Softmax(dim=-1)(pred))).mean(-1))\n",
        "# print(-(labels.float()@torch.log(nn.Softmax(dim=-1)(pred))))\n",
        "print(-(torch.log(nn.Softmax(dim=-1)(pred))))\n",
        "# print(pred,torch.log(pred).T)\n",
        "\n",
        "arange = torch.arange(pred.shape[-1]).repeat(pred.shape[0],1)\n",
        "# torch.where(1,0).bool()\n",
        "mask=(arange==pred)\n",
        "\n",
        "# 2*pred-1\n",
        "# (1-pred)\n",
        "(pred[mask] + (1-pred[mask]))\n",
        "\n",
        "log_preds = torch.log(torch.softmax(pred, dim=1))\n",
        "target_log_probs = log_preds[range(pred.shape[0]), labels]\n",
        "loss = -torch.mean(target_log_probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "1e61dfb5-44c6-4efd-eee0-67e412193ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-3cafa3463b16>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)# + self.z_coeff * torch.norm(z)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "# torch.norm(z, dim=-1)\n",
        "# -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "# in RL, distribution of action, if certainty is high, entropy is low\n",
        "\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "f11b49b9-382e-4f94-bdc2-fc93b88ca2f5",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-28fc5c7292bd>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=8, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 50 # 50 # 10 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 50 # 20 # 50 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 1 # 0.001 # 1 # ν cov Covariance\n",
        "        self.closs_coeff=10. # 100 # 100 # 100\n",
        "        # self.zloss_coeff=0. # 10 # 20 # 1\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        self.lx, self.lz = torch.empty((0,dim_a),device=device), torch.empty((0,dim_z),device=device) # [T,dim_az]\n",
        "        self.sx = self.jepa.enc(torch.zeros((1, 3,64,64)))\n",
        "        self.la = torch.empty(0,device=device)\n",
        "\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        if len(self.la)>1 or laction!=None:\n",
        "            self.update_h0(lstate, laction)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                self.sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(self.sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.la, self.lx, self.lz = lact, lx, lz\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx - torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    self.la = torch.cat([torch.tensor(laction, device=device), self.la[len(laction):]], dim=-1)\n",
        "                la = self.emb(self.la[:seq_len])\n",
        "\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5) # torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        optim_z = torch.optim.SGD([lz], lr=1e1) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e0 ; 3e-2 1e-1\n",
        "        lsx, la = lsx.detach(), la.detach() # [T, d_model], [T, dim_a]\n",
        "        # print(\"update_h0 lz\", lz.data)\n",
        "        self.jepa.pred.train()\n",
        "        for i in range(1): # 1?\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0.detach()) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                loss = F.mse_loss(out_, out.squeeze(0))\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            # print(\"update_h0 loss, lz\",i,loss.item(), lz.data)\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1].unsqueeze(0)\n",
        "        # print(\"update_h0\", self.lx.data)\n",
        "        self.la, self.lx, self.lz = self.la[seq_len:], self.lx[seq_len:], self.lz[seq_len:] # [T, dim_a], [T, dim_z]\n",
        "        return h0\n",
        "\n",
        "    def argm_s(self, sx, x, h0): # [1, d_model], [batch_, T, dim_a], [num_layers, 1, d_model] # batch argm z for search\n",
        "        batch_, T, _ = x.shape\n",
        "        batch = 16 # 16\n",
        "        z = nn.Parameter(torch.zeros((batch*batch_, T, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        optim_z = torch.optim.SGD([z], lr=1e4, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        with torch.no_grad(): z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch*batch_,1,1) # [batch*batch_, seq_len, dim_z]\n",
        "        sx, h0 = sx.detach(), h0.detach() # [1, d_model], [num_layers, 1, d_model]\n",
        "        x = x.detach().repeat(batch,1,1) # [batch, T, dim_a]\n",
        "        # print(\"argm\", z[0].squeeze())\n",
        "        for i in range(2): # 5\n",
        "            loss, lh0 = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"argm z loss\", z[0].squeeze().data, loss[0].squeeze().data)\n",
        "        idx = torch.argmin(loss.sum(-1).unflatten(0, (batch,batch_)), dim=0) # loss [batch*batch_, T] -> [batch_]\n",
        "        return torch.index_select(z, 0, idx) # [batch_, T,dim_z]\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch = 16\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        optim_x = torch.optim.SGD([x], lr=1e3) # 1e-1,1e-0,1e4 ; 1e2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        with torch.no_grad(): x[:,:self.lx.shape[0]] = self.lx.repeat(batch,1,1)[:,:T] # [seq_len, dim_az]\n",
        "        sx, h0 = sx.detach(), h0.detach() # [1, d_model], [num_layers, 1, d_model]\n",
        "        # print(\"search x\",x.squeeze().data)\n",
        "        for i in range(2): # 5\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data # [batch, T, dim_a]\n",
        "            z = self.argm_s(sx, x_,h0) # [batch,T, dim_z]\n",
        "            loss, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i, \"search x loss\", x.squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [T]\n",
        "        idx = torch.argmin(loss.sum(-1)) # loss [batch, T]\n",
        "        return lact[idx], lh0[:,:,idx], x.data[idx], z[idx] # [batch,T], [T, num_layers, batch, d_model], [batch,T, dim_a], [batch,T, dim_z]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz, h0, gamma=0.9): # 0.95 [1, d_model], [batch, T, dim_a/z], [num_layers,1, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        lsx, lh0 = self.rnn_it(sx, la, lz, h0)\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        syh0 = torch.cat([lsx, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,T,d_model], [T,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "        tcost = -self.tcost(syh0).unflatten(0, (batch, seq_len)).squeeze(-1)\n",
        "        c = (tcost + icost)*gamma**torch.arange(seq_len, device=device)\n",
        "        # if len(c.shape) == 1: print(\"rnn_pred c\", [f'{cc.item():g}' for cc in c.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        if len(tcost.shape) == 1: print(\"rnn_pred tcost\", [f'{cc.item():g}' for cc in tcost.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        return c, lh0\n",
        "\n",
        "    def rnn_it(self, sx, la, lz, h0): # 0.95 [1, d_model], [batch, T, dim_a/z], [num_layers,1, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        batch_ = batch//sx.shape[0]\n",
        "        sx, h0 = sx.repeat(batch_, 1), h0.repeat(1, batch_, 1)\n",
        "        lsx = torch.empty((batch, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1) # [batch, d_model+dim_a/z]\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            # sx = sx + out.squeeze(1) # [batch,seq_len,d_model] # h0 = h0 +\n",
        "            sx = out.squeeze(1) # [batch,1,d_model]\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        return lsx, lh0\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd): # best case z for train\n",
        "        # self.tcost.eval() # disable tcost dropout\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        batch = 64 # 16\n",
        "        lsy, la, rwd = lsy.repeat(batch,1,1), la.repeat(batch,1,1), rwd.repeat(batch,1) # [batch*batch_size, bptt, d_model], [batch*batch_size, d_model, dim_a], [batch*batch_size, bptt]\n",
        "        lz = nn.Parameter(torch.zeros((batch*batch_size, bptt, self.dim_z), device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lsy_, lh0 = self.rnn_it(sy_, la, lz, h0_)\n",
        "            # repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            repr_loss = ((lsy-lsy_)**2).unflatten(0, (batch,batch_size)).flatten(1).mean(-1)\n",
        "            syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch*batch_size,bptt,d_model], [bptt,num_layers,batch*batch_size,d_model] -> [batch*batch_size*bptt, (1+num_layers)*d_model]\n",
        "            # clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "            clossl = self.tcost.loss(syh0, rwd.flatten(), reduction='none').unflatten(0, (batch,batch_size*bptt)).mean(-1) # [batch*batch_size*bptt] -> [batch]\n",
        "            # z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl# + self.zloss_coeff * z_loss\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        idx = torch.argmin(cost)\n",
        "        return lz.unflatten(0, (batch,batch_size))[idx].squeeze(0).detach()\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        for batch, (state, action, reward) in enumerate(dataloader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            sy_ = self.jepa.enc(torch.zeros((batch_size, 3,64,64), device=device)).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "            state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "            for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    lsy = self.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "                    la = self.emb(act) # [batch_size, bptt, dim_a]\n",
        "                    lz = self.argm(lsy, sy_.squeeze(1), h0, la, rwd) # [batch_size, bptt, d_model],\n",
        "                    # with torch.no_grad(): lz.mul_(torch.rand_like(lz).uniform_(0.5)).mul_((torch.rand_like(lz)>0.1).bool()) # dropout without scaling\n",
        "                    with torch.no_grad(): lz.mul_(torch.rand_like(lz).uniform_(0)).mul_((torch.rand_like(lz)>0.5).bool()) # dropout without scaling\n",
        "                    lsy_, lh0 = self.rnn_it(sy_.squeeze(1), la, lz, h0)\n",
        "                    repr_loss = F.mse_loss(lsy, lsy_) # [batch_size, bptt, d_model]\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model] # not lsy_, else unstable\n",
        "                    clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                    closs = self.closs_coeff * clossl\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                    # pred = self.tcost(syh0).squeeze(-1).unflatten(0, rwd.shape) # [batch_size, bptt]\n",
        "                    # print(\"pred\",pred[0])\n",
        "                    # print(\"rwd\",rwd[0])\n",
        "                    # mask = torch.where(abs(rwd- pred)>0.5,1,0).bool()\n",
        "                    # print(\"rwd, pred, clossl\", rwd[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "                # torch.norm(lsy-torch.cat([sy_,lsy[:-1]], dim=1), dim=-1) # -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "                # prob = F.softmax(output, dim=-1)\n",
        "                # entropy = -torch.sum(prob * torch.log(prob + 1e-5), dim=-1)\n",
        "\n",
        "                # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                norm = torch.norm(lsy[0][0], dim=-1).item()\n",
        "                z_norm = torch.norm(lz[0][-1], dim=-1)\n",
        "                # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                # print(\"clossl, wrong\", clossl.item(), mask.sum())\n",
        "                # print(\"repr, std, cov, clossl, wrong\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), mask.sum().item())\n",
        "                print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                scaler.scale(loss).backward()\n",
        "                # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "                optim.zero_grad()\n",
        "                sy_, h0 = sy_.detach(), h0.detach()\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item(), \"z_norm\": z_norm.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.999)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "# !pip show torch triton\n",
        "# # !pip install --upgrade torch\n",
        "# !pip install --upgrade triton\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "# print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "# print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "# print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49RERFWFMgA_",
        "outputId": "23603c1c-4d96-4f45-da63-6bd602ac834f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0320, -0.0355, -0.0044,  ..., -0.0048,  0.0133,  0.0303],\n",
            "        [-0.0050,  0.0361, -0.0026,  ...,  0.0004, -0.0425,  0.0039]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# print(agent.jepa.enc.parameters().values()[0].requires_grad)\n",
        "# for name, param in agent.tcost.named_parameters():\n",
        "# # # for name, param in agent.named_parameters():\n",
        "# #     # print(name, param.requires_grad)\n",
        "#     print(name, param)\n",
        "\n",
        "# for name, param in agent.tcost.named_parameters(): print(param.data)\n",
        "\n",
        "# print(agent.tcost.1.weight.data)\n",
        "\n",
        "# print(agent.tcost.named_parameters()['tcost.1.weight'])\n",
        "\n",
        "# print(vars(agent.jepa.exp.named_parameters()['exp.1.weight']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "UEH1P802JkHU",
        "outputId": "31b3efd5-1338-4f55-9345-76a8d9371905"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'state' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2eab810ffe5c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# sx = agent.jepa.enc(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# dim_a, dim_z = 3, 8\n",
        "# batch, T = 4,6\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_a),device=device))\n",
        "# torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "# dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# x = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "# z = nn.Parameter(torch.zeros((batch, T, dim_z),device=device))\n",
        "# torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# state = torch.zeros((1, 3,64,64))\n",
        "# # state = torch.rand((1, 3,64,64), device=device)\n",
        "# sx = agent.jepa.enc(state)\n",
        "\n",
        "act = agent([state], k=4)\n",
        "# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\n",
        "# loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "# print(loss,c)\n",
        "# print(lact, lh0, lx, lz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "c46112c4-a939-4e53-c411-f20759634349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY\n",
            "From (redirected): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY&confirm=t&uuid=9757b9f5-2202-4964-a75c-bd00e34ac66a\n",
            "To: /content/buffer512.pkl\n",
            "100% 706M/706M [00:15<00:00, 44.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1 gru3 tcost1\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2 gru3 tcost1\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4 gru1 tcost1 drop\n",
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 1UDgNtFsWGAhvqR9lwA0QbMLhUtmip4ne -O agentoptim.pkl # M1 agentoptimgru3tcost1\n",
        "# !gdown 1-0oc6yucS5JXLHX1zqbYe3NTVMuhP_5r -O agentoptim.pkl # A2 agentoptim25251c25z3\n",
        "# !gdown 1U1CuCU1FugkrzPXsvTPpIX-wzWz6szl2 -O agentoptim.pkl # T4 agentoptimargm\n",
        "# !gdown 1CWZAtiEwSnglClJbq2LJTYlKhPN10gfo -O agentoptim.pkl # S3 agentoptimargm\n",
        "# !gdown 1XAbr6l1pCmcUCKR6kYlQ_dSDsOBqRg_j -O agentoptim.pkl # B2 argm2search2\n",
        "# !gdown 1UkQuf-IC2LYErSapkF6rZM1dv3svGI5P -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1-4sNf6mINCiD5YsBdQvCrlyqzzfS64si -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1MV9Qj_53Vu6wpe7nOFn47M5vDj7F7-gv -O agentoptim.pkl # S3 agentoptimargm2\n",
        "# !gdown 1--1Vl3337zugQng-j1qbptFY8EvhZA-T -O agentoptim.pkl # T4 agentoptimargm3 online\n",
        "# !gdown 1XHFBVPSH4T4FpUOBKN8X20xDQLNmL7go -O agentoptim.pkl # M1 agentoptimargm4\n",
        "# !gdown 1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH -O agentoptim.pkl # B2 agentoptimargm4\n",
        "# !gdown 1H31OMz5YBPfmgb7yxVePGddljS9cwVOF -O agentoptim.pkl # B2 agent_nores1\n",
        "\n",
        "\n",
        "# !gdown 1sCW9uvcdCJkCH5HQDdISLws5rMvmkmFR -O all_sd.pkl # M1 all_sd\n",
        "\n",
        "import pickle\n",
        "# !gdown 1j9hOq8_752duPB0PMYUJqabNvYoGLysX -O buffer512down.pkl # S\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "!gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB -O buffergo.pkl # S3\n",
        "# !gdown 1egXy0t_kn0M0oL6sbwixoVr7bqMfcB8j -O buffergo.pkl # T4\n",
        "# !gdown 1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ -O buffergo.pkl # B2\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "6c96d425-5076-43cc-f09f-8c47615da7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-db52b8ac5bcb>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load(folder+'agentoptim1.pkl', map_location=device).values()\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "# with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load(folder+'agent_res1.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load(folder+'agentoptim1.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# modelsd = transfer_sd(agent.state_dict(), modelsd)\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "# # optimsd = transfer_optim(agent.state_dict(), modelsd, optim.state_dict(), optimsd)\n",
        "optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = torch.load(folder+'all_sd.pkl', map_location=device)\n",
        "# # all_sd = torch.load('all_sd.pkl', map_location=device)\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in all_sd.items())\n",
        "# allsd = {}\n",
        "# for (k, v) in all_sd.items():\n",
        "#     try: allsd[convert[k]] = v\n",
        "#     except Exception as e: print('dict err', e)\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# tgt_sd = load_sd(agent.state_dict(), allsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# for i, (k,v) in enumerate(modelsd.items()):\n",
        "# for i, (k,v) in enumerate(agent.state_dict().items()):\n",
        "#     print(i,k,v.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# checkpoint = {'model': agentsd, 'optimizer': optim.state_dict(),}\n",
        "checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "torch.save(checkpoint, folder+'agentoptimgru3.pkl')\n",
        "# torch.save(checkpoint, folder+'agent_nores.pkl')\n",
        "# torch.save(checkpoint, 'agentoptim.pkl')\n",
        "\n",
        "# all_sd = {}\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# all_sd = store_sd(all_sd, agentsd)\n",
        "# # torch.save(all_sd, 'all_sd.pkl')\n",
        "# torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 100):] for episode in cleaned]\n",
        "        random.shuffle(cleaned)\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "\n",
        "    # def pop_unif(self, buffer_, n=3):\n",
        "    #     buffer_.pop(random.randrange(len(buffer_)))\n",
        "    #     return buffer_\n",
        "\n",
        "# while len(train_data.data)>10000:\n",
        "#     buffer.pop(random.randrange(len(buffer)))\n",
        "#     train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 #128\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True)\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # # [3,T,batch]\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e3fpbtNOiz1",
        "outputId": "e73a096c-4aba-40f6-e30d-cb8be7049780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1., -1.,  0.,  0.,  0., -1.,  0., -1.,  0.,  0.])\n",
            "tensor([ 0.,  0., -1., -1., -1., -1.,  0., -1.,  0.,  0.])\n",
            "tensor([-1.,  0., -1.,  0.,  0.,  0., -1., -1.,  0.,  0.])\n",
            "tensor([-1.,  0., -1., -1., -1.,  0.,  0.,  0.,  0., -1.])\n",
            "tensor([-9.9281e-01, -9.8925e-01, -3.0524e-03, -3.4155e-04, -3.4662e-04,\n",
            "        -9.8640e-01, -1.7098e-03, -9.9127e-01, -4.9040e-04, -1.0240e-03])\n",
            "tensor([-6.0285e-04, -5.8750e-03, -9.9606e-01, -9.9109e-01, -9.9567e-01,\n",
            "        -9.9033e-01, -9.3630e-03, -9.9367e-01, -1.7606e-03, -7.0756e-03])\n",
            "tensor([-9.7177e-01, -1.2001e-03, -9.9647e-01, -8.4650e-04, -1.0934e-01,\n",
            "        -7.8392e-04, -9.1979e-01, -9.9124e-01, -9.1642e-04, -1.2610e-03])\n",
            "tensor([-9.9308e-01, -1.1652e-03, -7.2673e-01, -9.9198e-01, -9.8900e-01,\n",
            "        -1.2084e-02, -1.0412e-03, -7.3442e-04, -3.2916e-03, -9.8409e-01])\n",
            "tensor(0.7608, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor(0.0024)\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "reward, pred tensor([]) tensor([])\n"
          ]
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "ctrain_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    ctrain_data=list(zip(state,reward))\n",
        "    ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# images, labels = images.to(device), labels.to(device)\n",
        "batch=40\n",
        "images, labels = images[:batch], labels[:batch]\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range(len(labels)//10):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# # try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch, agent.d_model), device=device)\n",
        "    # h0 = torch.empty((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device)\n",
        "    # torch.nn.init.xavier_normal_(h0)\n",
        "    sy = agent.jepa.enc(images.to(device)) # [batch_size, d_model]\n",
        "    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "    pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "\n",
        "    # print(pred)\n",
        "    for x in range(len(pred)//10):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(agent.tcost.loss(syh0, labels.to(device)).squeeze(-1))\n",
        "print(F.mse_loss(labels, pred))\n",
        "\n",
        "# torch.where(abs(labels- pred)>0.5,1,0)\n",
        "for x in range(len(pred)//10):\n",
        "    print(torch.where(abs(labels- pred)>0.5,1,0)[10*x:10*x+10])\n",
        "\n",
        "mask = torch.where(abs(labels- pred)>0.5,1,0).bool()\n",
        "print(\"reward, pred\", labels[mask].data, pred[mask].data)\n",
        "try: imshow(torchvision.utils.make_grid(images[mask], nrow=10))\n",
        "except ZeroDivisionError: pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# optim = torch.optim.SGD(agent.parameters(), 1e-1, momentum=0.9, dampening=0, weight_decay=0)\n",
        "# print(optim.param_groups[0][\"lr\"])\n",
        "# print(optim)\n",
        "optim.param_groups[0][\"lr\"] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGYjyJZ5aG5z",
        "outputId": "e8509839-3e6a-4a0f-d8be-6bcf1ccdaf83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-28fc5c7292bd>:232: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repr, std, cov, clossl, z, norm 0.03190275654196739 0.042572021484375 5.320165634155273 0.10700380057096481 0.07531029731035233 3.111328125\n",
            "repr, std, cov, clossl, z, norm 0.031660400331020355 0.0469970703125 4.942805290222168 0.12220074981451035 0.15836527943611145 4.55859375\n",
            "repr, std, cov, clossl, z, norm 0.03247496113181114 0.040313720703125 5.122428894042969 0.11727520823478699 0.19766424596309662 4.875\n",
            "repr, std, cov, clossl, z, norm 0.033629488199949265 0.040618896484375 5.329663276672363 0.18892142176628113 0.1580849438905716 4.87109375\n",
            "repr, std, cov, clossl, z, norm 0.03373674675822258 0.02386474609375 6.094228744506836 0.12166440486907959 0.16066819429397583 5.08984375\n",
            "repr, std, cov, clossl, z, norm 0.03503517434000969 0.02581787109375 5.816171646118164 0.13974826037883759 0.02547127567231655 3.09765625\n",
            "repr, std, cov, clossl, z, norm 0.03277933970093727 0.03497314453125 5.393190383911133 0.10890388488769531 0.17298167943954468 3.712890625\n",
            "repr, std, cov, clossl, z, norm 0.033373549580574036 0.03619384765625 5.535443305969238 0.18714919686317444 0.04538131132721901 4.5390625\n",
            "repr, std, cov, clossl, z, norm 0.033770397305488586 0.0263214111328125 5.938429832458496 0.11055106669664383 0.20333854854106903 4.5625\n",
            "repr, std, cov, clossl, z, norm 0.03592005744576454 0.024383544921875 5.9149556159973145 0.129307359457016 0.061403606086969376 4.609375\n",
            "repr, std, cov, clossl, z, norm 0.033326830714941025 0.037841796875 5.196943759918213 0.1495460867881775 0.03392802178859711 4.80859375\n",
            "repr, std, cov, clossl, z, norm 0.032252468168735504 0.03521728515625 5.386470317840576 0.1168488934636116 0.034329529851675034 4.94921875\n",
            "repr, std, cov, clossl, z, norm 0.03290833532810211 0.043243408203125 5.094152450561523 0.16258883476257324 0.05162263661623001 3.24609375\n",
            "repr, std, cov, clossl, z, norm 0.033308111131191254 0.047210693359375 4.743300914764404 0.14887374639511108 0.04084361717104912 3.396484375\n",
            "repr, std, cov, clossl, z, norm 0.03322763741016388 0.0261993408203125 6.004708290100098 0.1196165606379509 0.06391818076372147 4.96484375\n",
            "repr, std, cov, clossl, z, norm 0.03389160707592964 0.0268096923828125 5.796294689178467 0.10347192734479904 0.08348706364631653 3.841796875\n",
            "1\n",
            "repr, std, cov, clossl, z, norm 0.033035654574632645 0.037139892578125 5.413196563720703 0.10901352018117905 0.07700718939304352 3.568359375\n",
            "repr, std, cov, clossl, z, norm 0.03362080454826355 0.039703369140625 5.249567031860352 0.13597455620765686 0.12211157381534576 2.728515625\n",
            "repr, std, cov, clossl, z, norm 0.032172467559576035 0.04034423828125 4.986467361450195 0.11127987504005432 0.18099333345890045 2.923828125\n",
            "repr, std, cov, clossl, z, norm 0.03295915201306343 0.029571533203125 5.639369964599609 0.11500246077775955 0.08467568457126617 4.1171875\n",
            "repr, std, cov, clossl, z, norm 0.03171113505959511 0.0306549072265625 5.5130205154418945 0.09855392575263977 0.14463739097118378 4.796875\n",
            "repr, std, cov, clossl, z, norm 0.033366575837135315 0.0259857177734375 5.740411281585693 0.09659253805875778 0.1218450516462326 3.7265625\n",
            "repr, std, cov, clossl, z, norm 0.03548428416252136 0.0246124267578125 5.68852424621582 0.15626737475395203 0.028091594576835632 3.3671875\n",
            "repr, std, cov, clossl, z, norm 0.03199755772948265 0.032073974609375 5.331609725952148 0.10419406741857529 0.04145723581314087 3.498046875\n",
            "repr, std, cov, clossl, z, norm 0.03552117943763733 0.02783203125 5.67495584487915 0.13231641054153442 0.11879149824380875 3.5625\n",
            "repr, std, cov, clossl, z, norm 0.03587200865149498 0.0300750732421875 5.460536003112793 0.1905927211046219 0.1673923283815384 2.443359375\n",
            "repr, std, cov, clossl, z, norm 0.035485509783029556 0.0268402099609375 5.482885360717773 0.17973633110523224 0.10727111995220184 3.578125\n",
            "repr, std, cov, clossl, z, norm 0.03213557228446007 0.0300445556640625 5.576176166534424 0.11723453551530838 0.15580305457115173 8.7578125\n",
            "repr, std, cov, clossl, z, norm 0.03603701665997505 0.0215301513671875 6.117684841156006 0.14190639555454254 0.21022386848926544 5.09375\n",
            "repr, std, cov, clossl, z, norm 0.03134239464998245 0.0408935546875 5.033344268798828 0.08159177750349045 0.12870484590530396 4.87890625\n",
            "repr, std, cov, clossl, z, norm 0.03536501154303551 0.03192138671875 5.265162467956543 0.14087863266468048 0.13904376327991486 2.681640625\n",
            "repr, std, cov, clossl, z, norm 0.03522903472185135 0.032440185546875 5.386377334594727 0.11718973517417908 0.120455302298069 3.765625\n",
            "2\n",
            "repr, std, cov, clossl, z, norm 0.033553920686244965 0.034942626953125 5.469042778015137 0.13888448476791382 0.10432108491659164 4.84765625\n",
            "repr, std, cov, clossl, z, norm 0.03349548578262329 0.040374755859375 4.9031662940979 0.15481573343276978 0.0830838531255722 5.875\n",
            "repr, std, cov, clossl, z, norm 0.034433186054229736 0.0307464599609375 5.256923198699951 0.13609762489795685 0.18268102407455444 5.59375\n",
            "repr, std, cov, clossl, z, norm 0.032757025212049484 0.030303955078125 5.2718658447265625 0.10174886137247086 0.12117202579975128 3.94921875\n",
            "repr, std, cov, clossl, z, norm 0.03610093519091606 0.0176849365234375 6.03537654876709 0.13598956167697906 0.05505722388625145 5.15625\n",
            "repr, std, cov, clossl, z, norm 0.03537352383136749 0.0220794677734375 5.730465888977051 0.11399543285369873 0.11922622472047806 4.8046875\n",
            "repr, std, cov, clossl, z, norm 0.03317646309733391 0.0254974365234375 5.605498790740967 0.09045513719320297 0.07589269429445267 3.7578125\n",
            "repr, std, cov, clossl, z, norm 0.03531824052333832 0.028656005859375 5.453089714050293 0.20564796030521393 0.2778205871582031 4.14453125\n",
            "repr, std, cov, clossl, z, norm 0.032362595200538635 0.033477783203125 5.157417297363281 0.11421334743499756 0.15081185102462769 5.52734375\n",
            "repr, std, cov, clossl, z, norm 0.03302900120615959 0.0406494140625 4.840058326721191 0.09934742003679276 0.10327158123254776 4.015625\n",
            "repr, std, cov, clossl, z, norm 0.03482450544834137 0.0249176025390625 5.710290908813477 0.12773752212524414 0.09252163767814636 4.6640625\n",
            "repr, std, cov, clossl, z, norm 0.034422680735588074 0.02362060546875 5.711489200592041 0.11284879595041275 0.007790043018758297 3.5\n",
            "repr, std, cov, clossl, z, norm 0.035438574850559235 0.02850341796875 5.267963409423828 0.17219141125679016 0.15678200125694275 5.015625\n",
            "repr, std, cov, clossl, z, norm 0.03637443110346794 0.023834228515625 5.522334098815918 0.15293817222118378 0.039750561118125916 3.572265625\n",
            "repr, std, cov, clossl, z, norm 0.034078944474458694 0.0279388427734375 5.445544719696045 0.1283489167690277 0.17146024107933044 3.888671875\n",
            "repr, std, cov, clossl, z, norm 0.03578133136034012 0.0289764404296875 5.217688083648682 0.13119298219680786 0.10360369086265564 3.748046875\n",
            "3\n",
            "repr, std, cov, clossl, z, norm 0.03432531654834747 0.0284576416015625 5.363832473754883 0.14743153750896454 0.021655654534697533 3.171875\n",
            "repr, std, cov, clossl, z, norm 0.034581560641527176 0.034271240234375 5.0318603515625 0.13727082312107086 0.20752736926078796 4.57421875\n",
            "repr, std, cov, clossl, z, norm 0.03240378573536873 0.0293731689453125 5.156035423278809 0.12731970846652985 0.10300975292921066 3.404296875\n",
            "repr, std, cov, clossl, z, norm 0.03442300111055374 0.018096923828125 5.872553825378418 0.14721062779426575 0.0784013420343399 4.7890625\n",
            "repr, std, cov, clossl, z, norm 0.03316139429807663 0.0233001708984375 5.492530822753906 0.10572938621044159 0.08313164114952087 3.6953125\n",
            "repr, std, cov, clossl, z, norm 0.03474552929401398 0.02191162109375 5.482134819030762 0.16562557220458984 0.1344021111726761 4.0546875\n",
            "repr, std, cov, clossl, z, norm 0.036215584725141525 0.0243377685546875 5.528350830078125 0.1345728486776352 0.2627772390842438 6.296875\n",
            "repr, std, cov, clossl, z, norm 0.034155216068029404 0.0294647216796875 5.158106803894043 0.12406878173351288 0.13205483555793762 4.44140625\n",
            "repr, std, cov, clossl, z, norm 0.035636790096759796 0.0235748291015625 5.611380577087402 0.1374974101781845 0.037348661571741104 4.41796875\n",
            "repr, std, cov, clossl, z, norm 0.0349816158413887 0.0252685546875 5.40006160736084 0.145537868142128 0.1895758956670761 3.91015625\n",
            "repr, std, cov, clossl, z, norm 0.034153103828430176 0.0280303955078125 5.30441951751709 0.16250000894069672 0.07236188650131226 3.6640625\n",
            "repr, std, cov, clossl, z, norm 0.03559175506234169 0.0223541259765625 5.522965431213379 0.1309177130460739 0.05589676648378372 3.37890625\n",
            "repr, std, cov, clossl, z, norm 0.034482620656490326 0.0307769775390625 5.12823486328125 0.12449813634157181 0.07754350453615189 4.48828125\n",
            "repr, std, cov, clossl, z, norm 0.03353597968816757 0.03485107421875 4.91433048248291 0.1276325285434723 0.08878528326749802 4.6953125\n",
            "repr, std, cov, clossl, z, norm 0.03438969701528549 0.033416748046875 5.010844707489014 0.134567528963089 0.05445515364408493 3.494140625\n",
            "repr, std, cov, clossl, z, norm 0.03348176181316376 0.024627685546875 5.46907901763916 0.10120856016874313 0.12837673723697662 4.0078125\n",
            "4\n",
            "repr, std, cov, clossl, z, norm 0.0344545915722847 0.02630615234375 5.218300819396973 0.12585841119289398 0.11387845873832703 4.65625\n",
            "repr, std, cov, clossl, z, norm 0.036207396537065506 0.01776123046875 5.572536945343018 0.1798296719789505 0.008480857126414776 3.134765625\n",
            "repr, std, cov, clossl, z, norm 0.035008691251277924 0.0178985595703125 5.633996486663818 0.10939442366361618 0.13532045483589172 3.775390625\n",
            "repr, std, cov, clossl, z, norm 0.03572048619389534 0.0204315185546875 5.424186706542969 0.12931956350803375 0.27266865968704224 4.7578125\n",
            "repr, std, cov, clossl, z, norm 0.033263977617025375 0.0281829833984375 5.060397624969482 0.12336816638708115 0.10107838362455368 3.45703125\n",
            "repr, std, cov, clossl, z, norm 0.0320509672164917 0.03497314453125 4.763657569885254 0.11344252526760101 0.09223746508359909 3.525390625\n",
            "repr, std, cov, clossl, z, norm 0.03440037742257118 0.0310821533203125 4.970084190368652 0.14939790964126587 0.1199258342385292 4.37890625\n",
            "repr, std, cov, clossl, z, norm 0.03451498970389366 0.0224609375 5.327584743499756 0.10255603492259979 0.02509227767586708 4.078125\n",
            "repr, std, cov, clossl, z, norm 0.03571775183081627 0.0159759521484375 5.637050628662109 0.12010739743709564 0.1319970339536667 4.359375\n",
            "repr, std, cov, clossl, z, norm 0.03557700663805008 0.019378662109375 5.320596694946289 0.13774897158145905 0.1619434952735901 4.0234375\n",
            "repr, std, cov, clossl, z, norm 0.037012405693531036 0.01800537109375 5.4042158126831055 0.1175990104675293 0.041376255452632904 3.623046875\n",
            "repr, std, cov, clossl, z, norm 0.034566041082143784 0.0263214111328125 5.113150596618652 0.10887780785560608 0.08261137455701828 4.33984375\n",
            "repr, std, cov, clossl, z, norm 0.03371534124016762 0.02923583984375 4.900447368621826 0.107253298163414 0.20730851590633392 5.1796875\n",
            "repr, std, cov, clossl, z, norm 0.0363229401409626 0.0230560302734375 5.214521884918213 0.16930805146694183 0.08054506778717041 4.85546875\n",
            "repr, std, cov, clossl, z, norm 0.03262609243392944 0.02813720703125 4.965094566345215 0.10589633882045746 0.17169949412345886 5.109375\n",
            "repr, std, cov, clossl, z, norm 0.03738955035805702 0.0128021240234375 5.8740925788879395 0.15064099431037903 0.06548124551773071 3.2578125\n",
            "5\n",
            "repr, std, cov, clossl, z, norm 0.03466929495334625 0.0193023681640625 5.361204624176025 0.1537051498889923 0.19516795873641968 3.853515625\n",
            "repr, std, cov, clossl, z, norm 0.03160235285758972 0.0290985107421875 4.896553993225098 0.09220711141824722 0.04668768122792244 5.81640625\n",
            "repr, std, cov, clossl, z, norm 0.03385617211461067 0.032012939453125 4.835461616516113 0.09511607885360718 0.17649181187152863 4.328125\n",
            "repr, std, cov, clossl, z, norm 0.03517044335603714 0.0276947021484375 4.821934700012207 0.15083907544612885 0.30212196707725525 4.984375\n",
            "repr, std, cov, clossl, z, norm 0.035102009773254395 0.02191162109375 5.172113418579102 0.13401098549365997 0.17733696103096008 4.3671875\n",
            "repr, std, cov, clossl, z, norm 0.03604245185852051 0.0176544189453125 5.428524971008301 0.13081374764442444 0.004361169412732124 2.716796875\n",
            "repr, std, cov, clossl, z, norm 0.03463902696967125 0.0188140869140625 5.401330947875977 0.11961495876312256 0.13103163242340088 3.119140625\n",
            "repr, std, cov, clossl, z, norm 0.03458414971828461 0.0210723876953125 5.159992694854736 0.15806016325950623 0.13441303372383118 3.623046875\n",
            "repr, std, cov, clossl, z, norm 0.03571082279086113 0.0178375244140625 5.203428268432617 0.11540118604898453 0.012734572403132915 3.748046875\n",
            "repr, std, cov, clossl, z, norm 0.03470756486058235 0.0208587646484375 5.212908744812012 0.11353357881307602 0.1846824586391449 4.57421875\n",
            "repr, std, cov, clossl, z, norm 0.03684597462415695 0.02252197265625 5.115849494934082 0.16896496713161469 0.09346748888492584 4.37109375\n",
            "repr, std, cov, clossl, z, norm 0.034608978778123856 0.017578125 5.254552841186523 0.10743124783039093 0.10251596570014954 4.0390625\n",
            "repr, std, cov, clossl, z, norm 0.034357309341430664 0.0222320556640625 4.9738311767578125 0.11435530334711075 0.07485923916101456 3.90234375\n",
            "repr, std, cov, clossl, z, norm 0.03398618474602699 0.025054931640625 4.869409561157227 0.10669411718845367 0.18556267023086548 7.8671875\n",
            "repr, std, cov, clossl, z, norm 0.03490304574370384 0.0266265869140625 5.030372142791748 0.13141170144081116 0.029002578929066658 4.23046875\n",
            "repr, std, cov, clossl, z, norm 0.03441664204001427 0.01522064208984375 5.3885087966918945 0.09977712482213974 0.04861026257276535 4.96484375\n",
            "6\n",
            "repr, std, cov, clossl, z, norm 0.034170351922512054 0.016082763671875 5.280500411987305 0.11113104224205017 0.11995808035135269 4.22265625\n",
            "repr, std, cov, clossl, z, norm 0.037209104746580124 0.0173797607421875 5.207113265991211 0.1834024041891098 0.13704316318035126 5.05078125\n",
            "repr, std, cov, clossl, z, norm 0.03672576695680618 0.0214691162109375 4.927862644195557 0.1450522243976593 0.15175965428352356 4.4375\n",
            "repr, std, cov, clossl, z, norm 0.03592779487371445 0.022796630859375 4.836550712585449 0.09929120540618896 0.11064842343330383 3.955078125\n",
            "repr, std, cov, clossl, z, norm 0.034637436270713806 0.019012451171875 5.110448837280273 0.12003511935472488 0.12716564536094666 3.64453125\n",
            "repr, std, cov, clossl, z, norm 0.036660633981227875 0.01947021484375 5.08350944519043 0.11538118124008179 0.06651181727647781 4.6484375\n",
            "repr, std, cov, clossl, z, norm 0.033574946224689484 0.0160675048828125 5.1756439208984375 0.0820736289024353 0.012243407778441906 5.66015625\n",
            "repr, std, cov, clossl, z, norm 0.03612188249826431 0.01279449462890625 5.362573623657227 0.1162896379828453 0.023634081706404686 4.12109375\n",
            "repr, std, cov, clossl, z, norm 0.03454817086458206 0.0226898193359375 4.9893598556518555 0.1441134512424469 0.04937797412276268 4.07421875\n",
            "repr, std, cov, clossl, z, norm 0.03390977159142494 0.026641845703125 4.788742542266846 0.1694459170103073 0.029880106449127197 3.703125\n",
            "repr, std, cov, clossl, z, norm 0.03571845591068268 0.0266876220703125 4.733206748962402 0.11711718142032623 0.03232620283961296 4.0703125\n",
            "repr, std, cov, clossl, z, norm 0.03524494543671608 0.01494598388671875 5.277405738830566 0.11759122461080551 0.046060893684625626 3.486328125\n",
            "repr, std, cov, clossl, z, norm 0.03653354570269585 0.017120361328125 5.08897590637207 0.13185518980026245 0.11864961683750153 3.390625\n",
            "repr, std, cov, clossl, z, norm 0.03712999075651169 0.01558685302734375 5.272980213165283 0.14091594517230988 0.03415658324956894 3.9765625\n",
            "repr, std, cov, clossl, z, norm 0.035825543105602264 0.019866943359375 4.95114803314209 0.1121750995516777 0.0773872435092926 4.875\n",
            "repr, std, cov, clossl, z, norm 0.0350220762193203 0.0186767578125 4.919602870941162 0.12020499259233475 0.08848979324102402 4.859375\n",
            "7\n",
            "repr, std, cov, clossl, z, norm 0.033365726470947266 0.0225982666015625 4.923678398132324 0.10466545075178146 0.12609870731830597 3.7109375\n",
            "repr, std, cov, clossl, z, norm 0.03581102937459946 0.020050048828125 5.054584503173828 0.13549646735191345 0.09403304010629654 4.5078125\n",
            "repr, std, cov, clossl, z, norm 0.03677883744239807 0.0184173583984375 5.119132041931152 0.13644012808799744 0.11565137654542923 3.552734375\n",
            "repr, std, cov, clossl, z, norm 0.03325023874640465 0.0223236083984375 4.987197399139404 0.10890796780586243 0.11615952104330063 3.529296875\n",
            "repr, std, cov, clossl, z, norm 0.034866202622652054 0.023773193359375 4.768896102905273 0.16872242093086243 0.16941799223423004 3.90625\n",
            "repr, std, cov, clossl, z, norm 0.03753727301955223 0.01432037353515625 5.191042900085449 0.13793648779392242 0.07805202901363373 4.1171875\n",
            "repr, std, cov, clossl, z, norm 0.035531722009181976 0.0159759521484375 5.3516364097595215 0.13210415840148926 0.04615006595849991 4.44921875\n",
            "repr, std, cov, clossl, z, norm 0.034066807478666306 0.01702880859375 5.115607261657715 0.08881566673517227 0.21896019577980042 4.0078125\n",
            "repr, std, cov, clossl, z, norm 0.03530478477478027 0.0200653076171875 4.77195405960083 0.11432509869337082 0.17136788368225098 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.03407702594995499 0.02392578125 4.719321250915527 0.1743384450674057 0.1004742980003357 4.03125\n",
            "repr, std, cov, clossl, z, norm 0.0364295095205307 0.0183563232421875 5.071266174316406 0.147135391831398 0.14755743741989136 2.8515625\n",
            "repr, std, cov, clossl, z, norm 0.035605501383543015 0.02392578125 4.809696197509766 0.13291296362876892 0.22494281828403473 3.78125\n",
            "repr, std, cov, clossl, z, norm 0.03574688732624054 0.0200347900390625 4.744267463684082 0.13543540239334106 0.06581243127584457 3.740234375\n",
            "repr, std, cov, clossl, z, norm 0.03406510129570961 0.020172119140625 4.944368839263916 0.1272766888141632 0.09606858342885971 4.40625\n",
            "repr, std, cov, clossl, z, norm 0.03646759316325188 0.016998291015625 5.0553789138793945 0.1219237893819809 0.15465012192726135 3.74609375\n",
            "repr, std, cov, clossl, z, norm 0.034821346402168274 0.0186920166015625 4.993541717529297 0.1581571102142334 0.08326172083616257 4.38671875\n",
            "8\n",
            "repr, std, cov, clossl, z, norm 0.034810714423656464 0.0177154541015625 5.05912971496582 0.11156395077705383 0.06473182886838913 3.22265625\n",
            "repr, std, cov, clossl, z, norm 0.034673433750867844 0.0213623046875 4.722285270690918 0.12872038781642914 0.1263028234243393 8.5625\n",
            "repr, std, cov, clossl, z, norm 0.03405393287539482 0.016693115234375 4.98592472076416 0.1320176124572754 0.25432953238487244 4.1640625\n",
            "repr, std, cov, clossl, z, norm 0.03484690934419632 0.0147857666015625 5.033669471740723 0.13813146948814392 0.021001528948545456 4.81640625\n",
            "repr, std, cov, clossl, z, norm 0.03572307154536247 0.0147705078125 5.19283390045166 0.12411943078041077 0.03721299394965172 3.283203125\n",
            "repr, std, cov, clossl, z, norm 0.033885788172483444 0.021942138671875 4.701342582702637 0.09514635801315308 0.023558788001537323 4.56640625\n",
            "repr, std, cov, clossl, z, norm 0.03240669146180153 0.0193939208984375 4.787360191345215 0.10737022012472153 0.27440688014030457 4.06640625\n",
            "repr, std, cov, clossl, z, norm 0.03344373777508736 0.0233001708984375 4.570230960845947 0.10574586689472198 0.03932070732116699 3.896484375\n",
            "repr, std, cov, clossl, z, norm 0.03323448821902275 0.01611328125 4.961739540100098 0.15184412896633148 0.1519787311553955 4.25390625\n",
            "repr, std, cov, clossl, z, norm 0.03425374999642372 0.010009765625 5.512805461883545 0.10577470809221268 0.11236666142940521 4.2265625\n",
            "repr, std, cov, clossl, z, norm 0.035426173359155655 0.01442718505859375 5.075400352478027 0.10733165591955185 0.2152230590581894 4.375\n",
            "repr, std, cov, clossl, z, norm 0.03456595912575722 0.0233001708984375 4.515285491943359 0.13890108466148376 0.14158466458320618 4.33984375\n",
            "repr, std, cov, clossl, z, norm 0.03313302621245384 0.028594970703125 4.405394554138184 0.12506504356861115 0.119020015001297 5.0\n",
            "repr, std, cov, clossl, z, norm 0.0345458984375 0.0220794677734375 4.876628875732422 0.11484388262033463 0.1137474924325943 4.72265625\n",
            "repr, std, cov, clossl, z, norm 0.03410341590642929 0.01739501953125 4.943127155303955 0.13292139768600464 0.25057217478752136 4.08203125\n",
            "repr, std, cov, clossl, z, norm 0.03538138419389725 0.0189971923828125 4.831435680389404 0.11378908157348633 0.052125658839941025 3.44140625\n",
            "9\n",
            "repr, std, cov, clossl, z, norm 0.03484257310628891 0.017913818359375 4.781903266906738 0.1432252824306488 0.19161058962345123 4.91796875\n",
            "repr, std, cov, clossl, z, norm 0.03495081514120102 0.017333984375 4.799238204956055 0.11947145313024521 0.099629707634449 4.10546875\n",
            "repr, std, cov, clossl, z, norm 0.036688413470983505 0.01058197021484375 5.383213996887207 0.12089864164590836 0.16066470742225647 4.08203125\n",
            "repr, std, cov, clossl, z, norm 0.03326135501265526 0.0184783935546875 4.710785865783691 0.08709302544593811 0.1785009801387787 4.18359375\n",
            "repr, std, cov, clossl, z, norm 0.03441137075424194 0.022186279296875 4.5328216552734375 0.1365051120519638 0.10684249550104141 3.908203125\n",
            "repr, std, cov, clossl, z, norm 0.03354146331548691 0.0196075439453125 4.760019302368164 0.09248914569616318 0.16209767758846283 4.1328125\n",
            "repr, std, cov, clossl, z, norm 0.03530987724661827 0.020416259765625 4.656638145446777 0.12330513447523117 0.10524758696556091 4.87890625\n",
            "repr, std, cov, clossl, z, norm 0.03684166446328163 0.01110076904296875 5.093406677246094 0.14885954558849335 0.12082644551992416 3.333984375\n",
            "repr, std, cov, clossl, z, norm 0.03534479811787605 0.0142364501953125 4.897664546966553 0.12609364092350006 0.07776739448308945 3.3203125\n",
            "repr, std, cov, clossl, z, norm 0.03638235107064247 0.0169830322265625 4.857948303222656 0.17674575746059418 0.036032821983098984 3.94921875\n",
            "repr, std, cov, clossl, z, norm 0.03520637005567551 0.015594482421875 4.782970428466797 0.13201583921909332 0.07396198809146881 3.4375\n",
            "repr, std, cov, clossl, z, norm 0.033598124980926514 0.0233612060546875 4.378430366516113 0.10900641977787018 0.10299704968929291 4.32421875\n",
            "repr, std, cov, clossl, z, norm 0.03429705277085304 0.021026611328125 4.5535359382629395 0.11495278775691986 0.05131322517991066 3.84765625\n",
            "repr, std, cov, clossl, z, norm 0.03589014708995819 0.017547607421875 4.79005241394043 0.16105662286281586 0.132561594247818 3.787109375\n",
            "repr, std, cov, clossl, z, norm 0.03518461063504219 0.01306915283203125 4.901087760925293 0.092292919754982 0.055835992097854614 4.765625\n",
            "repr, std, cov, clossl, z, norm 0.035049840807914734 0.01174163818359375 4.937828063964844 0.10782051086425781 0.11409101635217667 4.54296875\n",
            "10\n",
            "repr, std, cov, clossl, z, norm 0.033615849912166595 0.0181884765625 4.571163654327393 0.0953814834356308 0.0866335853934288 2.263671875\n",
            "repr, std, cov, clossl, z, norm 0.03276476263999939 0.0220184326171875 4.470492839813232 0.12344680726528168 0.08815009891986847 3.642578125\n",
            "repr, std, cov, clossl, z, norm 0.03249543532729149 0.0213165283203125 4.420928001403809 0.13601087033748627 0.11754894256591797 4.7734375\n",
            "repr, std, cov, clossl, z, norm 0.034282322973012924 0.0150146484375 4.735208988189697 0.1054181456565857 0.0998242124915123 3.916015625\n",
            "repr, std, cov, clossl, z, norm 0.035168036818504333 0.00917816162109375 4.9650750160217285 0.1277008056640625 0.0846085399389267 5.5\n",
            "repr, std, cov, clossl, z, norm 0.038135431706905365 0.00408935546875 5.617509841918945 0.13415908813476562 0.08921736478805542 3.7265625\n",
            "repr, std, cov, clossl, z, norm 0.033657871186733246 0.0216827392578125 4.355960369110107 0.09453106671571732 0.20844362676143646 3.83203125\n",
            "repr, std, cov, clossl, z, norm 0.03403211012482643 0.029083251953125 4.071269989013672 0.19342610239982605 0.07893284410238266 4.30859375\n",
            "repr, std, cov, clossl, z, norm 0.03369302675127983 0.033538818359375 3.9760189056396484 0.13651765882968903 0.12300509214401245 4.34765625\n",
            "repr, std, cov, clossl, z, norm 0.03372490406036377 0.02655029296875 4.344695568084717 0.1409534364938736 0.07157599925994873 4.2265625\n",
            "repr, std, cov, clossl, z, norm 0.03368530422449112 0.021453857421875 4.429199695587158 0.1291196197271347 0.11414392292499542 3.5234375\n",
            "repr, std, cov, clossl, z, norm 0.03499820455908775 0.00878143310546875 5.176625728607178 0.11977847665548325 0.07294053584337234 4.0078125\n",
            "repr, std, cov, clossl, z, norm 0.03434082120656967 0.01611328125 4.604705333709717 0.12963250279426575 0.0579802505671978 3.296875\n",
            "repr, std, cov, clossl, z, norm 0.033551085740327835 0.01432037353515625 4.685563087463379 0.10656234622001648 0.17706702649593353 3.568359375\n",
            "repr, std, cov, clossl, z, norm 0.032732825726270676 0.0208587646484375 4.375182628631592 0.10150524228811264 0.09483735263347626 5.98828125\n",
            "repr, std, cov, clossl, z, norm 0.03381854668259621 0.01139068603515625 4.816941738128662 0.11333513259887695 0.10031922161579132 5.82421875\n",
            "11\n",
            "repr, std, cov, clossl, z, norm 0.034528493881225586 0.01218414306640625 4.768444061279297 0.12490831315517426 0.1763017326593399 4.109375\n",
            "repr, std, cov, clossl, z, norm 0.035069651901721954 0.01401519775390625 4.6532883644104 0.12670420110225677 0.022347429767251015 3.97265625\n",
            "repr, std, cov, clossl, z, norm 0.031521204859018326 0.02362060546875 4.224895000457764 0.09217621386051178 0.10212855786085129 3.0546875\n",
            "repr, std, cov, clossl, z, norm 0.03281789645552635 0.0170745849609375 4.627362251281738 0.12403668463230133 0.1411050409078598 3.603515625\n",
            "repr, std, cov, clossl, z, norm 0.03397243469953537 0.01438140869140625 4.5779924392700195 0.10022126883268356 0.19565501809120178 2.7421875\n",
            "repr, std, cov, clossl, z, norm 0.03469754382967949 0.004985809326171875 5.194927215576172 0.11404681205749512 0.12839753925800323 3.48828125\n",
            "repr, std, cov, clossl, z, norm 0.03382468223571777 0.01267242431640625 4.540083408355713 0.09714366495609283 0.13027377426624298 4.640625\n",
            "repr, std, cov, clossl, z, norm 0.0322781465947628 0.0185699462890625 4.451376914978027 0.14459466934204102 0.07828524708747864 4.078125\n",
            "repr, std, cov, clossl, z, norm 0.03383893892168999 0.0258941650390625 4.199899673461914 0.1256086826324463 0.08575180172920227 5.34765625\n",
            "repr, std, cov, clossl, z, norm 0.0333709791302681 0.0159912109375 4.506621360778809 0.11697164177894592 0.08566464483737946 3.44140625\n",
            "repr, std, cov, clossl, z, norm 0.0324542410671711 0.0183258056640625 4.403727054595947 0.09077243506908417 0.011462180875241756 3.0\n",
            "repr, std, cov, clossl, z, norm 0.03363656997680664 0.0146026611328125 4.564979076385498 0.138199120759964 0.07662474364042282 4.1484375\n",
            "repr, std, cov, clossl, z, norm 0.03502221405506134 0.005512237548828125 5.148810386657715 0.14456067979335785 0.11740374565124512 3.361328125\n",
            "repr, std, cov, clossl, z, norm 0.038325805217027664 0.0034236907958984375 5.292295455932617 0.17238667607307434 0.04959969222545624 4.32421875\n",
            "repr, std, cov, clossl, z, norm 0.03545525297522545 0.025604248046875 4.03510046005249 0.13823889195919037 0.15418805181980133 5.14453125\n",
            "repr, std, cov, clossl, z, norm 0.031869616359472275 0.032562255859375 3.9111688137054443 0.09272977709770203 0.14102159440517426 3.72265625\n",
            "12\n",
            "repr, std, cov, clossl, z, norm 0.0331694632768631 0.0333251953125 4.12790060043335 0.13276396691799164 0.3305128812789917 3.78125\n",
            "repr, std, cov, clossl, z, norm 0.031841523945331573 0.02874755859375 4.074831962585449 0.13393044471740723 0.13668157160282135 7.140625\n",
            "repr, std, cov, clossl, z, norm 0.032859161496162415 0.02740478515625 3.9777798652648926 0.1499931663274765 0.09905000030994415 6.08984375\n",
            "repr, std, cov, clossl, z, norm 0.03602045401930809 0.00839996337890625 4.836042404174805 0.12803931534290314 0.04194200783967972 4.53515625\n",
            "repr, std, cov, clossl, z, norm 0.03890091925859451 0.0036602020263671875 5.366246223449707 0.14212991297245026 0.16489794850349426 9.359375\n",
            "repr, std, cov, clossl, z, norm 0.03462202847003937 0.01617431640625 4.572103023529053 0.12718868255615234 0.08165793120861053 4.34375\n",
            "repr, std, cov, clossl, z, norm 0.03267155587673187 0.018280029296875 4.467411041259766 0.0770273357629776 0.1358911544084549 6.78125\n",
            "repr, std, cov, clossl, z, norm 0.03425309434533119 0.0235595703125 4.093575477600098 0.17440570890903473 0.06187910586595535 3.23046875\n",
            "repr, std, cov, clossl, z, norm 0.03391822427511215 0.0208587646484375 4.2318925857543945 0.12284654378890991 0.20231902599334717 3.4921875\n",
            "repr, std, cov, clossl, z, norm 0.035652901977300644 0.0128936767578125 4.536332130432129 0.10735528916120529 0.052868276834487915 3.458984375\n",
            "repr, std, cov, clossl, z, norm 0.03255530819296837 0.019775390625 4.2158026695251465 0.07843036204576492 0.09330114722251892 3.123046875\n",
            "repr, std, cov, clossl, z, norm 0.03625763952732086 0.01166534423828125 4.598221778869629 0.11301934719085693 0.03201957046985626 3.806640625\n",
            "repr, std, cov, clossl, z, norm 0.03521545231342316 0.0070648193359375 4.750130653381348 0.12232158333063126 0.27300822734832764 3.669921875\n",
            "repr, std, cov, clossl, z, norm 0.03784376010298729 0.00604248046875 4.864377021789551 0.11216887831687927 0.07670192420482635 4.84765625\n",
            "repr, std, cov, clossl, z, norm 0.035287436097860336 0.018951416015625 4.164676666259766 0.1557745635509491 0.09692501276731491 4.453125\n",
            "repr, std, cov, clossl, z, norm 0.03510357812047005 0.016326904296875 4.289272308349609 0.10716037452220917 0.08784684538841248 3.17578125\n",
            "13\n",
            "repr, std, cov, clossl, z, norm 0.0364869199693203 0.012786865234375 4.53346061706543 0.12164678424596786 0.11236156523227692 5.6015625\n",
            "repr, std, cov, clossl, z, norm 0.03531664237380028 0.011810302734375 4.4367523193359375 0.10233968496322632 0.043353911489248276 3.88671875\n",
            "repr, std, cov, clossl, z, norm 0.03455910086631775 0.014892578125 4.423774719238281 0.11637719720602036 0.07345952838659286 5.33203125\n",
            "repr, std, cov, clossl, z, norm 0.034713540226221085 0.0183563232421875 4.178886890411377 0.1623038798570633 0.02773471362888813 4.8984375\n",
            "repr, std, cov, clossl, z, norm 0.032458286732435226 0.016693115234375 4.245892524719238 0.08008719980716705 0.12861186265945435 4.25\n",
            "repr, std, cov, clossl, z, norm 0.03410999849438667 0.012298583984375 4.439725399017334 0.11689620465040207 0.08662811666727066 4.84375\n",
            "repr, std, cov, clossl, z, norm 0.036275576800107956 0.004253387451171875 5.043898105621338 0.10855798423290253 0.06102047860622406 3.33984375\n",
            "repr, std, cov, clossl, z, norm 0.036917127668857574 0.01003265380859375 4.52115535736084 0.14386358857154846 0.10789888352155685 4.47265625\n",
            "repr, std, cov, clossl, z, norm 0.03690209612250328 0.0128631591796875 4.3027119636535645 0.13835009932518005 0.18852968513965607 4.375\n",
            "repr, std, cov, clossl, z, norm 0.035527076572179794 0.02008056640625 4.024205207824707 0.1183285191655159 0.19778041541576385 3.158203125\n",
            "repr, std, cov, clossl, z, norm 0.03321965038776398 0.0217437744140625 3.9334187507629395 0.09531300514936447 0.08594640344381332 4.046875\n",
            "repr, std, cov, clossl, z, norm 0.03501192107796669 0.0173492431640625 4.171387195587158 0.16918079555034637 0.21221011877059937 4.38671875\n",
            "repr, std, cov, clossl, z, norm 0.032691169530153275 0.01045989990234375 4.496577262878418 0.0933637022972107 0.1964225322008133 4.453125\n",
            "repr, std, cov, clossl, z, norm 0.0360177718102932 0.0048370361328125 4.856568336486816 0.16130515933036804 0.19689878821372986 4.53125\n",
            "repr, std, cov, clossl, z, norm 0.03437627851963043 0.0133819580078125 4.229986190795898 0.13062700629234314 0.06904095411300659 4.5625\n",
            "repr, std, cov, clossl, z, norm 0.03663637861609459 0.0051727294921875 5.072742462158203 0.13565334677696228 0.04936542361974716 5.46484375\n",
            "14\n",
            "repr, std, cov, clossl, z, norm 0.03478594869375229 0.0160675048828125 4.136730194091797 0.13801032304763794 0.1002802848815918 3.86328125\n",
            "repr, std, cov, clossl, z, norm 0.032216429710388184 0.0279693603515625 3.618115186691284 0.08696750551462173 0.06547671556472778 3.818359375\n",
            "repr, std, cov, clossl, z, norm 0.0330590195953846 0.0245361328125 3.834019660949707 0.10515400022268295 0.08992841094732285 7.16015625\n",
            "repr, std, cov, clossl, z, norm 0.033841926604509354 0.01219940185546875 4.532214641571045 0.09230058640241623 0.17913617193698883 3.78125\n",
            "repr, std, cov, clossl, z, norm 0.03341296687722206 0.01363372802734375 4.346956729888916 0.09215298295021057 0.13929174840450287 3.630859375\n",
            "repr, std, cov, clossl, z, norm 0.0342194065451622 0.0161285400390625 4.210047721862793 0.10990932583808899 0.0594363734126091 3.189453125\n",
            "repr, std, cov, clossl, z, norm 0.033079467713832855 0.0139617919921875 4.252501010894775 0.11085501313209534 0.34434637427330017 4.65625\n",
            "repr, std, cov, clossl, z, norm 0.03579417243599892 0.005161285400390625 4.9448723793029785 0.10567832738161087 0.0063416119664907455 3.41796875\n",
            "repr, std, cov, clossl, z, norm 0.03670550882816315 0.007656097412109375 4.50752067565918 0.1286962330341339 0.040343306958675385 3.29296875\n",
            "repr, std, cov, clossl, z, norm 0.034594859927892685 0.0103607177734375 4.522097110748291 0.1210874542593956 0.06299520283937454 3.55859375\n",
            "repr, std, cov, clossl, z, norm 0.034137967973947525 0.016937255859375 4.034986972808838 0.09024913609027863 0.09234223514795303 3.306640625\n",
            "repr, std, cov, clossl, z, norm 0.03455144912004471 0.019287109375 4.029353618621826 0.11308020353317261 0.14210252463817596 4.2421875\n",
            "repr, std, cov, clossl, z, norm 0.0339188314974308 0.025421142578125 3.813068151473999 0.12241484224796295 0.06989306956529617 4.6484375\n",
            "repr, std, cov, clossl, z, norm 0.03357010334730148 0.0151824951171875 4.316671848297119 0.11114867776632309 0.14409436285495758 3.80859375\n",
            "repr, std, cov, clossl, z, norm 0.03478573262691498 0.01300048828125 4.18066930770874 0.11259869486093521 0.10155565291643143 4.19921875\n",
            "repr, std, cov, clossl, z, norm 0.03657540678977966 0.005096435546875 4.588461875915527 0.11481659859418869 0.10952373594045639 3.9453125\n",
            "15\n",
            "repr, std, cov, clossl, z, norm 0.03399738296866417 0.0110015869140625 4.309728622436523 0.07753525674343109 0.016368605196475983 4.078125\n",
            "repr, std, cov, clossl, z, norm 0.03667951375246048 0.0100250244140625 4.283997535705566 0.12077200412750244 0.07666558772325516 4.25\n",
            "repr, std, cov, clossl, z, norm 0.03659142553806305 0.0123443603515625 4.07504940032959 0.11492209881544113 0.08078619092702866 3.560546875\n",
            "repr, std, cov, clossl, z, norm 0.03302229195833206 0.01390838623046875 4.163597583770752 0.1074785366654396 0.09303279221057892 4.296875\n",
            "repr, std, cov, clossl, z, norm 0.033918917179107666 0.01485443115234375 4.201946258544922 0.1362694948911667 0.06419970840215683 2.1953125\n",
            "repr, std, cov, clossl, z, norm 0.03241988271474838 0.0170745849609375 3.981372356414795 0.10294512659311295 0.09851942956447601 3.62109375\n",
            "repr, std, cov, clossl, z, norm 0.03311476483941078 0.01422882080078125 4.193410396575928 0.09139477461576462 0.08920028805732727 6.7734375\n",
            "repr, std, cov, clossl, z, norm 0.03537331894040108 0.00766754150390625 4.3588948249816895 0.09222527593374252 0.1079329252243042 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.03565385565161705 0.0074462890625 4.342967987060547 0.13833515346050262 0.08916467428207397 3.453125\n",
            "repr, std, cov, clossl, z, norm 0.037309709936380386 0.003021240234375 4.784975528717041 0.13242852687835693 0.056047506630420685 5.14453125\n",
            "repr, std, cov, clossl, z, norm 0.036988046020269394 0.0068817138671875 4.454538345336914 0.11971832811832428 0.04316505044698715 4.69140625\n",
            "repr, std, cov, clossl, z, norm 0.034445296972990036 0.0177154541015625 3.955639362335205 0.11164291948080063 0.14929622411727905 4.39453125\n",
            "repr, std, cov, clossl, z, norm 0.036025241017341614 0.027130126953125 3.5479626655578613 0.1407661885023117 0.029270699247717857 3.333984375\n",
            "repr, std, cov, clossl, z, norm 0.032700248062610626 0.032470703125 3.4658708572387695 0.10646991431713104 0.06746531277894974 3.572265625\n",
            "repr, std, cov, clossl, z, norm 0.03423115983605385 0.0195770263671875 3.9630415439605713 0.10160272568464279 0.06886623054742813 3.94140625\n",
            "repr, std, cov, clossl, z, norm 0.033752210438251495 0.0152130126953125 4.043261528015137 0.094915010035038 0.132049098610878 4.08203125\n",
            "16\n",
            "repr, std, cov, clossl, z, norm 0.03436805307865143 0.0113983154296875 4.159605503082275 0.10949340462684631 0.11023958027362823 4.19140625\n",
            "repr, std, cov, clossl, z, norm 0.035939451307058334 0.004474639892578125 4.5319108963012695 0.10371638834476471 0.18666990101337433 4.3125\n",
            "repr, std, cov, clossl, z, norm 0.03646993264555931 0.0036754608154296875 4.66837215423584 0.09773615002632141 0.025439435616135597 3.34765625\n",
            "repr, std, cov, clossl, z, norm 0.0360381081700325 0.005645751953125 4.411706924438477 0.11433722823858261 0.06252805888652802 5.171875\n",
            "repr, std, cov, clossl, z, norm 0.03377792611718178 0.0204315185546875 3.650785446166992 0.10698803514242172 0.09757470339536667 3.685546875\n",
            "repr, std, cov, clossl, z, norm 0.032775189727544785 0.027679443359375 3.5643820762634277 0.11127255856990814 0.06348134577274323 3.771484375\n",
            "repr, std, cov, clossl, z, norm 0.033359307795763016 0.0218353271484375 3.8580780029296875 0.120426706969738 0.05859600752592087 4.296875\n",
            "repr, std, cov, clossl, z, norm 0.03345847874879837 0.017242431640625 3.9096860885620117 0.08526016026735306 0.08920621126890182 4.6796875\n",
            "repr, std, cov, clossl, z, norm 0.03524577617645264 0.006549835205078125 4.339107990264893 0.11184762418270111 0.08937927335500717 3.498046875\n",
            "repr, std, cov, clossl, z, norm 0.03611316904425621 0.00479888916015625 4.505875110626221 0.11787649244070053 0.19088587164878845 3.142578125\n",
            "repr, std, cov, clossl, z, norm 0.03591228649020195 0.005645751953125 4.395074367523193 0.08807165175676346 0.28123390674591064 5.37890625\n",
            "repr, std, cov, clossl, z, norm 0.03532376140356064 0.0145263671875 3.8944830894470215 0.08875228464603424 0.06749904900789261 4.80078125\n",
            "repr, std, cov, clossl, z, norm 0.03343109041452408 0.014678955078125 3.9463486671447754 0.08723446726799011 0.2107900232076645 4.046875\n",
            "repr, std, cov, clossl, z, norm 0.0341515950858593 0.01320648193359375 3.920583486557007 0.08077216893434525 0.124381422996521 3.712890625\n",
            "repr, std, cov, clossl, z, norm 0.0348249226808548 0.0140533447265625 3.9024674892425537 0.1606682986021042 0.10406170785427094 4.3046875\n",
            "repr, std, cov, clossl, z, norm 0.03379245474934578 0.0088653564453125 4.091666221618652 0.10993753373622894 0.12258505076169968 4.46875\n",
            "17\n",
            "repr, std, cov, clossl, z, norm 0.037258729338645935 0.006099700927734375 4.289118766784668 0.11906066536903381 0.05087011680006981 4.40625\n",
            "repr, std, cov, clossl, z, norm 0.03508773818612099 0.01068115234375 4.01630973815918 0.08579373359680176 0.016318291425704956 4.21484375\n",
            "repr, std, cov, clossl, z, norm 0.0347575768828392 0.014190673828125 3.982835292816162 0.11537700146436691 0.06415672600269318 3.90234375\n",
            "repr, std, cov, clossl, z, norm 0.03232654929161072 0.01541900634765625 3.9272096157073975 0.062082357704639435 0.0671875849366188 4.609375\n",
            "repr, std, cov, clossl, z, norm 0.03419864922761917 0.0108642578125 4.124714374542236 0.08908405900001526 0.0027329365257173777 3.984375\n",
            "repr, std, cov, clossl, z, norm 0.03459208086133003 0.00870513916015625 4.1016340255737305 0.08820997923612595 0.08863222599029541 4.5859375\n",
            "repr, std, cov, clossl, z, norm 0.03458424285054207 0.007598876953125 4.162631034851074 0.1287856101989746 0.050653573125600815 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.033910613507032394 0.00653839111328125 4.1602091789245605 0.12250088155269623 0.11388960480690002 4.17578125\n",
            "repr, std, cov, clossl, z, norm 0.0351082943379879 0.0064697265625 4.170210838317871 0.15908180177211761 0.08664948493242264 4.08984375\n",
            "repr, std, cov, clossl, z, norm 0.034410975873470306 0.0125885009765625 3.9106526374816895 0.09385506063699722 0.04683621600270271 3.48046875\n",
            "repr, std, cov, clossl, z, norm 0.035010043531656265 0.0129547119140625 3.826892375946045 0.1299750655889511 0.08474896103143692 4.01953125\n",
            "repr, std, cov, clossl, z, norm 0.03517354279756546 0.01141357421875 3.9044275283813477 0.12322020530700684 0.08942946791648865 3.380859375\n",
            "repr, std, cov, clossl, z, norm 0.036659713834524155 0.00881195068359375 4.178321361541748 0.11515727639198303 0.030381474643945694 4.1875\n",
            "repr, std, cov, clossl, z, norm 0.03515176475048065 0.00844573974609375 3.980545997619629 0.10124357044696808 0.07573592662811279 3.91015625\n",
            "repr, std, cov, clossl, z, norm 0.03512734919786453 0.0069732666015625 4.110075950622559 0.09806732088327408 0.01595674455165863 3.990234375\n",
            "repr, std, cov, clossl, z, norm 0.035062722861766815 0.00589752197265625 4.337734222412109 0.10727878659963608 0.1961473673582077 3.900390625\n",
            "18\n",
            "repr, std, cov, clossl, z, norm 0.03421365097165108 0.01465606689453125 3.7574102878570557 0.10175120085477829 0.1632828563451767 3.03125\n",
            "repr, std, cov, clossl, z, norm 0.033331774175167084 0.0158538818359375 3.835184097290039 0.0839601680636406 0.04630685970187187 4.2265625\n",
            "repr, std, cov, clossl, z, norm 0.0339667946100235 0.021240234375 3.663884162902832 0.11678066104650497 0.07010411471128464 4.9296875\n",
            "repr, std, cov, clossl, z, norm 0.03573250398039818 0.00862884521484375 4.146081924438477 0.13832388818264008 0.275333970785141 3.935546875\n",
            "repr, std, cov, clossl, z, norm 0.03630834072828293 0.00424957275390625 4.364783763885498 0.12959185242652893 0.08785634487867355 3.572265625\n",
            "repr, std, cov, clossl, z, norm 0.03634136542677879 0.005096435546875 4.337561130523682 0.10834816098213196 0.09359950572252274 3.43359375\n",
            "repr, std, cov, clossl, z, norm 0.03372739255428314 0.015594482421875 3.751375436782837 0.062435731291770935 0.1011420339345932 3.189453125\n",
            "repr, std, cov, clossl, z, norm 0.03350988030433655 0.0193328857421875 3.5952491760253906 0.11166732758283615 0.11762980371713638 3.8671875\n",
            "repr, std, cov, clossl, z, norm 0.032510507851839066 0.0143890380859375 3.868345260620117 0.09309499710798264 0.08934535831212997 4.234375\n",
            "repr, std, cov, clossl, z, norm 0.03423815593123436 0.006732940673828125 4.322236061096191 0.1247970387339592 0.007754489779472351 3.826171875\n",
            "repr, std, cov, clossl, z, norm 0.034134361892938614 0.005893707275390625 4.274903297424316 0.11184202134609222 0.13147060573101044 4.24609375\n",
            "repr, std, cov, clossl, z, norm 0.035343196243047714 0.008209228515625 4.054478645324707 0.12175817042589188 0.24367594718933105 3.916015625\n",
            "repr, std, cov, clossl, z, norm 0.03575458750128746 0.01491546630859375 3.765972852706909 0.14714695513248444 0.0979703813791275 3.99609375\n",
            "repr, std, cov, clossl, z, norm 0.032811205834150314 0.0150604248046875 3.829188823699951 0.0750705674290657 0.23069775104522705 3.060546875\n",
            "repr, std, cov, clossl, z, norm 0.03508654981851578 0.01311492919921875 3.8189218044281006 0.12288036942481995 0.16179358959197998 3.4453125\n",
            "repr, std, cov, clossl, z, norm 0.03458921238780022 0.005672454833984375 4.146210193634033 0.09522885084152222 0.09978087246417999 7.6328125\n",
            "19\n",
            "repr, std, cov, clossl, z, norm 0.03365596756339073 0.01097869873046875 3.902067184448242 0.09576191753149033 0.11707565933465958 3.541015625\n",
            "repr, std, cov, clossl, z, norm 0.03406762704253197 0.00699615478515625 4.099435329437256 0.10396848618984222 0.08043906837701797 4.19921875\n",
            "repr, std, cov, clossl, z, norm 0.03535966947674751 0.005588531494140625 4.1097893714904785 0.11401241272687912 0.19281426072120667 3.826171875\n",
            "repr, std, cov, clossl, z, norm 0.035289157181978226 0.0093231201171875 3.8776512145996094 0.09661045670509338 0.17165735363960266 5.33984375\n",
            "repr, std, cov, clossl, z, norm 0.032868001610040665 0.0171661376953125 3.8275105953216553 0.13852474093437195 0.06535636633634567 3.93359375\n",
            "repr, std, cov, clossl, z, norm 0.033792927861213684 0.010345458984375 4.030887603759766 0.10987631231546402 0.12498528510332108 3.982421875\n",
            "repr, std, cov, clossl, z, norm 0.0325758196413517 0.0176544189453125 3.5530858039855957 0.08124484121799469 0.0028568487614393234 3.625\n",
            "repr, std, cov, clossl, z, norm 0.03337670862674713 0.00989532470703125 3.8735296726226807 0.09522441029548645 0.0470300018787384 3.69921875\n",
            "repr, std, cov, clossl, z, norm 0.03261468932032585 0.00992584228515625 3.801891326904297 0.08486581593751907 0.03932158276438713 3.56640625\n",
            "repr, std, cov, clossl, z, norm 0.03496522456407547 0.0021514892578125 4.426591873168945 0.10189475864171982 0.03829732909798622 4.4296875\n",
            "repr, std, cov, clossl, z, norm 0.03615966811776161 0.00711822509765625 4.092066287994385 0.12035916745662689 0.09074728935956955 3.5390625\n",
            "repr, std, cov, clossl, z, norm 0.03139498457312584 0.016937255859375 3.5898804664611816 0.047777704894542694 0.22998405992984772 4.07421875\n",
            "repr, std, cov, clossl, z, norm 0.03167561814188957 0.0135955810546875 3.8326172828674316 0.09386441856622696 0.08838807791471481 3.9140625\n",
            "repr, std, cov, clossl, z, norm 0.03335709497332573 0.0128326416015625 3.7993597984313965 0.09912225604057312 0.08846427500247955 7.55078125\n",
            "repr, std, cov, clossl, z, norm 0.03370049595832825 0.00965118408203125 3.754514217376709 0.13775299489498138 0.101876400411129 4.7578125\n",
            "repr, std, cov, clossl, z, norm 0.032363783568143845 0.010284423828125 3.8509984016418457 0.08973658084869385 0.1520768702030182 3.05078125\n",
            "20\n",
            "repr, std, cov, clossl, z, norm 0.035272762179374695 0.004261016845703125 4.364232540130615 0.0871640220284462 0.22429539263248444 3.712890625\n",
            "repr, std, cov, clossl, z, norm 0.03542598336935043 0.007457733154296875 4.012497425079346 0.0993688553571701 0.10758820921182632 4.0625\n",
            "repr, std, cov, clossl, z, norm 0.03279376029968262 0.00919342041015625 3.8070645332336426 0.07015573233366013 0.08534963428974152 3.40625\n",
            "repr, std, cov, clossl, z, norm 0.03508154675364494 0.005657196044921875 4.08798885345459 0.114242322742939 0.09533684700727463 3.96484375\n",
            "repr, std, cov, clossl, z, norm 0.03143293783068657 0.017425537109375 3.599317789077759 0.08969509601593018 0.07705715298652649 3.7421875\n",
            "repr, std, cov, clossl, z, norm 0.032083094120025635 0.0140533447265625 3.567103862762451 0.07823126018047333 0.038727421313524246 4.28515625\n",
            "repr, std, cov, clossl, z, norm 0.03226384148001671 0.016632080078125 3.566915988922119 0.08089428395032883 0.1378909945487976 4.72265625\n",
            "repr, std, cov, clossl, z, norm 0.03331061452627182 0.00788116455078125 3.8562803268432617 0.09852662682533264 0.07190845906734467 3.490234375\n",
            "repr, std, cov, clossl, z, norm 0.034246526658535004 0.006237030029296875 3.9895639419555664 0.09334330260753632 0.08171959221363068 4.1796875\n",
            "repr, std, cov, clossl, z, norm 0.03635158762335777 0.00353240966796875 4.140894889831543 0.1145387515425682 0.06657934188842773 5.01953125\n",
            "repr, std, cov, clossl, z, norm 0.03506498411297798 0.00506591796875 3.9946048259735107 0.12837029993534088 0.11339395493268967 3.427734375\n",
            "repr, std, cov, clossl, z, norm 0.03338519111275673 0.01319122314453125 3.6040079593658447 0.07949495315551758 0.03851093351840973 4.42578125\n",
            "repr, std, cov, clossl, z, norm 0.03193395584821701 0.015350341796875 3.581113815307617 0.07306969165802002 0.16519926488399506 3.171875\n",
            "repr, std, cov, clossl, z, norm 0.03371681645512581 0.01242828369140625 3.720456123352051 0.0984044224023819 0.0542796365916729 4.5703125\n",
            "repr, std, cov, clossl, z, norm 0.03296868875622749 0.0195159912109375 3.3433547019958496 0.10214449465274811 0.02631511352956295 5.359375\n",
            "repr, std, cov, clossl, z, norm 0.03309784084558487 0.007656097412109375 3.855401039123535 0.07989964634180069 0.14728273451328278 4.0234375\n",
            "21\n",
            "repr, std, cov, clossl, z, norm 0.035647712647914886 0.00390625 4.255654335021973 0.11985442787408829 0.05575079098343849 4.7265625\n",
            "repr, std, cov, clossl, z, norm 0.03591274842619896 0.005340576171875 3.8814148902893066 0.12013313174247742 0.15809714794158936 4.96484375\n",
            "repr, std, cov, clossl, z, norm 0.03347279131412506 0.007293701171875 3.7364532947540283 0.09444298595190048 0.1308020055294037 4.671875\n",
            "repr, std, cov, clossl, z, norm 0.03379373624920845 0.009002685546875 3.672170639038086 0.08892575651407242 0.20799937844276428 4.8125\n",
            "repr, std, cov, clossl, z, norm 0.03297945111989975 0.0128173828125 3.5832407474517822 0.071677066385746 0.12451589852571487 3.724609375\n",
            "repr, std, cov, clossl, z, norm 0.0330493301153183 0.01239013671875 3.5269036293029785 0.10256442427635193 0.14876213669776917 3.822265625\n",
            "repr, std, cov, clossl, z, norm 0.0336184985935688 0.0067901611328125 3.840395450592041 0.10732490569353104 0.19867227971553802 5.02734375\n",
            "repr, std, cov, clossl, z, norm 0.032978419214487076 0.009979248046875 3.5931806564331055 0.07806964963674545 0.127401664853096 4.12109375\n",
            "repr, std, cov, clossl, z, norm 0.03519832715392113 0.007091522216796875 3.763784646987915 0.1061302125453949 0.09620808809995651 3.451171875\n",
            "repr, std, cov, clossl, z, norm 0.03327896445989609 0.00868988037109375 3.60445237159729 0.08050946891307831 0.06624793261289597 4.171875\n",
            "repr, std, cov, clossl, z, norm 0.034927092492580414 0.00464630126953125 3.929872989654541 0.08806303143501282 0.10828927159309387 4.65234375\n",
            "repr, std, cov, clossl, z, norm 0.03333713486790657 0.00469970703125 3.8278980255126953 0.0694357231259346 0.23219162225723267 3.841796875\n",
            "repr, std, cov, clossl, z, norm 0.034514959901571274 0.005340576171875 3.801570177078247 0.09183946251869202 0.05335627868771553 4.48046875\n",
            "repr, std, cov, clossl, z, norm 0.03237680345773697 0.016815185546875 3.3652377128601074 0.09080057591199875 0.14373207092285156 5.76953125\n",
            "repr, std, cov, clossl, z, norm 0.03119305521249771 0.0199127197265625 3.3293204307556152 0.08077522367238998 0.14708511531352997 3.7421875\n",
            "repr, std, cov, clossl, z, norm 0.03410862386226654 0.012176513671875 3.5965704917907715 0.12774398922920227 0.07224296778440475 4.08984375\n",
            "22\n",
            "repr, std, cov, clossl, z, norm 0.03429084271192551 0.00437164306640625 3.816850423812866 0.09079060703516006 0.03917773440480232 6.84765625\n",
            "repr, std, cov, clossl, z, norm 0.035598717629909515 0.0029277801513671875 3.977964162826538 0.08558770269155502 0.09483570605516434 5.05078125\n",
            "repr, std, cov, clossl, z, norm 0.03478986769914627 0.00403594970703125 3.9238152503967285 0.10705844312906265 0.17339836061000824 3.5546875\n",
            "repr, std, cov, clossl, z, norm 0.034233208745718 0.010284423828125 3.474116563796997 0.09696231782436371 0.11626584827899933 4.0078125\n",
            "repr, std, cov, clossl, z, norm 0.03351322188973427 0.01447296142578125 3.409087657928467 0.1069895327091217 0.10712256282567978 3.07421875\n",
            "repr, std, cov, clossl, z, norm 0.03162231668829918 0.0186767578125 3.327230453491211 0.09040012210607529 0.08058962225914001 2.119140625\n",
            "repr, std, cov, clossl, z, norm 0.03285558521747589 0.007404327392578125 3.8081214427948 0.08644765615463257 0.07385387271642685 3.921875\n",
            "repr, std, cov, clossl, z, norm 0.036237653344869614 0.0052337646484375 3.767547130584717 0.16217738389968872 0.15610837936401367 5.5078125\n",
            "repr, std, cov, clossl, z, norm 0.03360791876912117 0.005672454833984375 3.686431407928467 0.09854887425899506 0.16843028366565704 5.31640625\n",
            "repr, std, cov, clossl, z, norm 0.0333438441157341 0.0069427490234375 3.6691722869873047 0.08340038359165192 0.022457368671894073 5.265625\n",
            "repr, std, cov, clossl, z, norm 0.0341203399002552 0.007049560546875 3.604570150375366 0.11215528100728989 0.19959409534931183 4.83203125\n",
            "repr, std, cov, clossl, z, norm 0.03404782712459564 0.0098724365234375 3.4302759170532227 0.12823234498500824 0.1699942797422409 4.0546875\n",
            "repr, std, cov, clossl, z, norm 0.03333529457449913 0.0113677978515625 3.4282121658325195 0.170037642121315 0.0631309524178505 4.828125\n",
            "repr, std, cov, clossl, z, norm 0.0342763215303421 0.00499725341796875 3.724689483642578 0.10899320244789124 0.09799600392580032 4.8046875\n",
            "repr, std, cov, clossl, z, norm 0.03489416837692261 0.0029506683349609375 3.8944263458251953 0.10402635484933853 0.18101975321769714 5.015625\n",
            "repr, std, cov, clossl, z, norm 0.03410786762833595 0.00441741943359375 3.7598423957824707 0.09123595803976059 0.09368038922548294 3.68359375\n",
            "23\n",
            "repr, std, cov, clossl, z, norm 0.033996786922216415 0.01312255859375 3.4009368419647217 0.09410940855741501 0.1433083564043045 4.20703125\n",
            "repr, std, cov, clossl, z, norm 0.033506423234939575 0.0154876708984375 3.356358528137207 0.09626058489084244 0.06328047811985016 3.73046875\n",
            "repr, std, cov, clossl, z, norm 0.03301922231912613 0.01190185546875 3.574404239654541 0.10343364626169205 0.13038885593414307 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.03395841270685196 0.0068817138671875 3.732060432434082 0.11609777063131332 0.07103823125362396 7.70703125\n",
            "repr, std, cov, clossl, z, norm 0.03355695307254791 0.00690460205078125 3.695995330810547 0.14307773113250732 0.09778418391942978 4.37109375\n",
            "repr, std, cov, clossl, z, norm 0.03244999051094055 0.00681304931640625 3.77016544342041 0.10080067068338394 0.1339048147201538 3.521484375\n",
            "repr, std, cov, clossl, z, norm 0.033074844628572464 0.0068359375 3.688563346862793 0.08251268416643143 0.1272936463356018 2.7890625\n",
            "repr, std, cov, clossl, z, norm 0.03386029973626137 0.009124755859375 3.5546655654907227 0.09872280806303024 0.008030976168811321 4.3671875\n",
            "repr, std, cov, clossl, z, norm 0.035692594945430756 0.004573822021484375 3.953420400619507 0.23298560082912445 0.19765564799308777 4.515625\n",
            "repr, std, cov, clossl, z, norm 0.03429047390818596 0.007656097412109375 3.7734899520874023 0.088685043156147 0.05146363750100136 4.01953125\n",
            "repr, std, cov, clossl, z, norm 0.03516630083322525 0.01399993896484375 3.296152353286743 0.10653655976057053 0.14314940571784973 3.748046875\n",
            "repr, std, cov, clossl, z, norm 0.03212174028158188 0.0199737548828125 3.17703914642334 0.08192672580480576 0.036726970225572586 5.421875\n",
            "repr, std, cov, clossl, z, norm 0.033250100910663605 0.00887298583984375 3.63340425491333 0.10472962260246277 0.08651857078075409 2.75390625\n",
            "repr, std, cov, clossl, z, norm 0.0342986136674881 0.0029582977294921875 4.103816032409668 0.10969078540802002 0.1173727959394455 3.810546875\n",
            "repr, std, cov, clossl, z, norm 0.03271341323852539 0.007289886474609375 3.7770462036132812 0.07528875768184662 0.0702410489320755 3.203125\n",
            "repr, std, cov, clossl, z, norm 0.03336789831519127 0.0115203857421875 3.330353260040283 0.10394973307847977 0.05184436962008476 3.341796875\n",
            "24\n",
            "repr, std, cov, clossl, z, norm 0.03317882865667343 0.0097808837890625 3.4591829776763916 0.08361564576625824 0.09795480966567993 4.15234375\n",
            "repr, std, cov, clossl, z, norm 0.03330737724900246 0.0079345703125 3.5963001251220703 0.09421619027853012 0.05005823075771332 4.52734375\n",
            "repr, std, cov, clossl, z, norm 0.033633965998888016 0.006359100341796875 3.6334991455078125 0.12459857761859894 0.2592747211456299 3.884765625\n",
            "repr, std, cov, clossl, z, norm 0.03391929715871811 0.006290435791015625 3.690079689025879 0.11827123165130615 0.14565028250217438 7.5078125\n",
            "repr, std, cov, clossl, z, norm 0.03277529031038284 0.00730133056640625 3.591783285140991 0.08288592845201492 0.02357744798064232 4.07421875\n",
            "repr, std, cov, clossl, z, norm 0.03535434231162071 0.007274627685546875 3.5440614223480225 0.12801489233970642 0.14938564598560333 4.2890625\n",
            "repr, std, cov, clossl, z, norm 0.034120310097932816 0.0079803466796875 3.597630500793457 0.15474343299865723 0.07037310302257538 5.64453125\n",
            "repr, std, cov, clossl, z, norm 0.03259638696908951 0.0088348388671875 3.5499391555786133 0.07472551614046097 0.12669971585273743 4.296875\n",
            "repr, std, cov, clossl, z, norm 0.033474765717983246 0.00832366943359375 3.466852903366089 0.11487802118062973 0.06824387609958649 4.5625\n",
            "repr, std, cov, clossl, z, norm 0.03232390806078911 0.00946807861328125 3.345705509185791 0.09801000356674194 0.083546943962574 4.265625\n",
            "repr, std, cov, clossl, z, norm 0.033127229660749435 0.0059051513671875 3.7021284103393555 0.08730079233646393 0.07825566083192825 4.1875\n",
            "repr, std, cov, clossl, z, norm 0.03510209172964096 0.0046234130859375 3.6629478931427 0.13768337666988373 0.09325622767210007 4.75\n",
            "repr, std, cov, clossl, z, norm 0.03332222253084183 0.0071563720703125 3.522768974304199 0.08284452557563782 0.04910558834671974 4.1484375\n",
            "repr, std, cov, clossl, z, norm 0.03356270492076874 0.006103515625 3.5707671642303467 0.09221792966127396 0.04417067766189575 4.4609375\n",
            "repr, std, cov, clossl, z, norm 0.03269549459218979 0.007724761962890625 3.4608278274536133 0.09440521150827408 0.052580926567316055 4.26953125\n",
            "repr, std, cov, clossl, z, norm 0.032218270003795624 0.01421356201171875 3.181431531906128 0.09144354611635208 0.05486387386918068 5.21484375\n",
            "25\n",
            "repr, std, cov, clossl, z, norm 0.03218555822968483 0.00922393798828125 3.3488471508026123 0.13829725980758667 0.2481997311115265 3.51953125\n",
            "repr, std, cov, clossl, z, norm 0.03277746960520744 0.0036296844482421875 3.8171892166137695 0.08074501156806946 0.22962063550949097 3.658203125\n",
            "repr, std, cov, clossl, z, norm 0.03334696590900421 0.001865386962890625 4.020383834838867 0.07889381051063538 0.10757333040237427 2.802734375\n",
            "repr, std, cov, clossl, z, norm 0.03225026652216911 0.0083770751953125 3.374281406402588 0.0832473635673523 0.04158453643321991 4.40625\n",
            "repr, std, cov, clossl, z, norm 0.03260588273406029 0.0132904052734375 3.408459424972534 0.09555605053901672 0.14948444068431854 3.88671875\n",
            "repr, std, cov, clossl, z, norm 0.032244838774204254 0.0188751220703125 3.239865779876709 0.10105089843273163 0.024485338479280472 2.8359375\n",
            "repr, std, cov, clossl, z, norm 0.034664738923311234 0.00890350341796875 3.4925403594970703 0.11705014854669571 0.09157246351242065 3.984375\n",
            "repr, std, cov, clossl, z, norm 0.03508206829428673 0.0082244873046875 3.628876209259033 0.15532954037189484 0.05198070406913757 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.03434087708592415 0.006134033203125 3.5483148097991943 0.0966724157333374 0.21521522104740143 3.197265625\n",
            "repr, std, cov, clossl, z, norm 0.03294777497649193 0.0059051513671875 3.624598979949951 0.09401626139879227 0.05292723700404167 4.46875\n",
            "repr, std, cov, clossl, z, norm 0.03385230526328087 0.002758026123046875 3.83758544921875 0.08386187255382538 0.020130066201090813 5.23046875\n",
            "repr, std, cov, clossl, z, norm 0.03235036879777908 0.007076263427734375 3.511442184448242 0.09408608824014664 0.07275927811861038 3.3125\n",
            "repr, std, cov, clossl, z, norm 0.032787274569272995 0.0120697021484375 3.336848020553589 0.23806867003440857 0.02722819149494171 4.64453125\n",
            "repr, std, cov, clossl, z, norm 0.03333807736635208 0.01212310791015625 3.4122776985168457 0.08714278787374496 0.09752435237169266 4.7109375\n",
            "repr, std, cov, clossl, z, norm 0.033425550907850266 0.00788116455078125 3.4325833320617676 0.08864617347717285 0.09637050330638885 3.03125\n",
            "repr, std, cov, clossl, z, norm 0.03423631936311722 0.003849029541015625 3.704482078552246 0.08862785995006561 0.07235928624868393 3.82421875\n",
            "26\n",
            "repr, std, cov, clossl, z, norm 0.03255047649145126 0.00959014892578125 3.384573459625244 0.08316703885793686 0.10148970037698746 6.0234375\n",
            "repr, std, cov, clossl, z, norm 0.03325684741139412 0.010009765625 3.346384048461914 0.111421599984169 0.017982974648475647 4.1484375\n",
            "repr, std, cov, clossl, z, norm 0.03510609641671181 0.00511932373046875 3.5969223976135254 0.11181601881980896 0.2685944139957428 4.03125\n",
            "repr, std, cov, clossl, z, norm 0.03333034738898277 0.005123138427734375 3.5575881004333496 0.10512998700141907 0.14195887744426727 3.75\n",
            "repr, std, cov, clossl, z, norm 0.03382313251495361 0.00505828857421875 3.557494878768921 0.09695912152528763 0.09090182185173035 4.1953125\n",
            "repr, std, cov, clossl, z, norm 0.032641347497701645 0.01209259033203125 3.163752794265747 0.07994198799133301 0.12461147457361221 3.484375\n",
            "repr, std, cov, clossl, z, norm 0.03242143988609314 0.008514404296875 3.3049607276916504 0.10778141021728516 0.06202146038413048 3.41015625\n",
            "repr, std, cov, clossl, z, norm 0.03351183235645294 0.006107330322265625 3.493915319442749 0.10253147035837173 0.24249283969402313 4.68359375\n",
            "repr, std, cov, clossl, z, norm 0.03439566493034363 0.004123687744140625 3.5269219875335693 0.1356007307767868 0.10023806244134903 4.1953125\n",
            "repr, std, cov, clossl, z, norm 0.03404486924409866 0.00794219970703125 3.3958382606506348 0.11778968572616577 0.003207101719453931 3.884765625\n",
            "repr, std, cov, clossl, z, norm 0.03486407548189163 0.0050506591796875 3.4853334426879883 0.16128569841384888 0.24819427728652954 4.75\n",
            "repr, std, cov, clossl, z, norm 0.03498038277029991 0.0054168701171875 3.52241587638855 0.13335342705249786 0.0865626409649849 4.34765625\n",
            "repr, std, cov, clossl, z, norm 0.03326881304383278 0.01062774658203125 3.25178861618042 0.10702285170555115 0.1386590600013733 4.20703125\n",
            "repr, std, cov, clossl, z, norm 0.034200165420770645 0.00849151611328125 3.237325668334961 0.1332317739725113 0.19121889770030975 4.42578125\n",
            "repr, std, cov, clossl, z, norm 0.034612249583005905 0.007335662841796875 3.3280327320098877 0.14052151143550873 0.05380379781126976 3.990234375\n",
            "repr, std, cov, clossl, z, norm 0.034286003559827805 0.005706787109375 3.368460178375244 0.100624680519104 0.1395939588546753 3.869140625\n",
            "27\n",
            "repr, std, cov, clossl, z, norm 0.03458138555288315 0.005420684814453125 3.412126064300537 0.14247432351112366 0.13279509544372559 5.71484375\n",
            "repr, std, cov, clossl, z, norm 0.035702165216207504 0.003871917724609375 3.4048397541046143 0.13482487201690674 0.14010097086429596 3.796875\n",
            "repr, std, cov, clossl, z, norm 0.03520352393388748 0.00440216064453125 3.429999828338623 0.11593639850616455 0.049395520240068436 3.685546875\n",
            "repr, std, cov, clossl, z, norm 0.0344608835875988 0.0036525726318359375 3.52120304107666 0.10574690252542496 0.010561159811913967 4.4609375\n",
            "repr, std, cov, clossl, z, norm 0.03287094831466675 0.0124053955078125 3.0825185775756836 0.10714501887559891 0.131754532456398 3.099609375\n",
            "repr, std, cov, clossl, z, norm 0.03440587967634201 0.00800323486328125 3.26220965385437 0.13886287808418274 0.10270275175571442 4.890625\n",
            "repr, std, cov, clossl, z, norm 0.032073769718408585 0.0143890380859375 3.142714023590088 0.10151458531618118 0.15905073285102844 4.0546875\n",
            "repr, std, cov, clossl, z, norm 0.03246249631047249 0.010772705078125 3.257758140563965 0.18208186328411102 0.14258985221385956 4.171875\n",
            "repr, std, cov, clossl, z, norm 0.032930921763181686 0.004375457763671875 3.535557508468628 0.08833222091197968 0.05596701428294182 4.64453125\n",
            "repr, std, cov, clossl, z, norm 0.03749435767531395 0.0009503364562988281 3.8962788581848145 0.13463044166564941 0.20278051495552063 6.84375\n",
            "repr, std, cov, clossl, z, norm 0.03458602353930473 0.0033626556396484375 3.5479917526245117 0.0913742408156395 0.09554965794086456 4.5390625\n",
            "repr, std, cov, clossl, z, norm 0.0337989367544651 0.00876617431640625 3.3682641983032227 0.10557307302951813 0.08232572674751282 3.84375\n",
            "repr, std, cov, clossl, z, norm 0.03185619041323662 0.01861572265625 3.044642925262451 0.12951581180095673 0.06427990645170212 2.892578125\n",
            "repr, std, cov, clossl, z, norm 0.03186606243252754 0.021331787109375 2.820995807647705 0.12509620189666748 0.0459207184612751 3.548828125\n",
            "repr, std, cov, clossl, z, norm 0.032522089779376984 0.00882720947265625 3.4637598991394043 0.10731061547994614 0.09946122765541077 3.66015625\n",
            "repr, std, cov, clossl, z, norm 0.03455236181616783 0.0034236907958984375 3.600130558013916 0.12554587423801422 0.2240409106016159 4.2890625\n",
            "28\n",
            "repr, std, cov, clossl, z, norm 0.03594646975398064 0.000942230224609375 3.8938498497009277 0.11103185266256332 0.14475160837173462 5.44140625\n",
            "repr, std, cov, clossl, z, norm 0.03511229157447815 0.0037288665771484375 3.429983139038086 0.11676675826311111 0.11520164459943771 4.21484375\n",
            "repr, std, cov, clossl, z, norm 0.03483791649341583 0.017822265625 3.074277400970459 0.14081165194511414 0.024475794285535812 4.421875\n",
            "repr, std, cov, clossl, z, norm 0.03255527839064598 0.0102386474609375 3.356553554534912 0.07836363464593887 0.19144967198371887 2.974609375\n",
            "repr, std, cov, clossl, z, norm 0.031599100679159164 0.01556396484375 3.1001791954040527 0.08157570660114288 0.0893576592206955 3.435546875\n",
            "repr, std, cov, clossl, z, norm 0.033359672874212265 0.00891876220703125 3.267798900604248 0.09060400724411011 0.13168425858020782 4.2421875\n",
            "repr, std, cov, clossl, z, norm 0.03306378051638603 0.00325775146484375 3.603882312774658 0.14094361662864685 0.16088475286960602 6.23046875\n",
            "repr, std, cov, clossl, z, norm 0.03575333580374718 0.002246856689453125 3.650885581970215 0.15360799431800842 0.19440284371376038 4.5\n",
            "repr, std, cov, clossl, z, norm 0.033356163650751114 0.00354766845703125 3.479487419128418 0.08377662301063538 0.09296930581331253 3.228515625\n",
            "repr, std, cov, clossl, z, norm 0.03284554183483124 0.00701141357421875 3.262948989868164 0.0940089076757431 0.00474489014595747 4.09375\n",
            "repr, std, cov, clossl, z, norm 0.033946502953767776 0.0074920654296875 3.3035802841186523 0.1303626298904419 0.04097318649291992 6.30859375\n",
            "repr, std, cov, clossl, z, norm 0.03232579678297043 0.0124359130859375 3.090695381164551 0.10339759290218353 0.13673312962055206 3.791015625\n",
            "repr, std, cov, clossl, z, norm 0.03163254261016846 0.01349639892578125 3.0195202827453613 0.076200470328331 0.10991764068603516 3.126953125\n",
            "repr, std, cov, clossl, z, norm 0.03405033051967621 0.006877899169921875 3.2079291343688965 0.11099155247211456 0.0007466756505891681 3.9140625\n",
            "repr, std, cov, clossl, z, norm 0.03307646885514259 0.00388336181640625 3.4217519760131836 0.08433070033788681 0.009259123355150223 3.1640625\n",
            "repr, std, cov, clossl, z, norm 0.035089220851659775 0.0018062591552734375 3.6017022132873535 0.10429492592811584 0.08349611610174179 4.39453125\n",
            "29\n",
            "repr, std, cov, clossl, z, norm 0.03375358507037163 0.002788543701171875 3.4467692375183105 0.08775154501199722 0.07531654834747314 4.02734375\n",
            "repr, std, cov, clossl, z, norm 0.03385511785745621 0.006145477294921875 3.2356886863708496 0.10106901824474335 0.1766802817583084 5.0234375\n",
            "repr, std, cov, clossl, z, norm 0.03422496095299721 0.0066680908203125 3.206242561340332 0.09454307705163956 0.01074761338531971 3.66796875\n",
            "repr, std, cov, clossl, z, norm 0.033260803669691086 0.01580810546875 2.914067029953003 0.10816255211830139 0.06991013884544373 3.453125\n",
            "repr, std, cov, clossl, z, norm 0.032887376844882965 0.01119232177734375 3.1239638328552246 0.06804826110601425 0.0831061378121376 3.943359375\n",
            "repr, std, cov, clossl, z, norm 0.03191778063774109 0.01209259033203125 3.120663642883301 0.08246534317731857 0.12619565427303314 4.4921875\n",
            "repr, std, cov, clossl, z, norm 0.034091390669345856 0.0042572021484375 3.4512526988983154 0.13609366118907928 0.05257343500852585 3.708984375\n",
            "repr, std, cov, clossl, z, norm 0.03503274917602539 0.0008487701416015625 3.765536308288574 0.08376096189022064 0.09083828330039978 4.06640625\n",
            "repr, std, cov, clossl, z, norm 0.033296842128038406 0.002841949462890625 3.53406023979187 0.09881124645471573 0.25775277614593506 3.423828125\n",
            "repr, std, cov, clossl, z, norm 0.034539494663476944 0.00537109375 3.261094093322754 0.1402549296617508 0.11539703607559204 4.08203125\n",
            "repr, std, cov, clossl, z, norm 0.03219541907310486 0.0126953125 3.0735666751861572 0.1030706986784935 0.12122370302677155 4.421875\n",
            "repr, std, cov, clossl, z, norm 0.032478660345077515 0.01250457763671875 3.0528006553649902 0.09583072364330292 0.004787623416632414 4.51953125\n",
            "repr, std, cov, clossl, z, norm 0.033815186470746994 0.008056640625 3.1966662406921387 0.12181494385004044 0.20550036430358887 5.94921875\n",
            "repr, std, cov, clossl, z, norm 0.0345626138150692 0.0033664703369140625 3.5050554275512695 0.10545608401298523 0.12013702094554901 8.140625\n",
            "repr, std, cov, clossl, z, norm 0.033141568303108215 0.004917144775390625 3.3676133155822754 0.09695936739444733 0.11627800762653351 4.4609375\n",
            "repr, std, cov, clossl, z, norm 0.03455904871225357 0.006313323974609375 3.2565786838531494 0.1028917133808136 0.1710340976715088 5.33984375\n",
            "30\n",
            "repr, std, cov, clossl, z, norm 0.033046331256628036 0.0099334716796875 3.1442782878875732 0.08119633793830872 0.11289043724536896 4.4609375\n",
            "repr, std, cov, clossl, z, norm 0.03205113485455513 0.007965087890625 3.2626571655273438 0.08492368459701538 0.1264227330684662 2.98046875\n",
            "repr, std, cov, clossl, z, norm 0.03206842765212059 0.00800323486328125 3.2631428241729736 0.09712672233581543 0.13630270957946777 3.583984375\n",
            "repr, std, cov, clossl, z, norm 0.032528411597013474 0.0059967041015625 3.216020107269287 0.10425500571727753 0.0388575978577137 3.513671875\n",
            "repr, std, cov, clossl, z, norm 0.03279215097427368 0.0037097930908203125 3.322202682495117 0.11350793391466141 0.08973880112171173 5.1328125\n",
            "repr, std, cov, clossl, z, norm 0.03387616574764252 0.00272369384765625 3.429622173309326 0.12786269187927246 0.06726852059364319 4.16015625\n",
            "repr, std, cov, clossl, z, norm 0.032906655222177505 0.004817962646484375 3.291510581970215 0.10209177434444427 0.2543978989124298 3.58203125\n",
            "repr, std, cov, clossl, z, norm 0.033199187368154526 0.007450103759765625 3.1249451637268066 0.1136583462357521 0.0693177878856659 3.93359375\n",
            "repr, std, cov, clossl, z, norm 0.03167092800140381 0.0135040283203125 2.8900907039642334 0.08392361551523209 0.12115176767110825 4.40625\n",
            "repr, std, cov, clossl, z, norm 0.03392946347594261 0.00511932373046875 3.2567038536071777 0.10506633669137955 0.15093208849430084 4.18359375\n",
            "repr, std, cov, clossl, z, norm 0.033671990036964417 0.0035762786865234375 3.4151082038879395 0.11090326309204102 0.03336954861879349 4.2421875\n",
            "repr, std, cov, clossl, z, norm 0.0342513807117939 0.00397491455078125 3.337021827697754 0.09699723869562149 0.03196922317147255 4.30078125\n",
            "repr, std, cov, clossl, z, norm 0.03311031684279442 0.004940032958984375 3.265794515609741 0.08091306686401367 0.1670173704624176 7.26953125\n",
            "repr, std, cov, clossl, z, norm 0.03435374051332474 0.00617218017578125 3.2518067359924316 0.12590253353118896 0.13488157093524933 3.681640625\n",
            "repr, std, cov, clossl, z, norm 0.03170408681035042 0.0164794921875 2.7672629356384277 0.1621881127357483 0.06244930252432823 3.8203125\n",
            "repr, std, cov, clossl, z, norm 0.030515875667333603 0.01143646240234375 2.991917848587036 0.14454999566078186 0.053487032651901245 4.62890625\n",
            "31\n",
            "repr, std, cov, clossl, z, norm 0.03387397900223732 0.00453948974609375 3.4179186820983887 0.09738456457853317 0.125405415892601 4.80078125\n",
            "repr, std, cov, clossl, z, norm 0.03487422317266464 0.0014677047729492188 3.5692219734191895 0.10006950050592422 0.14373789727687836 5.31640625\n",
            "repr, std, cov, clossl, z, norm 0.0346367321908474 0.0019502639770507812 3.3315517902374268 0.10591132193803787 0.18063758313655853 3.578125\n",
            "repr, std, cov, clossl, z, norm 0.03439517691731453 0.004444122314453125 3.210878372192383 0.09221801906824112 0.04384049400687218 4.34375\n",
            "repr, std, cov, clossl, z, norm 0.031510595232248306 0.0127716064453125 3.0494332313537598 0.07979709655046463 0.06602534651756287 3.763671875\n",
            "repr, std, cov, clossl, z, norm 0.034218885004520416 0.01080322265625 3.15838623046875 0.125690296292305 0.16306038200855255 3.609375\n",
            "repr, std, cov, clossl, z, norm 0.031202415004372597 0.01275634765625 2.9925625324249268 0.09462082386016846 0.14151659607887268 4.109375\n",
            "repr, std, cov, clossl, z, norm 0.03150511533021927 0.0082550048828125 3.1894948482513428 0.08835463970899582 0.014386824332177639 3.623046875\n",
            "repr, std, cov, clossl, z, norm 0.03323286399245262 0.0035648345947265625 3.388246536254883 0.1059148758649826 0.11040522158145905 3.099609375\n",
            "repr, std, cov, clossl, z, norm 0.03337439149618149 0.0041046142578125 3.360816240310669 0.0963282361626625 0.13077689707279205 2.740234375\n",
            "repr, std, cov, clossl, z, norm 0.03241792693734169 0.0044708251953125 3.215322494506836 0.11496955901384354 0.10056605190038681 4.6171875\n",
            "repr, std, cov, clossl, z, norm 0.031181519851088524 0.01025390625 2.999307155609131 0.06984739005565643 0.07210135459899902 4.2890625\n",
            "repr, std, cov, clossl, z, norm 0.03391414135694504 0.005523681640625 3.3790602684020996 0.13210429251194 0.0564497634768486 5.2578125\n",
            "repr, std, cov, clossl, z, norm 0.03408336266875267 0.00440216064453125 3.3975768089294434 0.11460632085800171 0.049396246671676636 4.3828125\n",
            "repr, std, cov, clossl, z, norm 0.03451601043343544 0.0038356781005859375 3.3089404106140137 0.1145646795630455 0.1486905813217163 4.2890625\n",
            "repr, std, cov, clossl, z, norm 0.032759323716163635 0.006259918212890625 3.1367197036743164 0.09445314854383469 0.20519718527793884 3.517578125\n",
            "32\n",
            "repr, std, cov, clossl, z, norm 0.032336972653865814 0.01113128662109375 3.035982370376587 0.13589031994342804 0.06668973714113235 3.5546875\n",
            "repr, std, cov, clossl, z, norm 0.0330936536192894 0.006378173828125 3.167691230773926 0.16409850120544434 0.041676897555589676 3.625\n",
            "repr, std, cov, clossl, z, norm 0.030660629272460938 0.012115478515625 2.931427478790283 0.06582009047269821 0.09532621502876282 3.7890625\n",
            "repr, std, cov, clossl, z, norm 0.035343680530786514 0.002483367919921875 3.440828561782837 0.1060645654797554 0.03510770574212074 5.65234375\n",
            "repr, std, cov, clossl, z, norm 0.03460671007633209 0.003261566162109375 3.301694869995117 0.12784482538700104 0.07404197752475739 3.583984375\n",
            "repr, std, cov, clossl, z, norm 0.0323931984603405 0.0065765380859375 3.010563373565674 0.10877007246017456 0.1403154581785202 3.740234375\n",
            "repr, std, cov, clossl, z, norm 0.03300904482603073 0.0093536376953125 2.9510769844055176 0.10978048294782639 0.06026458367705345 4.25\n",
            "repr, std, cov, clossl, z, norm 0.033559057861566544 0.00445556640625 3.1097869873046875 0.12091682851314545 0.022214731201529503 3.77734375\n",
            "repr, std, cov, clossl, z, norm 0.031283654272556305 0.01107025146484375 2.860219955444336 0.0707697942852974 0.08257504552602768 5.421875\n",
            "repr, std, cov, clossl, z, norm 0.03288739547133446 0.005855560302734375 3.063861608505249 0.14743554592132568 0.10663951188325882 3.07421875\n",
            "repr, std, cov, clossl, z, norm 0.03097587078809738 0.00540924072265625 3.0724785327911377 0.05759847164154053 0.0950067862868309 3.89453125\n",
            "repr, std, cov, clossl, z, norm 0.0322340689599514 0.0014247894287109375 3.44803786277771 0.07167860120534897 0.08755648881196976 4.3828125\n",
            "repr, std, cov, clossl, z, norm 0.03308911621570587 0.0013170242309570312 3.3560590744018555 0.07980594784021378 0.03118722513318062 8.8671875\n",
            "repr, std, cov, clossl, z, norm 0.030096523463726044 0.00717926025390625 2.954157590866089 0.09675183147192001 0.181223064661026 3.818359375\n",
            "repr, std, cov, clossl, z, norm 0.029925432056188583 0.013946533203125 2.7650997638702393 0.11438979208469391 0.0030776874627918005 3.484375\n",
            "repr, std, cov, clossl, z, norm 0.03219359740614891 0.010009765625 3.029428005218506 0.1105349063873291 0.01901768334209919 3.76953125\n",
            "33\n",
            "repr, std, cov, clossl, z, norm 0.03060249797999859 0.0074310302734375 3.0525853633880615 0.07206034660339355 0.0372171625494957 3.912109375\n",
            "repr, std, cov, clossl, z, norm 0.03478552773594856 0.0015087127685546875 3.351015090942383 0.1124909296631813 0.04051605612039566 3.857421875\n",
            "repr, std, cov, clossl, z, norm 0.03261842578649521 0.0034942626953125 3.1497814655303955 0.07271606475114822 0.13054342567920685 4.8515625\n",
            "repr, std, cov, clossl, z, norm 0.030372632667422295 0.006557464599609375 3.0889220237731934 0.06618329137563705 0.09483587741851807 4.9921875\n",
            "repr, std, cov, clossl, z, norm 0.031038722023367882 0.01064300537109375 2.9032697677612305 0.10192003846168518 0.18567341566085815 5.2890625\n",
            "repr, std, cov, clossl, z, norm 0.031536877155303955 0.00799560546875 2.9481616020202637 0.08656597882509232 0.07649194449186325 3.921875\n",
            "repr, std, cov, clossl, z, norm 0.03107510693371296 0.0052032470703125 3.047428846359253 0.06098788231611252 0.03309754654765129 3.712890625\n",
            "repr, std, cov, clossl, z, norm 0.03278602287173271 0.002552032470703125 3.201481819152832 0.14071480929851532 0.06315241754055023 5.24609375\n",
            "repr, std, cov, clossl, z, norm 0.03251569718122482 0.006069183349609375 2.9874696731567383 0.11955174803733826 0.1966865211725235 4.8515625\n",
            "repr, std, cov, clossl, z, norm 0.03248220309615135 0.00455474853515625 3.1067755222320557 0.10484226047992706 0.07108811289072037 3.673828125\n",
            "repr, std, cov, clossl, z, norm 0.033498503267765045 0.0048065185546875 3.082918405532837 0.09483511000871658 0.13659900426864624 3.525390625\n",
            "repr, std, cov, clossl, z, norm 0.03329780325293541 0.004215240478515625 3.120251417160034 0.09692376852035522 0.04652194678783417 3.490234375\n",
            "repr, std, cov, clossl, z, norm 0.03279000520706177 0.005306243896484375 3.069425106048584 0.10484153777360916 0.05136055499315262 3.962890625\n",
            "repr, std, cov, clossl, z, norm 0.03306380659341812 0.00864410400390625 2.919821262359619 0.09580961614847183 0.050091441720724106 4.61328125\n",
            "repr, std, cov, clossl, z, norm 0.0312999002635479 0.0119171142578125 2.8487439155578613 0.09558311849832535 0.08458403497934341 4.30078125\n",
            "repr, std, cov, clossl, z, norm 0.03266555070877075 0.002544403076171875 3.238905429840088 0.15321311354637146 0.00032861775252968073 3.677734375\n",
            "34\n",
            "repr, std, cov, clossl, z, norm 0.03349289298057556 0.0006060600280761719 3.6629714965820312 0.11248630285263062 0.022250346839427948 5.45703125\n",
            "repr, std, cov, clossl, z, norm 0.03142854943871498 0.00823974609375 2.8711953163146973 0.13957969844341278 0.18429233133792877 4.7734375\n",
            "repr, std, cov, clossl, z, norm 0.03356311842799187 0.00778961181640625 2.9043045043945312 0.09306596219539642 0.1292010396718979 4.76171875\n",
            "repr, std, cov, clossl, z, norm 0.03255374729633331 0.008331298828125 3.0408573150634766 0.09469304978847504 0.04069788381457329 2.984375\n",
            "repr, std, cov, clossl, z, norm 0.03354133293032646 0.00522613525390625 3.086724042892456 0.09806738793849945 0.12731337547302246 3.953125\n",
            "repr, std, cov, clossl, z, norm 0.033504508435726166 0.006244659423828125 2.9665844440460205 0.08344290405511856 0.018881525844335556 3.197265625\n",
            "repr, std, cov, clossl, z, norm 0.03161757066845894 0.00556182861328125 3.0531773567199707 0.0904410257935524 0.11827659606933594 5.63671875\n",
            "repr, std, cov, clossl, z, norm 0.032986972481012344 0.00118255615234375 3.603269577026367 0.0941690057516098 0.041499119251966476 4.33984375\n",
            "repr, std, cov, clossl, z, norm 0.03112110309302807 0.00612640380859375 3.076549530029297 0.08725370466709137 0.06151771917939186 4.51953125\n",
            "repr, std, cov, clossl, z, norm 0.03284267708659172 0.005176544189453125 3.030759811401367 0.12141362577676773 0.01366987731307745 4.55859375\n",
            "repr, std, cov, clossl, z, norm 0.031085586175322533 0.007965087890625 2.923658847808838 0.09482844918966293 0.04244495928287506 4.56640625\n",
            "repr, std, cov, clossl, z, norm 0.03227931261062622 0.00786590576171875 2.9400033950805664 0.09890802204608917 0.0798439309000969 7.91796875\n",
            "repr, std, cov, clossl, z, norm 0.03212974593043327 0.006259918212890625 2.955077648162842 0.10048030316829681 0.08606438338756561 5.3046875\n",
            "repr, std, cov, clossl, z, norm 0.031520579010248184 0.00783538818359375 2.937747001647949 0.0841505229473114 0.22726812958717346 4.2578125\n",
            "repr, std, cov, clossl, z, norm 0.03279756009578705 0.0034942626953125 3.294888973236084 0.09842214733362198 0.18063350021839142 3.30078125\n",
            "repr, std, cov, clossl, z, norm 0.03306930139660835 0.003620147705078125 3.116471767425537 0.17920036613941193 0.07549633085727692 4.828125\n",
            "35\n",
            "repr, std, cov, clossl, z, norm 0.03190042823553085 0.009063720703125 2.817021608352661 0.09208738058805466 0.10380060970783234 4.23828125\n",
            "repr, std, cov, clossl, z, norm 0.0334276519715786 0.00315093994140625 3.112536668777466 0.10368157923221588 0.22671404480934143 5.0703125\n",
            "repr, std, cov, clossl, z, norm 0.03162551671266556 0.00559234619140625 2.99052095413208 0.0891527310013771 0.030186669901013374 3.634765625\n",
            "repr, std, cov, clossl, z, norm 0.03144170716404915 0.00533294677734375 3.0204715728759766 0.09836579859256744 0.11466868966817856 7.8203125\n",
            "repr, std, cov, clossl, z, norm 0.031826943159103394 0.004375457763671875 3.007939100265503 0.13546977937221527 0.110041044652462 6.62109375\n",
            "repr, std, cov, clossl, z, norm 0.029391130432486534 0.00798797607421875 2.8619608879089355 0.06296607106924057 0.014468011446297169 4.4375\n",
            "repr, std, cov, clossl, z, norm 0.03133301064372063 0.0033359527587890625 3.1123366355895996 0.07256586849689484 0.13001765310764313 3.537109375\n",
            "repr, std, cov, clossl, z, norm 0.03125619888305664 0.00629425048828125 2.9401485919952393 0.11242614686489105 0.0021693159360438585 1.7529296875\n",
            "repr, std, cov, clossl, z, norm 0.032193515449762344 0.00319671630859375 3.1018247604370117 0.0793754830956459 0.1588495969772339 3.85546875\n",
            "repr, std, cov, clossl, z, norm 0.03243562579154968 0.00409698486328125 3.0114083290100098 0.10531014949083328 0.07760555297136307 3.0078125\n",
            "repr, std, cov, clossl, z, norm 0.032949693500995636 0.006519317626953125 2.952596664428711 0.11507497727870941 0.18855750560760498 3.62890625\n",
            "repr, std, cov, clossl, z, norm 0.031255774199962616 0.01024627685546875 2.8059699535369873 0.10065017640590668 0.030663007870316505 2.76171875\n",
            "repr, std, cov, clossl, z, norm 0.031023701652884483 0.00632476806640625 3.010618209838867 0.1049729585647583 0.09062805771827698 3.412109375\n",
            "repr, std, cov, clossl, z, norm 0.03109079785645008 0.004207611083984375 3.0334932804107666 0.07664117962121964 0.12072600424289703 3.42578125\n",
            "repr, std, cov, clossl, z, norm 0.03255141153931618 0.003795623779296875 3.0720276832580566 0.10418537259101868 0.13308775424957275 5.07421875\n",
            "repr, std, cov, clossl, z, norm 0.03135537728667259 0.0026302337646484375 3.1967079639434814 0.1618271768093109 0.0705505833029747 4.95703125\n",
            "36\n",
            "repr, std, cov, clossl, z, norm 0.03160436451435089 0.00580596923828125 2.895472526550293 0.0879504382610321 0.08774638175964355 3.6640625\n",
            "repr, std, cov, clossl, z, norm 0.03159024566411972 0.008453369140625 2.800560474395752 0.10326242446899414 0.0751708596944809 3.451171875\n",
            "repr, std, cov, clossl, z, norm 0.03417893871665001 0.004390716552734375 3.217183828353882 0.13306944072246552 0.17300058901309967 3.626953125\n",
            "repr, std, cov, clossl, z, norm 0.03342340514063835 0.0024566650390625 3.160099506378174 0.1169338971376419 0.07426847517490387 3.271484375\n",
            "repr, std, cov, clossl, z, norm 0.03164665400981903 0.0097503662109375 2.776740550994873 0.14887568354606628 0.18432265520095825 4.0859375\n",
            "repr, std, cov, clossl, z, norm 0.03178663179278374 0.00809478759765625 2.8321666717529297 0.09264110028743744 0.165025532245636 4.078125\n",
            "repr, std, cov, clossl, z, norm 0.032287321984767914 0.0047607421875 3.050812244415283 0.1213567927479744 0.01878601685166359 6.203125\n",
            "repr, std, cov, clossl, z, norm 0.03238925710320473 0.0036220550537109375 3.0123729705810547 0.09131133556365967 0.2285042107105255 3.787109375\n",
            "repr, std, cov, clossl, z, norm 0.033020518720149994 0.00411224365234375 3.0186967849731445 0.08713214844465256 0.062762551009655 4.40625\n",
            "repr, std, cov, clossl, z, norm 0.03147316351532936 0.005817413330078125 3.039677619934082 0.09119481593370438 0.10551538318395615 5.11328125\n",
            "repr, std, cov, clossl, z, norm 0.03099866397678852 0.0032978057861328125 3.041137218475342 0.050971467047929764 0.09464734047651291 3.8828125\n",
            "repr, std, cov, clossl, z, norm 0.03093470074236393 0.009521484375 2.7935187816619873 0.10808931291103363 0.06653176248073578 3.99609375\n",
            "repr, std, cov, clossl, z, norm 0.029899057000875473 0.0081329345703125 2.918172597885132 0.07026959955692291 0.04913973808288574 4.84765625\n",
            "repr, std, cov, clossl, z, norm 0.029909495264291763 0.00612640380859375 2.8629150390625 0.09624074399471283 0.12762847542762756 4.9375\n",
            "repr, std, cov, clossl, z, norm 0.031165560707449913 0.0027980804443359375 3.082010269165039 0.0751791000366211 0.11055555939674377 4.69140625\n",
            "repr, std, cov, clossl, z, norm 0.03291609510779381 0.003055572509765625 2.967336654663086 0.09709424525499344 0.05710717290639877 4.23046875\n",
            "37\n",
            "repr, std, cov, clossl, z, norm 0.032733798027038574 0.001800537109375 3.0766963958740234 0.10900718718767166 0.07406925410032272 4.70703125\n",
            "repr, std, cov, clossl, z, norm 0.03158251941204071 0.003238677978515625 3.0831475257873535 0.07391923666000366 0.12491332739591599 4.5078125\n",
            "repr, std, cov, clossl, z, norm 0.03055535815656185 0.01238250732421875 2.6178102493286133 0.08500928431749344 0.05929819121956825 3.900390625\n",
            "repr, std, cov, clossl, z, norm 0.029384495690464973 0.0099945068359375 2.788693904876709 0.16327182948589325 0.05442485958337784 6.37890625\n",
            "repr, std, cov, clossl, z, norm 0.03037254326045513 0.004425048828125 3.030209541320801 0.07729319483041763 0.08429883420467377 4.10546875\n",
            "repr, std, cov, clossl, z, norm 0.033848702907562256 0.0021762847900390625 3.148815631866455 0.16086064279079437 0.024251066148281097 2.93359375\n",
            "repr, std, cov, clossl, z, norm 0.03366316482424736 0.00449371337890625 2.8659632205963135 0.14238107204437256 0.08853375911712646 4.52734375\n",
            "repr, std, cov, clossl, z, norm 0.03164254128932953 0.004795074462890625 2.7960445880889893 0.08522208034992218 0.032862767577171326 3.6796875\n",
            "repr, std, cov, clossl, z, norm 0.03395349159836769 0.00394439697265625 3.0111799240112305 0.11029574275016785 0.012690180912613869 5.09375\n",
            "repr, std, cov, clossl, z, norm 0.03277760371565819 0.00418853759765625 3.170872926712036 0.09847696870565414 0.0 4.484375\n",
            "repr, std, cov, clossl, z, norm 0.03133322671055794 0.0108184814453125 2.6636223793029785 0.14631065726280212 0.1443338245153427 4.2265625\n",
            "repr, std, cov, clossl, z, norm 0.03156336396932602 0.00797271728515625 2.8014445304870605 0.12215901911258698 0.06055540591478348 3.701171875\n",
            "repr, std, cov, clossl, z, norm 0.031183619052171707 0.003200531005859375 3.1004347801208496 0.15932229161262512 0.08924678713083267 3.81640625\n",
            "repr, std, cov, clossl, z, norm 0.032476216554641724 0.0015850067138671875 3.2339327335357666 0.11739680916070938 0.10311039537191391 4.45703125\n",
            "repr, std, cov, clossl, z, norm 0.031395137310028076 0.00553131103515625 2.8597140312194824 0.11537183821201324 0.11216143518686295 3.61328125\n",
            "repr, std, cov, clossl, z, norm 0.03261265531182289 0.0098419189453125 2.708646059036255 0.12552237510681152 0.09545179456472397 2.7734375\n",
            "38\n",
            "repr, std, cov, clossl, z, norm 0.03535587340593338 0.0018062591552734375 3.467817783355713 0.12478096038103104 0.013753063045442104 3.93359375\n",
            "repr, std, cov, clossl, z, norm 0.033552296459674835 0.0064544677734375 2.820995807647705 0.09967116266489029 0.14616325497627258 3.98828125\n",
            "repr, std, cov, clossl, z, norm 0.030621137470006943 0.0161895751953125 2.4909324645996094 0.11391249299049377 0.05705758184194565 3.640625\n",
            "repr, std, cov, clossl, z, norm 0.03133754804730415 0.01104736328125 2.7259621620178223 0.11092479526996613 0.1847774088382721 5.02734375\n",
            "repr, std, cov, clossl, z, norm 0.03145698085427284 0.001922607421875 3.323777437210083 0.32719725370407104 0.236017107963562 5.24609375\n",
            "repr, std, cov, clossl, z, norm 0.0350196547806263 0.000263214111328125 3.9983882904052734 0.1847616583108902 0.12356776744127274 4.6796875\n",
            "repr, std, cov, clossl, z, norm 0.032645948231220245 0.0008273124694824219 3.455993175506592 0.12456151843070984 0.047380492091178894 4.0390625\n",
            "repr, std, cov, clossl, z, norm 0.03229675814509392 0.00972747802734375 2.843003749847412 0.1354365199804306 0.01705954410135746 3.9921875\n",
            "repr, std, cov, clossl, z, norm 0.031102972105145454 0.0177764892578125 2.8098535537719727 0.13942575454711914 0.09890622645616531 4.14453125\n",
            "repr, std, cov, clossl, z, norm 0.032580289989709854 0.0163421630859375 2.913088321685791 0.15533432364463806 0.14041106402873993 4.1875\n",
            "repr, std, cov, clossl, z, norm 0.031334977596998215 0.01105499267578125 3.0543525218963623 0.16308334469795227 0.03168560937047005 3.00390625\n",
            "repr, std, cov, clossl, z, norm 0.03174137696623802 0.004436492919921875 3.147461414337158 0.1232917532324791 0.15851280093193054 3.529296875\n",
            "repr, std, cov, clossl, z, norm 0.03253575414419174 0.0035915374755859375 3.1869986057281494 0.20866309106349945 0.062217775732278824 4.50390625\n",
            "repr, std, cov, clossl, z, norm 0.03274952247738838 0.002658843994140625 3.2529382705688477 0.2736712098121643 0.04165182262659073 3.591796875\n",
            "repr, std, cov, clossl, z, norm 0.03099207766354084 0.0045166015625 3.1959710121154785 0.13130149245262146 0.009245868772268295 3.72265625\n",
            "repr, std, cov, clossl, z, norm 0.030820531770586967 0.008697509765625 3.027764320373535 0.10118205845355988 0.15098224580287933 4.65234375\n",
            "39\n",
            "repr, std, cov, clossl, z, norm 0.03178819641470909 0.0086517333984375 2.8214235305786133 0.1333075910806656 0.1289200484752655 4.33984375\n",
            "repr, std, cov, clossl, z, norm 0.030735645443201065 0.0058441162109375 2.9152779579162598 0.1479022055864334 0.05924601852893829 4.1796875\n",
            "repr, std, cov, clossl, z, norm 0.03180724382400513 0.0026988983154296875 3.1065585613250732 0.12456195801496506 0.14059709012508392 3.626953125\n",
            "repr, std, cov, clossl, z, norm 0.02985946089029312 0.00475311279296875 3.022732734680176 0.07435493916273117 0.054192814975976944 5.28125\n",
            "repr, std, cov, clossl, z, norm 0.030521616339683533 0.006397247314453125 2.9878997802734375 0.14004763960838318 0.13301514089107513 3.787109375\n",
            "repr, std, cov, clossl, z, norm 0.032836731523275375 0.005222320556640625 3.087174415588379 0.15779529511928558 0.048559870570898056 3.962890625\n",
            "repr, std, cov, clossl, z, norm 0.031244968995451927 0.009857177734375 2.739326000213623 0.16982002556324005 0.11152679473161697 3.8046875\n",
            "repr, std, cov, clossl, z, norm 0.03240158408880234 0.004688262939453125 2.974677562713623 0.16849562525749207 0.10921747982501984 4.328125\n",
            "repr, std, cov, clossl, z, norm 0.03216652572154999 0.003421783447265625 3.063157081604004 0.1168597862124443 0.23045282065868378 4.03125\n",
            "repr, std, cov, clossl, z, norm 0.03305569663643837 0.0034465789794921875 3.0090272426605225 0.1952664852142334 0.21584200859069824 3.927734375\n",
            "repr, std, cov, clossl, z, norm 0.032495759427547455 0.0089111328125 2.7243735790252686 0.16405758261680603 0.032866716384887695 4.55859375\n",
            "repr, std, cov, clossl, z, norm 0.03174949437379837 0.004077911376953125 2.9302878379821777 0.11694518476724625 0.17908985912799835 3.474609375\n",
            "repr, std, cov, clossl, z, norm 0.033424992114305496 0.00232696533203125 3.15377140045166 0.17203289270401 0.0387067012488842 3.64453125\n",
            "repr, std, cov, clossl, z, norm 0.030964115634560585 0.0092926025390625 2.7416539192199707 0.21592693030834198 0.22983776032924652 3.408203125\n",
            "repr, std, cov, clossl, z, norm 0.030601443722844124 0.00868988037109375 2.7914233207702637 0.14662456512451172 0.06884440779685974 3.884765625\n",
            "repr, std, cov, clossl, z, norm 0.031326036900281906 0.00937652587890625 2.691993474960327 0.13413496315479279 0.19031648337841034 3.91796875\n",
            "40\n",
            "repr, std, cov, clossl, z, norm 0.03167169168591499 0.0018978118896484375 3.039686679840088 0.09444846957921982 0.09360923618078232 3.640625\n",
            "repr, std, cov, clossl, z, norm 0.03300052136182785 0.0005183219909667969 3.2657337188720703 0.11476224660873413 0.07168638706207275 3.751953125\n",
            "repr, std, cov, clossl, z, norm 0.032226648181676865 0.0016736984252929688 3.0627355575561523 0.09614976495504379 0.08873103559017181 3.875\n",
            "repr, std, cov, clossl, z, norm 0.029965301975607872 0.00980377197265625 2.75522518157959 0.09750297665596008 0.19304828345775604 3.69140625\n",
            "repr, std, cov, clossl, z, norm 0.030257949605584145 0.0155181884765625 2.615029811859131 0.1499546617269516 0.1023225486278534 4.55859375\n",
            "repr, std, cov, clossl, z, norm 0.030467675998806953 0.0127105712890625 2.6287050247192383 0.08729485422372818 0.14113189280033112 4.5625\n",
            "repr, std, cov, clossl, z, norm 0.03011998161673546 0.01059722900390625 2.65604829788208 0.10519025474786758 0.071278415620327 4.4765625\n",
            "repr, std, cov, clossl, z, norm 0.03250681236386299 0.00150299072265625 3.1154513359069824 0.18506458401679993 0.14082631468772888 3.65234375\n",
            "repr, std, cov, clossl, z, norm 0.033133164048194885 0.0003185272216796875 3.408390998840332 0.20727607607841492 0.07581545412540436 6.28125\n",
            "repr, std, cov, clossl, z, norm 0.03411879390478134 0.0012311935424804688 3.153717279434204 0.1582612693309784 0.11856115609407425 4.203125\n",
            "repr, std, cov, clossl, z, norm 0.03449609503149986 0.0034027099609375 2.9490933418273926 0.18408812582492828 0.18993331491947174 4.890625\n",
            "repr, std, cov, clossl, z, norm 0.033637478947639465 0.009613037109375 2.6833300590515137 0.1722075641155243 0.06545940786600113 4.89453125\n",
            "repr, std, cov, clossl, z, norm 0.03308761864900589 0.00833892822265625 2.6586971282958984 0.14080514013767242 0.09933971613645554 3.869140625\n",
            "repr, std, cov, clossl, z, norm 0.03157731890678406 0.0086212158203125 2.7851123809814453 0.1451931595802307 0.09262118488550186 4.21484375\n",
            "repr, std, cov, clossl, z, norm 0.031421419233083725 0.00408172607421875 3.0884809494018555 0.1360781490802765 0.09366918355226517 3.677734375\n",
            "repr, std, cov, clossl, z, norm 0.031009750440716743 0.005039215087890625 3.054915428161621 0.18161602318286896 0.1167527511715889 3.185546875\n",
            "41\n",
            "repr, std, cov, clossl, z, norm 0.030476702377200127 0.008819580078125 2.753525733947754 0.10740233957767487 0.23460185527801514 4.72265625\n",
            "repr, std, cov, clossl, z, norm 0.03105209209024906 0.00293731689453125 3.064380645751953 0.10582120716571808 0.23307159543037415 5.44921875\n",
            "repr, std, cov, clossl, z, norm 0.030058549717068672 0.008148193359375 2.809088706970215 0.10567138344049454 0.09230861812829971 3.73828125\n",
            "repr, std, cov, clossl, z, norm 0.03245054557919502 0.0031528472900390625 3.0045061111450195 0.15014269948005676 0.05939917638897896 4.1640625\n",
            "repr, std, cov, clossl, z, norm 0.03170065954327583 0.00424957275390625 2.953636407852173 0.1263117641210556 0.11708087474107742 3.861328125\n",
            "repr, std, cov, clossl, z, norm 0.03475916013121605 0.002803802490234375 2.957014322280884 0.2253025770187378 0.19284865260124207 5.57421875\n",
            "repr, std, cov, clossl, z, norm 0.03148737549781799 0.0084991455078125 2.658151626586914 0.18232226371765137 0.08215656876564026 4.73046875\n",
            "repr, std, cov, clossl, z, norm 0.03151768445968628 0.002742767333984375 3.0635476112365723 0.0925159826874733 0.14208926260471344 4.8046875\n",
            "repr, std, cov, clossl, z, norm 0.032436419278383255 0.005748748779296875 2.8331050872802734 0.1181892454624176 0.04615120589733124 4.33203125\n",
            "repr, std, cov, clossl, z, norm 0.029862055554986 0.01311492919921875 2.579125165939331 0.11882677674293518 0.029902007430791855 3.302734375\n",
            "repr, std, cov, clossl, z, norm 0.030873281881213188 0.0023956298828125 3.1087589263916016 0.12148594856262207 0.11995398253202438 2.1640625\n",
            "repr, std, cov, clossl, z, norm 0.0330561138689518 0.0023784637451171875 3.0976901054382324 0.1599268615245819 0.07349471002817154 3.83203125\n",
            "repr, std, cov, clossl, z, norm 0.03097325749695301 0.0054931640625 2.8438949584960938 0.13225454092025757 0.07408592849969864 4.3359375\n",
            "repr, std, cov, clossl, z, norm 0.029054217040538788 0.00991058349609375 2.6114649772644043 0.09483163803815842 0.14295420050621033 5.5\n",
            "repr, std, cov, clossl, z, norm 0.03249875828623772 0.004638671875 2.8233838081359863 0.15818992257118225 0.07902802526950836 4.85546875\n",
            "repr, std, cov, clossl, z, norm 0.03339651599526405 0.00411224365234375 2.9135255813598633 0.14092092216014862 0.027083223685622215 3.712890625\n",
            "42\n",
            "repr, std, cov, clossl, z, norm 0.03316934406757355 0.0045928955078125 2.757643699645996 0.12121579796075821 0.046301040798425674 2.912109375\n",
            "repr, std, cov, clossl, z, norm 0.032494544982910156 0.0025119781494140625 2.8642938137054443 0.10915767401456833 0.1289638727903366 2.544921875\n",
            "repr, std, cov, clossl, z, norm 0.03137814626097679 0.0025348663330078125 3.0188026428222656 0.12236106395721436 0.13520146906375885 3.748046875\n",
            "repr, std, cov, clossl, z, norm 0.030683014541864395 0.00562286376953125 2.7556729316711426 0.15482363104820251 0.07972587645053864 5.67578125\n",
            "repr, std, cov, clossl, z, norm 0.030423717573285103 0.00730133056640625 2.632652521133423 0.1515710949897766 0.041290756314992905 2.982421875\n",
            "repr, std, cov, clossl, z, norm 0.03124803490936756 0.006359100341796875 2.726184844970703 0.11984635889530182 0.040894459933042526 3.142578125\n",
            "repr, std, cov, clossl, z, norm 0.031045731157064438 0.005878448486328125 2.7923402786254883 0.1148560419678688 0.10369237512350082 4.73828125\n",
            "repr, std, cov, clossl, z, norm 0.03234664350748062 0.004009246826171875 2.8050272464752197 0.1505795419216156 0.046283043920993805 5.01171875\n",
            "repr, std, cov, clossl, z, norm 0.031122244894504547 0.0008664131164550781 3.0840344429016113 0.08607763051986694 0.056040454655885696 6.49609375\n",
            "repr, std, cov, clossl, z, norm 0.029936032369732857 0.004985809326171875 2.733534574508667 0.14998094737529755 0.08367405086755753 3.986328125\n",
            "repr, std, cov, clossl, z, norm 0.028615791350603104 0.00907135009765625 2.703179359436035 0.09621289372444153 0.07280968129634857 4.1640625\n",
            "repr, std, cov, clossl, z, norm 0.031409505754709244 0.00560760498046875 2.819240093231201 0.1141175702214241 0.012347235344350338 3.8671875\n",
            "repr, std, cov, clossl, z, norm 0.031349699944257736 0.0030612945556640625 2.868589401245117 0.10466635227203369 0.1128101795911789 6.5\n",
            "repr, std, cov, clossl, z, norm 0.032160624861717224 0.003093719482421875 2.875687599182129 0.13282646238803864 0.2190455049276352 4.1171875\n",
            "repr, std, cov, clossl, z, norm 0.03147541359066963 0.0054473876953125 2.6813411712646484 0.11409922689199448 0.10754543542861938 3.501953125\n",
            "repr, std, cov, clossl, z, norm 0.030212413519620895 0.0057525634765625 2.689021110534668 0.11153177171945572 0.2301933914422989 4.2109375\n",
            "43\n",
            "repr, std, cov, clossl, z, norm 0.03152300789952278 0.00499725341796875 2.7312636375427246 0.1080043613910675 0.06937191635370255 3.78515625\n",
            "repr, std, cov, clossl, z, norm 0.032118141651153564 0.0023021697998046875 2.847456455230713 0.09324521571397781 0.1930711567401886 8.09375\n",
            "repr, std, cov, clossl, z, norm 0.02899664267897606 0.01012420654296875 2.516972303390503 0.0988001599907875 0.1520969420671463 3.771484375\n",
            "repr, std, cov, clossl, z, norm 0.03139360621571541 0.004451751708984375 2.789121150970459 0.10076926648616791 0.10624845325946808 4.234375\n",
            "repr, std, cov, clossl, z, norm 0.029282506555318832 0.002582550048828125 2.882408618927002 0.06767284125089645 0.03691549226641655 4.55859375\n",
            "repr, std, cov, clossl, z, norm 0.03179212659597397 0.0028285980224609375 2.817021369934082 0.10853009670972824 0.12738274037837982 4.36328125\n",
            "repr, std, cov, clossl, z, norm 0.0321018323302269 0.0017719268798828125 2.911911964416504 0.14862827956676483 0.10801192373037338 3.703125\n",
            "repr, std, cov, clossl, z, norm 0.030125878751277924 0.0051422119140625 2.7750492095947266 0.08633928745985031 0.12351783365011215 4.15625\n",
            "repr, std, cov, clossl, z, norm 0.029843678697943687 0.00856781005859375 2.5873708724975586 0.082986980676651 0.04014309123158455 3.439453125\n",
            "repr, std, cov, clossl, z, norm 0.030814003199338913 0.0063018798828125 2.574969530105591 0.10526464134454727 0.0816764310002327 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.03060554899275303 0.00856781005859375 2.601539134979248 0.1071338802576065 0.07043399661779404 3.904296875\n",
            "repr, std, cov, clossl, z, norm 0.03210313618183136 0.002685546875 2.874175786972046 0.11152312904596329 0.08653292059898376 5.34765625\n",
            "repr, std, cov, clossl, z, norm 0.03165259584784508 0.0011882781982421875 3.0308876037597656 0.0818941667675972 0.18785743415355682 4.25390625\n",
            "repr, std, cov, clossl, z, norm 0.030819391831755638 0.002716064453125 2.9077072143554688 0.14148545265197754 0.12191781401634216 3.943359375\n",
            "repr, std, cov, clossl, z, norm 0.03178488463163376 0.0016450881958007812 2.9157843589782715 0.12802794575691223 0.08478345721960068 4.25\n",
            "repr, std, cov, clossl, z, norm 0.03323424607515335 0.002887725830078125 2.8020143508911133 0.11848057806491852 0.1789732575416565 3.12109375\n",
            "44\n",
            "repr, std, cov, clossl, z, norm 0.03139543533325195 0.0136871337890625 2.423487663269043 0.13281244039535522 0.0808904618024826 3.984375\n",
            "repr, std, cov, clossl, z, norm 0.028723938390612602 0.020111083984375 2.3406119346618652 0.10914262384176254 0.13710662722587585 3.34375\n",
            "repr, std, cov, clossl, z, norm 0.030946001410484314 0.008514404296875 2.7212698459625244 0.08811691403388977 0.009609273634850979 3.076171875\n",
            "repr, std, cov, clossl, z, norm 0.032230038195848465 0.002193450927734375 3.0356926918029785 0.11151579767465591 0.13244374096393585 4.359375\n",
            "repr, std, cov, clossl, z, norm 0.03177696838974953 0.002056121826171875 2.8907668590545654 0.10500167310237885 0.11353474110364914 3.6171875\n",
            "repr, std, cov, clossl, z, norm 0.0322173535823822 0.002750396728515625 2.9198992252349854 0.10708305239677429 0.01429741084575653 3.400390625\n",
            "repr, std, cov, clossl, z, norm 0.0323776975274086 0.0021991729736328125 3.0684895515441895 0.11681735515594482 0.1173124685883522 4.9140625\n",
            "repr, std, cov, clossl, z, norm 0.030360378324985504 0.00525665283203125 2.7311863899230957 0.08299949765205383 0.03201841562986374 5.2109375\n",
            "repr, std, cov, clossl, z, norm 0.03032398410141468 0.00894927978515625 2.5431652069091797 0.0950845330953598 0.15484195947647095 3.146484375\n",
            "repr, std, cov, clossl, z, norm 0.030908657237887383 0.00569915771484375 2.6943821907043457 0.11209073662757874 0.03135690093040466 3.99609375\n",
            "repr, std, cov, clossl, z, norm 0.030171990394592285 0.006458282470703125 2.7030222415924072 0.10055338591337204 0.15153230726718903 3.9296875\n",
            "repr, std, cov, clossl, z, norm 0.031450871378183365 0.00431060791015625 2.800351142883301 0.11399638652801514 0.0277005173265934 6.49609375\n",
            "repr, std, cov, clossl, z, norm 0.03041628561913967 0.00366973876953125 2.7242226600646973 0.09597918391227722 0.05880102515220642 3.669921875\n",
            "repr, std, cov, clossl, z, norm 0.03157208487391472 0.00449371337890625 2.652787208557129 0.14845077693462372 0.17896169424057007 4.99609375\n",
            "repr, std, cov, clossl, z, norm 0.03288096934556961 0.00243377685546875 2.9684884548187256 0.10225967317819595 0.21107414364814758 4.45703125\n",
            "repr, std, cov, clossl, z, norm 0.033638715744018555 0.0008869171142578125 2.978184700012207 0.09711917489767075 0.07863041013479233 4.96875\n",
            "45\n",
            "repr, std, cov, clossl, z, norm 0.03267652168869972 0.00609588623046875 2.5999529361724854 0.12939439713954926 0.07580097019672394 4.4375\n",
            "repr, std, cov, clossl, z, norm 0.029432248324155807 0.006488800048828125 2.632056474685669 0.078702911734581 0.18758253753185272 3.484375\n",
            "repr, std, cov, clossl, z, norm 0.030582549050450325 0.00745391845703125 2.645237445831299 0.06877852976322174 0.09897720813751221 6.609375\n",
            "repr, std, cov, clossl, z, norm 0.02915843576192856 0.0118255615234375 2.458221912384033 0.08828968554735184 0.08692391216754913 4.3359375\n",
            "repr, std, cov, clossl, z, norm 0.03003128059208393 0.0035552978515625 2.791797637939453 0.07301755994558334 0.1362079679965973 3.67578125\n",
            "repr, std, cov, clossl, z, norm 0.0324845090508461 0.0014495849609375 2.8234827518463135 0.13316753506660461 0.10193769633769989 4.23046875\n",
            "repr, std, cov, clossl, z, norm 0.03225196897983551 0.002674102783203125 2.7385473251342773 0.07857678085565567 0.06850234419107437 2.9296875\n",
            "repr, std, cov, clossl, z, norm 0.03142654895782471 0.004528045654296875 2.6575303077697754 0.1515326350927353 0.12065360695123672 3.84765625\n",
            "repr, std, cov, clossl, z, norm 0.030922554433345795 0.004764556884765625 2.6218910217285156 0.09345756471157074 0.0825737863779068 4.5234375\n",
            "repr, std, cov, clossl, z, norm 0.030851509422063828 0.007762908935546875 2.5353825092315674 0.14836931228637695 0.12329648435115814 3.97265625\n",
            "repr, std, cov, clossl, z, norm 0.032278671860694885 0.001800537109375 2.896749258041382 0.08448711037635803 0.12567000091075897 4.609375\n",
            "repr, std, cov, clossl, z, norm 0.03257539123296738 0.0030059814453125 2.7507846355438232 0.09978372603654861 0.13214337825775146 4.484375\n",
            "repr, std, cov, clossl, z, norm 0.03044957108795643 0.0076904296875 2.472616195678711 0.10378529131412506 0.15719038248062134 3.861328125\n",
            "repr, std, cov, clossl, z, norm 0.03146572411060333 0.005695343017578125 2.6456942558288574 0.09757287800312042 0.10810437798500061 3.55859375\n",
            "repr, std, cov, clossl, z, norm 0.029959995299577713 0.00530242919921875 2.5802483558654785 0.07253722101449966 0.026768675073981285 3.837890625\n",
            "repr, std, cov, clossl, z, norm 0.033560674637556076 0.0013141632080078125 2.9300613403320312 0.11532098799943924 0.07009220868349075 4.63671875\n",
            "46\n",
            "repr, std, cov, clossl, z, norm 0.03147389739751816 0.0015363693237304688 2.862484931945801 0.10664508491754532 0.13450942933559418 3.712890625\n",
            "repr, std, cov, clossl, z, norm 0.032113492488861084 0.003787994384765625 2.6581218242645264 0.10974498838186264 0.062088798731565475 4.39453125\n",
            "repr, std, cov, clossl, z, norm 0.029876522719860077 0.0125274658203125 2.3412461280822754 0.1283322423696518 0.02980438992381096 5.015625\n",
            "repr, std, cov, clossl, z, norm 0.03188355639576912 0.004222869873046875 2.639641761779785 0.1161031723022461 0.1554580181837082 4.35546875\n",
            "repr, std, cov, clossl, z, norm 0.03019128553569317 0.007659912109375 2.525862216949463 0.08564451336860657 0.10169743746519089 6.0703125\n",
            "repr, std, cov, clossl, z, norm 0.0318029411137104 0.001178741455078125 2.906930923461914 0.0976446121931076 0.0928029865026474 5.1796875\n",
            "repr, std, cov, clossl, z, norm 0.03108706697821617 0.0022716522216796875 2.844209671020508 0.11791674047708511 0.05351017788052559 3.78515625\n",
            "repr, std, cov, clossl, z, norm 0.03168674558401108 0.0033740997314453125 2.656966209411621 0.07626612484455109 0.14427916705608368 3.900390625\n",
            "repr, std, cov, clossl, z, norm 0.030022284016013145 0.00658416748046875 2.554609775543213 0.08490514755249023 0.08042308688163757 3.1328125\n",
            "repr, std, cov, clossl, z, norm 0.030013583600521088 0.00762939453125 2.488107681274414 0.10282976925373077 0.25214603543281555 4.4609375\n",
            "repr, std, cov, clossl, z, norm 0.031465306878089905 0.0025539398193359375 2.8224868774414062 0.09831889718770981 0.13985925912857056 4.6875\n",
            "repr, std, cov, clossl, z, norm 0.0294415894895792 0.00516510009765625 2.598482847213745 0.18272660672664642 0.12625890970230103 3.251953125\n",
            "repr, std, cov, clossl, z, norm 0.031108355149626732 0.0013523101806640625 2.810262680053711 0.08864720165729523 0.2370738536119461 4.53125\n",
            "repr, std, cov, clossl, z, norm 0.031989846378564835 0.00201416015625 2.777317523956299 0.14540965855121613 0.29864996671676636 4.0625\n",
            "repr, std, cov, clossl, z, norm 0.030395230278372765 0.005901336669921875 2.563375949859619 0.11333436518907547 0.02609054185450077 3.279296875\n",
            "repr, std, cov, clossl, z, norm 0.030940227210521698 0.007350921630859375 2.6193974018096924 0.10078009217977524 0.11768274754285812 3.29296875\n",
            "47\n",
            "repr, std, cov, clossl, z, norm 0.03299393877387047 0.0024871826171875 2.71112060546875 0.10852161049842834 0.015147659927606583 4.8125\n",
            "repr, std, cov, clossl, z, norm 0.03164473548531532 0.004657745361328125 2.5783538818359375 0.10164227336645126 0.10336761176586151 3.30859375\n",
            "repr, std, cov, clossl, z, norm 0.030188148841261864 0.0036602020263671875 2.6566996574401855 0.09247086197137833 0.11623246222734451 4.20703125\n",
            "repr, std, cov, clossl, z, norm 0.031578559428453445 0.0036525726318359375 2.604447841644287 0.12585555016994476 0.06814077496528625 3.771484375\n",
            "repr, std, cov, clossl, z, norm 0.030571775510907173 0.006160736083984375 2.4976158142089844 0.1099584773182869 0.1409699022769928 4.328125\n",
            "repr, std, cov, clossl, z, norm 0.03081127256155014 0.00640869140625 2.463336229324341 0.09132484346628189 0.14782804250717163 2.76171875\n",
            "repr, std, cov, clossl, z, norm 0.029766902327537537 0.00370025634765625 2.6397335529327393 0.08045602589845657 0.07650452107191086 3.109375\n",
            "repr, std, cov, clossl, z, norm 0.030912140384316444 0.0021457672119140625 2.670246124267578 0.0904555395245552 0.07154737412929535 4.37109375\n",
            "repr, std, cov, clossl, z, norm 0.031564436852931976 0.0029354095458984375 2.6054999828338623 0.0873701423406601 0.0872219055891037 3.7578125\n",
            "repr, std, cov, clossl, z, norm 0.03015103191137314 0.0031414031982421875 2.7053256034851074 0.08979581296443939 0.06724318116903305 4.59765625\n",
            "repr, std, cov, clossl, z, norm 0.030856408178806305 0.0051116943359375 2.546717882156372 0.16923795640468597 0.16242127120494843 3.091796875\n",
            "repr, std, cov, clossl, z, norm 0.030809449031949043 0.0018310546875 2.7037816047668457 0.05954586714506149 0.09484168887138367 5.19921875\n",
            "repr, std, cov, clossl, z, norm 0.029069222509860992 0.00695037841796875 2.522254467010498 0.08761143684387207 0.07377149909734726 3.849609375\n",
            "repr, std, cov, clossl, z, norm 0.0290036853402853 0.005619049072265625 2.5033717155456543 0.08692990243434906 0.09466169774532318 4.078125\n",
            "repr, std, cov, clossl, z, norm 0.02962787076830864 0.002796173095703125 2.652522563934326 0.06548073887825012 0.11481396108865738 3.87890625\n",
            "repr, std, cov, clossl, z, norm 0.030254868790507317 0.0027790069580078125 2.655693292617798 0.09820349514484406 0.24526852369308472 3.77734375\n",
            "48\n",
            "repr, std, cov, clossl, z, norm 0.031152496114373207 0.00365447998046875 2.592373847961426 0.09738411754369736 0.05858638510107994 4.21875\n",
            "repr, std, cov, clossl, z, norm 0.029794901609420776 0.001697540283203125 2.6701712608337402 0.07930014282464981 0.053548164665699005 3.8828125\n",
            "repr, std, cov, clossl, z, norm 0.03002898208796978 0.003772735595703125 2.5640506744384766 0.06984513252973557 0.14418914914131165 4.60546875\n",
            "repr, std, cov, clossl, z, norm 0.02973916009068489 0.00643157958984375 2.4454572200775146 0.068258136510849 0.15693926811218262 4.02734375\n",
            "repr, std, cov, clossl, z, norm 0.02917773649096489 0.005435943603515625 2.5173487663269043 0.08220116794109344 0.05167112126946449 3.767578125\n",
            "repr, std, cov, clossl, z, norm 0.02993357554078102 0.004150390625 2.55572247505188 0.12426262348890305 0.07084604352712631 3.203125\n",
            "repr, std, cov, clossl, z, norm 0.029529962688684464 0.0050506591796875 2.5316669940948486 0.08639020472764969 0.17606496810913086 4.40625\n",
            "repr, std, cov, clossl, z, norm 0.029968930408358574 0.0034084320068359375 2.5777382850646973 0.14027179777622223 0.04615701362490654 3.396484375\n",
            "repr, std, cov, clossl, z, norm 0.029241137206554413 0.00374603271484375 2.5459165573120117 0.10328468680381775 0.05837756767868996 2.60546875\n",
            "repr, std, cov, clossl, z, norm 0.03044060617685318 0.002285003662109375 2.643845558166504 0.09145728498697281 0.046120535582304 3.140625\n",
            "repr, std, cov, clossl, z, norm 0.030153697356581688 0.003143310546875 2.5375969409942627 0.08736305683851242 0.11387376487255096 3.998046875\n",
            "repr, std, cov, clossl, z, norm 0.02895686775445938 0.003673553466796875 2.5434627532958984 0.07495354115962982 0.04841526597738266 3.861328125\n",
            "repr, std, cov, clossl, z, norm 0.028077632188796997 0.0044097900390625 2.5841078758239746 0.07160090655088425 0.10566879063844681 3.4609375\n",
            "repr, std, cov, clossl, z, norm 0.0292122270911932 0.00426483154296875 2.538853168487549 0.0861041322350502 0.1462678462266922 3.216796875\n",
            "repr, std, cov, clossl, z, norm 0.028638260439038277 0.005565643310546875 2.5076723098754883 0.06400116533041 0.007439120206981897 4.94140625\n",
            "repr, std, cov, clossl, z, norm 0.030365794897079468 0.0017414093017578125 2.775547981262207 0.101254403591156 0.17760352790355682 3.75390625\n",
            "49\n",
            "repr, std, cov, clossl, z, norm 0.03035721182823181 0.0017957687377929688 2.7647881507873535 0.07979907095432281 0.18698382377624512 4.8046875\n",
            "repr, std, cov, clossl, z, norm 0.028432296589016914 0.00931549072265625 2.3565969467163086 0.061654165387153625 0.24262525141239166 3.69140625\n",
            "repr, std, cov, clossl, z, norm 0.02932945266366005 0.00907135009765625 2.420029878616333 0.10333501547574997 0.07793919742107391 3.634765625\n",
            "repr, std, cov, clossl, z, norm 0.029996030032634735 0.001956939697265625 2.7303638458251953 0.05900684371590614 0.2731168866157532 3.654296875\n",
            "repr, std, cov, clossl, z, norm 0.030954645946621895 0.0006709098815917969 2.8451991081237793 0.07839100062847137 0.21864914894104004 4.76953125\n",
            "repr, std, cov, clossl, z, norm 0.030699443072080612 0.0013151168823242188 2.7776999473571777 0.07565312087535858 0.03237790986895561 3.810546875\n",
            "repr, std, cov, clossl, z, norm 0.029509838670492172 0.00557708740234375 2.4481256008148193 0.06861691921949387 0.08253531903028488 4.65625\n",
            "repr, std, cov, clossl, z, norm 0.028193192556500435 0.0125732421875 2.327846050262451 0.05706407502293587 0.0732344463467598 3.837890625\n",
            "repr, std, cov, clossl, z, norm 0.027843525633215904 0.011138916015625 2.406759738922119 0.15871647000312805 0.08598551899194717 3.376953125\n",
            "repr, std, cov, clossl, z, norm 0.030498487874865532 0.004474639892578125 2.6211934089660645 0.09774213284254074 0.11630905419588089 3.802734375\n",
            "repr, std, cov, clossl, z, norm 0.030796827748417854 0.0013885498046875 2.713383674621582 0.05574503913521767 0.02610926143825054 3.521484375\n",
            "repr, std, cov, clossl, z, norm 0.03229399025440216 0.0004048347473144531 2.892634391784668 0.09530726075172424 0.2715038061141968 3.060546875\n",
            "repr, std, cov, clossl, z, norm 0.03192039951682091 0.0009083747863769531 2.78121280670166 0.0809953510761261 0.036296963691711426 4.21484375\n",
            "repr, std, cov, clossl, z, norm 0.030719615519046783 0.00930023193359375 2.357706308364868 0.09987367689609528 0.0798131600022316 5.328125\n",
            "repr, std, cov, clossl, z, norm 0.027617625892162323 0.005451202392578125 2.6007885932922363 0.049159374088048935 0.08831964433193207 4.37109375\n",
            "repr, std, cov, clossl, z, norm 0.030166463926434517 0.0085296630859375 2.440181255340576 0.09983973205089569 0.052727121859788895 4.01171875\n",
            "50\n",
            "repr, std, cov, clossl, z, norm 0.031454816460609436 0.00226593017578125 2.606849193572998 0.16827422380447388 0.02783229388296604 4.24609375\n",
            "repr, std, cov, clossl, z, norm 0.03216055408120155 0.00279998779296875 2.566006660461426 0.1039048433303833 0.13053551316261292 3.59765625\n",
            "repr, std, cov, clossl, z, norm 0.03178507089614868 0.0017490386962890625 2.6933228969573975 0.0951194167137146 0.14610959589481354 3.861328125\n",
            "repr, std, cov, clossl, z, norm 0.029357122257351875 0.0025653839111328125 2.682180404663086 0.055786699056625366 0.07775721698999405 4.58984375\n",
            "repr, std, cov, clossl, z, norm 0.029248666018247604 0.007904052734375 2.4482431411743164 0.07957591861486435 0.10985203087329865 3.6484375\n",
            "repr, std, cov, clossl, z, norm 0.029203150421380997 0.0082855224609375 2.378174066543579 0.06502413749694824 0.08171208947896957 3.458984375\n",
            "repr, std, cov, clossl, z, norm 0.02982623688876629 0.006633758544921875 2.4098050594329834 0.07130643725395203 0.10002011060714722 3.5859375\n",
            "repr, std, cov, clossl, z, norm 0.03032318316400051 0.002056121826171875 2.6280412673950195 0.06589061766862869 0.19717752933502197 4.0859375\n",
            "repr, std, cov, clossl, z, norm 0.030511202290654182 0.0024318695068359375 2.716649293899536 0.08692684769630432 0.13212108612060547 3.939453125\n",
            "repr, std, cov, clossl, z, norm 0.02844475395977497 0.003482818603515625 2.584667682647705 0.04892348125576973 0.10592356324195862 4.328125\n",
            "repr, std, cov, clossl, z, norm 0.02928425744175911 0.002567291259765625 2.631183385848999 0.0645720586180687 0.0311043169349432 3.142578125\n",
            "repr, std, cov, clossl, z, norm 0.029011229053139687 0.0036468505859375 2.545020580291748 0.05918864160776138 0.056238360702991486 3.8203125\n",
            "repr, std, cov, clossl, z, norm 0.029166070744395256 0.0026988983154296875 2.6113715171813965 0.09131605178117752 0.02475696988403797 4.11328125\n",
            "repr, std, cov, clossl, z, norm 0.030191460624337196 0.00641632080078125 2.411111354827881 0.1171242892742157 0.06358437240123749 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.03080720081925392 0.002872467041015625 2.5502800941467285 0.0967555046081543 0.11707130819559097 3.76171875\n",
            "repr, std, cov, clossl, z, norm 0.030482225120067596 0.003139495849609375 2.535214424133301 0.07010111957788467 0.17741072177886963 3.650390625\n",
            "51\n",
            "repr, std, cov, clossl, z, norm 0.028731245547533035 0.00450897216796875 2.4253783226013184 0.04676974564790726 0.1492263525724411 3.767578125\n",
            "repr, std, cov, clossl, z, norm 0.029150499030947685 0.005680084228515625 2.436821460723877 0.04981270805001259 0.0356142483651638 2.974609375\n",
            "repr, std, cov, clossl, z, norm 0.030289500951766968 0.00296783447265625 2.5768518447875977 0.07139644771814346 0.08640289306640625 3.26953125\n",
            "repr, std, cov, clossl, z, norm 0.03071698360145092 0.002246856689453125 2.586850643157959 0.0875668004155159 0.08390623331069946 4.26953125\n",
            "repr, std, cov, clossl, z, norm 0.02890118770301342 0.00722503662109375 2.4181060791015625 0.08633918315172195 0.13632524013519287 4.61328125\n",
            "repr, std, cov, clossl, z, norm 0.0287338774651289 0.005702972412109375 2.4622740745544434 0.07886656373739243 0.07498271763324738 3.900390625\n",
            "repr, std, cov, clossl, z, norm 0.02996804565191269 0.0007925033569335938 2.8675198554992676 0.08217845857143402 0.08745904266834259 5.96875\n",
            "repr, std, cov, clossl, z, norm 0.02958349697291851 0.0021800994873046875 2.5755703449249268 0.08015845715999603 0.1311623603105545 3.494140625\n",
            "repr, std, cov, clossl, z, norm 0.030358383432030678 0.00316619873046875 2.557927131652832 0.09147607535123825 0.016265887767076492 3.9140625\n",
            "repr, std, cov, clossl, z, norm 0.028841422870755196 0.00847625732421875 2.375182628631592 0.08271310478448868 0.17718759179115295 4.5\n",
            "repr, std, cov, clossl, z, norm 0.028022117912769318 0.01021575927734375 2.3356740474700928 0.05729464069008827 0.07253916561603546 3.935546875\n",
            "repr, std, cov, clossl, z, norm 0.02968842163681984 0.005359649658203125 2.478198766708374 0.09454336017370224 0.17414003610610962 3.677734375\n",
            "repr, std, cov, clossl, z, norm 0.029990732669830322 0.0011796951293945312 2.6760342121124268 0.08188337087631226 0.0345185250043869 4.19921875\n",
            "repr, std, cov, clossl, z, norm 0.027586793527007103 0.0015392303466796875 2.616243362426758 0.09670770168304443 0.22714345157146454 3.806640625\n",
            "repr, std, cov, clossl, z, norm 0.030610544607043266 0.0009002685546875 2.7126007080078125 0.0640866681933403 0.10355795919895172 5.0234375\n",
            "repr, std, cov, clossl, z, norm 0.029776042327284813 0.0029773712158203125 2.6153564453125 0.11277656257152557 0.07258894294500351 4.49609375\n",
            "52\n",
            "repr, std, cov, clossl, z, norm 0.02899412252008915 0.00983428955078125 2.299065589904785 0.09217855334281921 0.12114913761615753 4.96484375\n",
            "repr, std, cov, clossl, z, norm 0.02774352952837944 0.0084686279296875 2.3436880111694336 0.13250961899757385 0.10938072949647903 2.80859375\n",
            "repr, std, cov, clossl, z, norm 0.02889757603406906 0.005054473876953125 2.4913387298583984 0.09295827895402908 0.050315048545598984 4.3046875\n",
            "repr, std, cov, clossl, z, norm 0.03100176528096199 0.0008053779602050781 2.717924118041992 0.08575925976037979 0.07202013581991196 4.33203125\n",
            "repr, std, cov, clossl, z, norm 0.029774174094200134 0.001300811767578125 2.70247220993042 0.08199630677700043 0.18351133167743683 3.447265625\n",
            "repr, std, cov, clossl, z, norm 0.02884609065949917 0.0026721954345703125 2.5756759643554688 0.07819169759750366 0.030606629326939583 4.31640625\n",
            "repr, std, cov, clossl, z, norm 0.03027212992310524 0.005039215087890625 2.4342007637023926 0.06070524454116821 0.006651713512837887 3.04296875\n",
            "repr, std, cov, clossl, z, norm 0.027713613584637642 0.0107574462890625 2.2567050457000732 0.0657317116856575 0.11057249456644058 3.52734375\n",
            "repr, std, cov, clossl, z, norm 0.028775742277503014 0.005741119384765625 2.3899593353271484 0.07709712535142899 0.13544690608978271 2.8671875\n",
            "repr, std, cov, clossl, z, norm 0.02891513518989086 0.0029773712158203125 2.5247700214385986 0.09006653726100922 0.040774155408144 4.12109375\n",
            "repr, std, cov, clossl, z, norm 0.02959231287240982 0.00116729736328125 2.6114532947540283 0.06851192563772202 0.07333001494407654 2.818359375\n",
            "repr, std, cov, clossl, z, norm 0.031411755830049515 0.0015897750854492188 2.621612548828125 0.10988198220729828 0.109018474817276 5.0234375\n",
            "repr, std, cov, clossl, z, norm 0.029912414029240608 0.0022182464599609375 2.537228584289551 0.06315147876739502 0.16843515634536743 4.2734375\n",
            "repr, std, cov, clossl, z, norm 0.030007995665073395 0.0028896331787109375 2.537247657775879 0.11359601467847824 0.018403802067041397 3.70703125\n",
            "repr, std, cov, clossl, z, norm 0.028460081666707993 0.00955963134765625 2.252499580383301 0.09718756377696991 0.10420703887939453 3.46484375\n",
            "repr, std, cov, clossl, z, norm 0.028751948848366737 0.00777435302734375 2.31638503074646 0.08381952345371246 0.11418865621089935 4.203125\n",
            "53\n",
            "repr, std, cov, clossl, z, norm 0.030812373384833336 0.003704071044921875 2.457367181777954 0.06456316262483597 0.26414600014686584 3.5390625\n",
            "repr, std, cov, clossl, z, norm 0.029806062579154968 0.003253936767578125 2.416962146759033 0.08383172750473022 0.10474839061498642 5.7109375\n",
            "repr, std, cov, clossl, z, norm 0.028987834230065346 0.0026874542236328125 2.531550407409668 0.0633884072303772 0.07354133576154709 2.841796875\n",
            "repr, std, cov, clossl, z, norm 0.02952415868639946 0.001132965087890625 2.678478240966797 0.07008866965770721 0.06007344648241997 3.94921875\n",
            "repr, std, cov, clossl, z, norm 0.03182809054851532 0.0008826255798339844 2.6453683376312256 0.0934399738907814 0.07164818048477173 3.291015625\n",
            "repr, std, cov, clossl, z, norm 0.026783939450979233 0.0082550048828125 2.2938480377197266 0.05119163542985916 0.1119454875588417 4.3828125\n",
            "repr, std, cov, clossl, z, norm 0.02816370502114296 0.0041961669921875 2.436279773712158 0.06965862214565277 0.06463851779699326 4.23046875\n",
            "repr, std, cov, clossl, z, norm 0.029048902913928032 0.00395965576171875 2.4252891540527344 0.06985991448163986 0.2595195472240448 3.6640625\n",
            "repr, std, cov, clossl, z, norm 0.028335344046354294 0.006175994873046875 2.328702449798584 0.05615609139204025 0.20771364867687225 3.578125\n",
            "repr, std, cov, clossl, z, norm 0.029460562393069267 0.001995086669921875 2.552149534225464 0.051985710859298706 0.029220324009656906 3.5390625\n",
            "repr, std, cov, clossl, z, norm 0.03034273162484169 0.0014171600341796875 2.6400718688964844 0.09144055843353271 0.07551468163728714 4.08203125\n",
            "repr, std, cov, clossl, z, norm 0.028218789026141167 0.006397247314453125 2.3399322032928467 0.06572948396205902 0.10042578727006912 3.638671875\n",
            "repr, std, cov, clossl, z, norm 0.028778156265616417 0.00521087646484375 2.422909736633301 0.09481897205114365 0.2553970217704773 3.728515625\n",
            "repr, std, cov, clossl, z, norm 0.029798856005072594 0.002071380615234375 2.5242466926574707 0.08806146681308746 0.05550583824515343 6.890625\n",
            "repr, std, cov, clossl, z, norm 0.02961399406194687 0.0035915374755859375 2.451364040374756 0.07624537497758865 0.12029442936182022 3.189453125\n",
            "repr, std, cov, clossl, z, norm 0.029788436368107796 0.00334930419921875 2.481912136077881 0.07359887659549713 0.294306218624115 4.23046875\n",
            "54\n",
            "repr, std, cov, clossl, z, norm 0.02823265641927719 0.00299072265625 2.4657511711120605 0.0380580760538578 0.24700212478637695 4.04296875\n",
            "repr, std, cov, clossl, z, norm 0.029858244583010674 0.0023956298828125 2.5052146911621094 0.06312645971775055 0.027908582240343094 2.95703125\n",
            "repr, std, cov, clossl, z, norm 0.029888425022363663 0.0048370361328125 2.355099678039551 0.08005032688379288 0.16213828325271606 3.583984375\n",
            "repr, std, cov, clossl, z, norm 0.02784402295947075 0.00624847412109375 2.3832483291625977 0.05708887055516243 0.1829432100057602 3.318359375\n",
            "repr, std, cov, clossl, z, norm 0.02853344939649105 0.00556182861328125 2.414430618286133 0.08328357338905334 0.10191582143306732 3.482421875\n",
            "repr, std, cov, clossl, z, norm 0.029703134670853615 0.002044677734375 2.499629020690918 0.06475120037794113 0.11381028592586517 3.994140625\n",
            "repr, std, cov, clossl, z, norm 0.02981024608016014 0.002017974853515625 2.499398946762085 0.07355333864688873 0.029905028641223907 3.712890625\n",
            "repr, std, cov, clossl, z, norm 0.029234128072857857 0.0035305023193359375 2.375886917114258 0.06721735000610352 0.041947830468416214 4.3671875\n",
            "repr, std, cov, clossl, z, norm 0.0288783498108387 0.0063323974609375 2.344022512435913 0.08260666579008102 0.20144303143024445 3.591796875\n",
            "repr, std, cov, clossl, z, norm 0.028888199478387833 0.0038127899169921875 2.412141799926758 0.07179758697748184 0.29132482409477234 3.384765625\n",
            "repr, std, cov, clossl, z, norm 0.030226677656173706 0.0017757415771484375 2.5564801692962646 0.05198836326599121 0.09422600269317627 3.552734375\n",
            "repr, std, cov, clossl, z, norm 0.02893398143351078 0.0028209686279296875 2.429072141647339 0.06862383335828781 0.1954207569360733 4.90625\n",
            "repr, std, cov, clossl, z, norm 0.02916133776307106 0.003841400146484375 2.419482469558716 0.05565421283245087 0.16806000471115112 4.89453125\n",
            "repr, std, cov, clossl, z, norm 0.028517231345176697 0.00608062744140625 2.337897300720215 0.08432518690824509 0.031060446053743362 3.734375\n",
            "repr, std, cov, clossl, z, norm 0.028623875230550766 0.003437042236328125 2.470672845840454 0.08790288865566254 0.05517346411943436 4.28515625\n",
            "repr, std, cov, clossl, z, norm 0.02939370647072792 0.0019779205322265625 2.4919509887695312 0.050500690937042236 0.02063496969640255 3.701171875\n",
            "55\n",
            "repr, std, cov, clossl, z, norm 0.03056904673576355 0.002063751220703125 2.4853148460388184 0.08613249659538269 0.04448342323303223 3.8359375\n",
            "repr, std, cov, clossl, z, norm 0.02927415259182453 0.002582550048828125 2.4645214080810547 0.043542180210351944 0.0983504056930542 4.7265625\n",
            "repr, std, cov, clossl, z, norm 0.02934509702026844 0.00315093994140625 2.44950008392334 0.06753887981176376 0.15622366964817047 5.109375\n",
            "repr, std, cov, clossl, z, norm 0.02911321260035038 0.006069183349609375 2.2996976375579834 0.035427603870630264 0.044373609125614166 3.494140625\n",
            "repr, std, cov, clossl, z, norm 0.028582848608493805 0.0078125 2.244706153869629 0.07005396485328674 0.30989301204681396 3.212890625\n",
            "repr, std, cov, clossl, z, norm 0.02796025201678276 0.0035877227783203125 2.445704460144043 0.05184337496757507 0.06680280715227127 4.34765625\n",
            "repr, std, cov, clossl, z, norm 0.029226021841168404 0.0022907257080078125 2.4884085655212402 0.0590813010931015 0.10708504170179367 4.11328125\n",
            "repr, std, cov, clossl, z, norm 0.028714703395962715 0.0036773681640625 2.4386889934539795 0.06264989823102951 0.059705574065446854 3.7421875\n",
            "repr, std, cov, clossl, z, norm 0.029545174911618233 0.0010890960693359375 2.566617012023926 0.054982781410217285 0.09132862091064453 3.0859375\n",
            "repr, std, cov, clossl, z, norm 0.028681214898824692 0.00400543212890625 2.3887720108032227 0.06492111831903458 0.0830259695649147 3.9921875\n",
            "repr, std, cov, clossl, z, norm 0.028921974822878838 0.004207611083984375 2.3478190898895264 0.07457456737756729 0.10633520036935806 3.83203125\n",
            "repr, std, cov, clossl, z, norm 0.02822192572057247 0.004901885986328125 2.306412696838379 0.09245797246694565 0.08486158400774002 3.81640625\n",
            "repr, std, cov, clossl, z, norm 0.02644864283502102 0.00743865966796875 2.273829460144043 0.04443821683526039 0.1795727163553238 3.890625\n",
            "repr, std, cov, clossl, z, norm 0.02789304219186306 0.003814697265625 2.404849052429199 0.042724188417196274 0.10140229016542435 3.63671875\n",
            "repr, std, cov, clossl, z, norm 0.032324500381946564 8.153915405273438e-05 2.939478874206543 0.1066620796918869 0.05533113703131676 4.38671875\n",
            "repr, std, cov, clossl, z, norm 0.02962886355817318 0.00044727325439453125 2.733289957046509 0.05690937861800194 0.0903247594833374 3.9140625\n",
            "56\n",
            "repr, std, cov, clossl, z, norm 0.02854624204337597 0.003322601318359375 2.477412700653076 0.1103041023015976 0.015307399444282055 4.453125\n",
            "repr, std, cov, clossl, z, norm 0.028934812173247337 0.006710052490234375 2.3852672576904297 0.06352044641971588 0.13932085037231445 2.837890625\n",
            "repr, std, cov, clossl, z, norm 0.028637956827878952 0.0109100341796875 2.240626573562622 0.07082993537187576 0.022091705352067947 4.0234375\n",
            "repr, std, cov, clossl, z, norm 0.027985282242298126 0.0098114013671875 2.2749223709106445 0.06590323895215988 0.031355854123830795 3.703125\n",
            "repr, std, cov, clossl, z, norm 0.029709409922361374 0.002117156982421875 2.5947344303131104 0.05655491724610329 0.08170025050640106 3.251953125\n",
            "repr, std, cov, clossl, z, norm 0.03004060871899128 0.0007371902465820312 2.6754183769226074 0.053216636180877686 0.01772991567850113 4.078125\n",
            "repr, std, cov, clossl, z, norm 0.03270696476101875 0.0013866424560546875 2.5991601943969727 0.09551971405744553 0.03528768569231033 3.154296875\n",
            "repr, std, cov, clossl, z, norm 0.03005141392350197 0.003124237060546875 2.403372049331665 0.08221552520990372 0.03893914818763733 3.283203125\n",
            "repr, std, cov, clossl, z, norm 0.031058084219694138 0.004886627197265625 2.3731069564819336 0.11192084848880768 0.09753879904747009 3.875\n",
            "repr, std, cov, clossl, z, norm 0.02806912362575531 0.0094146728515625 2.242126226425171 0.052881523966789246 0.06729499995708466 3.646484375\n",
            "repr, std, cov, clossl, z, norm 0.028648236766457558 0.006542205810546875 2.3546674251556396 0.08216117322444916 0.18707175552845 3.486328125\n",
            "repr, std, cov, clossl, z, norm 0.03152129799127579 0.0005512237548828125 2.6513936519622803 0.08801574259996414 0.027153130620718002 3.859375\n",
            "repr, std, cov, clossl, z, norm 0.028842777013778687 0.0014495849609375 2.5787606239318848 0.06311050802469254 0.1119178757071495 3.0078125\n",
            "repr, std, cov, clossl, z, norm 0.028572773560881615 0.002040863037109375 2.568856716156006 0.05952819064259529 0.020920535549521446 3.490234375\n",
            "repr, std, cov, clossl, z, norm 0.027177385985851288 0.00739288330078125 2.3032193183898926 0.03994937986135483 0.0562308244407177 3.150390625\n",
            "repr, std, cov, clossl, z, norm 0.030012797564268112 0.0037937164306640625 2.3695802688598633 0.1318630427122116 0.055645398795604706 3.78125\n",
            "57\n",
            "repr, std, cov, clossl, z, norm 0.029460348188877106 0.0064544677734375 2.2954416275024414 0.052864596247673035 0.1660206913948059 3.30859375\n",
            "repr, std, cov, clossl, z, norm 0.029500402510166168 0.0025177001953125 2.420286178588867 0.057800617069005966 0.12360507994890213 6.4453125\n",
            "repr, std, cov, clossl, z, norm 0.02947419509291649 0.002460479736328125 2.4822983741760254 0.07902540266513824 0.13441114127635956 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.029430007562041283 0.00396728515625 2.4119813442230225 0.05642823874950409 0.00610888609662652 3.275390625\n",
            "repr, std, cov, clossl, z, norm 0.029440458863973618 0.0035858154296875 2.3727598190307617 0.056544262915849686 0.1685623824596405 3.5\n",
            "repr, std, cov, clossl, z, norm 0.029584292322397232 0.002590179443359375 2.4191622734069824 0.08870790898799896 0.24372752010822296 3.921875\n",
            "repr, std, cov, clossl, z, norm 0.029106292873620987 0.003849029541015625 2.386779308319092 0.08103412389755249 0.08752494305372238 3.583984375\n",
            "repr, std, cov, clossl, z, norm 0.029564641416072845 0.00203704833984375 2.522380828857422 0.06132199987769127 0.030128248035907745 3.849609375\n",
            "repr, std, cov, clossl, z, norm 0.02863665670156479 0.00553131103515625 2.2827086448669434 0.049524858593940735 0.04022032022476196 3.68359375\n",
            "repr, std, cov, clossl, z, norm 0.028693269938230515 0.00437164306640625 2.3351070880889893 0.06317911297082901 0.05753348767757416 4.40234375\n",
            "repr, std, cov, clossl, z, norm 0.029822396114468575 0.00431060791015625 2.4063265323638916 0.08224943280220032 0.06646481901407242 2.458984375\n",
            "repr, std, cov, clossl, z, norm 0.029123004525899887 0.00315093994140625 2.4352352619171143 0.10850910097360611 0.13828732073307037 3.94921875\n",
            "repr, std, cov, clossl, z, norm 0.02840612828731537 0.0030612945556640625 2.3990256786346436 0.06141671910881996 0.09451349824666977 3.70703125\n",
            "repr, std, cov, clossl, z, norm 0.03071340173482895 0.0012407302856445312 2.4917330741882324 0.10851848125457764 0.31004825234413147 3.455078125\n",
            "repr, std, cov, clossl, z, norm 0.029561661183834076 0.0016450881958007812 2.521819829940796 0.07234429568052292 0.0941600352525711 3.98828125\n",
            "repr, std, cov, clossl, z, norm 0.03031635656952858 0.00241851806640625 2.395993709564209 0.09629800915718079 0.055952288210392 5.41796875\n",
            "58\n",
            "repr, std, cov, clossl, z, norm 0.02849440462887287 0.0088348388671875 2.1852128505706787 0.04413991793990135 0.11835820227861404 3.587890625\n",
            "repr, std, cov, clossl, z, norm 0.02871006913483143 0.01007843017578125 2.18155574798584 0.05069965869188309 0.13967885076999664 3.861328125\n",
            "repr, std, cov, clossl, z, norm 0.028111888095736504 0.00865936279296875 2.2065486907958984 0.062082502990961075 0.08259475231170654 3.697265625\n",
            "repr, std, cov, clossl, z, norm 0.030566465109586716 0.0014219284057617188 2.566211462020874 0.09078452736139297 0.15604057908058167 3.900390625\n",
            "repr, std, cov, clossl, z, norm 0.029112862423062325 0.00023603439331054688 2.80403995513916 0.04094095155596733 0.05060235783457756 5.34375\n",
            "repr, std, cov, clossl, z, norm 0.02888370491564274 0.0004630088806152344 2.711970329284668 0.14827598631381989 0.061961106956005096 5.0859375\n",
            "repr, std, cov, clossl, z, norm 0.029261069372296333 0.0013971328735351562 2.530150890350342 0.053547557443380356 0.0866939127445221 4.21484375\n",
            "repr, std, cov, clossl, z, norm 0.028198594227433205 0.0039005279541015625 2.357262372970581 0.05817476287484169 0.11191695928573608 3.55078125\n",
            "repr, std, cov, clossl, z, norm 0.027600834146142006 0.01318359375 2.125089645385742 0.02867921255528927 0.13884460926055908 5.7890625\n",
            "repr, std, cov, clossl, z, norm 0.029000742360949516 0.007228851318359375 2.3889811038970947 0.06865879893302917 0.03666166961193085 4.62890625\n",
            "repr, std, cov, clossl, z, norm 0.027925066649913788 0.0076141357421875 2.2971253395080566 0.06646544486284256 0.18531404435634613 3.314453125\n",
            "repr, std, cov, clossl, z, norm 0.03060617856681347 0.0017452239990234375 2.5390658378601074 0.06784134358167648 0.15094545483589172 3.52734375\n",
            "repr, std, cov, clossl, z, norm 0.03030647151172161 0.0009059906005859375 2.584839344024658 0.22122693061828613 0.044264569878578186 6.9375\n",
            "repr, std, cov, clossl, z, norm 0.029452932998538017 0.0007033348083496094 2.5600452423095703 0.04748868942260742 0.1881697177886963 3.8203125\n",
            "repr, std, cov, clossl, z, norm 0.030846141278743744 0.0036144256591796875 2.4601073265075684 0.10123129934072495 0.008183848112821579 3.576171875\n",
            "repr, std, cov, clossl, z, norm 0.027814297005534172 0.00785064697265625 2.3249940872192383 0.061422575265169144 0.2221953123807907 4.5703125\n",
            "59\n",
            "repr, std, cov, clossl, z, norm 0.027371849864721298 0.0049896240234375 2.384345531463623 0.06171557679772377 0.10671583563089371 3.966796875\n",
            "repr, std, cov, clossl, z, norm 0.02788623422384262 0.005519866943359375 2.2640652656555176 0.060579054057598114 0.1329466551542282 3.337890625\n",
            "repr, std, cov, clossl, z, norm 0.028689632192254066 0.0041656494140625 2.355424404144287 0.09208909422159195 0.0776636004447937 4.19140625\n",
            "repr, std, cov, clossl, z, norm 0.028208842501044273 0.0006103515625 2.5804224014282227 0.060747675597667694 0.03395504504442215 3.45703125\n",
            "repr, std, cov, clossl, z, norm 0.029431022703647614 0.0009927749633789062 2.558012008666992 0.09678348153829575 0.1959042102098465 4.19921875\n",
            "repr, std, cov, clossl, z, norm 0.029248623177409172 0.002223968505859375 2.461669445037842 0.09332235157489777 0.02885909378528595 3.64453125\n",
            "repr, std, cov, clossl, z, norm 0.027798883616924286 0.0099639892578125 2.1104230880737305 0.06775485724210739 0.025063207373023033 3.876953125\n",
            "repr, std, cov, clossl, z, norm 0.02690638042986393 0.0054779052734375 2.3572018146514893 0.11510764807462692 0.07478663325309753 3.990234375\n",
            "repr, std, cov, clossl, z, norm 0.027942487969994545 0.002361297607421875 2.4590394496917725 0.112945556640625 0.10098381340503693 3.70703125\n",
            "repr, std, cov, clossl, z, norm 0.028424981981515884 0.002140045166015625 2.3840606212615967 0.07276131957769394 0.05532325431704521 5.0078125\n",
            "repr, std, cov, clossl, z, norm 0.028609389439225197 0.002933502197265625 2.386864185333252 0.05761415511369705 0.06592828780412674 4.33203125\n",
            "repr, std, cov, clossl, z, norm 0.027892308309674263 0.004726409912109375 2.31687331199646 0.06122829392552376 0.027034418657422066 3.98828125\n",
            "repr, std, cov, clossl, z, norm 0.029198579490184784 0.00457000732421875 2.3261306285858154 0.08785402774810791 0.04032372310757637 3.14453125\n",
            "repr, std, cov, clossl, z, norm 0.028001021593809128 0.00626373291015625 2.255707025527954 0.09064546227455139 0.2588425576686859 3.51171875\n",
            "repr, std, cov, clossl, z, norm 0.030158989131450653 0.0008807182312011719 2.476498603820801 0.07880333811044693 0.02909701131284237 3.68359375\n",
            "repr, std, cov, clossl, z, norm 0.029649650678038597 0.001499176025390625 2.438232660293579 0.09539389610290527 0.13217832148075104 4.828125\n",
            "60\n",
            "repr, std, cov, clossl, z, norm 0.026893505826592445 0.0023250579833984375 2.436884880065918 0.05900049954652786 0.0836138129234314 3.330078125\n",
            "repr, std, cov, clossl, z, norm 0.0271589457988739 0.0060882568359375 2.292508602142334 0.13870471715927124 0.03540383279323578 4.03515625\n",
            "repr, std, cov, clossl, z, norm 0.027857575565576553 0.004230499267578125 2.3314013481140137 0.07687413692474365 0.11351917684078217 1.900390625\n",
            "repr, std, cov, clossl, z, norm 0.02765798009932041 0.0074920654296875 2.2517755031585693 0.08230564743280411 0.07859750837087631 2.9765625\n",
            "repr, std, cov, clossl, z, norm 0.02813846245408058 0.00196075439453125 2.4574460983276367 0.06523430347442627 0.01439901813864708 4.00390625\n",
            "repr, std, cov, clossl, z, norm 0.02885713055729866 0.00341033935546875 2.4113972187042236 0.06375319510698318 0.031071951612830162 3.861328125\n",
            "repr, std, cov, clossl, z, norm 0.026882532984018326 0.002605438232421875 2.410630702972412 0.08347226679325104 0.1547911912202835 3.283203125\n",
            "repr, std, cov, clossl, z, norm 0.02820899896323681 0.00298309326171875 2.3278608322143555 0.08145979791879654 0.027953194454312325 4.32421875\n",
            "repr, std, cov, clossl, z, norm 0.0269569531083107 0.00428009033203125 2.3204193115234375 0.05549956113100052 0.09008848667144775 3.27734375\n",
            "repr, std, cov, clossl, z, norm 0.02962612360715866 0.001796722412109375 2.4850800037384033 0.09324745088815689 0.15935419499874115 2.763671875\n",
            "repr, std, cov, clossl, z, norm 0.029594605788588524 0.001190185546875 2.525092601776123 0.0652511864900589 0.04981997609138489 4.125\n",
            "repr, std, cov, clossl, z, norm 0.027438167482614517 0.00507354736328125 2.285598039627075 0.0878882110118866 0.06905074417591095 3.62109375\n",
            "repr, std, cov, clossl, z, norm 0.02696399576961994 0.0093994140625 2.1256301403045654 0.0938657894730568 0.04845615103840828 3.283203125\n",
            "repr, std, cov, clossl, z, norm 0.027982646599411964 0.0026092529296875 2.4357314109802246 0.07320825755596161 0.05401407554745674 3.61328125\n",
            "repr, std, cov, clossl, z, norm 0.027607697993516922 0.003841400146484375 2.3435611724853516 0.06299353390932083 0.11593123525381088 4.52734375\n",
            "repr, std, cov, clossl, z, norm 0.028608908876776695 0.0015172958374023438 2.3413383960723877 0.07777222990989685 0.08625883609056473 3.76953125\n",
            "61\n",
            "repr, std, cov, clossl, z, norm 0.02928824909031391 0.00191497802734375 2.399750232696533 0.06939852982759476 0.054927386343479156 4.1640625\n",
            "repr, std, cov, clossl, z, norm 0.028717517852783203 0.001934051513671875 2.3935279846191406 0.05327926576137543 0.0907355546951294 4.171875\n",
            "repr, std, cov, clossl, z, norm 0.026494644582271576 0.005096435546875 2.2868547439575195 0.04227086901664734 0.0798579677939415 8.2890625\n",
            "repr, std, cov, clossl, z, norm 0.02692505344748497 0.004856109619140625 2.2880072593688965 0.05487833544611931 0.020600974559783936 4.38671875\n",
            "repr, std, cov, clossl, z, norm 0.027468444779515266 0.003963470458984375 2.3448877334594727 0.04811292141675949 0.10023466497659683 3.5234375\n",
            "repr, std, cov, clossl, z, norm 0.027152789756655693 0.004177093505859375 2.3156626224517822 0.059061333537101746 0.06658603996038437 4.05078125\n",
            "repr, std, cov, clossl, z, norm 0.027300650253891945 0.0008249282836914062 2.543893337249756 0.05258411914110184 0.08457856625318527 4.20703125\n",
            "repr, std, cov, clossl, z, norm 0.028856245800852776 0.001739501953125 2.480339527130127 0.08681076020002365 0.1504378765821457 9.140625\n",
            "repr, std, cov, clossl, z, norm 0.026261411607265472 0.005054473876953125 2.259774923324585 0.0328458771109581 0.036291178315877914 3.32421875\n",
            "repr, std, cov, clossl, z, norm 0.0277827400714159 0.0068511962890625 2.194180488586426 0.11257284879684448 0.08693070709705353 3.330078125\n",
            "repr, std, cov, clossl, z, norm 0.029608692973852158 0.003070831298828125 2.4134387969970703 0.07452142983675003 0.2090398222208023 4.40234375\n",
            "repr, std, cov, clossl, z, norm 0.028834542259573936 0.00212860107421875 2.3886828422546387 0.09992922842502594 0.4244275987148285 2.970703125\n",
            "repr, std, cov, clossl, z, norm 0.029578672721982002 0.002155303955078125 2.417619228363037 0.07472090423107147 0.18183574080467224 4.19921875\n",
            "repr, std, cov, clossl, z, norm 0.029033055528998375 0.0034008026123046875 2.2372422218322754 0.0566224567592144 0.12366659194231033 4.265625\n",
            "repr, std, cov, clossl, z, norm 0.027942420914769173 0.0054473876953125 2.248054027557373 0.06501194089651108 0.09091030061244965 3.140625\n",
            "repr, std, cov, clossl, z, norm 0.029152601957321167 0.004894256591796875 2.3285648822784424 0.08722558617591858 0.07991413027048111 2.970703125\n",
            "62\n",
            "repr, std, cov, clossl, z, norm 0.028007669374346733 0.00212860107421875 2.4184153079986572 0.04990270361304283 0.08531463891267776 4.5625\n",
            "repr, std, cov, clossl, z, norm 0.027306312695145607 0.00460052490234375 2.293659210205078 0.06358025223016739 0.04728784039616585 4.05859375\n",
            "repr, std, cov, clossl, z, norm 0.02785458043217659 0.0032787322998046875 2.3489112854003906 0.05328569933772087 0.07472854107618332 4.37890625\n",
            "repr, std, cov, clossl, z, norm 0.027483055368065834 0.0021800994873046875 2.4005942344665527 0.0557403638958931 0.16003450751304626 4.66015625\n",
            "repr, std, cov, clossl, z, norm 0.02773570641875267 0.0032196044921875 2.3436691761016846 0.08393203467130661 0.0 3.494140625\n",
            "repr, std, cov, clossl, z, norm 0.027417398989200592 0.004398345947265625 2.271772861480713 0.05276436731219292 0.11401040852069855 3.427734375\n",
            "repr, std, cov, clossl, z, norm 0.027615046128630638 0.0027942657470703125 2.3291187286376953 0.04687817394733429 0.08129893988370895 4.08984375\n",
            "repr, std, cov, clossl, z, norm 0.0274973101913929 0.002910614013671875 2.3482041358947754 0.04722502455115318 0.11442066729068756 3.615234375\n",
            "repr, std, cov, clossl, z, norm 0.026771223172545433 0.005146026611328125 2.2022786140441895 0.043961022049188614 0.12745662033557892 3.080078125\n",
            "repr, std, cov, clossl, z, norm 0.02619588002562523 0.0036144256591796875 2.2668251991271973 0.04805667698383331 0.03077072836458683 4.6328125\n",
            "repr, std, cov, clossl, z, norm 0.027439044788479805 0.0021572113037109375 2.2916030883789062 0.049395132809877396 0.1385388970375061 5.21875\n",
            "repr, std, cov, clossl, z, norm 0.028756853193044662 0.001926422119140625 2.343928575515747 0.0828225240111351 0.19994589686393738 3.96875\n",
            "repr, std, cov, clossl, z, norm 0.02923455461859703 0.000782012939453125 2.4590377807617188 0.09839687496423721 0.0731976330280304 3.369140625\n",
            "repr, std, cov, clossl, z, norm 0.025937441736459732 0.005702972412109375 2.1862800121307373 0.04389296844601631 0.25973424315452576 3.568359375\n",
            "repr, std, cov, clossl, z, norm 0.02810053341090679 0.0037593841552734375 2.2360243797302246 0.07501942664384842 0.09678608179092407 3.146484375\n",
            "repr, std, cov, clossl, z, norm 0.027416296303272247 0.00537872314453125 2.20771861076355 0.03862025961279869 0.0870765820145607 4.35546875\n",
            "63\n",
            "repr, std, cov, clossl, z, norm 0.028385939076542854 0.0016536712646484375 2.357222080230713 0.056498054414987564 0.17327351868152618 2.5390625\n",
            "repr, std, cov, clossl, z, norm 0.029496030882000923 0.001071929931640625 2.4483985900878906 0.04314390942454338 0.0869184136390686 7.53515625\n",
            "repr, std, cov, clossl, z, norm 0.028567509725689888 0.0033206939697265625 2.2424113750457764 0.058627910912036896 0.14990226924419403 3.55078125\n",
            "repr, std, cov, clossl, z, norm 0.026226520538330078 0.0120391845703125 2.0216870307922363 0.0750620886683464 0.1400514394044876 3.40234375\n",
            "repr, std, cov, clossl, z, norm 0.02770218811929226 0.002819061279296875 2.300321102142334 0.058615099638700485 0.11499180644750595 3.458984375\n",
            "repr, std, cov, clossl, z, norm 0.0268346406519413 0.003726959228515625 2.3218727111816406 0.02868450991809368 0.07731965184211731 4.53125\n",
            "repr, std, cov, clossl, z, norm 0.027643488720059395 0.00164031982421875 2.3881630897521973 0.12774063646793365 0.001035869587212801 2.96484375\n",
            "repr, std, cov, clossl, z, norm 0.028759924694895744 0.00112152099609375 2.395205497741699 0.06016726791858673 0.12412223219871521 4.03515625\n",
            "repr, std, cov, clossl, z, norm 0.028039727360010147 0.0015668869018554688 2.471440315246582 0.08053849637508392 0.0998181402683258 4.34765625\n",
            "repr, std, cov, clossl, z, norm 0.026196109130978584 0.00743865966796875 2.1867566108703613 0.04800639674067497 0.0590631477534771 4.25390625\n",
            "repr, std, cov, clossl, z, norm 0.027041077613830566 0.0033779144287109375 2.312063694000244 0.05197139084339142 0.2256084829568863 3.51953125\n",
            "repr, std, cov, clossl, z, norm 0.027442382648587227 0.0035724639892578125 2.266530990600586 0.06387963891029358 0.03481614217162132 3.279296875\n",
            "repr, std, cov, clossl, z, norm 0.026899423450231552 0.009124755859375 2.0754871368408203 0.05966026335954666 0.059632617980241776 3.064453125\n",
            "repr, std, cov, clossl, z, norm 0.028086036443710327 0.0011844635009765625 2.418117046356201 0.06634913384914398 0.1466287076473236 4.078125\n",
            "repr, std, cov, clossl, z, norm 0.027895307168364525 0.0006351470947265625 2.43971586227417 0.05031175538897514 0.14525458216667175 4.1953125\n",
            "repr, std, cov, clossl, z, norm 0.027355724945664406 0.002696990966796875 2.2937543392181396 0.055910032242536545 0.09243829548358917 3.943359375\n",
            "64\n",
            "repr, std, cov, clossl, z, norm 0.02684210240840912 0.002838134765625 2.349264621734619 0.037414588034152985 0.10638806968927383 3.83203125\n",
            "repr, std, cov, clossl, z, norm 0.02768879383802414 0.004711151123046875 2.2897067070007324 0.06761250644922256 0.0842634066939354 3.62890625\n",
            "repr, std, cov, clossl, z, norm 0.026461858302354813 0.005157470703125 2.205962657928467 0.0578506737947464 0.13890288770198822 3.3203125\n",
            "repr, std, cov, clossl, z, norm 0.02870291657745838 0.00246429443359375 2.25716495513916 0.07920889556407928 0.10994902998209 3.513671875\n",
            "repr, std, cov, clossl, z, norm 0.02922133356332779 0.001255035400390625 2.389303684234619 0.08077424764633179 0.07479867339134216 4.4921875\n",
            "repr, std, cov, clossl, z, norm 0.02863571047782898 0.001972198486328125 2.3442282676696777 0.07171882688999176 0.07434085011482239 4.3203125\n",
            "repr, std, cov, clossl, z, norm 0.02647925540804863 0.00501251220703125 2.1834897994995117 0.05614311248064041 0.1072101891040802 3.38671875\n",
            "repr, std, cov, clossl, z, norm 0.027181286364793777 0.0028858184814453125 2.279172420501709 0.03948577493429184 0.08313693851232529 3.845703125\n",
            "repr, std, cov, clossl, z, norm 0.02654697746038437 0.010284423828125 2.0411505699157715 0.05566975474357605 0.012186369858682156 2.916015625\n",
            "repr, std, cov, clossl, z, norm 0.026681235060095787 0.0022563934326171875 2.3355064392089844 0.03209541365504265 0.17337124049663544 3.01953125\n",
            "repr, std, cov, clossl, z, norm 0.027720317244529724 0.0007672309875488281 2.4508023262023926 0.04067186638712883 0.2312205731868744 3.77734375\n",
            "repr, std, cov, clossl, z, norm 0.02785903960466385 0.0017948150634765625 2.32797908782959 0.05863785743713379 0.23791217803955078 3.60546875\n",
            "repr, std, cov, clossl, z, norm 0.026326192542910576 0.00273895263671875 2.2779488563537598 0.05836201459169388 0.12147340923547745 3.755859375\n",
            "repr, std, cov, clossl, z, norm 0.02655787579715252 0.00232696533203125 2.2553935050964355 0.05477108061313629 0.07453568279743195 3.201171875\n",
            "repr, std, cov, clossl, z, norm 0.02624800056219101 0.005462646484375 2.195765256881714 0.050337404012680054 0.10901299864053726 3.912109375\n",
            "repr, std, cov, clossl, z, norm 0.027079695835709572 0.003894805908203125 2.2380735874176025 0.05825211852788925 0.033372875303030014 3.53125\n",
            "65\n",
            "repr, std, cov, clossl, z, norm 0.025745855644345284 0.00664520263671875 2.1363511085510254 0.06604666262865067 0.08072026073932648 2.861328125\n",
            "repr, std, cov, clossl, z, norm 0.026556916534900665 0.00151824951171875 2.364731550216675 0.03816249221563339 0.01990089938044548 3.693359375\n",
            "repr, std, cov, clossl, z, norm 0.028026573359966278 0.0006923675537109375 2.494734764099121 0.04326663911342621 0.1537911295890808 3.701171875\n",
            "repr, std, cov, clossl, z, norm 0.027002325281500816 0.0022220611572265625 2.280477523803711 0.059066884219646454 0.06661088764667511 4.70703125\n",
            "repr, std, cov, clossl, z, norm 0.027192195877432823 0.002544403076171875 2.2701761722564697 0.08331649750471115 0.19094794988632202 3.884765625\n",
            "repr, std, cov, clossl, z, norm 0.025630904361605644 0.00571441650390625 2.150087356567383 0.03775526583194733 0.07157647609710693 3.15625\n",
            "repr, std, cov, clossl, z, norm 0.026954280212521553 0.006580352783203125 2.1828527450561523 0.055253610014915466 0.1839735209941864 3.91796875\n",
            "repr, std, cov, clossl, z, norm 0.02617950364947319 0.0030364990234375 2.2605748176574707 0.038895606994628906 0.10877056419849396 2.91796875\n",
            "repr, std, cov, clossl, z, norm 0.028287474066019058 0.0019893646240234375 2.29715633392334 0.10428489744663239 0.0 3.599609375\n",
            "repr, std, cov, clossl, z, norm 0.027371348813176155 0.0010204315185546875 2.4035749435424805 0.020655840635299683 0.10956472158432007 4.4453125\n",
            "repr, std, cov, clossl, z, norm 0.02716267853975296 0.0027904510498046875 2.2455687522888184 0.040428731590509415 0.12012466043233871 3.0\n",
            "repr, std, cov, clossl, z, norm 0.028047017753124237 0.00199127197265625 2.32731556892395 0.07084029167890549 0.02589605189859867 5.36328125\n",
            "repr, std, cov, clossl, z, norm 0.02766023389995098 0.00514984130859375 2.188535213470459 0.11151523143053055 0.05877859145402908 3.275390625\n",
            "repr, std, cov, clossl, z, norm 0.02733142301440239 0.00489044189453125 2.1682534217834473 0.05457720533013344 0.08091120421886444 3.556640625\n",
            "repr, std, cov, clossl, z, norm 0.028136281296610832 0.00418853759765625 2.1576104164123535 0.06582652777433395 0.08374036848545074 3.865234375\n",
            "repr, std, cov, clossl, z, norm 0.02730526775121689 0.0023956298828125 2.2583203315734863 0.13645820319652557 0.043635547161102295 4.015625\n",
            "66\n",
            "repr, std, cov, clossl, z, norm 0.02960590459406376 0.0004429817199707031 2.4893136024475098 0.047699715942144394 0.0776231586933136 3.794921875\n",
            "repr, std, cov, clossl, z, norm 0.02802766114473343 0.0008511543273925781 2.4241390228271484 0.05094961076974869 0.03755951672792435 4.78515625\n",
            "repr, std, cov, clossl, z, norm 0.026440085843205452 0.00360870361328125 2.2112927436828613 0.03450361639261246 0.15197163820266724 2.90234375\n",
            "repr, std, cov, clossl, z, norm 0.030226783826947212 0.002777099609375 2.2430853843688965 0.08785991370677948 0.28762975335121155 4.296875\n",
            "repr, std, cov, clossl, z, norm 0.0274162907153368 0.01277923583984375 1.9870412349700928 0.0671529695391655 0.1117408499121666 3.876953125\n",
            "repr, std, cov, clossl, z, norm 0.025058656930923462 0.014923095703125 1.9949109554290771 0.05834624171257019 0.05927311256527901 4.03125\n",
            "repr, std, cov, clossl, z, norm 0.028485609218478203 0.001277923583984375 2.3812615871429443 0.06217123940587044 0.13792623579502106 3.771484375\n",
            "repr, std, cov, clossl, z, norm 0.030330723151564598 0.0001163482666015625 2.6065139770507812 0.06249753758311272 0.09855595231056213 3.69140625\n",
            "repr, std, cov, clossl, z, norm 0.029242459684610367 0.0004324913024902344 2.454380512237549 0.08376257866621017 0.1342434585094452 4.3671875\n",
            "repr, std, cov, clossl, z, norm 0.028341788798570633 0.0013151168823242188 2.428354263305664 0.056556299328804016 0.0425456240773201 3.7890625\n",
            "repr, std, cov, clossl, z, norm 0.02777235023677349 0.0044403076171875 2.1544880867004395 0.07150869816541672 0.21628141403198242 3.623046875\n",
            "repr, std, cov, clossl, z, norm 0.026426570490002632 0.0073089599609375 2.096353530883789 0.03955535963177681 0.1212485209107399 3.751953125\n",
            "repr, std, cov, clossl, z, norm 0.02683981880545616 0.0079803466796875 2.0804080963134766 0.0459294393658638 0.1674395203590393 3.634765625\n",
            "repr, std, cov, clossl, z, norm 0.027850965037941933 0.001800537109375 2.298962354660034 0.07595028728246689 0.10961311310529709 4.33984375\n",
            "repr, std, cov, clossl, z, norm 0.028292778879404068 0.00501251220703125 2.1676101684570312 0.0760604664683342 0.07449295371770859 3.69140625\n",
            "repr, std, cov, clossl, z, norm 0.028970476239919662 0.0007867813110351562 2.4187355041503906 0.0631609782576561 0.14047624170780182 2.958984375\n",
            "67\n",
            "repr, std, cov, clossl, z, norm 0.029194632545113564 0.0008306503295898438 2.3531785011291504 0.05898505076766014 0.09735395014286041 3.603515625\n",
            "repr, std, cov, clossl, z, norm 0.0277843177318573 0.002269744873046875 2.2217297554016113 0.05266473814845085 0.015547542832791805 3.337890625\n",
            "repr, std, cov, clossl, z, norm 0.026757482439279556 0.003589630126953125 2.24544620513916 0.06285647302865982 0.08264805376529694 3.919921875\n",
            "repr, std, cov, clossl, z, norm 0.02719639241695404 0.0031261444091796875 2.2186131477355957 0.07119740545749664 0.0987490862607956 3.5078125\n",
            "repr, std, cov, clossl, z, norm 0.0272279754281044 0.0048828125 2.163261651992798 0.07095051556825638 0.10358278453350067 3.318359375\n",
            "repr, std, cov, clossl, z, norm 0.027804741635918617 0.00482940673828125 2.128333806991577 0.06189041957259178 0.11500175297260284 3.642578125\n",
            "repr, std, cov, clossl, z, norm 0.02622128278017044 0.004909515380859375 2.1605725288391113 0.04053466394543648 0.1014290526509285 4.08203125\n",
            "repr, std, cov, clossl, z, norm 0.028814898803830147 0.001178741455078125 2.3546347618103027 0.05202506482601166 0.0374605692923069 3.45703125\n",
            "repr, std, cov, clossl, z, norm 0.027516135945916176 0.0015134811401367188 2.2869956493377686 0.04162871092557907 0.2771485149860382 3.400390625\n",
            "repr, std, cov, clossl, z, norm 0.02828965336084366 0.002414703369140625 2.2825536727905273 0.055083293467760086 0.03751958906650543 3.56640625\n",
            "repr, std, cov, clossl, z, norm 0.025232084095478058 0.006000518798828125 2.1766672134399414 0.03906994313001633 0.07347927242517471 3.31640625\n",
            "repr, std, cov, clossl, z, norm 0.0273645780980587 0.0037975311279296875 2.221740961074829 0.06350500881671906 0.09921403229236603 3.26171875\n",
            "repr, std, cov, clossl, z, norm 0.027429981157183647 0.0019969940185546875 2.2552452087402344 0.052273109555244446 0.2080901563167572 4.4609375\n",
            "repr, std, cov, clossl, z, norm 0.02554805390536785 0.003387451171875 2.17496395111084 0.03652523458003998 0.014536162838339806 3.994140625\n",
            "repr, std, cov, clossl, z, norm 0.028016991913318634 0.0014553070068359375 2.319978713989258 0.04231991991400719 0.1243394985795021 2.42578125\n",
            "repr, std, cov, clossl, z, norm 0.026592569425702095 0.002704620361328125 2.2582340240478516 0.041835181415081024 0.1315288543701172 3.349609375\n",
            "68\n",
            "repr, std, cov, clossl, z, norm 0.02746601589024067 0.004055023193359375 2.1595747470855713 0.0715096965432167 0.11249402910470963 4.43359375\n",
            "repr, std, cov, clossl, z, norm 0.025358038023114204 0.0078582763671875 2.0378365516662598 0.048194851726293564 0.007037516683340073 4.0078125\n",
            "repr, std, cov, clossl, z, norm 0.028045333921909332 0.0009284019470214844 2.349027156829834 0.04252642020583153 0.10748004168272018 3.2734375\n",
            "repr, std, cov, clossl, z, norm 0.029585083946585655 0.0007238388061523438 2.382049798965454 0.06830669194459915 0.1530502736568451 3.8671875\n",
            "repr, std, cov, clossl, z, norm 0.027753451839089394 0.001270294189453125 2.3024024963378906 0.05182202160358429 0.06806305795907974 3.564453125\n",
            "repr, std, cov, clossl, z, norm 0.027535192668437958 0.003131866455078125 2.217024564743042 0.03417307883501053 0.08552580326795578 3.876953125\n",
            "repr, std, cov, clossl, z, norm 0.026177722960710526 0.0094146728515625 2.0226097106933594 0.04393882676959038 0.021710172295570374 3.576171875\n",
            "repr, std, cov, clossl, z, norm 0.027135508134961128 0.00493621826171875 2.1182069778442383 0.06047840043902397 0.14749424159526825 3.12109375\n",
            "repr, std, cov, clossl, z, norm 0.0262814462184906 0.00225067138671875 2.2549405097961426 0.051685646176338196 0.14155162870883942 4.015625\n",
            "repr, std, cov, clossl, z, norm 0.026562318205833435 0.0023651123046875 2.1996614933013916 0.03689095377922058 0.14186616241931915 3.16796875\n",
            "repr, std, cov, clossl, z, norm 0.02616589516401291 0.0019292831420898438 2.3314552307128906 0.030131254345178604 0.07487456500530243 4.33984375\n",
            "repr, std, cov, clossl, z, norm 0.027333782985806465 0.0014629364013671875 2.276106834411621 0.04215208813548088 0.14693523943424225 4.1328125\n",
            "repr, std, cov, clossl, z, norm 0.026764720678329468 0.004695892333984375 2.122990369796753 0.045864563435316086 0.04527603089809418 3.544921875\n",
            "repr, std, cov, clossl, z, norm 0.02570517547428608 0.004344940185546875 2.1600022315979004 0.04699380695819855 0.26261037588119507 3.78125\n",
            "repr, std, cov, clossl, z, norm 0.027145765721797943 0.0030384063720703125 2.1882078647613525 0.0816112607717514 0.18093572556972504 2.501953125\n",
            "repr, std, cov, clossl, z, norm 0.02555658482015133 0.0027618408203125 2.2427706718444824 0.033399876207113266 0.029447052627801895 3.923828125\n",
            "69\n",
            "repr, std, cov, clossl, z, norm 0.02761617675423622 0.0007548332214355469 2.376004219055176 0.0763077363371849 0.13800694048404694 3.892578125\n",
            "repr, std, cov, clossl, z, norm 0.026148494333028793 0.002079010009765625 2.2405600547790527 0.023510398343205452 0.11577334254980087 3.201171875\n",
            "repr, std, cov, clossl, z, norm 0.027384022250771523 0.004146575927734375 2.137293815612793 0.05009983852505684 0.16564978659152985 4.53125\n",
            "repr, std, cov, clossl, z, norm 0.02641097456216812 0.004428863525390625 2.085723400115967 0.037016935646533966 0.04682043567299843 4.390625\n",
            "repr, std, cov, clossl, z, norm 0.026627693325281143 0.002532958984375 2.220090866088867 0.06996813416481018 0.07476956397294998 3.48046875\n",
            "repr, std, cov, clossl, z, norm 0.02781798131763935 0.00316619873046875 2.1906497478485107 0.044706277549266815 0.07068661600351334 4.80859375\n",
            "repr, std, cov, clossl, z, norm 0.02690753899514675 0.0023021697998046875 2.163975238800049 0.06678778678178787 0.10331239551305771 4.0546875\n",
            "repr, std, cov, clossl, z, norm 0.029383644461631775 0.0020313262939453125 2.313971519470215 0.07663343101739883 0.053565677255392075 3.919921875\n",
            "repr, std, cov, clossl, z, norm 0.027985133230686188 0.0025424957275390625 2.196826457977295 0.05829312652349472 0.082258440554142 3.134765625\n",
            "repr, std, cov, clossl, z, norm 0.027722887694835663 0.00276947021484375 2.1320486068725586 0.05446092411875725 0.1397053599357605 4.046875\n",
            "repr, std, cov, clossl, z, norm 0.026674512773752213 0.006053924560546875 2.045681953430176 0.06513716280460358 0.024088827893137932 3.32421875\n",
            "repr, std, cov, clossl, z, norm 0.0270516537129879 0.0045928955078125 2.0949547290802 0.05817388370633125 0.2330200970172882 7.37109375\n",
            "repr, std, cov, clossl, z, norm 0.02700827084481716 0.00458526611328125 2.1990456581115723 0.08990519493818283 0.06443676352500916 3.943359375\n",
            "repr, std, cov, clossl, z, norm 0.027124637737870216 0.0013103485107421875 2.3393993377685547 0.03800024464726448 0.20687122642993927 3.36328125\n",
            "repr, std, cov, clossl, z, norm 0.025946499779820442 0.001934051513671875 2.2352778911590576 0.04615500941872597 0.15109053254127502 5.0078125\n",
            "repr, std, cov, clossl, z, norm 0.02662028931081295 0.001033782958984375 2.393282890319824 0.036652822047472 0.09056924283504486 4.37890625\n",
            "70\n",
            "repr, std, cov, clossl, z, norm 0.026202045381069183 0.0033206939697265625 2.156691551208496 0.044257752597332 0.1385699361562729 3.90625\n",
            "repr, std, cov, clossl, z, norm 0.026000475510954857 0.006374359130859375 2.0500924587249756 0.0742400661110878 0.108096182346344 3.1328125\n",
            "repr, std, cov, clossl, z, norm 0.025901319459080696 0.00620269775390625 2.074483633041382 0.049007758498191833 0.09752900898456573 4.21875\n",
            "repr, std, cov, clossl, z, norm 0.02703247033059597 0.0014677047729492188 2.2966480255126953 0.04792816564440727 0.14699476957321167 3.755859375\n",
            "repr, std, cov, clossl, z, norm 0.025469578802585602 0.004505157470703125 2.0932295322418213 0.051490385085344315 0.25837352871894836 3.94140625\n",
            "repr, std, cov, clossl, z, norm 0.027520213276147842 0.0013942718505859375 2.316272735595703 0.05059557780623436 0.0474688783288002 3.85546875\n",
            "repr, std, cov, clossl, z, norm 0.027491385117173195 0.00023508071899414062 2.4733009338378906 0.039314307272434235 0.17557118833065033 4.80078125\n",
            "repr, std, cov, clossl, z, norm 0.02752547152340412 0.00154876708984375 2.2674355506896973 0.04953896999359131 0.1667722910642624 3.671875\n",
            "repr, std, cov, clossl, z, norm 0.026202654466032982 0.00795745849609375 2.0275237560272217 0.07877986878156662 0.054013121873140335 3.84765625\n",
            "repr, std, cov, clossl, z, norm 0.02535874769091606 0.00972747802734375 2.0146689414978027 0.06089356169104576 0.007921900600194931 4.09375\n",
            "repr, std, cov, clossl, z, norm 0.026960767805576324 0.0026950836181640625 2.2461891174316406 0.07092192769050598 0.10477472096681595 3.048828125\n",
            "repr, std, cov, clossl, z, norm 0.027257898822426796 0.002368927001953125 2.2042629718780518 0.048653144389390945 0.09300197660923004 4.30078125\n",
            "repr, std, cov, clossl, z, norm 0.026679381728172302 0.00133514404296875 2.2521915435791016 0.04908674955368042 0.10126946121454239 4.453125\n",
            "repr, std, cov, clossl, z, norm 0.028113940730690956 0.001033782958984375 2.352566719055176 0.06612002849578857 0.15906117856502533 4.91015625\n",
            "repr, std, cov, clossl, z, norm 0.02731899917125702 0.0024433135986328125 2.2292165756225586 0.050055503845214844 0.06520939618349075 3.76171875\n",
            "repr, std, cov, clossl, z, norm 0.02619408629834652 0.00643157958984375 2.078601360321045 0.031756505370140076 0.012935176491737366 3.455078125\n",
            "71\n",
            "repr, std, cov, clossl, z, norm 0.025931037962436676 0.0057525634765625 2.041823625564575 0.04630092531442642 0.033633578568696976 2.935546875\n",
            "repr, std, cov, clossl, z, norm 0.02601781114935875 0.003509521484375 2.185790777206421 0.037829022854566574 0.12154526263475418 4.00390625\n",
            "repr, std, cov, clossl, z, norm 0.027480993419885635 0.0025882720947265625 2.1831436157226562 0.06815637648105621 0.07044112682342529 3.78125\n",
            "repr, std, cov, clossl, z, norm 0.02623894065618515 0.001392364501953125 2.2997567653656006 0.03401409462094307 0.12361110746860504 3.19140625\n",
            "repr, std, cov, clossl, z, norm 0.026574138551950455 0.005374908447265625 2.0781946182250977 0.09511285275220871 0.13313569128513336 3.45703125\n",
            "repr, std, cov, clossl, z, norm 0.026598874479532242 0.0013599395751953125 2.2387070655822754 0.06596454977989197 0.16040430963039398 4.0625\n",
            "repr, std, cov, clossl, z, norm 0.02675173245370388 0.00278472900390625 2.1736042499542236 0.029723314568400383 0.07755614072084427 4.5546875\n",
            "repr, std, cov, clossl, z, norm 0.025811852887272835 0.00240325927734375 2.173736333847046 0.047745317220687866 0.06198614463210106 3.111328125\n",
            "repr, std, cov, clossl, z, norm 0.02606230601668358 0.00238800048828125 2.18703031539917 0.033157844096422195 0.05566677823662758 3.259765625\n",
            "repr, std, cov, clossl, z, norm 0.026260443031787872 0.009307861328125 1.9349384307861328 0.10467685759067535 0.14286021888256073 3.78125\n",
            "repr, std, cov, clossl, z, norm 0.02766714058816433 0.0009665489196777344 2.3214333057403564 0.06219150498509407 0.07709681987762451 3.498046875\n",
            "repr, std, cov, clossl, z, norm 0.026634318754076958 0.0008835792541503906 2.362833023071289 0.03669413551688194 0.13116253912448883 4.3125\n",
            "repr, std, cov, clossl, z, norm 0.026784637942910194 0.0024127960205078125 2.1825780868530273 0.03829994425177574 0.05017431080341339 4.265625\n",
            "repr, std, cov, clossl, z, norm 0.02654893882572651 0.00510406494140625 2.0621652603149414 0.06464973092079163 0.03424222767353058 3.48828125\n",
            "repr, std, cov, clossl, z, norm 0.027374539524316788 0.0035610198974609375 2.1156277656555176 0.08038930594921112 0.10623956471681595 2.51171875\n",
            "repr, std, cov, clossl, z, norm 0.025621775537729263 0.003398895263671875 2.1133830547332764 0.0845685750246048 0.15308910608291626 3.169921875\n",
            "72\n",
            "repr, std, cov, clossl, z, norm 0.026354197412729263 0.00264739990234375 2.164661169052124 0.06733953952789307 0.056600309908390045 3.693359375\n",
            "repr, std, cov, clossl, z, norm 0.026352867484092712 0.00168609619140625 2.263633966445923 0.04780520498752594 0.043316442519426346 3.53515625\n",
            "repr, std, cov, clossl, z, norm 0.026171674951910973 0.0036296844482421875 2.1300485134124756 0.059602756053209305 0.11404136568307877 4.16015625\n",
            "repr, std, cov, clossl, z, norm 0.026102997362613678 0.006809234619140625 1.982651948928833 0.061246976256370544 0.11147775501012802 6.703125\n",
            "repr, std, cov, clossl, z, norm 0.027626246213912964 0.0014324188232421875 2.271063804626465 0.06957269459962845 0.02379368059337139 4.5\n",
            "repr, std, cov, clossl, z, norm 0.02679530717432499 0.002086639404296875 2.1705265045166016 0.035289015620946884 0.10007189214229584 3.853515625\n",
            "repr, std, cov, clossl, z, norm 0.027932867407798767 0.0009918212890625 2.296473503112793 0.06591010838747025 0.214122474193573 3.185546875\n",
            "repr, std, cov, clossl, z, norm 0.02561984956264496 0.00287628173828125 2.2054803371429443 0.05716857686638832 0.12325984239578247 3.4609375\n",
            "repr, std, cov, clossl, z, norm 0.0259406678378582 0.00376129150390625 2.114434242248535 0.04423584043979645 0.018990498036146164 4.390625\n",
            "repr, std, cov, clossl, z, norm 0.026202939450740814 0.005046844482421875 2.066470146179199 0.0715385153889656 0.13170035183429718 3.037109375\n",
            "repr, std, cov, clossl, z, norm 0.024738430976867676 0.00862884521484375 1.9569069147109985 0.036844104528427124 0.11752941459417343 4.01953125\n",
            "repr, std, cov, clossl, z, norm 0.026094160974025726 0.0032176971435546875 2.1643521785736084 0.10098208487033844 0.16677740216255188 4.3203125\n",
            "repr, std, cov, clossl, z, norm 0.026367660611867905 0.0009584426879882812 2.377878427505493 0.04013138636946678 0.23158401250839233 4.28125\n",
            "repr, std, cov, clossl, z, norm 0.026882819831371307 0.00016164779663085938 2.4865293502807617 0.06187839433550835 0.04122968390583992 3.4296875\n",
            "repr, std, cov, clossl, z, norm 0.02683284506201744 0.00092315673828125 2.287520170211792 0.05655365809798241 0.11843808740377426 3.05859375\n",
            "repr, std, cov, clossl, z, norm 0.0246858112514019 0.005153656005859375 2.0906786918640137 0.04349339380860329 0.11158248782157898 2.419921875\n",
            "73\n",
            "repr, std, cov, clossl, z, norm 0.025262607261538506 0.00423431396484375 2.067120313644409 0.03520690277218819 0.052565816789865494 3.55859375\n",
            "repr, std, cov, clossl, z, norm 0.02573753334581852 0.00868988037109375 2.0061683654785156 0.05330906808376312 0.138812854886055 4.0078125\n",
            "repr, std, cov, clossl, z, norm 0.026907257735729218 0.0019512176513671875 2.2690844535827637 0.05026884004473686 0.04785808548331261 3.888671875\n",
            "repr, std, cov, clossl, z, norm 0.025778770446777344 0.0018596649169921875 2.2471041679382324 0.028132136911153793 0.07056128978729248 2.71484375\n",
            "repr, std, cov, clossl, z, norm 0.026463773101568222 0.00279998779296875 2.2163901329040527 0.04983838275074959 0.057323433458805084 4.00390625\n",
            "repr, std, cov, clossl, z, norm 0.026232628151774406 0.0013713836669921875 2.251492738723755 0.05256704241037369 0.09338939189910889 4.16796875\n",
            "repr, std, cov, clossl, z, norm 0.025901157408952713 0.00278472900390625 2.1498045921325684 0.030278649181127548 0.1353873908519745 4.00390625\n",
            "repr, std, cov, clossl, z, norm 0.025588903576135635 0.00403594970703125 2.0791516304016113 0.06186098977923393 0.07122880220413208 3.037109375\n",
            "repr, std, cov, clossl, z, norm 0.02559971995651722 0.003719329833984375 2.1129770278930664 0.05947321280837059 0.14148399233818054 3.65234375\n",
            "repr, std, cov, clossl, z, norm 0.025141596794128418 0.002197265625 2.172661066055298 0.034011129289865494 0.08750936388969421 3.908203125\n",
            "repr, std, cov, clossl, z, norm 0.02617608569562435 0.00356292724609375 2.0793418884277344 0.06469643115997314 0.04168647155165672 3.923828125\n",
            "repr, std, cov, clossl, z, norm 0.025208940729498863 0.0031108856201171875 2.1405253410339355 0.05715728551149368 0.006568273063749075 4.04296875\n",
            "repr, std, cov, clossl, z, norm 0.026391707360744476 0.0013513565063476562 2.278473138809204 0.06187332049012184 0.02274155057966709 3.607421875\n",
            "repr, std, cov, clossl, z, norm 0.02657291293144226 0.00212860107421875 2.137723207473755 0.06268059462308884 0.046735648065805435 4.0\n",
            "repr, std, cov, clossl, z, norm 0.02453400008380413 0.00572967529296875 2.0142762660980225 0.043086495250463486 0.10183132439851761 4.03125\n",
            "repr, std, cov, clossl, z, norm 0.02684858813881874 0.002414703369140625 2.15456223487854 0.05185592919588089 0.13875043392181396 3.267578125\n",
            "74\n",
            "repr, std, cov, clossl, z, norm 0.027864284813404083 0.0021495819091796875 2.196732997894287 0.07380644977092743 0.14962981641292572 3.5625\n",
            "repr, std, cov, clossl, z, norm 0.02491155080497265 0.00466156005859375 2.0558860301971436 0.02971225045621395 0.0434977151453495 4.3125\n",
            "repr, std, cov, clossl, z, norm 0.025673579424619675 0.0020656585693359375 2.2022128105163574 0.03375912457704544 0.09714660048484802 3.458984375\n",
            "repr, std, cov, clossl, z, norm 0.027633147314190865 0.001911163330078125 2.208559513092041 0.051520828157663345 0.1064564436674118 4.40234375\n",
            "repr, std, cov, clossl, z, norm 0.0262292567640543 0.0014667510986328125 2.172624111175537 0.04744207113981247 0.11336453258991241 3.287109375\n",
            "repr, std, cov, clossl, z, norm 0.02532973140478134 0.00577545166015625 2.0293946266174316 0.05286926031112671 0.11792595684528351 3.265625\n",
            "repr, std, cov, clossl, z, norm 0.02373521216213703 0.005863189697265625 2.0535717010498047 0.04693767428398132 0.10858475416898727 4.48046875\n",
            "repr, std, cov, clossl, z, norm 0.02534235455095768 0.00299072265625 2.159202814102173 0.04110149294137955 0.05804760754108429 3.73046875\n",
            "repr, std, cov, clossl, z, norm 0.0259272288531065 0.0012865066528320312 2.2169442176818848 0.04865488409996033 0.033678870648145676 3.736328125\n",
            "repr, std, cov, clossl, z, norm 0.02512517385184765 0.001415252685546875 2.237332820892334 0.03482118993997574 0.13153748214244843 3.587890625\n",
            "repr, std, cov, clossl, z, norm 0.025919796898961067 0.001720428466796875 2.224411725997925 0.02847377210855484 0.20849722623825073 4.14453125\n",
            "repr, std, cov, clossl, z, norm 0.025863680988550186 0.002254486083984375 2.102954864501953 0.033278077840805054 0.04135492444038391 4.03125\n",
            "repr, std, cov, clossl, z, norm 0.02508264034986496 0.005031585693359375 2.032078266143799 0.05354104936122894 0.07751934975385666 3.150390625\n",
            "repr, std, cov, clossl, z, norm 0.02429819479584694 0.00705718994140625 1.9986889362335205 0.036905303597450256 0.1074293777346611 4.421875\n",
            "repr, std, cov, clossl, z, norm 0.02583329565823078 0.00176239013671875 2.237330198287964 0.03503509238362312 0.08343144506216049 4.66015625\n",
            "repr, std, cov, clossl, z, norm 0.02684948220849037 0.0009036064147949219 2.258395195007324 0.03988327831029892 0.08820690214633942 3.380859375\n",
            "75\n",
            "repr, std, cov, clossl, z, norm 0.02641081064939499 0.00388336181640625 2.070157527923584 0.06871217489242554 0.08435066789388657 3.919921875\n",
            "repr, std, cov, clossl, z, norm 0.025101101025938988 0.0018291473388671875 2.213870048522949 0.04717350751161575 0.13427303731441498 4.44140625\n",
            "repr, std, cov, clossl, z, norm 0.02457684651017189 0.003810882568359375 2.1457653045654297 0.04305701702833176 0.026259662583470345 4.89453125\n",
            "repr, std, cov, clossl, z, norm 0.02650183066725731 0.0016613006591796875 2.157973289489746 0.05950424075126648 0.03714616596698761 3.619140625\n",
            "repr, std, cov, clossl, z, norm 0.025387125089764595 0.0037078857421875 2.090493679046631 0.04925476759672165 0.1150493323802948 6.9453125\n",
            "repr, std, cov, clossl, z, norm 0.025594422593712807 0.0040435791015625 2.0369508266448975 0.043050479143857956 0.1275976002216339 3.18359375\n",
            "repr, std, cov, clossl, z, norm 0.027311965823173523 0.0014333724975585938 2.174459457397461 0.08251171559095383 0.06607207655906677 3.2421875\n",
            "repr, std, cov, clossl, z, norm 0.024511920288205147 0.0028095245361328125 2.1286890506744385 0.033197492361068726 0.01006685197353363 3.33203125\n",
            "repr, std, cov, clossl, z, norm 0.024862539023160934 0.0021686553955078125 2.1605281829833984 0.018968459218740463 0.10480295121669769 3.54296875\n",
            "repr, std, cov, clossl, z, norm 0.024968363344669342 0.007564544677734375 1.920302391052246 0.08285101503133774 0.06925133615732193 4.359375\n",
            "repr, std, cov, clossl, z, norm 0.02751830965280533 0.0006513595581054688 2.3609273433685303 0.03015218675136566 0.1296611726284027 3.00390625\n",
            "repr, std, cov, clossl, z, norm 0.027170084416866302 0.0003390312194824219 2.2893238067626953 0.05645301192998886 0.150407075881958 3.958984375\n",
            "repr, std, cov, clossl, z, norm 0.025791887193918228 0.003875732421875 2.077052593231201 0.033064551651477814 0.05489277467131615 4.0390625\n",
            "repr, std, cov, clossl, z, norm 0.027114734053611755 0.0018749237060546875 2.172332286834717 0.058993156999349594 0.10294052958488464 3.044921875\n",
            "repr, std, cov, clossl, z, norm 0.024171603843569756 0.01100921630859375 1.933597445487976 0.04002443328499794 0.15259961783885956 3.095703125\n",
            "repr, std, cov, clossl, z, norm 0.02588428184390068 0.0029354095458984375 2.177814483642578 0.03854628652334213 0.1657327264547348 3.740234375\n",
            "76\n",
            "repr, std, cov, clossl, z, norm 0.025111693888902664 0.00476837158203125 2.0315449237823486 0.05004067346453667 0.09110645204782486 2.900390625\n",
            "repr, std, cov, clossl, z, norm 0.02470727637410164 0.0016307830810546875 2.1653928756713867 0.015797672793269157 0.05120697617530823 2.98046875\n",
            "repr, std, cov, clossl, z, norm 0.027268286794424057 0.000675201416015625 2.2752225399017334 0.059871282428503036 0.10700159519910812 4.1953125\n",
            "repr, std, cov, clossl, z, norm 0.025629030540585518 0.00080108642578125 2.261301279067993 0.045213207602500916 0.15598048269748688 3.80078125\n",
            "repr, std, cov, clossl, z, norm 0.02706320770084858 0.0010366439819335938 2.211057186126709 0.04086347669363022 0.16016221046447754 3.640625\n",
            "repr, std, cov, clossl, z, norm 0.025486089289188385 0.0077362060546875 1.9310775995254517 0.04408634081482887 0.1449248343706131 2.833984375\n",
            "repr, std, cov, clossl, z, norm 0.026749467477202415 0.00518035888671875 2.0402462482452393 0.04426125809550285 0.05644865706562996 2.58984375\n",
            "repr, std, cov, clossl, z, norm 0.024196717888116837 0.00860595703125 1.9665024280548096 0.02406478114426136 0.14209645986557007 3.96484375\n",
            "repr, std, cov, clossl, z, norm 0.0265723317861557 0.0018253326416015625 2.1899595260620117 0.04359409213066101 0.04214741662144661 2.962890625\n",
            "repr, std, cov, clossl, z, norm 0.026069067418575287 0.0011625289916992188 2.194196939468384 0.02473004348576069 0.12009607255458832 3.166015625\n",
            "repr, std, cov, clossl, z, norm 0.02634783275425434 0.0007357597351074219 2.257849931716919 0.04392130672931671 0.07977563142776489 4.4921875\n",
            "repr, std, cov, clossl, z, norm 0.025559812784194946 0.001491546630859375 2.2360832691192627 0.04054321348667145 0.03931726887822151 3.87890625\n",
            "repr, std, cov, clossl, z, norm 0.025180095806717873 0.0017490386962890625 2.194598436355591 0.022179828956723213 0.15062688291072845 3.697265625\n",
            "repr, std, cov, clossl, z, norm 0.02533601224422455 0.005859375 1.9429939985275269 0.05090390518307686 0.15346337854862213 3.583984375\n",
            "repr, std, cov, clossl, z, norm 0.025689993053674698 0.00493621826171875 2.0062997341156006 0.07825497537851334 0.12354674935340881 4.734375\n",
            "repr, std, cov, clossl, z, norm 0.025060946121811867 0.004657745361328125 2.024609327316284 0.06905845552682877 0.03973977640271187 3.138671875\n",
            "77\n",
            "repr, std, cov, clossl, z, norm 0.026969851925969124 0.0005316734313964844 2.2972054481506348 0.036316175013780594 0.2752916216850281 3.51953125\n",
            "repr, std, cov, clossl, z, norm 0.02715880796313286 0.0010013580322265625 2.273770332336426 0.026133345440030098 0.1279594600200653 3.537109375\n",
            "repr, std, cov, clossl, z, norm 0.027384700253605843 0.002201080322265625 2.0733516216278076 0.08432818204164505 0.003239134093746543 3.6640625\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "for i in range(100):\n",
        "    print(i)\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # # torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "    # torch.save(checkpoint, folder+'agent_nores1.pkl')\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "# batch64 28m58s 84\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "7c8eef2f-e0f8-4da0-d569-9ae36d65b100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "# ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n",
        "\n",
        "\n",
        "# # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "# 2  5/11 8\n",
        "# 1/10 4 7/9\n",
        "# 0  3/12 6\n",
        "\n",
        "# 13 11 14\n",
        "# 10 12 9\n",
        "\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "env = TimeLimit(env, max_episode_steps=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PraFUAPB3j7v",
        "outputId": "d1cd30c1-2ebf-4df7-fd6c-18a03aee88e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-28fc5c7292bd>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-50-28fc5c7292bd>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dided\n",
            "time\n",
            "[12, 12, 13, 12, 12, 12, 7, 13, 12, 12, 7, 12, 12, 12, 13, 7, 12, 12, 13, 7, 12, 13, 13, 12, 13, 12, 12, 7, 12, 13, 13, 7, 12, 12, 4, 2, 12, 12, 13, 10, 12, 12, 0, 13, 12, 12, 13, 7, 12, 13, 12, 13, 13, 13, 7, 7, 12, 12, 13, 8, 12, 13, 7, 12, 12, 13, 12, 10, 12, 12, 0, 13, 12, 12, 4, 13, 12, 12, 7, 12, 12, 13, 7, 10, 12, 12, 7, 12, 12, 12, 0, 7, 12, 12, 0, 13, 12, 13, 7, 7, 12, 12, 8, 7, 12, 12, 7, 8, 12, 12, 7, 12, 12, 12, 8, 13, 12, 12, 13, 7, 12, 12, 10, 7, 12, 12, 12, 13, 12, 12, 7, 12, 12, 12, 7, 10, 12, 12, 7, 7, 12, 12, 12, 13, 13, 12, 13, 10, 12, 12, 0, 8, 12, 12, 13, 7, 12, 12, 7, 4, 12, 12, 13, 8, 12, 12, 13, 7, 12, 12]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    lstate=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6], [1, 256]\n",
        "        lstate.append(state)\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # act = agent(state, k)\n",
        "            act = agent(lstate, k=k)\n",
        "            lstate=[]\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"dided\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD0pMl_-X72Z",
        "outputId": "bb05d65e-f0c4-41f0-c92d-a20e2d65c3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 1])\n"
          ]
        }
      ],
      "source": [
        "x=torch.rand(2,3)\n",
        "print(x)\n",
        "print(torch.argmin(x,dim=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9cm6KjvBrnNO"
      },
      "outputs": [],
      "source": [
        "# @title alllll\n",
        "for i in range(30):\n",
        "    # print(\"#### simulate ####\")\n",
        "    # buffer_=[]\n",
        "    for _ in range(5):\n",
        "        buffer = simulate(agent, buffer)\n",
        "        # buffer_ = simulate(agent, buffer_)\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # torch.save(checkpoint, folder+'agentoptim1.pkl')\n",
        "\n",
        "    # buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "    # with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "    print(\"train_data.data\",len(train_data.data))\n",
        "    while len(train_data.data)>20000: # 10000:6.9gb, 20000:5.5gb\n",
        "        buffer.pop(random.randrange(len(buffer)))\n",
        "        train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792,
          "referenced_widgets": [
            "4d169c3fccfc47b999dbde2baad0f5eb",
            "9f2655e556d04c2ab24a59d0e3e46004",
            "6b19ecb296c74aa39f7eac4f45526e62",
            "8b06111fc2a845989082207c1093253b",
            "5b1cef22b67548c2b6d141b28b588320",
            "deb8b1cb3008430b8bc26f48d9092d99",
            "19a0c3ba19594648b773013f71ca6552",
            "4166c7ed511040ddbf2ff15b17c2c829"
          ]
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "fcc79638-d4f4-4e49-ba41-6e6849c2c1d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.14.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:u7fq02d8) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.173 MB of 0.173 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d169c3fccfc47b999dbde2baad0f5eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>▄▂▄▄█▃▂▃▃▄▂▂▁▁▂▂▂▂▃▂▂▃▂▄▂▂▃▂▃▃▂▃▂▃▂▃▂▁▃▄</td></tr><tr><td>cov</td><td>█▇▇▆▇▇▆▇█▄▅▅█▅▅▄▄▅▅▅▃▄▅▃▄▄▃▃▃▄▄▃▃▃▂▃▂▁▁▂</td></tr><tr><td>repr</td><td>▇███▇█▅▅▅▄▄▄▃▄▃▄▃▃▂▃▂▃▄▃▂▁▂▂▂▃▂▃▂▂▂▂▂▁▂▅</td></tr><tr><td>std</td><td>▂▄▃▆▄▃▆▂▇▇▄▃▆▃▂▂▃▆▂▇▂▂▁▂▃█▄▁▂▆▃▁▃▁▁▃▂▅▁▄</td></tr><tr><td>z_norm</td><td>▅█▄▂▇▃▄▅▂▆▃▁▅▅▃▅▂▂▃▂▂▃▅▃▃▄▄▄▄▂▃▄▄▇▆▁▅▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>closs</td><td>0.39359</td></tr><tr><td>cov</td><td>1.78726</td></tr><tr><td>repr</td><td>0.02306</td></tr><tr><td>std</td><td>0.00711</td></tr><tr><td>z_norm</td><td>0.06123</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">different-plant-112</strong> at: <a href='https://wandb.ai/bobdole/procgen/runs/u7fq02d8' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/u7fq02d8</a><br/> View project at: <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241001_062301-u7fq02d8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:u7fq02d8). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241001_063742-hw120o4h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/hw120o4h' target=\"_blank\">trim-gorge-113</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/hw120o4h' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/hw120o4h</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(project=\"procgen\",\n",
        "    config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        _,T,_ = sx.shape\n",
        "        batch = 1\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            # print(i,x.data, z.squeeze(), loss.item())\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mY7BITKjSKC",
        "outputId": "cbad0ffc-e8e9-4a4d-bd3c-d6d9ddaf6e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2336301\n",
            "1278976\n",
            "399360\n",
            "1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-76-e20d23bca149>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=3, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # e = d_model**-0.5\n",
        "        # self.h0 = torch.empty((self.jepa.pred.num_layers, 1, d_model), device=device).uniform_(-e, e) # [num_layers, batch, d_model]\n",
        "        # self.h0 = torch.normal(mean=0, std=e, size=(self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # torch.nn.init.xavier_uniform_(self.h0) # xavier_uniform_, kaiming_normal_\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "        state = torch.zeros((1, 3,64,64), device=device)\n",
        "        self.sx = self.jepa.enc(state)\n",
        "\n",
        "    # def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        self.update_h0(lstate, laction)\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "            # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.h0=lh0[k].unsqueeze(1) # [num_layers, 1, d_model]\n",
        "        # self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        self.lx, self.lz = lx[k:], lz[k:] # [T, dim_a], [T, dim_z]\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "\n",
        "                # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "                # print(torch.cat(lstate, dim=0).shape)\n",
        "                # lsx = self.jepa.enc(torch.stack(lstate, dim=0))#.unsqueeze(0)\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx-torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                # batch, seq_len, _ = lstate.shape\n",
        "                # seq_len, _ = lstate.shape\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    try: la = self.emb(self.la[:seq_len])\n",
        "                    except:\n",
        "                        print(\"err self.la\")\n",
        "                        # la = self.emb([0]*seq_len)\n",
        "                        la = self.emb(torch.zeros(seq_len, dtype=int, device=device))\n",
        "\n",
        "        # lz = nn.Parameter(torch.zeros((batch, seq_len, self.dim_z),device=device))\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        # optim_z = torch.optim.SGD([lz], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([lz], 1e0, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "\n",
        "        for i in range(20): # num epochs\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                # print(sxaz.shape, self.h0.shape)\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                # sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                loss = F.mse_loss(out_, out)\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            print(lz.data)\n",
        "            with torch.no_grad(): lz.clamp_(min=-1, max=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1]\n",
        "        self.la = la[k:]\n",
        "        return h0\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                # x[:,:self.lx.shape[1]], z[:,:self.lz.shape[1]] = self.lx[:,:T], self.lz[:,:T]\n",
        "                x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].repeat(batch,1,1), self.lz[:T].repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            # print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            # print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i, \"loss\", loss.squeeze().data)\n",
        "            # print(x.shape,torch.argmax(-dist,dim=-1).shape,z.shape,loss.shape) # [16, 6, 3], [16, 6], [16, 6, 1], [16, 1]\n",
        "            # print(i, torch.cat([x,torch.argmax(-dist,dim=-1),z],dim=-1).squeeze().data)\n",
        "            print(i, \"x act z\", torch.cat([x[0],torch.argmax(-dist,dim=-1)[0].unsqueeze(-1),z[0]],dim=-1).squeeze().data)\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, t, dim_a], [batch, t, dim_z]\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        x = nn.Parameter(torch.empty((T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        optim_x = torch.optim.SGD([x], lr=1e3) # 1e-1,1e-0,1e4 ; 1e2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        with torch.no_grad(): x[:self.lx.shape[0]] = self.lx[:T] # [seq_len, dim_az]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        # print(\"search x\",x.squeeze().data)\n",
        "        for i in range(2): # 5\n",
        "            dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data # [T, dim_a]\n",
        "            z = self.argm_s(sx, x_,h0) # [T, dim_z]\n",
        "            loss, lh0 = self.rnn_pred(sx, x_.unsqueeze(0), z.unsqueeze(0), h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i, \"search x loss\", x.squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [T]\n",
        "        return lact, lh0, x.data, z # [T], [T, num_layers, batch, d_model], [T, dim_a], [T, dim_z]\n",
        "\n",
        "\n",
        "    def search_optimxz(self, sx, T=6, h0=None):\n",
        "        self.eval()\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 4 # 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1 ; sgd\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_uniform_(z) # xavier_normal_, xavier_uniform_\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        # optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e1, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].unsqueeze(0).repeat(batch,1,1), self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        print(\"search\", z[0].squeeze())\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        for i in range(10): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.sum().backward()\n",
        "            # optim_x.step(); optim_z.step()\n",
        "            # optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            print(i, \"search loss\", loss.squeeze().data)\n",
        "            # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "            print(i, \"search z\", z[0].squeeze().data)\n",
        "            # print(torch.argmin(dist,dim=-1).int())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        print(\"c\",torch.stack(c)[:,idx])\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "\n",
        "    def argm_s(self, sx, x, h0): # [1, d_model], [batch_, T, dim_a], [num_layers, 1, d_model] # batch argm z for search\n",
        "        batch_, T, _ = x.shape\n",
        "        batch = 16 # 16\n",
        "        # z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        z = nn.Parameter(torch.zeros((batch*batch_, T, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        optim_z = torch.optim.SGD([z], lr=1e4, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # with torch.no_grad(): z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_z]\n",
        "        with torch.no_grad(): z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch*batch_,1,1) # [batch, seq_len, dim_z]\n",
        "        sx, h0 = sx.detach(), h0.detach() # [1, d_model], [num_layers, 1, d_model]\n",
        "        x = x.detach().repeat(batch,1,1) # [batch, T, dim_a]\n",
        "        # print(\"argm\", z[0].squeeze())\n",
        "        for i in range(2): # 5\n",
        "            loss, lh0 = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"argm z loss\", z[0].squeeze().data, loss[0].squeeze().data)\n",
        "        # idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        idx = torch.argmin(loss.sum(-1).unflatten(0, (batch,batch_)), dim=0) # loss [batch*batch_, T]\n",
        "        return torch.index_select(z, 0, idx) # [batch_, T,dim_z]\n",
        "\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        c=[]\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            # print(sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            c.append(tcost)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    # imshow(state[0].cpu())\n",
        "                    # print(\"norm\", torch.norm(sy[0]-sy_[0], dim=-1))\n",
        "                    # # if torch.norm(sy[0]-sy_[0], dim=-1) > 1:\n",
        "                    # print(i, reward[0])\n",
        "                    # print(sy)\n",
        "                    # print(sy_)\n",
        "                    # print(sy[0]-sy_[0])\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "\n",
        "                    # cost loss\n",
        "                    # syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = 100*clossl\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batc h_size, d_model]\n",
        "            # sx=sy_\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "\n",
        "                    # z = self.jepa.argm(sy_, a, sy)\n",
        "                    z = self.argm(sy, sy_, h0, a, reward)\n",
        "                    with torch.no_grad(): z.mul_(torch.rand_like(z)).mul_((torch.rand_like(z)>0.5).bool()) # dropout without scailing\n",
        "\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "\n",
        "                    # cost loss\n",
        "                    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    # syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    # print(h0.requires_grad)\n",
        "                    # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "                    # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "                    # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    # torch.norm(sy-sx, dim=-1)\n",
        "                    # sx=sy\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    norm = torch.norm(sy, dim=-1)[0].item()\n",
        "                    z_norm = torch.norm(z)\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "\n",
        "                # lh0 = torch.zeros((rwd.shape[1],)+h0.shape, device=device)\n",
        "                # lz = torch.zeros((lsy.shape[0], lsy.shape[1], self.dim_z), device=device)\n",
        "                    # for name, param in agent.tcost.named_parameters():\n",
        "                    #     print(\"param.data\",param.max().item(),param.min().item())\n",
        "                    #     print(\"agent.tcost\",param.data)\n",
        "\n",
        "# # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "# ema_model = torch.optim.swa_utils.AveragedModel(model, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n",
        "# for epoch in range(300):\n",
        "#       for input, target in loader:\n",
        "#           optimizer.zero_grad()\n",
        "#           loss_fn(model(input), target).backward()\n",
        "#           optimizer.step()\n",
        "#           ema_model.update_parameters(model)\n",
        "# # Update bn statistics for the ema_model at the end\n",
        "# torch.optim.swa_utils.update_bn(loader, ema_model)\n",
        "# # Use ema_model to make predictions on test data\n",
        "# preds = ema_model(test_input)\n",
        "\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd): # best case z for train\n",
        "        # self.tcost.eval() # disable tcost dropout\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(2): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lsy_, lh0 = self.rnn_it(sy_, la, lz, h0_)\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            clossl = self.tcost.loss(syh0, rwd.flatten(), reduction='none')\n",
        "            # z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl# + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        return lz.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "\n",
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cg0BI2TwY9-p"
      },
      "outputs": [],
      "source": [
        "# @title z.grad.data = -z.grad.data\n",
        "\n",
        "# self.eval()\n",
        "batch = 4 # 16\n",
        "x = nn.Parameter(torch.empty((batch, T, agent.dim_a),device=device))\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# optim_ = torch.optim.SGD([x,z], lr=1e1) # 3e3\n",
        "optim_ = torch.optim.AdamW([x,z], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "print(\"search z\", z[0].squeeze())\n",
        "print(\"search x\", x[0].squeeze())\n",
        "sx, h0 = sx.detach(), h0.detach()\n",
        "for i in range(10): # num epochs\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    z.grad.data = -z.grad.data\n",
        "    optim_.step()\n",
        "    optim_.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "    print(i, \"search loss\", loss.squeeze().data)\n",
        "    print(i, \"search z\", z[0].squeeze().data)\n",
        "    print(i, \"search x\", x[0].squeeze().data)\n",
        "dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "print(\"c\",torch.stack(c)[:,idx])\n",
        "# return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "# print(lact[idx], lh0[:,:,idx,:], x[idx], z[idx])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5VMebkQ1mJtD"
      },
      "outputs": [],
      "source": [
        "# @title argm agent.rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def argm(sx, x,h0, lr=3e3): # 3e3\n",
        "    # agent.eval()\n",
        "    # batch_size, T, _ = sx.shape\n",
        "    batch = 16 # 16\n",
        "    z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "    torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "    optim_z = torch.optim.SGD([z], lr=1e3, maximize=True) # 3e3\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.LBFGS([z], max_iter=5, lr=1)\n",
        "\n",
        "    # print(\"argm\", z[0].squeeze())\n",
        "    sx, h0 = sx.detach(), h0.detach()\n",
        "    x = x.detach().repeat(batch,1,1)\n",
        "    for i in range(5): # num epochs\n",
        "        # print(sx.shape, x.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "        loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "        loss.sum().backward()\n",
        "        optim_z.step()\n",
        "        optim_z.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "        # print(i, \"argm loss\", loss.squeeze().data)\n",
        "        # print(i, \"argm z\", z[0].squeeze().data)\n",
        "    idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "    return z[idx].unsqueeze(0)\n",
        "\n",
        "\n",
        "T=1\n",
        "xx = torch.empty((1, T, agent.dim_a))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "# print(x.shape)\n",
        "optim_x = torch.optim.SGD([x], lr=1e1) # 1e-1,1e-0,1e4 ; 1e2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "\n",
        "state = torch.zeros((1, 3,64,64))\n",
        "with torch.no_grad():\n",
        "    sx = agent.jepa.enc(state).detach()\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "print(time.time()-start)\n",
        "\n",
        "print(\"search\",x.squeeze().data)\n",
        "for i in range(20): # 5\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    z = argm(sx, x_,h0)\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [1, 1, 3], [1, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "    print(i, \"search loss\", x.squeeze().data, loss.item())\n",
        "    # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "\n",
        "# z sgd 1e3\n",
        "# 9 search loss tensor([0.0142, 0.0142, 0.0142, 0.0142])\n",
        "# 9 search z tensor([-0.3381, -0.7005, -0.5877, -0.0664, -0.1439,  0.0283,  0.0541, -0.1439])\n",
        "\n",
        "# x sgd 1e2\n",
        "# 1 tensor([0.3561, 0.3059, 0.8830]) 0.014148875139653683\n",
        "# 9 tensor([0.3560, 0.3064, 0.8828]) 2.328815611463142e-07\n",
        "\n",
        "# 1e0\n",
        "# 19 tensor([-0.5768,  0.5778,  0.5774]) 6.543130552927323e-07\n",
        "# 19 tensor([0.3570, 0.6689, 0.6521]) 2.474381801675918e-07\n",
        "# 19 tensor([0.5783, 0.5765, 0.5772]) 1.519319567933053e-07\n",
        "# 19 tensor([0.3427, 0.6795, 0.6487]) 4.220427456402831e-07\n",
        "#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YVeF35lpwqlJ"
      },
      "outputs": [],
      "source": [
        "# @title test batch argm\n",
        "\n",
        "def argm(lsy, sy, h0, la, rwd): # best case z for train\n",
        "    # self.tcost.eval() # disable tcost dropout\n",
        "    batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "    lz = nn.Parameter(torch.zeros((batch_size, bptt, agent.dim_z), device=device))\n",
        "    torch.nn.init.normal_(lz, mean=0., std=.01/lz.shape[-1]**0.5)\n",
        "    optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "    # z = nn.Parameter(torch.zeros((batch_size, bptt, 1), device=device))\n",
        "    # torch.nn.init.normal_(z, mean=0., std=.01/z.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([z], lr=1e-1) # 1e-2\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    # optim = torch.optim.AdamW([lz], 1e-2, (0.9, 0.999)) # 1e-1\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1) # 1e-1\n",
        "    # optim = torch.optim.SGD([lz], lr=1e-2, momentum=0.9) # 1e-2\n",
        "    lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "    # print(z.squeeze().data[-1])\n",
        "    print(lz.squeeze().data[-1])\n",
        "    for i in range(20): # 10\n",
        "        # lz = torch.cat([z, torch.zeros((batch_size, bptt, agent.dim_z-z.shape[-1]), device=device)], dim=-1)\n",
        "        sy_, h0_ = sy.detach(), h0.detach()\n",
        "        lsy_, lh0 = agent.rnn_it(sy_, la, lz, h0_)\n",
        "        repr_loss = F.mse_loss(lsy, lsy_)\n",
        "        syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "        clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "        cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i,lz.squeeze().data, cost.item())\n",
        "        k=-6\n",
        "        # print(i,lz.squeeze().data[k:], cost.item(), repr_loss.item(), clossl.item())\n",
        "        print(i, cost.item(), repr_loss.item(), clossl.item())\n",
        "        # print(i,z.squeeze().data[-6], cost.item(), repr_loss.item(), clossl.item())\n",
        "        # with torch.no_grad(): pred = agent.tcost(syh0)\n",
        "        # print(\"pred\",pred.squeeze().data[k:])\n",
        "        # print(i, cost.item())\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "    # self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    # print(z.squeeze().data)\n",
        "    print(lsy_.squeeze().data[-1][:5])\n",
        "    with torch.no_grad(): pred = agent.tcost(syh0)\n",
        "    print(\"rwd\",rwd.squeeze().data)\n",
        "    print(\"rwd\",rwd.squeeze().data[k:])\n",
        "    print(\"pred\",pred.squeeze().data)\n",
        "    return lz.detach()\n",
        "\n",
        "\n",
        "def argm(lsy, sy, h0, la, rwd): # best case z for train\n",
        "    # self.tcost.eval() # disable tcost dropout\n",
        "    batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "    # lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "\n",
        "    batch = 64 # 16\n",
        "    lz = nn.Parameter(torch.zeros((batch*batch_size, bptt, agent.dim_z), device=device))\n",
        "    # print(lsy.shape, sy.shape, h0.shape, la.shape, rwd.shape)\n",
        "    sy = sy.detach().repeat(batch,1)\n",
        "    la = la.detach().repeat(batch,1,1)\n",
        "    rwd = rwd.detach().repeat(batch,1)\n",
        "    lsy = lsy.detach().repeat(batch,1,1)\n",
        "    h0 = h0.detach().repeat(1,batch,1)\n",
        "    # print(lsy.shape, sy.shape, h0.shape, la.shape, rwd.shape)\n",
        "    # [batch*batch_size, bptt, d_model], [batch*batch_size, d_model], [num_layers, batch*batch_size, d_model], [batch*batch_size, d_model, dim_a], [batch*batch_size, bptt]\n",
        "    torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "    optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    print(lz.squeeze().data[-1])\n",
        "    lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "    for i in range(2): # 10\n",
        "        sy_, h0_ = sy.detach(), h0.detach()\n",
        "        lsy_, lh0 = agent.rnn_it(sy_, la, lz, h0_)\n",
        "        # repr_loss = F.mse_loss(lsy, lsy_)\n",
        "        repr_loss = ((lsy-lsy_)**2).unflatten(0, (batch,batch_size)).flatten(1).mean(-1)\n",
        "        syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "        # clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "        clossl = agent.tcost.loss(syh0, rwd.flatten(), reduction='none').unflatten(0, (batch,batch_size*bptt)).mean(-1)\n",
        "        cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        print(i, cost.item(), repr_loss.item(), clossl.item())\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "    # self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    # return lz.detach()\n",
        "    idx = torch.argmin(cost.unflatten(0, (batch_size,batch)), dim=1) # choose best x even with greatest adv z\n",
        "    print(lsy_.squeeze().data[-1][:5])\n",
        "    with torch.no_grad(): pred = agent.tcost(syh0)\n",
        "    print(\"rwd\",rwd.squeeze().data)\n",
        "    print(\"rwd\",rwd.squeeze().data[k:])\n",
        "    print(\"pred\",pred.squeeze().data)\n",
        "    return lz.unflatten(0, (batch,batch_size))[idx].squeeze(0).detach()\n",
        "\n",
        "\n",
        "\n",
        "# nn.HuberLoss() # near 0 is L2, outside is L1. less sensitive to outliers than L2? use like mse\n",
        "\n",
        "\n",
        "# for batch, (state, action, reward) in enumerate(train_loader):\n",
        "#     imshow(torchvision.utils.make_grid(state[0].cpu(), nrow=10))\n",
        "#     break\n",
        "# it = iter(train_loader)\n",
        "state, action, reward = next(it)\n",
        "imshow(torchvision.utils.make_grid(state[0].cpu(), nrow=10))\n",
        "\n",
        "\n",
        "# for batch, (state, action, reward) in enumerate(train_loader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "sy_ = agent.jepa.enc(torch.zeros((batch_size, 3,64,64), device=device)).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "# state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "bptt=25#25\n",
        "for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        lsy = agent.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "        la = agent.emb(act) # [batch_size, bptt, dim_a]\n",
        "        lz = argm(lsy, sy_.squeeze(1), h0, la, rwd) # [batch_size, bptt, d_model],\n",
        "    break\n",
        "# break\n",
        "# print(lz.squeeze().data)\n",
        "\n",
        "# import torch\n",
        "# la = torch.arange(15).reshape(3, 5)\n",
        "# la = la.repeat(2,1)\n",
        "# print(la)\n",
        "# print(la.unflatten(0, (2,3)))\n",
        "# # idx = torch.argmin(loss.unflatten(0, (batch_size,batch)), dim=1) # choose best x even with greatest adv z\n",
        "\n",
        "\n",
        "\n",
        "# sgd\n",
        "# 49 tensor([-0.2660, -0.0749, -0.1358, -0.0611, -0.0368, -0.0041,  0.0691,  0.0764]) 31.68710708618164\n",
        "# 9 tensor([ 0.1100,  0.0433,  0.0141,  0.0167, -0.1715, -0.0177,  0.1303,  0.0350]) 31.716447830200195\n",
        "# 4 tensor([ 0.1101,  0.0435,  0.0145,  0.0172, -0.1721, -0.0174,  0.1302,  0.0355]) 31.693767547607422\n",
        "# 49 tensor([ 0.1081,  0.0407,  0.0088,  0.0105, -0.1647, -0.0212,  0.1312,  0.0288]) 31.741121292114258\n",
        "\n",
        "# 49 tensor([-0.0199,  0.0148, -0.0822,  0.0207, -0.0466,  0.0609,  0.0938,  0.0440]) 31.563823699951172 0.04605092480778694 0.31333568692207336\n",
        "# 1 tensor([ 0.0220, -0.2026,  0.0452,  0.1610, -0.0399,  0.1887, -0.0626,  0.0763]) 31.575210571289062 0.044768013060092926 0.31351369619369507\n",
        "\n",
        "# 49 tensor([ 0.0784,  0.1738,  0.1562, -0.2048, -0.1269,  0.1467,  0.0242, -0.0849]) 31.541893005371094 0.04185735061764717 0.31332606077194214\n",
        "# tensor([-4.4024e-02,  1.5736e-01,  1.3286e-01,  9.2922e-02, -1.6661e-02,\n",
        "\n",
        "# 49 tensor([ 0.1917, -0.2293,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]) 31.557968139648438 0.04187680780887604 0.3134858310222626\n",
        "# tensor([-0.0824,  0.0021,  0.0125,  0.3892, -0.1041])\n",
        "# 49 tensor([-0.2967, -0.3361]) 31.584087371826172 0.04877716675400734 0.3134020268917084\n",
        "# tensor([-0.0822,  0.0095, -0.0060,  0.6069, -0.2085])\n",
        "# 49 tensor([0.0765, 0.0352]) 31.576648712158203 0.046897441148757935 0.3134216070175171\n",
        "# tensor([-0.0218,  0.1031,  0.1796,  0.2206, -0.0100])\n",
        "\n",
        "# 49 tensor(0.2663) 35.62339782714844 0.05059394985437393 0.3537042737007141\n",
        "# tensor([-0.0054,  0.1192,  0.1170,  0.2161, -0.0741])\n",
        "# 49 tensor(0.2765) 31.58695411682129 0.0482589527964592 0.31345659494400024\n",
        "# tensor([-0.0076,  0.0901,  0.1391,  0.1246, -0.0049])\n",
        "# 49 tensor(0.4091) 31.576698303222656 0.04228857159614563 0.31365254521369934\n",
        "# tensor([-0.0398,  0.0932,  0.0598,  0.2072, -0.0510])\n",
        "\n",
        "# 49 tensor(0.2947) 31.615554809570312 0.05741892755031586 0.31328460574150085\n",
        "# tensor([ 0.0335,  0.0820,  0.0564,  0.2376, -0.0323])\n",
        "\n",
        "\n",
        "# 31.553237915039062 0.039482660591602325 0.3135582506656647\n",
        "# tensor([ 0.0844, -0.0316,  0.4948,  0.1411, -0.1128, -0.1359,  0.6359, -0.1350,\n",
        "#          0.2688,  0.2809, -0.3960,  0.3010,  0.4960, -0.1607, -0.1109,  0.0719,\n",
        "#          0.1345, -0.2463,  0.2220, -0.0375,  0.0021, -0.2562,  0.2311, -0.0293,\n",
        "#          0.3958])\n",
        "# tensor([-0.0477,  0.1160,  0.1507,  0.1311, -0.0629])\n",
        "# 31.62661361694336 0.05305254086852074 0.31361350417137146\n",
        "# tensor([-0.6893,  0.4970,  0.6107, -0.2914, -0.1954, -0.3417,  0.6757, -0.2306,\n",
        "#          0.5352, -0.3400, -0.1038,  0.3860,  0.2331,  0.1387,  0.5997, -0.0533,\n",
        "#         -0.4756, -0.4694,  0.2403,  0.1279,  0.0730, -0.2124, -0.0072, -0.2314,\n",
        "#         -0.2863])\n",
        "# tensor([-0.0535,  0.0922,  0.1302,  0.5576, -0.1107])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "# optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def argm(self, sx, lr=3e3): # 3e3\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch_size, T, _ = sx.shape\n",
        "        batch = 16\n",
        "        # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "        z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "        optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "        sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "        # sx = sx.detach()\n",
        "        for i in range(20): # 10\n",
        "            # print(sx.shape,z.shape)\n",
        "            sxz = torch.cat([sx, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                cost = model(sxz)\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "            # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "            # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        # return z.detach()\n",
        "        # print(\"argm z\",z.squeeze().data)\n",
        "        # print(\"cost\",cost.squeeze())\n",
        "        idx = torch.argmax(loss)\n",
        "        # return z[idx].detach().unsqueeze(0)\n",
        "        return z[:,idx].detach()\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=1\n",
        "        T=1\n",
        "        x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "\n",
        "        lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "        # ratio = 6e0\n",
        "        lr = 1e-1 # adamw 1e-1\n",
        "        ratio = 4\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        # optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze())\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "        for i in range(50):\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = argm(x)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "            # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "            # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                x.clamp_(min=-1, max=1)\n",
        "            # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "        idx = torch.argmax(loss)\n",
        "        print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd):\n",
        "        self.tcost.eval()\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "            lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "                for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                    syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                    out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                    lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                    lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "                repr_loss = F.mse_loss(lsy, lsy_)\n",
        "                syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "                # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "                cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        return lz.detach()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jx0k_ndHOEMe"
      },
      "outputs": [],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "N2TGs69fnrZo",
        "outputId": "e7624553-e17a-4a9f-85a4-512720ed329a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcost.1.weight torch.Size([2, 512])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU(267, 256, batch_first=True, dropout=0.2)\n"
          ]
        }
      ],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "# for name, param in agent.emb.named_parameters():\n",
        "for name, param in agent.tcost.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjEWGq2WGi9a",
        "outputId": "649e3612-f156-496e-d8d5-fc576110e2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0015, -0.0132,  0.0280,  ...,  0.0297,  0.0289,  0.0152],\n",
            "        [ 0.0168,  0.0031, -0.0288,  ..., -0.0064, -0.0137, -0.0085]])\n"
          ]
        }
      ],
      "source": [
        "# print(vars(agent.jepa.pred.))\n",
        "# print(vars(agent.tcost.state_dict()))\n",
        "# print(agent.jepa.pred._parameters.keys())\n",
        "# print(agent.jepa.pred._parameters['weight_ih_l0'])\n",
        "# print(agent.jepa.pred._parameters['weight_hh_l2']) # weight_hh_l0, weight_hh_l2\n",
        "# print(agent.tcost.state_dict().keys())\n",
        "print(agent.tcost.state_dict()['tcost.1.weight']) # tcost.2.bias, tcost.4.bias\n",
        "# print(agent.tcost.named_parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_xnBFjXVxgz"
      },
      "outputs": [],
      "source": [
        "# @title transfer weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 10  # size of the first dimension\n",
        "a = 5   # size of the extra nodes to omit\n",
        "m = 8   # output dimension\n",
        "\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "target_layer = nn.Linear(d, m)\n",
        "# source_layer = nn.Linear(d, m)\n",
        "# target_layer = nn.Linear(d+a, m)\n",
        "\n",
        "def transfer(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt.weight[:, :src.weight.shape[1]].copy_(src.weight[:, :tgt.weight.shape[1]])\n",
        "        tgt.bias.copy_(src.bias)\n",
        "    return tgt,src\n",
        "\n",
        "target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "src_sd = source_layer.state_dict()\n",
        "tgt_sd = target_layer.state_dict()\n",
        "\n",
        "def transfersd(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt['weight'][:, :src['weight'].shape[1]].copy_(src['weight'][:, :tgt['weight'].shape[1]])\n",
        "        tgt['bias'].copy_(src['bias'])\n",
        "    return tgt\n",
        "\n",
        "tgt_sd = transfersd(tgt_sd, src_sd)\n",
        "target_layer.load_state_dict(tgt_sd)\n",
        "\n",
        "\n",
        "agent_src = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "\n",
        "# agent.tcost = TCost((1+agent.jepa.pred.num_layers)*agent.d_model) # replace tcost\n",
        "\n",
        "agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# agent.jepa.pred\n",
        "# target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(vars(agent.jepa.pred))\n",
        "# gru = agent.jepa.pred\n",
        "# gru = agent_src.jepa.pred\n",
        "# for wht_name in gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, gru._parameters[wht_name].shape)\n",
        "\n",
        "# weight_ih_l0 dim_z=3: [768, 262] , dim_z=1: [768, 260]\n",
        "# weight_hh_l0 torch.Size([768, 256])\n",
        "# bias_ih_l0 torch.Size([768])\n",
        "# bias_hh_l0 torch.Size([768])\n",
        "\n",
        "# tgt_gru = agent.jepa.pred\n",
        "# src_gru = agent_src.jepa.pred\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "\n",
        "tgt_gru[]\n",
        "def transfer_gru(tgt_gru, src_gru): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(len(tgt_gru._all_weights), len(src_gru._all_weights))):\n",
        "        # for lyr in tgt_gru._all_weights:\n",
        "            lyr = tgt_gru._all_weights[i]\n",
        "            for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "                # print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "                tgt_wht, src_wht = tgt_gru._parameters[wht_name], src_gru._parameters[wht_name]\n",
        "                if len(tgt_wht.shape)==2:\n",
        "                    tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "                elif len(tgt_wht.shape)==1:\n",
        "                    tgt_gru._parameters[wht_name] = src_wht\n",
        "    return tgt_gru\n",
        "tgt_gru = transfer_gru(tgt_gru, src_gru)\n",
        "\n",
        "# for wht_name in tgt_gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d_model=256; dim_a=3; dim_z=1; dim_v=512\n",
        "\n",
        "pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "# pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "print(pred._all_weights)\n",
        "for lyr in pred._all_weights:\n",
        "    for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "        print(wht_name, pred._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(pred.state_dict().keys())\n",
        "\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "print(tgt_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "print(src_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "\n",
        "print(tgt_gru.state_dict()['bias_ih_l0'][:10])\n",
        "print(src_gru.state_dict()['bias_ih_l0'][:10])\n",
        "tgt_gru.state_dict().keys()\n",
        "src_gru.state_dict().keys()\n",
        "\n",
        "# tgt_gru\n",
        "# src_gru\n",
        "for wht_name in tgt_gru.state_dict().keys():\n",
        "    if not wht_name in src_gru.state_dict().keys(): continue\n",
        "    print(wht_name)\n",
        "    # print(tgt_gru.state_dict()[wht_name])\n",
        "    # tgt_gru.state_dict()[wht_name].copy_(src_gru.state_dict()[wht_name])\n",
        "\n",
        "tgt_sd = tgt_gru.state_dict()\n",
        "src_sd = src_gru.state_dict()\n",
        "def transfer_sd(tgt_sd, src_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            # print(wht_name)\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            if len(tgt_wht.shape)==2:\n",
        "                tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "            elif len(tgt_wht.shape)==1:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "    return tgt_sd\n",
        "tgt_sd = transfer_sd(tgt_sd, src_sd)\n",
        "print(tgt_sd['weight_ih_l0'][0][:10])\n",
        "print(tgt_sd['bias_ih_l0'][:10])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CwApoQMMKzB",
        "outputId": "98f67f91-ef5b-406f-b852-5a93130f9e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0012018680572509766\n",
            "tensor([0.2797, 0.2218, 0.2731, 0.3268, 0.2632, 0.2914, 0.3217, 0.2845])\n"
          ]
        }
      ],
      "source": [
        "# @title test init norm\n",
        "print(agent.emb.state_dict()['weight'].norm(dim=-1))\n",
        "\n",
        "# x = torch.rand(16)\n",
        "x = torch.rand(8,16)\n",
        "# print(x)\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1.0)\n",
        "# torch.nn.init.xavier_normal_(x)\n",
        "import time\n",
        "start = time.time()\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # 0.00966, 0.000602, 0.0004\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1./x.shape[-1]**0.5)\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "print(time.time()-start)\n",
        "# std = ((Sum (xi-mean)^2)/ N)^(1/2)\n",
        "# print(x)\n",
        "# print(((x**2).sum())**(0.5))\n",
        "print(torch.norm(x, dim=-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_UlGz6Xoq3"
      },
      "source": [
        "## archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B1yvJkX89C_o"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein\n",
        "import torch\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    # cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    # dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # cs = (x-y).cumsum(dim=-1)\n",
        "    cs = (x-y) @ torch.tril(torch.ones(x.shape[0], x.shape[0]))\n",
        "    # dist = weight * torch.abs(cs)\n",
        "    dist = weight * cs**2\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "\n",
        "def soft_wasserstein_loss(x, y, smoothing=0.1):\n",
        "    # Normalise distributions\n",
        "    x = x / x.sum()\n",
        "    y = y / y.sum()\n",
        "    # Compute the cumulative distributions (CDFs) with a small smoothing factor\n",
        "    cdf_x = torch.cumsum(x, dim=-1) + smoothing\n",
        "    cdf_y = torch.cumsum(y, dim=-1) + smoothing\n",
        "    # Compute smooth Wasserstein distance (L2 distance between CDFs)\n",
        "    distance = torch.norm(cdf_x - cdf_y, p=2)  # L2 distance instead of L1 for smoother gradients\n",
        "    return distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5], dtype=float))\n",
        "x = nn.Parameter(torch.tensor([-0.01, -0.0, -0.99], dtype=torch.float))\n",
        "y = torch.tensor([0.0, 0.0, -1.0], dtype=torch.float)\n",
        "\n",
        "# x = nn.Parameter(torch.rand(1024, dtype=float))\n",
        "# y = torch.rand(1024, dtype=float)\n",
        "# a = len(train_data.buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "a=1/45\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "print(weight)\n",
        "dist = wasserstein(x, y, weight=weight)\n",
        "print(time.time() - start)\n",
        "print(dist)  # Should output 0.7\n",
        "# dist.backward()\n",
        "\n",
        "# 0.0004496574401855469\n",
        "# 0.000331878662109375\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3nfZRhVc9Ssp"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein sinkhorn train\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# agent.eval()\n",
        "# batch_size, T, _ = sx.shape\n",
        "x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3) # 3e3\n",
        "# optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.999)) # ? 1e0 ; 3e-2 1e-1\n",
        "# optim = torch.optim.AdamW([x], 1e-0, (0.9, 0.95)) # ? 1e0 ; 3e-2 1e-1\n",
        "y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=torch.float)\n",
        "a=1/45\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "# print(weight)\n",
        "\n",
        "# loss = wasserstein(x, y, weight=weight)\n",
        "# loss = wasserstein(x, y)\n",
        "# loss = sinkhorn(x, y)\n",
        "# loss.backward()\n",
        "# print(x.grad)\n",
        "\n",
        "\n",
        "for i in range(50): # num epochs\n",
        "    loss = wasserstein(x, y, weight=weight)\n",
        "    # loss = sinkhorn(x, y)\n",
        "    # loss = sinkhorn(x, y,0.05,80)\n",
        "    loss.sum().backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(x.data, loss.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def sinkhorn(x, y, epsilon=0.05, max_iters=100):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "\n",
        "    # Compute the cost matrix: here the cost is the squared distance between indices\n",
        "    # (|i-j|^2 for each position i, j)\n",
        "    posx = torch.arange(x.shape[-1], dtype=torch.float).unsqueeze(1)\n",
        "    posy = torch.arange(y.shape[-1], dtype=torch.float).unsqueeze(0)\n",
        "    cost_matrix = (posx - posy).pow(2)  # squared distance\n",
        "\n",
        "    # Initialize the dual variables\n",
        "    u = torch.zeros_like(x)\n",
        "    v = torch.zeros_like(y)\n",
        "\n",
        "    # Sinkhorn iterations\n",
        "    K = torch.exp(-cost_matrix / epsilon)  # Kernel matrix, regularised with epsilon\n",
        "    for _ in range(max_iters):\n",
        "        u = x / (K @ (y / (K.t() @ u + 1e-8)) + 1e-8)\n",
        "        v = y / (K.t() @ (x / (K @ v + 1e-8)) + 1e-8)\n",
        "    # print(K,u.data,v.data)\n",
        "    plan = torch.diag(u) @ K @ torch.diag(v)\n",
        "    dist = torch.sum(plan * cost_matrix)\n",
        "    return dist\n",
        "\n",
        "# Example\n",
        "x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float, requires_grad=True)\n",
        "y = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float)\n",
        "# x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "# y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=float)\n",
        "\n",
        "# dist = sinkhorn(x, y)\n",
        "dist = sinkhorn(x, y, 0.05,80)\n",
        "dist.backward()  # To compute gradients with respect to x\n",
        "\n",
        "print(dist.item())\n",
        "print(x.grad)\n",
        "\n",
        "# [2.0000e+07, 3.0000e+07, 1.0000e-08]) tensor([       0.,        0., 49999996.] episodes>=80\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s1_GgDzoDyYB"
      },
      "outputs": [],
      "source": [
        "# @title torchrl.data.PrioritizedReplayBuffer\n",
        "from torchrl.data import LazyMemmapStorage, LazyTensorStorage, ListStorage\n",
        "buffer_lazytensor = ReplayBuffer(storage=LazyTensorStorage(size))\n",
        "\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "buffer_lazymemmap = ReplayBuffer(storage=LazyMemmapStorage(size), batch_size=32, sampler=SamplerWithoutReplacement())\n",
        "\n",
        "\n",
        "from torchrl.data import ListStorage, PrioritizedReplayBuffer\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "rb = PrioritizedReplayBuffer(alpha=0.7, beta=0.9, storage=ListStorage(10))\n",
        "data = range(10)\n",
        "rb.extend(data)\n",
        "# rb.extend(buffer)\n",
        "\n",
        "\n",
        "sample = rb.sample(3)\n",
        "print(sample)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gcvgdCB1h1_E"
      },
      "outputs": [],
      "source": [
        "# @title torch.optim.LBFGS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Example of a deep nonlinear model f(x)\n",
        "class DeepNonlinearModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNonlinearModel, self).__init__()\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(10, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "f = DeepNonlinearModel()\n",
        "# x = torch.randn(1, 10, requires_grad=True)\n",
        "# xx = torch.randn((1,10))\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "\n",
        "# Define loss function (mean squared error for this example)\n",
        "target = torch.tensor([[0.0]])  # Target output\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "def closure():\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    return loss\n",
        "\n",
        "optimizer = torch.optim.LBFGS([x], lr=1.0, max_iter=5)  # Limit to 2-3 iterations for speed\n",
        "start_time = time.time()\n",
        "for _ in range(2):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step(closure)  # Perform a step of optimisation\n",
        "    print(loss.item())\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "start_time = time.time()\n",
        "optimizer = torch.optim.SGD([x], lr=1e1, maximize=True) # 3e3\n",
        "for _ in range(5):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step()  # Perform a step of optimisation\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    print(loss.item())\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gfzJ6zoL6vGc"
      },
      "outputs": [],
      "source": [
        "# @title torch.optim.LBFGS\n",
        "\n",
        "# def closure():\n",
        "#     optimizer.zero_grad()  # Zero out the gradients\n",
        "#     output = f(x)          # Forward pass through the model\n",
        "#     loss = loss_fn(output, target)  # Calculate the loss\n",
        "#     loss.backward()         # Backpropagate\n",
        "#     return loss\n",
        "\n",
        "batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "z = nn.Parameter(torch.zeros((batch_size, bptt, 1), device=device))\n",
        "torch.nn.init.normal_(z, mean=0., std=.3/z.shape[-1]**0.5)\n",
        "lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "print(z.squeeze().data[-1])\n",
        "# print(lz.squeeze().data[-1])\n",
        "\n",
        "\n",
        "# for batch, (state, action, reward) in enumerate(train_loader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "sy = agent.jepa.enc(torch.zeros((batch_size, 3,64,64), device=device))#.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "\n",
        "def closure():\n",
        "    lz = torch.cat([z, torch.zeros((batch_size, bptt, agent.dim_z-z.shape[-1]), device=device)], dim=-1)\n",
        "    sy_, h0_ = sy.detach(), h0.detach()\n",
        "    lsy_, lh0 = agent.rnn_it(sy_, la, lz, h0_)\n",
        "    repr_loss = F.mse_loss(lsy, lsy_)\n",
        "    syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "    clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "    cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl\n",
        "    cost.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "    return cost\n",
        "\n",
        "import time\n",
        "optimizer = torch.optim.LBFGS([z], lr=1e-2, max_iter=10)  # Limit to 2-3 iterations for speed\n",
        "start_time = time.time()\n",
        "for _ in range(20):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step(closure)  # Perform a step of optimisation\n",
        "    print(loss.item())\n",
        "\n",
        "end_time = time.time()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8WHjFn2gmzI"
      },
      "source": [
        "## plot 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VX5IExbRriwm"
      },
      "outputs": [],
      "source": [
        "# @title sklearn RBF\n",
        "# https://gist.github.com/eljost/2c4e1af652ef02b2989da341c5569af7\n",
        "# from nn_plot.ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import scipy.stats as st\n",
        "\n",
        "# np.random.seed(1)\n",
        "def func(x):\n",
        "    # print(x.shape)\n",
        "    # x= np.sum(x**2, axis=-1)\n",
        "    x=np.random.rand(x.shape[0])\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "res = 50\n",
        "num_pts=15\n",
        "X=np.random.rand(num_pts,2)*res\n",
        "# Y = func(X)\n",
        "Y=np.random.rand(num_pts)\n",
        "# print(X);print(Y)\n",
        "\n",
        "lim = 1\n",
        "# lin = np.linspace(-lim, lim, res)\n",
        "lin = np.linspace(0, res, res)\n",
        "x1, x2 = np.meshgrid(lin, lin)\n",
        "xx = np.vstack((x1.flatten(), x2.flatten())).T\n",
        "\n",
        "kernel = RBF()\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gp.fit(X, Y)\n",
        "# print(\"Learned kernel\", gp.kernel_)\n",
        "y_mean, y_cov = gp.predict(xx, return_cov=True)\n",
        "\n",
        "posteriors = st.multivariate_normal.rvs(mean=y_mean, cov=y_cov, size=1)\n",
        "\n",
        "ax = plt.figure().add_subplot(projection='3d')\n",
        "Z=posteriors.reshape(-1, res)\n",
        "# ax.plot_surface(x1, x2, Z)\n",
        "ax.plot_surface(x1, x2, Z, cmap='rainbow', alpha=0.7)\n",
        "\n",
        "# ax.plot_surface(x1, x2, posteriors.reshape(-1, res))\n",
        "ax.contour(x1, x2, Z, zdir='z', offset=-1, cmap='coolwarm') # https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html#sphx-glr-gallery-mplot3d-contour3d-3-py\n",
        "# ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-0.4, 0.5))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-1, 2))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, c=zdata, cmap='Greens');\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, cmap='Greens');\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SusX7gpzxFNL",
        "outputId": "9f14a9da-e188-49ba-f5f5-70192ff33134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-2.6321], grad_fn=<AddBackward0>)\n",
            "tensor([2.7358]) tensor([-4.7358])\n"
          ]
        }
      ],
      "source": [
        "# @title chatgpt RBFKernelLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBFKernelLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        self.centres = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        dists = torch.cdist(x, self.centres, p=2) ** 2\n",
        "        return torch.exp(-dists / (2 * self.sigma ** 2))\n",
        "\n",
        "class SaddlePointNetwork(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(SaddlePointNetwork, self).__init__()\n",
        "        self.rbf_layer = RBFKernelLayer(in_features, out_features, sigma)\n",
        "        self.linear = nn.Linear(out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rbf_output = self.rbf_layer(x)\n",
        "        # Introduce a saddle point structure\n",
        "        linear_output = self.linear(rbf_output)\n",
        "        # Example saddle function: x^2 - y^2\n",
        "        saddle_output = torch.sum(linear_output[:, :1]**2 - linear_output[:, 1:]**2, dim=1, keepdim=True)\n",
        "        return saddle_output\n",
        "\n",
        "# sin(ax)sin(bx)\n",
        "# (x^2 - y^2)\n",
        "import torch\n",
        "\n",
        "def rbf_saddle(x, y, gamma=1.0, a=1.0, b=1.0):\n",
        "    # RBF-like term\n",
        "    rbf_term = torch.exp(-gamma * torch.norm(x - y, p=2)**2)\n",
        "    # Saddle point term\n",
        "    saddle_term = (a * x)**2 - (b * y)**2\n",
        "    return rbf_term + saddle_term\n",
        "\n",
        "# Example usage\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "output = rbf_saddle(x, y)\n",
        "print(output)\n",
        "\n",
        "# Compute gradients\n",
        "output.backward()\n",
        "print(x.grad, y.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rTmCo7pm0NxL"
      },
      "outputs": [],
      "source": [
        "# @title plot 3d\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "y = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "X, Y = torch.meshgrid(x, y)\n",
        "Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fWZaQTDFg1",
        "outputId": "4c5ced88-54f1-436e-89f9-66f1c8396373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000, -0.7231,  0.3792,  0.0000]]) tensor([0.3362])\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title shape NN\n",
        "num_pts=1\n",
        "\n",
        "# X=torch.rand(num_pts,4)*2-1\n",
        "# X=torch.cat([torch.tensor([0,0]).unsqueeze(0),torch.rand(num_pts,2)*2-1], dim=-1)\n",
        "X=torch.cat([torch.zeros(1,1),torch.rand(num_pts,2)*2-1,torch.zeros(1,1)], dim=-1)\n",
        "Y=torch.rand(num_pts)\n",
        "print(X,Y)\n",
        "optim = torch.optim.SGD(model.parameters(), 1e-1)\n",
        "\n",
        "# model.train()\n",
        "pred = model(X)\n",
        "# print(Y.shape,pred.shape)\n",
        "# loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "loss = F.mse_loss(Y, pred.squeeze(-1))\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "boDd__PE2sGy"
      },
      "outputs": [],
      "source": [
        "# @title plot NN\n",
        "\n",
        "xx = torch.linspace(-1, 1, 100)\n",
        "yy = torch.linspace(-1, 1, 100)\n",
        "X, Y = torch.meshgrid(xx, yy) # [100,100]\n",
        "xy = torch.cat([X.unsqueeze(-1), torch.zeros(X.shape+(2,)), Y.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "with torch.no_grad(): Z = model(xy).squeeze(-1)\n",
        "# print(Z)\n",
        "# print(Z.shape)\n",
        "\n",
        "# Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qW6BYoXsX57o"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle same time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def f(x, y):\n",
        "    return x ** 2 - y ** 2 + x * y\n",
        "# (x-y)(x+y)+xy\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "dim_x, dim_z = 3, 8\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "xx = torch.empty((1, T, dim_x))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "# x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "\n",
        "# tensor([[0.6478, 0.0531, 0.0861]]) tensor([[-1.,  1.]]) 0.2974517047405243\n",
        "# tensor([-0.9419, -1.0000,  0.4416, -1.0000,  1.0000,  0.2963])\n",
        "\n",
        "# x = nn.Parameter(torch.tensor([[0.6478, 0.0531, 0.0861]]))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "z = nn.Parameter(torch.empty((batch, T, dim_z)))\n",
        "# z = torch.empty((1, T, 1))\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# z = nn.Parameter(z.repeat(batch,1,1))\n",
        "# z = nn.Parameter(torch.tensor([[-1.,  1.]]))\n",
        "# optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "# optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.95), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "# .95,1e-1,3e-1\n",
        "# .99,\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(10): # num epochs\n",
        "    # loss = f(x,z)\n",
        "    # loss = f(x.sum(-1),z)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step(); optim_z.step()\n",
        "    optim_x.zero_grad(); optim_z.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x.squeeze()[0].data, z[0].squeeze().data, loss[0].squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "        z.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "# idx = torch.argmax(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GJdFpDr2wIMT"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle argm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def argm(sx, lr=3e3): # 3e3\n",
        "    # batch=sx.size(dim=0)\n",
        "    batch_size, T, _ = sx.shape\n",
        "    batch = 16\n",
        "    # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "    # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "    z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "    torch.nn.init.xavier_uniform_(z)\n",
        "    # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "    # sx = sx.detach()\n",
        "    for i in range(20): # 10\n",
        "        # print(sx.shape,z.shape)\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            cost = model(sxz)\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "        # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    # return z.detach()\n",
        "    print(cost.squeeze().data)\n",
        "    idx = torch.argmax(cost.squeeze(), dim=1)\n",
        "    return z[torch.arange(z.shape[0]),idx].detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(xx.clone())\n",
        "x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50):\n",
        "    z = argm(x)\n",
        "    # print(x.shape,z.shape)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "# print(\"z\",z)\n",
        "# print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjIJP6RlEv2",
        "outputId": "447fdefd-452b-437d-c228-1847492b36f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = torch.randn(16, 16)\n",
        "# print((b==torch.max(b)).nonzero())\n",
        "x = torch.randn(10, 3)\n",
        "idx = torch.randint(3,(10,))\n",
        "# print(x[:,idx].shape)\n",
        "print(x[torch.arange(x.shape[0]),idx].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HcOidvtW9KAH"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cX71EprCMSNG"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim bad?\n",
        "\n",
        "import torch\n",
        "\n",
        "def transfer_optim(src_optim, tgt_optim, param_mapping):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    tgt_sd = tgt_optim.state_dict()\n",
        "\n",
        "    # Iterate over each parameter in the target optimizer\n",
        "    for (tgt_idx, target_param) in enumerate(tgt_optim.param_groups[0]['params']):\n",
        "        target_id = id(target_param)\n",
        "\n",
        "        # Find the corresponding source parameter using param_mapping\n",
        "        if target_id in param_mapping:\n",
        "            source_param = param_mapping[target_id]\n",
        "            source_id = id(source_param)\n",
        "\n",
        "            # If there's an existing state for the source parameter, transfer it\n",
        "            if source_id in src_sd['state']:\n",
        "                source_state = src_sd['state'][source_id]\n",
        "                target_state = {}\n",
        "\n",
        "                # Handle momentum/first and second moments (e.g., `exp_avg`, `exp_avg_sq` in Adam)\n",
        "                for key in source_state.keys():\n",
        "                    if source_state[key].shape == target_param.shape: target_state[key] = source_state[key].clone()\n",
        "                    # If size doesn't match, either copy what you can or initialise new values\n",
        "                    elif key in ['exp_avg', 'exp_avg_sq']:  # Momentums (specific to Adam-like optimizers)\n",
        "                        target_state[key] = torch.zeros_like(target_param)\n",
        "                        target_state[key][:source_param.numel()] = source_state[key].flatten()[:target_param.numel()]\n",
        "                    else: target_state[key] = torch.zeros_like(target_param) # init\n",
        "                tgt_sd['state'][target_id] = target_state\n",
        "\n",
        "    # Load the updated state dict back into the target optimizer\n",
        "    tgt_optim.load_state_dict(tgt_sd)\n",
        "    return tgt_optim\n",
        "# {'state': {0: {'step': tensor(1.), 'exp_avg': tensor, 'exp_avg_sq': tensor}, 1: }}\n",
        "\n",
        "\n",
        "\n",
        "model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "source_optimizer = optim.AdamW(model_src.parameters())\n",
        "target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "dummy_input = torch.randn(3, 10)\n",
        "dummy_target = torch.randn(3, 5)\n",
        "criterion = torch.nn.MSELoss()\n",
        "output = model_src(dummy_input)\n",
        "loss = criterion(output, dummy_target)\n",
        "loss.backward()\n",
        "source_optimizer.step()\n",
        "\n",
        "param_mapping = {id(tgt_param): src_param for src_param, tgt_param in zip(model_src.parameters(), model_tgt.parameters())}\n",
        "target_optimizer = transfer_optim(source_optimizer, target_optimizer, param_mapping)\n",
        "\n",
        "print(source_optimizer.state_dict())\n",
        "print(target_optimizer.state_dict())\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title transfer_optim bad? 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    opt_state_dict = optimizer.state_dict()\n",
        "    for group in opt_state_dict['param_groups']:\n",
        "        # For each parameter index (p in param group refers to the layer parameters)\n",
        "        for param_idx, p in enumerate(group['params']):\n",
        "            print(p,source_layer.weight)\n",
        "            if p == source_layer.weight:\n",
        "                # Find the corresponding target layer parameter (in this case, target_layer.weight)\n",
        "                target_param = target_layer.weight\n",
        "                source_state = optimizer.state[p]  # Get the state for the source parameter\n",
        "\n",
        "                # If the parameter is found in the optimizer's state dict\n",
        "                if 'exp_avg' in source_state and 'exp_avg_sq' in source_state:\n",
        "                    exp_avg = source_state['exp_avg']  # First moment (momentum)\n",
        "                    exp_avg_sq = source_state['exp_avg_sq']  # Second moment (variance)\n",
        "\n",
        "                    # Handle input dimension mismatch (copy/truncate or pad)\n",
        "                    source_in_dim = source_layer.weight.shape[1]\n",
        "                    target_in_dim = target_layer.weight.shape[1]\n",
        "\n",
        "                    # Copy optimizer state (exp_avg and exp_avg_sq) accordingly\n",
        "                    with torch.no_grad():\n",
        "                        # Copy the available part and initialize new dimensions to zero\n",
        "                        new_exp_avg = torch.zeros_like(target_param)\n",
        "                        new_exp_avg_sq = torch.zeros_like(target_param)\n",
        "                        # new_exp_avg[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        # new_exp_avg_sq[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        new_exp_avg[:, :source_in_dim] = exp_avg[:, :target_in_dim]\n",
        "                        new_exp_avg_sq[:, :source_in_dim] = exp_avg_sq[:, :target_in_dim]\n",
        "\n",
        "                    # Update the target layer's optimizer state\n",
        "                    optimizer.state[target_param] = {\n",
        "                        'exp_avg': new_exp_avg,\n",
        "                        'exp_avg_sq': new_exp_avg_sq,\n",
        "                        'step': source_state['step']  # Keep the same step count\n",
        "                    }\n",
        "\n",
        "                # Handle the bias (if it exists)\n",
        "                if hasattr(source_layer, 'bias') and hasattr(target_layer, 'bias'):\n",
        "                    source_bias = optimizer.state[source_layer.bias]\n",
        "                    target_bias = target_layer.bias\n",
        "\n",
        "                    optimizer.state[target_bias] = source_bias\n",
        "    return optimizer\n",
        "\n",
        "# Example usage:\n",
        "d = 10  # Input dimension of the source layer\n",
        "a = 5   # Extra nodes to be omitted or added in the target layer\n",
        "m = 8   # Output dimension (same for both)\n",
        "\n",
        "# Source layer (input dimension d+a)\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "\n",
        "# Target layer (input dimension d, or d+a, or arbitrary)\n",
        "target_layer = nn.Linear(d, m)\n",
        "\n",
        "# Optimizer (using AdamW in this case)\n",
        "optimizer = torch.optim.AdamW(source_layer.parameters())\n",
        "\n",
        "# Perform weight transfer (from d+a to d or vice versa) here (assumed done already)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "# Transfer optimizer states\n",
        "optimizer = transfer_optimizer_state(source_layer, target_layer, optimizer)\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    state_dict = optimizer.state_dict()\n",
        "    for old_param, new_param in zip(source_layer.parameters(), target_layer.parameters()):\n",
        "        # If old_param exists in optimizer state\n",
        "        if old_param in state_dict['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = state_dict['state'][old_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                if key in ['exp_avg', 'exp_avg_sq']:  # for Adam or AdamW momentum estimates\n",
        "                    # Handle the shape adjustment (copy, shrink, or randomly initialise the extra nodes)\n",
        "                    new_state[key] = torch.zeros_like(new_param)  # Initialise with zeros\n",
        "                    new_state[key][:old_param.shape[0]] = value[:new_param.shape[0]]  # Copy old values\n",
        "                    # else:\n",
        "                    #     new_state[key] = value.clone()  # Copy directly if shapes match\n",
        "                else:\n",
        "                    new_state[key] = value  # Copy other states directly if they exist\n",
        "\n",
        "            # Set the new parameter in optimizer state\n",
        "            state_dict['state'][new_param] = new_state\n",
        "            # Remove the old parameter from the optimizer state\n",
        "            del state_dict['state'][old_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(state_dict)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optim(src_model, tgt_model, src_optim, tgt_optim):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    for src_param, tgt_param in zip(src_model.parameters(), tgt_model.parameters()):\n",
        "        # If src_param exists in optimizer state\n",
        "        if src_param in src_sd['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = src_sd['state'][src_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                new_state[key] = torch.zeros_like(tgt_param)  # Initialise with zeros\n",
        "                new_state[key][:src_param.shape[0]] = value[:tgt_param.shape[0]]  # Copy old values\n",
        "\n",
        "            src_sd['state'][tgt_param] = new_state\n",
        "            del src_sd['state'][src_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(src_sd)\n",
        "    return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "LKUSzmYLLuRh",
        "outputId": "07ca4b89-257b-4205-c5c8-6a96474ae82a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-186620617543>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# j=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mwht_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwht_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(o)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title rename wht_name\n",
        "# wht_name='jepa.enc.cnn.0.weight'\n",
        "wht_name='jepa.pred.weight_ih_l0'\n",
        "# wht_name='emb.weight'\n",
        "# print(o.isnumeric())\n",
        "# mask = [x.isnumeric() for x in o]\n",
        "# print(o[mask])\n",
        "na_=''\n",
        "# j=0\n",
        "\n",
        "for wht_name in agent.state_dict().keys():\n",
        "    o=wht_name.split('.')\n",
        "    # print(o)\n",
        "    name=wht_name\n",
        "    print(\"####\", wht_name)\n",
        "    for i in range(len(o)):\n",
        "        c = o[i]\n",
        "        if c.isnumeric():\n",
        "            na = '.'.join(o[:i])\n",
        "            me = '.'.join(o[i+1:])\n",
        "            # print(c_,c, c_<c, )\n",
        "            c=int(c)\n",
        "            if na!=na_: # param name diff\n",
        "                j=0 # reset num\n",
        "                c_=c # track wht_name num\n",
        "                na_=na # track param name\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('1', name)\n",
        "            elif c_<c: # same param name, diff num\n",
        "                j+=1\n",
        "                c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('2', name)\n",
        "            else: # same param name, same num\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('3', name)\n",
        "    print('4', name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CACQCCaxA_Y",
        "outputId": "b5d127cd-18ce-49e5-b1e2-d883cb34125a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1746836772511624\n"
          ]
        }
      ],
      "source": [
        "# @title geomloss, Python Optimal Transport\n",
        "# !pip install geomloss[full]\n",
        "\n",
        "import torch\n",
        "from geomloss import SamplesLoss  # See also ImagesLoss, VolumesLoss\n",
        "\n",
        "# # Create some large point clouds in 3D\n",
        "# x = torch.randn(100000, 3, requires_grad=True).cuda()\n",
        "# y = torch.randn(200000, 3).cuda()\n",
        "\n",
        "# x = torch.rand(1000, 1)\n",
        "# y = torch.rand(1000, 1)\n",
        "x = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "y = torch.tensor([0, 1, 0]).float().unsqueeze(-1)\n",
        "# k=1.\n",
        "# y = torch.tensor([k, k, k]).float().unsqueeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01) # 0.05, quadratic, Wasserstein-2. low blur => closer to true Wasserstein dist but slower compute\n",
        "\n",
        "loss = loss_fn(x, y)  # By default, use constant weights = 1/number of samples\n",
        "print(loss)\n",
        "# g_x, = torch.autograd.grad(L, [x])\n",
        "\n",
        "# [0, 1, 0]: 2.4253e-12, 2.4253e-12\n",
        "# [0, 0, 0.1]: 0.1350; [0, 0, 0.5]: 0.0417; [0, 0, 1]: 0\n",
        "# k=0.: 0.1666; k=0.1: 0.1383; k=0.333: 0.1111; k=0.5: 0.1250; k=1.: 0.3333\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "# Define x and y as n-dimensional tensors representing mass distributions\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# y = torch.tensor([0, 0, 1], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# x = torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1))\n",
        "x = nn.Parameter(torch.tensor([0,1.5,0]).float().unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "\n",
        "# Create a position tensor representing the index of each element\n",
        "positions_x = torch.arange(x.shape[0], dtype=float).unsqueeze(1)\n",
        "positions_y = torch.arange(y.shape[0], dtype=float).unsqueeze(1)\n",
        "\n",
        "# Sinkhorn loss using GeomLoss\n",
        "loss_fn = SamplesLoss(\"sinkhorn\", p=1, blur=0.05)  # p=1 for Wasserstein-1\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=1, blur=0.05, scaling=0.9, debias=True)\n",
        "\n",
        "transport_cost = loss_fn(positions_x, x, positions_y, y)\n",
        "\n",
        "print(transport_cost.item())\n",
        "# 1.298424361328248\n",
        "\n",
        "transport_cost.backward()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install POT\n",
        "\n",
        "import ot\n",
        "import numpy as np\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / np.sum(x)\n",
        "    # y = y / np.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = np.abs(np.arange(n)[:, None] - np.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = np.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "x = np.array([0.2, 0.3, 0.5])\n",
        "y = np.array([0, 0, 1])\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "# distance.backward()\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / torch.sum(x)\n",
        "    # y = y / torch.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = torch.abs(torch.arange(n)[:, None] - torch.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = torch.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "# x = np.array([0.2, 0.3, 0.5])\n",
        "# y = np.array([0, 0, 1])\n",
        "x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float())#.unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float()#.unsqueeze(-1)\n",
        "\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "distance.backward()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MqBL9hljvW-5"
      },
      "outputs": [],
      "source": [
        "# @title batchify argm train\n",
        "\n",
        "def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "    self.jepa.pred.train()\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "    lsx=sx.unsqueeze(1)\n",
        "    h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "    lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "    # print(lsx.shape, la.shape, lz.shape)\n",
        "    c=[]\n",
        "    for t in range(seq_len):\n",
        "        a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "        # print(sx.shape, a.shape, z.shape)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            tcost = -self.tcost(syh0)\n",
        "        c.append(tcost)\n",
        "        lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "        lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        cost += (tcost + icost)*gamma**t\n",
        "    return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "\n",
        "def argm(self, sy, sy_, h0, a, reward, lr=3e3): # 3e3\n",
        "    self.tcost.eval()\n",
        "    batch_size = sy.shape[0] # [batch_size, d_model]\n",
        "    z = nn.Parameter(torch.zeros((batch_size, self.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(z)\n",
        "    torch.nn.init.normal_(z, mean=0., std=.3/z.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([z], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    sy, sy_ = sy.detach(), sy_.detach()\n",
        "    out = sy - sy_\n",
        "    h0, a, reward = h0.detach(), a.detach(), reward.detach()\n",
        "    for i in range(10): # 10\n",
        "        with torch.amp.autocast('cuda'):\n",
        "\n",
        "\n",
        "\n",
        "            syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "            out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            repr_loss = F.mse_loss(out, out_[:, -1, :])\n",
        "            # syh0 = torch.cat([sy.flatten(1),F.dropout(h0_, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            syh0 = torch.cat([sy.flatten(1),h0_.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "            z_loss = torch.abs(z).sum() # z_loss = torch.norm(z)\n",
        "            print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def argm(lsy, sy, h0, la, rwd):\n",
        "    # lz = agent.argm(out, h0, la, reward)\n",
        "    agent.tcost.eval()\n",
        "    batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "    lz = nn.Parameter(torch.zeros((batch_size, bptt, agent.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(lz)\n",
        "    torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "    for i in range(3): # 10\n",
        "        sy_, h0_ = sy.detach(), h0.detach()\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "        with torch.cuda.amp.autocast():\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0_ = agent.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl + agent.zloss_coeff * z_loss\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    agent.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return lz.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# closs_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01)\n",
        "bptt = 25\n",
        "for batch, Sar in enumerate(train_loader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "    state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "    sy_ = agent.jepa.enc(state).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    # sx=sy_\n",
        "    state, action, reward = Sar # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "    state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "    for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "        with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "            lsy = agent.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "            la = agent.emb(act) # [batch_size, bptt, dim_a]\n",
        "            out = lsy - torch.cat([sy_, lsy[:,:-1]], dim=1)\n",
        "            # lz = agent.argm(out, h0, la, reward)\n",
        "            lz = argm(lsy, sy_, h0, la, rwd)\n",
        "            # lz = torch.zeros((batch_size, bptt, agent.dim_z), device=device)\n",
        "\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0 = agent.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            std_loss, cov_loss = agent.jepa.v_creg(agent.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "            jloss = agent.jepa.sim_coeff * repr_loss + agent.jepa.std_coeff * std_loss + agent.jepa.cov_coeff * cov_loss\n",
        "\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            # print(\"syh0, rwd\",syh0.shape,rwd.shape)\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            # reward_ = agent.tcost(syh0)\n",
        "            # clossl = wasserstein(rwd, reward_)#.squeeze(-1)\n",
        "            closs = agent.closs_coeff * clossl\n",
        "\n",
        "            # print(h0.requires_grad)\n",
        "            # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "            # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "            # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "            # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "            # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "            loss = jloss + closs\n",
        "\n",
        "            # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "            norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "            z_norm = torch.norm(z)\n",
        "            # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "            # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "            print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            optim.zero_grad()\n",
        "            sy_, h0 = sy_.detach(), h0.detach()\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "o0UHSueqx9W8"
      },
      "outputs": [],
      "source": [
        "# @title test tcost3\n",
        "# def train_jepa(self, dataloader, optim, bptt=25): #32\n",
        "#     self.train()\n",
        "#     for batch, (state, action, reward) in enumerate(dataloader): # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "#         state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "#         with torch.cuda.amp.autocast():\n",
        "#             lsy = self.jepa.enc(state.flatten(end_dim=1)) # [batch_size, bptt, d_model]\n",
        "#             # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "#             # lsy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "#             std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy))\n",
        "#             jloss = self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "#             clossl = self.tcost.loss(lsy, reward.flatten(end_dim=1))\n",
        "#             closs = self.closs_coeff * clossl\n",
        "\n",
        "#             pred = self.tcost(lsy).squeeze(-1).unflatten(0, reward.shape) # [batch_size, bptt]\n",
        "#             print(\"pred\",pred[0])\n",
        "#             print(\"rwd\",reward[0])\n",
        "#             mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "#             # # print(\"rwd, pred, clossl\", rwd[mask].data, pred[mask].data, clossl.item())\n",
        "#             # try: imshow(torchvision.utils.make_grid(st[0].cpu(), nrow=10))\n",
        "#             # # try: imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "#             # except ZeroDivisionError: pass\n",
        "\n",
        "#         loss = jloss + closs\n",
        "\n",
        "#         print(\"std, cov, clossl, wrong\", std_loss.item(), cov_loss.item(), clossl.item(), mask.sum().item())\n",
        "#         # print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "#         scaler.scale(loss).backward()\n",
        "#         # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "#         scaler.step(optim)\n",
        "#         scaler.update()\n",
        "#         optim.zero_grad()\n",
        "#         try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "#         except: pass\n",
        "\n",
        "# # for i in range(5):\n",
        "# #     print(i)\n",
        "# #     train_jepa(agent, train_loader, optim)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Jt_UlGz6Xoq3",
        "m8WHjFn2gmzI",
        "wUhKd009Qvk3"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d169c3fccfc47b999dbde2baad0f5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f2655e556d04c2ab24a59d0e3e46004",
              "IPY_MODEL_6b19ecb296c74aa39f7eac4f45526e62"
            ],
            "layout": "IPY_MODEL_8b06111fc2a845989082207c1093253b"
          }
        },
        "9f2655e556d04c2ab24a59d0e3e46004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b1cef22b67548c2b6d141b28b588320",
            "placeholder": "​",
            "style": "IPY_MODEL_deb8b1cb3008430b8bc26f48d9092d99",
            "value": "0.173 MB of 0.173 MB uploaded\r"
          }
        },
        "6b19ecb296c74aa39f7eac4f45526e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a0c3ba19594648b773013f71ca6552",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4166c7ed511040ddbf2ff15b17c2c829",
            "value": 1
          }
        },
        "8b06111fc2a845989082207c1093253b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1cef22b67548c2b6d141b28b588320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb8b1cb3008430b8bc26f48d9092d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19a0c3ba19594648b773013f71ca6552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4166c7ed511040ddbf2ff15b17c2c829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}